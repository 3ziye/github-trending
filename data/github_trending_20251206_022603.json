[
  {
    "basic_info": {
      "name": "Valdi",
      "full_name": "Snapchat/Valdi",
      "owner": "Snapchat",
      "description": "Valdi is a cross-platform UI framework that delivers native performance without sacrificing developer velocity.",
      "url": "https://github.com/Snapchat/Valdi",
      "clone_url": "https://github.com/Snapchat/Valdi.git",
      "ssh_url": "git@github.com:Snapchat/Valdi.git",
      "homepage": "https://discord.gg/uJyNEeYX2U",
      "created_at": "2025-11-06T17:33:28Z",
      "updated_at": "2025-12-06T02:18:53Z",
      "pushed_at": "2025-12-06T00:22:55Z"
    },
    "stats": {
      "stars": 15755,
      "forks": 530,
      "watchers": 15755,
      "open_issues": 55,
      "size": 82359
    },
    "tech_info": {
      "language": "C++",
      "languages": {
        "C++": 8572773,
        "TypeScript": 4873655,
        "JavaScript": 2334883,
        "Swift": 1888769,
        "C": 1666128,
        "Kotlin": 1186386,
        "Objective-C": 1005698,
        "Objective-C++": 564462,
        "Starlark": 434096,
        "Java": 43293,
        "Shell": 39457,
        "Smarty": 7040,
        "HTML": 4271,
        "Python": 3905,
        "Pug": 645,
        "Makefile": 426,
        "Dockerfile": 235,
        "Linker Script": 171,
        "CSS": 140,
        "Go": 86,
        "SCSS": 20
      },
      "license": "Other",
      "topics": [
        "android",
        "cross-platform",
        "ios",
        "typescript",
        "valdi"
      ]
    },
    "content": {
      "readme": "# Valdi\n\n[![License: MIT](https://img.shields.io/badge/License-MIT-blue.svg)](./LICENSE.md)\n[![Platforms](https://img.shields.io/badge/platform-iOS%20%7C%20Android%20%7C%20macOS-lightgrey)](./docs/INSTALL.md)\n[![Status](https://img.shields.io/badge/status-beta-yellow)]()\n[![Discord](https://img.shields.io/discord/1285677307163574322?color=7289da&label=Discord&logo=discord&logoColor=white)](https://discord.gg/uJyNEeYX2U)\n[![TypeScript](https://img.shields.io/badge/TypeScript-5.x-blue?logo=typescript)](https://www.typescriptlang.org/)\n[![Documentation](https://img.shields.io/badge/docs-available-brightgreen)](./docs/README.md)\n[![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg)](./CONTRIBUTING.md)\n\n> [!NOTE]\n> **Beta Status:** Valdi has been widely used in Snap's production apps for the last 8 years. We're calling this a beta because our tools and documentation need more battle testing in the open source world. Valdi will exit beta when we're happy with the developer experience.\n\n**Valdi is a cross-platform UI framework that delivers native performance without sacrificing developer velocity.** Write your UI once in declarative TypeScript, and it compiles directly to native views on iOS, Android, and macOSâ€”no web views, no JavaScript bridges. \n\n## Quick Example\n\nA basic Valdi component:\n\n```tsx\nimport { Component } from 'valdi_core/src/Component';\n\nclass HelloWorld extends Component {\n  onRender() {\n    const message = 'Hello World! ğŸ‘»';\n    <view backgroundColor='#FFFC00' padding={30}>\n      <label color='black' value={message} />\n    </view>;\n  }\n}\n```\n\n<p align=\"center\">\n  <img src=\"./docs/docs/assets/start-about/IMG_1445.jpg\" width=\"400\" alt=\"Hello World example running on iOS\" />\n</p>\n\n## Quick Start\n\n**Prerequisites:** Xcode (macOS only) - everything else is automatic!\n\n```bash\n# Install Valdi CLI\nnpm install -g @snap/valdi\n\n# One-command setup (installs all dependencies)\nvaldi dev_setup\n\n# Create your first project\nmkdir my_project && cd my_project\nvaldi bootstrap\nvaldi install ios  # or android\n```\n\n> [!TIP]\n> **Editor Extensions:** For the best development experience, install the [Valdi VSCode/Cursor extensions](./docs/INSTALL.md#vscodecursor-setup-optional-but-recommended) for syntax highlighting, debugging, and device logs during hot reload.\n\n## Quick Links\n\n- [Getting Started Guide](./docs/INSTALL.md)\n- [Documentation](./docs/README.md)\n- [Codelabs](./docs/docs/start-code-lab.md)\n- [API Reference](./docs/api/api-quick-reference.md)\n- [Frequently Asked Questions](./docs/docs/faq.md)\n- [Component Library](https://github.com/Snapchat/Valdi_Widgets)\n\n## Why Choose Valdi?\n\nValdi is a cross-platform UI framework designed to solve the fundamental problem of cross-platform development: velocity vs. runtime performance. For 8 years, it has powered a large portion of Snap's production apps.\n\n### True Native Performance\n\nUnlike frameworks that rely on web views or JavaScript bridges, Valdi compiles declaratively rendered TypeScript components into platform-native views. Valdi also includes several other performance advantages:\n\n- **[Automatic view recycling](./docs/docs/performance-view-recycling.md)** - Global view pooling system reuses native views across all screens, dramatically reducing inflation latency\n- **Optimized component rendering** - Components re-render independently without triggering parent re-renders, enabling fast incremental updates\n- **Optimized layout engine** - C++ layout engine runs on the main thread with minimal marshalling overhead\n- **Viewport-aware rendering** - Only visible views are inflated, making infinite scrolling performant by default\n\nLearn more in our [Performance Optimization Guide](./docs/docs/performance-optimization.md).\n\n### Developer Experience Built for Speed\n\nValdi eliminates the traditional compile-test-debug cycle that slows native development:\n\n- **Instant hot reload** - See changes in milliseconds on iOS, Android, or desktop without recompiling\n- **[Full VSCode debugging](./docs/docs/workflow-hermes-debugger.md)** - Set breakpoints, inspect variables, profile performance, and capture heap dumps directly in VSCode\n- **Familiar syntax** - TSX components with TypeScript for type safety\n\n### Flexible Adoption Model\n\nValdi integrates easily into existing apps - start small and scale as needed:\n\n- **[Embed Valdi in native](./docs/docs/native-bindings.md)** - Drop Valdi components into existing UIKit or Android view hierarchies\n- **[Embed native in Valdi](./docs/docs/native-customviews.md)** - Use platform-specific views within Valdi layouts via `<custom-view>`\n- **[Polyglot modules](./docs/docs/native-polyglot.md)** - Write performance-critical code in C++, Swift, Kotlin, or Objective-C with type-safe bindings to TypeScript\n- **[Full-stack architecture](./docs/docs/advanced-full-stack.md)** - Build entire features in Valdi with worker threads for background processing, eliminating platform-specific bridge code\n\n### Deep Native Integration\n\nValdi ",
      "default_branch": "main"
    },
    "fetched_at": "2025-12-06T02:26:04.580585"
  },
  {
    "basic_info": {
      "name": "llm-council",
      "full_name": "karpathy/llm-council",
      "owner": "karpathy",
      "description": "LLM Council works together to answer your hardest questions",
      "url": "https://github.com/karpathy/llm-council",
      "clone_url": "https://github.com/karpathy/llm-council.git",
      "ssh_url": "git@github.com:karpathy/llm-council.git",
      "homepage": "",
      "created_at": "2025-11-22T23:24:14Z",
      "updated_at": "2025-12-06T02:25:52Z",
      "pushed_at": "2025-11-22T23:35:21Z"
    },
    "stats": {
      "stars": 9764,
      "forks": 1618,
      "watchers": 9764,
      "open_issues": 61,
      "size": 262
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 24729,
        "JavaScript": 20694,
        "CSS": 9346,
        "Shell": 625,
        "HTML": 357
      },
      "license": null,
      "topics": []
    },
    "content": {
      "readme": "# LLM Council\n\n![llmcouncil](header.jpg)\n\nThe idea of this repo is that instead of asking a question to your favorite LLM provider (e.g. OpenAI GPT 5.1, Google Gemini 3.0 Pro, Anthropic Claude Sonnet 4.5, xAI Grok 4, eg.c), you can group them into your \"LLM Council\". This repo is a simple, local web app that essentially looks like ChatGPT except it uses OpenRouter to send your query to multiple LLMs, it then asks them to review and rank each other's work, and finally a Chairman LLM produces the final response.\n\nIn a bit more detail, here is what happens when you submit a query:\n\n1. **Stage 1: First opinions**. The user query is given to all LLMs individually, and the responses are collected. The individual responses are shown in a \"tab view\", so that the user can inspect them all one by one.\n2. **Stage 2: Review**. Each individual LLM is given the responses of the other LLMs. Under the hood, the LLM identities are anonymized so that the LLM can't play favorites when judging their outputs. The LLM is asked to rank them in accuracy and insight.\n3. **Stage 3: Final response**. The designated Chairman of the LLM Council takes all of the model's responses and compiles them into a single final answer that is presented to the user.\n\n## Vibe Code Alert\n\nThis project was 99% vibe coded as a fun Saturday hack because I wanted to explore and evaluate a number of LLMs side by side in the process of [reading books together with LLMs](https://x.com/karpathy/status/1990577951671509438). It's nice and useful to see multiple responses side by side, and also the cross-opinions of all LLMs on each other's outputs. I'm not going to support it in any way, it's provided here as is for other people's inspiration and I don't intend to improve it. Code is ephemeral now and libraries are over, ask your LLM to change it in whatever way you like.\n\n## Setup\n\n### 1. Install Dependencies\n\nThe project uses [uv](https://docs.astral.sh/uv/) for project management.\n\n**Backend:**\n```bash\nuv sync\n```\n\n**Frontend:**\n```bash\ncd frontend\nnpm install\ncd ..\n```\n\n### 2. Configure API Key\n\nCreate a `.env` file in the project root:\n\n```bash\nOPENROUTER_API_KEY=sk-or-v1-...\n```\n\nGet your API key at [openrouter.ai](https://openrouter.ai/). Make sure to purchase the credits you need, or sign up for automatic top up.\n\n### 3. Configure Models (Optional)\n\nEdit `backend/config.py` to customize the council:\n\n```python\nCOUNCIL_MODELS = [\n    \"openai/gpt-5.1\",\n    \"google/gemini-3-pro-preview\",\n    \"anthropic/claude-sonnet-4.5\",\n    \"x-ai/grok-4\",\n]\n\nCHAIRMAN_MODEL = \"google/gemini-3-pro-preview\"\n```\n\n## Running the Application\n\n**Option 1: Use the start script**\n```bash\n./start.sh\n```\n\n**Option 2: Run manually**\n\nTerminal 1 (Backend):\n```bash\nuv run python -m backend.main\n```\n\nTerminal 2 (Frontend):\n```bash\ncd frontend\nnpm run dev\n```\n\nThen open http://localhost:5173 in your browser.\n\n## Tech Stack\n\n- **Backend:** FastAPI (Python 3.10+), async httpx, OpenRouter API\n- **Frontend:** React + Vite, react-markdown for rendering\n- **Storage:** JSON files in `data/conversations/`\n- **Package Management:** uv for Python, npm for JavaScript\n",
      "default_branch": "master"
    },
    "fetched_at": "2025-12-06T02:26:05.688077"
  },
  {
    "basic_info": {
      "name": "awesome-nanobanana-pro",
      "full_name": "ZeroLu/awesome-nanobanana-pro",
      "owner": "ZeroLu",
      "description": "ğŸš€ An awesome list of curated Nano Banana pro prompts and examples. Your go-to resource for mastering prompt engineering and exploring the creative potential of the Nano banana pro(Nano banana 2) AI image model.",
      "url": "https://github.com/ZeroLu/awesome-nanobanana-pro",
      "clone_url": "https://github.com/ZeroLu/awesome-nanobanana-pro.git",
      "ssh_url": "git@github.com:ZeroLu/awesome-nanobanana-pro.git",
      "homepage": "https://nanobananaproprompts.com",
      "created_at": "2025-11-10T13:51:03Z",
      "updated_at": "2025-12-06T02:19:17Z",
      "pushed_at": "2025-12-05T03:12:32Z"
    },
    "stats": {
      "stars": 5152,
      "forks": 393,
      "watchers": 5152,
      "open_issues": 4,
      "size": 89
    },
    "tech_info": {
      "language": null,
      "languages": {},
      "license": "MIT License",
      "topics": [
        "gemini",
        "nanobanana",
        "nanobanana-pro",
        "nanobanana2",
        "nanobananapro",
        "prompt-engineering",
        "prompt-guide",
        "prompts"
      ]
    },
    "content": {
      "readme": "[Last updated on 2025.12.04: Added bathroom mirror selfie, fisheye character selfie, 3D renders, magazine covers, torn paper art, and other prompts]\n\n# Awesome Nano Banana Pro ğŸŒ\n\n[![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/sindresorhus/awesome) [![License: CC BY 4.0](https://img.shields.io/badge/License-CC_BY_4.0-lightgrey.svg)](https://creativecommons.org/licenses/by/4.0/) [![GitHub stars](https://img.shields.io/github/stars/ZeroLu/awesome-nanobanana-pro?style=social)](https://github.com/ZeroLu/awesome-nanobanana-pro/stargazers)\n\n> A curated collection of the **best Nano Banana prompts**, image generation styles, and resources for advanced AI visual experiments.\n\nThis repository focuses on **high-fidelity image prompts** sourced from X (Twitter), WeChat, Replicate, and top prompt engineers. Whether you are looking for **photorealistic portraits**, **stylized aesthetics**, or complex creative experiments, you will find the most effective inputs here to unlock the full potential of the model. \n\n### Try the prompts with actual Nano Banana Pro here ğŸ‘‰ https://nanobananaprompts.com\n\nConsider subscribing to [this free newsletter](https://zerolu.substack.com/p/hello-there) or follow me on [X](https://x.com/zerolu_eth) if you want more high quality content like this.\n\n### Sponsor One: [thesorawatermarkremover.com](https://thesorawatermarkremover.com)\n\n**ğŸ„ Cyber Week Discount: Use `CYBER10` bfore `Dec 8th,2025` to get 10% off at checkout! ğŸ„**\n\n[<img width=\"600\" height=\"265\" alt=\"image\" src=\"https://github.com/user-attachments/assets/b087445c-d3ad-4152-8e28-33a5ca49d4b5\" />](https://thesorawatermarkremover.com)\n\n\n### Sponsor Two: [Polymeric Cloud Limited](https://www.polymericcloud.com/)\n<table>\n<tr>\n<td width=\"60%\" valign=\"top\">\n<img src=\"https://img.shields.io/badge/Enterprise%20Access-Available-blue\" alt=\"Badge\">\n<h3 style=\"margin-top: 10px;\">ğŸ¢ Enterprise Token Access</h3>\n<p>For production workloads requiring stability, security, and high concurrency, we offer enterprise-grade token access for <b>Nano Banana Pro</b> & <b>Gemini 3 Pro</b>.</p>\n<b>Common use cases:</b>\n<ul>\n<li>SaaS platforms embedding AI models</li>\n<li>High-volume batch generation (50kâ€“1M+)</li>\n<li>Stable domestic availability & concurrency</li>\n<li>Compliance, invoicing, procurement workflows</li>\n<li>Enterprise-level key isolation & security</li>\n</ul>\n<hr>\n<p>ğŸ“Œ <b>Authorized Google Cloud Partner</b><br>\n<a href=\"https://cloud.google.com/find-a-partner/partblockchain\">Click here for verification check</a></p>\n</td>\n<td width=\"40%\" valign=\"top\">\n<div align=\"center\">\n<br>\n<h3>ğŸ“® Contact for Access</h3>\n  <img width=\"100\" height=\"100\" alt=\"image\" src=\"https://github.com/user-attachments/assets/02b037b3-d9c6-4bf4-a6a4-dd583b932849\" /><br>\n<p>ğŸ“± <b>WeChat Support</b></p>\n<p>\nâœ‰ï¸ <b>Email</b><br>\n<code>support@polymericcloud.com</code>\n<br><br>\nâ˜ï¸ <b>Hotline</b><br>\n<code>4006107779-5500</code>\n</p>\n</div>\n</td>\n</tr>\n</table>\n\nThis repo gets up to **20,000 visitors per day**, [mail me](mailto:iamzerolu@gmail.com) if you want a limited sponsor slot!\n\n## ğŸ“– Table of Contents\n\n1. [Photorealism & Aesthetics](#1-photorealism--aesthetics)\n2. [Creative Experiments](#2-creative-experiments)\n3. [Education & Knowledge](#3-education--knowledge)\n4. [E-commerce & Virtual Studio](#4-e-commerce--virtual-studio)\n5. [Workplace & Productivity](#5-workplace--productivity)\n6. [Photo Editing & Restoration](#6-photo-editing--restoration)\n7. [Interior Design](#7-interior-design)\n8. [Social Media & Marketing](#8-social-media--marketing)\n9. [Daily Life & Translation](#9-daily-life--translation)\n10. [Social Networking & Avatars](#10-social-networking--avatars)\n11. [Resources](#11-resources)\n12. [Contributing](#12-contributing)\n\n---\n\n## 1. Photorealism & Aesthetics\n\nOptimize your visual output with these **high-fidelity prompts**. These are designed to utilize the model's ability to render complex lighting, textures, and specific eras.\n\n### 1.1. Hyper-Realistic Crowd Composition\n*Handling complex compositions with multiple famous faces and specific lighting.*\n<img width=\"400\" alt=\"Celebrity Crowd\" src=\"https://github.com/user-attachments/assets/3a056a8d-904e-4b3e-b0d2-b5122758b7f5\" />\n\n**Prompt:**\n```text\nCreate a hyper-realistic, ultra-sharp, full-color large-format image featuring a massive group of celebrities from different eras, all standing together in a single wide cinematic frame. The image must look like a perfectly photographed editorial cover with impeccable lighting, lifelike skin texture, micro-details of hair, pores, reflections, and fabric fibers.\n\nGENERAL STYLE & MOOD: Photorealistic, 8k, shallow depth of field, soft natural fill light + strong golden rim light. High dynamic range, calibrated color grading. Skin tones perfectly accurate. Crisp fabric detail with individual threads visible. Balanced composition, slightly wide-angle lens (35mm), center-weighted. All celebritie",
      "default_branch": "main"
    },
    "fetched_at": "2025-12-06T02:26:06.823441"
  },
  {
    "basic_info": {
      "name": "Z-Image",
      "full_name": "Tongyi-MAI/Z-Image",
      "owner": "Tongyi-MAI",
      "description": null,
      "url": "https://github.com/Tongyi-MAI/Z-Image",
      "clone_url": "https://github.com/Tongyi-MAI/Z-Image.git",
      "ssh_url": "git@github.com:Tongyi-MAI/Z-Image.git",
      "homepage": null,
      "created_at": "2025-11-26T09:18:10Z",
      "updated_at": "2025-12-06T02:16:28Z",
      "pushed_at": "2025-12-04T11:27:04Z"
    },
    "stats": {
      "stars": 4737,
      "forks": 250,
      "watchers": 4737,
      "open_issues": 38,
      "size": 51664
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 83168
      },
      "license": "Apache License 2.0",
      "topics": []
    },
    "content": {
      "readme": "<h1 align=\"center\">âš¡ï¸- Image<br><sub><sup>An Efficient Image Generation Foundation Model with Single-Stream Diffusion Transformer</sup></sub></h1>\n\n<div align=\"center\">\n\n[![Official Site](https://img.shields.io/badge/Official%20Site-333399.svg?logo=homepage)](https://tongyi-mai.github.io/Z-Image-blog/)&#160;\n[![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97%20Checkpoint-Z--Image--Turbo-yellow)](https://huggingface.co/Tongyi-MAI/Z-Image-Turbo)&#160;\n[![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97%20Online_Demo-Z--Image--Turbo-blue)](https://huggingface.co/spaces/Tongyi-MAI/Z-Image-Turbo)&#160;\n[![ModelScope Model](https://img.shields.io/badge/ğŸ¤–%20Checkpoint-Z--Image--Turbo-624aff)](https://www.modelscope.cn/models/Tongyi-MAI/Z-Image-Turbo)&#160;\n[![ModelScope Space](https://img.shields.io/badge/ğŸ¤–%20Online_Demo-Z--Image--Turbo-17c7a7)](https://www.modelscope.cn/aigc/imageGeneration?tab=advanced&versionId=469191&modelType=Checkpoint&sdVersion=Z_IMAGE_TURBO&modelUrl=modelscope%253A%252F%252FTongyi-MAI%252FZ-Image-Turbo%253Frevision%253Dmaster%7D%7BOnline)&#160;\n[![Art Gallery PDF](https://img.shields.io/badge/%F0%9F%96%BC%20Art_Gallery-PDF-ff69b4)](assets/Z-Image-Gallery.pdf)&#160;\n[![Web Art Gallery](https://img.shields.io/badge/%F0%9F%8C%90%20Web_Art_Gallery-online-00bfff)](https://modelscope.cn/studios/Tongyi-MAI/Z-Image-Gallery/summary)&#160;\n<a href=\"https://arxiv.org/abs/2511.22699\" target=\"_blank\"><img src=\"https://img.shields.io/badge/Report-b5212f.svg?logo=arxiv\" height=\"21px\"></a>\n\n\nWelcome to the official repository for the Z-Imageï¼ˆé€ ç›¸ï¼‰project!\n\n</div>\n\n\n\n## âœ¨ Z-Image\n\nZ-Image is a powerful and highly efficient image generation model with **6B** parameters. Currently there are three variants:\n\n- ğŸš€ **Z-Image-Turbo** â€“ A distilled version of Z-Image that matches or exceeds leading competitors with only **8 NFEs** (Number of Function Evaluations). It offers **âš¡ï¸sub-second inference latencyâš¡ï¸** on enterprise-grade H800 GPUs and fits comfortably within **16G VRAM consumer devices**. It excels in photorealistic image generation, bilingual text rendering (English & Chinese), and robust instruction adherence.\n\n- ğŸ§± **Z-Image-Base** â€“ The non-distilled foundation model. By releasing this checkpoint, we aim to unlock the full potential for community-driven fine-tuning and custom development.\n\n- âœï¸ **Z-Image-Edit** â€“ A variant fine-tuned on Z-Image specifically for image editing tasks. It supports creative image-to-image generation with impressive instruction-following capabilities, allowing for precise edits based on natural language prompts.\n\n### ğŸ“¥ Model Zoo\n\n| Model | Hugging Face                                                                                                                                                                                                                                                                                                              | ModelScope                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |\n| :--- |:--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| **Z-Image-Turbo** | [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97%20Checkpoint%20-Z--Image--Turbo-yellow)](https://huggingface.co/Tongyi-MAI/Z-Image-Turbo) <br> [![Hugging Face Space](https://img.shields.io/badge/%F0%9F%A4%97%20Online%20Demo-Z--Image--Turbo-blue)](https://huggingface.co/spaces/Tongyi-MAI/Z-Image-Turbo) | [![ModelScope Model](https://img.shields.io/badge/ğŸ¤–%20%20Checkpoint-Z--Image--Turbo-624aff)](https://www.modelscope.cn/models/Tongyi-MAI/Z-Image-Turbo) <br> [![ModelScope Space](https://img.shields.io/badge/%F0%9F%A4%96%20Online%20Demo-Z--Image--Turbo-17c7a7)](https://www.modelscope.cn/aigc/imageGeneration?tab=advanced&versionId=469191&modelType=Checkpoint&sdVersion=Z_IMAGE_TURBO&modelUrl=modelscope%3A%2F%2FTongyi-MAI%2FZ-Image-Turbo%3Frevision%3Dmaster) |\n| **",
      "default_branch": "main"
    },
    "fetched_at": "2025-12-06T02:26:07.949766"
  },
  {
    "basic_info": {
      "name": "gitlogue",
      "full_name": "unhappychoice/gitlogue",
      "owner": "unhappychoice",
      "description": "A cinematic Git commit replay tool for the terminal, turning your Git history into a living, animated story.",
      "url": "https://github.com/unhappychoice/gitlogue",
      "clone_url": "https://github.com/unhappychoice/gitlogue.git",
      "ssh_url": "git@github.com:unhappychoice/gitlogue.git",
      "homepage": "",
      "created_at": "2025-11-08T21:22:33Z",
      "updated_at": "2025-12-06T01:36:14Z",
      "pushed_at": "2025-12-04T20:34:27Z"
    },
    "stats": {
      "stars": 3414,
      "forks": 72,
      "watchers": 3414,
      "open_issues": 9,
      "size": 81092
    },
    "tech_info": {
      "language": "Rust",
      "languages": {
        "Rust": 196436,
        "Tree-sitter Query": 21004,
        "JavaScript": 19194,
        "Shell": 8335,
        "Nix": 2428,
        "Ruby": 1231,
        "Handlebars": 464
      },
      "license": "ISC License",
      "topics": [
        "cli",
        "cli-tool",
        "code-animation",
        "commit-history",
        "developer-tools",
        "git",
        "git-history",
        "git-visualization",
        "productivity",
        "ratatui",
        "rust",
        "screensaver",
        "syntax-highlighting",
        "terminal",
        "terminal-app",
        "terminal-based",
        "terminal-screensaver",
        "tree-sitter",
        "tui",
        "visualization"
      ]
    },
    "content": {
      "readme": "# gitlogue\n\n<a title=\"This tool is Tool of The Week on Terminal Trove, The $HOME of all things in the terminal\" href=\"https://terminaltrove.com/gitlogue/\"><img src=\"https://cdn.terminaltrove.com/media/badges/tool_of_the_week/svg/terminal_trove_tool_of_the_week_green_on_black_bg.svg\" alt=\"Terminal Trove Tool of The Week\" height=\"48\" /></a>\n\n<p align=\"center\">\n  <img src=\"docs/assets/demo.gif\" alt=\"gitlogue demo\" style=\"max-width: 100%; width: 800px;\" />\n</p>\n\nA cinematic Git commit replay tool for the terminal, turning your Git history into a living, animated story.\n\nWatch commits unfold with realistic typing animations, syntax highlighting, and file tree transitions, transforming code changes into a visual experience.\n\n## Installation\n\n### Using Install Script (Recommended)\n\n```bash\ncurl -fsSL https://raw.githubusercontent.com/unhappychoice/gitlogue/main/install.sh | bash\n```\n\n### Using Homebrew\n\n```bash\nbrew install gitlogue\n```\n\n### Using Cargo\n\n```bash\ncargo install gitlogue\n```\n\n### On Arch Linux\n\n```bash\npacman -S gitlogue\n```\n\n### Using Nix\n\n```bash\n# Run directly without installation\nnix run github:unhappychoice/gitlogue\n\n# Or install to your profile\nnix profile install github:unhappychoice/gitlogue\n\n# For flake-based configurations, add to your inputs:\n# inputs.gitlogue.url = \"github:unhappychoice/gitlogue\";\n# Then use: inputs.gitlogue.packages.${system}.default\n```\n\n### From Source\n\n```bash\ngit clone https://github.com/unhappychoice/gitlogue.git\ncd gitlogue\ncargo install --path .\n```\n\nSee the [Installation Guide](docs/installation.md) for more options and troubleshooting.\n\n## Features\n\nğŸ¬ **Commit Replay as Animation** â€” Realistic typing, cursor movement, deletions, and file operations  \nğŸ¨ **Tree-sitter Syntax Highlighting** â€” 29 languages supported  \nğŸŒ³ **Project File Tree** â€” Directory structure with change statistics  \nğŸ–¥ï¸ **Screensaver Mode** â€” Endless random commit playback  \nğŸ­ **Themes** â€” 9 built-in themes + full customization support  \nâš¡ **Fast & Lightweight** â€” Built with Rust for performance  \n\n## Usage\n\n### Popular Use Cases\n\nğŸ–¥ï¸  **Screensaver** â€” Ambient coding display for your workspace  \nğŸ“ **Education** â€” Visualize how code evolved over time  \nğŸ“º **Presentations** â€” Replay real commit histories live  \nğŸ¬ **Content Creation** â€” Record demos with VHS or asciinema  \nğŸ¨ **Desktop Ricing** â€” A living decoration for your terminal  \nğŸ’¼ **Look Busy Mode** â€” Appear productive during meetings\n\n> [!WARNING]\n> **Not a True Screensaver** â€” gitlogue does not include traditional screensaver functions like power management or screen blanking. It's purely a visual display tool.\n>\n> **OLED Burn-in Risk** â€” Static elements (like the editor background and border lines) may cause burn-in on OLED displays over extended periods. LCD displays are generally safe from this issue.\n\n### Quick Start\n\n```bash\n# Start the cinematic screensaver\ngitlogue\n\n# View a specific commit\ngitlogue --commit abc123\n\n# Replay a range of commits\ngitlogue --commit HEAD~5..HEAD\n\n# Replay commits in chronological order (oldest first)\ngitlogue --order asc\n\n# Loop a specific commit continuously\ngitlogue --commit abc123 --loop\n\n# Loop through a commit range\ngitlogue --commit HEAD~10..HEAD --loop\n\n# Filter commits by author or email (case-insensitive partial match)\ngitlogue --author \"john\"\n\n# Filter commits by date\ngitlogue --after \"2024-01-01\"\ngitlogue --before \"1 week ago\"\ngitlogue --after \"2024-06-01\" --before \"2024-07-01\"\n\n# Use a different theme\ngitlogue --theme dracula\n\n# Adjust typing speed (ms per character)\ngitlogue --speed 20\n\n# Set different speeds for different file types\ngitlogue --speed-rule \"*.java:50\" --speed-rule \"*.xml:5\"\n\n# Ignore specific file patterns (e.g., notebooks, lock files)\ngitlogue --ignore \"*.ipynb\" --ignore \"poetry.lock\"\n\n# Use an ignore file\ngitlogue --ignore-file .gitlogue-ignore\n\n# List available themes\ngitlogue theme list\n\n# Set default theme\ngitlogue theme set dracula\n\n# Combine options\ngitlogue --commit HEAD~5 --author \"john\" --theme nord --speed 15 --ignore \"*.ipynb\"\n```\n\n## Configuration\n\ngitlogue can be configured via `~/.config/gitlogue/config.toml`.  \nYou can set the default theme, typing speed, and background preferences.\n\nSee the [Configuration Guide](docs/configuration.md) for full options and examples.\n\n## Supported Languages\n\nBash, C, C#, C++, Clojure, CSS, Dart, Elixir, Erlang, Go, Haskell, HTML, Java, JavaScript, JSON, Kotlin, Lua, Markdown, PHP, Python, Ruby, Rust, Scala, Svelte, Swift, TypeScript, XML, YAML, Zig\n\n## Documentation\n\n[Installation Guide](docs/installation.md)  \n[Usage Guide](docs/usage.md)  \n[Configuration Guide](docs/configuration.md)  \n[Theme Customization](docs/themes.md)  \n[Contributing Guidelines](docs/CONTRIBUTING.md)  \n[Architecture Overview](docs/ARCHITECTURE.md)\n\n## Related Projects\n\n### Git Visualization & Coding\n\n- [**GitType**](https://github.com/unhappychoice/gittype) - A CLI code-typing game that turns your source code into typing challenges\n\n### Terminal Screensave",
      "default_branch": "main"
    },
    "fetched_at": "2025-12-06T02:26:09.092048"
  },
  {
    "basic_info": {
      "name": "Depth-Anything-3",
      "full_name": "ByteDance-Seed/Depth-Anything-3",
      "owner": "ByteDance-Seed",
      "description": "Depth Anything 3",
      "url": "https://github.com/ByteDance-Seed/Depth-Anything-3",
      "clone_url": "https://github.com/ByteDance-Seed/Depth-Anything-3.git",
      "ssh_url": "git@github.com:ByteDance-Seed/Depth-Anything-3.git",
      "homepage": "https://depth-anything-3.github.io/",
      "created_at": "2025-11-12T08:44:03Z",
      "updated_at": "2025-12-06T00:45:49Z",
      "pushed_at": "2025-12-04T08:10:20Z"
    },
    "stats": {
      "stars": 3238,
      "forks": 256,
      "watchers": 3238,
      "open_issues": 86,
      "size": 22616
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 657399,
        "Jupyter Notebook": 650520
      },
      "license": "Apache License 2.0",
      "topics": []
    },
    "content": {
      "readme": "<div align=\"center\">\n<h1 style=\"border-bottom: none; margin-bottom: 0px \">Depth Anything 3: Recovering the Visual Space from Any Views</h1>\n<!-- <h2 style=\"border-top: none; margin-top: 3px;\">Recovering the Visual Space from Any Views</h2> -->\n\n\n[**Haotong Lin**](https://haotongl.github.io/)<sup>&ast;</sup> Â· [**Sili Chen**](https://github.com/SiliChen321)<sup>&ast;</sup> Â· [**Jun Hao Liew**](https://liewjunhao.github.io/)<sup>&ast;</sup> Â· [**Donny Y. Chen**](https://donydchen.github.io)<sup>&ast;</sup> Â· [**Zhenyu Li**](https://zhyever.github.io/) Â· [**Guang Shi**](https://scholar.google.com/citations?user=MjXxWbUAAAAJ&hl=en) Â· [**Jiashi Feng**](https://scholar.google.com.sg/citations?user=Q8iay0gAAAAJ&hl=en)\n<br>\n[**Bingyi Kang**](https://bingykang.github.io/)<sup>&ast;&dagger;</sup>\n\n&dagger;project lead&emsp;&ast;Equal Contribution\n\n<a href=\"https://arxiv.org/abs/2511.10647\"><img src='https://img.shields.io/badge/arXiv-Depth Anything 3-red' alt='Paper PDF'></a>\n<a href='https://depth-anything-3.github.io'><img src='https://img.shields.io/badge/Project_Page-Depth Anything 3-green' alt='Project Page'></a>\n<a href='https://huggingface.co/spaces/depth-anything/Depth-Anything-3'><img src='https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Demo-blue'></a>\n<!-- <a href='https://huggingface.co/datasets/depth-anything/VGB'><img src='https://img.shields.io/badge/Benchmark-VisGeo-yellow' alt='Benchmark'></a> -->\n<!-- <a href='https://huggingface.co/datasets/depth-anything/data'><img src='https://img.shields.io/badge/Benchmark-xxx-yellow' alt='Data'></a> -->\n\n</div>\n\nThis work presents **Depth Anything 3 (DA3)**, a model that predicts spatially consistent geometry from\narbitrary visual inputs, with or without known camera poses.\nIn pursuit of minimal modeling, DA3 yields two key insights:\n- ğŸ’ A **single plain transformer** (e.g., vanilla DINO encoder) is sufficient as a backbone without architectural specialization,\n- âœ¨ A singular **depth-ray representation** obviates the need for complex multi-task learning.\n\nğŸ† DA3 significantly outperforms\n[DA2](https://github.com/DepthAnything/Depth-Anything-V2) for monocular depth estimation,\nand [VGGT](https://github.com/facebookresearch/vggt) for multi-view depth estimation and pose estimation.\nAll models are trained exclusively on **public academic datasets**.\n\n<!-- <p align=\"center\">\n  <img src=\"assets/images/da3_teaser.png\" alt=\"Depth Anything 3\" width=\"100%\">\n</p> -->\n<p align=\"center\">\n  <img src=\"assets/images/demo320-2.gif\" alt=\"Depth Anything 3 - Left\" width=\"70%\">\n</p>\n<p align=\"center\">\n  <img src=\"assets/images/da3_radar.png\" alt=\"Depth Anything 3\" width=\"100%\">\n</p>\n\n\n## ğŸ“° News\n- **30-11-2025:** Add [`use_ray_pose`](#use-ray-pose) and [`ref_view_strategy`](docs/funcs/ref_view_strategy.md) (reference view selection for multi-view inputs).   \n- **25-11-2025:** Add [Awesome DA3 Projects](#-awesome-da3-projects), a community-driven section featuring DA3-based applications.\n- **14-11-2025:** Paper, project page, code and models are all released.\n\n## âœ¨ Highlights\n\n### ğŸ† Model Zoo\nWe release three series of models, each tailored for specific use cases in visual geometry.\n\n- ğŸŒŸ **DA3 Main Series** (`DA3-Giant`, `DA3-Large`, `DA3-Base`, `DA3-Small`) These are our flagship foundation models, trained with a unified depth-ray representation. By varying the input configuration, a single model can perform a wide range of tasks:\n  + ğŸŒŠ **Monocular Depth Estimation**: Predicts a depth map from a single RGB image.\n  + ğŸŒŠ **Multi-View Depth Estimation**: Generates consistent depth maps from multiple images for high-quality fusion.\n  + ğŸ¯ **Pose-Conditioned Depth Estimation**: Achieves superior depth consistency when camera poses are provided as input.\n  + ğŸ“· **Camera Pose Estimation**:  Estimates camera extrinsics and intrinsics from one or more images.\n  + ğŸŸ¡ **3D Gaussian Estimation**: Directly predicts 3D Gaussians, enabling high-fidelity novel view synthesis.\n\n- ğŸ“ **DA3 Metric Series** (`DA3Metric-Large`) A specialized model fine-tuned for metric depth estimation in monocular settings, ideal for applications requiring real-world scale.\n\n- ğŸ” **DA3 Monocular Series** (`DA3Mono-Large`). A dedicated model for high-quality relative monocular depth estimation. Unlike disparity-based models (e.g.,  [Depth Anything 2](https://github.com/DepthAnything/Depth-Anything-V2)), it directly predicts depth, resulting in superior geometric accuracy.\n\nğŸ”— Leveraging these available models, we developed a **nested series** (`DA3Nested-Giant-Large`). This series combines a any-view giant model with a metric model to reconstruct visual geometry at a real-world metric scale.\n\n### ğŸ› ï¸ Codebase Features\nOur repository is designed to be a powerful and user-friendly toolkit for both practical application and future research.\n- ğŸ¨ **Interactive Web UI & Gallery**: Visualize model outputs and compare results with an easy-to-use Gradio-based web interface.\n- âš¡ **Flexible Command-Line Interface (CLI)**: Power",
      "default_branch": "main"
    },
    "fetched_at": "2025-12-06T02:26:10.224251"
  },
  {
    "basic_info": {
      "name": "sbox-public",
      "full_name": "Facepunch/sbox-public",
      "owner": "Facepunch",
      "description": "s&box is a modern game engine, built on Valve's Source 2 and the latest .NET technology, it provides a modern intuitive editor for creating games",
      "url": "https://github.com/Facepunch/sbox-public",
      "clone_url": "https://github.com/Facepunch/sbox-public.git",
      "ssh_url": "git@github.com:Facepunch/sbox-public.git",
      "homepage": "https://sbox.game",
      "created_at": "2025-11-24T08:18:33Z",
      "updated_at": "2025-12-05T23:13:41Z",
      "pushed_at": "2025-12-05T17:43:53Z"
    },
    "stats": {
      "stars": 3232,
      "forks": 197,
      "watchers": 3232,
      "open_issues": 33,
      "size": 10450
    },
    "tech_info": {
      "language": "C#",
      "languages": {
        "C#": 16169802,
        "HLSL": 736321,
        "HTML": 262739,
        "SCSS": 163476,
        "ReScript": 46444,
        "CSS": 20573,
        "Python": 3675,
        "Batchfile": 261
      },
      "license": "Other",
      "topics": [
        "gamedev",
        "sbox",
        "source2"
      ]
    },
    "content": {
      "readme": "<div align=\"center\">\n  <img src=\"https://sbox.game/img/sbox-logo-square.svg\" width=\"80px\" alt=\"s&box logo\">\n\n  [Website] | [Getting Started] | [Forums] | [Documentation] | [Contributing]\n</div>\n\n[Website]: https://sbox.game/\n[Getting Started]: https://sbox.game/dev/doc/about/getting-started/first-steps/\n[Forums]: https://sbox.game/f/\n[Documentation]: https://sbox.game/dev/doc/\n[Contributing]: CONTRIBUTING.md\n\n# s&box\n\ns&box is a modern game engine, built on Valve's Source 2 and the latest .NET technology, it provides a modern intuitive editor for creating games.\n\n![s&box editor](https://files.facepunch.com/matt/1b2211b1/sbox-dev_FoZ5NNZQTi.jpg)\n\nIf your goal is to create games using s&box, please start with the [getting started guide](https://sbox.game/dev/doc/about/getting-started/first-steps/).\nThis repository is for building the engine from source for those who want to contribute to the development of the engine.\n\n## Getting the Engine\n\n### Steam\n\nYou can download and install the s&box editor directly from [Steam](https://sbox.game/give-me-that).\n\n### Compiling from Source\n\nIf you want to build from source, this repository includes all the necessary files to compile the engine yourself.\n\n#### Prerequisites\n\n* [Git](https://git-scm.com/install/windows)\n* [Visual Studio 2026](https://visualstudio.microsoft.com/)\n* [.NET 10 SDK](https://dotnet.microsoft.com/en-us/download)\n\n#### Building\n\n```bash\n# Clone the repo\ngit clone https://github.com/Facepunch/sbox-public.git\n```\n\nOnce you've cloned the repo simply run `Bootstrap.bat` which will download dependencies and build the engine.\n\nThe game and editor can be run from the binaries in the game folder.\n\n## Contributing\n\nIf you would like to contribute to the engine, please see the [contributing guide](CONTRIBUTING.md).\n\nIf you want to report bugs or request new features, see [sbox-issues](https://github.com/Facepunch/sbox-issues/).\n\n## Documentation\n\nFull documentation, tutorials, and API references are available at [sbox.game/dev/](https://sbox.game/dev/).\n\n## License\n\nThe s&box engine source code is licensed under the [MIT License](LICENSE.md).\n\nCertain native binaries in `game/bin` are not covered by the MIT license. These binaries are distributed under the s&box EULA. You must agree to the terms of the EULA to use them.\n\nThis project includes third-party components that are separately licensed.\nThose components are not covered by the MIT license above and remain subject\nto their original licenses as indicated in `game/thirdpartylegalnotices`.",
      "default_branch": "master"
    },
    "fetched_at": "2025-12-06T02:26:11.366126"
  },
  {
    "basic_info": {
      "name": "awesome-nano-banana-pro-prompts",
      "full_name": "YouMind-OpenLab/awesome-nano-banana-pro-prompts",
      "owner": "YouMind-OpenLab",
      "description": "ğŸŒ 500+ selected Nano Banana Pro prompts with images, multilingual support, and instant gallery preview. Open-source prompt engineering library",
      "url": "https://github.com/YouMind-OpenLab/awesome-nano-banana-pro-prompts",
      "clone_url": "https://github.com/YouMind-OpenLab/awesome-nano-banana-pro-prompts.git",
      "ssh_url": "git@github.com:YouMind-OpenLab/awesome-nano-banana-pro-prompts.git",
      "homepage": "https://youmind.com/nano-banana-pro-prompts",
      "created_at": "2025-11-23T07:17:48Z",
      "updated_at": "2025-12-06T02:20:43Z",
      "pushed_at": "2025-12-06T00:52:02Z"
    },
    "stats": {
      "stars": 2979,
      "forks": 269,
      "watchers": 2979,
      "open_issues": 0,
      "size": 20954
    },
    "tech_info": {
      "language": "TypeScript",
      "languages": {
        "TypeScript": 104948
      },
      "license": "Other",
      "topics": [
        "awesome",
        "gemini-3-pro-image-preview",
        "gemini-ai",
        "nano-banana",
        "nano-banana-2",
        "nano-banana-pro",
        "prompt-engineering"
      ]
    },
    "content": {
      "readme": "# ğŸš€ Awesome Nano Banana Pro Prompts\n\n[![Awesome](https://awesome.re/badge.svg)](https://github.com/sindresorhus/awesome)\n[![GitHub stars](https://img.shields.io/github/stars/YouMind-OpenLab/awesome-nano-banana-pro-prompts?style=social)](https://github.com/YouMind-OpenLab/awesome-nano-banana-pro-prompts)\n[![License: CC BY 4.0](https://img.shields.io/badge/License-CC%20BY%204.0-lightgrey.svg)](https://creativecommons.org/licenses/by/4.0/)\n[![Update README](https://github.com/YouMind-OpenLab/awesome-nano-banana-pro-prompts/actions/workflows/update-readme.yml/badge.svg)](https://github.com/YouMind-OpenLab/awesome-nano-banana-pro-prompts/actions)\n[![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg)](docs/CONTRIBUTING.md)\n\n> ğŸ¨ A curated collection of creative prompts for Google's Nano Banana Pro\n\n> ğŸ’¡ **Note**: If you're interested in Gemini 3 prompts, feel free to check out our other repository with 50+ curated prompts: https://github.com/YouMind-OpenLab/awesome-gemini-3-prompts\n\n> âš ï¸ **Copyright Notice**: All prompts are collected from the community for educational purposes. If you believe any content infringes on your rights, please [open an issue](https://github.com/YouMind-OpenLab/awesome-nano-banana-pro-prompts/issues/new?template=bug-report.yml) and we will remove it promptly.\n\n---\n\n[![English](https://img.shields.io/badge/English-Current-brightgreen)](README.md) [![ç®€ä½“ä¸­æ–‡](https://img.shields.io/badge/%E7%AE%80%E4%BD%93%E4%B8%AD%E6%96%87-Click%20to%20View-lightgrey)](README_zh.md) [![ç¹é«”ä¸­æ–‡](https://img.shields.io/badge/%E7%B9%81%E9%AB%94%E4%B8%AD%E6%96%87-Click%20to%20View-lightgrey)](README_zh-TW.md) [![æ—¥æœ¬èª](https://img.shields.io/badge/%E6%97%A5%E6%9C%AC%E8%AA%9E-Click%20to%20View-lightgrey)](README_ja-JP.md) [![í•œêµ­ì–´](https://img.shields.io/badge/%ED%95%9C%EA%B5%AD%EC%96%B4-Click%20to%20View-lightgrey)](README_ko-KR.md) [![à¹„à¸—à¸¢](https://img.shields.io/badge/%E0%B9%84%E0%B8%97%E0%B8%A2-Click%20to%20View-lightgrey)](README_th-TH.md) [![Tiáº¿ng Viá»‡t](https://img.shields.io/badge/Ti%E1%BA%BFng%20Vi%E1%BB%87t-Click%20to%20View-lightgrey)](README_vi-VN.md) [![à¤¹à¤¿à¤¨à¥à¤¦à¥€](https://img.shields.io/badge/%E0%A4%B9%E0%A4%BF%E0%A4%A8%E0%A5%8D%E0%A4%A6%E0%A5%80-Click%20to%20View-lightgrey)](README_hi-IN.md) [![EspaÃ±ol](https://img.shields.io/badge/Espa%C3%B1ol-Click%20to%20View-lightgrey)](README_es-ES.md) [![EspaÃ±ol (LatinoamÃ©rica)](https://img.shields.io/badge/Espa%C3%B1ol%20(Latinoam%C3%A9rica)-Click%20to%20View-lightgrey)](README_es-419.md) [![Deutsch](https://img.shields.io/badge/Deutsch-Click%20to%20View-lightgrey)](README_de-DE.md) [![FranÃ§ais](https://img.shields.io/badge/Fran%C3%A7ais-Click%20to%20View-lightgrey)](README_fr-FR.md) [![Italiano](https://img.shields.io/badge/Italiano-Click%20to%20View-lightgrey)](README_it-IT.md) [![PortuguÃªs (Brasil)](https://img.shields.io/badge/Portugu%C3%AAs%20(Brasil)-Click%20to%20View-lightgrey)](README_pt-BR.md) [![PortuguÃªs](https://img.shields.io/badge/Portugu%C3%AAs-Click%20to%20View-lightgrey)](README_pt-PT.md) [![TÃ¼rkÃ§e](https://img.shields.io/badge/T%C3%BCrk%C3%A7e-Click%20to%20View-lightgrey)](README_tr-TR.md)\n\n---\n\n## ğŸŒ View in Web Gallery\n\n<div align=\"center\">\n\n![Cover](public/images/nano-banana-pro-prompts-cover-en.png)\n\n![List](public/images/nano-banana-pro-prompts-list-en.png)\n\n</div>\n\n**[ğŸ‘‰ Browse on YouMind Nano Banana Pro Prompts Gallery](https://youmind.com/nano-banana-pro-prompts)**\n\nWhy use our gallery?\n\n| Feature | GitHub README | youmind.com Gallery |\n|---------|--------------|---------------------|\n| ğŸ¨ Visual Layout | Linear list | Beautiful Masonry Grid |\n| ğŸ” Search | Ctrl+F only | Full-text search with filters |\n| ğŸ¤– AI One-Click Generation | - | AI one-click generation |\n| ğŸ“± Mobile | Basic | Fully responsive |\n\n---\n\n## ğŸ“– Table of Contents\n\n- [ğŸŒ View in Web Gallery](#-view-in-web-gallery)\n- [ğŸ¤” What is Nano Banana Pro?](#-what-is-nano-banana-pro)\n- [ğŸ“Š Statistics](#-statistics)\n- [ğŸ”¥ Featured Prompts](#-featured-prompts)\n- [ğŸ“‹ All Prompts](#-all-prompts)\n- [ğŸ¤ How to Contribute](#-how-to-contribute)\n- [ğŸ“„ License](#-license)\n- [ğŸ™ Acknowledgements](#-acknowledgements)\n- [â­ Star History](#-star-history)\n\n---\n\n## ğŸ¤” What is Nano Banana Pro?\n\n**Nano Banana Pro** is Google's latest multimodal AI model featuring:\n\n- ğŸ¯ **Multimodal Understanding** - Process text, images, and video\n- ğŸ¨ **High-Quality Generation** - Photorealistic to artistic styles\n- âš¡ **Fast Iteration** - Quick edits and variations\n- ğŸŒˆ **Diverse Styles** - From pixel art to oil paintings\n- ğŸ”§ **Precise Control** - Detailed composition and lighting\n- ğŸ“ **Complex Scenes** - Multi-object, multi-character rendering\n\nğŸ“š **Learn More:** [Nano Banana Pro: 10 Real Cases](https://youmind.com/blog/nano-banana-pro-10-real-cases)\n\n### ğŸš€ Raycast Integration\n\nSome prompts support **dynamic arguments** using [Raycast Snippets](https://raycast.com/help/snippets) syntax. Look for the ğŸš€ Raycast Friendly badge!\n\n**Example:**\n```\nA quote card with \"{argument name=\"quote\" default=\"Stay hungry, stay foolish\"}\"\nb",
      "default_branch": "main"
    },
    "fetched_at": "2025-12-06T02:26:12.567806"
  },
  {
    "basic_info": {
      "name": "RedInk",
      "full_name": "HisMax/RedInk",
      "owner": "HisMax",
      "description": "çº¢å¢¨ - åŸºäºğŸŒNano Banana ProğŸŒ çš„ä¸€ç«™å¼å°çº¢ä¹¦å›¾æ–‡ç”Ÿæˆå™¨ ã€Šä¸€å¥è¯ä¸€å¼ å›¾ç‰‡ç”Ÿæˆå°çº¢ä¹¦å›¾æ–‡ã€‹ Red Ink - A one-stop Xiaohongshu image-and-text generator based on the ğŸŒNano Banana ProğŸŒ, \"One Sentence, One Image: Generate Xiaohongshu Text and Images.\"",
      "url": "https://github.com/HisMax/RedInk",
      "clone_url": "https://github.com/HisMax/RedInk.git",
      "ssh_url": "git@github.com:HisMax/RedInk.git",
      "homepage": "",
      "created_at": "2025-11-25T10:12:54Z",
      "updated_at": "2025-12-06T02:18:23Z",
      "pushed_at": "2025-11-29T19:43:23Z"
    },
    "stats": {
      "stars": 2953,
      "forks": 586,
      "watchers": 2953,
      "open_issues": 12,
      "size": 19227
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 189067,
        "Vue": 117985,
        "TypeScript": 33936,
        "CSS": 23861,
        "Dockerfile": 1568,
        "HTML": 349
      },
      "license": "Other",
      "topics": []
    },
    "content": {
      "readme": "![](images/logo.png)\n\n---\n\n[![License: CC BY-NC-SA 4.0](https://img.shields.io/badge/License-CC%20BY--NC--SA%204.0-lightgrey.svg)](https://creativecommons.org/licenses/by-nc-sa/4.0/)\n[![Python 3.11+](https://img.shields.io/badge/python-3.11+-blue.svg)](https://www.python.org/downloads/)\n[![Vue 3](https://img.shields.io/badge/vue-3.x-green.svg)](https://vuejs.org/)\n\n# çº¢å¢¨ - å°çº¢ä¹¦AIå›¾æ–‡ç”Ÿæˆå™¨\n\n> è®©ä¼ æ’­ä¸å†éœ€è¦é—¨æ§›ï¼Œè®©åˆ›ä½œä»æœªå¦‚æ­¤ç®€å•\n\n![](images/index.gif)\n\n<p align=\"center\">\n  <em>çº¢å¢¨é¦–é¡µ</em>\n</p>\n\n<p align=\"center\">\n  <img src=\"images/showcase-grid.png\" alt=\"ä½¿ç”¨çº¢å¢¨ç”Ÿæˆçš„å„ç±»å°çº¢ä¹¦å°é¢\" width=\"600\"/>\n</p>\n\n<p align=\"center\">\n  <em>ä½¿ç”¨çº¢å¢¨ç”Ÿæˆçš„å„ç±»å°çº¢ä¹¦å°é¢ - AIé©±åŠ¨ï¼Œé£æ ¼ç»Ÿä¸€ï¼Œæ–‡å­—å‡†ç¡®</em>\n</p>\n\n\n\n## å†™åœ¨å‰é¢\n\nå‰æ®µæ—¶é—´é»˜å­åœ¨ Linux.do å‘äº†ä¸€ä¸ªç”¨ Nano banana Pro åš PPT çš„å¸–å­,æ”¶è·äº† 600 å¤šä¸ªèµã€‚å¾ˆå¤šäººç”¨ğŸŒNano banana Pro å»åšäº§å“å®£ä¼ å›¾ã€ç›´æ¥ç”Ÿæˆæ¼«ç”»ç­‰ç­‰ã€‚æˆ‘å°±åœ¨æƒ³:**ä¸ºä»€ä¹ˆä¸æ‹¿ğŸŒ2æ¥åšç‚¹æ›´åŠŸåˆ©ã€æ›´åˆºæ¿€çš„äº‹æƒ…?**\n\näºæ˜¯å°±æœ‰äº†è¿™ä¸ªé¡¹ç›®ã€‚ä¸€å¥è¯ä¸€å¼ å›¾ç‰‡ç”Ÿæˆå°çº¢ä¹¦å›¾æ–‡\n\n---\n\n## âœ¨ æ•ˆæœå±•ç¤º\n\n### è¾“å…¥ä¸€å¥è¯,å°±èƒ½ç”Ÿæˆå®Œæ•´çš„å°çº¢ä¹¦å›¾æ–‡\n\n#### æç¤ºè¯ï¼šç§‹å­£æ˜¾ç™½ç¾ç”²ï¼ˆæš—å¹¿ä¸€ä¸ªï¼šé»˜å­ç‰Œç¾ç”²ï¼‰ï¼Œå›¾ç‰‡ æ˜¯æˆ‘çš„å°çº¢ä¹¦ä¸»é¡µã€‚ç¬¦åˆæˆ‘çš„é£æ ¼ç”Ÿæˆ\n\n#### åŒæ—¶æˆ‘è¿˜æˆªå›¾äº†æˆ‘çš„å°çº¢ä¹¦ä¸»é¡µï¼ŒåŒ…æ‹¬æˆ‘çš„å¤´åƒï¼Œç­¾åï¼ŒèƒŒæ™¯ï¼Œå§“åä»€ä¹ˆçš„\n\n![ç¤ºä¾‹1](./images/example-1.png)\n\n#### ç„¶åç­‰å¾…10-20ç§’åï¼Œå°±ä¼šæœ‰æ¯ä¸€é¡µçš„å¤§çº²ï¼Œå¤§å®¶å¯ä»¥æ ¹æ®çš„è‡ªå·±çš„éœ€æ±‚å»è°ƒæ•´é¡µé¢é¡ºåºï¼ˆä¸å»ºè®®ï¼‰ï¼Œè‡ªå®šä¹‰æ¯ä¸€ä¸ªé¡µé¢çš„å†…å®¹ï¼ˆè¿™ä¸ªå¾ˆå»ºè®®ï¼‰\n\n![ç¤ºä¾‹2](./images/example-2.png)\n\n#### é¦–å…ˆç”Ÿæˆçš„æ˜¯å°é¢é¡µ\n\n![ç¤ºä¾‹3](./images/example-3.png)\n\n#### ç„¶åç¨ç­‰ä¸€ä¼šå„¿åï¼Œä¼šç”Ÿæˆåé¢çš„æ‰€æœ‰é¡µé¢ï¼ˆè¿™é‡Œæ˜¯å¹¶å‘ç”Ÿæˆçš„æ‰€æœ‰é¡µé¢ï¼ˆé»˜è®¤æ˜¯15ä¸ªï¼‰ï¼Œå¦‚æœå¤§å®¶çš„APIä¾›åº”å•†æ— æ³•æ”¯æŒé«˜å¹¶å‘çš„è¯ï¼Œè®°å¾—è¦å»æ”¹ä¸€ä¸‹è®¾ç½®ï¼‰\n\n![ç¤ºä¾‹4](./images/example-4.png)\n\n---\n\n## ğŸ—ï¸ æŠ€æœ¯æ¶æ„\n\n### åç«¯\n- **è¯­è¨€**: Python 3.11+\n- **æ¡†æ¶**: Flask\n- **AI æ¨¡å‹**:\n  - Gemini 3 (æ–‡æ¡ˆç”Ÿæˆ)\n  - ğŸŒNano banana Pro (å›¾ç‰‡ç”Ÿæˆ)\n- **åŒ…ç®¡ç†**: uv\n\n### å‰ç«¯\n- **æ¡†æ¶**: Vue 3 + TypeScript\n- **æ„å»º**: Vite\n- **çŠ¶æ€ç®¡ç†**: Pinia\n\n---\n\n## ğŸ“¦ å¦‚ä½•è‡ªå·±éƒ¨ç½²\n\n### æ–¹å¼ä¸€ï¼šDocker éƒ¨ç½²ï¼ˆæ¨èï¼‰\n\n**æœ€ç®€å•çš„éƒ¨ç½²æ–¹å¼ï¼Œä¸€è¡Œå‘½ä»¤å³å¯å¯åŠ¨ï¼š**\n\n```bash\ndocker run -d -p 12398:12398 -v ./history:/app/history -v ./output:/app/output histonemax/redink:latest\n```\n\nè®¿é—® http://localhost:12398ï¼Œåœ¨ Web ç•Œé¢çš„**è®¾ç½®é¡µé¢**é…ç½®ä½ çš„ API Key å³å¯ä½¿ç”¨ã€‚\n\n**ä½¿ç”¨ docker-composeï¼ˆå¯é€‰ï¼‰ï¼š**\n\nä¸‹è½½ [docker-compose.yml](https://github.com/HisMax/RedInk/blob/main/docker-compose.yml) åï¼š\n\n```bash\ndocker-compose up -d\n```\n\n**Docker éƒ¨ç½²è¯´æ˜ï¼š**\n- å®¹å™¨å†…ä¸åŒ…å«ä»»ä½• API Keyï¼Œéœ€è¦åœ¨ Web ç•Œé¢é…ç½®\n- ä½¿ç”¨ `-v ./history:/app/history` æŒä¹…åŒ–å†å²è®°å½•\n- ä½¿ç”¨ `-v ./output:/app/output` æŒä¹…åŒ–ç”Ÿæˆçš„å›¾ç‰‡\n- å¯é€‰ï¼šæŒ‚è½½è‡ªå®šä¹‰é…ç½®æ–‡ä»¶ `-v ./text_providers.yaml:/app/text_providers.yaml`\n\n---\n\n### æ–¹å¼äºŒï¼šæœ¬åœ°å¼€å‘éƒ¨ç½²\n\n**å‰ç½®è¦æ±‚ï¼š**\n- Python 3.11+\n- Node.js 18+\n- pnpm\n- uv\n\n### 1. å…‹éš†é¡¹ç›®\n```bash\ngit clone https://github.com/HisMax/RedInk.git\ncd RedInk\n```\n\n### 2. é…ç½® API æœåŠ¡\n\nå¤åˆ¶é…ç½®æ¨¡æ¿æ–‡ä»¶ï¼š\n```bash\ncp text_providers.yaml.example text_providers.yaml\ncp image_providers.yaml.example image_providers.yaml\n```\n\nç¼–è¾‘é…ç½®æ–‡ä»¶ï¼Œå¡«å…¥ä½ çš„ API Key å’ŒæœåŠ¡é…ç½®ã€‚ä¹Ÿå¯ä»¥å¯åŠ¨ååœ¨ Web ç•Œé¢çš„**è®¾ç½®é¡µé¢**è¿›è¡Œé…ç½®ã€‚\n\n### 3. å®‰è£…åç«¯ä¾èµ–\n```bash\nuv sync\n```\n\n### 4. å®‰è£…å‰ç«¯ä¾èµ–\n```bash\ncd frontend\npnpm install\n```\n\n### 5. å¯åŠ¨æœåŠ¡\n\n**å¯åŠ¨åç«¯:**\n```bash\nuv run python -m backend.app\n```\nè®¿é—®: http://localhost:12398\n\n**å¯åŠ¨å‰ç«¯:**\n```bash\ncd frontend\npnpm dev\n```\nè®¿é—®: http://localhost:5173\n\n---\n\n## ğŸ® ä½¿ç”¨æŒ‡å—\n\n### åŸºç¡€ä½¿ç”¨\n1. **è¾“å…¥ä¸»é¢˜**: åœ¨é¦–é¡µè¾“å…¥æƒ³è¦åˆ›ä½œçš„ä¸»é¢˜,å¦‚\"å¦‚ä½•åœ¨å®¶åšæ‹¿é“\"\n2. **ç”Ÿæˆå¤§çº²**: AI è‡ªåŠ¨ç”Ÿæˆ 6-9 é¡µçš„å†…å®¹å¤§çº²\n3. **ç¼–è¾‘ç¡®è®¤**: å¯ä»¥ç¼–è¾‘å’Œè°ƒæ•´æ¯ä¸€é¡µçš„æè¿°\n4. **ç”Ÿæˆå›¾ç‰‡**: ç‚¹å‡»ç”Ÿæˆ,å®æ—¶æŸ¥çœ‹è¿›åº¦\n5. **ä¸‹è½½ä½¿ç”¨**: ä¸€é”®ä¸‹è½½æ‰€æœ‰å›¾ç‰‡\n\n### è¿›é˜¶ä½¿ç”¨\n- **ä¸Šä¼ å‚è€ƒå›¾ç‰‡**: é€‚åˆå“ç‰Œæ–¹,ä¿æŒå“ç‰Œè§†è§‰é£æ ¼\n- **ä¿®æ”¹æè¿°è¯**: ç²¾ç¡®æ§åˆ¶æ¯ä¸€é¡µçš„å†…å®¹å’Œæ„å›¾\n- **é‡æ–°ç”Ÿæˆ**: å¯¹ä¸æ»¡æ„çš„é¡µé¢å•ç‹¬é‡æ–°ç”Ÿæˆ\n\n---\n\n## ğŸ”§ é…ç½®è¯´æ˜\n\n### é…ç½®æ–¹å¼\n\né¡¹ç›®æ”¯æŒä¸¤ç§é…ç½®æ–¹å¼ï¼š\n\n1. **Web ç•Œé¢é…ç½®ï¼ˆæ¨èï¼‰**ï¼šå¯åŠ¨æœåŠ¡åï¼Œåœ¨è®¾ç½®é¡µé¢å¯è§†åŒ–é…ç½®\n2. **YAML æ–‡ä»¶é…ç½®**ï¼šç›´æ¥ç¼–è¾‘é…ç½®æ–‡ä»¶\n\n### æ–‡æœ¬ç”Ÿæˆé…ç½®\n\né…ç½®æ–‡ä»¶: `text_providers.yaml`\n\n```yaml\n# å½“å‰æ¿€æ´»çš„æœåŠ¡å•†\nactive_provider: openai\n\nproviders:\n  # OpenAI å®˜æ–¹æˆ–å…¼å®¹æ¥å£\n  openai:\n    type: openai_compatible\n    api_key: sk-xxxxxxxxxxxxxxxxxxxx\n    base_url: https://api.openai.com/v1\n    model: gpt-4o\n\n  # Google Geminiï¼ˆåŸç”Ÿæ¥å£ï¼‰\n  gemini:\n    type: google_gemini\n    api_key: AIzaxxxxxxxxxxxxxxxxxxxxxxxxx\n    model: gemini-2.0-flash\n```\n\n### å›¾ç‰‡ç”Ÿæˆé…ç½®\n\né…ç½®æ–‡ä»¶: `image_providers.yaml`\n\n```yaml\n# å½“å‰æ¿€æ´»çš„æœåŠ¡å•†\nactive_provider: gemini\n\nproviders:\n  # Google Gemini å›¾ç‰‡ç”Ÿæˆ\n  gemini:\n    type: google_genai\n    api_key: AIzaxxxxxxxxxxxxxxxxxxxxxxxxx\n    model: gemini-3-pro-image-preview\n    high_concurrency: false  # é«˜å¹¶å‘æ¨¡å¼\n\n  # OpenAI å…¼å®¹æ¥å£\n  openai_image:\n    type: image_api\n    api_key: sk-xxxxxxxxxxxxxxxxxxxx\n    base_url: https://your-api-endpoint.com\n    model: dall-e-3\n    high_concurrency: false\n```\n\n### é«˜å¹¶å‘æ¨¡å¼è¯´æ˜\n\n- **å…³é—­ï¼ˆé»˜è®¤ï¼‰**ï¼šå›¾ç‰‡é€å¼ ç”Ÿæˆï¼Œé€‚åˆ GCP 300$ è¯•ç”¨è´¦å·æˆ–æœ‰é€Ÿç‡é™åˆ¶çš„ API\n- **å¼€å¯**ï¼šå›¾ç‰‡å¹¶è¡Œç”Ÿæˆï¼ˆæœ€å¤š15å¼ åŒæ—¶ï¼‰ï¼Œé€Ÿåº¦æ›´å¿«ï¼Œä½†éœ€è¦ API æ”¯æŒé«˜å¹¶å‘\n\nâš ï¸ **GCP 300$ è¯•ç”¨è´¦å·ä¸å»ºè®®å¯ç”¨é«˜å¹¶å‘**ï¼Œå¯èƒ½ä¼šè§¦å‘é€Ÿç‡é™åˆ¶å¯¼è‡´ç”Ÿæˆå¤±è´¥ã€‚\n\n---\n\n## âš ï¸ æ³¨æ„äº‹é¡¹\n\n1. **API é…é¢é™åˆ¶**:\n   - æ³¨æ„ Gemini å’Œå›¾ç‰‡ç”Ÿæˆ API çš„è°ƒç”¨é…é¢\n   - GCP è¯•ç”¨è´¦å·å»ºè®®å…³é—­é«˜å¹¶å‘æ¨¡å¼\n\n2. **ç”Ÿæˆæ—¶é—´**:\n   - å›¾ç‰‡ç”Ÿæˆéœ€è¦æ—¶é—´,è¯·è€å¿ƒç­‰å¾…ï¼ˆä¸è¦ç¦»å¼€é¡µé¢ï¼‰\n\n---\n\n## ğŸ¤ å‚ä¸è´¡çŒ®\n\næ¬¢è¿æäº¤ Issue å’Œ Pull Request!\n\nå¦‚æœè¿™ä¸ªé¡¹ç›®å¯¹ä½ æœ‰å¸®åŠ©,æ¬¢è¿ç»™ä¸ª Star â­\n\n### æœªæ¥è®¡åˆ’\n- [ ] æ”¯æŒæ›´å¤šå›¾ç‰‡æ ¼å¼ï¼Œä¾‹å¦‚ä¸€å¥è¯ç”Ÿæˆä¸€å¥—PPTä»€ä¹ˆçš„\n- [x] å†å²è®°å½•ç®¡ç†ä¼˜åŒ–\n- [ ] å¯¼å‡ºä¸ºå„ç§æ ¼å¼(PDFã€é•¿å›¾ç­‰)\n\n---\n\n## æ›´æ–°æ—¥å¿—\n\n### v1.4.0 (2025-11-30)\n- ğŸ—ï¸ åç«¯æ¶æ„é‡æ„ï¼šæ‹†åˆ†å•ä½“è·¯ç”±ä¸ºæ¨¡å—åŒ–è“å›¾ï¼ˆhistoryã€imagesã€generationã€outlineã€configï¼‰\n- ğŸ—ï¸ å‰ç«¯ç»„ä»¶é‡æ„ï¼šæå–å¯å¤ç”¨ç»„ä»¶ï¼ˆImageGalleryModalã€OutlineModalã€ShowcaseBackgroundç­‰ï¼‰\n- âœ¨ ä¼˜åŒ–é¦–é¡µè®¾è®¡ï¼Œç§»é™¤å†—ä½™å†…å®¹åŒºå—\n- âœ¨ èƒŒæ™¯å›¾ç‰‡é¢„åŠ è½½å’Œæ¸å…¥åŠ¨ç”»ï¼Œæå‡åŠ è½½ä½“éªŒ\n- âœ¨ å†å²è®°å½•æŒä¹…åŒ–æ”¯æŒï¼ˆDockeréƒ¨ç½²ï¼‰\n- ğŸ”§ ä¿®å¤å†å²è®°å½•é¢„è§ˆå’Œå¤§çº²æŸ¥çœ‹åŠŸèƒ½\n- ğŸ”§ ä¼˜åŒ–Modalç»„ä»¶å¯è§æ€§æ§åˆ¶\n- ğŸ§ª æ–°å¢65ä¸ªåç«¯å•å…ƒæµ‹è¯•\n\n### v1.3.0 (2025-11-26)\n- âœ¨ æ–°å¢ Docker æ”¯æŒï¼Œä¸€é”®éƒ¨ç½²\n- âœ¨ å‘å¸ƒå®˜æ–¹ Docker é•œåƒåˆ° Docker Hub: `histonemax/redink`\n- ğŸ”§ Flask è‡ªåŠ¨æ£€æµ‹å‰ç«¯æ„å»ºäº§ç‰©ï¼Œæ”¯æŒå•å®¹å™¨éƒ¨ç½²\n- ğŸ”§ Docker é•œåƒå†…ç½®ç©ºç™½é…ç½®æ¨¡æ¿ï¼Œä¿æŠ¤ API Key å®‰å…¨\n- ğŸ“ æ›´æ–° READMEï¼Œæ·»åŠ  Docker éƒ¨ç½²è¯´æ˜\n\n### v1.2.0 (2025-11-26)\n- âœ¨ æ–°å¢ç‰ˆæƒä¿¡æ¯å±•ç¤ºï¼Œæ‰€æœ‰é¡µé¢æ˜¾ç¤ºå¼€æºåè®®å’Œé¡¹ç›®é“¾æ¥\n- âœ¨ ä¼˜åŒ–å›¾ç‰‡é‡æ–°ç”ŸæˆåŠŸèƒ½ï¼Œæ”¯æŒå•å¼ å›¾ç‰‡é‡ç»˜\n- âœ¨ é‡æ–°ç”Ÿæˆå›¾ç‰‡æ—¶ä¿æŒé£æ ¼ä¸€è‡´ï¼Œä¼ é€’å®Œæ•´ä¸Šä¸‹æ–‡ï¼ˆå°é¢å›¾ã€å¤§çº²ã€ç”¨æˆ·è¾“å…¥ï¼‰\n- âœ¨ ä¿®å¤å›¾ç‰‡ç¼“å­˜é—®é¢˜ï¼Œé‡æ–°ç”Ÿæˆçš„å›¾ç‰‡ç«‹å³åˆ·æ–°æ˜¾ç¤º\n- âœ¨ ç»Ÿä¸€æ–‡æœ¬ç”Ÿæˆå®¢æˆ·ç«¯æ¥å£ï¼Œæ”¯æŒ Google Gemini å’Œ OpenAI å…¼å®¹æ¥å£è‡ªåŠ¨åˆ‡æ¢\n- âœ¨ æ–°å¢ Web ç•Œé¢é…ç½®åŠŸèƒ½ï¼Œå¯è§†åŒ–ç®¡ç† API æœåŠ¡å•†\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-12-06T02:26:13.722246"
  },
  {
    "basic_info": {
      "name": "misaka26",
      "full_name": "straight-tamago/misaka26",
      "owner": "straight-tamago",
      "description": "iOS /iPadOS 16.0 - 26.1, An ultimate customization tool, uilitizing the bug that makes TrollRestore possible. ",
      "url": "https://github.com/straight-tamago/misaka26",
      "clone_url": "https://github.com/straight-tamago/misaka26.git",
      "ssh_url": "git@github.com:straight-tamago/misaka26.git",
      "homepage": "",
      "created_at": "2025-11-16T14:48:46Z",
      "updated_at": "2025-12-05T20:49:18Z",
      "pushed_at": "2025-11-19T13:53:41Z"
    },
    "stats": {
      "stars": 2927,
      "forks": 123,
      "watchers": 2927,
      "open_issues": 272,
      "size": 21
    },
    "tech_info": {
      "language": null,
      "languages": {},
      "license": null,
      "topics": []
    },
    "content": {
      "readme": "# misaka26\nAn ultimate customization tool, uilitizing the bug that makes [TrollRestore](https://github.com/JJTech0130/TrollRestore) possible.\n### iOS /iPadOS 16.0 - 26.1 & 26.2 beta 1\n**Supported iOS 16.0 ~ 26.1 & 26.2 beta 1**\n\n> [!NOTE]\n> **Use this tool at your own risk. There is a chance you may bootloop, so create a backup before using.**\n\nDevelopers: [@34306](https://github.com/34306) [@straight-tamago](https://github.com/straight-tamago)\n\n<a href=\"https://github.com/straight-tamago/misaka26/releases/latest\"><img src=\"https://img.shields.io/github/v/release/straight-tamago/misaka26?color=d774d5\" /></a>\n<a href=\"https://github.com/straight-tamago/misaka26/releases\"><img src=\"https://img.shields.io/github/downloads/straight-tamago/misaka26/total?color=d774d5\" /></a>\n\n## Join Discord Support ğŸ‰ \nâ€¢ Misaka Support âœ¨ **(Sever 1)**:   \n<a href='https://discord.gg/KSExeZVAGX'><img align='center' alt='Discord' src='https://img.shields.io/discord/1156843198799421490?color=36309d&label=DISCORD&logo=discord&logoColor=white&style=for-the-badge'></a>  \nâ€¢ Misaka Support âœ¨ **(Sever 2)**:  \n<a href='https://discord.gg/mVrPxY3X6W'><img align='center' alt='Discord' src='https://img.shields.io/discord/1074625970029477919?color=36309d&label=DISCORD&logo=discord&logoColor=white&style=for-the-badge'></a>   \n\n### Installation:\nTo fix permission issues on macOS, run:\n```bash\nxattr -c /path/to/misaka26.app\n```\n\n## How to Use\n\nDownload lastest misaka26 on Release tab, place it to `/Applications`\nWhen open it, go to System Settings > Privacy and Security > Security > Allow running this Application > Open anyway\n\n1. Generate your MobileGestalt by using this [Shortcut](https://routinehub.co/shortcut/23246/#/login)\n2. AirDrop or send it to macOS\n3. Select it in misaka26 application\n4. Enable features\n5. Apply\n6. Profit?\n\n## Supported Features\n- **TrollPad (MultiTasking)** (NEW, require macOS)\n(iOS 18.0+) (need respring, read below)\n- **Enable PWM (iOS 26.0+)**\n- **Enable Security Research Device (SRD) mode (iOS 26.0+)**\n- **Allow Install M chip/Pro chip games on AppStore (eg: RE4) (iOS 26.0+)**\n\n\n- **TrollStore Installer (iOS 15.2 ~ 16.7RC (20H18) & 17.0)**\n- **Dynamic Island** (iOS 16.0+)\n- **Charge Limit** (iOS 17.0+)\n- **Boot Chime** (iOS 17.0+)\n- **Stage Manager** (iOS 16.0+)\n- **Shutter Sound** (iOS 16.0+)  \nPlease do not use camera silence for the purpose of voyeurism. For photographing pets, etc.\n- **Always-on Display (AoD)** (iOS 18.0+)\n- **Apple Pencil** (iOS 18.0+)\n- **Action Button** (iOS 17.0+)\n- **Internal Storage** (iOS 17.0+)\n- **Clock UI** (iOS 18.0+)\n- **SOS Collision** (iOS 18.0+)\n- **TapToWake** (iPhone SE 2/3, iOS 18.0+)\n- **Apple Intelligence** (iOS 18.1 Beta, ALL DEVICES ON 18.1)\n- **Landscape FaceID** (iOS 17.0+)\n- **Old Photo UI** (iOS 18.0+)\n- **iPad Apps Support** (iOS 16.0+)\n- **Developer Mode & Metal HUD** (iOS 16.0+)\n- **CameraControl** (18.0+)\n- **AoD Vibrancy** (may affect others tweak, iOS 18.0+)\n- **Sleep apnea** (iOS 18.0+)\n- **Find My Friend (KH/A devices)**\n\n**Some upcoming features:** â¤ï¸\n- **Disable call record greetings**\n- **Allow modify custom path**\n\n\n**You will need to do a respring to take effect, download this [ipa](https://github.com/34306/mdc0/releases/download/1.0/respringapp.ipa) to respring**\n\n## icon\n\n## Credit\n- [Duy Tran](https://github.com/khanhduytran0) for the exploit writeup\n- [hanakim3945](https://github.com/hanakim3945)\n- pengubow for those new flags\n\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-12-06T02:26:14.826732"
  },
  {
    "basic_info": {
      "name": "reader3",
      "full_name": "karpathy/reader3",
      "owner": "karpathy",
      "description": "Quick illustration of how one can easily read books together with LLMs. It's great and I highly recommend it.",
      "url": "https://github.com/karpathy/reader3",
      "clone_url": "https://github.com/karpathy/reader3.git",
      "ssh_url": "git@github.com:karpathy/reader3.git",
      "homepage": null,
      "created_at": "2025-11-18T02:37:00Z",
      "updated_at": "2025-12-06T02:18:00Z",
      "pushed_at": "2025-11-18T02:37:51Z"
    },
    "stats": {
      "stars": 2492,
      "forks": 304,
      "watchers": 2492,
      "open_issues": 11,
      "size": 271
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 13925,
        "HTML": 8921
      },
      "license": null,
      "topics": []
    },
    "content": {
      "readme": "# reader 3\n\n![reader3](reader3.png)\n\nA lightweight, self-hosted EPUB reader that lets you read through EPUB books one chapter at a time. This makes it very easy to copy paste the contents of a chapter to an LLM, to read along. Basically - get epub books (e.g. [Project Gutenberg](https://www.gutenberg.org/) has many), open them up in this reader, copy paste text around to your favorite LLM, and read together and along.\n\nThis project was 90% vibe coded just to illustrate how one can very easily [read books together with LLMs](https://x.com/karpathy/status/1990577951671509438). I'm not going to support it in any way, it's provided here as is for other people's inspiration and I don't intend to improve it. Code is ephemeral now and libraries are over, ask your LLM to change it in whatever way you like.\n\n## Usage\n\nThe project uses [uv](https://docs.astral.sh/uv/). So for example, download [Dracula EPUB3](https://www.gutenberg.org/ebooks/345) to this directory as `dracula.epub`, then:\n\n```bash\nuv run reader3.py dracula.epub\n```\n\nThis creates the directory `dracula_data`, which registers the book to your local library. We can then run the server:\n\n```bash\nuv run server.py\n```\n\nAnd visit [localhost:8123](http://localhost:8123/) to see your current Library. You can easily add more books, or delete them from your library by deleting the folder. It's not supposed to be complicated or complex.\n\n## License\n\nMIT",
      "default_branch": "master"
    },
    "fetched_at": "2025-12-06T02:26:15.960239"
  },
  {
    "basic_info": {
      "name": "omnilingual-asr",
      "full_name": "facebookresearch/omnilingual-asr",
      "owner": "facebookresearch",
      "description": "Omnilingual ASR Open-Source Multilingual SpeechRecognition for 1600+ Languages",
      "url": "https://github.com/facebookresearch/omnilingual-asr",
      "clone_url": "https://github.com/facebookresearch/omnilingual-asr.git",
      "ssh_url": "git@github.com:facebookresearch/omnilingual-asr.git",
      "homepage": null,
      "created_at": "2025-11-06T22:38:00Z",
      "updated_at": "2025-12-06T02:17:42Z",
      "pushed_at": "2025-11-19T18:07:58Z"
    },
    "stats": {
      "stars": 2380,
      "forks": 199,
      "watchers": 2380,
      "open_issues": 21,
      "size": 1026
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 295799
      },
      "license": "Other",
      "topics": []
    },
    "content": {
      "readme": "<div align=\"center\">\n  <img src=\"./omniASR_header.jpg\" alt=\"Header image with a collage of on-the-ground photos from the transcription gathering efforts in Pakistan and Liberia.\" width=\"100%\" />\n  <p><i>Photographs captured during corpus creation efforts in Pakistan and Liberia.</i></p>\n</div>\n\n# Omnilingual ASR: Open-Source Multilingual Speech Recognition for 1600+ Languages\n\nOmnilingual ASR is an open-source speech recognition system supporting over 1,600 languages â€” including hundreds never previously covered by any ASR technology. Designed for broad accessibility, it enables new languages to be added with just a few paired examples without requiring specialized expertise or large datasets. By combining scalable zero-shot learning with a flexible model family, Omnilingual ASR aims to make speech technology more inclusive and adaptable for communities and researchers worldwide.\n\n* [Huggingface Demo](https://huggingface.co/spaces/facebook/omniasr-transcriptions)\n* [Huggingface Dataset](https://huggingface.co/datasets/facebook/omnilingual-asr-corpus)\n* [Paper](https://ai.meta.com/research/publications/omnilingual-asr-open-source-multilingual-speech-recognition-for-1600-languages/)\n* [Blogpost](http://ai.meta.com/blog/omnilingual-asr-advancing-automatic-speech-recognition)\n\n<div align=\"center\">\n  <img src=\"./result_table.png\" alt=\"Performance results table\" width=\"100%\" />\n  <p><i>Our 7B-LLM-ASR system achieves state-of-the-art performance across 1,600+ languages, with character error rates (CER) below 10 for 78% of those languages.</i></p>\n</div>\n\n\n## Documentation\n\n### Quick Start\n- **[Installation & Basic Usage](#installation)** - Setup and first transcription\n- **[Inference Pipeline](src/omnilingual_asr/models/inference/README.md)** - Comprehensive transcription guide with batch processing, language conditioning, and context examples\n- **[Supported Languages](#supported-languages)** - View the complete list of 1600+ supported languages\n\n\n### Models & Architecture\n- **[Model Specifications](#model-architectures)** - Available models, parameters, and memory requirements\n- **[Architecture Overview](src/omnilingual_asr/models/README.md)** - Technical details on W2V, CTC, and LLM model families\n- **[Asset Management](src/omnilingual_asr/cards/README.md)** - Configuration system for models, tokenizers, and datasets\n\n### Training & Data Pipeline\n- **[Data Preparation](workflows/dataprep/README.md)** - End-to-end guide for multilingual dataset preparation, HuggingFace integration, and parquet processing\n- **[Training Recipes](workflows/recipes/wav2vec2/asr/README.md)** - Pre-configured workflows for CTC and LLM model training\n\n---\n\n## Installation\n\nThe models were developed using [fairseq2](https://github.com/facebookresearch/fairseq2), a research-focused sequence modeling toolkit. While we provide a **reference** inference pipeline that works across platforms, audio support requires [libsndfile](https://github.com/facebookresearch/fairseq2?tab=readme-ov-file#system-dependencies) (Mac: `brew install libsndfile`; Windows may need an additional [setup](https://github.com/facebookresearch/fairseq2?tab=readme-ov-file#installing-on-windows)).\n\n```bash\n# using pip\npip install omnilingual-asr\n\n# using uv\nuv add omnilingual-asr\n```\n\n## Inference\n\n```python\nfrom omnilingual_asr.models.inference.pipeline import ASRInferencePipeline\n\npipeline = ASRInferencePipeline(model_card=\"omniASR_LLM_7B\")\n\naudio_files = [\"/path/to/eng_audio1.flac\", \"/path/to/deu_audio2.wav\"]\nlang = [\"eng_Latn\", \"deu_Latn\"]\ntranscriptions = pipeline.transcribe(audio_files, lang=lang, batch_size=2)\n```\n\nMore details on running specific models can be found in the [src/omnilingual_asr/models/inference](/src/omnilingual_asr/models/inference/README.md) directory.\n\n> **âš ï¸ Important:** Currently only audio files shorter than 40 seconds are accepted for inference. We plan to add support for transcribing unlimited-length audio files shortly.\n\n### Supported Languages\n\nTo view the full list of 1600+ supported languages, you can access the language list [programmatically](/src/omnilingual_asr/models/wav2vec2_llama/lang_ids.py):\n\n```python\nfrom omnilingual_asr.models.wav2vec2_llama.lang_ids import supported_langs\n\n# Print all supported languages\nprint(f\"Total supported languages: {len(supported_langs)}\")\nprint(supported_langs)\n\n# Check if a specific language is supported\nif \"eng_Latn\" in supported_langs:\n    print(\"English (Latin script) is supported!\")\n```\n\nLanguages follow the format `{language_code}_{script}`, for example `eng_Latn` - English (Latin script), `cmn_Hans` - Mandarin Chinese (Simplified), ...\n\n### Using the HuggingFace Dataset ğŸ¤—\n\nWe provide a large-scale multilingual speech dataset on HuggingFace under CC-BY-4.0 License: [`facebook/omnilingual-asr-corpus`](https://huggingface.co/datasets/facebook/omnilingual-asr-corpus).\nThis dataset can be directly used with our inference pipeline for evaluation or testing:\n\n```bash\npip install \"omnilingual-asr[dat",
      "default_branch": "main"
    },
    "fetched_at": "2025-12-06T02:26:17.087067"
  },
  {
    "basic_info": {
      "name": "mgrep",
      "full_name": "mixedbread-ai/mgrep",
      "owner": "mixedbread-ai",
      "description": "A calm, CLI-native way to semantically grep everything, like code, images, pdfs and more.",
      "url": "https://github.com/mixedbread-ai/mgrep",
      "clone_url": "https://github.com/mixedbread-ai/mgrep.git",
      "ssh_url": "git@github.com:mixedbread-ai/mgrep.git",
      "homepage": "https://demo.mgrep.mixedbread.com",
      "created_at": "2025-11-06T01:01:47Z",
      "updated_at": "2025-12-06T02:17:01Z",
      "pushed_at": "2025-12-04T18:53:53Z"
    },
    "stats": {
      "stars": 1989,
      "forks": 84,
      "watchers": 1989,
      "open_issues": 25,
      "size": 547
    },
    "tech_info": {
      "language": "TypeScript",
      "languages": {
        "TypeScript": 85062,
        "Shell": 9080,
        "Python": 3116,
        "JavaScript": 1505
      },
      "license": "Apache License 2.0",
      "topics": []
    },
    "content": {
      "readme": "<div align=\"center\">\n  <a href=\"https://github.com/mixedbread-ai/mgrep\">\n    <img src=\"public/logo_mb.svg\" alt=\"mgrep\" width=\"96\" height=\"96\" />\n  </a>\n  <h1>mgrep</h1>\n  <p><em>A calm, CLI-native way to semantically grep everything, like code, images, pdfs and more.</em></p>\n  <a href=\"https://www.npmjs.com/package/@mixedbread/mgrep\"><img src=\"https://badge.fury.io/js/@mixedbread%2Fcli.svg\" alt=\"npm version\" /></a>\n  <a href=\"https://opensource.org/licenses/Apache-2.0\"><img src=\"https://img.shields.io/badge/License-Apache%202.0-blue.svg\" alt=\"License: Apache 2.0\" /></a><br>\n  <a href=\"https://demo.mgrep.mixedbread.com\"><img src=\"https://img.shields.io/badge/Playground-Try%20it%20now-brightgreen\" alt=\"Playground: Try it now\" /></a>\n\n  <br>\n\n  <p align=\"center\">\n    <video src=\"https://github.com/user-attachments/assets/7cb6d2ab-f96b-4092-9088-abbca85b0d52\" controls=\"controls\" style=\"max-width: 730px;\">\n      Your browser does not support the video tag.\n    </video>\n  </p>\n</div>\n\n## Why mgrep?\n- Natural-language search that feels as immediate as `grep`.\n- Semantic, multilingual & multimodal (audio, video support coming soon!)\n- Smooth background indexing via `mgrep watch`, designed to detect and keep up-to-date everything that matters inside any git repository.\n- Friendly device-login flow and first-class coding agent integrations.\n- Built for agents and humans alike, and **designed to be a helpful tool**, not a restrictive harness: quiet output, thoughtful defaults, and escape hatches everywhere.\n- Reduces the token usage of your agent by 2x while maintaining superior performance\n\n```bash\n# index once\nmgrep watch\n\n# then ask your repo things in natural language\nmgrep \"where do we set up auth?\"\n```\n\n## Quick Start\n\n1. **Install**\n   ```bash\n   npm install -g @mixedbread/mgrep    # or pnpm / bun\n   ```\n\n2. **Sign in once**\n   ```bash\n   mgrep login\n   ```\n   A browser window (or verification URL) guides you through Mixedbread authentication.\n\n   **Alternative: API Key Authentication**\n   For CI/CD or headless environments, set the `MXBAI_API_KEY` environment variable:\n   ```bash\n   export MXBAI_API_KEY=your_api_key_here\n   ```\n   This bypasses the browser login flow entirely.\n\n3. **Index a project**\n   ```bash\n   cd path/to/repo\n   mgrep watch\n   ```\n   `watch` performs an initial sync, respects `.gitignore`, then keeps the Mixedbread store updated as files change.\n\n4. **Search anything**\n   ```bash\n   mgrep \"where do we set up auth?\" src/lib\n   mgrep -m 25 \"store schema\"\n   ```\n   Searches default to the current working directory unless you pass a path.\n\n**Today, `mgrep` works great on:** code, text, PDFs, images.  \n**Coming soon:** audio & video.\n\n## Using it with Coding Agents\n\n`mgrep` supports assisted installation commands for many agents:\n- `mgrep install-claude-code` for Claude Code\n- `mgrep install-opencode` for OpenCode\n- `mgrep install-codex` for Codex\n- `mgrep install-droid` for Factory Droid\n\nThese commands sign you in (if needed) and add Mixedbread `mgrep` support to the\nagent. After that you only have to start the agent in your project folder, thats\nit.\n\n### More Agents Coming Soon\n\nMore agents (Cursor, Windsurf, etc.) are on the wayâ€”this section will grow as soon as each integration lands.\n\n## Making your agent smarter\n\nWe plugged `mgrep` into Claude Code and ran a benchmark of 50 QA tasks to evaluate the economics of `mgrep` against `grep`.\n\n![mgrep benchmark](public/bench.jpg)\n\nIn our 50-task benchmark, `mgrep`+Claude Code used ~2x fewer tokens than grep-based workflows at similar or better judged quality.\n\n`mgrep` finds the relevant snippets in a few semantic queries first, and the model spends its capacity on reasoning instead of scanning through irrelevant code from endless `grep` attempts. You can [Try it yourself](http://demo.mgrep.mixedbread.com).\n\n*Note: Win Rate (%) was calculated by using an LLM as a judge.*\n\n## Why we built mgrep\n\n`grep` is an amazing tool. It's lightweight, compatible with just about every machine on the planet, and will reliably surface any potential match within any target folder.\n\nBut grep is **from 1973**, and it carries the limitations of its era: you need exact patterns and it slows down considerably in the cases where you need it most, on large codebases.\n\nWorst of all, if you're looking for deeply-buried critical business logic, you cannot describe it: you have to be able to accurately guess what kind of naming patterns would have been used by the previous generations of engineers at your workplace for `grep` to find it. This will often result in watching a coding agent desperately try hundreds of patterns, filling its token window, and your upcoming invoice, with thousands of tokens. \n\nBut it doesn't have to be this way. Everything else in our toolkit is increasingly tailored to understand us, and so should our search tools. `mgrep` is our way to bring `grep` to 2025, integrating all of the advances in semantic understanding and code-search, without sacri",
      "default_branch": "main"
    },
    "fetched_at": "2025-12-06T02:26:18.213881"
  },
  {
    "basic_info": {
      "name": "server-survival",
      "full_name": "pshenok/server-survival",
      "owner": "pshenok",
      "description": "Tower defense game that teaches cloud architecture. Build infrastructure, survive traffic, learn scaling.",
      "url": "https://github.com/pshenok/server-survival",
      "clone_url": "https://github.com/pshenok/server-survival.git",
      "ssh_url": "git@github.com:pshenok/server-survival.git",
      "homepage": "https://pshenok.github.io/server-survival/",
      "created_at": "2025-11-21T02:36:29Z",
      "updated_at": "2025-12-06T01:47:59Z",
      "pushed_at": "2025-12-06T00:14:06Z"
    },
    "stats": {
      "stars": 1734,
      "forks": 94,
      "watchers": 1734,
      "open_issues": 16,
      "size": 12848
    },
    "tech_info": {
      "language": "JavaScript",
      "languages": {
        "JavaScript": 97207,
        "HTML": 42632,
        "CSS": 5137
      },
      "license": "MIT License",
      "topics": [
        "aws",
        "cloud",
        "devops",
        "education",
        "game",
        "javascript",
        "learning",
        "open-source",
        "simulation",
        "threejs",
        "tower-defense"
      ]
    },
    "content": {
      "readme": "# Server Survival ğŸ–¥ï¸ğŸ”¥\n\n![Gameplay Demo](assets/gameplay.gif)\n\n**Server Survival** is an interactive 3D simulation game where you play as a **Cloud Architect**. Your mission is to build and scale a resilient cloud infrastructure to handle increasing traffic loads while fighting off DDoS attacks and managing your budget.\n\n## ğŸ® How to Play\n\n### Objective\nSurvive as long as possible! Manage your **Budget ($)** and **Reputation (%)**.\n- **Earn Money** by successfully processing legitimate traffic (Web & API).\n- **Lose Reputation** if requests fail or if Fraud traffic slips through.\n- **Game Over** if Reputation hits 0% or you go bankrupt.\n\n### Traffic Types\n- ğŸŸ¢ **Web Traffic (Green):** Needs to be stored in **S3**.\n- ğŸŸ  **API Traffic (Orange):** Needs to be processed and saved to a **Database**.\n- ğŸŸ£ **Fraud/DDoS (Pink):** Must be blocked by a **WAF**.\n\n### Infrastructure & Services\nBuild your architecture using the toolbar. Each service has a cost and upkeep:\n\n| Service | Cost | Upkeep | Function |\n| :--- | :--- | :--- | :--- |\n| **WAF** | $40 | Low | **Firewall.** The first line of defense. Blocks Fraud traffic. |\n| **SQS** | $40 | Low | **Queue.** Buffers requests during spikes. Prevents drops. |\n| **ALB** | $50 | Medium | **Load Balancer.** Distributes traffic to multiple Compute instances. |\n| **Compute** | $80 | High | **EC2 Instance.** Processes requests. **Upgradeable (Tiers 1-3).** |\n| **ElastiCache** | $75 | Medium | **Redis Cache.** Caches responses to reduce DB load. |\n| **Database** | $180 | Very High | **RDS.** Destination for API traffic. **Upgradeable (Tiers 1-3).** |\n| **S3** | $30 | Low | **Storage.** Destination for Web traffic. |\n\n### Scoring & Economy\n- **Web Request:** +$0.80 / +5 Score\n- **API Request:** +$1.20 / +8 Score\n- **Fraud Blocked:** +10 Score\n- **Fraud Leak:** -8 Reputation\n- **Upkeep Scaling:** Costs increase 1x to 2x over 10 minutes.\n\n### Game Modes\n\n#### Survival Mode\nThe classic experience - survive as long as possible against escalating traffic.\n\n#### Sandbox Mode\nA fully customizable testing environment for experimenting with any architecture:\n\n| Control | Description |\n| :--- | :--- |\n| **Budget** | Set any starting budget (slider 0-10K, or type any amount) |\n| **RPS** | Control traffic rate (0 = stopped, or type 100+ for stress tests) |\n| **Traffic Mix** | Adjust WEB/API/FRAUD percentages independently |\n| **Burst** | Spawn instant bursts of specific traffic types |\n| **Upkeep Toggle** | Enable/disable service costs |\n| **Clear All** | Reset all services and restore budget |\n\n**No game over in Sandbox** - experiment freely!\n\n### Last Features\n- **Sandbox Mode:** Full control over budget, traffic rate, traffic mix, and burst spawning\n- **ESC Menu:** Press ESC to pause and open main menu, Resume to return (stays paused)\n- **Service Upgrades:** Click on **Compute** or **Database** instances with their respective tools to upgrade them.\n    - **Tier 2:** Increased capacity (Cost: $200/$400).\n    - **Tier 3:** Maximum capacity (Cost: $250/$600).\n- **Smart Load Balancing:** ALBs now use **Round Robin** to distribute traffic evenly across all connected instances.\n- **Visual Feedback:** Upgradeable services glow when hovered, and rings indicate their current tier.\n\n### Controls\n- **Left Click:** Select tools, place services, and connect nodes.\n- **Right Click + Drag:** Pan the camera.\n- **ESC:** Open main menu and pause game. Press again or click Resume to close menu (stays paused).\n- **Camera Reset:** Press `R` to reset the camera position.\n- **Birds-Eye View:** Press `T` to switch between isometric and top-down view.\n- **Hide HUD:** Press `H` to toggle UI panels.\n- **Connect Tool:** Click two nodes to create a connection (flow direction matters!).\n    - *Valid Flows:* Internet -> WAF -> ALB -> SQS -> Compute -> Cache -> (DB/S3)\n- **Delete Tool:** Remove services to recover 50% of the cost.\n- **Time Controls:** Pause, Play (1x), and Fast Forward (3x).\n\n## ğŸ§  Strategy Tips\n1.  **Block Fraud First:** Always place a WAF immediately connected to the Internet. Fraud leaks destroy reputation fast.\n2.  **Scale Compute:** As traffic increases, a single Compute node won't be enough. Use an ALB to split traffic across multiple Compute nodes.\n3.  **Watch Your Queues:** If a service's queue fills up (red ring), requests will fail. Add more capacity!\n4.  **Budget Wisely:** Databases are expensive. Don't over-provision them early on.\n\n## ğŸ› ï¸ Tech Stack\n\n- **Core:** Vanilla JavaScript (ES6+)\n- **Rendering:** [Three.js](https://threejs.org/) for 3D visualization.\n- **Styling:** [Tailwind CSS](https://tailwindcss.com/) for the glassmorphism UI.\n- **Build:** No build step required! Just standard HTML/CSS/JS.\n\n## ğŸš€ Getting Started\n\n1.  Clone the repository.\n2.  Open `index.html` in your modern web browser.\n3.  Start building your cloud empire!\n\n\n## ğŸ’¬ Community\n\nJoin our Discord server to discuss strategies and share your high scores:\n[Join Discord](https://discord.gg/zqcF8CXK)\n\n---\n*Built with code and",
      "default_branch": "main"
    },
    "fetched_at": "2025-12-06T02:26:19.352456"
  },
  {
    "basic_info": {
      "name": "supertonic",
      "full_name": "supertone-inc/supertonic",
      "owner": "supertone-inc",
      "description": "Lightning-fast, on-device TTS â€” running natively via ONNX.",
      "url": "https://github.com/supertone-inc/supertonic",
      "clone_url": "https://github.com/supertone-inc/supertonic.git",
      "ssh_url": "git@github.com:supertone-inc/supertonic.git",
      "homepage": "https://huggingface.co/spaces/Supertone/supertonic",
      "created_at": "2025-11-18T08:23:58Z",
      "updated_at": "2025-12-06T01:43:50Z",
      "pushed_at": "2025-11-27T08:52:36Z"
    },
    "stats": {
      "stars": 1633,
      "forks": 146,
      "watchers": 1633,
      "open_issues": 31,
      "size": 508
    },
    "tech_info": {
      "language": "JavaScript",
      "languages": {
        "JavaScript": 51184,
        "Swift": 50606,
        "C++": 45623,
        "Java": 40553,
        "C#": 38525,
        "Go": 32474,
        "Rust": 31267,
        "Dart": 26402,
        "Python": 17651,
        "Shell": 11698,
        "CSS": 7843,
        "HTML": 3479,
        "CMake": 3332,
        "Ruby": 1507
      },
      "license": "MIT License",
      "topics": [
        "cpp",
        "csharp",
        "go",
        "ios",
        "java",
        "lightweight",
        "nodejs",
        "on-device",
        "python",
        "rust",
        "swift",
        "text-to-speech",
        "tt",
        "tts",
        "web"
      ]
    },
    "content": {
      "readme": "# Supertonic â€” Lightning Fast, On-Device TTS\n\n[![Demo](https://img.shields.io/badge/ğŸ¤—%20Hugging%20Face-Demo-yellow)](https://huggingface.co/spaces/Supertone/supertonic#interactive-demo)\n[![Models](https://img.shields.io/badge/ğŸ¤—%20Hugging%20Face-Models-blue)](https://huggingface.co/Supertone/supertonic)\n\n<p align=\"center\">\n  <img src=\"img/Supertonic_IMG_v02_4x.webp\" alt=\"Supertonic Banner\">\n</p>\n\n**Supertonic** is a lightning-fast, on-device text-to-speech system designed for **extreme performance** with minimal computational overhead. Powered by ONNX Runtime, it runs entirely on your deviceâ€”no cloud, no API calls, no privacy concerns.\n\n## Demo\n\n### Raspberry Pi\n\nWatch Supertonic running on a **Raspberry Pi**, demonstrating on-device, real-time text-to-speech synthesis:\n\nhttps://github.com/user-attachments/assets/ea66f6d6-7bc5-4308-8a88-1ce3e07400d2\n\n### E-Reader\n\nExperience Supertonic on an **Onyx Boox Go 6** e-reader in airplane mode, achieving an average RTF of 0.3Ã— with zero network dependency:\n\nhttps://github.com/user-attachments/assets/64980e58-ad91-423a-9623-78c2ffc13680\n\n---\n\n> ğŸ§ **Try it now**: Experience Supertonic in your browser with our [**Interactive Demo**](https://huggingface.co/spaces/Supertone/supertonic#interactive-demo), or get started with pre-trained models from [**Hugging Face Hub**](https://huggingface.co/Supertone/supertonic)\n\n## ğŸ“° Update News\n\n**2025.11.24** - Added Flutter SDK support with macOS compatibility\n\n### Table of Contents\n\n- [Why Supertonic?](#why-supertonic)\n- [Language Support](#language-support)\n- [Getting Started](#getting-started)\n- [Performance](#performance)\n- [Citation](#citation)\n- [License](#license)\n\n## Why Supertonic?\n\n- **âš¡ Blazingly Fast**: Generates speech up to **167Ã— faster than real-time** on consumer hardware (M4 Pro)â€”unmatched by any other TTS system\n- **ğŸª¶ Ultra Lightweight**: Only **66M parameters**, optimized for efficient on-device performance with minimal footprint\n- **ğŸ“± On-Device Capable**: **Complete privacy** and **zero latency**â€”all processing happens locally on your device\n- **ğŸ¨ Natural Text Handling**: Seamlessly processes numbers, dates, currency, abbreviations, and complex expressions without pre-processing\n- **âš™ï¸ Highly Configurable**: Adjust inference steps, batch processing, and other parameters to match your specific needs\n- **ğŸ§© Flexible Deployment**: Deploy seamlessly across servers, browsers, and edge devices with multiple runtime backends.\n\n## Language Support\n\nWe provide ready-to-use TTS inference examples across multiple ecosystems:\n\n| Language/Platform | Path | Description |\n|-------------------|------|-------------|\n| [**Python**](py/) | `py/` | ONNX Runtime inference |\n| [**Node.js**](nodejs/) | `nodejs/` | Server-side JavaScript |\n| [**Browser**](web/) | `web/` | WebGPU/WASM inference |\n| [**Java**](java/) | `java/` | Cross-platform JVM |\n| [**C++**](cpp/) | `cpp/` | High-performance C++ |\n| [**C#**](csharp/) | `csharp/` | .NET ecosystem |\n| [**Go**](go/) | `go/` | Go implementation |\n| [**Swift**](swift/) | `swift/` | macOS applications |\n| [**iOS**](ios/) | `ios/` | Native iOS apps |\n| [**Rust**](rust/) | `rust/` | Memory-safe systems |\n| [**Flutter**](flutter/) | `flutter/` | Cross-platform apps |\n\n> For detailed usage instructions, please refer to the README.md in each language directory.\n\n## Getting Started\n\nFirst, clone the repository:\n\n```bash\ngit clone https://github.com/supertone-inc/supertonic.git\ncd supertonic\n```\n\n### Prerequisites\n\nBefore running the examples, download the ONNX models and preset voices, and place them in the `assets` directory:\n\n> **Note:** The Hugging Face repository uses Git LFS. Please ensure Git LFS is installed and initialized before cloning or pulling large model files.\n> - macOS: `brew install git-lfs && git lfs install`\n> - Generic: see `https://git-lfs.com` for installers\n\n```bash\ngit clone https://huggingface.co/Supertone/supertonic assets\n```\n\n### Quick Start\n\n**Python Example** ([Details](py/))\n```bash\ncd py\nuv sync\nuv run example_onnx.py\n```\n\n**Node.js Example** ([Details](nodejs/))\n```bash\ncd nodejs\nnpm install\nnpm start\n```\n\n**Browser Example** ([Details](web/))\n```bash\ncd web\nnpm install\nnpm run dev\n```\n\n**Java Example** ([Details](java/))\n```bash\ncd java\nmvn clean install\nmvn exec:java\n```\n\n**C++ Example** ([Details](cpp/))\n```bash\ncd cpp\nmkdir build && cd build\ncmake .. && cmake --build . --config Release\n./example_onnx\n```\n\n**C# Example** ([Details](csharp/))\n```bash\ncd csharp\ndotnet restore\ndotnet run\n```\n\n**Go Example** ([Details](go/))\n```bash\ncd go\ngo mod download\ngo run example_onnx.go helper.go\n```\n\n**Swift Example** ([Details](swift/))\n```bash\ncd swift\nswift build -c release\n.build/release/example_onnx\n```\n\n**Rust Example** ([Details](rust/))\n```bash\ncd rust\ncargo build --release\n./target/release/example_onnx\n```\n\n**iOS Example** ([Details](ios/))\n```bash\ncd ios/ExampleiOSApp\nxcodegen generate\nopen ExampleiOSApp.xcodeproj\n```\n- In Xcode: Targets â†’ ExampleiOSApp â†’ Signing: ",
      "default_branch": "main"
    },
    "fetched_at": "2025-12-06T02:26:20.482733"
  },
  {
    "basic_info": {
      "name": "xiaomi-miloco",
      "full_name": "XiaoMi/xiaomi-miloco",
      "owner": "XiaoMi",
      "description": "Xiaomi Miloco",
      "url": "https://github.com/XiaoMi/xiaomi-miloco",
      "clone_url": "https://github.com/XiaoMi/xiaomi-miloco.git",
      "ssh_url": "git@github.com:XiaoMi/xiaomi-miloco.git",
      "homepage": null,
      "created_at": "2025-11-06T13:01:59Z",
      "updated_at": "2025-12-05T17:18:07Z",
      "pushed_at": "2025-11-28T13:14:55Z"
    },
    "stats": {
      "stars": 1631,
      "forks": 104,
      "watchers": 1631,
      "open_issues": 63,
      "size": 23989
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 1050918,
        "JavaScript": 506494,
        "Shell": 90272,
        "C++": 83167,
        "Less": 71704,
        "HTML": 53752,
        "CSS": 9737,
        "Dockerfile": 5505,
        "C": 2948,
        "CMake": 1813
      },
      "license": "Other",
      "topics": []
    },
    "content": {
      "readme": "# Xiaomi Miloco\n\n**Xiaomi Local Copilot** is a future exploration solution for smart homes. Using Xiaomi Home cameras as the source of visual information and a self-developed LLM as its core, it connects all IoT devices throughout the house. Based on the development paradigm of LLM, it enables users to define various family needs and rules in natural language, achieving broader and more creative smart device integration.\n\n<div align=\"center\">\n\nEnglish | [ç®€ä½“ä¸­æ–‡](README.zh_Hans.md)\n\n</div>\n\n## News\n\n- [2025-11] Xiaomi Miloco Framework Open Source\n\n## Key Features\n\n1. New Interaction Paradigm: Based on the development paradigm of LLM, rule-setting and complex device command control can be completed through natural language interaction.\n2. New Use for Visual Data: Using camera data streams as a source of perceptual information, the LLM is used to analyze various home scene events contained in the visual data to respond to user queries.\n3. On-Device LLM: The home scene tasks are split into two stages: planning and visual understanding. It provides Xiaomi's self-developed on-device model to realize on-device video understanding and ensure family privacy and security.\n4. Xiaomi Home Ecosystem: It connects with the Xiaomi Home ecosystem, supports the retrieval and execution of Mi Home devices and scenes, and supports sending customized content for Xiao Home notifications.\n\n    <img src=\"assets/images/ai_center.jpg\" width=\"60%\" />\n\n## Quick Start\n\n### System Requirements\n\n- **Hardware Requirements**\n```Plain Text\nCPU: x64 architecture\nGraphics Card: NVIDIA 30 series and above, 8GB VRAM minimum (recommended 12GB and above)\nStorage: Recommended 16GB or more available space (for local model storage)\n```\n\n- **Software Requirements**\n```Plain Text\nOperating System:\n  - Linux: x64 architecture, recommended Ubuntu 22.04 and above LTS versions\n  - Windows: x64 architecture, recommended Windows 10 and above, requires WSL2 support\n  - macOS: Not currently supported\nDocker: Version 20.10 and above, requires docker compose support\nNVIDIA Driver: NVIDIA driver with CUDA support\nNVIDIA Container Toolkit: For Docker GPU support\n```\n\n### Install\n\n> **Note**: Please ensure your system meets the above hardware and software requirements. Windows systems need to enter the WSL environment.\n\n**Install with Docker**  \nOne-click installation via command line\n```bash\nbash -c \"$(wget -qO- https://xiaomi-miloco.cnbj1.mi-fds.com/xiaomi-miloco/install.sh)\"\n```\nOr download the source code first, then execute the one-click installation script:\n```bash\ngit clone https://github.com/XiaoMi/xiaomi-miloco.git\n\nbash scripts/install.sh\n```\nFor detailed installation steps, please refer to the [Docker Deployment Documentation](docs/environment-setup.md).\n\n**Install with source code**  \nFor source code installation steps, please refer to the [Development Guide](docs/development/developer-setup.md).\n\n## Usage Documentation\n\nPlease refer to the [Usage Documentation](docs/usage/README.md).\n\n## Contributing\n\nPlease refer to the [Contributing Guide](CONTRIBUTING.md).\n\n## License\n\nFor license details, please see [LICENSE.md](LICENSE.md).\n\n**Important Notice**: This project is limited to non-commercial use only. Without written authorization from Xiaomi Corporation, this project may not be used for developing applications, web services, or other forms of software.\n\n## Security Issues\n\nIf you discover potential security issues in this project, or believe you may have found a security issue, please notify the [Miloco Team](xiaomi-miloco@xiaomi.com) via our vulnerability reporting email. Please do not create public GitHub Issues.\n\n## Contact Us\n\n### Issue Reporting\n\nFor issue reporting, please participate through the following methods:\n- Submit a [GitHub Issue](https://github.com/XiaoMi/xiaomi-miloco/issues/new/)\n\n### Technical Discussion\n\n- GitHub [Discussions](https://github.com/XiaoMi/xiaomi-miloco/discussions/)\n- Project Discussion Group (WeChat):\n\n  <img src=\"assets/images/miloco_wechat_group_17.jpeg\" width=\"30%\" />\n  <img src=\"assets/images/miloco_wechat_15.jpeg\" width=\"30%\" />\n  <img src=\"assets/images/miloco_wechat_group_12.jpeg\" width=\"30%\" />\n\n\n\n### Join Us\n\nThe **Xiaomi Miloco** team is hiring. Send your resume to `xiaomi-miloco@xiaomi.com`, and it will be delivered directly to the project lead.\n\n## Acknowledgments\n\nThank you to the original team members who worked hard for Milocoï¼šzhaoyã€yangyongjieã€xxã€Changyuã€yykã€junhuiã€éƒ­å…´å®ã€47ã€afeiã€‚\n\nYour passion and talent are the fundamental driving force behind Miloco's continuous innovation and progress.\n\nSpecial thanks to:\n- The [llama.cpp](https://github.com/ggml-org/llama.cpp) open source project for providing inference backend capabilities\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-12-06T02:26:21.606618"
  },
  {
    "basic_info": {
      "name": "JiT",
      "full_name": "LTH14/JiT",
      "owner": "LTH14",
      "description": "PyTorch implementation of JiT https://arxiv.org/abs/2511.13720",
      "url": "https://github.com/LTH14/JiT",
      "clone_url": "https://github.com/LTH14/JiT.git",
      "ssh_url": "git@github.com:LTH14/JiT.git",
      "homepage": "",
      "created_at": "2025-11-10T22:37:40Z",
      "updated_at": "2025-12-05T23:16:24Z",
      "pushed_at": "2025-11-18T03:24:51Z"
    },
    "stats": {
      "stars": 1619,
      "forks": 88,
      "watchers": 1619,
      "open_issues": 22,
      "size": 67601
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 56577
      },
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "## Just image Transformer (JiT) for Pixel-space Diffusion\n\n[![arXiv](https://img.shields.io/badge/arXiv%20paper-2511.13720-b31b1b.svg)](https://arxiv.org/abs/2511.13720)&nbsp;\n\n<p align=\"center\">\n  <img src=\"demo/visual.jpg\" width=\"100%\">\n</p>\n\n\nThis is a PyTorch/GPU re-implementation of the paper [Back to Basics: Let Denoising Generative Models Denoise](https://arxiv.org/abs/2511.13720):\n\n```\n@article{li2025jit,\n  title={Back to Basics: Let Denoising Generative Models Denoise},\n  author={Li, Tianhong and He, Kaiming},\n  journal={arXiv preprint arXiv:2511.13720},\n  year={2025}\n}\n```\n\nJiT adopts a minimalist and self-contained design for pixel-level high-resolution image diffusion. \nThe original implementation was in JAX+TPU. This re-implementation is in PyTorch+GPU.\n\n<p align=\"center\">\n  <img src=\"demo/jit.jpg\" width=\"40%\">\n</p>\n\n### Dataset\nDownload [ImageNet](http://image-net.org/download) dataset, and place it in your `IMAGENET_PATH`.\n\n### Installation\n\nDownload the code:\n```\ngit clone https://github.com/LTH14/JiT.git\ncd JiT\n```\n\nA suitable [conda](https://conda.io/) environment named `jit` can be created and activated with:\n\n```\nconda env create -f environment.yaml\nconda activate jit\n```\n\nIf you get ```undefined symbol: iJIT_NotifyEvent``` when importing ```torch```, simply\n```\npip uninstall torch\npip install torch==2.5.1 --index-url https://download.pytorch.org/whl/cu124\n```\nCheck this [issue](https://github.com/conda/conda/issues/13812#issuecomment-2071445372) for more details.\n\n### Training\nThe below training scripts have been tested on 8 H200 GPUs.\n\nExample script for training JiT-B/16 on ImageNet 256x256 for 600 epochs:\n```\ntorchrun --nproc_per_node=8 --nnodes=1 --node_rank=0 \\\nmain_jit.py \\\n--model JiT-B/16 \\\n--proj_dropout 0.0 \\\n--P_mean -0.8 --P_std 0.8 \\\n--img_size 256 --noise_scale 1.0 \\\n--batch_size 128 --blr 5e-5 \\\n--epochs 600 --warmup_epochs 5 \\\n--gen_bsz 128 --num_images 50000 --cfg 2.9 --interval_min 0.1 --interval_max 1.0 \\\n--output_dir ${OUTPUT_DIR} --resume ${OUTPUT_DIR} \\\n--data_path ${IMAGENET_PATH} --online_eval\n```\n\nExample script for training JiT-B/32 on ImageNet 512x512 for 600 epochs:\n```\ntorchrun --nproc_per_node=8 --nnodes=1 --node_rank=0 \\\nmain_jit.py \\\n--model JiT-B/32 \\\n--proj_dropout 0.0 \\\n--P_mean -0.8 --P_std 0.8 \\\n--img_size 512 --noise_scale 2.0 \\\n--batch_size 128 --blr 5e-5 \\\n--epochs 600 --warmup_epochs 5 \\\n--gen_bsz 128 --num_images 50000 --cfg 2.9 --interval_min 0.1 --interval_max 1.0 \\\n--output_dir ${OUTPUT_DIR} --resume ${OUTPUT_DIR} \\\n--data_path ${IMAGENET_PATH} --online_eval\n```\n\nExample script for training JiT-H/16 on ImageNet 256x256 for 600 epochs:\n```\ntorchrun --nproc_per_node=8 --nnodes=1 --node_rank=0 \\\nmain_jit.py \\\n--model JiT-H/16 \\\n--proj_dropout 0.2 \\\n--P_mean -0.8 --P_std 0.8 \\\n--img_size 256 --noise_scale 1.0 \\\n--batch_size 128 --blr 5e-5 \\\n--epochs 600 --warmup_epochs 5 \\\n--gen_bsz 128 --num_images 50000 --cfg 2.2 --interval_min 0.1 --interval_max 1.0 \\\n--output_dir ${OUTPUT_DIR} --resume ${OUTPUT_DIR} \\\n--data_path ${IMAGENET_PATH} --online_eval\n```\n\n### Evaluation\n\nEvaluate a trained JiT:\n```\ntorchrun --nproc_per_node=8 --nnodes=1 --node_rank=0 \\\nmain_jit.py \\\n--model JiT-B/16 \\\n--img_size 256 --noise_scale 1.0 \\\n--gen_bsz 128 --num_images 50000 --cfg 2.9 --interval_min 0.1 --interval_max 1.0 \\\n--output_dir ${CKPT_DIR} --resume ${CKPT_DIR} \\\n--data_path ${IMAGENET_PATH} --evaluate_gen\n```\n\nWe use a customized [```torch-fidelity```](https://github.com/LTH14/torch-fidelity)\nto evaluate FID and IS against a reference image folder or statistics. You can use ```prepare_ref.py```\nto prepare the reference image folder, or directly use our pre-computed reference stats\nunder ```fid_stats```.\n\n### Acknowledgements\n\nWe thank Google TPU Research Cloud (TRC) for granting us access to TPUs, and the MIT\nORCD Seed Fund Grants for supporting GPU resources.\n\n### Contact\n\nIf you have any questions, feel free to contact me through email (tianhong@mit.edu). Enjoy!\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-12-06T02:26:22.731456"
  },
  {
    "basic_info": {
      "name": "ClashMac",
      "full_name": "666OS/ClashMac",
      "owner": "666OS",
      "description": "åŸºäº Clash (mihomo) å†…æ ¸çš„è½»é‡çº§ macOS èœå•æ å®¢æˆ·ç«¯ã€‚Lightweight macOS Clash (mihomo) menu bar client.",
      "url": "https://github.com/666OS/ClashMac",
      "clone_url": "https://github.com/666OS/ClashMac.git",
      "ssh_url": "git@github.com:666OS/ClashMac.git",
      "homepage": "https://github.com/666OS/ClashMac",
      "created_at": "2025-11-28T04:41:02Z",
      "updated_at": "2025-12-06T02:15:06Z",
      "pushed_at": "2025-12-05T11:58:42Z"
    },
    "stats": {
      "stars": 1562,
      "forks": 62,
      "watchers": 1562,
      "open_issues": 18,
      "size": 4023
    },
    "tech_info": {
      "language": null,
      "languages": {},
      "license": "Other",
      "topics": [
        "clash",
        "clash-meta",
        "macos",
        "mihomo"
      ]
    },
    "content": {
      "readme": "<div align=\"center\">\n\n**Languages:** [English](README.md) | [ç®€ä½“ä¸­æ–‡](README.zh-CN.md)\n\n</div>\n\n---\n\n## âš ï¸ Important Notice\n\nThe main project is currently not open source. This repository is primarily for publishing releases and collecting feedback.  \nAll third-party open-source components used in this application have their licenses publicly disclosed as required.\n\nWe will evaluate whether to open more content in the future based on project progress.  \nThank you for your understanding and support! For more details, please refer to the [Pinned Announcement](https://github.com/666OS/ClashMac/issues/15)\n\n<br>\n<p align=\"center\">\n  <img src=\"assets/clashmac-logo.png\" alt=\"ClashMac Screenshot\" width=\"300\" style=\"filter: drop-shadow(0 4px 12px rgba(0, 0, 0, 0.15));\">\n</p>\n<h1 align=\"center\">ClashMac</h1>\n<h3 align=\"center\" style=\"margin-top: 0; margin-bottom: 20px;\">Lightweight macOS Clash Menu Bar Client</h3>\n\n<p align=\"center\" style=\"margin-top: 0; margin-bottom: 50px;\">\n  <a href=\"https://github.com/666OS/ClashMac/releases/latest\">\n    <img src=\"https://img.shields.io/github/v/release/666OS/ClashMac?style=flat-square&logo=github&color=green\" alt=\"Latest Release\">\n  </a>\n  <a href=\"https://github.com/666OS/ClashMac/releases\">\n    <img src=\"https://img.shields.io/github/downloads/666OS/ClashMac/total?style=flat-square&logo=dropbox&logoColor=white&color=gold\" alt=\"Downloads\">\n  </a>\n  <a href=\"https://t.me/Pinched666\">\n    <img src=\"https://img.shields.io/badge/Telegram-Channel-blue?style=flat-square&logo=telegram\">\n  </a>\n</p>\n\n<p align=\"center\">\n  <img src=\"assets/screenshot.png\" alt=\"ClashMac Screenshot\" width=\"400\">\n</p>\n\n## Features\n\n- **Native Application** - Built with SwiftUI, seamlessly integrated with the system\n- **Lightweight & Efficient** - Menu bar app with minimal resource usage\n- **Network Takeover** - One-click enable/disable system proxy & enhanced mode\n- **Privileged Helper** - Password-free management of system proxy and kernel\n- **Real-time Traffic Monitoring** - SSE push with millisecond-level updates\n- **Visualization Panel** - Traffic statistics, connections count, memory usage\n- **Web Dashboard** - Integrated control panel access\n- **In-App Updates** - Auto update functionality\n- **Subscription Management** - Auto-update subscription sources\n- **Configuration Switching** - Multiple config files support\n\n## Download\n\nDownload the latest version from the [Releases page](https://github.com/666OS/ClashMac/releases/latest):\n\n- **Apple Silicon (M1/M2/M3/M4)**: `ClashMac-v*-macos-arm64.zip`\n- **Intel Mac**: `ClashMac-v*-macos-x86_64.zip`\n\n**Compatible Configuration**: Please refer to [Test Configurations](https://github.com/666OS/YYDS/tree/main/mihomo/config)\n\n**Installation Steps**:\n1. Unzip the downloaded zip file\n2. Drag `ClashMac.app` to the \"Applications\" folder\n3. On first launch, right-click and select \"Open\" (to bypass security check)\n\n**Tip**: Not sure which Mac you have? Click the  menu at the top left â†’ About This Mac, and check the \"Chip\" information.\n\n> **Note: Mac Gatekeeper may block unsigned applications**  \n> Since ClashMac is not Apple notarized, macOS will not allow it to open directly by default.\n\n### Solutions\n\n#### Method 1: Allow in System Settings\n1. Try to open ClashMac, click \"Done\" when the security warning appears\n2. Open **System Settings** â†’ **Privacy & Security**\n3. Scroll down and find the message: \"ClashMac was blocked from opening\"\n4. Click \"Open Anyway\" next to it\n5. Click \"Open Anyway\" again in the popup dialog\n\n#### Method 2: Remove Restrictions via Terminal\nEnter in Terminal:\n\n```bash\nxattr -cr /Applications/ClashMac.app\n```\nPress Enter and reopen the application\n\n\n#### Method 3: Remove Quarantine Attribute\n\nEnter in Terminal:\n```bash\nxattr -d com.apple.quarantine /Applications/ClashMac.app\n```\nPress Enter and reopen the application\n\n## License\n\nClashMac is a proprietary, closed-source application.  \nOnly binary releases are provided in this repository.\n\nThis project uses third-party open-source components.  \nThe full list of licenses is available at:\n\n[THIRD_PARTY_LICENSES](https://github.com/666OS/ClashMac/blob/main/THIRD_PARTY_LICENSES.txt) \n\n## Acknowledgments\n\n- [mihomo](https://github.com/MetaCubeX/mihomo)\n- [Vernesong](https://github.com/vernesong/mihomo)\n- [Zashboard](https://github.com/Zephyruso/zashboard)\n\n## Star History\n[![Star History Chart](https://api.star-history.com/svg?repos=666OS/ClashMac&type=Date)](https://star-history.com/#666OS/ClashMac&Date)\n\n---\n\n<p align=\"center\">\n  Made with â¤ï¸ for macOS\n</p>\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-12-06T02:26:23.846748"
  },
  {
    "basic_info": {
      "name": "HunyuanVideo-1.5",
      "full_name": "Tencent-Hunyuan/HunyuanVideo-1.5",
      "owner": "Tencent-Hunyuan",
      "description": "HunyuanVideo-1.5: A leading lightweight video generation model",
      "url": "https://github.com/Tencent-Hunyuan/HunyuanVideo-1.5",
      "clone_url": "https://github.com/Tencent-Hunyuan/HunyuanVideo-1.5.git",
      "ssh_url": "git@github.com:Tencent-Hunyuan/HunyuanVideo-1.5.git",
      "homepage": "https://hunyuan.tencent.com/video/zh?tabIndex=0",
      "created_at": "2025-11-20T06:15:42Z",
      "updated_at": "2025-12-06T02:26:01Z",
      "pushed_at": "2025-12-05T16:37:01Z"
    },
    "stats": {
      "stars": 1543,
      "forks": 74,
      "watchers": 1543,
      "open_issues": 22,
      "size": 448
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 482314
      },
      "license": "Other",
      "topics": [
        "image-to-video",
        "text-to-video",
        "video-generation"
      ]
    },
    "content": {
      "readme": "[ä¸­æ–‡æ–‡æ¡£](./README_CN.md)\n\n# HunyuanVideo-1.5\n\n<div align=\"center\">\n\n<img src=\"./assets/logo.png\" alt=\"HunyuanVideo-1.5 Logo\" width=\"80%\">\n\n# ğŸ¬ HunyuanVideo-1.5: A leading lightweight video generation model\n\n</div>\n\n\n<div align=\"center\">\n<!-- <img src=\"./assets/banner.png\" alt=\"HunyuanVideo-1.5 Banner\" width=\"800\"> -->\n\n</div>\n\n\nHunyuanVideo-1.5 is a video generation model that delivers top-tier quality with only 8.3B parameters, significantly lowering the barrier to usage. It runs smoothly on consumer-grade GPUs, making it accessible for every developer and creator. This repository provides the implementation and tools needed to generate creative videos.\n\n\n<div align=\"center\">\n  <a href=\"https://hunyuan.tencent.com/video/zh?tabIndex=0\" target=\"_blank\"><img src=https://img.shields.io/badge/Official%20Site-333399.svg?logo=homepage height=22px></a>\n  <a href=https://huggingface.co/tencent/HunyuanVideo-1.5 target=\"_blank\"><img src=https://img.shields.io/badge/%F0%9F%A4%97%20Models-d96902.svg height=22px></a>\n  <a href=https://github.com/Tencent-Hunyuan/HunyuanVideo-1.5 target=\"_blank\"><img src= https://img.shields.io/badge/Page-bb8a2e.svg?logo=github height=22px></a>\n  <a href=\"https://arxiv.org/pdf/2511.18870\" target=\"_blank\"><img src=https://img.shields.io/badge/Report-b5212f.svg?logo=arxiv height=22px></a>\n  <a href=https://x.com/TencentHunyuan target=\"_blank\"><img src=https://img.shields.io/badge/Hunyuan-black.svg?logo=x height=22px></a>\n  <a href=\"https://github.com/Tencent-Hunyuan/HunyuanVideo-1.5/blob/main/assets/HunyuanVideo_1_5_Prompt_Handbook_EN.md\" target=\"_blank\"><img src=https://img.shields.io/badge/ğŸ“š-PromptHandBook-blue.svg?logo=book height=22px></a> <br/>\n  <a href=\"./ComfyUI/README.md\" target=\"_blank\"><img src=https://img.shields.io/badge/ComfyUI-blue.svg?logo=book height=22px></a>\n  <a href=\"https://github.com/ModelTC/LightX2V\" target=\"_blank\"><img src=https://img.shields.io/badge/LightX2V-yellow.svg?logo=book height=22px></a>\n  <a href=\"https://tusi.cn/models/933574988890423836\" target=\"_blank\"><img src=https://img.shields.io/badge/åå¸-purple.svg?logo=book height=22px></a>\n  <a href=\"https://tensor.art/models/933574988890423836\" target=\"_blank\"><img src=https://img.shields.io/badge/TensorArt-cyan.svg?logo=book height=22px></a>\n\n</div>\n\n\n<p align=\"center\">\n    ğŸ‘ Join our <a href=\"./assets/wechat.png\" target=\"_blank\">WeChat</a> and <a href=\"https://discord.gg/ehjWMqF5wY\">Discord</a> | \nğŸ’» <a href=\"https://hunyuan.tencent.com/video/zh?tabIndex=0\">Official website Try our model!</a>&nbsp&nbsp\n</p>\n\n## ğŸ”¥ğŸ”¥ğŸ”¥ News\n* ğŸš€ Dec 05, 2025: **New Release**: We now release the [480p I2V step-distilled model](https://huggingface.co/tencent/HunyuanVideo-1.5/tree/main/transformer/480p_i2v_step_distilled), which generates videos in 8 or 12 steps (recommended)! On RTX 4090, end-to-end generation time is reduced by 75%, and a single RTX 4090 can generate videos within **75 seconds**. The step-distilled model maintains comparable quality to the original model while achieving significant speedup. See [Step Distillation Comparison](./assets/step_distillation_comparison.md) for detailed quality comparisons. For even faster generation, you can also try 4 steps (faster speed with slightly reduced quality). **To enable the step-distilled model, run `generate.py` with the `--enable_step_distill` parameter.** See [Usage](#-usage) for detailed usage instructions. ğŸ”¥ğŸ”¥ğŸ”¥ğŸ†•\n* ğŸ“š Dec 05, 2025: **Training Code Released**: We now open-source the training code for HunyuanVideo-1.5! The training script (`train.py`) provides a full training pipeline with support for distributed training, FSDP, context parallel, gradient checkpointing, and more. HunyuanVideo-1.5 is trained using the Muon optimizer, which we have open-sourced in the [Training](#-training) section. **If you would like to continue training our model or fine-tune it with LoRA, please use the Muon optimizer.** See [Training](#-training) section for detailed usage instructions. ğŸ”¥ğŸ”¥ğŸ”¥ğŸ†•\n* ğŸ‰ **Diffusers Support**: HunyuanVideo-1.5 is now available on Hugging Face Diffusers! Check out [Diffusers collection](https://huggingface.co/collections/hunyuanvideo-community/hunyuanvideo-15) for easy integration. ğŸ”¥ğŸ”¥ğŸ”¥ğŸ†•\n* ğŸš€ Nov 27, 2025: We now support cache inference (deepcache, teacache, taylorcache), achieving significant speedup! Pull the latest code to try it. ğŸ”¥ğŸ”¥ğŸ”¥ğŸ†• \n* ğŸš€ Nov 24, 2025: We now support deepcache inference.\n* ğŸ‘‹ Nov 20, 2025: We release the inference code and model weights of HunyuanVideo-1.5.\n\n\n## ğŸ¥ Demo\n<div align=\"center\">\n  <video src=\"https://github.com/user-attachments/assets/d45ec78e-ea40-47f1-8d4d-f4d9a0682e2d\" width=\"60%\"> </video>\n</div>\n\n## ğŸ§© Community Contributions\n\nIf you develop/use HunyuanVideo-1.5 in your projects, welcome to let us know.\n\n- **Diffusers** - [HunyuanVideo-1.5 Diffusers](https://huggingface.co/collections/hunyuanvideo-community/hunyuanvideo-15): Official Hugging Face Diffusers integration for HunyuanVideo-1.5. Easily use HunyuanVideo-1.5 with the Diffusers",
      "default_branch": "main"
    },
    "fetched_at": "2025-12-06T02:26:24.984925"
  },
  {
    "basic_info": {
      "name": "moss-kernel",
      "full_name": "hexagonal-sun/moss-kernel",
      "owner": "hexagonal-sun",
      "description": "Rust Linux-compatible kernel",
      "url": "https://github.com/hexagonal-sun/moss-kernel",
      "clone_url": "https://github.com/hexagonal-sun/moss-kernel.git",
      "ssh_url": "git@github.com:hexagonal-sun/moss-kernel.git",
      "homepage": null,
      "created_at": "2025-11-20T14:54:10Z",
      "updated_at": "2025-12-05T23:16:52Z",
      "pushed_at": "2025-12-01T19:55:42Z"
    },
    "stats": {
      "stars": 1532,
      "forks": 55,
      "watchers": 1532,
      "open_issues": 10,
      "size": 1194
    },
    "tech_info": {
      "language": "Rust",
      "languages": {
        "Rust": 848710,
        "Assembly": 7733,
        "Shell": 5776,
        "Linker Script": 1161,
        "RenderScript": 1
      },
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "# moss\n\n![Architecture](https://img.shields.io/badge/arch-aarch64-blue)\n![Language](https://img.shields.io/badge/language-Rust-orange)\n![License](https://img.shields.io/badge/license-MIT-yellow)\n\n![Moss Boot Demo](etc/moss_demo.gif)\n\n**moss** is a Unix-like, Linux-compatible kernel written in Rust and Aarch64\nassembly.\n\nIt features a modern, asynchronous core, a modular architecture abstraction\nlayer, and binary compatibility with Linux userspace applications (currently\ncapable of running most BusyBox commands).\n\n## Features\n\n### Architecture & Memory\n* Full support for aarch64.\n* A well-defined HAL allowing for easy porting to other architectures (e.g.,\n  x86_64, RISC-V).\n* Memory Management:\n    * Full MMU enablement and page table management.\n    * Copy-on-Write (CoW) pages.\n    * Safe copy to/from userspace async functions.\n    * Kernel and userspace page fault management.\n    * Buddy allocator for physical addresses and `smalloc` for boot allocations\n      and tracking memory reservations.\n\n### Async Core\nOne of the defining features of `moss` is its usage of Rust's `async/await`\nmodel within the kernel context:\n* All non-trivial system calls are written as `async` functions, sleep-able\n  functions are prefixed with `.await`.\n* The compiler enforces that spinlocks cannot be held over sleep points,\n  eliminating a common class of kernel deadlocks.\n\n###  Process Management\n* Full task management including scheduling and task migration via IPIs.\n* Currently implements [51 Linux syscalls](./etc/syscalls_linux_aarch64.md); sufficient to execute most BusyBox\n  commands.\n* Advanced forking capabilities via `clone()`.\n* Process and thread signal delivery and raising support.\n\n### VFS & Filesystems\n* Virtual File System with full async abstractions.\n* Drivers:\n    * Ramdisk block device implementation.\n    * FAT32 filesystem driver (ro).\n    * `devtmpfs` driver for kernel character device access.\n\n## `libkernel` & Testing\n`moss` is built on top of `libkernel`, a utility library designed to be\narchitecture-agnostic. This allows logic to be tested on a host machine (e.g.,\nx86) before running on bare metal.\n\n* Address Types: Strong typing for `VA` (Virtual), `PA` (Physical), and `UA`\n  (User) addresses.\n* Containers: `VMA` management, generic page-based ring buffer (`kbuf`), and\n  waker sets.\n* Sync Primitives: `spinlock`, `mutex`, `condvar`, `per_cpu`.\n* Test Suite: A comprehensive suite of 230+ tests ensuring functionality across\n  architectures (e.g., validating Aarch64 page table parsing logic on an x86\n  host).\n\n## Building and Running\n\n### Prerequisites\nYou will need QEMU for aarch64 emulation and dosfstools to create the virtual file system.\n\n```bash\nsudo apt install qemu-system-aarch64 dosfstools\n```\n\nAdditionally you will need a version of the [aarch64-none-elf](https://developer.arm.com/Tools%20and%20Software/GNU%20Toolchain) toolchain installed.\n\n#### Any OS\nTo install aarch64-none-elf on any os, download the correct release of `aarch64-none-elf` onto your computer, unpack it, then export the `bin` folder to path.\n\n#### macOS\nThere is experimental support for macOS in the scripts/mac-experimental folder. The scripts in there are not guaranteed to work for all macOS users and has only been tested on an M4 Apple Silicon MacBook Air.\n\n#### NixOS\n\nRun the following command\n\n```bash\nnix shell nixpkgs#pkgsCross.aarch64-embedded.stdenv.cc nixpkgs#pkgsCross.aarch64-embedded.stdenv.cc.bintools\n```\n\n### Preparing the image\n\nFirst, run the following script to prepare the binaries for the image:\n```bash\n./scripts/build-deps.sh\n```\n\nThis will download and build the necessary dependencies for the kernel and put them\ninto the `build` directory.\n\nOnce that is done, you can create the image using the following command:\n```bash\nsudo ./scripts/create-image.sh\n```\n\nThis will create an image file named `moss.img` in the root directory of the project,\nformat it as VFAT 32 and create the necessary files and directories for the kernel.\n\nThis script needs to run with sudo to mount the image through a loop device,\nwhich is required to properly create the image for the kernel to work.\n\n### Running via QEMU\n\nTo build the kernel and launch it in QEMU:\n\n``` bash\ncargo run --release\n```\n\n\n### Running the Test Suite\nBecause `libkernel` is architecturally decoupled, you can run the logic tests on\nyour host machine:\n\n``` bash\ncargo test -p libkernel --target x86_64-unknown-linux-gnu\n```\n\n\n### Roadmap & Status\n\nmoss is under active development. Current focus areas include:\n\n* Basic Linux Syscall Compatibility (Testing through BusyBox).\n* Networking Stack: TCP/IP implementation.\n* Scheduler Improvements: Task load balancing.\n* A fully read/write capable filesystem driver (e.g., ext2/4).\n* Expanding coverage beyond the current 49 calls.\n\n## Contributing\n\nContributions are welcome! Whether you are interested in writing a driver,\nporting to x86, or adding syscalls.\n\n## License\nDistributed under the MIT License. See LICENSE for more informati",
      "default_branch": "master"
    },
    "fetched_at": "2025-12-06T02:26:26.104891"
  }
]