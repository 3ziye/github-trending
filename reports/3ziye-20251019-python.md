## ğŸ“ çƒ­ç‚¹é¡¹ç›®-20251019-python

<div style="background-color: #f5f5f5; padding: 15px; border-radius: 8px; margin: 20px 0;">
- ğŸ¤– æœ¬æŠ¥å‘ŠåŸºäº GitHub API è‡ªåŠ¨ç”Ÿæˆ
- ğŸ“… æ•°æ®æ›´æ–°æ—¶é—´: 2025-10-19 10:43:03 (ä¸Šæµ·æ—¶åŒº)
- ğŸŒŸ æ˜Ÿæ ‡æ•°ç­‰ç»Ÿè®¡ä¿¡æ¯ä¸ºç”Ÿæˆæ—¶çš„å®æ—¶æ•°æ®
- ğŸ“š é¡¹ç›®ä¿¡æ¯æ¥æºäºå„é¡¹ç›®çš„ README æ–‡æ¡£
- ğŸ’¡ çƒ­åº¦æŒ‡æ•°è®¡ç®—æ–¹å¼: æ˜Ÿæ ‡æ•° + Forkæ•° Ã— 0.5
- æœ¬æŠ¥å‘Šç”± ä¸‰å­å¶å¼€æº github-trendingé¡¹ç›®åˆ†æå·¥å…·è‡ªåŠ¨ç”Ÿæˆ
</div>

## ğŸ”— ç›¸å…³é“¾æ¥

- [GitHub API æ–‡æ¡£](https://docs.github.com/en/rest)
- [é¡¹ç›®æ•°æ®è·å–å™¨æºç ](https://github.com/3ziye/github-trending)

---

# çƒ­ç‚¹é¡¹ç›®-20251019-python

<div align="center">
ğŸ“Š <strong>ç”Ÿæˆæ—¶é—´</strong>: 2025å¹´10æœˆ19æ—¥ 10:43  â€¢  
ğŸ¯ <strong>é¡¹ç›®æ•°é‡</strong>: 20 ä¸ª  â€¢  
â±ï¸ <strong>çƒ­åº¦æ—¶é—´</strong>: æœˆæ¦œ  â€¢  
ğŸ”¥ <strong>æ•°æ®æ¥æº</strong>: GitHub API
</div>

---

## ğŸš€ çƒ­é—¨é¡¹ç›®è¯¦æƒ…

### 1. nanochat

**ğŸ”¥ çƒ­åº¦æŒ‡æ•°**: 27,479

**é¡¹ç›®åç§°**: [karpathy/nanochat](https://github.com/karpathy/nanochat)
**é¡¹ç›®æè¿°**: The best ChatGPT that $100 can buy.
**ä¸»è¦è¯­è¨€**: Python
**è®¸å¯è¯**: None
**â­ æ˜Ÿæ ‡æ•°**: 26,157
**ğŸ´ Fork æ•°**: 2,645
**ğŸ‘€ å…³æ³¨æ•°**: 26,157
**ğŸ› å¼€æ”¾é—®é¢˜**: 74
**æœ€åæ›´æ–°**: 2025-10-19 (0åˆ†é’Ÿå‰)
**ğŸ“– é¡¹ç›®æ–‡æ¡£**: [README](3ziye-20251019-python/1-nanochat-Readme.md)

**ğŸ  é¡¹ç›®ä¸»é¡µ**: æ— 

**ğŸ“‚ å…‹éš†åœ°å€**: `https://github.com/karpathy/nanochat.git`

**ğŸ’» æŠ€æœ¯æ ˆ**: **ä¸»è¦è¯­è¨€**: Python

#### ğŸ¨ é€‚ç”¨åœºæ™¯
- æ•°æ®ç§‘å­¦ä¸åˆ†æ
- æœºå™¨å­¦ä¹ é¡¹ç›®
- Web åº”ç”¨å¼€å‘

---

### 2. skills

**ğŸ”¥ çƒ­åº¦æŒ‡æ•°**: 5,271

**é¡¹ç›®åç§°**: [anthropics/skills](https://github.com/anthropics/skills)
**é¡¹ç›®æè¿°**: Public repository for Skills
**ä¸»è¦è¯­è¨€**: Python
**è®¸å¯è¯**: None
**â­ æ˜Ÿæ ‡æ•°**: 5,077
**ğŸ´ Fork æ•°**: 388
**ğŸ‘€ å…³æ³¨æ•°**: 5,077
**ğŸ› å¼€æ”¾é—®é¢˜**: 18
**æœ€åæ›´æ–°**: 2025-10-19 (0åˆ†é’Ÿå‰)
**ğŸ“– é¡¹ç›®æ–‡æ¡£**: [README](3ziye-20251019-python/2-skills-Readme.md)

**ğŸ  é¡¹ç›®ä¸»é¡µ**: æ— 

**ğŸ“‚ å…‹éš†åœ°å€**: `https://github.com/anthropics/skills.git`

**ğŸ’» æŠ€æœ¯æ ˆ**: **ä¸»è¦è¯­è¨€**: Python

#### ğŸ¯ æ ¸å¿ƒç‰¹æ€§
- [What are skills?](https://support.claude.com/en/articles/12512176-what-are-skills)
- [Using skills in Claude](https://support.claude.com/en/articles/12512180-using-skills-in-claude)
- [How to create custom skills](https://support.claude.com/en/articles/12512198-creating-custom-skills)
- [Equipping agents for the real world with Agent Skills](https://anthropic.com/engineering/equipping-agents-for-the-real-world-with-agent-skills)
**algorithmic-art** - Create generative art using p5.js with seeded randomness, flow fields, and particle systems
**canvas-design** - Design beautiful visual art in .png and .pdf formats using design philosophies
**slack-gif-creator** - Create animated GIFs optimized for Slack's size constraints
**artifacts-builder** - Build comp

#### ğŸ¨ é€‚ç”¨åœºæ™¯
- æ•°æ®ç§‘å­¦ä¸åˆ†æ
- æœºå™¨å­¦ä¹ é¡¹ç›®
- Web åº”ç”¨å¼€å‘

---

### 3. TinyRecursiveModels

**ğŸ”¥ çƒ­åº¦æŒ‡æ•°**: 5,005

**é¡¹ç›®åç§°**: [SamsungSAILMontreal/TinyRecursiveModels](https://github.com/SamsungSAILMontreal/TinyRecursiveModels)
**é¡¹ç›®æè¿°**: æš‚æ— æè¿°
**ä¸»è¦è¯­è¨€**: Python
**è®¸å¯è¯**: MIT License
**â­ æ˜Ÿæ ‡æ•°**: 4,702
**ğŸ´ Fork æ•°**: 607
**ğŸ‘€ å…³æ³¨æ•°**: 4,702
**ğŸ› å¼€æ”¾é—®é¢˜**: 20
**æœ€åæ›´æ–°**: 2025-10-19 (5åˆ†é’Ÿå‰)
**ğŸ“– é¡¹ç›®æ–‡æ¡£**: [README](3ziye-20251019-python/3-TinyRecursiveModels-Readme.md)

**ğŸ  é¡¹ç›®ä¸»é¡µ**: æ— 

**ğŸ“‚ å…‹éš†åœ°å€**: `https://github.com/SamsungSAILMontreal/TinyRecursiveModels.git`

**ğŸ’» æŠ€æœ¯æ ˆ**: **ä¸»è¦è¯­è¨€**: Python

#### ğŸ¯ æ ¸å¿ƒç‰¹æ€§
- Python 3.10 (or similar)
- Cuda 12.6.0 (or similar)

#### ğŸ¨ é€‚ç”¨åœºæ™¯
- æ•°æ®ç§‘å­¦ä¸åˆ†æ
- æœºå™¨å­¦ä¹ é¡¹ç›®
- Web åº”ç”¨å¼€å‘

---

### 4. neutts-air

**ğŸ”¥ çƒ­åº¦æŒ‡æ•°**: 3,502

**é¡¹ç›®åç§°**: [neuphonic/neutts-air](https://github.com/neuphonic/neutts-air)
**é¡¹ç›®æè¿°**: On-device TTS model by Neuphonic
**ä¸»è¦è¯­è¨€**: Python
**è®¸å¯è¯**: Apache License 2.0
**â­ æ˜Ÿæ ‡æ•°**: 3,352
**ğŸ´ Fork æ•°**: 301
**ğŸ‘€ å…³æ³¨æ•°**: 3,352
**ğŸ› å¼€æ”¾é—®é¢˜**: 25
**æœ€åæ›´æ–°**: 2025-10-19 (34åˆ†é’Ÿå‰)
**ğŸ“– é¡¹ç›®æ–‡æ¡£**: [README](3ziye-20251019-python/4-neutts-air-Readme.md)

**ğŸ  é¡¹ç›®ä¸»é¡µ**: æ— 

**ğŸ“‚ å…‹éš†åœ°å€**: `https://github.com/neuphonic/neutts-air.git`

**ğŸ’» æŠ€æœ¯æ ˆ**: **ä¸»è¦è¯­è¨€**: Python

#### ğŸ¯ æ ¸å¿ƒç‰¹æ€§
- ğŸ—£Best-in-class realism for its size - produces natural, ultra-realistic voices that sound human
- ğŸ“±Optimised for on-device deployment - provided in GGML format, ready to run on phones, laptops, or even Raspberry Pis
- ğŸ‘«Instant voice cloning - create your own speaker with as little as 3 seconds of audio
- ğŸš„Simple LM + codec architecture built off a 0.5B backbone - the sweet spot between speed, size, and quality for real-world applications
**Supported Languages**: English
**Audio Codec**: [NeuCodec](https://huggingface.co/neuphonic/neucodec) - our 50hz neural audio codec that achieves exceptional audio quality at low bitrates using a single codebook
**Context Window**: 2048 tokens, enough for processing ~30 seconds of audio (including prompt duration)
**Format**: Available in GGML format for efficient on-device inference

#### ğŸ¨ é€‚ç”¨åœºæ™¯
- æ•°æ®ç§‘å­¦ä¸åˆ†æ
- æœºå™¨å­¦ä¹ é¡¹ç›®
- Web åº”ç”¨å¼€å‘

---

### 5. bdh

**ğŸ”¥ çƒ­åº¦æŒ‡æ•°**: 3,147

**é¡¹ç›®åç§°**: [pathwaycom/bdh](https://github.com/pathwaycom/bdh)
**é¡¹ç›®æè¿°**: Baby Dragon Hatchling (BDH) â€“ Architecture and Code
**ä¸»è¦è¯­è¨€**: Python
**è®¸å¯è¯**: MIT License
**â­ æ˜Ÿæ ‡æ•°**: 3,093
**ğŸ´ Fork æ•°**: 109
**ğŸ‘€ å…³æ³¨æ•°**: 3,093
**ğŸ› å¼€æ”¾é—®é¢˜**: 2
**æœ€åæ›´æ–°**: 2025-10-19 (2å°æ—¶å‰)
**ğŸ“– é¡¹ç›®æ–‡æ¡£**: [README](3ziye-20251019-python/5-bdh-Readme.md)

**ğŸ  é¡¹ç›®ä¸»é¡µ**: æ— 

**ğŸ“‚ å…‹éš†åœ°å€**: `https://github.com/pathwaycom/bdh.git`

**ğŸ’» æŠ€æœ¯æ ˆ**: **ä¸»è¦è¯­è¨€**: Python

#### ğŸ¯ æ ¸å¿ƒç‰¹æ€§
**Scale-free network topology** mimicking biological connectivity
**Locally interacting neuron particles** with excitatory/inhibitory dynamics
**Hebbian working memory** based on synaptic plasticity, displaying monosemanticity
**GPU-friendly state-space formulation** for efficient implementation
**Interpretable activations** that are sparse and positive
- Read a

#### ğŸ¨ é€‚ç”¨åœºæ™¯
- æ•°æ®ç§‘å­¦ä¸åˆ†æ
- æœºå™¨å­¦ä¹ é¡¹ç›®
- Web åº”ç”¨å¼€å‘

---

### 6. HunyuanImage-3.0

**ğŸ”¥ çƒ­åº¦æŒ‡æ•°**: 2,276

**é¡¹ç›®åç§°**: [Tencent-Hunyuan/HunyuanImage-3.0](https://github.com/Tencent-Hunyuan/HunyuanImage-3.0)
**é¡¹ç›®æè¿°**: HunyuanImage-3.0: A Powerful Native Multimodal Model for Image Generation
**ä¸»è¦è¯­è¨€**: Python
**è®¸å¯è¯**: Other
**â­ æ˜Ÿæ ‡æ•°**: 2,231
**ğŸ´ Fork æ•°**: 91
**ğŸ‘€ å…³æ³¨æ•°**: 2,231
**ğŸ› å¼€æ”¾é—®é¢˜**: 25
**æœ€åæ›´æ–°**: 2025-10-19 (3å°æ—¶å‰)
**ğŸ“– é¡¹ç›®æ–‡æ¡£**: [README](3ziye-20251019-python/6-HunyuanImage-3.0-Readme.md)

**ğŸ  é¡¹ç›®ä¸»é¡µ**: https://hunyuan.tencent.com/image

**ğŸ“‚ å…‹éš†åœ°å€**: `https://github.com/Tencent-Hunyuan/HunyuanImage-3.0.git`

**ğŸ’» æŠ€æœ¯æ ˆ**: **ä¸»è¦è¯­è¨€**: Python

#### ğŸ·ï¸ é¡¹ç›®æ ‡ç­¾
- `image-generation`
- `native-multimodal-model`

#### ğŸ¯ æ ¸å¿ƒç‰¹æ€§
**September 28, 2025**: ğŸ“– **HunyuanImage-3.0 Technical Report Released** - Comprehensive technical documentation now available
**September 28, 2025**: ğŸš€ **HunyuanImage-3.0 Open Source Release** - Inference code and model weights publicly available
- HunyuanImage-3.0 (Image Generation Model)
- [ğŸ”¥ğŸ”¥ğŸ”¥ News](#-news)
- [ğŸ§© Community Contributions](#-community-contributions)
- [ğŸ“‘ Open-source Plan](#-open-source-plan)
- [ğŸ“– Introduction](#-introduction)
- [âœ¨ Key Features](#-key-features)

#### ğŸ¨ é€‚ç”¨åœºæ™¯
- æ•°æ®ç§‘å­¦ä¸åˆ†æ
- æœºå™¨å­¦ä¹ é¡¹ç›®
- Web åº”ç”¨å¼€å‘

---

### 7. dexter

**ğŸ”¥ çƒ­åº¦æŒ‡æ•°**: 1,667

**é¡¹ç›®åç§°**: [virattt/dexter](https://github.com/virattt/dexter)
**é¡¹ç›®æè¿°**: An autonomous agent for deep financial research
**ä¸»è¦è¯­è¨€**: Python
**è®¸å¯è¯**: None
**â­ æ˜Ÿæ ‡æ•°**: 1,576
**ğŸ´ Fork æ•°**: 183
**ğŸ‘€ å…³æ³¨æ•°**: 1,576
**ğŸ› å¼€æ”¾é—®é¢˜**: 5
**æœ€åæ›´æ–°**: 2025-10-19 (11åˆ†é’Ÿå‰)
**ğŸ“– é¡¹ç›®æ–‡æ¡£**: [README](3ziye-20251019-python/7-dexter-Readme.md)

**ğŸ  é¡¹ç›®ä¸»é¡µ**: æ— 

**ğŸ“‚ å…‹éš†åœ°å€**: `https://github.com/virattt/dexter.git`

**ğŸ’» æŠ€æœ¯æ ˆ**: **ä¸»è¦è¯­è¨€**: Python

#### ğŸ¯ æ ¸å¿ƒç‰¹æ€§
**Intelligent Task Planning**: Automatically decomposes complex queries into structured research steps
**Autonomous Execution**: Selects and executes the right tools to gather financial data
**Self-Validation**: Checks its own work and iterates until tasks are complete
**Real-Time Financial Data**: Access to income statements, balance sheets, and cash flow statements
**Safety Features**: Built-in loop detection and step limits to prevent runaway execution
- Python 3.10 or higher
- [uv](https://github.com/astral-sh/uv) package manager
- OpenAI API key

#### ğŸ¨ é€‚ç”¨åœºæ™¯
- æ•°æ®ç§‘å­¦ä¸åˆ†æ
- æœºå™¨å­¦ä¹ é¡¹ç›®
- Web åº”ç”¨å¼€å‘

---

### 8. agentic-design-patterns-cn

**ğŸ”¥ çƒ­åº¦æŒ‡æ•°**: 1,422

**é¡¹ç›®åç§°**: [ginobefun/agentic-design-patterns-cn](https://github.com/ginobefun/agentic-design-patterns-cn)
**é¡¹ç›®æè¿°**: ã€ŠAgentic Design Patternsã€‹ä¸­æ–‡ç¿»è¯‘ç‰ˆ
**ä¸»è¦è¯­è¨€**: Python
**è®¸å¯è¯**: None
**â­ æ˜Ÿæ ‡æ•°**: 1,344
**ğŸ´ Fork æ•°**: 157
**ğŸ‘€ å…³æ³¨æ•°**: 1,344
**ğŸ› å¼€æ”¾é—®é¢˜**: 1
**æœ€åæ›´æ–°**: 2025-10-19 (4åˆ†é’Ÿå‰)
**ğŸ“– é¡¹ç›®æ–‡æ¡£**: [README](3ziye-20251019-python/8-agentic-design-patterns-cn-Readme.md)

**ğŸ  é¡¹ç›®ä¸»é¡µ**: æ— 

**ğŸ“‚ å…‹éš†åœ°å€**: `https://github.com/ginobefun/agentic-design-patterns-cn.git`

**ğŸ’» æŠ€æœ¯æ ˆ**: **ä¸»è¦è¯­è¨€**: Python

#### ğŸ¯ æ ¸å¿ƒç‰¹æ€§
- ğŸ“š **ä¸­è‹±æ–‡å¯¹ç…§** - å®Œæ•´çš„åŒè¯­å¯¹ç…§ç¿»è¯‘
- ğŸ¨ **é«˜äº®æ˜¾ç¤º** - ä¸­æ–‡å†…å®¹ä½¿ç”¨é»„è‰²é«˜äº®ï¼Œæ˜“äºåŒºåˆ†
- ğŸ“ **æ ¼å¼è§„èŒƒ** - ä¸¥æ ¼éµå¾ª Markdown æ ‡å‡†å’Œç¿»è¯‘è§„èŒƒ
- ğŸ”— **ä»£ç é“¾æ¥** - ä¿ç•™æ‰€æœ‰åŸä¹¦ä»£ç ç¤ºä¾‹é“¾æ¥
- âš¡ **æŒç»­æ›´æ–°** - é€ç« ç¿»è¯‘ï¼ŒæŒç»­æ›´æ–°è¿›åº¦

#### ğŸ¨ é€‚ç”¨åœºæ™¯
- æ•°æ®ç§‘å­¦ä¸åˆ†æ
- æœºå™¨å­¦ä¹ é¡¹ç›®
- Web åº”ç”¨å¼€å‘

---

### 9. DreamOmni2

**ğŸ”¥ çƒ­åº¦æŒ‡æ•°**: 1,275

**é¡¹ç›®åç§°**: [dvlab-research/DreamOmni2](https://github.com/dvlab-research/DreamOmni2)
**é¡¹ç›®æè¿°**: This project is the official implementation of 'DreamOmni2: Multimodal Instruction-based Editing and Generation''
**ä¸»è¦è¯­è¨€**: Python
**è®¸å¯è¯**: Apache License 2.0
**â­ æ˜Ÿæ ‡æ•°**: 1,217
**ğŸ´ Fork æ•°**: 117
**ğŸ‘€ å…³æ³¨æ•°**: 1,217
**ğŸ› å¼€æ”¾é—®é¢˜**: 13
**æœ€åæ›´æ–°**: 2025-10-19 (5åˆ†é’Ÿå‰)
**ğŸ“– é¡¹ç›®æ–‡æ¡£**: [README](3ziye-20251019-python/9-DreamOmni2-Readme.md)

**ğŸ  é¡¹ç›®ä¸»é¡µ**: æ— 

**ğŸ“‚ å…‹éš†åœ°å€**: `https://github.com/dvlab-research/DreamOmni2.git`

**ğŸ’» æŠ€æœ¯æ ˆ**: **ä¸»è¦è¯­è¨€**: Python

#### ğŸ·ï¸ é¡¹ç›®æ ‡ç­¾
- `image-editing`
- `image-generation`
- `unified-generation-editing-model`

#### ğŸ¯ æ ¸å¿ƒç‰¹æ€§
- ğŸ”¥**2025.10.10**: Release DreamOmni2 [editing demo](https://huggingface.co/spaces/wcy1122/DreamOmni2-Edit) and [generation demo](https://huggingface.co/spaces/wcy1122/DreamOmni2-Gen)
- ğŸ”¥**2025.10.10**: Release DreamOmni2 [Benchmark](https://huggingface.co/datasets/xiabs/DreamOmni2Bench).
- ğŸ”¥**2025.10.10**: Release DreamOmni2's [codes](https://github.com/dvlab-research/DreamOmni2) and [models](https://huggingface.co/xiabs/DreamOmni2).
- ğŸ”¥**2025.10.09**: Release DreamOmni2 [tech report](https://arxiv.org/html/2510.06679v1).

#### ğŸ¨ é€‚ç”¨åœºæ™¯
- æ•°æ®ç§‘å­¦ä¸åˆ†æ
- æœºå™¨å­¦ä¹ é¡¹ç›®
- Web åº”ç”¨å¼€å‘

---

### 10. RAE

**ğŸ”¥ çƒ­åº¦æŒ‡æ•°**: 1,184

**é¡¹ç›®åç§°**: [bytetriper/RAE](https://github.com/bytetriper/RAE)
**é¡¹ç›®æè¿°**: Official PyTorch Implementation of "Diffusion Transformers with Representation Autoencoders"
**ä¸»è¦è¯­è¨€**: Python
**è®¸å¯è¯**: MIT License
**â­ æ˜Ÿæ ‡æ•°**: 1,171
**ğŸ´ Fork æ•°**: 26
**ğŸ‘€ å…³æ³¨æ•°**: 1,171
**ğŸ› å¼€æ”¾é—®é¢˜**: 1
**æœ€åæ›´æ–°**: 2025-10-19 (2å°æ—¶å‰)
**ğŸ“– é¡¹ç›®æ–‡æ¡£**: [README](3ziye-20251019-python/10-RAE-Readme.md)

**ğŸ  é¡¹ç›®ä¸»é¡µ**: æ— 

**ğŸ“‚ å…‹éš†åœ°å€**: `https://github.com/bytetriper/RAE.git`

**ğŸ’» æŠ€æœ¯æ ˆ**: **ä¸»è¦è¯­è¨€**: Python

#### ğŸ¯ æ ¸å¿ƒç‰¹æ€§
- A PyTorch implementation of RAE and pretrained weights.
- A PyTorch implementation of LightningDiT, DiT<sup>DH</sup> and pretrained weights.
- Training and sampling scripts for the two-stage RAE+DiT pipeline.
- A TPU implementation of RAE and pretrained weights.
- Sampling of RAE and DiT<sup>DH</sup> on TPU.

#### ğŸ¨ é€‚ç”¨åœºæ™¯
- æ•°æ®ç§‘å­¦ä¸åˆ†æ
- æœºå™¨å­¦ä¹ é¡¹ç›®
- Web åº”ç”¨å¼€å‘

---

### 11. VLA-Adapter

**ğŸ”¥ çƒ­åº¦æŒ‡æ•°**: 1,207

**é¡¹ç›®åç§°**: [OpenHelix-Team/VLA-Adapter](https://github.com/OpenHelix-Team/VLA-Adapter)
**é¡¹ç›®æè¿°**: VLA-Adapter: An Effective Paradigm for Tiny-Scale Vision-Language-Action Model
**ä¸»è¦è¯­è¨€**: Python
**è®¸å¯è¯**: MIT License
**â­ æ˜Ÿæ ‡æ•°**: 1,155
**ğŸ´ Fork æ•°**: 105
**ğŸ‘€ å…³æ³¨æ•°**: 1,155
**ğŸ› å¼€æ”¾é—®é¢˜**: 7
**æœ€åæ›´æ–°**: 2025-10-19 (1å°æ—¶å‰)
**ğŸ“– é¡¹ç›®æ–‡æ¡£**: [README](3ziye-20251019-python/11-VLA-Adapter-Readme.md)

**ğŸ  é¡¹ç›®ä¸»é¡µ**: https://vla-adapter.github.io/

**ğŸ“‚ å…‹éš†åœ°å€**: `https://github.com/OpenHelix-Team/VLA-Adapter.git`

**ğŸ’» æŠ€æœ¯æ ˆ**: **ä¸»è¦è¯­è¨€**: Python

#### ğŸ·ï¸ é¡¹ç›®æ ‡ç­¾
- `embodied-ai`
- `robotics`
- `vision-language-action-model`

#### ğŸ¯ æ ¸å¿ƒç‰¹æ€§
**[2025/09/22]** We released our codes! An enhanced **Pro** version is also released (this version conforms to the pipeline in the original paper, but is optimized in implementation). Everyone is welcome to use it!ğŸ‰
**[2025/09/13]** Our paper listed in the [Trending Paper](https://huggingface.co/papers/trending) in HF! â­
**[2025/09/12]** We released the original version of the VLA-Adapter for four LIBERO models on [HuggingFace](https://huggingface.co/VLA-Adapter).
**[2025/09/11]** We released our paper on [ArXiv](https://arxiv.org/abs/2509.09372).
- [x]  Release **checkpoints** for reproduction.
- [x]  Release [VLA-Adapter v2 paper](https://arxiv.org/abs/2509.09372).
- [ ]  A more **powerful version**, **VLA-Adapter++**, and a detailed **technical report** ğŸ“ will be released soon.<br/>

#### ğŸ¨ é€‚ç”¨åœºæ™¯
- æ•°æ®ç§‘å­¦ä¸åˆ†æ
- æœºå™¨å­¦ä¹ é¡¹ç›®
- Web åº”ç”¨å¼€å‘

---

### 12. Paper2Video

**ğŸ”¥ çƒ­åº¦æŒ‡æ•°**: 1,180

**é¡¹ç›®åç§°**: [showlab/Paper2Video](https://github.com/showlab/Paper2Video)
**é¡¹ç›®æè¿°**: Automatic Video Generation from Scientific Papers
**ä¸»è¦è¯­è¨€**: Python
**è®¸å¯è¯**: MIT License
**â­ æ˜Ÿæ ‡æ•°**: 1,110
**ğŸ´ Fork æ•°**: 141
**ğŸ‘€ å…³æ³¨æ•°**: 1,110
**ğŸ› å¼€æ”¾é—®é¢˜**: 3
**æœ€åæ›´æ–°**: 2025-10-19 (28åˆ†é’Ÿå‰)
**ğŸ“– é¡¹ç›®æ–‡æ¡£**: [README](3ziye-20251019-python/12-Paper2Video-Readme.md)

**ğŸ  é¡¹ç›®ä¸»é¡µ**: https://showlab.github.io/Paper2Video/

**ğŸ“‚ å…‹éš†åœ°å€**: `https://github.com/showlab/Paper2Video.git`

**ğŸ’» æŠ€æœ¯æ ˆ**: **ä¸»è¦è¯­è¨€**: Python

#### ğŸ¯ æ ¸å¿ƒç‰¹æ€§
**Input:** a paper â• an image â• an audio
**Output:** a presentation video
- [x] [2025.10.15] We update a new version without talking-head for fast generation!
- [x] [2025.10.11] Our work receives attention on [YC Hacker News](https://news.ycombinator.com/item?id=45553701).
- [x] [2025.10.9] Thanks AK for sharing our work on [Twitter](https://x.com/_akhaliq/status/1976099830004072849)!
- [x] [2025.10.9] Our work is reported by [Medium](https://medium.com/@dataism/how-ai-learned-to-make-scientific-videos-from-slides-to-a-talking-head-0d807e491b27).
- [x] [2025.10.8] Check out our demo video below!
- [x] [2025.10.7] We release the [arxiv paper](https://arxiv.org/abs/2510.05096).

#### ğŸ¨ é€‚ç”¨åœºæ™¯
- æ•°æ®ç§‘å­¦ä¸åˆ†æ
- æœºå™¨å­¦ä¹ é¡¹ç›®
- Web åº”ç”¨å¼€å‘

---

### 13. DeepSeek-V3.2-Exp

**ğŸ”¥ çƒ­åº¦æŒ‡æ•°**: 927

**é¡¹ç›®åç§°**: [deepseek-ai/DeepSeek-V3.2-Exp](https://github.com/deepseek-ai/DeepSeek-V3.2-Exp)
**é¡¹ç›®æè¿°**: æš‚æ— æè¿°
**ä¸»è¦è¯­è¨€**: Python
**è®¸å¯è¯**: MIT License
**â­ æ˜Ÿæ ‡æ•°**: 899
**ğŸ´ Fork æ•°**: 56
**ğŸ‘€ å…³æ³¨æ•°**: 899
**ğŸ› å¼€æ”¾é—®é¢˜**: 13
**æœ€åæ›´æ–°**: 2025-10-19 (4å°æ—¶å‰)
**ğŸ“– é¡¹ç›®æ–‡æ¡£**: [README](3ziye-20251019-python/13-DeepSeek-V3.2-Exp-Readme.md)

**ğŸ  é¡¹ç›®ä¸»é¡µ**: æ— 

**ğŸ“‚ å…‹éš†åœ°å€**: `https://github.com/deepseek-ai/DeepSeek-V3.2-Exp.git`

**ğŸ’» æŠ€æœ¯æ ˆ**: **ä¸»è¦è¯­è¨€**: Python

#### ğŸ¯ æ ¸å¿ƒç‰¹æ€§
- DeepSeek Sparse Attention (

#### ğŸ¨ é€‚ç”¨åœºæ™¯
- æ•°æ®ç§‘å­¦ä¸åˆ†æ
- æœºå™¨å­¦ä¹ é¡¹ç›®
- Web åº”ç”¨å¼€å‘

---

### 14. ml-simplefold

**ğŸ”¥ çƒ­åº¦æŒ‡æ•°**: 896

**é¡¹ç›®åç§°**: [apple/ml-simplefold](https://github.com/apple/ml-simplefold)
**é¡¹ç›®æè¿°**: æš‚æ— æè¿°
**ä¸»è¦è¯­è¨€**: Python
**è®¸å¯è¯**: MIT License
**â­ æ˜Ÿæ ‡æ•°**: 868
**ğŸ´ Fork æ•°**: 57
**ğŸ‘€ å…³æ³¨æ•°**: 868
**ğŸ› å¼€æ”¾é—®é¢˜**: 19
**æœ€åæ›´æ–°**: 2025-10-18 (11å°æ—¶å‰)
**ğŸ“– é¡¹ç›®æ–‡æ¡£**: [README](3ziye-20251019-python/14-ml-simplefold-Readme.md)

**ğŸ  é¡¹ç›®ä¸»é¡µ**: æ— 

**ğŸ“‚ å…‹éš†åœ°å€**: `https://github.com/apple/ml-simplefold.git`

**ğŸ’» æŠ€æœ¯æ ˆ**: **ä¸»è¦è¯­è¨€**: Python

#### ğŸ¨ é€‚ç”¨åœºæ™¯
- æ•°æ®ç§‘å­¦ä¸åˆ†æ
- æœºå™¨å­¦ä¹ é¡¹ç›®
- Web åº”ç”¨å¼€å‘

---

### 15. AgentFlow

**ğŸ”¥ çƒ­åº¦æŒ‡æ•°**: 863

**é¡¹ç›®åç§°**: [lupantech/AgentFlow](https://github.com/lupantech/AgentFlow)
**é¡¹ç›®æè¿°**: AgentFlow: In-the-Flow Agentic System Optimization
**ä¸»è¦è¯­è¨€**: Python
**è®¸å¯è¯**: MIT License
**â­ æ˜Ÿæ ‡æ•°**: 818
**ğŸ´ Fork æ•°**: 91
**ğŸ‘€ å…³æ³¨æ•°**: 818
**ğŸ› å¼€æ”¾é—®é¢˜**: 4
**æœ€åæ›´æ–°**: 2025-10-19 (56åˆ†é’Ÿå‰)
**ğŸ“– é¡¹ç›®æ–‡æ¡£**: [README](3ziye-20251019-python/15-AgentFlow-Readme.md)

**ğŸ  é¡¹ç›®ä¸»é¡µ**: https://agentflow.stanford.edu

**ğŸ“‚ å…‹éš†åœ°å€**: `https://github.com/lupantech/AgentFlow.git`

**ğŸ’» æŠ€æœ¯æ ˆ**: **ä¸»è¦è¯­è¨€**: Python

#### ğŸ·ï¸ é¡¹ç›®æ ‡ç­¾
- `agentic-ai`
- `agentic-systems`
- `llms`
- `llms-reasoning`
- `multi-agent-systems`
- `reinforcement-learning`
- `tool-augmented`

#### ğŸ¯ æ ¸å¿ƒç‰¹æ€§
**[2025.10.16]** ğŸ† Our paper has been accepted by [**NeurIPS 2025 Efficient Reasoning Workshop**](https://efficient-reasoning.github.io/)!
**[2025.10.13]** ğŸ“¸ Excited to have a tutorial video for AgentFlow covered by Discover AI on **[YouTube](https://www.youtube.com/watch?v=kIQbCQIH1SI)**!
**[2025.10.10]** ğŸš€ Our X [post](https://x.com/lupantech/status/1976016000345919803) received **1K+ likes**! Feel free to check out the post and join the discussion! ğŸ’¬
**[2025.10.08]** ğŸ”¥ We are honored to be featured as ğŸ¤— HuggingFace **[Daily Paper #2](https://huggingface.co/papers/2510.05592)**.

#### ğŸ¨ é€‚ç”¨åœºæ™¯
- æ•°æ®ç§‘å­¦ä¸åˆ†æ
- æœºå™¨å­¦ä¹ é¡¹ç›®
- Web åº”ç”¨å¼€å‘

---

### 16. MiMo-Audio

**ğŸ”¥ çƒ­åº¦æŒ‡æ•°**: 817

**é¡¹ç›®åç§°**: [XiaomiMiMo/MiMo-Audio](https://github.com/XiaomiMiMo/MiMo-Audio)
**é¡¹ç›®æè¿°**: MiMo-Audio: Audio Language Models are Few-Shot Learners
**ä¸»è¦è¯­è¨€**: Python
**è®¸å¯è¯**: Apache License 2.0
**â­ æ˜Ÿæ ‡æ•°**: 781
**ğŸ´ Fork æ•°**: 73
**ğŸ‘€ å…³æ³¨æ•°**: 781
**ğŸ› å¼€æ”¾é—®é¢˜**: 31
**æœ€åæ›´æ–°**: 2025-10-19 (2å°æ—¶å‰)
**ğŸ“– é¡¹ç›®æ–‡æ¡£**: [README](3ziye-20251019-python/16-MiMo-Audio-Readme.md)

**ğŸ  é¡¹ç›®ä¸»é¡µ**: https://xiaomimimo.github.io/MiMo-Audio-Demo/

**ğŸ“‚ å…‹éš†åœ°å€**: `https://github.com/XiaomiMiMo/MiMo-Audio.git`

**ğŸ’» æŠ€æœ¯æ ˆ**: **ä¸»è¦è¯­è¨€**: Python

#### ğŸ¨ é€‚ç”¨åœºæ™¯
- æ•°æ®ç§‘å­¦ä¸åˆ†æ
- æœºå™¨å­¦ä¹ é¡¹ç›®
- Web åº”ç”¨å¼€å‘

---

### 17. Code2Video

**ğŸ”¥ çƒ­åº¦æŒ‡æ•°**: 774

**é¡¹ç›®åç§°**: [showlab/Code2Video](https://github.com/showlab/Code2Video)
**é¡¹ç›®æè¿°**: Video generation via code
**ä¸»è¦è¯­è¨€**: Python
**è®¸å¯è¯**: MIT License
**â­ æ˜Ÿæ ‡æ•°**: 727
**ğŸ´ Fork æ•°**: 95
**ğŸ‘€ å…³æ³¨æ•°**: 727
**ğŸ› å¼€æ”¾é—®é¢˜**: 0
**æœ€åæ›´æ–°**: 2025-10-19 (5å°æ—¶å‰)
**ğŸ“– é¡¹ç›®æ–‡æ¡£**: [README](3ziye-20251019-python/17-Code2Video-Readme.md)

**ğŸ  é¡¹ç›®ä¸»é¡µ**: https://showlab.github.io/Code2Video/

**ğŸ“‚ å…‹éš†åœ°å€**: `https://github.com/showlab/Code2Video.git`

**ğŸ’» æŠ€æœ¯æ ˆ**: **ä¸»è¦è¯­è¨€**: Python

#### ğŸ·ï¸ é¡¹ç›®æ ‡ç­¾
- `coding`
- `education`
- `multi-agent`
- `video-generation`

#### ğŸ¨ é€‚ç”¨åœºæ™¯
- æ•°æ®ç§‘å­¦ä¸åˆ†æ
- æœºå™¨å­¦ä¹ é¡¹ç›®
- Web åº”ç”¨å¼€å‘

---

### 18. DEIMv2

**ğŸ”¥ çƒ­åº¦æŒ‡æ•°**: 757

**é¡¹ç›®åç§°**: [Intellindust-AI-Lab/DEIMv2](https://github.com/Intellindust-AI-Lab/DEIMv2)
**é¡¹ç›®æè¿°**: [DEIMv2] Real Time Object Detection Meets DINOv3 
**ä¸»è¦è¯­è¨€**: Python
**è®¸å¯è¯**: Other
**â­ æ˜Ÿæ ‡æ•°**: 720
**ğŸ´ Fork æ•°**: 74
**ğŸ‘€ å…³æ³¨æ•°**: 720
**ğŸ› å¼€æ”¾é—®é¢˜**: 21
**æœ€åæ›´æ–°**: 2025-10-19 (4åˆ†é’Ÿå‰)
**ğŸ“– é¡¹ç›®æ–‡æ¡£**: [README](3ziye-20251019-python/18-DEIMv2-Readme.md)

**ğŸ  é¡¹ç›®ä¸»é¡µ**: https://intellindust-ai-lab.github.io/projects/DEIMv2/

**ğŸ“‚ å…‹éš†åœ°å€**: `https://github.com/Intellindust-AI-Lab/DEIMv2.git`

**ğŸ’» æŠ€æœ¯æ ˆ**: **ä¸»è¦è¯­è¨€**: Python

#### ğŸ·ï¸ é¡¹ç›®æ ‡ç­¾
- `detection-transformer`
- `dinov3`
- `object-detection`
- `real-time`
- `real-time-detection`

#### ğŸ¯ æ ¸å¿ƒç‰¹æ€§
- Equal Contribution &nbsp;&nbsp; â€  Corresponding Author
- [x] **\[2025.10.2\]** [DEIMv2 has been integrated into X-AnyLabeling!](https://github.com/Intellindust-AI-Lab/DEIMv2/issues/25#issue-3473960491) Many thanks to the X-AnyLabeling maintainers for making this possible.
- [x] **\[2025.9.26\]** Release DEIMv2 series.
- [1. ğŸ¤– Model Zoo](#1-model-zoo)
- [2. âš¡ Quick Start](#2-quick-start)
- [3. ğŸ› ï¸ Usage](#3-usage)
- [4. ğŸ§° Tools](#4-tools)
- [5. ğŸ“œ Citation](#5-citation)

#### ğŸ¨ é€‚ç”¨åœºæ™¯
- æ•°æ®ç§‘å­¦ä¸åˆ†æ
- æœºå™¨å­¦ä¹ é¡¹ç›®
- Web åº”ç”¨å¼€å‘

---

### 19. LuoGen-agent

**ğŸ”¥ çƒ­åº¦æŒ‡æ•°**: 751

**é¡¹ç›®åç§°**: [LuoGen-AI/LuoGen-agent](https://github.com/LuoGen-AI/LuoGen-agent)
**é¡¹ç›®æè¿°**: ä¸€é”®äº§å‡ºçˆ†æ¬¾è§†é¢‘ï¼š1.è‡ªåŠ¨æå–å¯¹æ ‡æ–‡æ¡ˆ 2.è‡ªåŠ¨è¿›è¡Œæ–‡æ¡ˆä»¿å†™ 3.è‡ªåŠ¨æ ¹æ®æ–‡æ¡ˆå£°éŸ³å…‹éš† 4.è‡ªåŠ¨ç”Ÿæˆæ•°å­—äººå£æ’­ 5.è‡ªåŠ¨æ·»åŠ å­—å¹• 6.è‡ªåŠ¨æ·»åŠ èƒŒæ™¯éŸ³ä¹ 7.è‡ªåŠ¨æ·»åŠ è§†é¢‘æ ‡é¢˜ 8.è‡ªåŠ¨ç”Ÿæˆè§†é¢‘å°é¢ 9.è‡ªåŠ¨å°†è§†é¢‘å‘å¸ƒåˆ°å„å¹³å°
**ä¸»è¦è¯­è¨€**: Python
**è®¸å¯è¯**: GNU General Public License v3.0
**â­ æ˜Ÿæ ‡æ•°**: 714
**ğŸ´ Fork æ•°**: 74
**ğŸ‘€ å…³æ³¨æ•°**: 714
**ğŸ› å¼€æ”¾é—®é¢˜**: 3
**æœ€åæ›´æ–°**: 2025-10-19 (8å°æ—¶å‰)
**ğŸ“– é¡¹ç›®æ–‡æ¡£**: [README](3ziye-20251019-python/19-LuoGen-agent-Readme.md)

**ğŸ  é¡¹ç›®ä¸»é¡µ**: æ— 

**ğŸ“‚ å…‹éš†åœ°å€**: `https://github.com/LuoGen-AI/LuoGen-agent.git`

**ğŸ’» æŠ€æœ¯æ ˆ**: **ä¸»è¦è¯­è¨€**: Python

#### ğŸ¯ æ ¸å¿ƒç‰¹æ€§
- ç”±äºä»£ç ä½“ç§¯åŠæ¨¡å‹æ–‡ä»¶è¿‡å¤§ï¼Œè¯·è¯¸ä½ç§»æ­¥ [ä»£ç åœ°å€](ä»£ç åœ°å€.txt) è¿›è¡Œä¸‹è½½ã€‚
- ç”±äºè¯¥åº”ç”¨ä¸ºæœ¬åœ°è¿è¡Œçš„å®¢æˆ·ç«¯åº”ç”¨ï¼Œä¸ºäº†è¯¸ä½çš„ä½¿ç”¨ä½“éªŒï¼Œå…ˆè¿›è¡Œ [ä½¿ç”¨å‰å¿…è£…](ä½¿ç”¨å‰å¿…è£….txt) è¿›è¡Œä¸‹è½½å®‰è£…ã€‚
- ğŸ“ **æ™ºèƒ½æ–‡æ¡ˆå¤„ç†**ï¼šè‡ªåŠ¨æå–å¯¹æ ‡æ–‡æ¡ˆ + æ™ºèƒ½ä»¿å†™ä¼˜åŒ–
- ğŸ¤ **å£°éŸ³å…‹éš†**ï¼šåŸºäº Whisper å’Œ CosyVoice å®ç°é«˜ä¿çœŸè¯­éŸ³åˆæˆ
- ğŸ‘¥ **æ•°å­—äººç”Ÿæˆ**ï¼šé›†æˆ HeyGem å®ç°è‡ªç„¶å£æ’­æ•ˆæœ
- ğŸ¬ **å…¨æµç¨‹è§†é¢‘åˆ¶ä½œ**ï¼šå­—å¹•/BGM/æ ‡é¢˜/å°é¢è‡ªåŠ¨ç”Ÿæˆ + å¤šå¹³å°å‘å¸ƒ
- [social-auto-upload](https://github.com/...) - å¤šå¹³å°å‘å¸ƒæ¡†æ¶
- [CosyVoice](https://github.com/tencent-ailab/cosyvoice) - é«˜è´¨é‡è¯­éŸ³åˆæˆ

#### ğŸ¨ é€‚ç”¨åœºæ™¯
- æ•°æ®ç§‘å­¦ä¸åˆ†æ
- æœºå™¨å­¦ä¹ é¡¹ç›®
- Web åº”ç”¨å¼€å‘

---

### 20. LongLive

**ğŸ”¥ çƒ­åº¦æŒ‡æ•°**: 727

**é¡¹ç›®åç§°**: [NVlabs/LongLive](https://github.com/NVlabs/LongLive)
**é¡¹ç›®æè¿°**: LongLive: Real-time Interactive Long Video Generation
**ä¸»è¦è¯­è¨€**: Python
**è®¸å¯è¯**: Other
**â­ æ˜Ÿæ ‡æ•°**: 706
**ğŸ´ Fork æ•°**: 42
**ğŸ‘€ å…³æ³¨æ•°**: 706
**ğŸ› å¼€æ”¾é—®é¢˜**: 9
**æœ€åæ›´æ–°**: 2025-10-19 (6å°æ—¶å‰)
**ğŸ“– é¡¹ç›®æ–‡æ¡£**: [README](3ziye-20251019-python/20-LongLive-Readme.md)

**ğŸ  é¡¹ç›®ä¸»é¡µ**: https://nvlabs.github.io/LongLive

**ğŸ“‚ å…‹éš†åœ°å€**: `https://github.com/NVlabs/LongLive.git`

**ğŸ’» æŠ€æœ¯æ ˆ**: **ä¸»è¦è¯­è¨€**: Python

#### ğŸ·ï¸ é¡¹ç›®æ ‡ç­¾
- `efficient-tuning`
- `interactive`
- `long-context`
- `real-time`
- `sparse-attention`
- `video-genenratio`

#### ğŸ¯ æ ¸å¿ƒç‰¹æ€§
1. **Long Video Gen**: LongLive supports up to 240s video generation, with visual consistency.
- 2. **Real-time Inference**: LongLive supports 20.7 FPS generation speed on a single H100 GPU, and 24.8 FPS with FP8 quantization with marginal quality loss.
- 3. **Efficient Fine-tuning**: LongLive extends a short-clip model to minute-long generation in 32 H100 GPU-days.

#### ğŸ¨ é€‚ç”¨åœºæ™¯
- æ•°æ®ç§‘å­¦ä¸åˆ†æ
- æœºå™¨å­¦ä¹ é¡¹ç›®
- Web åº”ç”¨å¼€å‘

---