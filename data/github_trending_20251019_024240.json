[
  {
    "basic_info": {
      "name": "nanochat",
      "full_name": "karpathy/nanochat",
      "owner": "karpathy",
      "description": "The best ChatGPT that $100 can buy.",
      "url": "https://github.com/karpathy/nanochat",
      "clone_url": "https://github.com/karpathy/nanochat.git",
      "ssh_url": "git@github.com:karpathy/nanochat.git",
      "homepage": "",
      "created_at": "2025-10-13T13:46:35Z",
      "updated_at": "2025-10-19T02:42:20Z",
      "pushed_at": "2025-10-17T15:35:41Z"
    },
    "stats": {
      "stars": 26157,
      "forks": 2645,
      "watchers": 26157,
      "open_issues": 74,
      "size": 74
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 287804,
        "HTML": 20102,
        "Rust": 16657,
        "Shell": 10846
      },
      "license": null,
      "topics": []
    },
    "content": {
      "readme": "# nanochat\n\n![nanochat logo](dev/nanochat.png)\n\n> The best ChatGPT that $100 can buy.\n\nThis repo is a full-stack implementation of an LLM like ChatGPT in a single, clean, minimal, hackable, dependency-lite codebase. nanochat is designed to run on a single 8XH100 node via scripts like [speedrun.sh](speedrun.sh), that run the entire pipeline start to end. This includes tokenization, pretraining, finetuning, evaluation, inference, and web serving over a simple UI so that you can talk to your own LLM just like ChatGPT. nanochat will become the capstone project of the course LLM101n being developed by Eureka Labs.\n\n## Talk to it\n\nTo get a sense of the endpoint of this repo, you can currently find [nanochat d32](https://github.com/karpathy/nanochat/discussions/8) hosted on [nanochat.karpathy.ai](https://nanochat.karpathy.ai/). \"d32\" means that this model has 32 layers in the Transformer neural network. This model has 1.9 billion parameters, it was trained on 38 billion tokens by simply running the single script [run1000.sh](run1000.sh), and the total cost of training was ~$800 (about 33 hours training time on 8XH100 GPU node). While today this is enough to outperform GPT-2 of 2019, it falls dramatically short of moden Large Language Models like GPT-5. When talking to these micro models, you'll see that they make a lot of mistakes, they are a little bit naive and silly and they hallucinate a ton, a bit like children. It's kind of amusing. But what makes nanochat unique is that it is fully yours - fully configurable, tweakable, hackable, and trained by you from start to end. To train and talk to your own, we turn to...\n\n## Quick start\n\nThe fastest way to feel the magic is to run the speedrun script [speedrun.sh](speedrun.sh), which trains and inferences the $100 tier of nanochat. On an 8XH100 node at $24/hr, this gives a total run time of about 4 hours. Boot up a new 8XH100 GPU box from your favorite provider (e.g. I use and like [Lambda](https://lambda.ai/service/gpu-cloud)), and kick off the training script:\n\n```bash\nbash speedrun.sh\n```\n\nAlternatively, since the script runs for 4 hours, I like to launch it like this inside a new screen session `speedrun` (and also log output to `speedrun.log`):\n\n```bash\nscreen -L -Logfile speedrun.log -S speedrun bash speedrun.sh\n```\n\nSee the [screen cheatsheet](https://gist.github.com/jctosta/af918e1618682638aa82) if you are less familiar. You can watch it go inside the screen session, or detach with `Ctrl-a d` and `tail speedrun.log` to view progress. Now wait 4 hours. Once it's done, you can talk to your LLM via the ChatGPT-like web UI. Make sure again that your local uv virtual environment is active (run `source .venv/bin/activate`), and serve it:\n\n```bash\npython -m scripts.chat_web\n```\n\nAnd then visit the URL shown. Make sure to access it correctly, e.g. on Lambda use the public IP of the node you're on, followed by the port, so for example [http://209.20.xxx.xxx:8000/](http://209.20.xxx.xxx:8000/), etc. Then talk to your LLM as you'd normally talk to ChatGPT! Get it to write stories or poems. Ask it to tell you who you are to see a hallucination. Ask it why the sky is blue. Or why it's green. The speedrun is a 4e19 FLOPs capability model so it's a bit like talking to a kindergartener :).\n\n---\n\n<img width=\"2672\" height=\"1520\" alt=\"image\" src=\"https://github.com/user-attachments/assets/ed39ddf8-2370-437a-bedc-0f39781e76b5\" />\n\n---\n\nYou can also `cat report.md` file which appeared in the project directory and contains the \"report card\" of the run, i.e. a bunch of evaluations and metrics. At the very end, you'll see a summary table, for example:\n\n---\n\n- Characters: 333,989\n- Lines: 8,304\n- Files: 44\n- Tokens (approx): 83,497\n- Dependencies (uv.lock lines): 2,004\n\n| Metric          | BASE     | MID      | SFT      | RL       |\n|-----------------|----------|----------|----------|----------|\n| CORE            | 0.2219   | -        | -        | -        |\n| ARC-Challenge   | -        | 0.2875   | 0.2807   | -        |\n| ARC-Easy        | -        | 0.3561   | 0.3876   | -        |\n| GSM8K           | -        | 0.0250   | 0.0455   | 0.0758   |\n| HumanEval       | -        | 0.0671   | 0.0854   | -        |\n| MMLU            | -        | 0.3111   | 0.3151   | -        |\n| ChatCORE        | -        | 0.0730   | 0.0884   | -        |\n\nTotal wall clock time: 3h51m\n\n---\n\n(Your table might be missing the RL number by default). For a lot more information around the speedrun script and what to look for and expect, please refer to the walkthrough that I posted in Discussions of the repo: [\"Introducing nanochat: The best ChatGPT that $100 can buy\"](https://github.com/karpathy/nanochat/discussions/1).\n\n## Bigger models\n\nUnsurprisingly, $100 is not enough to train a highly performant ChatGPT clone. In fact, LLMs are famous for their multi-million dollar capex. For our purposes, I think there are two more scales of interest. First is the ~$300 tier d26 model (i.e. depth=26) that trains in ~12",
      "default_branch": "master"
    },
    "fetched_at": "2025-10-19T02:42:41.165529"
  },
  {
    "basic_info": {
      "name": "skills",
      "full_name": "anthropics/skills",
      "owner": "anthropics",
      "description": "Public repository for Skills",
      "url": "https://github.com/anthropics/skills",
      "clone_url": "https://github.com/anthropics/skills.git",
      "ssh_url": "git@github.com:anthropics/skills.git",
      "homepage": null,
      "created_at": "2025-09-22T15:53:31Z",
      "updated_at": "2025-10-19T02:42:22Z",
      "pushed_at": "2025-10-18T16:39:46Z"
    },
    "stats": {
      "stars": 5077,
      "forks": 388,
      "watchers": 5077,
      "open_issues": 18,
      "size": 3089
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 566237,
        "JavaScript": 45621,
        "HTML": 20844,
        "Shell": 11441
      },
      "license": null,
      "topics": []
    },
    "content": {
      "readme": "# Skills\nSkills are folders of instructions, scripts, and resources that Claude loads dynamically to improve performance on specialized tasks. Skills teach Claude how to complete specific tasks in a repeatable way, whether that's creating documents with your company's brand guidelines, analyzing data using your organization's specific workflows, or automating personal tasks.\n\nFor more information, check out:\n- [What are skills?](https://support.claude.com/en/articles/12512176-what-are-skills)\n- [Using skills in Claude](https://support.claude.com/en/articles/12512180-using-skills-in-claude)\n- [How to create custom skills](https://support.claude.com/en/articles/12512198-creating-custom-skills)\n- [Equipping agents for the real world with Agent Skills](https://anthropic.com/engineering/equipping-agents-for-the-real-world-with-agent-skills)\n\n# About This Repository\n\nThis repository contains example skills that demonstrate what's possible with Claude's skills system. These examples range from creative applications (art, music, design) to technical tasks (testing web apps, MCP server generation) to enterprise workflows (communications, branding, etc.).\n\nEach skill is self-contained in its own directory with a `SKILL.md` file containing the instructions and metadata that Claude uses. Browse through these examples to get inspiration for your own skills or to understand different patterns and approaches.\n\nThe example skills in this repo are open source (Apache 2.0). We've also included the document creation & editing skills that power [Claude's document capabilities](https://www.anthropic.com/news/create-files) under the hood in the [`document-skills/`](./document-skills/) folder. These are source-available, not open source, but we wanted to share these with developers as a reference for more complex skills that are actively used in a production AI application.\n\n**Note:** These are reference examples for inspiration and learning. They showcase general-purpose capabilities rather than organization-specific workflows or sensitive content.\n\n## Disclaimer\n\n**These skills are provided for demonstration and educational purposes only.** While some of these capabilities may be available in Claude, the implementations and behaviors you receive from Claude may differ from what is shown in these examples. These examples are meant to illustrate patterns and possibilities. Always test skills thoroughly in your own environment before relying on them for critical tasks.\n\n# Example Skills\n\nThis repository includes a diverse collection of example skills demonstrating different capabilities:\n\n## Creative & Design\n- **algorithmic-art** - Create generative art using p5.js with seeded randomness, flow fields, and particle systems\n- **canvas-design** - Design beautiful visual art in .png and .pdf formats using design philosophies\n- **slack-gif-creator** - Create animated GIFs optimized for Slack's size constraints\n\n## Development & Technical\n- **artifacts-builder** - Build complex claude.ai HTML artifacts using React, Tailwind CSS, and shadcn/ui components\n- **mcp-server** - Guide for creating high-quality MCP servers to integrate external APIs and services\n- **webapp-testing** - Test local web applications using Playwright for UI verification and debugging\n\n## Enterprise & Communication\n- **brand-guidelines** - Apply Anthropic's official brand colors and typography to artifacts\n- **internal-comms** - Write internal communications like status reports, newsletters, and FAQs\n- **theme-factory** - Style artifacts with 10 pre-set professional themes or generate custom themes on-the-fly\n\n## Meta Skills\n- **skill-creator** - Guide for creating effective skills that extend Claude's capabilities\n- **template-skill** - A basic template to use as a starting point for new skills\n\n# Document Skills\n\nThe `document-skills/` subdirectory contains skills that Anthropic developed to help Claude create various document file formats. These skills demonstrate advanced patterns for working with complex file formats and binary data:\n\n- **docx** - Create, edit, and analyze Word documents with support for tracked changes, comments, formatting preservation, and text extraction\n- **pdf** - Comprehensive PDF manipulation toolkit for extracting text and tables, creating new PDFs, merging/splitting documents, and handling forms\n- **pptx** - Create, edit, and analyze PowerPoint presentations with support for layouts, templates, charts, and automated slide generation\n- **xlsx** - Create, edit, and analyze Excel spreadsheets with support for formulas, formatting, data analysis, and visualization\n\n**Important Disclaimer:** These document skills are point-in-time snapshots and are not actively maintained or updated. Versions of these skills ship pre-included with Claude. They are primarily intended as reference examples to illustrate how Anthropic approaches developing more complex skills that work with binary file formats and document structures.\n\n# Try in Claude Code, C",
      "default_branch": "main"
    },
    "fetched_at": "2025-10-19T02:42:42.275756"
  },
  {
    "basic_info": {
      "name": "TinyRecursiveModels",
      "full_name": "SamsungSAILMontreal/TinyRecursiveModels",
      "owner": "SamsungSAILMontreal",
      "description": null,
      "url": "https://github.com/SamsungSAILMontreal/TinyRecursiveModels",
      "clone_url": "https://github.com/SamsungSAILMontreal/TinyRecursiveModels.git",
      "ssh_url": "git@github.com:SamsungSAILMontreal/TinyRecursiveModels.git",
      "homepage": null,
      "created_at": "2025-10-07T13:24:28Z",
      "updated_at": "2025-10-19T02:37:37Z",
      "pushed_at": "2025-10-08T19:46:47Z"
    },
    "stats": {
      "stars": 4702,
      "forks": 607,
      "watchers": 4702,
      "open_issues": 20,
      "size": 1266
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 147529
      },
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "# Less is More: Recursive Reasoning with Tiny Networks\n\nThis is the codebase for the paper: \"Less is More: Recursive Reasoning with Tiny Networks\". TRM is a recursive reasoning approach that achieves amazing scores of 45% on ARC-AGI-1 and 8% on ARC-AGI-2 using a tiny 7M parameters neural network.\n\n[Paper](https://arxiv.org/abs/2510.04871)\n\n### Motivation\n\nTiny Recursion Model (TRM) is a recursive reasoning model that achieves amazing scores of 45% on ARC-AGI-1 and 8% on ARC-AGI-2 with a tiny 7M parameters neural network. The idea that one must rely on massive foundational models trained for millions of dollars by some big corporation in order to achieve success on hard tasks is a trap. Currently, there is too much focus on exploiting LLMs rather than devising and expanding new lines of direction. With recursive reasoning, it turns out that â€œless is moreâ€: you donâ€™t always need to crank up model size in order for a model to reason and solve hard problems. A tiny model pretrained from scratch, recursing on itself and updating its answers over time, can achieve a lot without breaking the bank.\n\nThis work came to be after I learned about the recent innovative Hierarchical Reasoning Model (HRM). I was amazed that an approach using small models could do so well on hard tasks like the ARC-AGI competition (reaching 40% accuracy when normally only Large Language Models could compete). But I kept thinking that it is too complicated, relying too much on biological arguments about the human brain, and that this recursive reasoning process could be greatly simplified and improved. Tiny Recursion Model (TRM) simplifies recursive reasoning to its core essence, which ultimately has nothing to do with the human brain, does not require any mathematical (fixed-point) theorem, nor any hierarchy.\n\n### How TRM works\n\n<p align=\"center\">\n  <img src=\"https://AlexiaJM.github.io/assets/images/TRM_fig.png\" alt=\"TRM\"  style=\"width: 30%;\">\n</p>\n\nTiny Recursion Model (TRM) recursively improves its predicted answer y with a tiny network. It starts with the embedded input question x and initial embedded answer y and latent z. For up to K improvements steps, it tries to improve its answer y. It does so by i) recursively updating n times its latent z given the question x, current answer y, and current latent z (recursive reasoning), and then ii) updating its answer y given the current answer y and current latent z. This recursive process allows the model to progressively improve its answer (potentially addressing any errors from its previous answer) in an extremely parameter-efficient manner while minimizing overfitting.\n\n### Requirements\n\n- Python 3.10 (or similar)\n- Cuda 12.6.0 (or similar)\n\n```bash\npip install --upgrade pip wheel setuptools\npip install --pre --upgrade torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu126 # install torch based on your cuda version\npip install -r requirements.txt # install requirements\npip install --no-cache-dir --no-build-isolation adam-atan2 \nwandb login YOUR-LOGIN # login if you want the logger to sync results to your Weights & Biases (https://wandb.ai/)\n```\n\n### Dataset Preparation\n\n```bash\n# ARC-AGI-1\npython -m dataset.build_arc_dataset \\\n  --input-file-prefix kaggle/combined/arc-agi \\\n  --output-dir data/arc1concept-aug-1000 \\\n  --subsets training evaluation concept \\\n  --test-set-name evaluation\n\n# ARC-AGI-2\npython -m dataset.build_arc_dataset \\\n  --input-file-prefix kaggle/combined/arc-agi \\\n  --output-dir data/arc2concept-aug-1000 \\\n  --subsets training2 evaluation2 concept \\\n  --test-set-name evaluation2\n\n## Note: You cannot train on both ARC-AGI-1 and ARC-AGI-2 and evaluate them both because ARC-AGI-2 training data contains some ARC-AGI-1 eval data\n\n# Sudoku-Extreme\npython dataset/build_sudoku_dataset.py --output-dir data/sudoku-extreme-1k-aug-1000  --subsample-size 1000 --num-aug 1000  # 1000 examples, 1000 augments\n\n# Maze-Hard\npython dataset/build_maze_dataset.py # 1000 examples, 8 augments\n```\n\n## Experiments\n\n### ARC-AGI-1 (assuming 4 H-100 GPUs):\n\n```bash\nrun_name=\"pretrain_att_arc1concept_4\"\ntorchrun --nproc-per-node 4 --rdzv_backend=c10d --rdzv_endpoint=localhost:0 --nnodes=1 pretrain.py \\\narch=trm \\\ndata_paths=\"[data/arc1concept-aug-1000]\" \\\narch.L_layers=2 \\\narch.H_cycles=3 arch.L_cycles=4 \\\n+run_name=${run_name} ema=True\n\n```\n\n*Runtime:* ~3 days\n\n### ARC-AGI-2 (assuming 4 H-100 GPUs):\n\n```bash\nrun_name=\"pretrain_att_arc2concept_4\"\ntorchrun --nproc-per-node 4 --rdzv_backend=c10d --rdzv_endpoint=localhost:0 --nnodes=1 pretrain.py \\\narch=trm \\\ndata_paths=\"[data/arc2concept-aug-1000]\" \\\narch.L_layers=2 \\\narch.H_cycles=3 arch.L_cycles=4 \\\n+run_name=${run_name} ema=True\n\n```\n\n*Runtime:* ~3 days\n\n### Sudoku-Extreme (assuming 1 L40S GPU):\n\n```bash\nrun_name=\"pretrain_mlp_t_sudoku\"\npython pretrain.py \\\narch=trm \\\ndata_paths=\"[data/sudoku-extreme-1k-aug-1000]\" \\\nevaluators=\"[]\" \\\nepochs=50000 eval_interval=5000 \\\nlr=1e-4 puzzle_emb_lr=1e-4 weight_decay=1.0 puzzle",
      "default_branch": "main"
    },
    "fetched_at": "2025-10-19T02:42:43.376867"
  },
  {
    "basic_info": {
      "name": "neutts-air",
      "full_name": "neuphonic/neutts-air",
      "owner": "neuphonic",
      "description": "On-device TTS model by Neuphonic",
      "url": "https://github.com/neuphonic/neutts-air",
      "clone_url": "https://github.com/neuphonic/neutts-air.git",
      "ssh_url": "git@github.com:neuphonic/neutts-air.git",
      "homepage": null,
      "created_at": "2025-10-02T12:48:55Z",
      "updated_at": "2025-10-19T02:08:39Z",
      "pushed_at": "2025-10-17T10:20:23Z"
    },
    "stats": {
      "stars": 3352,
      "forks": 301,
      "watchers": 3352,
      "open_issues": 25,
      "size": 1906
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 14928
      },
      "license": "Apache License 2.0",
      "topics": []
    },
    "content": {
      "readme": "# NeuTTS Air â˜ï¸\n\nHuggingFace ğŸ¤—: [Model](https://huggingface.co/neuphonic/neutts-air), [Q8 GGUF](https://huggingface.co/neuphonic/neutts-air-q8-gguf), [Q4 GGUF](https://huggingface.co/neuphonic/neutts-air-q4-gguf) [Spaces](https://huggingface.co/spaces/neuphonic/neutts-air)\n\n[Demo Video](https://github.com/user-attachments/assets/020547bc-9e3e-440f-b016-ae61ca645184)\n\n_Created by [Neuphonic](http://neuphonic.com/) - building faster, smaller, on-device voice AI_\n\nState-of-the-art Voice AI has been locked behind web APIs for too long. NeuTTS Air is the worldâ€™s first super-realistic, on-device, TTS speech language model with instant voice cloning. Built off a 0.5B LLM backbone, NeuTTS Air brings natural-sounding speech, real-time performance, built-in security and speaker cloning to your local device - unlocking a new category of embedded voice agents, assistants, toys, and compliance-safe apps.\n\n## Key Features\n\n- ğŸ—£Best-in-class realism for its size - produces natural, ultra-realistic voices that sound human\n- ğŸ“±Optimised for on-device deployment - provided in GGML format, ready to run on phones, laptops, or even Raspberry Pis\n- ğŸ‘«Instant voice cloning - create your own speaker with as little as 3 seconds of audio\n- ğŸš„Simple LM + codec architecture built off a 0.5B backbone - the sweet spot between speed, size, and quality for real-world applications\n\n> [!CAUTION]\n> Websites like neutts.com are popping up and they're not affliated with Neuphonic, our github or this repo.\n>\n> We are on neuphonic.com only. Please be careful out there! ğŸ™\n\n## Model Details\n\nNeuTTS Air is built off Qwen 0.5B - a lightweight yet capable language model optimised for text understanding and generation - as well as a powerful combination of technologies designed for efficiency and quality:\n\n- **Supported Languages**: English\n- **Audio Codec**: [NeuCodec](https://huggingface.co/neuphonic/neucodec) - our 50hz neural audio codec that achieves exceptional audio quality at low bitrates using a single codebook\n- **Context Window**: 2048 tokens, enough for processing ~30 seconds of audio (including prompt duration)\n- **Format**: Available in GGML format for efficient on-device inference\n- **Responsibility**: Watermarked outputs\n- **Inference Speed**: Real-time generation on mid-range devices\n- **Power Consumption**: Optimised for mobile and embedded devices\n\n## Get Started\n\n> [!NOTE]\n> We have added a [streaming example](examples/basic_streaming_example.py) using the `llama-cpp-python` library as well as a [finetuning script](examples/finetune.py). For finetuning, please refer to the [finetune guide](TRAINING.md) for more details.\n\n1. **Clone Git Repo**\n\n   ```bash\n   git clone https://github.com/neuphonic/neutts-air.git\n   cd neutts-air\n   ```\n\n2. **Install `espeak` (required dependency)**\n\n   Please refer to the following link for instructions on how to install `espeak`:\n\n   https://github.com/espeak-ng/espeak-ng/blob/master/docs/guide.md\n\n   ```bash\n   # Mac OS\n   brew install espeak\n\n   # Ubuntu/Debian\n   sudo apt install espeak\n   ```\n\n   Mac users may need to put the following lines at the top of the neutts.py file.\n\n   ```python\n   from phonemizer.backend.espeak.wrapper import EspeakWrapper\n   _ESPEAK_LIBRARY = '/opt/homebrew/Cellar/espeak/1.48.04_1/lib/libespeak.1.1.48.dylib'  #use the Path to the library.\n   EspeakWrapper.set_library(_ESPEAK_LIBRARY)\n   ```\n\n   Windows users may need to run (see https://github.com/bootphon/phonemizer/issues/163)\n\n   ```pwsh\n   $env:PHONEMIZER_ESPEAK_LIBRARY = \"c:\\Program Files\\eSpeak NG\\libespeak-ng.dll\"\n   $env:PHONEMIZER_ESPEAK_PATH = \"c:\\Program Files\\eSpeak NG\"\n   setx PHONEMIZER_ESPEAK_LIBRARY \"c:\\Program Files\\eSpeak NG\\libespeak-ng.dll\"\n   setx PHONEMIZER_ESPEAK_PATH \"c:\\Program Files\\eSpeak NG\"\n   ```\n\n3. **Install Python dependencies**\n\n   The requirements file includes the dependencies needed to run the model with PyTorch.\n   When using an ONNX decoder or a GGML model, some dependencies (such as PyTorch) are no longer required.\n\n   The inference is compatible and tested on `python>=3.11`.\n\n   ```bash\n   pip install -r requirements.txt\n   ```\n\n4. **(Optional) Install Llama-cpp-python to use the `GGUF` models.**\n\n   ```bash\n   pip install llama-cpp-python\n   ```\n\n   To run llama-cpp with GPU suport (CUDA, MPS) support please refer to:\n   https://pypi.org/project/llama-cpp-python/\n\n5. **(Optional) Install onnxruntime to use the `.onnx` decoder.**\n   If you want to run the onnxdecoder\n   ```bash\n   pip install onnxruntime\n   ```\n\n## Running the Model\n\nRun the basic example script to synthesize speech:\n\n```bash\npython -m examples.basic_example \\\n  --input_text \"My name is Dave, and um, I'm from London\" \\\n  --ref_audio samples/dave.wav \\\n  --ref_text samples/dave.txt\n```\n\nTo specify a particular model repo for the backbone or codec, add the `--backbone` argument. Available backbones are listed in [NeuTTS-Air huggingface collection](https://huggingface.co/collections/neuphonic/neutts-air-68cc14b7033b4",
      "default_branch": "main"
    },
    "fetched_at": "2025-10-19T02:42:44.543155"
  },
  {
    "basic_info": {
      "name": "bdh",
      "full_name": "pathwaycom/bdh",
      "owner": "pathwaycom",
      "description": "Baby Dragon Hatchling (BDH) â€“ Architecture and Code",
      "url": "https://github.com/pathwaycom/bdh",
      "clone_url": "https://github.com/pathwaycom/bdh.git",
      "ssh_url": "git@github.com:pathwaycom/bdh.git",
      "homepage": "",
      "created_at": "2025-09-30T12:05:01Z",
      "updated_at": "2025-10-19T00:14:41Z",
      "pushed_at": "2025-10-14T07:57:02Z"
    },
    "stats": {
      "stars": 3093,
      "forks": 109,
      "watchers": 3093,
      "open_issues": 2,
      "size": 1005
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 8721
      },
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "# Baby Dragon Hatchling\n\n## **Bridging the Gap Between Transformers and the Brain**\n\n**Baby Dragon Hatchling (BDH)** is a biologically inspired large language model architecture that connects principles of deep learning with the foundations of neuroscience. Developed by researchers at [Pathway](https://pathway.com), BDH provides a theoretical and practical framework for understanding the emergence of reasoning and generalization in artificial systems.\n\nThis repository contains the official implementation from the paper:\n> *A. Kosowski, P. UznaÅ„ski, J. Chorowski, Z. Stamirowska, M. Bartoszkiewicz.*\n> [_The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain_](https://doi.org/10.48550/arXiv.2509.26507), arXiv (2025).\n\n\n## Overview\n\nBDH represents a **scale-free, locally interacting network of neurons** capable of intrinsic reasoning dynamics. BDH scales like a Transformer on performance benchmarksâ€”yet retains full interpretability and theoretical grounding in the fine-grained dynamics of neuron interactions.\n\n**Key properties:**\n\n- **Scale-free network topology** mimicking biological connectivity\n- **Locally interacting neuron particles** with excitatory/inhibitory dynamics\n- **Hebbian working memory** based on synaptic plasticity, displaying monosemanticity\n- **GPU-friendly state-space formulation** for efficient implementation\n- **Interpretable activations** that are sparse and positive\n\nBDH formalizes a bridge between **neural computation and machine-based language understanding**. It shows how **macro reasoning behavior** in large AI models emerges from **micro-level neuron dynamics**, guided by principles of graph theory and local computation.\n\nEmpirically, BDH matches **GPT-2â€“scale Transformers** across language and translation tasks at equivalent parameter scales (10Mâ€“1B).\n\n\n***\n\n## Architecture\n\n<img src=\"figs/architecture.png\" width=\"600\"/>\n\n***\n\n## Relation to Transformers\n\n<img src=\"figs/vocab.png\" width=\"600\"/>\n\nBDH and the Transformer share attention-inspired computation; however, BDHâ€™s graph-based architecture makes its attention **emerge naturally from neuron-level interactions**, reflecting attention as seen in biological systems.\n\n***\n\n## Scaling Laws\n\n<img src=\"figs/bdh_scaling.png\" width=\"600\"/>\n\nBDH follows **Transformer-like scaling laws**, maintaining parameter efficiency while achieving interpretability at any scale.\n\n***\n\n## Installation and Training\n\n```bash\n# install dependencies\npip install -r requirements.txt\n\n# train BDH on a toy dataset\npython train.py\n```\n\n<!--For visualization and interpretability analysis, explore the example notebooks in `notebooks/`.-->\n\n\n\n## Learn and Discuss\n\n- Watch the *SuperDataScience podcast* [â–¶ï¸ *Dragon Hatchling: The Missing Link Between Transformers and the Brain*](https://www.youtube.com/watch?v=mfV44-mtg7c) (72 min.) featuring Adrianâ€¯Kosowski in conversation with Jonâ€¯Krohn, unpacking BDHâ€™s neuron-level architecture and sparse reasoning dynamics.\n\n- Read about BDH in\n[*Forbes*](https://www.forbes.com/sites/victordey/2025/10/08/can-ai-learn-and-evolve-like-a-brain-pathways-bold-research-thinks-so/),\n[*Semafor*](https://www.semafor.com/article/10/01/2025/new-ai-research-claims-to-be-getting-closer-to-modeling-human-brain),\n[*The Turing Post*](https://www.turingpost.com/p/fod-121-300-million-to-start-a-big-promise-for-science#the-freshest-research-papers-catego),\n[*Quantum Zeitgeist*](https://quantumzeitgeist.com/palo-alto-ai-firm-pathway-unveils-post-transformer-architecture-for-autonomous-ai/),\n[*Golem*](https://www.golem.de/news/neue-ki-architektur-was-ist-baby-dragon-hatchling-2510-201047-2.html),\nand elsewhere in the media.\n\n- Discuss and share the BDH paper on:\n[*Hugging Face Papers*](https://huggingface.co/papers/2509.26507), \n[*Alphaxiv*](https://alphaxiv.org/abs/2509.26507),\nand [*EmergentMind*](https://emergentmind.com/papers/2509.26507).\n\n## Community Forks\n\n- [adamskrodzki/bdh](https://github.com/adamskrodzki/bdh): dynamic vocabulary, stateful attention\n- [mosure/burn_dragon_hatchling](https://github.com/mosure/burn_dragon_hatchling): Burn port\n- [severian42/bdh](https://github.com/severian42/bdh): MLX port\n- [Git-Faisal/bdh](https://github.com/Git-Faisal/bdh)\n- [GrahLnn/bdh](https://github.com/GrahLnn/bdh)\n\n## Acknowledgements\nWe thank Andrej Karpathy for the [nanoGPT](https://github.com/karpathy/nanoGPT/) code and the tiny Shapespeare dataset used in this demonstration.\n\nBDH research stands at the intersection of **AI architecture**, **biological learning models**, and **theoretical computer science**â€”an effort to map the *equations of reasoning* between artificial and biological intelligence.\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-10-19T02:42:45.693035"
  },
  {
    "basic_info": {
      "name": "HunyuanImage-3.0",
      "full_name": "Tencent-Hunyuan/HunyuanImage-3.0",
      "owner": "Tencent-Hunyuan",
      "description": "HunyuanImage-3.0: A Powerful Native Multimodal Model for Image Generation",
      "url": "https://github.com/Tencent-Hunyuan/HunyuanImage-3.0",
      "clone_url": "https://github.com/Tencent-Hunyuan/HunyuanImage-3.0.git",
      "ssh_url": "git@github.com:Tencent-Hunyuan/HunyuanImage-3.0.git",
      "homepage": "https://hunyuan.tencent.com/image",
      "created_at": "2025-09-27T07:18:47Z",
      "updated_at": "2025-10-18T23:26:35Z",
      "pushed_at": "2025-10-14T08:42:04Z"
    },
    "stats": {
      "stars": 2231,
      "forks": 91,
      "watchers": 2231,
      "open_issues": 25,
      "size": 34784
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 374880,
        "Shell": 806
      },
      "license": "Other",
      "topics": [
        "image-generation",
        "native-multimodal-model"
      ]
    },
    "content": {
      "readme": "[ä¸­æ–‡æ–‡æ¡£](./README_zh_CN.md)\n\n<div align=\"center\">\n\n<img src=\"./assets/logo.png\" alt=\"HunyuanImage-3.0 Logo\" width=\"600\">\n\n# ğŸ¨ HunyuanImage-3.0: A Powerful Native Multimodal Model for Image Generation\n\n</div>\n\n\n<div align=\"center\">\n<img src=\"./assets/banner.png\" alt=\"HunyuanImage-3.0 Banner\" width=\"800\">\n\n</div>\n\n<div align=\"center\">\n  <a href=https://hunyuan.tencent.com/image target=\"_blank\"><img src=https://img.shields.io/badge/Official%20Site-333399.svg?logo=homepage height=22px></a>\n  <a href=https://huggingface.co/tencent/HunyuanImage-3.0 target=\"_blank\"><img src=https://img.shields.io/badge/%F0%9F%A4%97%20Models-d96902.svg height=22px></a>\n  <a href=https://github.com/Tencent-Hunyuan/HunyuanImage-3.0 target=\"_blank\"><img src= https://img.shields.io/badge/Page-bb8a2e.svg?logo=github height=22px></a>\n  <a href=https://arxiv.org/pdf/2509.23951 target=\"_blank\"><img src=https://img.shields.io/badge/Report-b5212f.svg?logo=arxiv height=22px></a>\n  <a href=https://x.com/TencentHunyuan target=\"_blank\"><img src=https://img.shields.io/badge/Hunyuan-black.svg?logo=x height=22px></a>\n  <a href=https://docs.qq.com/doc/DUVVadmhCdG9qRXBU target=\"_blank\"><img src=https://img.shields.io/badge/ğŸ“š-PromptHandBook-blue.svg?logo=book height=22px></a>\n</div>\n\n\n<p align=\"center\">\n    ğŸ‘ Join our <a href=\"./assets/WECHAT.md\" target=\"_blank\">WeChat</a> and <a href=\"https://discord.gg/ehjWMqF5wY\">Discord</a> | \nğŸ’» <a href=\"https://hunyuan.tencent.com/modelSquare/home/play?modelId=289&from=/visual\">Official website(å®˜ç½‘) Try our model!</a>&nbsp&nbsp\n</p>\n\n## ğŸ”¥ğŸ”¥ğŸ”¥ News\n- **September 28, 2025**: ğŸ“– **HunyuanImage-3.0 Technical Report Released** - Comprehensive technical documentation now available\n- **September 28, 2025**: ğŸš€ **HunyuanImage-3.0 Open Source Release** - Inference code and model weights publicly available\n\n\n## ğŸ§© Community Contributions\n\nIf you develop/use HunyuanImage-3.0 in your projects, welcome to let us know.\n\n## ğŸ“‘ Open-source Plan\n\n- HunyuanImage-3.0 (Image Generation Model)\n  - [x] Inference \n  - [x] HunyuanImage-3.0 Checkpoints\n  - [ ] HunyuanImage-3.0-Instruct Checkpoints (with reasoning)\n  - [ ] VLLM Support\n  - [ ] Distilled Checkpoints\n  - [ ] Image-to-Image Generation\n  - [ ] Multi-turn Interaction\n\n\n## ğŸ—‚ï¸ Contents\n- [ğŸ”¥ğŸ”¥ğŸ”¥ News](#-news)\n- [ğŸ§© Community Contributions](#-community-contributions)\n- [ğŸ“‘ Open-source Plan](#-open-source-plan)\n- [ğŸ“– Introduction](#-introduction)\n- [âœ¨ Key Features](#-key-features)\n- [ğŸ› ï¸ Dependencies and Installation](#-dependencies-and-installation)\n  - [ğŸ’» System Requirements](#-system-requirements)\n  - [ğŸ“¦ Environment Setup](#-environment-setup)\n  - [ğŸ“¥ Install Dependencies](#-install-dependencies)\n  - [Performance Optimizations](#performance-optimizations)\n- [ğŸš€ Usage](#-usage)\n  - [ğŸ”¥ Quick Start with Transformers](#-quick-start-with-transformers)\n  - [ğŸ  Local Installation & Usage](#-local-installation--usage)\n  - [ğŸ¨ Interactive Gradio Demo](#-interactive-gradio-demo)\n- [ğŸ§± Models Cards](#-models-cards)\n- [ğŸ“ Prompt Guide](#-prompt-guide)\n  - [Manually Writing Prompts](#manually-writing-prompts)\n  - [System Prompt For Automatic Rewriting the Prompt](#system-prompt-for-automatic-rewriting-the-prompt)\n  - [Advanced Tips](#advanced-tips)\n  - [More Cases](#more-cases)\n- [ğŸ“Š Evaluation](#-evaluation)\n- [ğŸ“š Citation](#-citation)\n- [ğŸ™ Acknowledgements](#-acknowledgements)\n- [ğŸŒŸğŸš€  Github Star History](#-github-star-history)\n\n---\n\n## ğŸ“– Introduction\n\n**HunyuanImage-3.0** is a groundbreaking native multimodal model that unifies multimodal understanding and generation within an autoregressive framework. Our text-to-image module achieves performance **comparable to or surpassing** leading closed-source models.\n\n\n<div align=\"center\">\n  <img src=\"./assets/framework.png\" alt=\"HunyuanImage-3.0 Framework\" width=\"90%\">\n</div>\n\n## âœ¨ Key Features\n\n* ğŸ§  **Unified Multimodal Architecture:** Moving beyond the prevalent DiT-based architectures, HunyuanImage-3.0 employs a unified autoregressive framework. This design enables a more direct and integrated modeling of text and image modalities, leading to surprisingly effective and contextually rich image generation.\n\n* ğŸ† **The Largest Image Generation MoE Model:** This is the largest open-source image generation Mixture of Experts (MoE) model to date. It features 64 experts and a total of 80 billion parameters, with 13 billion activated per token, significantly enhancing its capacity and performance.\n\n* ğŸ¨ **Superior Image Generation Performance:** Through rigorous dataset curation and advanced reinforcement learning post-training, we've achieved an optimal balance between semantic accuracy and visual excellence. The model demonstrates exceptional prompt adherence while delivering photorealistic imagery with stunning aesthetic quality and fine-grained details.\n\n* ğŸ’­ **Intelligent World-Knowledge Reasoning:** The unified multimodal architecture endows HunyuanImage-3.0 with powerful reasoning capabilities. It leverages its extensive world knowledge to intelligently interpret user int",
      "default_branch": "main"
    },
    "fetched_at": "2025-10-19T02:42:46.828378"
  },
  {
    "basic_info": {
      "name": "dexter",
      "full_name": "virattt/dexter",
      "owner": "virattt",
      "description": "An autonomous agent for deep financial research",
      "url": "https://github.com/virattt/dexter",
      "clone_url": "https://github.com/virattt/dexter.git",
      "ssh_url": "git@github.com:virattt/dexter.git",
      "homepage": null,
      "created_at": "2025-10-14T21:02:00Z",
      "updated_at": "2025-10-19T02:31:49Z",
      "pushed_at": "2025-10-18T00:35:08Z"
    },
    "stats": {
      "stars": 1576,
      "forks": 183,
      "watchers": 1576,
      "open_issues": 5,
      "size": 102
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 50952,
        "JavaScript": 228
      },
      "license": null,
      "topics": []
    },
    "content": {
      "readme": "# Dexter ğŸ¤–\n\nDexter is an autonomous financial research agent that thinks, plans, and learns as it works. It performs analysis using task planning, self-reflection, and real-time market data. Think Claude Code, but built specifically for financial research.\n\n\n<img width=\"979\" height=\"651\" alt=\"Screenshot 2025-10-14 at 6 12 35â€¯PM\" src=\"https://github.com/user-attachments/assets/5a2859d4-53cf-4638-998a-15cef3c98038\" />\n\n## Overview\n\nDexter takes complex financial questions and turns them into clear, step-by-step research plans. It runs those tasks using live market data, checks its own work, and refines the results until it has a confident, data-backed answer.  \n\nItâ€™s not just another chatbot.  Itâ€™s an agent that plans ahead, verifies its progress, and keeps iterating until the job is done.\n\n**Key Capabilities:**\n- **Intelligent Task Planning**: Automatically decomposes complex queries into structured research steps\n- **Autonomous Execution**: Selects and executes the right tools to gather financial data\n- **Self-Validation**: Checks its own work and iterates until tasks are complete\n- **Real-Time Financial Data**: Access to income statements, balance sheets, and cash flow statements\n- **Safety Features**: Built-in loop detection and step limits to prevent runaway execution\n\n[![Twitter Follow](https://img.shields.io/twitter/follow/virattt?style=social)](https://twitter.com/virattt)\n\n## Quick Start\n\n### Prerequisites\n\n- Python 3.10 or higher\n- [uv](https://github.com/astral-sh/uv) package manager\n- OpenAI API key\n- Financial Datasets API key (get one at [financialdatasets.ai](https://financialdatasets.ai))\n\n### Installation\n\n1. Clone the repository:\n```bash\ngit clone https://github.com/virattt/dexter.git\ncd dexter\n```\n\n2. Install dependencies with uv:\n```bash\nuv sync\n```\n\n3. Set up your environment variables:\n```bash\n# Copy the example environment file\ncp env.example .env\n\n# Edit .env and add your API keys\n# OPENAI_API_KEY=your-openai-api-key\n# FINANCIAL_DATASETS_API_KEY=your-financial-datasets-api-key\n```\n\n### Usage\n\nRun Dexter in interactive mode:\n```bash\nuv run dexter-agent\n```\n\n### Example Queries\n\nTry asking Dexter questions like:\n- \"What was Apple's revenue growth over the last 4 quarters?\"\n- \"Compare Microsoft and Google's operating margins for 2023\"\n- \"Analyze Tesla's cash flow trends over the past year\"\n- \"What is Amazon's debt-to-equity ratio based on recent financials?\"\n\nDexter will automatically:\n1. Break down your question into research tasks\n2. Fetch the necessary financial data\n3. Perform calculations and analysis\n4. Provide a comprehensive, data-rich answer\n\n## Architecture\n\nDexter uses a multi-agent architecture with specialized components:\n\n- **Planning Agent**: Analyzes queries and creates structured task lists\n- **Action Agent**: Selects appropriate tools and executes research steps\n- **Validation Agent**: Verifies task completion and data sufficiency\n- **Answer Agent**: Synthesizes findings into comprehensive responses\n\n## Project Structure\n\n```\ndexter/\nâ”œâ”€â”€ src/\nâ”‚   â”œâ”€â”€ dexter/\nâ”‚   â”‚   â”œâ”€â”€ agent.py      # Main agent orchestration logic\nâ”‚   â”‚   â”œâ”€â”€ model.py      # LLM interface\nâ”‚   â”‚   â”œâ”€â”€ tools.py      # Financial data tools\nâ”‚   â”‚   â”œâ”€â”€ prompts.py    # System prompts for each component\nâ”‚   â”‚   â”œâ”€â”€ schemas.py    # Pydantic models\nâ”‚   â”‚   â”œâ”€â”€ utils/        # Utility functions\nâ”‚   â”‚   â””â”€â”€ cli.py        # CLI entry point\nâ”œâ”€â”€ pyproject.toml\nâ””â”€â”€ uv.lock\n```\n\n## Configuration\n\nDexter supports configuration via the `Agent` class initialization:\n\n```python\nfrom dexter.agent import Agent\n\nagent = Agent(\n    max_steps=20,              # Global safety limit\n    max_steps_per_task=5       # Per-task iteration limit\n)\n```\n\n## How to Contribute\n\n1. Fork the repository\n2. Create a feature branch\n3. Commit your changes\n4. Push to the branch\n5. Create a Pull Request\n\n**Important**: Please keep your pull requests small and focused.  This will make it easier to review and merge.\n\n\n## License\n\nThis project is licensed under the MIT License.\n\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-10-19T02:42:47.948885"
  },
  {
    "basic_info": {
      "name": "agentic-design-patterns-cn",
      "full_name": "ginobefun/agentic-design-patterns-cn",
      "owner": "ginobefun",
      "description": "ã€ŠAgentic Design Patternsã€‹ä¸­æ–‡ç¿»è¯‘ç‰ˆ",
      "url": "https://github.com/ginobefun/agentic-design-patterns-cn",
      "clone_url": "https://github.com/ginobefun/agentic-design-patterns-cn.git",
      "ssh_url": "git@github.com:ginobefun/agentic-design-patterns-cn.git",
      "homepage": null,
      "created_at": "2025-10-09T04:36:28Z",
      "updated_at": "2025-10-19T02:38:57Z",
      "pushed_at": "2025-10-19T01:53:18Z"
    },
    "stats": {
      "stars": 1344,
      "forks": 157,
      "watchers": 1344,
      "open_issues": 1,
      "size": 3768
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 85942
      },
      "license": null,
      "topics": []
    },
    "content": {
      "readme": "[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/ginobefun-agentic-design-patterns-cn-badge.png)](https://mseep.ai/app/ginobefun-agentic-design-patterns-cn)\n\n# Agentic Design Patterns | <mark>æ™ºèƒ½ä½“è®¾è®¡æ¨¡å¼</mark>\n\n## A Hands-On Guide to Building Intelligent Systems | <mark>æ„å»ºæ™ºèƒ½ç³»ç»Ÿçš„å®è·µæŒ‡å—</mark>\n\n[![License: CC BY-NC 4.0](https://img.shields.io/badge/License-CC%20BY--NC%204.0-lightgrey.svg)](https://creativecommons.org/licenses/by-nc/4.0/)\n[![GitHub stars](https://img.shields.io/github/stars/ginobefun/agentic-design-patterns-cn)](https://github.com/ginobefun/agentic-design-patterns-cn/stargazers)\n[![GitHub forks](https://img.shields.io/github/forks/ginobefun/agentic-design-patterns-cn)](https://github.com/ginobefun/agentic-design-patterns-cn/network)\n\n**åŸä¹¦ä½œè€… (Author)**: [Antonio Gulli](https://www.linkedin.com/in/searchguy/)\n\n**åŸä¹¦é“¾æ¥ (Original Book)**: [Amazon](https://www.amazon.com/Agentic-Design-Patterns-Hands-Intelligent/dp/3032014018/)\n\n**åŸå§‹æ–‡æ¡£é“¾æ¥ (Original Book Link)**: [Google Docs](https://docs.google.com/document/d/1rsaK53T3Lg5KoGwvf8ukOUvbELRtH-V0LnOIFDxBryE/preview?tab=t.0#heading=h.pxcur8v2qagu)\n\n---\n\n## ğŸ“– é¡¹ç›®ç®€ä»‹ | Project Description\n\næœ¬é¡¹ç›®æ˜¯å¯¹ Antonio Gulli æ‰€è‘—ã€ŠAgentic Design Patterns: A Hands-On Guide to Building Intelligent Systemsã€‹çš„**ä¸­è‹±æ–‡å¯¹ç…§ç¿»è¯‘**ã€‚è¯¥ä¹¦æ˜¯ä¸€éƒ¨å…¨é¢çš„æŠ€æœ¯æŒ‡å—ï¼Œæ¶µç›–äº†ç°ä»£äººå·¥æ™ºèƒ½ç³»ç»Ÿä¸­æ™ºèƒ½ä½“ (Agent) è®¾è®¡çš„æ ¸å¿ƒæ¦‚å¿µå’Œå®è·µæ–¹æ³•ã€‚\n\nThis project is a **bilingual Chinese-English translation** of \"Agentic Design Patterns: A Hands-On Guide to Building Intelligent Systems\" by Antonio Gulli. The book is a comprehensive technical guide covering core concepts and practical approaches to agent design in modern AI systems.\n\n---\n\n## ğŸ¯ é¡¹ç›®ç‰¹è‰² | Key Features\n\n- ğŸ“š **ä¸­è‹±æ–‡å¯¹ç…§** - å®Œæ•´çš„åŒè¯­å¯¹ç…§ç¿»è¯‘\n- ğŸ¨ **é«˜äº®æ˜¾ç¤º** - ä¸­æ–‡å†…å®¹ä½¿ç”¨é»„è‰²é«˜äº®ï¼Œæ˜“äºåŒºåˆ†\n- ğŸ“ **æ ¼å¼è§„èŒƒ** - ä¸¥æ ¼éµå¾ª Markdown æ ‡å‡†å’Œç¿»è¯‘è§„èŒƒ\n- ğŸ”— **ä»£ç é“¾æ¥** - ä¿ç•™æ‰€æœ‰åŸä¹¦ä»£ç ç¤ºä¾‹é“¾æ¥\n- âš¡ **æŒç»­æ›´æ–°** - é€ç« ç¿»è¯‘ï¼ŒæŒç»­æ›´æ–°è¿›åº¦\n\n---\n\n## ğŸ“‹ ç¿»è¯‘è¿›åº¦ | Translation Progress\n\n**<mark>æ€»é¡µæ•°ï¼š424 é¡µ</mark>** | **Total: 424 Pages**\n\n### å‰ç½®å†…å®¹ | Front Matter\n\n| ç« èŠ‚ | æ¦‚è¿° | è´Ÿè´£äºº | AI ç¿»è¯‘ | äººå·¥è¯„å®¡ | äº¤å‰è¯„å®¡ |\n|------|------|--------|---------|----------|----------|\n| [çŒ®è¾](01-Dedication.md) | ä½œè€…çš„çŒ®è¾ä¸è‡´æ•¬ | @ginobefun | âœ… | âœ… | â³ |\n| [è‡´è°¢](02-Acknowledgment.md) | è‡´è°¢ä¸æ„Ÿè°¢åå• | @ginobefun | âœ… | âœ… | â³ |\n| [åºè¨€](03-Foreword.md) | æœ¬ä¹¦çš„åºè¨€ä¸èƒŒæ™¯ä»‹ç» | @ginobefun | âœ… | âœ… | â³ |\n| [æ€æƒ³é¢†è¢–çš„æ´è§](04-Thought-Leader.md) | æƒåŠ›ä¸è´£ä»»çš„æ·±åº¦æ€è€ƒ | @ginobefun | âœ… | âœ… | â³ |\n| [ä»‹ç»](05-Introduction.md) | å…¨ä¹¦å¼•è¨€ä¸æ ¸å¿ƒæ¦‚å¿µ | @ginobefun | âœ… | âœ… | â³ |\n| [ä»€ä¹ˆæ˜¯\"æ™ºèƒ½ä½“\"ï¼Ÿ](06-What-Makes-Agent.md) | å®šä¹‰ AI ç³»ç»Ÿçš„\"æ™ºèƒ½ä½“\"ç‰¹å¾ | @ginobefun | âœ… | âœ… | â³ |\n\n### ç¬¬ä¸€éƒ¨åˆ†ï¼šæ ¸å¿ƒè®¾è®¡æ¨¡å¼ | Part One: Core Patterns (103 é¡µ)\n\n| ç« èŠ‚ | è®¾è®¡æ¨¡å¼æ¦‚è¿° | è´Ÿè´£äºº | AI ç¿»è¯‘ | äººå·¥è¯„å®¡ | äº¤å‰è¯„å®¡ |\n|------|-------------|--------|---------|----------|----------|\n| [ç¬¬ 1 ç« ï¼šæç¤ºé“¾](07-Chapter-01-Prompt-Chaining.md) | åˆ†è€Œæ²»ä¹‹çš„ä»»åŠ¡åˆ†è§£æ¨¡å¼ï¼Œå°†å¤æ‚ä»»åŠ¡åˆ†è§£ä¸ºå¤„ç†æµæ°´çº¿ | @ginobefun | âœ… | âœ… | â³ |\n| [ç¬¬ 2 ç« ï¼šè·¯ç”±](08-Chapter-02-Routing.md) | æ™ºèƒ½å†³ç­–ä¸åŠ¨æ€åˆ†å‘ï¼Œæ ¹æ®æƒ…å¢ƒé€‰æ‹©æœ€ä½³è¡ŒåŠ¨è·¯å¾„ | @ginobefun | âœ… | âœ… | â³ |\n| [ç¬¬ 3 ç« ï¼šå¹¶è¡ŒåŒ–](09-Chapter-03-Parallelization.md) | å¹¶å‘æ‰§è¡Œä¸æ€§èƒ½æå‡ï¼ŒåŒæ—¶æ‰§è¡Œå¤šä¸ªç‹¬ç«‹ä»»åŠ¡ | @ginobefun | âœ… | âœ… | â³ |\n| [ç¬¬ 4 ç« ï¼šåæ€](10-Chapter-04-Reflection.md) | è‡ªæˆ‘è¯„ä¼°å’Œè¿­ä»£æ”¹è¿›ï¼Œé€šè¿‡åé¦ˆå¾ªç¯ä¼˜åŒ–è¾“å‡ºè´¨é‡ | @ginobefun | âœ… | âœ… | â³ |\n| [ç¬¬ 5 ç« ï¼šå·¥å…·ä½¿ç”¨](11-Chapter-05-Tool-Use.md) | å¤–éƒ¨å·¥å…·ä¸ API é›†æˆï¼Œæ‰©å±•æ™ºèƒ½ä½“èƒ½åŠ›è¾¹ç•Œ | @ginobefun | âœ… | âœ… | â³ |\n| [ç¬¬ 6 ç« ï¼šè§„åˆ’](12-Chapter-06-Planning.md) | å¤šæ­¥éª¤è®¡åˆ’åˆ¶å®šä¸æ‰§è¡Œï¼Œå®ç°å¤æ‚ç›®æ ‡åˆ†è§£ | @ginobefun | âœ… | âŒ | âŒ |\n| [ç¬¬ 7 ç« ï¼šå¤šæ™ºèƒ½ä½“åä½œ](13-Chapter-07-Multi-Agent-Collaboration.md) | ååŒå·¥ä½œæ¶æ„ï¼Œå¤šä¸ªæ™ºèƒ½ä½“é…åˆå®Œæˆä»»åŠ¡ | @ginobefun | âœ…  | âŒ | âŒ |\n\n### ç¬¬äºŒéƒ¨åˆ†ï¼šé«˜çº§è®¾è®¡æ¨¡å¼ | Part Two: Advanced Patterns (61 é¡µ)\n\n| ç« èŠ‚ | è®¾è®¡æ¨¡å¼æ¦‚è¿° | è´Ÿè´£äºº | AI ç¿»è¯‘ | äººå·¥è¯„å®¡ | äº¤å‰è¯„å®¡ |\n|------|-------------|--------|---------|----------|----------|\n| [ç¬¬ 8 ç« ï¼šè®°å¿†ç®¡ç†](14-Chapter-08-Memory-Management.md) | çŸ­æœŸå’Œé•¿æœŸè®°å¿†ç®¡ç†ï¼Œç»´æŒä¸Šä¸‹æ–‡è¿ç»­æ€§ | @éƒ‘æ¶› | âœ… | âœ… | âŒ |\n| [ç¬¬ 9 ç« ï¼šå­¦ä¹ ä¸é€‚åº”](15-Chapter-09-Learning-and-Adaptation.md) | ä»ç»éªŒä¸­å­¦ä¹ ï¼ŒæŒç»­ä¼˜åŒ–æ™ºèƒ½ä½“è¡Œä¸º | @é™ˆè¯—ä¸­ | â³ | âŒ | âŒ |\n| [ç¬¬ 10 ç« ï¼šæ¨¡å‹ä¸Šä¸‹æ–‡åè®®](16-Chapter-10-Model-Context-Protocol.md) | æ ‡å‡†åŒ–äº¤äº’åè®®ï¼Œè§„èŒƒæ™ºèƒ½ä½“é€šä¿¡æ–¹å¼ | @éƒ‘æ¶› | â³ | âŒ | âŒ |\n| [ç¬¬ 11 ç« ï¼šç›®æ ‡è®¾å®šä¸ç›‘æ§](17-Chapter-11-Goal-Setting-and-Monitoring.md) | åŠ¨æ€ç›®æ ‡ç®¡ç†ï¼Œå®æ—¶è¿½è¸ªä»»åŠ¡è¿›å±• | @ææµªæºª | â³ | âŒ | âŒ |\n\n### ç¬¬ä¸‰éƒ¨åˆ†ï¼šé›†æˆè®¾è®¡æ¨¡å¼ | Part Three: Integration Patterns (34 é¡µ)\n\n| ç« èŠ‚ | è®¾è®¡æ¨¡å¼æ¦‚è¿° | è´Ÿè´£äºº | AI ç¿»è¯‘ | äººå·¥è¯„å®¡ | äº¤å‰è¯„å®¡ |\n|------|-------------|--------|---------|----------|----------|\n| [ç¬¬ 12 ç« ï¼šå¼‚å¸¸å¤„ç†ä¸æ¢å¤](18-Chapter-12-Exception-Handling-and-Recovery.md) | ä¼˜é›…é”™è¯¯å¤„ç†ï¼Œç¡®ä¿ç³»ç»Ÿç¨³å®šæ€§ | å¾…å®š | âŒ | âŒ | âŒ |\n| [ç¬¬ 13 ç« ï¼šäººæœºåä½œ](19-Chapter-13-Human-in-the-Loop.md) | äººæœºåä½œå†³ç­–ï¼Œèåˆäººç±»æ™ºæ…§ä¸ AI èƒ½åŠ› | @æ›¾æ±‰ | âœ… | âœ… | â³ |\n| [ç¬¬ 14 ç« ï¼šçŸ¥è¯†æ£€ç´¢ (RAG)](20-Chapter-14-Knowledge-Retrieval.md) | æ£€ç´¢å¢å¼ºç”ŸæˆæŠ€æœ¯ï¼Œç»“åˆå¤–éƒ¨çŸ¥è¯†åº“ | @EE | âœ… | âœ… | â³ |\n\n### ç¬¬å››éƒ¨åˆ†ï¼šç”Ÿäº§è®¾è®¡æ¨¡å¼ | Part Four: Production Patterns (114 é¡µ)\n\n| ç« èŠ‚ | è®¾è®¡æ¨¡å¼æ¦‚è¿° | è´Ÿè´£äºº | AI ç¿»è¯‘ | äººå·¥è¯„å®¡ | äº¤å‰è¯„å®¡ |\n|------|-------------|--------|---------|----------|----------|\n| [ç¬¬ 15 ç« ï¼šæ™ºèƒ½ä½“é—´é€šä¿¡ (A2A)](21-Chapter-15-Inter-Agent-Communication.md) | æ™ºèƒ½ä½“é€šä¿¡åè®®ï¼Œå®ç°æ™ºèƒ½ä½“é—´é«˜æ•ˆäº¤äº’ | @æœµæœµè‚¥ | âœ… | âŒ | âŒ |\n| [ç¬¬ 16 ç« ï¼šèµ„æºæ„ŸçŸ¥ä¼˜åŒ–](22-Chapter-16-Resource-Aware-Optimization.md) | èµ„æºä¼˜åŒ–ç®¡ç†ï¼Œå¹³è¡¡æ€§èƒ½ä¸æˆæœ¬ | @IsaacZhaoo | âŒ | âŒ | âŒ |\n| [ç¬¬ 17 ç« ï¼šæ¨ç†æŠ€æœ¯](23-Chapter-17-Reasoning-Techniques.md) | å¢å¼ºæ¨ç†èƒ½åŠ›ï¼Œæå‡å†³ç­–è´¨é‡ | @Diqing | âŒ | âŒ | âŒ |\n| [ç¬¬ 18 ç« ï¼šæŠ¤æ /å®‰å…¨æ¨¡å¼](24-Chapter-18-Guardrails-Safety-Patterns.md) | å®‰å…¨ä¿éšœæœºåˆ¶ï¼Œé˜²æ­¢ä¸å½“è¡Œä¸º | å¾…å®š | âŒ | âŒ | âŒ |\n| [ç¬¬ 19 ç« ï¼šè¯„ä¼°ä¸ç›‘æ§](25-Chapter-19-Evaluation-and-Monitoring.md) | æ€§èƒ½è¯„ä¼°ä½“ç³»ï¼Œé‡åŒ–æ™ºèƒ½ä½“è¡¨ç° | @æœµæœµè‚¥ | âŒ | âŒ | âŒ |\n| [ç¬¬ 20 ç« ï¼šä¼˜å…ˆçº§æ’åº](26-Chapter-20-Priori",
      "default_branch": "main"
    },
    "fetched_at": "2025-10-19T02:42:49.068261"
  },
  {
    "basic_info": {
      "name": "DreamOmni2",
      "full_name": "dvlab-research/DreamOmni2",
      "owner": "dvlab-research",
      "description": "This project is the official implementation of 'DreamOmni2: Multimodal Instruction-based Editing and Generation''",
      "url": "https://github.com/dvlab-research/DreamOmni2",
      "clone_url": "https://github.com/dvlab-research/DreamOmni2.git",
      "ssh_url": "git@github.com:dvlab-research/DreamOmni2.git",
      "homepage": "",
      "created_at": "2025-09-28T05:20:51Z",
      "updated_at": "2025-10-19T02:37:16Z",
      "pushed_at": "2025-10-11T05:23:18Z"
    },
    "stats": {
      "stars": 1217,
      "forks": 117,
      "watchers": 1217,
      "open_issues": 13,
      "size": 15838
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 142712
      },
      "license": "Apache License 2.0",
      "topics": [
        "image-editing",
        "image-generation",
        "unified-generation-editing-model"
      ]
    },
    "content": {
      "readme": "# DreamOmni2: Multimodal Instruction-based Editing and Generation\n\n<p align=\"center\">\n    <a href=\"https://arxiv.org/html/2510.06679v1\">\n            <img alt=\"Build\" src=\"https://img.shields.io/badge/arXiv%20paper-2510.06679v1-b31b1b.svg\">\n    </a>\n    <a href=\"https://pbihao.github.io/projects/DreamOmni2/index.html\">\n        <img alt=\"Project Page\" src=\"https://img.shields.io/badge/Project-Page-blue\">\n    </a>\n    <a href=\"https://www.youtube.com/watch?v=8xpoiRK57uU\">\n        <img alt=\"Video Demo\" src=\"https://img.shields.io/badge/Video-Demo-red\">\n    </a>\n    <a href=\"https://huggingface.co/datasets/xiabs/DreamOmni2Bench\">\n        <img alt=\"Build\" src=\"https://img.shields.io/badge/DreamOmni2-Benchmark-green\">\n    </a>\n    <a href=\"https://huggingface.co/xiabs/DreamOmni2\">\n        <img alt=\"Build\" src=\"https://img.shields.io/badge/ğŸ¤—-HF%20Model-yellow\">\n    </a>    \n    <a href=\"https://huggingface.co/spaces/wcy1122/DreamOmni2-Edit\">\n        <img alt=\"Build\" src=\"https://img.shields.io/badge/ğŸ¤—-HF%20Editing%20Demo-yellow\">\n    </a>\n    <a href=\"https://huggingface.co/spaces/wcy1122/DreamOmni2-Gen\">\n        <img alt=\"Build\" src=\"https://img.shields.io/badge/ğŸ¤—-HF%20Generation%20Demo-yellow\">\n    </a>\n</p>\n\n## ğŸ”¥ News\n- ğŸ”¥**2025.10.10**: Release DreamOmni2 [editing demo](https://huggingface.co/spaces/wcy1122/DreamOmni2-Edit) and [generation demo](https://huggingface.co/spaces/wcy1122/DreamOmni2-Gen)\n- ğŸ”¥**2025.10.10**: Release DreamOmni2 [Benchmark](https://huggingface.co/datasets/xiabs/DreamOmni2Bench).\n- ğŸ”¥**2025.10.10**: Release DreamOmni2's [codes](https://github.com/dvlab-research/DreamOmni2) and [models](https://huggingface.co/xiabs/DreamOmni2).\n- ğŸ”¥**2025.10.09**: Release DreamOmni2 [tech report](https://arxiv.org/html/2510.06679v1).\n\n\n<p align=\"center\">\n  <img width=\"600\" src=\"imgs/gallery.png\">\n</p>\n\n\n<div align=\"center\">\n  <a href=\"https://cloud.video.taobao.com/vod/HxWB8i8sYkh0DdfvfByoMHqRtezNMCpWJdjzWTOCqdY.mp4\">\n    <img src=\"imgs/cover.png\" alt=\"Watch the video\" style=\"width: 600px;\">\n  </a>\n</div>\n\n\n## Introduction\n\n**(1) Multimodal Instruction-based Generation**\n\nFor traditional subject-driven generation based on concrete objects, DreamOmni2 achieves the best results among open-source models, showing superior identity and pose consistency. Additionally, DreamOmni2 can reference abstract attributes (such as material, texture, makeup, hairstyle, posture, design style, artistic style, etc.), even surpassing commercial models in this area.\n\n**(2) Multimodal Instruction-based Editing**\n\nBeyond traditional instruction-based editing models, DreamOmni2 supports multimodal instruction editing. In everyday editing tasks, there are often elements that are difficult to describe purely with language and require reference images. Our model addresses this need, supporting references to any concrete objects and abstract attributes, with performance comparable to commercial models.\n\n**(3) Unified Generation and Editing Model**\n\nBuilding upon these two new tasks, we introduce DreamOmni2, which is capable of multimodal instruction-based editing and generation under any concrete or abstract concept guidance. Overall, DreamOmni2 is a more intelligent and powerful open-sourced unified generation and editing model, offering enhanced capabilities across a wide range of tasks.\n\n## Editing and Generation Model?\nEditing and generation are distinct tasks. Editing requires strict consistency in preserving the non-edited areas of the source image, while generation only needs to retain the ID, IP, or attribution from the reference image as per the instructions, allowing the entire image to be regenerated with a focus on aesthetics. Weâ€™ve found that the instructions for generation and editing are often similar, so weâ€™ve separated these two tasks to make it easier for users to choose the appropriate task type.\n\n## Quick Start\n\n### Requirements and Installation\n\nFirst, install the necessary dependencies:\n```bash\ngit clone https://github.com/dvlab-research/DreamOmni2\ncd ./DreamOmni2\npip install -r requirements.txt\n```\n\nNext, download the DreamOmni2 weights into the models folder.\n\n```bash\nhuggingface-cli download --resume-download --local-dir-use-symlinks False xiabs/DreamOmni2 --local-dir ./models\n```\n\n### Inference\n\nMultimodal Instriction-based Editing\n\n**Notably, for editing tasks, due to the format settings of the training data, we need to place the image to be edited in the first position.**\n\n```bash\npython3 /mnt/bn/unifygen/xiabin_dev/iclr2026/DreamOmni2/inference_edit.py \\\n    --input_img_path \"example_input/edit_tests/src.jpg\" \"example_input/edit_tests/ref.jpg\" \\\n    --input_instruction \"Make the woman from the second image stand on the road in the first image.\" \\\n    --output_path \"example_input/edit_tests/edit_res.png\"\n```\n\nMultimodal Instriction-based Generation\n```bash\npython3 /mnt/bn/unifygen/xiabin_dev/iclr2026/DreamOmni2/inference_gen.py \\\n    --input_img_path \"example_input/gen_tests/img1.jpg\" \"example_input/gen_tests/",
      "default_branch": "main"
    },
    "fetched_at": "2025-10-19T02:42:50.168490"
  },
  {
    "basic_info": {
      "name": "RAE",
      "full_name": "bytetriper/RAE",
      "owner": "bytetriper",
      "description": "Official PyTorch Implementation of \"Diffusion Transformers with Representation Autoencoders\"",
      "url": "https://github.com/bytetriper/RAE",
      "clone_url": "https://github.com/bytetriper/RAE.git",
      "ssh_url": "git@github.com:bytetriper/RAE.git",
      "homepage": null,
      "created_at": "2025-09-28T07:29:37Z",
      "updated_at": "2025-10-19T00:30:24Z",
      "pushed_at": "2025-10-15T09:15:27Z"
    },
    "stats": {
      "stars": 1171,
      "forks": 26,
      "watchers": 1171,
      "open_issues": 1,
      "size": 282
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 245415
      },
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "## Diffusion Transformers with Representation Autoencoders (RAE)<br><sub>Official PyTorch Implementation</sub>\n\n### [Paper](https://arxiv.org/abs/2510.11690) | [Project Page](https://rae-dit.github.io/) \n\n\nThis repository contains **PyTorch/GPU** and **TorchXLA/TPU** implementations of our paper: \nDiffusion Transformers with Representation Autoencoders. For JAX/TPU implementation, please refer to [diffuse_nnx](https://github.com/willisma/diffuse_nnx)\n\n> [**Diffusion Transformers with Representation Autoencoders**](https://arxiv.org/abs/2510.11690)<br>\n> [Boyang Zheng](https://bytetriper.github.io/), [Nanye Ma](https://willisma.github.io), [Shengbang Tong](https://tsb0601.github.io/),  [Saining Xie](https://www.sainingxie.com)\n> <br>New York University<br>\n\nWe present Representation Autoencoders (RAE), a class of autoencoders that utilize  pretrained, frozen representation encoders such as [DINOv2](https://arxiv.org/abs/2304.07193) and [SigLIP2](https://arxiv.org/abs/2502.14786) as encoders with trained ViT decoders. RAE can be used in a two-stage training pipeline for high-fidelity image synthesis, where a Stage 2 diffusion model is trained on the latent space of a pretrained RAE to generate images.\n\nThis repository contains:\n\nPyTorch/GPU:\n* A PyTorch implementation of RAE and pretrained weights.\n* A PyTorch implementation of LightningDiT, DiT<sup>DH</sup> and pretrained weights.\n* Training and sampling scripts for the two-stage RAE+DiT pipeline.\n\nTorchXLA/TPU:\n* A TPU implementation of RAE and pretrained weights.\n* Sampling of RAE and DiT<sup>DH</sup> on TPU.\n\n## Environment\n\n### Dependency Setup\n1. Create environment and install via `uv`:\n   ```bash\n   conda create -n rae python=3.10 -y\n   conda activate rae\n   pip install uv\n   \n   # Install PyTorch 2.2.0 with CUDA 12.1\n   uv pip install torch==2.2.0 torchvision==0.17.0 torchaudio --index-url https://download.pytorch.org/whl/cu121\n   \n   # Install other dependencies\n   uv pip install timm==0.9.16 accelerate==0.23.0 torchdiffeq==0.2.5 wandb\n   uv pip install \"numpy<2\" transformers einops omegaconf\n   ```\n\n## Data & Model Preparation\n\n### Download Pre-trained Models\n\nWe release three kind of models: RAE decoders, DiT<sup>DH</sup> diffusion transformers and stats for latent normalization. To download all models at once:\n\n\n```bash\n\ncd RAE\npip install huggingface_hub\nhf download nyu-visionx/RAE-collections \\\n  --local-dir models \n```\n\n\nTo download specific models, run:\n```bash\nhf download nyu-visionx/RAE-collections \\\n  <remote_model_path> \\\n  --local-dir models \n```\n\n### Prepare Dataset\n\n1. Download ImageNet-1k.\n2. Point Stage 1 and Stage 2 scripts to the training split via `--data-path`.\n\n\n## Config-based Initialization\n\nAll training and sampling entrypoints are driven by OmegaConf YAML files. A\nsingle config describes the Stage 1 autoencoder, the Stage 2 diffusion model,\nand the solver used during training or inference. A minimal example looks like:\n\n```yaml\nstage_1:\n   target: stage1.RAE\n   params: { ... }\n   ckpt: <path_to_ckpt>  \n\nstage_2:\n   target: stage2.models.DDT.DiTwDDTHead\n   params: { ... }\n   ckpt: <path_to_ckpt>  \n\ntransport:\n   params:\n      path_type: Linear\n      prediction: velocity\n      ...\nsampler:\n   mode: ODE\n   params:\n      num_steps: 50\n      ...\nguidance:\n   method: cfg/autoguidance\n   scale: 1.0\n   ...\nmisc:\n   latent_size: [768, 16, 16]\n   num_classes: 1000\ntraining:\n   ...\n```\n\n- `stage_1` instantiates the frozen encoder and trainable decoder. For Stage 1\n  training you can point to an existing checkpoint via `stage_1.ckpt` or start\n  from `pretrained_decoder_path`.\n- `stage_2` defines the diffusion transformer. During sampling you must provide\n  `ckpt`; during training you typically omit it so weights initialise randomly.\n- `transport`, `sampler`, and `guidance` select the forward/backward SDE/ODE\n  integrator and optional classifier-free or autoguidance schedule.\n- `misc` collects shapes, class counts, and scaling constants used by both\n  stages.\n- `training` contains defaults that the training scripts consume (epochs,\n  learning rate, EMA decay, gradient accumulation, etc.).\n\nStage 1 training configs additionally include a top-level `gan` block that\nconfigures the discriminator architecture and the LPIPS/GAN loss schedule.\n\n\n### Provided Configs:\n\n#### Stage1\n\nWe release decoders for DINOv2-B, SigLIP-B, MAE-B, at `configs/stage1/pretrained/`.\n\nThere is also a training script for training a ViT-XL decoder on DINOv2-B: `configs/stage1/training/DINOv2-B_decXL.yaml`\n\n#### Stage2\n\nWe release our best model, DiT<sup>DH</sup>-XL and it's guidance model on both $256\\times 256$ and $512\\times 512$, at `configs/stage2/sampling/`.\n\nWe also provide training configs for DiT<sup>DH</sup> at `configs/stage2/training/`.\n\n## Stage 1: Representation Autoencoder\n\n### Train the decoder\n\n`src/train_stage1.py` fine-tunes the ViT decoder while keeping the\nrepresentation encoder frozen. Launch it with PyTorch DDP (single or multi-GPU):\n\n```bash\ntor",
      "default_branch": "main"
    },
    "fetched_at": "2025-10-19T02:42:51.299799"
  },
  {
    "basic_info": {
      "name": "VLA-Adapter",
      "full_name": "OpenHelix-Team/VLA-Adapter",
      "owner": "OpenHelix-Team",
      "description": "VLA-Adapter: An Effective Paradigm for Tiny-Scale Vision-Language-Action Model",
      "url": "https://github.com/OpenHelix-Team/VLA-Adapter",
      "clone_url": "https://github.com/OpenHelix-Team/VLA-Adapter.git",
      "ssh_url": "git@github.com:OpenHelix-Team/VLA-Adapter.git",
      "homepage": "https://vla-adapter.github.io/",
      "created_at": "2025-09-20T10:20:54Z",
      "updated_at": "2025-10-19T01:41:03Z",
      "pushed_at": "2025-10-13T14:38:04Z"
    },
    "stats": {
      "stars": 1155,
      "forks": 105,
      "watchers": 1155,
      "open_issues": 7,
      "size": 19419
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 804168
      },
      "license": "MIT License",
      "topics": [
        "embodied-ai",
        "robotics",
        "vision-language-action-model"
      ]
    },
    "content": {
      "readme": "<div align=\"center\">\n  <img src=\"figure/LOGO2.png\" width=\"70%\" style=\"vertical-align:-7px;\" />\n\n\n[![Paper](https://img.shields.io/badge/Paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/pdf/2509.09372) [![Hugging Face Collection](https://img.shields.io/badge/Models-fcd022?style=for-the-badge&logo=huggingface&logoColor=white)](https://huggingface.co/VLA-Adapter) [![Twitter](https://img.shields.io/badge/AK-%23000000.svg?style=for-the-badge&logo=x&logoColor=white)](https://x.com/_akhaliq/status/1966610780838621241) [![WeChat](https://img.shields.io/badge/WeChat--Group-07C160?style=for-the-badge&logo=wechat&logoColor=white)](https://github.com/OpenHelix-Team/VLA-Adapter/issues/1)\n\n</div>\n\n### The official implementation of **VLA-Adapter**.\n<br/>\n\n<div id=\"top\" align=\"center\">\n<p align=\"center\">\n<img src=figure/Framework.png width=90% />\n</p>\n</div>\n\n> **ğŸ“ Paper: https://arxiv.org/abs/2509.09372**<br/>\n> **ğŸŒ Project page: https://vla-adapter.github.io/**<br/>\n> **ğŸ¤— HuggingFace: https://huggingface.co/VLA-Adapter**<br/>\n> **Github: https://github.com/OpenHelix-Team/VLA-Adapter**\n\n<br/>\n\n## :loudspeaker: News!\n- **[2025/09/22]** We released our codes! An enhanced **Pro** version is also released (this version conforms to the pipeline in the original paper, but is optimized in implementation). Everyone is welcome to use it!ğŸ‰\n- **[2025/09/13]** Our paper won the ğŸ¥‡**first place** in the [daily list](https://huggingface.co/papers/date/2025-09-12), the ğŸ¥ˆ**second place** in the [weekly list](https://huggingface.co/papers/week/2025-W37), and ğŸ¥‰**third place** in the [Monthly list](https://huggingface.co/papers/month/2025-09) in HF! â­\n- **[2025/09/13]** Our paper listed in the [Trending Paper](https://huggingface.co/papers/trending) in HF! â­\n- **[2025/09/12]** We released the original version of the VLA-Adapter for four LIBERO models on [HuggingFace](https://huggingface.co/VLA-Adapter).\n- **[2025/09/11]** We released our paper on [ArXiv](https://arxiv.org/abs/2509.09372).\n\n<br/>\n\n## :black_nib: TODO List<a name=\"todo\"></a>\n\n- [x]  Release **checkpoints** for reproduction.\n- [x]  Release [VLA-Adapter v2 paper](https://arxiv.org/abs/2509.09372).\n- [ ]  A more **powerful version**, **VLA-Adapter++**, and a detailed **technical report** ğŸ“ will be released soon.<br/>\n- [ ]  Continue to update the code to adapt to various **real-world systems** deployments, including the configuration of our paper, Franka, UR-5, and AGILE Piper.<br/>\n- [ ]  It will soon be compatible with **various foundation models**, including but not limited to [VPP](https://arxiv.org/abs/2412.14803), [Ï€0.5](https://arxiv.org/abs/2504.16054).<br/>\n- [ ]  We will update the **diffusion transformers** and **flow matching** policy networks in the future, and the results will be updated in the subsequent VLA-Adapter++ technical report.\n- [ ]  We will also update and give more experiments on **Frozen backbone**.\n- [ ]  We will expand its **generalization** further in the future. Work is in progress! So please stay tuned!\n- [ ]  **RL post-training** is also in progress. Interested researchers are welcome to join us in building this foundation!\n- [ ]  **The dual-system compatibility** of VLA-Adapter is under exploration!\n\n\n<br/>\n\n## ğŸŒŸ Table of Contents\n\n- [:rocket: Quick Start](#rocket-quick-start) \n  - [Conda Environment of VLA-Adapter](#conda-environment-of-vla-adapter)\n  - [Install Dependencies](#install-dependencies)\n- [:pencil: Data Preparation](#pencil-data-preparation) \n  - [LIBERO Benchmark](#libero-benchmark)\n  - [CALVIN Benchmark](#calvin-benchmark)\n  - [:video_game: Our Dependencies](#video_game-our-dependencies)\n  - [:pushpin: Benchmark Location](#pushpin-benchmark-location)\n- [âš“ VLM backbone](#vlm)\n- [:fire: Training for Different Configurations](#fire-training-for-different-configurations) &emsp; => Provides **training configurations** for GPUs ranging from **10GB** to **80GB** of VRAM.\n  - [:books: Related File for Training](#books-related-file-for-training)\n  - [:ledger: How to Train on Extremely Limited VRAM GPUs](#ledger-how-to-train-on-extremely-limited-vram-gpus) &emsp; => A card with 10GB-12GB *(e.g. NVIDIA GeForce RTX 2080Ti, 3060, 3080, 4070, 4080, and 5070)*\n  - [:ledger: How to Train on Low VRAM GPUs](#ledger-how-to-train-on-low-vram-gpus) &emsp; => A card with 24GB *(e.g. NVIDIA GeForce RTX 3090 and 4090)*\n  - [:ledger: How to Train on Larger VRAM GPUs](#ledger-how-to-train-on-larger-vram-gpus) &emsp; => A Consumer GPU with 32GB *(e.g. NVIDIA GeForce RTX 5090)* &emsp; A Professional-Grade GPU with 40GB-48GB *(e.g. NVIDIA A100-40GB, A800-40GB, L20, and RTX A6000).*\n  - [:ledger: How to Train on Sufficient VRAM GPUs](#ledger-how-to-train-on-sufficient-vram-gpus) &emsp; => Professional-Grade GPUs with â‰¥80GB *(e.g. NVIDIA A100-80GB, A800-80GB, H100, H800, H20-NVLink, and GB200).*\n- [:mechanical_arm: Inference](#mechanical_arm-inference)\n  - [:books: Related File for Inference](#books-related-file-for-inference)\n  -",
      "default_branch": "main"
    },
    "fetched_at": "2025-10-19T02:42:52.418810"
  },
  {
    "basic_info": {
      "name": "Paper2Video",
      "full_name": "showlab/Paper2Video",
      "owner": "showlab",
      "description": "Automatic Video Generation from Scientific Papers",
      "url": "https://github.com/showlab/Paper2Video",
      "clone_url": "https://github.com/showlab/Paper2Video.git",
      "ssh_url": "git@github.com:showlab/Paper2Video.git",
      "homepage": "https://showlab.github.io/Paper2Video/",
      "created_at": "2025-10-03T08:50:16Z",
      "updated_at": "2025-10-19T02:14:52Z",
      "pushed_at": "2025-10-17T12:03:01Z"
    },
    "stats": {
      "stars": 1110,
      "forks": 141,
      "watchers": 1110,
      "open_issues": 3,
      "size": 392261
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 930971,
        "TeX": 250773,
        "BibTeX Style": 26973,
        "Shell": 5155
      },
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "# Paper2Video\n\n<p align=\"right\">\n  <b>English</b> | <a href=\"./README-CN.md\">ç®€ä½“ä¸­æ–‡</a>\n</p>\n\n\n<p align=\"center\">\n  <b>Paper2Video: Automatic Video Generation from Scientific Papers</b>\n<br>\nä»å­¦æœ¯è®ºæ–‡è‡ªåŠ¨ç”Ÿæˆæ¼”è®²è§†é¢‘\n</p>\n\n<p align=\"center\">\n  <a href=\"https://zeyu-zhu.github.io/webpage/\">Zeyu Zhu*</a>,\n  <a href=\"https://qhlin.me/\">Kevin Qinghong Lin*</a>,\n  <a href=\"https://scholar.google.com/citations?user=h1-3lSoAAAAJ&hl=en\">Mike Zheng Shou</a> <br>\n  Show Lab, National University of Singapore\n</p>\n\n\n<p align=\"center\">\nÂ  <a href=\"https://arxiv.org/abs/2510.05096\">ğŸ“„ Paper</a> &nbsp; | &nbsp;\n  <a href=\"https://huggingface.co/papers/2510.05096\">ğŸ¤— Daily Paper</a> &nbsp; | &nbsp;\nÂ  <a href=\"https://huggingface.co/datasets/ZaynZhu/Paper2Video\">ğŸ“Š Dataset</a> &nbsp; | &nbsp;\nÂ  <a href=\"https://showlab.github.io/Paper2Video/\">ğŸŒ Project Website</a> &nbsp; | &nbsp;\nÂ  <a href=\"https://x.com/KevinQHLin/status/1976105129146257542\">ğŸ’¬ X (Twitter)</a>\n</p>\n\n- **Input:** a paper â• an image â• an audio\n  \n| Paper | Image | Audio |\n|--------|--------|--------|\n| <img src=\"https://github.com/showlab/Paper2Video/blob/page/assets/hinton/paper.png\" width=\"180\"/><br>[ğŸ”— Paper link](https://arxiv.org/pdf/1509.01626) | <img src=\"https://github.com/showlab/Paper2Video/blob/page/assets/hinton/hinton_head.jpeg\" width=\"180\"/> <br>Hinton's photo| <img src=\"assets/sound.png\" width=\"180\"/><br>[ğŸ”— Audio sample](https://github.com/showlab/Paper2Video/blob/page/assets/hinton/ref_audio_10.wav) |\n\n\n- **Output:** a presentation video\n\n\n\nhttps://github.com/user-attachments/assets/39221a9a-48cb-4e20-9d1c-080a5d8379c4\n\n\n\n\nCheck out more examples at [ğŸŒ project page](https://showlab.github.io/Paper2Video/).\n\n## ğŸ”¥ Update\n**Any contributions are welcome!**\n- [x] [2025.10.15] We update a new version without talking-head for fast generation!\n- [x] [2025.10.11] Our work receives attention on [YC Hacker News](https://news.ycombinator.com/item?id=45553701).\n- [x] [2025.10.9] Thanks AK for sharing our work on [Twitter](https://x.com/_akhaliq/status/1976099830004072849)!\n- [x] [2025.10.9] Our work is reported by [Medium](https://medium.com/@dataism/how-ai-learned-to-make-scientific-videos-from-slides-to-a-talking-head-0d807e491b27).\n- [x] [2025.10.8] Check out our demo video below!\n- [x] [2025.10.7] We release the [arxiv paper](https://arxiv.org/abs/2510.05096).\n- [x] [2025.10.6] We release the [code](https://github.com/showlab/Paper2Video) and [dataset](https://huggingface.co/datasets/ZaynZhu/Paper2Video).\n- [x] [2025.9.28] Paper2Video has been accepted to the **Scaling Environments for Agents Workshop([SEA](https://sea-workshop.github.io/)) at NeurIPS 2025**.\n\n\nhttps://github.com/user-attachments/assets/a655e3c7-9d76-4c48-b946-1068fdb6cdd9\n\n\n\n\n---\n\n### Table of Contents\n- [ğŸŒŸ Overview](#-overview)\n- [ğŸš€ Quick Start: PaperTalker](#-try-papertalker-for-your-paper-)\n  - [1. Requirements](#1-requirements)\n  - [2. Configure LLMs](#2-configure-llms)\n  - [3. Inference](#3-inference)\n- [ğŸ“Š Evaluation: Paper2Video](#-evaluation-paper2video)\n- [ğŸ˜¼ Fun: Paper2Video for Paper2Video](#-fun-paper2video-for-paper2video)\n- [ğŸ™ Acknowledgements](#-acknowledgements)\n- [ğŸ“Œ Citation](#-citation)\n\n---\n\n## ğŸŒŸ Overview\n<p align=\"center\">\n  <img src=\"assets/teaser.png\" alt=\"Overview\" width=\"100%\">\n</p>\n\nThis work solves two core problems for academic presentations:\n\n- **Left: How to create a presentation video from a paper?**  \n  *PaperTalker* â€” an agent that integrates **slides**, **subtitling**, **cursor grounding**, **speech synthesis**, and **talking-head video rendering**.\n\n- **Right: How to evaluate a presentation video?**  \n  *Paper2Video* â€” a benchmark with well-designed metrics to evaluate presentation quality.\n\n\n---\n\n## ğŸš€ Try PaperTalker for your Paper!\n<p align=\"center\">\n  <img src=\"assets/method.png\" alt=\"Approach\" width=\"100%\">\n</p>\n\n### 1. Requirements\nPrepare the environment:\n```bash\ncd src\nconda create -n p2v python=3.10\nconda activate p2v\npip install -r requirements.txt\nconda install -c conda-forge tectonic\n```\n**[Optional] [Skip](#2-configure-llms) this part if you do not need a human presenter.**\n\nDownload the dependent code and follow the instructions in **[Hallo2](https://github.com/fudan-generative-vision/hallo2)** to download the model weight.\n```bash\ngit clone https://github.com/fudan-generative-vision/hallo2.git\n```\nYou need to **prepare the environment separately for talking-head generation** to potential avoide package conflicts, please refer to  <a href=\"git clone https://github.com/fudan-generative-vision/hallo2.git\">Hallo2</a>. After installing, use `which python` to get the python environment path.\n```bash\ncd hallo2\nconda create -n hallo python=3.10\nconda activate hallo\npip install -r requirements.txt\n```\n\n### 2. Configure LLMs\nExport your **API credentials**:\n```bash\nexport GEMINI_API_KEY=\"your_gemini_key_here\"\nexport OPENAI_API_KEY=\"your_openai_key_here\"\n```\nThe best practice is to use **GPT4.1** or **Gemini2.5-Pro** for both LLM and VLMs. We also support locally depl",
      "default_branch": "main"
    },
    "fetched_at": "2025-10-19T02:42:53.547556"
  },
  {
    "basic_info": {
      "name": "DeepSeek-V3.2-Exp",
      "full_name": "deepseek-ai/DeepSeek-V3.2-Exp",
      "owner": "deepseek-ai",
      "description": null,
      "url": "https://github.com/deepseek-ai/DeepSeek-V3.2-Exp",
      "clone_url": "https://github.com/deepseek-ai/DeepSeek-V3.2-Exp.git",
      "ssh_url": "git@github.com:deepseek-ai/DeepSeek-V3.2-Exp.git",
      "homepage": null,
      "created_at": "2025-09-29T03:25:13Z",
      "updated_at": "2025-10-18T22:11:39Z",
      "pushed_at": "2025-10-02T02:49:20Z"
    },
    "stats": {
      "stars": 899,
      "forks": 56,
      "watchers": 899,
      "open_issues": 13,
      "size": 1077
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 59696
      },
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "# DeepSeek-V3.2-Exp\n\n<!-- markdownlint-disable first-line-h1 -->\n<!-- markdownlint-disable html -->\n<!-- markdownlint-disable no-duplicate-header -->\n\n<div align=\"center\">\n  <img src=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/logo.svg?raw=true\" width=\"60%\" alt=\"DeepSeek-V3\" />\n</div>\n<hr>\n<div align=\"center\" style=\"line-height: 1;\">\n  <a href=\"https://www.deepseek.com/\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Homepage\" src=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/badge.svg?raw=true\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://chat.deepseek.com/\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Chat\" src=\"https://img.shields.io/badge/ğŸ¤–%20Chat-DeepSeek%20V3-536af5?color=536af5&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://huggingface.co/deepseek-ai\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Hugging Face\" src=\"https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-DeepSeek%20AI-ffc107?color=ffc107&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n</div>\n<div align=\"center\" style=\"line-height: 1;\">\n  <a href=\"https://discord.gg/Tc7c45Zzu5\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Discord\" src=\"https://img.shields.io/badge/Discord-DeepSeek%20AI-7289da?logo=discord&logoColor=white&color=7289da\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/qr.jpeg?raw=true\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Wechat\" src=\"https://img.shields.io/badge/WeChat-DeepSeek%20AI-brightgreen?logo=wechat&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://twitter.com/deepseek_ai\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Twitter Follow\" src=\"https://img.shields.io/badge/Twitter-deepseek_ai-white?logo=x&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n</div>\n<div align=\"center\" style=\"line-height: 1;\">\n  <a href=\"LICENSE\" style=\"margin: 2px;\">\n    <img alt=\"License\" src=\"https://img.shields.io/badge/License-MIT-f5de53?&color=f5de53\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n</div>\n\n## Introduction\n\n\nWe are excited to announce the official release of DeepSeek-V3.2-Exp, an experimental version of our model. As an intermediate step toward our next-generation architecture, V3.2-Exp builds upon V3.1-Terminus by introducing DeepSeek Sparse Attentionâ€”a sparse attention mechanism designed to explore and validate optimizations for training and inference efficiency in long-context scenarios.\n\nThis experimental release represents our ongoing research into more efficient transformer architectures, particularly focusing on improving computational efficiency when processing extended text sequences.\n\n<div align=\"center\">\n <img src=\"cost.jpg\" >\n</div>\n\n- DeepSeek Sparse Attention (DSA) achieves fine-grained sparse attention for the first time, delivering substantial improvements in long-context training and inference efficiency while maintaining virtually identical model output quality.\n\n\n- To rigorously evaluate the impact of introducing sparse attention, we deliberately aligned the training configurations of DeepSeek-V3.2-Exp with V3.1-Terminus. Across public benchmarks in various domains, DeepSeek-V3.2-Exp demonstrates performance on par with V3.1-Terminus.\n\n\n| Benchmark | DeepSeek-V3.1-Terminus | DeepSeek-V3.2-Exp |\n| :--- | :---: | :---: |\n| **Reasoning Mode w/o Tool Use** | | |\n| MMLU-Pro | 85.0 | 85.0 |\n| GPQA-Diamond | 80.7 | 79.9 |\n| Humanity's Last Exam | 21.7 | 19.8 |\n| LiveCodeBench | 74.9 | 74.1 |\n| AIME 2025 | 88.4 | 89.3 |\n| HMMT 2025 | 86.1 | 83.6 |\n| Codeforces | 2046 | 2121 |\n| Aider-Polyglot | 76.1 | 74.5 |\n| **Agentic Tool Use** | | |\n| BrowseComp | 38.5 | 40.1 |\n| BrowseComp-zh | 45.0 | 47.9 |\n| SimpleQA | 96.8 | 97.1 |\n| SWE Verified | 68.4 | 67.8 |\n| SWE-bench Multilingual | 57.8 | 57.9 |\n| Terminal-bench | 36.7 | 37.7 |\n\n\n\n## Open-Source Kernels\n\nFor TileLang kernels with **better readability and research-purpose design**, please refer to [TileLang](https://github.com/tile-ai/tilelang/tree/main/examples/deepseek_v32).\n\nFor **high-performance CUDA kernels**, indexer logit kernels (including paged versions) are available in [DeepGEMM](https://github.com/deepseek-ai/DeepGEMM/pull/200). Sparse attention kernels are released in [FlashMLA](https://github.com/deepseek-ai/FlashMLA/pull/98).\n\n\n\n## How to Run Locally\n\n### HuggingFace\nWe provide an updated inference demo code in the [inference](https://huggingface.co/deepseek-ai/DeepSeek-V3.2-Exp/tree/main/inference) folder to help the community quickly get started with our model and understand its architectural details.\n\nFirst convert huggingface model weights to the the format required by our inference demo. Set `MP` to match your available GPU count:\n```bash\ncd inference\nexpor",
      "default_branch": "main"
    },
    "fetched_at": "2025-10-19T02:42:54.661178"
  },
  {
    "basic_info": {
      "name": "ml-simplefold",
      "full_name": "apple/ml-simplefold",
      "owner": "apple",
      "description": null,
      "url": "https://github.com/apple/ml-simplefold",
      "clone_url": "https://github.com/apple/ml-simplefold.git",
      "ssh_url": "git@github.com:apple/ml-simplefold.git",
      "homepage": null,
      "created_at": "2025-09-23T03:08:49Z",
      "updated_at": "2025-10-18T15:10:43Z",
      "pushed_at": "2025-09-27T04:47:33Z"
    },
    "stats": {
      "stars": 868,
      "forks": 57,
      "watchers": 868,
      "open_issues": 19,
      "size": 1251
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 662611,
        "Jupyter Notebook": 11558
      },
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "\n<h1 align=\"center\"><strong>SimpleFold: Folding Proteins is Simpler than You Think</strong></h1>\n\n\n<div align=\"center\">\n\nThis github repository accompanies the research paper, [*SimpleFold: Folding Proteins is Simpler than You Think*](https://arxiv.org/abs/2509.18480) (Arxiv 2025).\n\n*Yuyang Wang, Jiarui Lu, Navdeep Jaitly, Joshua M. Susskind, Miguel Angel Bautista*\n\n[[`Paper`](https://arxiv.org/abs/2509.18480)]  [[`BibTex`](#citation)]\n\n<img src=\"assets/intro.png\" width=\"750\">\n\n</div>\n\n\n## Introduction\n\nWe introduce SimpleFold, the first flow-matching based protein folding model that solely uses general purpose transformer layers. SimpleFold does not rely on expensive modules like triangle attention or pair representation biases, and is trained via a generative flow-matching objective. We scale SimpleFold to 3B parameters and train it on more than 8.6M distilled protein structures together with experimental PDB data. To the best of our knowledge, SimpleFold is the largest scale folding model ever developed. On standard folding benchmarks, SimpleFold-3B model achieves competitive performance compared to state-of-the-art baselines. Due to its generative training objective, SimpleFold also demonstrates strong performance in ensemble prediction. SimpleFold challenges the reliance on complex domain-specific architectures designs in folding, highlighting an alternative yet important avenue of progress in protein structure prediction.\n\n</div>\n\n\n## Installation\n\nTo install `simplefold` package from github repository, run\n```\ngit clone https://github.com/apple/ml-simplefold.git\ncd ml-simplefold\nconda create -n simplefold python=3.10\npython -m pip install -U pip build; pip install -e .\n```\nIf you want to use MLX backend on Apple silicon: \n```\npip install mlx==0.28.0\npip install git+https://github.com/facebookresearch/esm.git\n```\n\n## Example \n\nWe provide a jupyter notebook [`sample.ipynb`](sample.ipynb) to predict protein structures from example protein sequences. \n\n## Inference\n\nOnce you have `simplefold` package installed, you can predict the protein structure from target fasta file(s) via the following command line. We provide support for both [PyTorch](https://pytorch.org/) and [MLX](https://mlx-framework.org/) (recommended for Apple hardware) backends in inference. \n```\nsimplefold \\\n    --simplefold_model simplefold_100M \\  # specify folding model in simplefold_100M/360M/700M/1.1B/1.6B/3B\n    --num_steps 500 --tau 0.01 \\        # specify inference setting\n    --nsample_per_protein 1 \\           # number of generated conformers per target\n    --plddt \\                           # output pLDDT\n    --fasta_path [FASTA_PATH] \\         # path to the target fasta directory or file\n    --output_dir [OUTPUT_DIR] \\         # path to the output directory\n    --backend [mlx, torch]              # choose from MLX and PyTorch for inference backend \n```\n\n## Evaluation\n\nWe provide predicted structures from SimpleFold of different model sizes:\n```\nhttps://ml-site.cdn-apple.com/models/simplefold/cameo22_predictions.zip # predicted structures of CAMEO22\nhttps://ml-site.cdn-apple.com/models/simplefold/casp14_predictions.zip  # predicted structures of CASP14\nhttps://ml-site.cdn-apple.com/models/simplefold/apo_predictions.zip     # predicted structures of Apo\nhttps://ml-site.cdn-apple.com/models/simplefold/codnas_predictions.zip  # predicted structures of Fold-switch (CoDNaS)\n```\nWe use the docker image of [openstructure](https://git.scicore.unibas.ch/schwede/openstructure/) 2.9.1 to evaluate generated structures for folding tasks (i.e., CASP14/CAMEO22). Once having the docker image enabled, you can run evaluation via:\n```\npython src/simplefold/evaluation/analyze_folding.py \\\n    --data_dir [PATH_TO_TARGET_MMCIF] \\\n    --sample_dir [PATH_TO_PREDICTED_MMCIF] \\\n    --out_dir [PATH_TO_OUTPUT] \\\n    --max-workers [NUMBER_OF_WORKERS]\n```\nTo evaluate results of two-state prediction (i.e., Apo/CoDNaS), one need to compile the [TMsore](https://zhanggroup.org/TM-score/TMscore.cpp) and then run evaluation via:\n```\npython src/simplefold/evaluation/analyze_two_state.py \\ \n    --data_dir [PATH_TO_TARGET_DATA_DIRECTORY] \\\n    --sample_dir [PATH_TO_PREDICTED_PDB] \\\n    --tm_bin [PATH_TO_TMscore_BINARY] \\\n    --task apo \\ # choose from apo and codnas\n    --nsample 5\n```\n\n## Train\n\nYou can also train or tune SimpleFold on your end. Instructions below include details for SimpleFold training. \n\n### Data preparation\n\n#### Training targets\n\nSimpleFold is trained on joint datasets including experimental structures from [PDB](https://www.rcsb.org/), as well as distilled predictions from [AFDB SwissProt](https://alphafold.ebi.ac.uk/download#swissprot-section) and [AFESM](https://afesm.foldseek.com/). Target lists of filtered SwissProt and AFESM targets thta are used in our training can be found:\n```\nhttps://ml-site.cdn-apple.com/models/simplefold/swissprot_list.csv # list of filted SwissProt (~270K targets)\nhttps://ml-site.cdn-apple.com/models/simplefold/af",
      "default_branch": "main"
    },
    "fetched_at": "2025-10-19T02:42:55.879142"
  },
  {
    "basic_info": {
      "name": "AgentFlow",
      "full_name": "lupantech/AgentFlow",
      "owner": "lupantech",
      "description": "AgentFlow: In-the-Flow Agentic System Optimization",
      "url": "https://github.com/lupantech/AgentFlow",
      "clone_url": "https://github.com/lupantech/AgentFlow.git",
      "ssh_url": "git@github.com:lupantech/AgentFlow.git",
      "homepage": "https://agentflow.stanford.edu",
      "created_at": "2025-09-27T02:17:05Z",
      "updated_at": "2025-10-19T01:47:02Z",
      "pushed_at": "2025-10-17T04:50:45Z"
    },
    "stats": {
      "stars": 818,
      "forks": 91,
      "watchers": 818,
      "open_issues": 4,
      "size": 7031
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 563581,
        "Shell": 14322
      },
      "license": "MIT License",
      "topics": [
        "agentic-ai",
        "agentic-systems",
        "llms",
        "llms-reasoning",
        "multi-agent-systems",
        "reinforcement-learning",
        "tool-augmented"
      ]
    },
    "content": {
      "readme": "<a name=\"readme-top\"></a>\n\n<p align=\"center\">\n  <picture>\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"assets/img/logo.png\">\n    <img alt=\"AgentFlow\" src=\"assets/img/logo.png\" width=31%>\n  </picture>\n</p>\n\n<h3 align=\"center\">\nAgentFlow: In-the-Flow Agentic System Optimization\n</h3>\n\n\n<!--- BADGES: START --->\n<p align=\"center\">\n    <a href=\"https://arxiv.org/abs/2510.05592\"><img src=\"https://img.shields.io/badge/arXiv-2510.05592-B31B1B.svg?logo=arxiv\" alt=\"Arxiv\"></a>\n    <a href=\"https://huggingface.co/spaces/AgentFlow/agentflow\"><img src=\"https://img.shields.io/badge/Gradio-Demo-F97316.svg?logo=gradio\" alt=\"Gradio Demo\"></a>\n    <a href=\"https://huggingface.co/papers/2510.05592\"><img src=\"https://img.shields.io/badge/Huggingface-Paper-FFD21E.svg?logo=huggingface\" alt=\"Huggingface Paper\"></a>\n    <a href=\"https://huggingface.co/AgentFlow\"><img src=\"https://img.shields.io/badge/Huggingface-Model-FFD21E.svg?logo=huggingface\" alt=\"Huggingface Model\"></a>\n    <a href=\"https://agentflow.stanford.edu/\"><img src=\"https://img.shields.io/badge/Website-AgentFlow-E5426E?logo=kashflow\" alt=\"Website\"></a>\n    <a href=\"https://x.com/lupantech/status/1976016000345919803\"><img src=\"https://img.shields.io/badge/Coverage-AgentFlow-2176BC.svg?logo=x\" alt=\"X\"></a>\n    <a href=\"https://www.youtube.com/watch?v=kIQbCQIH1SI\"><img src=\"https://img.shields.io/badge/YouTube-Tutorial-FF0000?logo=youtube\" alt=\"Youtube\"></a>\n    <a href=\"https://join.slack.com/t/agentflow-co/shared_invite/zt-3f712xngl-LfxS4gmftAeKvcxR3nSkWQ\"><img src=\"https://img.shields.io/badge/Slack-AgentFlow-D41544.svg?logo=slack\" alt=\"Slack\"></a>\n    <a href=\"https://github.com/lupantech/AgentFlow/blob/main/assets/img/wechat_group.jpg\">\n  <img src=\"https://img.shields.io/badge/Wechat-AgentFlow-07C160.svg?logo=wechat\" alt=\"Wechat AgentFlow\">\n</a>\n  \n  </p>\n<!--- BADGES: END --->\n\n\n## ğŸ“£ News\n- **[2025.10.16]** ğŸ† Our paper has been accepted by [**NeurIPS 2025 Efficient Reasoning Workshop**](https://efficient-reasoning.github.io/)!\n- **[2025.10.13]** ğŸ“¸ Excited to have a tutorial video for AgentFlow covered by Discover AI on **[YouTube](https://www.youtube.com/watch?v=kIQbCQIH1SI)**!\n- **[2025.10.10]** ğŸš€ Our X [post](https://x.com/lupantech/status/1976016000345919803) received **1K+ likes**! Feel free to check out the post and join the discussion! ğŸ’¬\n- **[2025.10.08]** ğŸ”¥ We are honored to be featured as ğŸ¤— HuggingFace **[Daily Paper #2](https://huggingface.co/papers/2510.05592)**.\n\n## ğŸŒŸ Why AgentFlow?\nAgentFlow is a **trainable, tool-integrated agentic framework** designed to overcome the **scalability** and **generalization limits** of todayâ€™s tool-augmented reasoning approaches. \n\nUnlike prevailing approaches such as [Search-R1](https://github.com/PeterGriffinJin/Search-R1) which train a **single LLM** to interleave reasoning steps with tool calls, **AgentFlow** introduces a **modular agentic system** with four specialized modules: ğŸ§­ **Planner**, ğŸ›  **Executor**, âœ… **Verifier**, and âœï¸ **Generator**.\n\n![framework_overall](assets/img/framework.png)\n\nFor effective planning and tool use, the framework directly **optimizes planner agent within the system** in an **online fashion** using **Flow-based Group Refined Policy Optimization (Flow-GRPO)**, achieving superior performance across diverse domains with improved tool-calling reliability and long-horizon reasoning capabilities.\n\n![flow_grpo](assets/img/flow_grpo.png)\n\n## ğŸ“º YouTube Tutorial\nExcited to have a tutorial video for AgentFlow covered by [Discover AI](https://www.youtube.com/@code4AI) on YouTube!\n\n<!-- [![AgentFlow Tutorial](https://img.youtube.com/vi/kIQbCQIH1SI/0.jpg)](https://www.youtube.com/watch?v=kIQbCQIH1SI) -->\n\n<div align=\"center\">\n  <a href=\"https://www.youtube.com/watch?v=kIQbCQIH1SI\">\n    <img src=\"https://img.youtube.com/vi/kIQbCQIH1SI/maxresdefault.jpg\" alt=\"AgentFlow Tutorial\" width=\"100%\">\n  </a>\n</div>\n\n\n## ğŸš€ Key Features\n\n- ğŸ§© **Modular Agentic System** â€“ Four specialized agent modules (**Planner**, **Executor**, **Verifier**, **Generator**) that coordinate via evolving memory and integrated tools across multiple turns.  \n- ğŸ”— **Multi-Tool Integration** â€“ Seamlessly connect with diverse tool ecosystems, including `base_generator`, `python_coder`, `google_search`, `wikipedia_search`, `web_search`, and more.  \n- ğŸ¯ **Flow-GRPO Algorithm** â€“ Enables **in-the-flow agent optimization** for **long-horizon reasoning tasks** with sparse rewards.\n- ğŸ“ˆ **Proven Results** â€“ **AgentFlow (7B Backbone)** beats top baselines on 10 benchmarks, with **+14.9% search**, **+14.0% agentic**, **+14.5% math**, **+4.1% science**, even outperforming ~200B-parameter **GPT-4o**.\n\n## ğŸ† Experiments\n\n### ğŸ“Š Main Results\n**AgentFlow (Qwen-2.5-7B-Instruct Backbone)** outperforms top baselines on 10 benchmarks:  \n- **+14.9%** on search  \n- **+14.0%** on agentic reasoning  \n- **+14.5%** on math  \n- **+4.1%** on science  \n\nğŸ’¡ Even surpasses larger proprietary models like **GPT-4o (~200B)**.\n\n![main_table1](assets/img/maintabl",
      "default_branch": "main"
    },
    "fetched_at": "2025-10-19T02:42:56.995180"
  },
  {
    "basic_info": {
      "name": "MiMo-Audio",
      "full_name": "XiaomiMiMo/MiMo-Audio",
      "owner": "XiaomiMiMo",
      "description": "MiMo-Audio: Audio Language Models are Few-Shot Learners",
      "url": "https://github.com/XiaomiMiMo/MiMo-Audio",
      "clone_url": "https://github.com/XiaomiMiMo/MiMo-Audio.git",
      "ssh_url": "git@github.com:XiaomiMiMo/MiMo-Audio.git",
      "homepage": "https://xiaomimimo.github.io/MiMo-Audio-Demo/",
      "created_at": "2025-09-19T00:46:49Z",
      "updated_at": "2025-10-18T23:59:12Z",
      "pushed_at": "2025-09-20T19:03:26Z"
    },
    "stats": {
      "stars": 781,
      "forks": 73,
      "watchers": 781,
      "open_issues": 31,
      "size": 6029
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 223792
      },
      "license": "Apache License 2.0",
      "topics": []
    },
    "content": {
      "readme": "<div align=\"center\">\n  <picture>\n    <source srcset=\"https://github.com/XiaomiMiMo/MiMo-VL/raw/main/figures/Xiaomi_MiMo_darkmode.png?raw=true\" media=\"(prefers-color-scheme: dark)\">\n    <img src=\"https://github.com/XiaomiMiMo/MiMo-VL/raw/main/figures/Xiaomi_MiMo.png?raw=true\" width=\"60%\" alt=\"Xiaomi-MiMo\" />\n  </picture>\n</div>\n\n<h3 align=\"center\">\n  <b>\n    <span>â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”</span>\n    <br/>\n    MiMo Audio: Audio Language Models are Few-Shot Learners\n    <br/>\n    <span>â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”</span>\n    <br/>\n  </b>\n</h3>\n\n<br/>\n\n<div align=\"center\" style=\"line-height: 1;\">\n  |\n  <a href=\"https://huggingface.co/collections/XiaomiMiMo/mimo-audio-68cc7202692c27dae881cce0\" target=\"_blank\">ğŸ¤— HuggingFace</a>\n  &nbsp;|\n  <a href=\"https://github.com/XiaomiMiMo/MiMo-Audio/blob/main/MiMo-Audio-Technical-Report.pdf\" target=\"_blank\">ğŸ“„ Paper</a>\n  &nbsp;|\n  <a href=\"https://xiaomimimo.github.io/MiMo-Audio-Demo\" target=\"_blank\">ğŸ“° Blog</a>\n  &nbsp;|\n  <a href=\"https://huggingface.co/spaces/XiaomiMiMo/mimo_audio_chat\" target=\"_blank\">ğŸ”¥ Online Demo</a>\n  &nbsp;|\n  <a href=\"https://github.com/XiaomiMiMo/MiMo-Audio-Eval\" target=\"_blank\">ğŸ“Š MiMo-Audio-Eval</a>\n  &nbsp;|\n\n  <br/>\n</div>\n\n<br/>\n\n## Introduction\n\nExisting audio language models typically rely on task-specific fine-tuning to accomplish particular audio tasks. In contrast, humans are able to generalize to new audio tasks with only a few examples or simple instructions. GPT-3 has shown that scaling next-token prediction pretraining enables strong generalization capabilities in text, and we believe this paradigm is equally applicable to the audio domain. By scaling MiMo-Audio's pretraining data to over one hundred million of hours, we observe the emergence of few-shot learning capabilities across a diverse set of audio tasks. We develop a systematic evaluation of these capabilities and find that MiMo-Audio-7B-Base achieves SOTA performance on both speech intelligence and audio understanding benchmarks among open-source models. Beyond standard metrics, MiMo-Audio-7B-Base generalizes to tasks absent from its training data, such as voice conversion, style transfer, and speech editing. MiMo-Audio-7B-Base also demonstrates powerful speech continuation capabilities, capable of generating highly realistic talk shows, recitations, livestreaming and debates. At the post-training stage, we curate a diverse instruction-tuning corpus and introduce thinking mechanisms into both audio understanding and generation. MiMo-Audio-7B-Instruct achieves open-source SOTA on audio understanding benchmarks, spoken dialogue benchmarks and instruct-TTS evaluations, approaching or surpassing closed-source models.\n\n\n![Results](assets/Results.png)\n\n\n\n## Architecture\n### MiMo-Audio-Tokenizer\nMiMo-Audio-Tokenizer is a 1.2B-parameter Transformer operating at 25 Hz. It employs an eight-layer RVQ stack to generate 200 tokens per second. By jointly optimizing semantic and reconstruction objectives, we train MiMo-Audio-Tokenizer from scratch on a 10-million-hour corpus, achieving superior reconstruction quality and facilitating downstream language modeling.\n\n![Tokenizer](assets/tokenizer.png)\n\nMiMo-Audio couples a patch encoder, an LLM, and a patch decoder to improve modeling efficiency for high-rate sequences and bridge the length mismatch between speech and text. The patch encoder aggregates four consecutive time steps of RVQ tokens into a single patch, downsampling the sequence to a 6.25 Hz representation for the LLM. The patch decoder autoregressively generates the full 25 Hz RVQ token sequence via a delayed-generation scheme.\n### MiMo-Audio\n![Arch](assets/architecture.png)\n\n##  Explore MiMo-Audio Now! ğŸš€ğŸš€ğŸš€\n- ğŸ§ **Try the Hugging Face demo:** [MiMo-Audio Demo](https://huggingface.co/spaces/XiaomiMiMo/mimo_audio_chat)\n- ğŸ“° **Read the Official Blog:** [MiMo-Audio Blog](https://xiaomimimo.github.io/MiMo-Audio-Demo)\n- ğŸ“„ **Dive into the Technical Report:** [MiMo-Audio Technical Report](https://github.com/XiaomiMiMo/MiMo-Audio/blob/main/MiMo-Audio-Technical-Report.pdf)\n\n\n## Model Download\n| Models   | ğŸ¤— Hugging Face |\n|-------|-------|\n| MiMo-Audio-Tokenizer | [XiaomiMiMo/MiMo-Audio-Tokenizer](https://huggingface.co/XiaomiMiMo/MiMo-Audio-Tokenizer) |\n| MiMo-Audio-7B-Base | [XiaomiMiMo/MiMo-Audio-7B-Base](https://huggingface.co/XiaomiMiMo/MiMo-Audio-7B-Base) |\n| MiMo-Audio-7B-Instruct | [XiaomiMiMo/MiMo-Audio-7B-Instruct](https://huggingface.co/XiaomiMiMo/MiMo-Audio-7B-Instruct) |\n\n\n```bash\npip install huggingface-hub\n\nhf download XiaomiMiMo/MiMo-Audio-Tokenizer --local-dir ./models/MiMo-Audio-Tokenizer\nhf download XiaomiMiMo/MiMo-Audio-7B-Base --local-dir ./models/MiMo-Audio-7B-Base\nhf download XiaomiMiMo/MiMo-Audio-7B-Instruct --local-dir ./models/MiMo-Audio-7B-Instruct\n```\n\n## Getting Started\n\nSpin up the MiMo-Audio demo in minutes with the built-in Gradio app.\n\n### Prerequisites (Linux)\n\n* Python 3.12\n* CUDA >= 12.0\n\n### Installation\n\n```bash\ngit clone https",
      "default_branch": "main"
    },
    "fetched_at": "2025-10-19T02:42:58.138072"
  },
  {
    "basic_info": {
      "name": "Code2Video",
      "full_name": "showlab/Code2Video",
      "owner": "showlab",
      "description": "Video generation via code",
      "url": "https://github.com/showlab/Code2Video",
      "clone_url": "https://github.com/showlab/Code2Video.git",
      "ssh_url": "git@github.com:showlab/Code2Video.git",
      "homepage": "https://showlab.github.io/Code2Video/",
      "created_at": "2025-09-29T08:15:44Z",
      "updated_at": "2025-10-18T21:42:39Z",
      "pushed_at": "2025-10-16T06:02:34Z"
    },
    "stats": {
      "stars": 727,
      "forks": 95,
      "watchers": 727,
      "open_issues": 0,
      "size": 130198
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 179924,
        "Shell": 2370
      },
      "license": "MIT License",
      "topics": [
        "coding",
        "education",
        "multi-agent",
        "video-generation"
      ]
    },
    "content": {
      "readme": "\n# Code2Video: Video Generation via Code\n\n<p align=\"right\">\n  <b>English</b> | <a href=\"./README.zh-CN.md\">ç®€ä½“ä¸­æ–‡</a>\n</p>\n\n<p align=\"center\">\n  <b>Code2Video: A Code-centric Paradigm for Educational Video Generation</b>\n<br>\nä»¥ä»£ç ä¸ºä¸­å¿ƒçš„æ•™å­¦è§†é¢‘ç”Ÿæˆæ–°èŒƒå¼\n</p>\n<video src=\"assets/video.mp4\" width=\"600\" controls>\n  Your browser does not support the video tag.\n</video>\n\n\n\n\n\n\n<p align=\"center\">\n  <a href=\"https://scholar.google.com.hk/citations?user=9lIMS-EAAAAJ&hl=zh-CN&oi=sra\">Yanzhe Chen*</a>,\n  <a href=\"https://qhlin.me/\">Kevin Qinghong Lin*</a>,\n  <a href=\"https://scholar.google.com/citations?user=h1-3lSoAAAAJ&hl=en\">Mike Zheng Shou</a> <br>\n  Show Lab @ National University of Singapore\n</p>\n\n\n<p align=\"center\">\nÂ  <a href=\"https://arxiv.org/abs/2510.01174\">ğŸ“„ Paper</a> &nbsp; | &nbsp;\nÂ  <a href=\"https://huggingface.co/papers/2510.01174\">ğŸ¤— Daily Paper</a> &nbsp; | &nbsp;\nÂ  <a href=\"https://huggingface.co/datasets/YanzheChen/MMMC\">ğŸ¤— Dataset</a> &nbsp; | &nbsp;\nÂ  <a href=\"https://showlab.github.io/Code2Video/\">ğŸŒ Project Website</a> &nbsp; | &nbsp;\nÂ  <a href=\"https://x.com/KevinQHLin/status/1974199353695941114\">ğŸ’¬ X (Twitter)</a>\n</p>\n\nhttps://github.com/user-attachments/assets/d906423f-734a-41c9-b102-b113ad3b3c25\n\n\n\n<!-- <p align=\"center\">\n<table>\n  <thead>\n    <tr>\n      <th style=\"text-align: center;\">Learning Topic</th>\n      <th style=\"text-align: center;\">Veo3</th>\n      <th style=\"text-align: center;\">Wan2.2</th>\n      <th style=\"text-align: center;\">Code2Video (Ours)</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td style=\"text-align: center; vertical-align: middle;\"><strong>Hanoi Problem</strong></td>\n      <td style=\"text-align: center;\">\n        <img src=\"assets/videos/veo/Hanoi.gif\">\n      </td>\n      <td style=\"text-align: center;\">\n        <img src=\"assets/videos/wan/Hanoi.gif\">\n      </td>\n      <td style=\"text-align: center;\">\n        <img src=\"assets/videos/code2video/Hanoi_4K_SpeedUp.gif\">\n      </td>\n    </tr>\n    <tr>\n      <td style=\"text-align: center; vertical-align: middle;\"><strong>Large Language Model</strong></td>\n      <td style=\"text-align: center;\">\n        <img src=\"assets/videos/veo/LLM.gif\">\n      </td>\n      <td style=\"text-align: center;\">\n        <img src=\"assets/videos/wan/LLM.gif\">\n      </td>\n      <td style=\"text-align: center;\">\n        <img src=\"assets/videos/code2video/LLM_speed.gif\">\n      </td>\n    </tr>\n    <tr>\n      <td style=\"text-align: center; vertical-align: middle;\"><strong>Pure Fourier Series</strong></td>\n      <td style=\"text-align: center;\">\n        <img src=\"assets/videos/veo/fourier.gif\">\n      </td>\n      <td style=\"text-align: center;\">\n        <img src=\"assets/videos/wan/fourier.gif\">\n      </td>\n      <td style=\"text-align: center;\">\n        <img src=\"assets/videos/code2video/fourier_speed.gif\">\n      </td>\n    </tr>\n    </tbody>\n</table>\n</p> -->\n\n<p align=\"center\">\n<table style=\"width: 90%; border-collapse: collapse; text-align: center; margin: auto;\">\n  <thead>\n    <tr>\n      <th style=\"text-align: center; padding: 8px;\">Learning Topic</th>\n      <th style=\"text-align: center; padding: 8px;\">Veo3</th>\n      <th style=\"text-align: center; padding: 8px;\">Wan2.2</th>\n      <th style=\"text-align: center; padding: 8px;\">Code2Video (Ours)</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td style=\"text-align: center; vertical-align: middle; font-weight: bold;\">Hanoi Problem</td>\n      <td>\n        <div style=\"width: 320px; aspect-ratio: 16 / 9; overflow: hidden; margin: auto;\">\n          <img src=\"assets/videos/veo/Hanoi.gif\" style=\"width: 100%; height: 100%; object-fit: cover;\">\n        </div>\n      </td>\n      <td>\n        <div style=\"width: 320px; aspect-ratio: 16 / 9; overflow: hidden; margin: auto;\">\n          <img src=\"assets/videos/wan/Hanoi.gif\" style=\"width: 100%; height: 100%; object-fit: cover;\">\n        </div>\n      </td>\n      <td>\n        <div style=\"width: 320px; aspect-ratio: 16 / 9; overflow: hidden; margin: auto;\">\n          <img src=\"assets/videos/code2video/Hanoi_4K_SpeedUp.gif\" style=\"width: 100%; height: 100%; object-fit: cover;\">\n        </div>\n      </td>\n    </tr>\n    <tr>\n      <td style=\"text-align: center; vertical-align: middle; font-weight: bold;\">Large Language Model</td>\n      <td>\n        <div style=\"width: 320px; aspect-ratio: 16 / 9; overflow: hidden; margin: auto;\">\n          <img src=\"assets/videos/veo/LLM.gif\" style=\"width: 100%; height: 100%; object-fit: cover;\">\n        </div>\n      </td>\n      <td>\n        <div style=\"width: 320px; aspect-ratio: 16 / 9; overflow: hidden; margin: auto;\">\n          <img src=\"assets/videos/wan/LLM.gif\" style=\"width: 100%; height: 100%; object-fit: cover;\">\n        </div>\n      </td>\n      <td>\n        <div style=\"width: 320px; aspect-ratio: 16 / 9; overflow: hidden; margin: auto;\">\n          <img src=\"assets/videos/code2video/LLM_speed.gif\" style=\"width: 100%; height: 100%; object-fit: cover;\">\n        </div>\n      </td>\n    </tr>\n    <tr>\n      <td style=\"text-align: center; vertical-align",
      "default_branch": "main"
    },
    "fetched_at": "2025-10-19T02:42:59.254798"
  },
  {
    "basic_info": {
      "name": "DEIMv2",
      "full_name": "Intellindust-AI-Lab/DEIMv2",
      "owner": "Intellindust-AI-Lab",
      "description": "[DEIMv2] Real Time Object Detection Meets DINOv3 ",
      "url": "https://github.com/Intellindust-AI-Lab/DEIMv2",
      "clone_url": "https://github.com/Intellindust-AI-Lab/DEIMv2.git",
      "ssh_url": "git@github.com:Intellindust-AI-Lab/DEIMv2.git",
      "homepage": "https://intellindust-ai-lab.github.io/projects/DEIMv2/",
      "created_at": "2025-09-19T02:27:19Z",
      "updated_at": "2025-10-19T02:38:42Z",
      "pushed_at": "2025-10-12T02:30:48Z"
    },
    "stats": {
      "stars": 720,
      "forks": 74,
      "watchers": 720,
      "open_issues": 21,
      "size": 533
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 564218,
        "Shell": 2759
      },
      "license": "Other",
      "topics": [
        "detection-transformer",
        "dinov3",
        "object-detection",
        "real-time",
        "real-time-detection"
      ]
    },
    "content": {
      "readme": "<h2 align=\"center\">\n  Real-Time Object Detection Meets DINOv3\n</h2>\n\n<p align=\"center\">\n    <a href=\"https://github.com/Intellindust-AI-Lab/DEIMv2/blob/master/LICENSE\">\n        <img alt=\"license\" src=\"https://img.shields.io/badge/LICENSE-Apache%202.0-blue\">\n    </a>\n    <a href=\"https://arxiv.org/abs/2509.20787\">\n        <img alt=\"arXiv\" src=\"https://img.shields.io/badge/arXiv-2509.20787-red\">\n    </a>\n   <a href=\"https://intellindust-ai-lab.github.io/projects/DEIMv2/\">\n        <img alt=\"project webpage\" src=\"https://img.shields.io/badge/Webpage-DEIMv2-purple\">\n    </a>\n    <a href=\"https://github.com/Intellindust-AI-Lab/DEIMv2/pulls\">\n        <img alt=\"prs\" src=\"https://img.shields.io/github/issues-pr/Intellindust-AI-Lab/DEIMv2\">\n    </a>\n    <a href=\"https://github.com/Intellindust-AI-Lab/DEIMv2/issues\">\n        <img alt=\"issues\" src=\"https://img.shields.io/github/issues/Intellindust-AI-Lab/DEIMv2?color=olive\">\n    </a>\n    <a href=\"https://github.com/Intellindust-AI-Lab/DEIMv2\">\n        <img alt=\"stars\" src=\"https://img.shields.io/github/stars/Intellindust-AI-Lab/DEIMv2\">\n    </a>\n    <a href=\"mailto:shenxi@intellindust.com\">\n        <img alt=\"Contact Us\" src=\"https://img.shields.io/badge/Contact-Email-yellow\">\n    </a>\n</p>\n\n<p align=\"center\">\n    DEIMv2 is an evolution of the DEIM framework while leveraging the rich features from DINOv3. Our method is designed with various model sizes, from an ultra-light version up to S, M, L, and X, to be adaptable for a wide range of scenarios. Across these variants, DEIMv2 achieves state-of-the-art performance, with the S-sized model notably surpassing 50 AP on the challenging COCO benchmark.\n</p>\n\n---\n\n\n<div align=\"center\">\n  <a href=\"http://www.shihuahuang.cn\">Shihua Huang</a><sup>1*</sup>,&nbsp;&nbsp;\n  Yongjie Hou<sup>1,2*</sup>,&nbsp;&nbsp;\n  Longfei Liu<sup>1*</sup>,&nbsp;&nbsp;\n  <a href=\"https://xuanlong-yu.github.io/\">Xuanlong Yu</a><sup>1</sup>,&nbsp;&nbsp;\n  <a href=\"https://xishen0220.github.io\">Xi Shen</a><sup>1â€ </sup>&nbsp;&nbsp;\n</div>\n\n  \n<p align=\"center\">\n<i>\n1. <a href=\"https://intellindust-ai-lab.github.io\"> Intellindust AI Lab</a> &nbsp;&nbsp; 2. Xiamen University &nbsp; <br> \n* Equal Contribution &nbsp;&nbsp; â€  Corresponding Author\n</i>\n</p>\n\n\n<p align=\"center\">\n<strong>If you like our work, please give us a â­!</strong>\n</p>\n\n\n<p align=\"center\">\n  <img src=\"./figures/deimv2_coco_AP_vs_Params.png\" alt=\"Image 1\" width=\"49%\">\n  <img src=\"./figures/deimv2_coco_AP_vs_GFLOPs.png\" alt=\"Image 2\" width=\"49%\">\n</p>\n\n</details>\n\n \n  \n## ğŸš€ Updates\n- [x] **\\[2025.10.2\\]** [DEIMv2 has been integrated into X-AnyLabeling!](https://github.com/Intellindust-AI-Lab/DEIMv2/issues/25#issue-3473960491) Many thanks to the X-AnyLabeling maintainers for making this possible.\n- [x] **\\[2025.9.26\\]** Release DEIMv2 series.\n\n## ğŸ§­ Table of Content\n* [1. ğŸ¤– Model Zoo](#1-model-zoo)\n* [2. âš¡ Quick Start](#2-quick-start)\n* [3. ğŸ› ï¸ Usage](#3-usage)\n* [4. ğŸ§° Tools](#4-tools)\n* [5. ğŸ“œ Citation](#5-citation)\n* [6. ğŸ™ Acknowledgement](#6-acknowledgement)\n* [7. â­ Star History](#7-star-history)\n  \n  \n## 1. Model Zoo\n\n| Model | Dataset | AP | #Params | GFLOPs | Latency (ms) | config | checkpoint | log |\n| :---: | :---: | :---: | :---: | :---: |:------------:| :---: | :---: | :---: |\n| **Atto** | COCO | **23.8** | 0.5M | 0.8 |     1.10     | [yml](./configs/deimv2/deimv2_hgnetv2_atto_coco.yml) | [ckpt](https://drive.google.com/file/d/18sRJXX3FBUigmGJ1y5Oo_DPC5C3JCgYc/view?usp=sharing) | [log](https://drive.google.com/file/d/1M7FLN8EeVHG02kegPN-Wxf_9BlkghZfj/view?usp=sharing) |\n| **Femto** | COCO | **31.0** | 1.0M | 1.7 |     1.45     | [yml](./configs/deimv2/deimv2_hgnetv2_femto_coco.yml) | [ckpt](https://drive.google.com/file/d/16hh6l9Oln9TJng4V0_HNf_Z7uYb7feds/view?usp=sharing) | [log](https://drive.google.com/file/d/1_KWVfOr3bB5TMHTNOmDIAO-tZJmKB9-b/view?usp=sharing) |\n| **Pico** | COCO | **38.5** | 1.5M | 5.2 |     2.13     | [yml](./configs/deimv2/deimv2_hgnetv2_pico_coco.yml) | [ckpt](https://drive.google.com/file/d/1PXpUxYSnQO-zJHtzrCPqQZ3KKatZwzFT/view?usp=sharing) | [log](https://drive.google.com/file/d/1GwyWotYSKmFQdVN9k2MM6atogpbh0lo1/view?usp=sharing) |\n| **N** | COCO | **43.0** | 3.6M | 6.8 |     2.32     | [yml](./configs/deimv2/deimv2_hgnetv2_n_coco.yml) | [ckpt](https://drive.google.com/file/d/1G_Q80EVO4T7LZVPfHwZ3sT65FX5egp9K/view?usp=sharing) | [log](https://drive.google.com/file/d/1QhYfRrUy8HrihD3OwOMJLC-ATr97GInV/view?usp=sharing) |\n| **S** | COCO | **50.9** | 9.7M | 25.6 |     5.78     | [yml](./configs/deimv2/deimv2_dinov3_s_coco.yml) | [ckpt](https://drive.google.com/file/d/1MDOh8UXD39DNSew6rDzGFp1tAVpSGJdL/view?usp=sharing) | [log](https://drive.google.com/file/d/1ydA4lWiTYusV1s3WHq5jSxIq39oxy-Nf/view?usp=sharing) |\n| **M** | COCO | **53.0** | 18.1M | 52.2 |     8.80     | [yml](./configs/deimv2/deimv2_dinov3_m_coco.yml) | [ckpt](https://drive.google.com/file/d/1nPKDHrotusQ748O1cQXJfi5wdShq6bKp/view?usp=sharing) | [log](https://drive.google.com/file/d/1i05Q1-O9UH-2Vb",
      "default_branch": "main"
    },
    "fetched_at": "2025-10-19T02:43:00.383306"
  },
  {
    "basic_info": {
      "name": "LuoGen-agent",
      "full_name": "LuoGen-AI/LuoGen-agent",
      "owner": "LuoGen-AI",
      "description": "ä¸€é”®äº§å‡ºçˆ†æ¬¾è§†é¢‘ï¼š1.è‡ªåŠ¨æå–å¯¹æ ‡æ–‡æ¡ˆ 2.è‡ªåŠ¨è¿›è¡Œæ–‡æ¡ˆä»¿å†™ 3.è‡ªåŠ¨æ ¹æ®æ–‡æ¡ˆå£°éŸ³å…‹éš† 4.è‡ªåŠ¨ç”Ÿæˆæ•°å­—äººå£æ’­ 5.è‡ªåŠ¨æ·»åŠ å­—å¹• 6.è‡ªåŠ¨æ·»åŠ èƒŒæ™¯éŸ³ä¹ 7.è‡ªåŠ¨æ·»åŠ è§†é¢‘æ ‡é¢˜ 8.è‡ªåŠ¨ç”Ÿæˆè§†é¢‘å°é¢ 9.è‡ªåŠ¨å°†è§†é¢‘å‘å¸ƒåˆ°å„å¹³å°",
      "url": "https://github.com/LuoGen-AI/LuoGen-agent",
      "clone_url": "https://github.com/LuoGen-AI/LuoGen-agent.git",
      "ssh_url": "git@github.com:LuoGen-AI/LuoGen-agent.git",
      "homepage": null,
      "created_at": "2025-10-02T12:12:18Z",
      "updated_at": "2025-10-18T18:12:02Z",
      "pushed_at": "2025-10-03T12:03:31Z"
    },
    "stats": {
      "stars": 714,
      "forks": 74,
      "watchers": 714,
      "open_issues": 3,
      "size": 217
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 55666,
        "Batchfile": 1966
      },
      "license": "GNU General Public License v3.0",
      "topics": []
    },
    "content": {
      "readme": "# `çœŸæ­£çš„å•†ä¸šçº§åº”ç”¨` ğŸš€ ä¸€é”®ç”Ÿæˆçˆ†æ¬¾è§†é¢‘è‡ªåŠ¨åŒ–å·¥å…·\n\n![å®¢æˆ·ç«¯UI](show.png)\n\n- ç”±äºä»£ç ä½“ç§¯åŠæ¨¡å‹æ–‡ä»¶è¿‡å¤§ï¼Œè¯·è¯¸ä½ç§»æ­¥ [ä»£ç åœ°å€](ä»£ç åœ°å€.txt) è¿›è¡Œä¸‹è½½ã€‚\n- ç”±äºè¯¥åº”ç”¨ä¸ºæœ¬åœ°è¿è¡Œçš„å®¢æˆ·ç«¯åº”ç”¨ï¼Œä¸ºäº†è¯¸ä½çš„ä½¿ç”¨ä½“éªŒï¼Œå…ˆè¿›è¡Œ [ä½¿ç”¨å‰å¿…è£…](ä½¿ç”¨å‰å¿…è£….txt) è¿›è¡Œä¸‹è½½å®‰è£…ã€‚\n\n> è¯¸å¤šä¸ä¾¿ï¼Œæ•¬è¯·è°…è§£ã€‚\n\n**é¡¹ç›®æè¿°**  \næœ¬å·¥å…·é€šè¿‡è‡ªåŠ¨åŒ–æµç¨‹ï¼Œå¸®åŠ©ç”¨æˆ·å¿«é€Ÿç”Ÿæˆé«˜è´¨é‡çš„æ•°å­—äººå£æ’­è§†é¢‘å¹¶å‘å¸ƒè‡³å¤šå¹³å°ã€‚æ ¸å¿ƒåŠŸèƒ½åŒ…æ‹¬ï¼š\n- ğŸ“ **æ™ºèƒ½æ–‡æ¡ˆå¤„ç†**ï¼šè‡ªåŠ¨æå–å¯¹æ ‡æ–‡æ¡ˆ + æ™ºèƒ½ä»¿å†™ä¼˜åŒ–\n- ğŸ¤ **å£°éŸ³å…‹éš†**ï¼šåŸºäº Whisper å’Œ CosyVoice å®ç°é«˜ä¿çœŸè¯­éŸ³åˆæˆ\n- ğŸ‘¥ **æ•°å­—äººç”Ÿæˆ**ï¼šé›†æˆ HeyGem å®ç°è‡ªç„¶å£æ’­æ•ˆæœ\n- ğŸ¬ **å…¨æµç¨‹è§†é¢‘åˆ¶ä½œ**ï¼šå­—å¹•/BGM/æ ‡é¢˜/å°é¢è‡ªåŠ¨ç”Ÿæˆ + å¤šå¹³å°å‘å¸ƒ\n\n## ğŸŒŸ æ ¸å¿ƒåŠŸèƒ½\n| åŠŸèƒ½æ¨¡å—          | æŠ€æœ¯å®ç°                     |\n|-------------------|------------------------------|\n| è¯­éŸ³å…‹éš†          | Whisperï¼ˆè¯­éŸ³è¯†åˆ«ï¼‰ + CosyVoiceï¼ˆè¯­éŸ³åˆæˆï¼‰ |\n| æ•°å­—äººå£æ’­        | HeyGem æ•°å­—äººå¼•æ“            |\n| è§†é¢‘åæœŸ          | FFmpegï¼ˆåˆæˆï¼‰ + åŠ¨æ€å­—å¹•    |\n| å¤šå¹³å°å‘å¸ƒ        | å¹³å° API é›†æˆï¼ˆæŠ–éŸ³/Bç«™ç­‰ï¼‰  |\n\n\n## ğŸ¤ è‡´è°¢\næœ¬é¡¹ç›®åŸºäºä»¥ä¸‹ä¼˜ç§€å¼€æºé¡¹ç›®æ„å»ºï¼š\n- [social-auto-upload](https://github.com/...) - å¤šå¹³å°å‘å¸ƒæ¡†æ¶\n- [CosyVoice](https://github.com/tencent-ailab/cosyvoice) - é«˜è´¨é‡è¯­éŸ³åˆæˆ\n- [HeyGem](https://github.com/...) - æ•°å­—äººé©±åŠ¨å¼•æ“\n- [Whisper](https://github.com/openai/whisper) - ç²¾å‡†è¯­éŸ³è¯†åˆ«\n\n\n## âš ï¸ä½¿ç”¨é™åˆ¶\n- æœ¬é¡¹ç›®ä»…é™ä¸ªäººå­¦ä¹ ã€ç ”ç©¶ä½¿ç”¨ï¼Œä¸¥ç¦ä»»ä½•å½¢å¼çš„å•†ä¸šç”¨é€”ï¼ˆåŒ…æ‹¬ä½†ä¸é™äºå”®å–å·¥å…·ã€æä¾›ä»˜è´¹æœåŠ¡ç­‰ï¼‰ã€‚\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-10-19T02:43:01.522544"
  },
  {
    "basic_info": {
      "name": "LongLive",
      "full_name": "NVlabs/LongLive",
      "owner": "NVlabs",
      "description": "LongLive: Real-time Interactive Long Video Generation",
      "url": "https://github.com/NVlabs/LongLive",
      "clone_url": "https://github.com/NVlabs/LongLive.git",
      "ssh_url": "git@github.com:NVlabs/LongLive.git",
      "homepage": "https://nvlabs.github.io/LongLive",
      "created_at": "2025-09-22T22:39:24Z",
      "updated_at": "2025-10-18T20:02:00Z",
      "pushed_at": "2025-10-13T09:42:05Z"
    },
    "stats": {
      "stars": 706,
      "forks": 42,
      "watchers": 706,
      "open_issues": 9,
      "size": 442264
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 628496,
        "Shell": 796
      },
      "license": "Other",
      "topics": [
        "efficient-tuning",
        "interactive",
        "long-context",
        "real-time",
        "sparse-attention",
        "video-genenratio"
      ]
    },
    "content": {
      "readme": "<p align=\"center\" style=\"border-radius: 10px\">\n  <img src=\"assets/LongLive-logo.png\" width=\"100%\" alt=\"logo\"/>\n</p>\n\n# ğŸ¬ LongLive: Real-time Interactive Long Video Generation\n\n[![Paper](https://img.shields.io/badge/ArXiv-Paper-brown)](https://arxiv.org/abs/2509.22622)\n[![Code](https://img.shields.io/badge/GitHub-LongLive-blue)](https://github.com/NVlabs/LongLive)\n[![Model](https://img.shields.io/badge/HuggingFace-Model-yellow)](https://huggingface.co/Efficient-Large-Model/LongLive-1.3B)\n[![Video](https://img.shields.io/badge/YouTube-Video-red)](https://www.youtube.com/watch?v=CO1QC7BNvig)\n[![Demo](https://img.shields.io/badge/Demo-Page-bron)](https://nvlabs.github.io/LongLive)\n\n<div align=\"center\">\n\n[![Watch the video](assets/video-first-frame.png)](https://www.youtube.com/watch?v=CO1QC7BNvig)\n[![Watch the video](assets/Comparison_with_Sora2.png)](https://x.com/yukangchen_/status/1973405662177529993)\n\n</div>\n\n## ğŸ’¡ TLDR: Turn interactive prompts into long videosâ€”instantly, as you type!\n\n**LongLive: Real-time Interactive Long Video Generation [[Paper](https://arxiv.org/abs/2509.22622)]** <br />\n[Shuai Yang](https://andysonys.github.io/), [Wei Huang](https://aaron-weihuang.com/), [Ruihang Chu](https://ruihang-chu.github.io/), [Yicheng Xiao](https://easonxiao-888.github.io/), [Yuyang Zhao](https://yuyangzhao.com/), [Xianbang Wang](https://peppaking8.github.io/), [Muyang Li](https://lmxyy.me/), [Enze Xie](https://xieenze.github.io/), [Yingcong Chen](https://www.yingcong.me/), [Yao Lu](https://scholar.google.com/citations?user=OI7zFmwAAAAJ&hl=en), [Song Han](http://songhan.mit.edu/), [Yukang Chen](https://yukangchen.com/) <br />\n\nWe present LongLive, a frame-level autoregressive (AR) framework for real-time and interactive long video generation. Long video generation presents challenges in both efficiency and quality. Diffusion and Diffusion-Forcing models can produce high-quality videos but suffer from low efficiency due to bidirectional attention. Causal attention AR models support KV caching for faster inference, but often degrade in quality on long videos due to memory challenges during long-video training. In addition, beyond static prompt-based generation, interactive capabilities, such as streaming prompt inputs, are critical for dynamic content creation, enabling users to guide narratives in real time. This interactive requirement significantly increases complexity, especially in ensuring visual consistency and semantic coherence during prompt transitions. To address these challenges, LongLive adopts a causal, frame-level AR design that integrates a KV-recache mechanism that refreshes cached states with new prompts for smooth, adherent switches; streaming long tuning to enable long video training and to align training and inference (train-long-test-long); and short window attention paired with a frame-level attention sink, shorten as frame sink, preserving long-range consistency while enabling faster generation. With these key designs, LongLive fine-tunes a 1.3B-parameter short-clip model to minute-long generation in just 32 GPU-days. At inference, LongLive sustains 20.7 FPS on a single NVIDIA H100, achieves strong performance on VBench in both short and long videos. LongLive supports up to 240-second videos on a single H100 GPU. LongLive further supports INT8-quantized inference with only marginal quality loss.\n\n## TABLE OF CONTENTS\n1. [News](#news)\n2. [Highlights](#highlights)\n3. [Introduction](#introduction)\n4. [Installation](#installation)\n5. [Inference](#inference)\n6. [Training](#training)\n7. [How to contribute](#how-to-contribute)\n8. [Citation](#citation)\n9. [License](#license)\n10. [Acknowledgement](#acknowledgement)\n\n## News\n- [x] [2025.10.11] Many thanks to @yondonfu for building an interactive UI based on LongLive. Please check it here: https://github.com/daydreamlive/scope\n- [x] [2025.10.1] We compare Sora2 (+ GPT-5 prompt engineering) with LongLive-1.3B in the interactive long video generation. See [here](https://x.com/yukangchen_/status/1973405662177529993) for details.\n- [x] [2025.9.30] We release [example prompts](https://github.com/NVlabs/LongLive/tree/main/example) to reproduce our demo videos.\n- [x] [2025.9.29] We release [Paper](https://arxiv.org/abs/2509.22622), this GitHub repo [LongLive](https://github.com/NVlabs/LongLive) with all training and inference code, the model weight [LongLive-1.3B](https://huggingface.co/Efficient-Large-Model/LongLive-1.3B), and demo page [Website](https://nvlabs.github.io/LongLive).\n\n## Highlights\n1. **Long Video Gen**: LongLive supports up to 240s video generation, with visual consistency.\n2. **Real-time Inference**: LongLive supports 20.7 FPS generation speed on a single H100 GPU, and 24.8 FPS with FP8 quantization with marginal quality loss.\n3. **Efficient Fine-tuning**: LongLive extends a short-clip model to minute-long generation in 32 H100 GPU-days.\n\n## Introduction\n<p align=\"center\" style=\"border-radius: 10px\">\n  <img src=\"assets/pipeline.jpg\" width",
      "default_branch": "main"
    },
    "fetched_at": "2025-10-19T02:43:02.659769"
  }
]