[
  {
    "basic_info": {
      "name": "Awesome-Nano-Banana-images",
      "full_name": "PicoTrex/Awesome-Nano-Banana-images",
      "owner": "PicoTrex",
      "description": "A curated collection of fun and creative examples generated with Nano Banana🍌, Gemini-2.5-flash-image based model. We also release Nano-consistent-150K openly to support the community's development of image generation and unified models(click to website to see our blog)",
      "url": "https://github.com/PicoTrex/Awesome-Nano-Banana-images",
      "clone_url": "https://github.com/PicoTrex/Awesome-Nano-Banana-images.git",
      "ssh_url": "git@github.com:PicoTrex/Awesome-Nano-Banana-images.git",
      "homepage": "https://picotrex.github.io/Awesome-Nano-Banana-images/",
      "created_at": "2025-08-28T17:03:09Z",
      "updated_at": "2025-09-27T02:00:51Z",
      "pushed_at": "2025-09-24T09:59:16Z"
    },
    "stats": {
      "stars": 12382,
      "forks": 1266,
      "watchers": 12382,
      "open_issues": 11,
      "size": 182957
    },
    "tech_info": {
      "language": null,
      "languages": {},
      "license": "Apache License 2.0",
      "topics": [
        "awesome",
        "gemini-2-5-flash-image",
        "nano-banana"
      ]
    },
    "content": {
      "readme": "<div align=\"center\">\n\n<img src=\"images/logo.jpg\"  alt=\"输入图片\"> \n\n[![License: CC BY 4.0](https://img.shields.io/badge/License-CC_BY_4.0-lightgrey.svg)](LICENSE)\n[![Chinese](https://img.shields.io/badge/Chinese-Click_to_View-orange)](README.md)\n[![English](https://img.shields.io/badge/English-Click_to_View-yellow)](README_en.md)\n[![Japanese](https://img.shields.io/badge/日本語-クリックして表示-green)](README_ja.md)\n[![Korean](https://img.shields.io/badge/한국어-눌러서_보기-blue)](README_kr.md)\n[![Spanish](https://img.shields.io/badge/Español-Ver_Traducción-blueviolet)](README_es.md)\n[![Turkish](https://img.shields.io/badge/Türkçe-Görüntülemek_için_Tıklayın-red)](README_tr.md)\n\n</div>\n\n> [!NOTE]\n> 我们提出 Nano-consistent-150k——首个基于 Nano-Banana 构建、规模超过 150k 的高质量数据集，专为在多样而复杂的编辑场景中保持人物身份一致性而设计。其一大特点是卓越的身份一致性：针对同一人像，我们在多种任务与指令下提供了 35 种以上不同的编辑结果。以一致的人物身份为锚点，该数据集使得围绕同一主体在多种编辑任务、指令与模态之间无缝衔接的交错（interleaved）数据构建成为可能。\n<a href='https://picotrex.github.io/Awesome-Nano-Banana-images/'><img src='https://img.shields.io/badge/🌐 Website-Blog-orange' height=\"25\"></a>\n<a href='https://huggingface.co/datasets/Yejy53/Nano-consistent-150k'><img src='https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Dataset-yellow' height=\"25\"></a>\n\n## 🍌 Introduction\n\n欢迎来到 Nano-banana 精选图片库！🤗 \n\n**我们收集了Nano-banana在各个任务场景下生成的令人惊艳的图片和提示词**，全方位展示Google在图像生成与编辑的无限可能。希望能帮助你更好地了解Nano-banana。快一起解锁 Nano-banana 的多图融合与创意编辑力量吧！✨\n\n这些案例主要来源于Twitter/ X 🐦、小红书📕等自媒体平台。\n\n喜欢就点 ⭐ Star 收藏起来吧！\n\n## 📰 News\n\n- **2025年9月24日：** 5️⃣ 第五次仓库更新\n- **2025年9月18日：** 我们发布了 [**Nano-consistent-150k**](https://picotrex.github.io/Awesome-Nano-Banana-images/) 数据集\n- **2025年9月16日：** 4️⃣ 第四次仓库更新\n- **2025年9月9日：** 3️⃣ 第三次仓库更新\n- **2025年9月3日：** 2️⃣ 第二次仓库更新\n- **2025年8月28日：** 🎉 1️⃣ ${\\color{red} 第一次\\ Awesome-Nano-Banana-images \\ 更新!}$\n\n## 📑 Menu\n\n- [🍌 Introduction](#-introduction)\n- [📰 News](#-news)\n- [📑 Menu](#-menu)\n- [🖼️ 例子](#️-例子)\n  - [例 1: 插画变手办（by @ZHO\\_ZHO\\_ZHO）](#例-1-插画变手办by-zho_zho_zho)\n  - [例 2: 根据地图箭头生成地面视角图片（by @tokumin）](#例-2-根据地图箭头生成地面视角图片by-tokumin)\n  - [例 3: 真实世界的AR信息化（by @bilawalsidhu）](#例-3-真实世界的ar信息化by-bilawalsidhu)\n  - [例 4: 分离出3D建筑/制作等距模型（by @Zieeett）](#例-4-分离出3d建筑制作等距模型by-zieeett)\n  - [例 5: 不同时代自己的照片（by @AmirMushich）](#例-5-不同时代自己的照片by-amirmushich)\n  - [例 6: 多参考图像生成（by @MrDavids1）](#例-6-多参考图像生成by-mrdavids1)\n  - [例 7: 自动修图（by @op7418）](#例-7-自动修图by-op7418)\n  - [例 8: 手绘图控制多角色姿态（by @op7418）](#例-8-手绘图控制多角色姿态by-op7418)\n  - [例 9: 跨视角图像生成（by @op7418）](#例-9-跨视角图像生成by-op7418)\n  - [例 10: 定制人物贴纸（by @op7418）](#例-10-定制人物贴纸by-op7418)\n  - [例 11: 动漫转真人Coser（by @ZHO\\_ZHO\\_ZHO）](#例-11-动漫转真人coserby-zho_zho_zho)\n  - [例 12: 生成角色设定（by @ZHO\\_ZHO\\_ZHO）](#例-12-生成角色设定by-zho_zho_zho)\n  - [例 13: 色卡线稿上色（by @ZHO\\_ZHO\\_ZHO）](#例-13-色卡线稿上色by-zho_zho_zho)\n  - [例 14: 文章信息图（by @黄建同学）](#例-14-文章信息图by-黄建同学)\n  - [例 15: 更换多种发型（by @balconychy）](#例-15-更换多种发型by-balconychy)\n  - [例 16: 模型标注讲解图（by @berryxia\\_ai）](#例-16-模型标注讲解图by-berryxia_ai)\n  - [例 17: 定制大理石雕塑（by @umesh\\_ai）](#例-17-定制大理石雕塑by-umesh_ai)\n  - [例 18: 根据食材做菜（by @Gdgtify）](#例-18-根据食材做菜by-gdgtify)\n  - [例 19: 数学题推理（by @Gorden Sun）](#例-19-数学题推理by-gorden-sun)\n  - [例 20: 旧照片上色（by @GeminiApp）](#例-20-旧照片上色by-geminiapp)\n  - [例 21: OOTD穿搭（by @302.AI）](#例-21-ootd穿搭by-302ai)\n  - [例 22: 人物换衣（by @skirano）](#例-22-人物换衣by-skirano)\n  - [例 23: 多视图结果生成（by @Error\\_HTTP\\_404）](#例-23-多视图结果生成by-error_http_404)\n  - [例 24: 电影分镜（by @GeminiApp）](#例-24-电影分镜by-geminiapp)\n  - [例 25: 人物姿势修改（by @arrakis\\_ai）](#例-25-人物姿势修改by-arrakis_ai)\n  - [例 26: 线稿图生成图像（by @ZHO\\_ZHO\\_ZHO）](#例-26-线稿图生成图像by-zho_zho_zho)\n  - [例 27: 为图像添加水印（by @AiMachete）](#例-27-为图像添加水印by-aimachete)\n  - [例 28: 知识推理生成图像（by @icreatelife）](#例-28-知识推理生成图像by-icreatelife)\n  - [例 29: 红笔批注（by @AiMachete）](#例-29-红笔批注by-aimachete)\n  - [例 30: 爆炸的食物（by @icreatelife）](#例-30-爆炸的食物by-icreatelife)\n  - [例 31: 制作漫画书（by @icreatelife）](#例-31-制作漫画书by-icreatelife)\n  - [例 32: 动作人偶（by @icreatelife）](#例-32-动作人偶by-icreatelife)\n  - [例 33: 地图生成等距建筑（by @demishassabis）](#例-33-地图生成等距建筑by-demishassabis)\n  - [例 34: 参考图控制人物表情（by @ZHO\\_ZHO\\_ZHO）](#例-34-参考图控制人物表情by-zho_zho_zho)\n  - [例 35: 插画绘画过程四格（by @ZHO\\_ZHO\\_ZHO）](#例-35-插画绘画过程四格by-zho_zho_zho)\n  - [例 36: 虚拟试妆（by @ZHO\\_ZHO\\_ZHO）](#例-36-虚拟试妆by-zho_zho_zho)\n  - [例 37: 妆面分析（by @ZHO\\_ZHO\\_ZHO）](#例-37-妆面分析by-zho_zho_zho)\n  - [例 38: Google地图视角下的中土世界（by @TechHallo）](#例-38-google地图视角下的中土世界by-techhallo)\n  - [例 39: 印刷插画生成（by @Umesh）](#例-39-印刷插画生成by-umesh)\n  - [例 40: 超多人物姿势生成（by @tapehead\\_Lab）](#例-40-超多人物姿势生成by-tapehead_lab)\n  - [例 41: 物品包装生成（by @ZHO\\_ZHO\\_ZHO）](#例-41-物品包装生成by-zho_zho_zho)\n  - [例 42: 叠加滤镜/材质（by @ZHO\\_ZHO\\_ZHO）](#例-42-叠加滤镜材质by-zho_zho_zho)\n  - [例 43: 控制人物脸型（by @ZHO\\_ZHO\\_ZHO）](#例-43-控制人物脸型by-zho_zho_zho)\n  - [例 44: 光影控制（by @ZHO\\_ZHO\\_ZHO）](#例-44-光影控制by-zho_zho_zho)\n  - [例 45: 乐高玩具小人（by @ZHO\\_ZHO\\_ZHO）](#例-45-乐高玩具小人by-zho_zho_zho)\n  - [例 46: 高达模型小人（by @ZHO\\_ZHO\\_ZHO）](#例-46-高达模型小人by-zho_zho_zho)\n  - [例 47: 硬件拆解图（by @AIimagined）](#例-47-硬件拆解图by-aiimagined)\n  - [例 48: 食物卡路里标注（by @icreatelife）](#例-48-食物卡路里标注by-icreatelife)\n  - [例 49: 提取信息并放置透明图层（by @nglprz）](#例-49-提取信息并放置透明图层by-nglprz)\n  - [例 50: 图像外扩修复（by @bwabbage）](#例-50-图像外扩修复",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-27T02:13:57.073998"
  },
  {
    "basic_info": {
      "name": "term.everything",
      "full_name": "mmulet/term.everything",
      "owner": "mmulet",
      "description": "Run any GUI app in the terminal❗",
      "url": "https://github.com/mmulet/term.everything",
      "clone_url": "https://github.com/mmulet/term.everything.git",
      "ssh_url": "git@github.com:mmulet/term.everything.git",
      "homepage": "",
      "created_at": "2025-09-07T02:52:48Z",
      "updated_at": "2025-09-27T00:50:27Z",
      "pushed_at": "2025-09-26T22:01:08Z"
    },
    "stats": {
      "stars": 6033,
      "forks": 131,
      "watchers": 6033,
      "open_issues": 17,
      "size": 53552
    },
    "tech_info": {
      "language": "TypeScript",
      "languages": {
        "TypeScript": 219331,
        "C++": 39626,
        "JavaScript": 2607,
        "Meson": 2286,
        "Shell": 1981,
        "C": 314
      },
      "license": "GNU Affero General Public License v3.0",
      "topics": [
        "alacritty",
        "cli",
        "foss",
        "iterm2",
        "kitty",
        "linux",
        "ssh",
        "terminal",
        "wayland",
        "wayland-compositor"
      ]
    },
    "content": {
      "readme": "\n\n\n<table>\n  <tr>\n    <td valign=\"middle\">\n      <img width=\"128\" height=\"128\" alt=\"icon2\" src=\"./resources/icon.png\" />\n    </td>\n    <td><h1>Term.Everything❗</h1></td>\n    <td><a href=\"https://github.com/mmulet/term.everything/releases\">Download the beta test now!</a></td>\n    <td><a href=\"./resources/HowIDidIt.md\">HowIDidIt.md</a></td>\n  </tr>\n  <tr>\n    <td></td>\n    <td>Works on both x11 and Wayland host systems.</td>\n    <td></td>\n    <td></td>\n  </tr>\n</table>\n\n## Run every GUI app in the terminal!\n\n![warp_into_terminal0001-0195](./resources/graphics/warp_in_2.gif)\n\n## Even over ssh!\nBehold as I play a [video game in a font](https://github.com/mmulet/font-game-engine) in a web browser in a terminal transmitted over ssh (with one hand tied behind my back)!\n\n![ssh_example](./resources/graphics/ssh_example.gif)\n\n### Read about how it works!\nCheck out [HowIDidIt.md](./resources/HowIDidIt.md)\n\n## More Examples\nThe quality of the window is limited to the number of rows and columns in your\nterminal. If you increase the resolution (ctrl - in alacritty, check your\nterminal) the quality will go up, (but performance may go down).\n\nHere I open up the Wing It! movie, and increase the quality until I get both\na good frame rate and resolution:\n\n![increase resolution](./resources/graphics/show_increase_res.gif)\n\n----------------\n\nIf your terminal supports images (like [kitty](https://sw.kovidgoyal.net/kitty/)\nor [iterm2](https://iterm2.com/)) you can render windows at full resolution\n(performance may degrade).\n\nIn this example, on my mac, I open iTerm2 ssh into ubuntu and open firefox\nat full resolution:\n\n![full_resultion](resources/graphics/full_resultion.gif)\n\n------------\n\nI feel like every single day I hear about another terminal file viewer. I say, stop making terminal file viewers because you can just use the file viewer you already have! In your terminal!\n\n![file_manager](./resources/graphics/file_manager.gif)\n\n-------------\n\nTerminal in a terminal in a terminal in a terminal in a terminal.... it's terminals all the way down.\n![terminal_in_terminal](./resources/graphics/terminal_in_terminal.gif)\n\n-------------\nWith only a small amount hacking, it can run Doom (shareware episode)!\n\n![Doom](./resources/graphics/doom.gif)\n------\nRun an entire Desktop in your terminal!\n[@ismail-yilmaz](https://github.com/ismail-yilmaz) is running Firefox, on [KDE Neon](https://neon.kde.org) in a [VM](https://gitlab.gnome.org/GNOME/gnome-boxes) on [Bobcat](https://github.com/ismail-yilmaz/Bobcat)\n![Desktop in VM](./resources/graphics/desktop_in_vm.gif)\n\nAnd this isn't even full resolution! Checkout the [full vid in in the discussions](https://github.com/mmulet/term.everything/discussions/16#discussioncomment-14390137)\n\n## About\n`term.everything❗` is a Linux CLI program to run GUI windows in your terminal. Specifically, `term.everything❗` is a built-from-scratch [Wayland](https://wiki.archlinux.org/title/Wayland) compositor that outputs to a terminal rather than your monitor.\n\n>Don't know what Wayland is or just want to know more about how this works? Then, head over to [HowIDidIt.md](./resources/HowIDidIt.md) where I will explain how everything works in detail.\n\n## Try it out!\n[Download the beta test now!](https://github.com/mmulet/term.everything/releases)\n\n## Roadmap\n1. [x] Term some things <--- This is where we are at\n  - Some apps or (even most apps) may fail to launch or even crash! Please create [an issue]( https://github.com/mmulet/term.everything/issues) if you have problems.\n2. [ ] Term most things\n3. [ ] Term everything❗\n\n## Help and Usage\nCheck out the [help file here](./resources/help.md) for a usage guide on how to use `term.everything❗`\n\n## Contributing\nterm.everything❗ is written in developer friendly [Typescript](https://www.typescriptlang.org/) using the [bun](https://bun.com/) engine, with a just a smidge of C++.\nSee [./Contributing.md](./Contributing.md).\n\n## Legal:\n\nterm.everything❗ copyright 2025 Late for Dinner Studios, LLC\n---\nFontemon copyright 2021 Late for Dinner Studios, LLC\n---\nWing It! movie is licensed under the Creative Commons Attribution 4.0 license\n[Wing it licensing page](https://studio.blender.org/projects/wing-it/pages/licensing/)\nAttribution:\n(CC) Blender Foundation | studio.blender.org\n---\nDoom shareware episode is copyright 1993 id Software\n---\n\n## Bonus:\nThis is Gwerm the Term Worm.\n\n![this is gwern](./resources/graphics/this_is_gwern.gif)\n\nHe is doing okay. Thanks for asking.\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-27T02:13:58.239272"
  },
  {
    "basic_info": {
      "name": "chrome-devtools-mcp",
      "full_name": "ChromeDevTools/chrome-devtools-mcp",
      "owner": "ChromeDevTools",
      "description": "Chrome DevTools for coding agents",
      "url": "https://github.com/ChromeDevTools/chrome-devtools-mcp",
      "clone_url": "https://github.com/ChromeDevTools/chrome-devtools-mcp.git",
      "ssh_url": "git@github.com:ChromeDevTools/chrome-devtools-mcp.git",
      "homepage": "https://npmjs.org/package/chrome-devtools-mcp",
      "created_at": "2025-09-11T10:39:55Z",
      "updated_at": "2025-09-27T02:11:34Z",
      "pushed_at": "2025-09-26T14:04:06Z"
    },
    "stats": {
      "stars": 3554,
      "forks": 187,
      "watchers": 3554,
      "open_issues": 27,
      "size": 1322
    },
    "tech_info": {
      "language": "TypeScript",
      "languages": {
        "TypeScript": 195000,
        "JavaScript": 5815
      },
      "license": "Apache License 2.0",
      "topics": [
        "browser",
        "chrome",
        "chrome-devtools",
        "debugging",
        "devtools",
        "mcp",
        "mcp-server",
        "puppeteer"
      ]
    },
    "content": {
      "readme": "# Chrome DevTools MCP\n\n[![npm chrome-devtools-mcp package](https://img.shields.io/npm/v/chrome-devtools-mcp.svg)](https://npmjs.org/package/chrome-devtools-mcp)\n\n`chrome-devtools-mcp` lets your coding agent (such as Gemini, Claude, Cursor or Copilot)\ncontrol and inspect a live Chrome browser. It acts as a Model-Context-Protocol\n(MCP) server, giving your AI coding assistant access to the full power of\nChrome DevTools for reliable automation, in-depth debugging, and performance analysis.\n\n## Key features\n\n- **Get performance insights**: Uses [Chrome\n  DevTools](https://github.com/ChromeDevTools/devtools-frontend) to record\n  traces and extract actionable performance insights.\n- **Advanced browser debugging**: Analyze network requests, take screenshots and\n  check the browser console.\n- **Reliable automation**. Uses\n  [puppeteer](https://github.com/puppeteer/puppeteer) to automate actions in\n  Chrome and automatically wait for action results.\n\n## Disclaimers\n\n`chrome-devtools-mcp` exposes content of the browser instance to the MCP clients\nallowing them to inspect, debug, and modify any data in the browser or DevTools.\nAvoid sharing sensitive or personal information that you don't want to share with\nMCP clients.\n\n## Requirements\n\n- [Node.js 22.12.0](https://nodejs.org/) or newer.\n- [Chrome](https://www.google.com/chrome/) current stable version or newer.\n- [npm](https://www.npmjs.com/).\n\n## Getting started\n\nAdd the following config to your MCP client:\n\n```json\n{\n  \"mcpServers\": {\n    \"chrome-devtools\": {\n      \"command\": \"npx\",\n      \"args\": [\"chrome-devtools-mcp@latest\"]\n    }\n  }\n}\n```\n\n> [!NOTE]  \n> Using `chrome-devtools-mcp@latest` ensures that your MCP client will always use the latest version of the Chrome DevTools MCP server.\n\n### MCP Client configuration\n\n<details>\n  <summary>Claude Code</summary>\n    Use the Claude Code CLI to add the Chrome DevTools MCP server (<a href=\"https://docs.anthropic.com/en/docs/claude-code/mcp\">guide</a>):\n\n```bash\nclaude mcp add chrome-devtools npx chrome-devtools-mcp@latest\n```\n\n</details>\n\n<details>\n  <summary>Cline</summary>\n  Follow https://docs.cline.bot/mcp/configuring-mcp-servers and use the config provided above.\n</details>\n\n<details>\n  <summary>Codex</summary>\n  Follow the <a href=\"https://github.com/openai/codex/blob/main/docs/advanced.md#model-context-protocol-mcp\">configure MCP guide</a>\n  using the standard config from above. You can also install the Chrome DevTools MCP server using the Codex CLI:\n\n```bash\ncodex mcp add chrome-devtools -- npx chrome-devtools-mcp@latest\n```\n\n</details>\n\n<details>\n  <summary>Copilot / VS Code</summary>\n  Follow the MCP install <a href=\"https://code.visualstudio.com/docs/copilot/chat/mcp-servers#_add-an-mcp-server\">guide</a>,\n  with the standard config from above. You can also install the Chrome DevTools MCP server using the VS Code CLI:\n  \n  ```bash\n  code --add-mcp '{\"name\":\"chrome-devtools\",\"command\":\"npx\",\"args\":[\"chrome-devtools-mcp@latest\"]}'\n  ```\n</details>\n\n<details>\n  <summary>Cursor</summary>\n\n**Click the button to install:**\n\n[<img src=\"https://cursor.com/deeplink/mcp-install-dark.svg\" alt=\"Install in Cursor\">](https://cursor.com/en/install-mcp?name=chrome-devtools&config=eyJjb21tYW5kIjoibnB4IGNocm9tZS1kZXZ0b29scy1tY3BAbGF0ZXN0In0%3D)\n\n**Or install manually:**\n\nGo to `Cursor Settings` -> `MCP` -> `New MCP Server`. Use the config provided above.\n\n</details>\n\n<details>\n  <summary>Gemini CLI</summary>\nInstall the Chrome DevTools MCP server using the Gemini CLI.\n\n**Project wide:**\n\n```bash\ngemini mcp add chrome-devtools npx chrome-devtools-mcp@latest\n```\n\n**Globally:**\n\n```bash\ngemini mcp add -s user chrome-devtools npx chrome-devtools-mcp@latest\n```\n\nAlternatively, follow the <a href=\"https://github.com/google-gemini/gemini-cli/blob/main/docs/tools/mcp-server.md#how-to-set-up-your-mcp-server\">MCP guide</a> and use the standard config from above.\n\n</details>\n\n<details>\n  <summary>Gemini Code Assist</summary>\n  Follow the <a href=\"https://cloud.google.com/gemini/docs/codeassist/use-agentic-chat-pair-programmer#configure-mcp-servers\">configure MCP guide</a>\n  using the standard config from above.\n</details>\n\n<details>\n  <summary>JetBrains AI Assistant & Junie</summary>\n\nGo to `Settings | Tools | AI Assistant | Model Context Protocol (MCP)` -> `Add`. Use the config provided above.\nThe same way chrome-devtools-mcp can be configured for JetBrains Junie in `Settings | Tools | Junie | MCP Settings` -> `Add`. Use the config provided above.\n\n</details>\n\n### Your first prompt\n\nEnter the following prompt in your MCP Client to check if everything is working:\n\n```\nCheck the performance of https://developers.chrome.com\n```\n\nYour MCP client should open the browser and record a performance trace.\n\n> [!NOTE]  \n> The MCP server will start the browser automatically once the MCP client uses a tool that requires a running browser instance. Connecting to the Chrome DevTools MCP server on its own will not automatically start the browser.\n\n## To",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-27T02:13:59.382856"
  },
  {
    "basic_info": {
      "name": "LidAngleSensor",
      "full_name": "samhenrigold/LidAngleSensor",
      "owner": "samhenrigold",
      "description": "tfw when you when your lid when uhh angle your lid sensor",
      "url": "https://github.com/samhenrigold/LidAngleSensor",
      "clone_url": "https://github.com/samhenrigold/LidAngleSensor.git",
      "ssh_url": "git@github.com:samhenrigold/LidAngleSensor.git",
      "homepage": "https://samhenri.gold",
      "created_at": "2025-09-06T19:07:20Z",
      "updated_at": "2025-09-26T21:53:57Z",
      "pushed_at": "2025-09-08T21:30:26Z"
    },
    "stats": {
      "stars": 3334,
      "forks": 124,
      "watchers": 3334,
      "open_issues": 31,
      "size": 1486
    },
    "tech_info": {
      "language": "Objective-C",
      "languages": {
        "Objective-C": 49586
      },
      "license": "Apache License 2.0",
      "topics": []
    },
    "content": {
      "readme": "# Lid Angle Sensor\n\nHi, I’m Sam Gold. Did you know that you have ~rights~ a lid angle sensor in your MacBook? [The ~Constitution~ human interface device utility says you do.](https://youtu.be/wqnHtGgVAUE?t=21)\n\nThis is a little utility that shows the angle from the sensor and, optionally, plays a wooden door creaking sound if you adjust it reeaaaaaal slowly.\n\n## FAQ\n\n**What is a lid angle sensor?**\n\nDespite what the name would have you believe, it is a sensor that detects the angle of the lid.\n\n**Which devices have a lid angle sensor?**\n\nIt was introduced with the 2019 16-inch MacBook Pro. If your laptop is newer, you probably have it. [People have reported](https://github.com/samhenrigold/LidAngleSensor/issues/13) that it **does not work on M1 devices**, I have not yet figured out a fix.\n\n**My laptop should have it, why doesn't it show up?**\n\nI've only tested this on my M4 MacBook Pro and have hard-coded it to look for a specific sensor. If that doesn't work, try running [this script](https://gist.github.com/samhenrigold/42b5a92d1ee8aaf2b840be34bff28591) and report the output in [an issue](https://github.com/samhenrigold/LidAngleSensor/issues/new/choose).\n\nKnown problematic models:\n\n- M1 MacBook Air\n- M1 MacBook Pro\n\n**Can I use this on my iMac?**\n\n~~Not yet tested. Feel free to slam your computer into your desk and make a PR with your results.~~\n\n[It totally works](https://github.com/samhenrigold/LidAngleSensor/issues/33). If it doesn't work for you, try slamming your computer harder?\n\n**Why?**\n\nA lot of free time. I'm open to full-time work in NYC or remote. I'm a designer/design-engineer. https://samhenri.gold\n\n**No I mean like why does my laptop need to know the exact angle of its lid?**\n\nOh. I don't know.\n\n**Can I contribute?**\n\nI guess.\n\n**Why does it say it's by Lisa?**\n\nI signed up for my developer account when I was a kid, used my mom's name, and now it's stuck that way forever and I can't change it. That's life.\n\n**How come the audio feels kind of...weird?**\n\nI'm bad at audio.\n\n**Where did the sound effect come from?**\n\nLEGO Batman 3: Beyond Gotham. But you knew that already.\n\n**Can I turn off the sound?**\n\nYes, never click \"Start Audio\". But this energy isn't encouraged.\n\n## Building\n\nAccording to [this issue](https://github.com/samhenrigold/LidAngleSensor/issues/12), building requires having Xcode installed. I've only tested this on Xcode 26. YMMV.\n\n## Installation\n\nVia Homebrew:\n\n```shell\nbrew install lidanglesensor\n```\n\n## Related projects\n\n- [Python library that taps into this sensor](https://github.com/tcsenpai/pybooklid)\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-27T02:14:00.538423"
  },
  {
    "basic_info": {
      "name": "RustGPT",
      "full_name": "tekaratzas/RustGPT",
      "owner": "tekaratzas",
      "description": "An transformer based LLM. Written completely in Rust",
      "url": "https://github.com/tekaratzas/RustGPT",
      "clone_url": "https://github.com/tekaratzas/RustGPT.git",
      "ssh_url": "git@github.com:tekaratzas/RustGPT.git",
      "homepage": null,
      "created_at": "2025-09-13T22:05:55Z",
      "updated_at": "2025-09-27T01:59:14Z",
      "pushed_at": "2025-09-25T13:50:46Z"
    },
    "stats": {
      "stars": 2690,
      "forks": 215,
      "watchers": 2690,
      "open_issues": 4,
      "size": 191
    },
    "tech_info": {
      "language": "Rust",
      "languages": {
        "Rust": 62444
      },
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "# 🦀 Rust LLM from Scratch\n\n[![Rust](https://github.com/tekaratzas/RustGPT/actions/workflows/rust.yml/badge.svg)](https://github.com/tekaratzas/RustGPT/actions/workflows/rust.yml)\n\nhttps://github.com/user-attachments/assets/ec4a4100-b03a-4b3c-a7d6-806ea54ed4ed\n\nA complete **Large Language Model implementation in pure Rust** with no external ML frameworks. Built from the ground up using only `ndarray` for matrix operations.\n\n## 🚀 What This Is\n\nThis project demonstrates how to build a transformer-based language model from scratch in Rust, including:\n- **Pre-training** on factual text completion\n- **Instruction tuning** for conversational AI\n- **Interactive chat mode** for testing\n- **Full backpropagation** with gradient clipping\n- **Modular architecture** with clean separation of concerns\n\n## ❌ What This Isn't\n\nThis is not a production grade LLM. It is so far away from the larger models.\n\nThis is just a toy project that demonstrates how these models work under the hood.\n\n## 🔍 Key Files to Explore\n\nStart with these two core files to understand the implementation:\n\n- **[`src/main.rs`](src/main.rs)** - Training pipeline, data preparation, and interactive mode\n- **[`src/llm.rs`](src/llm.rs)** - Core LLM implementation with forward/backward passes and training logic\n\n## 🏗️ Architecture\n\nThe model uses a **transformer-based architecture** with the following components:\n\n```\nInput Text → Tokenization → Embeddings → Transformer Blocks → Output Projection → Predictions\n```\n\n### Project Structure\n\n```\nsrc/\n├── main.rs              # 🎯 Training pipeline and interactive mode\n├── llm.rs               # 🧠 Core LLM implementation and training logic\n├── lib.rs               # 📚 Library exports and constants\n├── transformer.rs       # 🔄 Transformer block (attention + feed-forward)\n├── self_attention.rs    # 👀 Multi-head self-attention mechanism\n├── feed_forward.rs      # ⚡ Position-wise feed-forward networks\n├── embeddings.rs        # 📊 Token embedding layer\n├── output_projection.rs # 🎰 Final linear layer for vocabulary predictions\n├── vocab.rs            # 📝 Vocabulary management and tokenization\n├── layer_norm.rs       # 🧮 Layer normalization\n└── adam.rs             # 🏃 Adam optimizer implementation\n\ntests/\n├── llm_test.rs         # Tests for core LLM functionality\n├── transformer_test.rs # Tests for transformer blocks\n├── self_attention_test.rs # Tests for attention mechanisms\n├── feed_forward_test.rs # Tests for feed-forward layers\n├── embeddings_test.rs  # Tests for embedding layers\n├── vocab_test.rs       # Tests for vocabulary handling\n├── adam_test.rs        # Tests for optimizer\n└── output_projection_test.rs # Tests for output layer\n```\n\n## 🧪 What The Model Learns\n\nThe implementation includes two training phases:\n\n1. **Pre-training**: Learns basic world knowledge from factual statements\n   - \"The sun rises in the east and sets in the west\"\n   - \"Water flows downhill due to gravity\"\n   - \"Mountains are tall and rocky formations\"\n\n2. **Instruction Tuning**: Learns conversational patterns\n   - \"User: How do mountains form? Assistant: Mountains are formed through tectonic forces...\"\n   - Handles greetings, explanations, and follow-up questions\n\n## 🚀 Quick Start\n\n```bash\n# Clone and run\ngit clone https://github.com/tekaratzas/RustGPT.git\ncd RustGPT\ncargo run\n\n# The model will:\n# 1. Build vocabulary from training data\n# 2. Pre-train on factual statements (100 epochs)\n# 3. Instruction-tune on conversational data (100 epochs)\n# 4. Enter interactive mode for testing\n```\n\n## 🎮 Interactive Mode\n\nAfter training, test the model interactively:\n\n```\nEnter prompt: How do mountains form?\nModel output: Mountains are formed through tectonic forces or volcanism over long geological time periods\n\nEnter prompt: What causes rain?\nModel output: Rain is caused by water vapor in clouds condensing into droplets that become too heavy to remain airborne\n```\n\n## 🧮 Technical Implementation\n\n### Model Configuration\n- **Vocabulary Size**: Dynamic (built from training data)\n- **Embedding Dimension**: 128 (defined by `EMBEDDING_DIM` in `src/lib.rs`)\n- **Hidden Dimension**: 256 (defined by `HIDDEN_DIM` in `src/lib.rs`)\n- **Max Sequence Length**: 80 tokens (defined by `MAX_SEQ_LEN` in `src/lib.rs`)\n- **Architecture**: 3 Transformer blocks + embeddings + output projection\n\n### Training Details\n- **Optimizer**: Adam with gradient clipping\n- **Pre-training LR**: 0.0005 (100 epochs)\n- **Instruction Tuning LR**: 0.0001 (100 epochs)\n- **Loss Function**: Cross-entropy loss\n- **Gradient Clipping**: L2 norm capped at 5.0\n\n### Key Features\n- **Custom tokenization** with punctuation handling\n- **Greedy decoding** for text generation\n- **Gradient clipping** for training stability\n- **Modular layer system** with clean interfaces\n- **Comprehensive test coverage** for all components\n\n## 🔧 Development\n\n```bash\n# Run all tests\ncargo test\n\n# Test specific components\ncargo test --test llm_test\ncargo test --test transformer_test\ncargo test --test self_attention_test\n\n# Buil",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-27T02:14:01.720863"
  },
  {
    "basic_info": {
      "name": "shimmy",
      "full_name": "Michael-A-Kuykendall/shimmy",
      "owner": "Michael-A-Kuykendall",
      "description": "⚡ Python-free Rust inference server — OpenAI-API compatible. GGUF + SafeTensors, hot model swap, auto-discovery, single binary. FREE now, FREE forever.",
      "url": "https://github.com/Michael-A-Kuykendall/shimmy",
      "clone_url": "https://github.com/Michael-A-Kuykendall/shimmy.git",
      "ssh_url": "git@github.com:Michael-A-Kuykendall/shimmy.git",
      "homepage": "",
      "created_at": "2025-08-28T22:55:46Z",
      "updated_at": "2025-09-27T01:15:35Z",
      "pushed_at": "2025-09-24T02:59:52Z"
    },
    "stats": {
      "stars": 2615,
      "forks": 172,
      "watchers": 2615,
      "open_issues": 2,
      "size": 213430
    },
    "tech_info": {
      "language": "Rust",
      "languages": {
        "Rust": 617150,
        "C": 158306,
        "C++": 77564,
        "Shell": 46795,
        "TOML": 18131,
        "Python": 14810,
        "TypeScript": 10203,
        "JavaScript": 7526,
        "YAML": 6387,
        "Dockerfile": 4809,
        "Batchfile": 4747,
        "Ruby": 931
      },
      "license": "MIT License",
      "topics": [
        "api-server",
        "command-line-tool",
        "developer-tools",
        "gguf",
        "huggingface",
        "huggingface-models",
        "huggingface-transformers",
        "inference-server",
        "llama",
        "llamacpp",
        "llm-inference",
        "local-ai",
        "lora",
        "machine-learning",
        "ollama-api",
        "openai-compatible",
        "rust",
        "rust-crate",
        "transformers"
      ]
    },
    "content": {
      "readme": "<div align=\"center\">\r\n  <img src=\"assets/shimmy-logo.png\" alt=\"Shimmy Logo\" width=\"300\" height=\"auto\" />\r\n  \r\n  # The Privacy-First Alternative to Ollama\r\n  \r\n  ### 🔒 Local AI Without the Lock-in 🚀\r\n\r\n  [![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\r\n  [![Security](https://img.shields.io/badge/Security-Audited-green)](https://github.com/Michael-A-Kuykendall/shimmy/security)\r\n  [![Crates.io](https://img.shields.io/crates/v/shimmy.svg)](https://crates.io/crates/shimmy)\r\n  [![Downloads](https://img.shields.io/crates/d/shimmy.svg)](https://crates.io/crates/shimmy)\r\n  [![Rust](https://img.shields.io/badge/rust-stable-brightgreen.svg)](https://rustup.rs/)\r\n  [![GitHub Stars](https://img.shields.io/github/stars/Michael-A-Kuykendall/shimmy?style=social)](https://github.com/Michael-A-Kuykendall/shimmy/stargazers)\r\n  \r\n  [![💝 Sponsor this project](https://img.shields.io/badge/💝_Sponsor_this_project-ea4aaa?style=for-the-badge&logo=github&logoColor=white)](https://github.com/sponsors/Michael-A-Kuykendall)\r\n</div>\r\n\r\n**Shimmy will be free forever.** No asterisks. No \"free for now.\" No pivot to paid.\r\n\r\n### 💝 Support Shimmy's Growth\r\n\r\n🚀 **If Shimmy helps you, consider [sponsoring](https://github.com/sponsors/Michael-A-Kuykendall) — 100% of support goes to keeping it free forever.**\r\n\r\n- **$5/month**: Coffee tier ☕ - Eternal gratitude + sponsor badge  \r\n- **$25/month**: Bug prioritizer 🐛 - Priority support + name in [SPONSORS.md](SPONSORS.md)\r\n- **$100/month**: Corporate backer 🏢 - Logo placement + monthly office hours  \r\n- **$500/month**: Infrastructure partner 🚀 - Direct support + roadmap input\r\n\r\n[**🎯 Become a Sponsor**](https://github.com/sponsors/Michael-A-Kuykendall) | See our amazing [sponsors](SPONSORS.md) 🙏\r\n\r\n---\r\n\r\n## Drop-in OpenAI API Replacement for Local LLMs\r\n\r\nShimmy is a **5.1MB single-binary** that provides **100% OpenAI-compatible endpoints** for GGUF models. Point your existing AI tools to Shimmy and they just work — locally, privately, and free.\r\n\r\n## 🤔 What are you building with Shimmy?\r\n\r\n**New developer tools and specifications included!** Whether you're forking Shimmy for your application or integrating it as a service, we now provide:\r\n\r\n- **🔧 Integration Templates**: Copy-paste guidance for embedding Shimmy in your projects\r\n- **📋 Development Specifications**: GitHub Spec-Kit methodology for planning Shimmy-based features\r\n- **🛡️ Architectural Guarantees**: Constitutional principles ensuring Shimmy stays reliable and lightweight\r\n- **📖 Complete Documentation**: Everything you need to build on Shimmy's foundation\r\n\r\n**Building something cool with Shimmy?** These tools help you do it systematically and reliably.\r\n\r\n### 🚀 **GitHub Spec-Kit Integration**\r\nShimmy now includes [GitHub's brand-new Spec-Kit methodology](https://github.com/github/spec-kit) – specification-driven development that just launched in September 2025! Get professional-grade development workflows:\r\n\r\n- **🏗️ Systematic Development**: `/specify` → `/plan` → `/tasks` → implement\r\n- **🤖 AI-Native Workflow**: Works with Claude Code, GitHub Copilot, and other AI assistants  \r\n- **📋 Professional Templates**: Complete specification and planning frameworks\r\n- **🛡️ Constitutional Protection**: Built-in governance and architectural validation\r\n\r\n[**📖 Complete Developer Guide →**](DEVELOPERS.md) • [**🛠️ Learn GitHub Spec-Kit →**](https://github.com/github/spec-kit)\r\n\r\n### Try it in 30 seconds\r\n\r\n```bash\r\n# 1) Install + run\r\ncargo install shimmy --features huggingface\r\nshimmy serve &\r\n\r\n# 2) See models and pick one\r\nshimmy list\r\n\r\n# 3) Smoke test the OpenAI API\r\ncurl -s http://127.0.0.1:11435/v1/chat/completions \\\r\n  -H 'Content-Type: application/json' \\\r\n  -d '{\r\n        \"model\":\"REPLACE_WITH_MODEL_FROM_list\",\r\n        \"messages\":[{\"role\":\"user\",\"content\":\"Say hi in 5 words.\"}],\r\n        \"max_tokens\":32\r\n      }' | jq -r '.choices[0].message.content'\r\n```\r\n\r\n## 🚀 Works with Your Existing Tools\r\n\r\n**No code changes needed** - just change the API endpoint:\r\n\r\n- **VSCode Extensions**: Point to `http://localhost:11435`\r\n- **Cursor Editor**: Built-in OpenAI compatibility  \r\n- **Continue.dev**: Drop-in model provider\r\n- **Any OpenAI client**: Python, Node.js, curl, etc.\r\n\r\n### Use with OpenAI SDKs\r\n\r\n- Node.js (openai v4)\r\n\r\n```ts\r\nimport OpenAI from \"openai\";\r\n\r\nconst openai = new OpenAI({\r\n  baseURL: \"http://127.0.0.1:11435/v1\",\r\n  apiKey: \"sk-local\", // placeholder, Shimmy ignores it\r\n});\r\n\r\nconst resp = await openai.chat.completions.create({\r\n  model: \"REPLACE_WITH_MODEL\",\r\n  messages: [{ role: \"user\", content: \"Say hi in 5 words.\" }],\r\n  max_tokens: 32,\r\n});\r\n\r\nconsole.log(resp.choices[0].message?.content);\r\n```\r\n\r\n- Python (openai>=1.0.0)\r\n\r\n```python\r\nfrom openai import OpenAI\r\n\r\nclient = OpenAI(base_url=\"http://127.0.0.1:11435/v1\", api_key=\"sk-local\")\r\n\r\nresp = client.chat.completions.create(\r\n    model=\"REPLACE_WITH_MODEL\",\r\n    messages=[{\"role\": \"user\", \"content\": \"Say hi i",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-27T02:14:02.875510"
  },
  {
    "basic_info": {
      "name": "ZHO-nano-banana-Creation",
      "full_name": "ZHO-ZHO-ZHO/ZHO-nano-banana-Creation",
      "owner": "ZHO-ZHO-ZHO",
      "description": "我的 nano-banana 创意玩法大合集！  持续更新中！",
      "url": "https://github.com/ZHO-ZHO-ZHO/ZHO-nano-banana-Creation",
      "clone_url": "https://github.com/ZHO-ZHO-ZHO/ZHO-nano-banana-Creation.git",
      "ssh_url": "git@github.com:ZHO-ZHO-ZHO/ZHO-nano-banana-Creation.git",
      "homepage": null,
      "created_at": "2025-08-28T13:09:00Z",
      "updated_at": "2025-09-27T01:51:40Z",
      "pushed_at": "2025-09-18T22:04:39Z"
    },
    "stats": {
      "stars": 2504,
      "forks": 246,
      "watchers": 2504,
      "open_issues": 3,
      "size": 85
    },
    "tech_info": {
      "language": null,
      "languages": {},
      "license": "GNU General Public License v3.0",
      "topics": []
    },
    "content": {
      "readme": "<img width=\"1152\" height=\"1536\" alt=\"Mask group封面\" src=\"https://github.com/user-attachments/assets/48d727e7-7998-4d2a-aa37-59beb41ac23e\" />\n\n\n<div align=\"center\">\n   \n# Nano-Banana Creation ZHO\n   \n我的 Nano-Banana 原创玩法大合集 | [完整帖](https://x.com/ZHO_ZHO_ZHO/status/1961073677681983926) | My Nano-Banana Creation collection\n\n！！！注意标明出处哦！！！\n\n\n\n<img width=\"1032\" height=\"1373\" alt=\"Group 320\" src=\"https://github.com/user-attachments/assets/8f6ec719-1097-43f5-b56f-ef68123e2203\" />\n\n\n\n<div align=\"left\">\n\n\n# （1）目录（46项）\n\n- [1️⃣ 出圈/火的用法：图片变手办/手办视频](#1️⃣-出圈火的用法图片变手办手办视频)\n- [2️⃣ 名人/指定人物（上传图片）超写实照片级生成](#2️⃣-名人指定人物上传图片超写实照片级生成)\n- [3️⃣ 指定人物短视频：人像特征保持 + 切换视角 + veo3 首尾帧](#3️⃣-指定人物短视频人像特征保持--切换视角--veo3-首尾帧)\n- [4️⃣ 建筑图转模型/建模](#4️⃣-建筑图转模型建模)\n- [5️⃣ 连续编辑 + 物体组合 + 背景设计](#5️⃣-连续编辑--物体组合--背景设计)\n- [6️⃣ 高清修复](#6️⃣-高清修复)\n- [7️⃣ 物体组合/版本对比](#7️⃣-物体组合版本对比)\n- [8️⃣ 商品广告短片：指定人物 + 商品](#8️⃣-商品广告短片指定人物--商品)\n- [9️⃣ 人群中分离指定模糊人物 + 高清生成](#9️⃣-人群中分离指定模糊人物--高清生成)\n- [1️⃣0️⃣ 图转线稿 + 色卡上色](#1️⃣0️⃣-图转线稿--色卡上色)\n- [1️⃣1️⃣ 一句话生成一套角色设定/故事书](#1️⃣1️⃣-一句话生成一套角色设定故事书)\n- [1️⃣2️⃣ 虚实结合/跨次元：插画人物探店](#1️⃣2️⃣-虚实结合跨次元插画人物探店)\n- [1️⃣3️⃣ 指定人物 + 设计 实景体验/效果图](#1️⃣3️⃣-指定人物--设计-实景体验效果图)\n- [1️⃣4️⃣ 精准替换视频人物](#1️⃣4️⃣-精准替换视频人物)\n- [1️⃣5️⃣ 动漫转真人（接近 1:1 还原）](#1️⃣5️⃣-动漫转真人接近-11-还原)\n- [1️⃣6️⃣ 高质量摄影：指定人物 + 高质量姿势参考](#1️⃣6️⃣-高质量摄影指定人物--高质量姿势参考)\n- [1️⃣7️⃣ 图片转人偶玩具](#1️⃣7️⃣-图片转人偶玩具)\n- [1️⃣8️⃣ 图片转-funko-pop-公仔](#1️⃣8️⃣-图片转-funko-pop-公仔)\n- [1️⃣9️⃣ 图片转乐高](#1️⃣9️⃣-图片转乐高)\n- [2️⃣0️⃣ 图片转针织玩偶](#2️⃣0️⃣-图片转针织玩偶)\n- [2️⃣1️⃣ 图片转芭比娃娃](#2️⃣1️⃣-图片转芭比娃娃)\n- [2️⃣2️⃣ 万物变高达](#2️⃣2️⃣-万物变高达)\n- [2️⃣3️⃣ 赛博生娃？！两张人脸生成孩子脸](#2️⃣3️⃣-赛博生娃两张人脸生成孩子脸)\n- [2️⃣4️⃣ 产品设计图转真实效果/渲染](#2️⃣4️⃣-产品设计图转真实效果渲染)\n- [2️⃣5️⃣ 随手拍秒变专业摄影大片？！nano-banana-拯救你的废片](#2️⃣5️⃣-随手拍秒变专业摄影大片nano-banana-拯救你的废片)\n- [2️⃣6️⃣ 光影参考](#2️⃣6️⃣-光影参考)\n- [2️⃣7️⃣ 使用光影人偶做打光参考](#2️⃣7️⃣-使用光影人偶做打光参考)\n- [2️⃣8️⃣ 生成绘画/渲染过程四宫格](#2️⃣8️⃣-生成绘画渲染过程四宫格)\n- [2️⃣9️⃣ 一句话-照片变插画-还附带绘画过程](#2️⃣9️⃣-一句话-照片变插画-还附带绘画过程)\n- [3️⃣0️⃣ 脸型参考/控制，秒变卡通形象](#3️⃣0️⃣-脸型参考控制秒变卡通形象)\n- [3️⃣1️⃣ 一句咒语任何风格变写实](#3️⃣1️⃣-一句咒语任何风格变写实)\n- [3️⃣2️⃣ 曲面屏贴图](#3️⃣2️⃣-曲面屏贴图)\n- [3️⃣3️⃣ 直接为图片中的曲面大屏生成-指定的-裸眼-3d-内容](#3️⃣3️⃣-直接为图片中的曲面大屏生成-指定的-裸眼-3d-内容)\n- [3️⃣4️⃣ 任意图片（明星/动漫）变挂件挂在自己/女朋友包包上](#3️⃣4️⃣-任意图片明星动漫变挂件挂在自己女朋友包包上)\n- [3️⃣5️⃣ 叠加指定-材质质感/效果](#3️⃣5️⃣-叠加指定-材质质感效果)\n- [3️⃣6️⃣ 照片变娃娃](#3️⃣6️⃣-照片变娃娃)\n- [3️⃣7️⃣ 产品包装贴合](#3️⃣7️⃣-产品包装贴合)\n- [3️⃣8️⃣ 把指定图片贴在大阶梯上](#3️⃣8️⃣-把指定图片贴在大阶梯上)\n- [3️⃣9️⃣ 虚拟试妆化指定妆面](#3️⃣9️⃣-虚拟试妆化指定妆面)\n- [4️⃣0️⃣ 妆面分析-+-优化建议](#4️⃣0️⃣-妆面分析--优化建议)\n- [4️⃣1️⃣ 工业设计-手绘-秒变-实景效果](#4️⃣1️⃣-工业设计-手绘-秒变-实景效果)\n- [4️⃣2️⃣ 工业设计套图马克笔、水彩、分析图、渲染图](#4️⃣2️⃣-工业设计套图马克笔水彩分析图渲染图)\n- [4️⃣3️⃣ 表情准确参考动漫、真人都没问题](#4️⃣3️⃣-表情准确参考动漫真人都没问题)\n- [4️⃣4️⃣ 动物拟人表情](#4️⃣4️⃣-动物拟人表情)\n- [4️⃣5️⃣ 绝美卡片设计](#4️⃣5️⃣-绝美卡片设计)\n- [4️⃣6️⃣ 多人物插画集](#4️⃣6️⃣-多人物插画集)\n\n\n# （2）在线体验/本地部署（开源）：🍌 三件套\n\n\n- 窗口式：[1️⃣ Nano Bananary｜香蕉超市](#1️⃣-nano-bananary香蕉超市)\n\n- 白板/画布式：[2️⃣ BananaPod｜香蕉铺子](#2️⃣-bananapod香蕉铺子)\n\n- 节点/工作流式：[3️⃣ BananaFlow｜香蕉工厂](#3️⃣-bananaflow香蕉工厂)\n\n\n\n## 1️⃣ Nano Bananary｜香蕉超市\n\n<img width=\"1251\" height=\"2051\" alt=\"Group 336\" src=\"https://github.com/user-attachments/assets/51a70ae5-9f94-4dc3-88ca-8ef688d1ee5c\" />\n\n\n所有玩法已陆续上线我自己的开源 APP：Nano Bananary｜香蕉超市，无需提示词，丝滑衔接，已上线 视频生成 功能\n\n\n一键直达：[Nano Bananary｜香蕉超市](https://github.com/ZHO-ZHO-ZHO/Nano-Bananary)\n\n\n\n## 2️⃣ BananaPod｜香蕉铺子\n\n\n<img width=\"2261\" height=\"1410\" alt=\"screenshot-20250909-020322\" src=\"https://github.com/user-attachments/assets/9997b969-c773-4f7b-9c0b-dfeed5cf9e71\" />\n\n各种玩法已内置，支持手绘生图，支持多图框选，轻松构建创意画板\n\n一键直达：[BananaPod｜香蕉铺子](https://github.com/ZHO-ZHO-ZHO/BananaPod)\n\n\n\n## 3️⃣ BananaFlow｜香蕉工厂\n\n<img width=\"2667\" height=\"3518\" alt=\"Group 357\" src=\"https://github.com/user-attachments/assets/a934c87c-90a6-4628-8866-428c07eb434d\" />\n\nNanoBanana + Veo 的开源工作流创意平台，工作流 + 窗口 双模式纵享丝滑，各种玩法已内置，轻松构建创意工作流\n\n一键直达：[BananaFlow｜香蕉工厂](https://github.com/ZHO-ZHO-ZHO/BananaFlow-ZHO)\n\n\n\n# （3）原创玩法 + 提示词\n\n## 1️⃣ 出圈/火的用法：图片变手办/手办视频\n\nhttps://github.com/user-attachments/assets/6440b1eb-4ecb-4d77-8dba-63c6d61c1ad3\n\n1）变手办 Prompt：\n\n```\nturn this photo into a character figure. Behind it, place a box with the character’s image printed on it, and a computer showing the Blender modeling process on its screen. In front of the box, add a round plastic base with the character figure standing on it. set the scene indoors if possible\n```\n\n2）变视频 Prompt（Veo3）：\n\n```\nA pair of hands picks up the figurine and examines it closely\n```\n\n<details>\n<summary>我的原帖</summary>\n\n  \n[图片变手办](https://x.com/ZHO_ZHO_ZHO/status/1958539464994959715)\n\n[图片变手办优化版](https://x.com/ZHO_ZHO_ZHO/status/1959524430520135749)\n\n[手办变视频](https://x.com/ZHO_ZHO_ZHO/status/1958550998815023573)\n\n\n</details>\n\n\n\n## 2️⃣ 名人/指定人物（上传图片）超写实照片级生成\n\n\n<img width=\"894\" height=\"900\" alt=\"image\" src=\"https://github.com/user-attachments/assets/0a0fd607-66c4-4993-bd10-fe3f1b734ef0\" />\n\n\nPrompt：\n\n```\n画面采用中景近乎半身的构图，镜头与人物几乎平视，但透视感强烈，但因为主体微微前倾，视觉上产生一种略带俯视感的压缩效果，让观者与模特之间的距离显得亲密而直接。人物微微抬头冲向镜头，有种拽姐的感觉。闪光灯从正面偏左上方打来，制造出硬朗的高光与深重阴影——墨镜镜片上有明显高光反射，人物后方墙面出现淡淡的投影，整体呈现典型的“直闪”质感：颗粒感轻微，可见胶片风格或高感光度数码拍摄的粗粝纹理。人物面部稍稍有点过度曝光\n\n色彩基调以低饱和的中性色为主：黑色的宽肩丝绒西装外套占据画面最大面积，面料似乎是丝绒，带有细腻的绒面纹路；内搭的黑色丝绸吊带（搬运不标出处死妈）。模特下装是黑色超短裙和薄透质感的连裤袜。背景左侧堆放的墨绿色与咖色棒球帽、右侧叠放的亮蓝色夹克及黑色头盔等杂物，在柔和阴",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-27T02:14:04.027486"
  },
  {
    "basic_info": {
      "name": "Qwen3-Omni",
      "full_name": "QwenLM/Qwen3-Omni",
      "owner": "QwenLM",
      "description": "Qwen3-omni is a natively end-to-end, omni-modal LLM developed by the Qwen team at Alibaba Cloud, capable of understanding text, audio, images, and video, as well as generating speech in real time.",
      "url": "https://github.com/QwenLM/Qwen3-Omni",
      "clone_url": "https://github.com/QwenLM/Qwen3-Omni.git",
      "ssh_url": "git@github.com:QwenLM/Qwen3-Omni.git",
      "homepage": null,
      "created_at": "2025-09-21T09:46:10Z",
      "updated_at": "2025-09-27T02:13:02Z",
      "pushed_at": "2025-09-27T02:03:00Z"
    },
    "stats": {
      "stars": 2155,
      "forks": 87,
      "watchers": 2155,
      "open_issues": 1,
      "size": 26895
    },
    "tech_info": {
      "language": "Jupyter Notebook",
      "languages": {
        "Jupyter Notebook": 38834506,
        "Python": 29676
      },
      "license": "Apache License 2.0",
      "topics": []
    },
    "content": {
      "readme": "# Qwen3-Omni\n\n<br>\n\n<p align=\"center\">\n    <img src=\"https://qianwen-res.oss-cn-beijing.aliyuncs.com//Qwen3-Omni/qwen3_omni_logo.png\" width=\"400\"/>\n<p>\n\n<p align=\"center\">\n        💜 <a href=\"https://chat.qwen.ai/\"><b>Qwen Chat</b></a>&nbsp&nbsp | &nbsp&nbsp🤗 <a href=\"https://huggingface.co/collections/Qwen/qwen3-omni-68d100a86cd0906843ceccbe\">Hugging Face</a>&nbsp&nbsp | &nbsp&nbsp🤖 <a href=\"https://modelscope.cn/collections/Qwen3-Omni-867aef131e7d4f\">ModelScope</a>&nbsp&nbsp | &nbsp&nbsp📑 <a href=\"https://qwen.ai/blog?id=65f766fc2dcba7905c1cb69cc4cab90e94126bf4&from=research.latest-advancements-list\">Blog</a>&nbsp&nbsp | &nbsp&nbsp📚 <a href=\"https://github.com/QwenLM/Qwen3-Omni/tree/main/cookbooks\">Cookbooks</a>&nbsp&nbsp | &nbsp&nbsp📑 <a href=\"https://arxiv.org/pdf/2509.17765\">Paper</a>&nbsp&nbsp\n<br>\n🖥️ <a href=\"https://huggingface.co/spaces/Qwen/Qwen3-Omni-Demo\">Hugging Face Demo</a>&nbsp&nbsp | &nbsp&nbsp 🖥️ <a href=\"https://modelscope.cn/studios/Qwen/Qwen3-Omni-Demo\">ModelScope Demo</a>&nbsp&nbsp | &nbsp&nbsp💬 <a href=\"https://github.com/QwenLM/Qwen/blob/main/assets/wechat.png\">WeChat (微信)</a>&nbsp&nbsp | &nbsp&nbsp🫨 <a href=\"https://discord.gg/CV4E9rpNSD\">Discord</a>&nbsp&nbsp | &nbsp&nbsp📑 <a href=\"https://help.aliyun.com/zh/model-studio/user-guide/qwen-omni\">API</a>\n\n</p>\n\nWe release **Qwen3-Omni**, the natively end-to-end multilingual omni-modal foundation models. It is designed to process diverse inputs including text, images, audio, and video, while delivering real-time streaming responses in both text and natural speech. Click the video below for more information 😃\n\n<details open>\n<summary>English Version</summary>\n<a href=\"https://youtu.be/_zdOrPju4_g\" target=\"_blank\">\n  <img src=\"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/videocover.png\" alt=\"Open English Video\"/>\n</a>\n</details>\n\n<details>\n<summary>Chinese Version</summary>\n<a href=\"https://youtu.be/Wtjsw5deXfQ\" target=\"_blank\">\n  <img src=\"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/videocover.png\" alt=\"打开中文视频\"/>\n</a>\n</details>\n\n\n## News\n* 2025.09.26: ⭐️⭐️⭐️ Qwen3-Omni reaches top-1 on Hugging Face Trending! \n* 2025.09.22: 🎉🎉🎉 We have released [Qwen3-Omni](https://huggingface.co/collections/Qwen/qwen3-omni-68d100a86cd0906843ceccbe). For more details, please check our [blog](https://qwen.ai/blog?id=65f766fc2dcba7905c1cb69cc4cab90e94126bf4&from=research.latest-advancements-list)!\n\n## Contents <!-- omit in toc -->\n\n- [Overview](#overview)\n  - [Introduction](#introduction)\n  - [Model Architecture](#model-architecture)\n  - [Cookbooks for Usage Cases](#cookbooks-for-usage-cases)\n- [QuickStart](#quickstart)\n  - [Model Description and Download](#model-description-and-download)\n  - [Transformers Usage](#transformers-usage)\n  - [vLLM Usage](#vllm-usage)\n  - [DashScope API Usage](#dashscope-api-usage)\n  - [Usage Tips (Recommended Reading)](#usage-tips-recommended-reading)\n- [Interaction with Qwen3-Omni](#interaction-with-qwen3-omni)\n  - [Online Demo](#online-demo)\n  - [Real-Time Interaction](#real-time-interaction)\n  - [Launch Local Web UI Demo](#launch-local-web-ui-demo)\n- [Docker](#-docker)\n- [Evaluation](#evaluation)\n  - [Performance of Qwen3-Omni](#performance-of-qwen3-omni)\n  - [Setting for Evaluation](#setting-for-evaluation)\n- [Citation](#citation)\n\n## Overview\n### Introduction\n\n<p align=\"center\">\n    <img src=\"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/q3o_introduction.png\" width=\"90%\"/>\n<p>\n\nQwen3-Omni is the natively end-to-end multilingual omni-modal foundation models. It processes text, images, audio, and video, and delivers real-time streaming responses in both text and natural speech. We introduce several architectural upgrades to improve performance and efficiency. Key features:\n\n* **State-of-the-art across modalities**: Early text-first pretraining and mixed multimodal training provide native multimodal support. While achieving strong audio and audio-video results, unimodal text and image performance does not regress. Reaches SOTA on 22 of 36 audio/video benchmarks and open-source SOTA on 32 of 36; ASR, audio understanding, and voice conversation performance is comparable to Gemini 2.5 Pro.\n\n* **Multilingual**: Supports 119 text languages, 19 speech input languages, and 10 speech output languages.\n  - **Speech Input**: English, Chinese, Korean, Japanese, German, Russian, Italian, French, Spanish, Portuguese, Malay, Dutch, Indonesian, Turkish, Vietnamese, Cantonese, Arabic, Urdu.\n  - **Speech Output**: English, Chinese, French, German, Russian, Italian, Spanish, Portuguese, Japanese, Korean.\n\n* **Novel Architecture**: MoE-based Thinker–Talker design with AuT pretraining for strong general representations, plus a multi-codebook design that drives latency to a minimum.\n\n* **Real-time Audio/Video Interaction**: Low-latency streaming with natural turn-taking and immediate text or speech responses.\n\n* **Flexible Control**: Customize behavior via system prompts for fine-grained control and easy ",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-27T02:14:05.180115"
  },
  {
    "basic_info": {
      "name": "Dayflow",
      "full_name": "JerryZLiu/Dayflow",
      "owner": "JerryZLiu",
      "description": "Generate a timeline of your day, automatically",
      "url": "https://github.com/JerryZLiu/Dayflow",
      "clone_url": "https://github.com/JerryZLiu/Dayflow.git",
      "ssh_url": "git@github.com:JerryZLiu/Dayflow.git",
      "homepage": null,
      "created_at": "2025-09-23T01:58:21Z",
      "updated_at": "2025-09-27T02:11:46Z",
      "pushed_at": "2025-09-26T00:34:07Z"
    },
    "stats": {
      "stars": 1878,
      "forks": 55,
      "watchers": 1878,
      "open_issues": 20,
      "size": 41104
    },
    "tech_info": {
      "language": "Swift",
      "languages": {
        "Swift": 736897,
        "Shell": 25722
      },
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "\n<div align=\"center\">\n  <img src=\"docs/images/dayflow_header.png\" alt=\"Dayflow\" width=\"400\">\n</div>\n\n<div align=\"center\">\n  <em>A timeline of your day, automatically.</em><br>\n  Turns your screen activity into a clean timeline with AI summaries and distraction highlights.\n</div>\n\n<div align=\"center\">\n  <!-- Badges -->\n  <img src=\"https://img.shields.io/badge/macOS-13%2B-000?logo=apple\" alt=\"Platform: macOS 13+\">\n  <img src=\"https://img.shields.io/badge/SwiftUI-✓-orange\" alt=\"SwiftUI\">\n  <img src=\"https://img.shields.io/badge/Updates-Sparkle-informational\" alt=\"Updates: Sparkle\">\n  <img src=\"https://img.shields.io/badge/AI-Gemini%20or%20Local-blue\" alt=\"AI: Gemini / Local\">\n  <img src=\"https://img.shields.io/badge/License-MIT-green\" alt=\"License: MIT\">\n</div>\n\n<div align=\"center\">\n  <img src=\"docs/images/hero_animation_1080p.gif\" alt=\"Dayflow Hero Animation\" width=\"800\">\n</div>\n\n<div align=\"center\">\n  <a href=\"https://github.com/JerryZLiu/Dayflow/releases/latest\">\n    <img src=\"https://img.shields.io/badge/Download%20for%20Mac-⬇%20%20Dayflow.dmg-blue?style=for-the-badge&logo=apple\" alt=\"Download for Mac\">\n  </a>\n</div>\n\n<p align=\"center\">\n  <a href=\"#quickstart\">Quickstart</a> •\n  <a href=\"#why-i-built-dayflow\">Why I built Dayflow</a> •\n  <a href=\"#features\">Features</a> •\n  <a href=\"#how-it-works\">How it works</a> •\n  <a href=\"#installation\">Installation</a> •\n  <a href=\"#data--privacy\">Data & Privacy</a> •\n  <a href=\"#debug--developer-tools\">Debug & Developer Tools</a> •\n  <a href=\"#auto-updates-sparkle\">Auto‑updates</a> •\n  <a href=\"#contributing\">Contributing</a>\n</p>\n\n---\n\n## What is Dayflow?\n\nDayflow is a **native macOS app** (SwiftUI) that records your screen at **1 FPS**, analyzes it **every 15 minutes** with AI, and generates a **timeline** of your activities with summaries. \nIt's lightweight (25MB app size) and uses ~100MB of RAM and <1% cpu. \n\n> _Privacy‑minded by design_: You choose your AI provider. Use **Gemini** (bring your own API key) or **local models** (Ollama / LM Studio). See **Data & Privacy** for details.\n\n\n## Why I built Dayflow\n\nI built Dayflow after realizing that my calendar wasn't the source of truth for how I actually spent my time. My screen was. I wanted a calm, trustworthy timeline that let me see my workday without turning into yet another dashboard I had to maintain.\n\nDayflow stands for ownership and privacy by default. You control the data, you choose the AI provider, and you can keep everything local if that's what makes you comfortable. It's MIT licensed and fully open source because anything that watches your screen all day should be completely transparent about what it does with that information. The app should feel like a quiet assistant: respectful of your attention, honest about what it captures, and easy to shut off.\n\n\n---\n\n## Features\n\n- **Automatic timeline** of your day with concise summaries.\n- **1 FPS recording** - minimal CPU/storage impact.\n- **15-minute analysis intervals** for timely updates.\n- **Watch timelapses of your day**.\n- **Auto storage cleanup** - removes old recordings after 3 days.\n- **Distraction highlights** to see what pulled you off‑task.\n- **Native UX** built with **SwiftUI**.\n- **Auto‑updates** with **Sparkle** (daily check + background download).\n\n### Coming soon\n\n- **Infinitely customizable dashboard** — ask any question about your workday, pipe the answers into tiles you arrange yourself, and track trends over time.\n\n  <div align=\"center\">\n    <img src=\"docs/images/DashboardPreview.png\" alt=\"Dayflow dashboard preview\" width=\"800\">\n  </div>\n\n- **Daily journal** — review the highlights Dayflow captured, reflect with guided prompts, and drop screenshots or notes alongside your generated timeline.\n\n  <div align=\"center\">\n    <img src=\"docs/images/JournalPreview.png\" alt=\"Dayflow journal preview\" width=\"800\">\n  </div>\n\n## How it works\n\n1) **Capture** — Records screen at 1 FPS in 15-second chunks.\n2) **Analyze** — Every 15 minutes, sends recent footage to AI.\n3) **Generate** — AI creates timeline cards with activity summaries.\n4) **Display** — Shows your day as a visual timeline.\n5) **Cleanup** — Auto-deletes recordings older than 3 days.\n\n### AI Processing Pipeline\n\nThe efficiency of your timeline generation depends on your chosen AI provider:\n\n```mermaid\nflowchart LR\n    subgraph Gemini[\"Gemini Flow: 2 LLM Calls\"]\n        direction LR\n        GV[Video] --> GU[Upload + Transcribe<br/>1 LLM call] --> GC[Generate Cards<br/>1 LLM call] --> GD[Done]\n    end\n\n    subgraph Local[\"Local Flow: 33+ LLM Calls\"]\n        direction LR\n        LV[Video] --> LE[Extract 30 frames] --> LD[30 descriptions<br/>30 LLM calls] --> LM[Merge<br/>1 call] --> LT[Title<br/>1 call] --> LC[Merge Check<br/>1 call] --> LMC[Merge Cards<br/>1 call] --> LD2[Done]\n    end\n\n    %% Styling\n    classDef geminiFlow fill:#e8f5e8,stroke:#4caf50,stroke-width:2px\n    classDef localFlow fill:#fff8e1,stroke:#ff9800,stroke-width:2px\n    classDef geminiStep fill:#4caf50,color:#fff\n    cla",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-27T02:14:06.327901"
  },
  {
    "basic_info": {
      "name": "linear-algebra",
      "full_name": "little-book-of/linear-algebra",
      "owner": "little-book-of",
      "description": "A concise, beginner-friendly introduction to the core ideas of linear algebra.",
      "url": "https://github.com/little-book-of/linear-algebra",
      "clone_url": "https://github.com/little-book-of/linear-algebra.git",
      "ssh_url": "git@github.com:little-book-of/linear-algebra.git",
      "homepage": "https://little-book-of.github.io/linear-algebra/",
      "created_at": "2025-09-02T03:28:30Z",
      "updated_at": "2025-09-27T00:30:35Z",
      "pushed_at": "2025-09-26T09:42:31Z"
    },
    "stats": {
      "stars": 1752,
      "forks": 53,
      "watchers": 1752,
      "open_issues": 7,
      "size": 11916
    },
    "tech_info": {
      "language": "Jupyter Notebook",
      "languages": {
        "Jupyter Notebook": 2325068,
        "TeX": 1387617,
        "Makefile": 1007
      },
      "license": "Other",
      "topics": []
    },
    "content": {
      "readme": "# The Little Book of Linear Algebra\n\nA concise, beginner-friendly introduction to the core ideas of Linear Algebra. \n\nA clear, hands-on guide to the geometry and structure behind vectors and matrices.\n\n## Formats\n\n- [Download PDF](releases/book.pdf) - print-ready\n- [Download EPUB](releases/book.epub) - e-reader friendly\n- [View LaTeX](releases/book.tex) - `.tex` source\n- [Read on GitHub Pages](https://little-book-of.github.io/linear-algebra/) - online website\n- [Notebook](https://little-book-of.github.io/linear-algebra/books/en-US/lab.html) - Learning with Python\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](\n  https://colab.research.google.com/github/little-book-of/linear-algebra/blob/main/releases/lab.ipynb\n)\n\nFor old version (v1), here is the link\n\n- [Download PDF (v1)](archived/v1/book.pdf) - print-ready\n- [Download EPUB (v1)](archived/v1/book.epub) - e-reader friendly\n- [View LaTeX (v1)](archived/v1/book.tex) - `.tex` source\n\n## Build it yourself (Quarto)\n\nWe use [Quarto](https://quarto.org/docs/get-started/) to generate all outputs.\n\nPreview locally:\n\n```bash\nquarto preview\n```\n\nRender outputs:\n\n```bash\n# All configured formats\nquarto render\n\n# Individual formats\nquarto render --to html     # site into docs/\nquarto render --to pdf      # docs/book.pdf\nquarto render --to epub     # docs/book.epub\nquarto render --to latex    # docs/book-latex/book.tex\n```\n\n# The Book\n\n## Chapter 1. Vectors, scalars, and geometry \n\n#### Opening\n```\nArrows in the air,\ndirections whisper softly—\nthe plane comes alive.\n```\n\n### 1. Scalars, Vectors, and Coordinate Systems\n\nWhen we begin learning linear algebra, everything starts with the simplest building blocks: scalars and vectors. A scalar is just a single number, like 3, –7, or π. It carries only magnitude and no direction. Scalars are what we use for counting, measuring length, or scaling other objects up and down. A vector, by contrast, is an ordered collection of numbers. You can picture it as an arrow pointing somewhere in space, or simply as a list like (2, 5) in 2D or (1, –3, 4) in 3D. Where scalars measure \"how much,\" vectors measure both \"how much\" and \"which way.\"\n\n#### Coordinate Systems\n\nTo talk about vectors, we need a coordinate system. Imagine laying down two perpendicular axes on a sheet of paper: the x-axis (left to right) and the y-axis (up and down). Every point on the sheet can be described with two numbers: how far along the x-axis, and how far along the y-axis. This pair of numbers is a vector in 2D. Add a z-axis pointing up from the page, and you have 3D space. Each coordinate system gives us a way to describe vectors numerically, even though the underlying \"space\" is the same.\n\n#### Visualizing Scalars vs. Vectors\n\n- A scalar is like a single tick mark on a ruler.\n- A vector is like an arrow that starts at the origin (0, 0, …) and ends at the point defined by its components.\n  For example, the vector (3, 4) in 2D points from the origin to the point 3 units along the x-axis and 4 units along the y-axis.\n\n#### Why Start Here?\n\nUnderstanding the difference between scalars and vectors is the foundation for everything else in linear algebra. Every concept-matrices, linear transformations, eigenvalues-eventually reduces to how we manipulate vectors and scale them with scalars. Without this distinction, the rest of the subject would have no anchor.\n\n#### Why It Matters\n\nNearly every field of science and engineering depends on this idea. Physics uses vectors for velocity, acceleration, and force. Computer graphics uses them to represent points, colors, and transformations. Data science treats entire datasets as high-dimensional vectors. By mastering scalars and vectors early, you unlock the language in which modern science and technology are written.\n\n#### Try It Yourself\n\n1. Draw an x- and y-axis on a piece of paper. Plot the vector (2, 3).\n2. Now draw the vector (–1, 4). Compare their directions and lengths.\n3. Think: which of these two vectors points \"more upward\"? Which is \"longer\"?\n\nThese simple experiments already give you intuition for the operations you'll perform again and again in linear algebra.\n\n### 2. Vector Notation, Components, and Arrows\n\nLinear algebra gives us powerful ways to describe and manipulate vectors, but before we can do anything with them, we need a precise notation system. Notation is not just cosmetic-it tells us how to read, write, and think about vectors clearly and unambiguously. In this section, we'll explore how vectors are written, how their components are represented, and how we can interpret them visually as arrows.\n\n#### Writing Vectors\n\nVectors are usually denoted by lowercase letters in bold (like $\\mathbf{v}, \\mathbf{w}, \\mathbf{x}$)  \nor with an arrow overhead (like $\\vec{v}$).  \n\nFor instance, the vector $\\mathbf{v} = (2, 5)$ is the same as $\\vec{v} = (2, 5)$.  \n\nThe style depends on context: mathematicians often use bold, physicists often use arrows.  \nIn handwritten notes, people",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-27T02:14:07.525803"
  },
  {
    "basic_info": {
      "name": "map-anything",
      "full_name": "facebookresearch/map-anything",
      "owner": "facebookresearch",
      "description": "MapAnything: Universal Feed-Forward Metric 3D Reconstruction",
      "url": "https://github.com/facebookresearch/map-anything",
      "clone_url": "https://github.com/facebookresearch/map-anything.git",
      "ssh_url": "git@github.com:facebookresearch/map-anything.git",
      "homepage": "",
      "created_at": "2025-09-04T14:37:36Z",
      "updated_at": "2025-09-26T22:01:57Z",
      "pushed_at": "2025-09-25T18:12:18Z"
    },
    "stats": {
      "stars": 1748,
      "forks": 80,
      "watchers": 1748,
      "open_issues": 22,
      "size": 6043
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 1727145,
        "Shell": 157713
      },
      "license": "Apache License 2.0",
      "topics": [
        "3d-reconstruction",
        "ai",
        "calibration",
        "depth-completion",
        "depth-estimation",
        "image-to-3d",
        "multi-view-stereo",
        "robotics",
        "sfm"
      ]
    },
    "content": {
      "readme": "<div align=\"center\">\n<h1>MapAnything: Universal Feed-Forward Metric <br>3D Reconstruction</h1>\n<a href=\"https://map-anything.github.io/assets/MapAnything.pdf\"><img src=\"https://img.shields.io/badge/Paper-blue\" alt=\"Paper\"></a>\n<a href=\"https://arxiv.org/abs/2509.13414\"><img src=\"https://img.shields.io/badge/arXiv-2509.13414-b31b1b\" alt=\"arXiv\"></a>\n<a href=\"https://map-anything.github.io/\"><img src=\"https://img.shields.io/badge/Project_Page-green\" alt=\"Project Page\"></a>\n<a href=\"https://huggingface.co/spaces/facebook/map-anything\"><img src='https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Demo-blue'></a>\n<br>\n<br>\n<strong>\n<a href=\"https://nik-v9.github.io/\">Nikhil Keetha<sup>1,2</sup></a>\n&nbsp;&nbsp;\n<a href=\"https://sirwyver.github.io/\">Norman Müller<sup>1</sup></a>\n&nbsp;&nbsp;\n<a href=\"https://demuc.de/\">Johannes Schönberger<sup>1</sup></a>\n&nbsp;&nbsp;\n<a href=\"https://www.linkedin.com/in/lorenzoporzi\">Lorenzo Porzi<sup>1</sup></a>\n&nbsp;&nbsp;\n<a href=\"https://infinity1096.github.io/\">Yuchen Zhang<sup>2</sup></a>\n<br>\n<a href=\"https://tobiasfshr.github.io/\">Tobias Fischer<sup>1</sup></a>\n&nbsp;&nbsp;\n<a href=\"https://www.linkedin.com/in/arno-knapitsch\">Arno Knapitsch<sup>1</sup></a>\n&nbsp;&nbsp;\n<a href=\"https://www.linkedin.com/in/duncan-zauss\">Duncan Zauss<sup>1</sup></a>\n&nbsp;&nbsp;\n<a href=\"https://ethanweber.me/\">Ethan Weber<sup>1</sup></a>\n&nbsp;&nbsp;\n<a href=\"https://www.linkedin.com/in/nelsonantunes7\">Nelson Antunes<sup>1</sup></a>\n<br>\n<a href=\"https://x.com/jonathonluiten?lang=en\">Jonathon Luiten<sup>1</sup></a>\n&nbsp;&nbsp;\n<a href=\"https://m.lopezantequera.com/\">Manuel Lopez-Antequera<sup>1</sup></a>\n&nbsp;&nbsp;\n<a href=\"https://scholar.google.com/citations?user=484sccEAAAAJ\">Samuel Rota Bulò<sup>1</sup></a>\n&nbsp;&nbsp;\n<a href=\"https://richardt.name/\">Christian Richardt<sup>1</sup></a>\n<br>\n<a href=\"https://www.cs.cmu.edu/~deva/\">Deva Ramanan<sup>2</sup></a>\n&nbsp;&nbsp;\n<a href=\"https://theairlab.org/team/sebastian/\">Sebastian Scherer<sup>2</sup></a>\n&nbsp;&nbsp;\n<a href=\"https://www.linkedin.com/in/peter-kontschieder-2a6410134\">Peter Kontschieder<sup>1</sup></a>\n<br>\n<br>\n<sup>1</sup> Meta &nbsp;&nbsp;\n<sup>2</sup> Carnegie Mellon University\n</strong>\n\n</div>\n\n## Overview\n\nMapAnything is a simple, end-to-end trained transformer model that directly regresses the factored metric 3D geometry of a scene given various types of inputs (images, calibration, poses, or depth). A single feed-forward model supports over 12 different 3D reconstruction tasks including multi-image sfm, multi-view stereo, monocular metric depth estimation, registration, depth completion and more.\n\n![Overview](./assets/teaser.png)\n\n## Table of Contents\n\n- [Quick Start](#quick-start)\n  - [Installation](#installation)\n  - [Image-Only Inference](#image-only-inference)\n  - [Multi-Modal Inference](#multi-modal-inference)\n- [Interactive Demos](#interactive-demos)\n  - [Online Demo](#online-demo)\n  - [Local Gradio Demo](#local-gradio-demo)\n  - [Rerun Demo](#rerun-demo)\n- [COLMAP & GSplat Support](#colmap--gsplat-support)\n  - [Exporting to COLMAP Format](#exporting-to-colmap-format)\n  - [Integration with Gaussian Splatting](#integration-with-gaussian-splatting)\n- [Data Processing for Training & Benchmarking](#data-processing-for-training--benchmarking)\n- [Training](#training)\n- [Benchmarking](#benchmarking)\n- [Code License](#code-license)\n- [Models](#models)\n- [Building Blocks for MapAnything](#building-blocks-for-mapanything)\n- [Acknowledgments](#acknowledgments)\n- [Citation](#citation)\n\n## Quick Start\n\n### Installation\n\n```bash\ngit clone https://github.com/facebookresearch/map-anything.git\ncd map-anything\n\n# Create and activate conda environment\nconda create -n mapanything python=3.12 -y\nconda activate mapanything\n\n# Optional: Install torch, torchvision & torchaudio specific to your system\n# Install MapAnything\npip install -e .\n\n# For all optional dependencies\n# See pyproject.toml for more details\npip install -e \".[all]\"\npre-commit install\n```\n\nNote that we don't pin a specific version of PyTorch or CUDA in our requirements. Please feel free to install PyTorch based on your specific system.\n\n### Image-Only Inference\n\nFor metric 3D reconstruction from images without additional geometric inputs:\n\n```python\n# Optional config for better memory efficiency\nimport os\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n\n# Required imports\nimport torch\nfrom mapanything.models import MapAnything\nfrom mapanything.utils.image import load_images\n\n# Get inference device\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# Init model - This requries internet access or the huggingface hub cache to be pre-downloaded\n# For Apache 2.0 license model, use \"facebook/map-anything-apache\"\nmodel = MapAnything.from_pretrained(\"facebook/map-anything\").to(device)\n\n# Load and preprocess images from a folder or list of paths\nimages = \"path/to/your/images/\"  # or [\"path/to/img1.jpg\", \"path/to/img2.jpg\", ...]\nviews = loa",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-27T02:14:08.661329"
  },
  {
    "basic_info": {
      "name": "Super-Mario-Bros.-Remastered-Public",
      "full_name": "JHDev2006/Super-Mario-Bros.-Remastered-Public",
      "owner": "JHDev2006",
      "description": "A Remake / Celebration of the original 'Super Mario Bros.' games. Features new levels, custom modes, new characters, alongside a full level editor / custom level system!",
      "url": "https://github.com/JHDev2006/Super-Mario-Bros.-Remastered-Public",
      "clone_url": "https://github.com/JHDev2006/Super-Mario-Bros.-Remastered-Public.git",
      "ssh_url": "git@github.com:JHDev2006/Super-Mario-Bros.-Remastered-Public.git",
      "homepage": "",
      "created_at": "2025-09-13T15:29:58Z",
      "updated_at": "2025-09-27T00:49:12Z",
      "pushed_at": "2025-09-26T20:38:06Z"
    },
    "stats": {
      "stars": 1446,
      "forks": 176,
      "watchers": 1446,
      "open_issues": 189,
      "size": 54150
    },
    "tech_info": {
      "language": "GDScript",
      "languages": {
        "GDScript": 960105,
        "GAP": 36590,
        "C#": 10724,
        "GDShader": 2322
      },
      "license": "GNU General Public License v3.0",
      "topics": []
    },
    "content": {
      "readme": "# Super Mario Bros Remastered\nA Remake / Celebration of the original 'Super Mario Bros.' games. Features new levels, custom modes, new characters, alongside a full level editor / custom level system!\n\n<img width=\"3840\" height=\"2160\" alt=\"SMB1R_BANNER_printable\" src=\"https://github.com/user-attachments/assets/ed0e97a8-614a-44e2-b69f-2654fca6196c\" />\n\n### Art by [@krystalphantasm.bsky.social](https://bsky.app/profile/krystalphantasm.bsky.social/post/3lvgmgvjeks2f)\n\n### Download: https://github.com/JHDev2006/Super-Mario-Bros.-Remastered-Public/releases\n\n# Requires an original SMB1 NES ROM to play! None of the original assets are contained in the source code, unless it was originally made by us!\n\n# This does NOT act as a replacement for the original Super Mario Bros. games. Super Mario Bros. & Super Mario Bros.: The Lost Levels, can be played now on Nintendo Switch, through Nintendo Switch Online\n\n## Features\n- Super Mario Bros., Super Mario Bros.: The Lost Levels, Super Mario Bros. Special and All Night Nippon: Super Mario Bros. Fully recreated from the ground up!\n- Improved physics / level design\n- Resource Packs! Fully customize how the game looks and sounds.\n- Custom Characters - Add in your own characters to use in game.\n- Fully Open Source!\n- Level Share Square Partnered\n- Portable mode by creating `portable.txt` in the executable directory\n\n## Downloading\n\n### Windows/Linux\n1. Go to the 'Releases' page\n2. Look for the latest version\n3. Download the .zip for your OS\n4. Extract and run\n5. Enjoy!\n\n### macOS (Unofficial)\n1. Go to the [macOS repo](https://github.com/yuriko-shimizu/Super-Mario-Bros.-Remastered-Public-Mac/releases)\n2. (NOTE: THIS IS AN UNOFFICIAL FORK OF THE GAME)\n3. Look for the latest version\n4. Download the .zip file\n5. Extract, drag into the 'Applictions' folder and run\n6. Enjoy!\n\n## Importing for editing\n1. Download the source\n2. Download Godot 4.5 beta 3\n3. Import the project\n4. Enjoy!\n\n## Contributing\nYou are more than welcome to contribute any fixes / improvements you'd like, simply open a pull request, and I'll review it ASAP!\n\n## System Requirements\n\nPlease refer to the Godot engine requirements for minimum and recommended hardware specifications.\n\n[Minimum Requirements](https://docs.godotengine.org/en/latest/about/system_requirements.html#desktop-or-laptop-pc-minimum)\n\n[Recommended Requirements](https://docs.godotengine.org/en/latest/about/system_requirements.html#id3)\n\n\n## Issues\nWhen opening an issue, please keep it to one report, per post, and try and be as helpful as possible, when telling me what has occured, so that its as easy to fix as possible.\nPlease do not open issues, for feature requests, suggestions, or opinions. BUG REPORTS ONLY\n\n## Known Issues\nThere are a couple known issues, mainly due to being built off of Godot, and these issues existing in the engine itself.\n- Steam deck controls do not work natively, you can circumvent this by setting up controller bindings to emulate keys instead, apologies.\n- Physics are weird, when interacting with corners + the camera barrier\n- Drop shadows jitter when playing with \"Smooth Rendering\"\n- Several entities jitter at times.\n- Blocks + coins, respawn when reloading resource packs\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-27T02:14:09.803495"
  },
  {
    "basic_info": {
      "name": "VoxCPM",
      "full_name": "OpenBMB/VoxCPM",
      "owner": "OpenBMB",
      "description": "VoxCPM: Tokenizer-Free TTS for Context-Aware Speech Generation and True-to-Life Voice Cloning",
      "url": "https://github.com/OpenBMB/VoxCPM",
      "clone_url": "https://github.com/OpenBMB/VoxCPM.git",
      "ssh_url": "git@github.com:OpenBMB/VoxCPM.git",
      "homepage": "",
      "created_at": "2025-09-16T03:41:49Z",
      "updated_at": "2025-09-27T02:04:11Z",
      "pushed_at": "2025-09-23T06:12:14Z"
    },
    "stats": {
      "stars": 1397,
      "forks": 147,
      "watchers": 1397,
      "open_issues": 19,
      "size": 1454
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 112327
      },
      "license": "Apache License 2.0",
      "topics": []
    },
    "content": {
      "readme": "## 🎙️ VoxCPM: Tokenizer-Free TTS for Context-Aware Speech Generation and True-to-Life Voice Cloning\n\n\n[![Project Page](https://img.shields.io/badge/Project%20Page-GitHub-blue)](https://github.com/OpenBMB/VoxCPM/) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-OpenBMB-yellow)](https://huggingface.co/openbmb/VoxCPM-0.5B) [![ModelScope](https://img.shields.io/badge/ModelScope-OpenBMB-purple)](https://modelscope.cn/models/OpenBMB/VoxCPM-0.5B)  [![Live Playground](https://img.shields.io/badge/Live%20PlayGround-Demo-orange)](https://huggingface.co/spaces/OpenBMB/VoxCPM-Demo) [![Samples](https://img.shields.io/badge/Page-Samples-red)](https://openbmb.github.io/VoxCPM-demopage)\n\n\n\n<div align=\"center\">\n  <img src=\"assets/voxcpm_logo.png\" alt=\"VoxCPM Logo\" width=\"40%\">\n</div>\n\n<div align=\"center\">\n\n👋 Contact us on [WeChat](assets/wechat.png)\n\n</div>\n\n## News \n* [2025.09.16] 🔥 🔥 🔥  We Open Source the VoxCPM-0.5B [weights](https://huggingface.co/openbmb/VoxCPM-0.5B)!\n* [2025.09.16] 🎉 🎉 🎉  We Provide the [Gradio PlayGround](https://huggingface.co/spaces/OpenBMB/VoxCPM-Demo) for VoxCPM-0.5B, try it now! \n\n## Overview\n\nVoxCPM is a novel tokenizer-free Text-to-Speech (TTS) system that redefines realism in speech synthesis. By modeling speech in a continuous space, it overcomes the limitations of discrete tokenization and enables two flagship capabilities: context-aware speech generation and true-to-life zero-shot voice cloning.\n\nUnlike mainstream approaches that convert speech to discrete tokens, VoxCPM uses an end-to-end diffusion autoregressive architecture that directly generates continuous speech representations from text. Built on [MiniCPM-4](https://huggingface.co/openbmb/MiniCPM4-0.5B) backbone, it achieves implicit semantic-acoustic decoupling through hierachical language modeling and FSQ constraints, greatly enhancing both expressiveness and generation stability.\n\n<div align=\"center\">\n  <img src=\"assets/voxcpm_model.png\" alt=\"VoxCPM Model Architecture\" width=\"90%\">\n</div>\n\n\n###  🚀 Key Features\n- **Context-Aware, Expressive Speech Generation** - VoxCPM comprehends text to infer and generate appropriate prosody, delivering speech with remarkable expressiveness and natural flow. It spontaneously adapts speaking style based on content, producing highly fitting vocal expression trained on a massive 1.8 million-hour bilingual corpus.\n- **True-to-Life Voice Cloning** - With only a short reference audio clip, VoxCPM performs accurate zero-shot voice cloning, capturing not only the speaker’s timbre but also fine-grained characteristics such as accent, emotional tone, rhythm, and pacing to create a faithful and natural replica.\n- **High-Efficiency Synthesis** - VoxCPM supports streaming synthesis with a Real-Time Factor (RTF) as low as 0.17 on a consumer-grade NVIDIA RTX 4090 GPU, making it possible for real-time applications.\n\n\n\n\n\n##  Quick Start\n\n### 🔧 Install from PyPI\n``` sh\npip install voxcpm\n```\n### 1.  Model Download (Optional)\nBy default, when you first run the script, the model will be downloaded automatically, but you can also download the model in advance.\n- Download VoxCPM-0.5B\n    ```\n    from huggingface_hub import snapshot_download\n    snapshot_download(\"openbmb/VoxCPM-0.5B\")\n    ```\n- Download ZipEnhancer and SenseVoice-Small. We use ZipEnhancer to enhance speech prompts and SenseVoice-Small for speech prompt ASR in the web demo.\n    ```\n    from modelscope import snapshot_download\n    snapshot_download('iic/speech_zipenhancer_ans_multiloss_16k_base')\n    snapshot_download('iic/SenseVoiceSmall')\n    ```\n\n### 2. Basic Usage\n```python\nimport soundfile as sf\nimport numpy as np\nfrom voxcpm import VoxCPM\n\nmodel = VoxCPM.from_pretrained(\"openbmb/VoxCPM-0.5B\")\n\n# Non-streaming\nwav = model.generate(\n    text=\"VoxCPM is an innovative end-to-end TTS model from ModelBest, designed to generate highly expressive speech.\",\n    prompt_wav_path=None,      # optional: path to a prompt speech for voice cloning\n    prompt_text=None,          # optional: reference text\n    cfg_value=2.0,             # LM guidance on LocDiT, higher for better adherence to the prompt, but maybe worse\n    inference_timesteps=10,   # LocDiT inference timesteps, higher for better result, lower for fast speed\n    normalize=True,           # enable external TN tool\n    denoise=True,             # enable external Denoise tool\n    retry_badcase=True,        # enable retrying mode for some bad cases (unstoppable)\n    retry_badcase_max_times=3,  # maximum retrying times\n    retry_badcase_ratio_threshold=6.0, # maximum length restriction for bad case detection (simple but effective), it could be adjusted for slow pace speech\n)\n\nsf.write(\"output.wav\", wav, 16000)\nprint(\"saved: output.wav\")\n\n# Streaming\nchunks = []\nfor chunk in model.generate_streaming(\n    text = \"Streaming text to speech is easy with VoxCPM!\",\n    # supports same args as above\n):\n    chunks.append(chunk)\nwav = np.concatenate(chunks)\n\nsf.write(\"output_streaming.",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-27T02:14:10.945842"
  },
  {
    "basic_info": {
      "name": "cagent",
      "full_name": "docker/cagent",
      "owner": "docker",
      "description": "Agent Builder and Runtime by Docker Engineering",
      "url": "https://github.com/docker/cagent",
      "clone_url": "https://github.com/docker/cagent.git",
      "ssh_url": "git@github.com:docker/cagent.git",
      "homepage": "",
      "created_at": "2025-09-01T12:14:45Z",
      "updated_at": "2025-09-26T21:45:10Z",
      "pushed_at": "2025-09-26T21:45:05Z"
    },
    "stats": {
      "stars": 1204,
      "forks": 115,
      "watchers": 1204,
      "open_issues": 48,
      "size": 12324
    },
    "tech_info": {
      "language": "Go",
      "languages": {
        "Go": 762360,
        "Dockerfile": 2127
      },
      "license": "Apache License 2.0",
      "topics": [
        "agents",
        "ai"
      ]
    },
    "content": {
      "readme": "# 🤖 `cagent` 🤖\n\n> A powerful, easy to use, customizable multi-agent runtime that orchestrates AI agents with\n> specialized capabilities and tools, and the interactions between agents.\n\n![cagent in action](docs/assets/cagent-run.gif)\n\n## ✨ What is `cagent`? ✨\n\n`cagent` lets you create and run intelligent AI agents, where each agent has\nspecialized knowledge, tools, and capabilities.\n\nThink of it as allowing you to quickly build, share and run a team of virtual experts that\ncollaborate to solve complex problems for you.\n\nAnd it's dead easy to use!\n\n⚠️ Note: `cagent` is in active development, **breaking changes are to be expected** ⚠️\n\n### Your First Agent\n\nExample [basic_agent.yaml](/examples/basic_agent.yaml):\n\nCreating agents with cagent is very simple. They are described in a short yaml file, like this one:\n\n```yaml\nagents:\n  root:\n    model: openai/gpt-5-mini\n    description: A helpful AI assistant\n    instruction: |\n      You are a knowledgeable assistant that helps users with various tasks.\n      Be helpful, accurate, and concise in your responses.\n```\n\nRun it in a terminal with `cagent run basic_agent.yaml`.\n\nMany more examples can be found [here](/examples/README.md)!\n\n### Improving an agent with MCP tools\n\n`cagent` supports MCP servers, enabling agents to use a wide variety of external tools and services.\n\nIt supports three transport types: `stdio`, `http` and `sse`.\n\nGiving an agent access to tools via MCP is a quick way to greatly improve its capabilities, the quality of its results and its general useful-ness.\n\nGet started quickly with the [Docker MCP Toolkit](https://docs.docker.com/ai/mcp-catalog-and-toolkit/toolkit/) and [catalog](https://docs.docker.com/ai/mcp-catalog-and-toolkit/catalog/)\n\nHere, we're giving the same basic agent from the example above access to a **containerized** `duckduckgo` mcp server and it's tools by using Docker's MCP Gateway:\n\n```yaml\nversion: \"2\"\n\nagents:\n  root:\n    model: openai/gpt-5-mini\n    description: A helpful AI assistant\n    instruction: |\n      You are a knowledgeable assistant that helps users with various tasks.\n      Be helpful, accurate, and concise in your responses.\n    toolsets:\n      - type: mcp\n        ref: docker:duckduckgo # stdio transport\n```\n\nWhen using a containerized server via the Docker MCP gateway, you can configure any required settings/secrets/authentication using the [Docker MCP Toolkit](https://docs.docker.com/ai/mcp-catalog-and-toolkit/toolkit/#example-use-the-github-official-mcp-server) in Docker Desktop.\n\nAside from the containerized MCP severs the Docker MCP Gateway provides, any standard MCP server can be used with cagent!\n\nHere's an example similar to the above but adding `read_file` and `write_file` tools from the `rust-mcp-filesystem` MCP server:\n\n```yaml\nversion: \"2\"\n\nagents:\n  root:\n    model: openai/gpt-5-mini\n    description: A helpful AI assistant\n    instruction: |\n      You are a knowledgeable assistant that helps users with various tasks.\n      Be helpful, accurate, and concise in your responses. Write your search results to disk.\n    toolsets:\n      - type: mcp\n        ref: docker:duckduckgo\n      - type: mcp\n        command: rust-mcp-filesystem # installed with `cargo install rust-mcp-filesystem`\n        args: [\"--allow-write\", \".\"]\n        tools: [\"read_file\", \"write_file\"] # Optional: specific tools only\n        env:\n          - \"RUST_LOG=debug\"\n```\n\nSee [the USAGE docs](./docs/USAGE.md#tool-configuration) for more detailed information and examples\n\n### 🎯 Key Features\n\n- **🏗️ Multi-agent architecture** - Create specialized agents for different domains.\n- **🔧 Rich tool ecosystem** - Agents can use external tools and APIs via the MCP protocol.\n- **🔄 Smart delegation** - Agents can automatically route tasks to the most suitable specialist.\n- **📝 YAML configuration** - Declarative model and agent configuration.\n- **💭 Advanced reasoning** - Built-in \"think\", \"todo\" and \"memory\" tools for complex problem-solving.\n- **🌐 Multiple AI providers** - Support for OpenAI, Anthropic, Gemini and [Docker Model Runner](https://docs.docker.com/ai/model-runner/).\n\n## 🚀 Quick Start 🚀\n\n### Installation\n\n[Prebuilt binaries](https://github.com/docker/cagent/releases) for Windows, macOS and Linux can be found on the releases page of the [project's GitHub repository](https://github.com/docker/cagent/releases)\n\nOnce you've downloaded the appropriate binary for your platform, you may need to give it executable permissions.\nOn macOS and Linux, this is done with the following command:\n\n```sh\n# linux amd64 build example\nchmod +x /path/to/downloads/cagent-linux-amd64\n```\n\nYou can then rename the binary to `cagent` and configure your `PATH` to be able to find it (configuration varies by platform).\n\n### **Set your API keys**\n\nBased on the models you configure your agents to use, you will need to set the corresponding provider API key accordingly,\nall theses keys are optional, you will likely need at least one of these, though:\n\n```bash\n# For OpenAI",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-27T02:14:12.113626"
  },
  {
    "basic_info": {
      "name": "multi-agent-coding-system",
      "full_name": "Danau5tin/multi-agent-coding-system",
      "owner": "Danau5tin",
      "description": "Reached #13 on Stanford's Terminal Bench leaderboard. Orchestrator, explorer & coder agents working together with intelligent context sharing.",
      "url": "https://github.com/Danau5tin/multi-agent-coding-system",
      "clone_url": "https://github.com/Danau5tin/multi-agent-coding-system.git",
      "ssh_url": "git@github.com:Danau5tin/multi-agent-coding-system.git",
      "homepage": "",
      "created_at": "2025-08-30T19:23:45Z",
      "updated_at": "2025-09-26T10:41:45Z",
      "pushed_at": "2025-09-07T16:01:23Z"
    },
    "stats": {
      "stars": 1198,
      "forks": 145,
      "watchers": 1198,
      "open_issues": 2,
      "size": 849
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 139397,
        "Shell": 483
      },
      "license": null,
      "topics": []
    },
    "content": {
      "readme": "# 🤓 Orchestrator: A multi-agent AI coder. Reached #13 on Stanford's TerminalBench. Open sourced!\n\nTL;DR:\n- Over the weekend, quite unexpectedly, I made a multi-agent AI system that places slightly higher than Claude Code on Stanford's TerminalBench leaderboard (13th place).\n- This AI system consists of an orchestration agent that dispatches multiple explorer and coder agents to do all the work.\n- The orchestrator explicitly defines what knowledge artifacts subagents must return, then reuses and synthesises these artifacts across future tasks - creating compound intelligence where each action builds meaningfully on previous discoveries.\n\n![Orchestrator with claude-sonnet-4 on standford's terminal bench](./readme_imgs/orchestrator-sonnet-4-stanford-terminal-bench-leaderboard.png)\n\n## How the System Works\n\n![System architecture overview](readme_imgs/orch_agent_sys_arch.png)\n\nThe orchestrator acts as the brain of the operation - it receives the user's task but never touches code directly. Instead, it:\n\n1. **Analyses** the task and breaks it into focused subtasks\n2. **Dispatches** explorer agents to understand the system\n3. **Delegates** implementation work to coder agents with precise instructions\n4. **Verifies** all changes through additional explorer agents\n5. **Maintains** the context store with all discovered knowledge\n\nThe orchestrator's lack of direct code access forces proper delegation and verification patterns, leading to more strategic solutions.\n\nFor a full breakdown of this project's code structure, see [here](./PROJECT_STRUCTURE.md)\n\n\n## 📈 Evaluation Results\n\n### Performance on TerminalBench\n\n[Terminal bench](https://www.tbench.ai/) is a brilliant benchmark created by Stanford and [Laude Institute](https://www.laude.org/) to quantify agents' ability to complete complex tasks in the terminal. My Orchestrator system achieved **13th place** on the leaderboard, demonstrating competitive performance against leading AI coding assistants.\n\nI ran the Orchestrator evaluations with both Claude-4-Sonnet and also Qwen3-Coder-480B-A35B:\n\n![Performance comparison chart](readme_imgs/perf_chart.png)\n![Orchestrator with qwen-3-coder on standford's terminal bench](./readme_imgs/orchestrator-qwen-3-coder-stanford-terminal-bench-leaderboard.png)\n\nThis image shows Qwen-3-Coder performance on the benchmark. The screenshot towards the top of this README shows Sonnet-4 performance.\n\n### Cost & Efficiency\n\nOne of the most striking results is the amount of tokens used by Sonnet-4 as opposed to Qwen3-Coder.\n\nThe below table shows the total tokens (input and output included) processed across the TerminalBench evaluation run (5 attempts at 80 tasks = 400 trajectories).\n\n| Model | Success Rate | Total Evaluation Cost | Token Usage |\n|-------|--------------|------------|-------------|\n| **Claude Sonnet-4** | 37.0% | $263.56* | 93.2M tokens |\n| **Qwen-3-Coder** | 19.7% | $217.83 | 14.7M tokens |\n\n*Claude Sonnet-4 costs reflect heavy caching usage, reducing actual API costs\n\n\n## 🤖 The Agents\n\nWhile all agents use the same underlying LLM, each operates with its own context window, specialised system message, and distinct toolset. This creates functionally different agents optimised for their specific roles.\n\n### 🎯 Orchestrator Agent\n[System message](./src/agents/system_msgs/md_files/orchestrator_sys_msg_v0.1.md)\n**Role:** Strategic coordinator and persistent intelligence layer  \n**Capabilities:** Task decomposition, context management, subagent delegation  \n**Tools:** Task creation, subagent launching, context store management  \n**Restrictions:** Cannot read or modify code directly - operates purely at architectural level  \n\nThe orchestrator maintains the complete picture across all tasks, tracking discoveries and progress. It crafts precise task descriptions that explicitly specify what contexts subagents should return, ensuring focused and valuable information gathering.\n\n**Trust Calibration Strategy:**  \nThe orchestrator employs adaptive delegation based on task complexity:\n- **Low Complexity Tasks**: Grants extremely high autonomy to the coder agent for simple modifications and bug fixes\n- **Medium/Large Tasks**: Maintains strong trust but uses iterative decomposition - breaking complex problems into atomic, verifiable steps\n- **Verification Philosophy**: Uses explorer agents liberally to verify progress, especially when tasks involve critical functionality\n\n\n### 🔍 Explorer Agent \n[System message](./src/agents/system_msgs/md_files/explorer_sys_msg_v0.1.md) \n**Role:** Read-only investigation and verification specialist  \n**Capabilities:** System exploration, code analysis, test execution, verification  \n**Tools:** File reading, search operations (grep/glob), bash commands, temporary script creation  \n**Restrictions:** Cannot modify existing files - strictly read-only operations  \n\nExplorers gather intelligence about the codebase, verify implementations, and discover system behaviors. They create knowledge artifacts that eliminat",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-27T02:14:13.276826"
  },
  {
    "basic_info": {
      "name": "sj.h",
      "full_name": "rxi/sj.h",
      "owner": "rxi",
      "description": "A tiny little JSON parsing library",
      "url": "https://github.com/rxi/sj.h",
      "clone_url": "https://github.com/rxi/sj.h.git",
      "ssh_url": "git@github.com:rxi/sj.h.git",
      "homepage": null,
      "created_at": "2025-09-19T18:20:20Z",
      "updated_at": "2025-09-26T23:35:25Z",
      "pushed_at": "2025-09-21T18:29:44Z"
    },
    "stats": {
      "stars": 1140,
      "forks": 23,
      "watchers": 1140,
      "open_issues": 3,
      "size": 10
    },
    "tech_info": {
      "language": "C",
      "languages": {
        "C": 4081
      },
      "license": "The Unlicense",
      "topics": []
    },
    "content": {
      "readme": "# sj.h\nA tiny little JSON parsing library\n\n- ~150 lines of C99\n- Zero-allocations with minimal state\n- Error messages with `line:column:` location\n- No number parsing: `strtod`, `atoi`? Handle them how you want\n- No string parsing: bring your own unicode surrogate pair handling (or don't)\n\n\n## Usage\nA small program to load a rectangle from a JSON string into a `Rect` struct:\n```c\nchar *json_text = \"{ \\\"x\\\": 10, \\\"y\\\": 20, \\\"w\\\": 30, \\\"h\\\": 40 }\";\n\ntypedef struct { int x, y, w, h; } Rect;\n\nbool eq(sj_Value val, char *s) {\n    size_t len = val.end - val.start;\n    return strlen(s) == len && !memcmp(s, val.start, len);\n}\n\nint main(void) {\n    Rect rect = {0};\n\n    sj_Reader r = sj_reader(json_text, strlen(json_text));\n    sj_Value obj = sj_read(&r);\n\n    sj_Value key, val;\n    while (sj_iter_object(&r, obj, &key, &val)) {\n        if (eq(key, \"x\")) { rect.x = atoi(val.start); }\n        if (eq(key, \"y\")) { rect.y = atoi(val.start); }\n        if (eq(key, \"w\")) { rect.w = atoi(val.start); }\n        if (eq(key, \"h\")) { rect.h = atoi(val.start); }\n    }\n\n    printf(\"rect: { %d, %d, %d, %d }\\n\", rect.x, rect.y, rect.w, rect.h);\n    return 0;\n}\n```\n\nSee the [**demo**](demo/) folder for further usage examples.\n\n\n## License\nThis is free and unencumbered software released into the public domain. See\n[LICENSE](LICENSE) for details.",
      "default_branch": "master"
    },
    "fetched_at": "2025-09-27T02:14:14.425606"
  },
  {
    "basic_info": {
      "name": "LongCat-Flash-Chat",
      "full_name": "meituan-longcat/LongCat-Flash-Chat",
      "owner": "meituan-longcat",
      "description": null,
      "url": "https://github.com/meituan-longcat/LongCat-Flash-Chat",
      "clone_url": "https://github.com/meituan-longcat/LongCat-Flash-Chat.git",
      "ssh_url": "git@github.com:meituan-longcat/LongCat-Flash-Chat.git",
      "homepage": null,
      "created_at": "2025-08-30T16:00:04Z",
      "updated_at": "2025-09-26T18:50:39Z",
      "pushed_at": "2025-09-26T03:15:19Z"
    },
    "stats": {
      "stars": 1130,
      "forks": 52,
      "watchers": 1130,
      "open_issues": 17,
      "size": 20091
    },
    "tech_info": {
      "language": null,
      "languages": {},
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "# LongCat-Flash-Chat\n\n<div align=\"center\">\n  <img src=\"figures/longcat_logo.svg\" width=\"45%\" alt=\"LongCat-Flash\" />\n</div>\n<hr>\n\n<div align=\"center\" style=\"line-height: 1;\">\n  <a href=\"https://longcat.ai/\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Chat\" src=\"https://img.shields.io/badge/🤖%20Chat-LongCat--Flash--Chat-ADFF2F?color=29E154&logoColor=white\"  fill-opacity=\"1\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://huggingface.co/meituan-longcat\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Hugging Face\" src=\"https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-LongCat-ffc107?color=ffc107&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n</div>\n\n<div align=\"center\" style=\"line-height: 1;\">\n  <a href=\"https://github.com/meituan-longcat/LongCat-Flash-Chat/blob/main/figures/wechat_official_accounts.png\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Wechat\" src=\"https://img.shields.io/badge/WeChat-LongCat-brightgreen?logo=wechat&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://x.com/Meituan_LongCat\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Twitter Follow\" src=\"https://img.shields.io/badge/Twitter-LongCat-white?logo=x&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n</div>\n\n<div align=\"center\" style=\"line-height: 1;\">\n  <a href=\"LICENSE\" style=\"margin: 2px;\">\n    <img alt=\"License\" src=\"https://img.shields.io/badge/License-MIT-f5de53?&color=f5de53\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n</div>\n\n<p align=\"center\">\n  <a href=\"https://arxiv.org/abs/2509.01322\"><b>Tech Report</b>&nbsp;📄</a>\n</p>\n\n## Model Introduction\nWe introduce LongCat-Flash, a powerful and efficient language model with 560 billion total parameters, featuring an innovative Mixture-of-Experts (MoE) architecture. The model incorporates a dynamic computation mechanism that activates 18.6B∼31.3B parameters (averaging∼27B) based on contextual demands, optimizing both computational efficiency and performance. To achieve advanced training and inference efficiency, we employ a shortcut-connected architecture that expands computation-communication overlap window, achieving over 100 tokens per second (TPS) for inference cost-effectively. Our comprehensive training and scaling strategies ensure stable, efficient training, while tailored data strategies enhance model performance.\n\nNow we release LongCat-Flash-Chat, a non-thinking foundation model that delivers highly competitive performance among leading models, with exceptional strengths in agentic tasks.\n\n\n### Key Features\n\n#### 🌟 Scalable Architectural Design for Computational Efficiency\n\nLongCat-Flash is designed and optimized under two key principles: efficient computation utilization, as well as  efficient training and inference. Specifically, (1) As not all tokens are equal, we introduce the zero-computation experts mechanism in MoE blocks to allocate a dynamic computation budget to important tokens based on their significance, i.e., activating 18.6 to 31.3 billion parameters (out of 560 billion total) based on contextual demands. To ensure consistent computation load, we employ expert bias adjusted by a PID-controller, maintaining an average of∼27 billion activated parameters per token. (2) As communication overhead becomes a bottleneck during MoE model scaling, we incorporate the Shortcut-connected MoE (ScMoE) design to expand the computation-communication overlap window. Combined with customized infrastructure optimizations, this design enables training at a massive scale of over tens of thousands accelerators and inference with high throughput and low latency.\n\n\n#### 🌟 Effective Model Scaling Strategy\n\nEffectively and efficiently scaling model size remains a key challenge in strategy design. To this end, we develop a comprehensive stability-and-scaling framework for robustly training large-scale models: (1) We successfully apply a hyperparameter transfer strategy to such a large model, predicting optimal hyperparameter configurations by leveraging results from smaller proxy models with theoretical guarantees. (2) We initialize the model using a model-growth mechanism based on a refined half-scale checkpoint, achieving improved performance compared to conventional initialization methods. (3) A multi-pronged stability suite incorporates principled router-gradient balancing, a hidden z-loss to suppress massive activations, and fine-tuned optimizer configurations. (4) To enhance the reliability of large-scale cluster training, we introduce deterministic computation. This guarantees the exact reproducibility of experiments and enables the detection of SDC (Silent Data Corruption) during the training process. These interventions ensure that LongCat-Flash ’s training remains stable, with no irrecoverable loss spikes.\n\n#### 🌟 Multi-Stage Training Pipeline for Agentic Capability\nThrough a metic",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-27T02:14:15.562176"
  },
  {
    "basic_info": {
      "name": "Nano-Bananary",
      "full_name": "ZHO-ZHO-ZHO/Nano-Bananary",
      "owner": "ZHO-ZHO-ZHO",
      "description": "香蕉超市｜各种玩法一键生成，无需提示词，支持局部涂选、连续编辑",
      "url": "https://github.com/ZHO-ZHO-ZHO/Nano-Bananary",
      "clone_url": "https://github.com/ZHO-ZHO-ZHO/Nano-Bananary.git",
      "ssh_url": "git@github.com:ZHO-ZHO-ZHO/Nano-Bananary.git",
      "homepage": null,
      "created_at": "2025-09-05T10:18:39Z",
      "updated_at": "2025-09-27T01:14:46Z",
      "pushed_at": "2025-09-16T15:14:05Z"
    },
    "stats": {
      "stars": 1069,
      "forks": 197,
      "watchers": 1069,
      "open_issues": 5,
      "size": 104
    },
    "tech_info": {
      "language": "TypeScript",
      "languages": {
        "TypeScript": 154642,
        "HTML": 2693
      },
      "license": null,
      "topics": []
    },
    "content": {
      "readme": "<div align=\"center\">\n<img width=\"2816\" height=\"1536\" alt=\"Group 333\" src=\"https://github.com/user-attachments/assets/0800046e-71aa-4dee-93a6-731b9f914a35\" />\n\n\n\n# Nano Bananary ｜ 香蕉超市 ｜ ZHO\n\n\n\n<img width=\"1251\" height=\"2051\" alt=\"Group 336\" src=\"https://github.com/user-attachments/assets/6b1fc8a2-e86b-403b-be21-97eb2ff65034\" />\n\n\n\n<img width=\"1556\" height=\"1086\" alt=\"screenshot-20250905-191422\" src=\"https://github.com/user-attachments/assets/23953d15-8ebb-4574-bbc9-74b44b79f341\" />\n\n\n</div>\n\n\n## 🆕 新增 视频生成 + 中文界面 + 浅色主题！\n\n\n### 1）中文界面 + 浅色主题 一键切换！\n\n<img width=\"1311\" height=\"738\" alt=\"screenshot-20250916-153520\" src=\"https://github.com/user-attachments/assets/476241e6-b7bf-4f66-81c5-fdaaa0695bcd\" />\n\n\n### 2）上线视频生成功能！所有玩法一键转化为视频，纵享丝滑！\n\n\nhttps://github.com/user-attachments/assets/4cce75d2-9783-41a4-94cc-2837c365d5a8\n\n\n\n\nhttps://github.com/user-attachments/assets/be93114f-3bc4-4ddf-947c-4694268444e4\n\n\n\n\n## 1）无需提示词，丝滑衔接\n\n\n\nhttps://github.com/user-attachments/assets/39976fe1-fafe-4ecf-94b2-5f6053f92c7f\n\n\n\n✅各种玩法一键生成，无需提示词，支持局部涂选、连续编辑：\n\n1⃣选择用法\n\n2⃣上传图片\n\n3⃣点击生成/局部涂选\n\n4⃣直接发送到新玩法中继续\n\n\n## 2）核心功能：每次输出都能直接作为输入进行下一次编辑/生成\n\n\n\nhttps://github.com/user-attachments/assets/ca1cc851-ccca-44c6-b3f4-138b0650c0f9\n\n\n\n<img width=\"1664\" height=\"1248\" alt=\"comparison-image-1757069194058\" src=\"https://github.com/user-attachments/assets/6260a835-8404-4772-a152-303d10ab9551\" />\n\n\n<img width=\"1392\" height=\"1280\" alt=\"screenshot-20250905-220201\" src=\"https://github.com/user-attachments/assets/5cb573dc-78dd-4c5c-8194-9172af94d65d\" />\n\n\n\n<img width=\"2048\" height=\"2507\" alt=\"Group 334\" src=\"https://github.com/user-attachments/assets/a4dd528c-e0a6-4a52-844c-ff0c28d0c99c\" />\n\n\n\n## Online\n\n\nUse in AI Studio: https://ai.studio/apps/drive/1JknFrFFdiOm7FIA8MLOJa_vtJN2g24c1\n\n\n## Run Locally\n\n**Prerequisites:**  Node.js\n\n\n1. Install dependencies:\n   `npm install`\n2. Set the `GEMINI_API_KEY` in [.env.local](.env.local) to your Gemini API key\n3. Run the app:\n   `npm run dev`\n\n\n\n\n## 更新日志\n\n- 20250916\n\n  新增 视频生成 + 中文界面 + 浅色主题\n\n\n- 20250906\n\n  功能更新：增加历史记录功能，方便直接使用已生成的图像作为输入\n\n  玩法更新：已经把 我的[🍌提示词库](https://github.com/ZHO-ZHO-ZHO/ZHO-nano-banana-Creation)的主要玩法更新上去了\n\n\n- 20250905\n \n  更新配色：black & orange\n\n  新增功能：\n  \n  1⃣ 放大预览\n  \n  2⃣横置对比 + 对比图直接下载（方便直接发社交媒体）\n  \n  3⃣滑块对比：方便原图对照\n  \n  4⃣自定义提示词模块：方便个性化生成\n\n\n- 20250095\n  \n  创建项目\n  \n\n## Stars \n\n[![Star History Chart](https://api.star-history.com/svg?repos=ZHO-ZHO-ZHO/Nano-Bananary&type=Date)](https://star-history.com/#ZHO-ZHO-ZHO/Nano-Bananary&Date)\n\n\n## 关于我 | About me\n\n📬 **联系我**：\n- 邮箱：zhozho3965@gmail.com\n  \n\n🔗 **社交媒体**：\n- 个人页：[-Zho-](https://jike.city/zho)\n- Bilibili：[我的B站主页](https://space.bilibili.com/484366804)\n- X（Twitter）：[我的Twitter](https://twitter.com/ZHO_ZHO_ZHO)\n- 小红书：[我的小红书主页](https://www.xiaohongshu.com/user/profile/63f11530000000001001e0c8?xhsshare=CopyLink&appuid=63f11530000000001001e0c8&apptime=1690528872)\n\n💡 **支持我**：\n- B站：[B站充电](https://space.bilibili.com/484366804)\n- 爱发电：[为我充电](https://afdian.com/a/ZHOZHO)\n\n\n## Credits\n\n[Gemini 2.5 Flash Image](https://gemini.google.com/app)\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-27T02:14:16.708544"
  },
  {
    "basic_info": {
      "name": "hallbayes",
      "full_name": "leochlon/hallbayes",
      "owner": "leochlon",
      "description": null,
      "url": "https://github.com/leochlon/hallbayes",
      "clone_url": "https://github.com/leochlon/hallbayes.git",
      "ssh_url": "git@github.com:leochlon/hallbayes.git",
      "homepage": null,
      "created_at": "2025-09-01T12:51:01Z",
      "updated_at": "2025-09-27T02:06:55Z",
      "pushed_at": "2025-09-27T00:07:37Z"
    },
    "stats": {
      "stars": 1067,
      "forks": 100,
      "watchers": 1067,
      "open_issues": 9,
      "size": 193012
    },
    "tech_info": {
      "language": "HTML",
      "languages": {
        "HTML": 268076,
        "TeX": 97584,
        "Python": 97395,
        "JavaScript": 3738,
        "Shell": 1350,
        "Batchfile": 935
      },
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "# Hallucination Risk Calculator & Prompt Re-engineering Toolkit (OpenAI-only)\n\n**Post-hoc calibration without retraining** for large language models. This toolkit turns a raw prompt into:  \n1) a **bounded hallucination risk** using the Expectation-level Decompression Law (EDFL), and  \n2) a **decision** to **ANSWER** or **REFUSE** under a target SLA, with transparent math (nats).\n\nIt supports two deployment modes:\n\n- **Evidence-based:** prompts include *evidence/context*; rolling priors are built by erasing that evidence.\n- **Closed-book:** prompts have *no evidence*; rolling priors are built by semantic masking of entities/numbers/titles.\n\nAll scoring relies **only** on the OpenAI Chat Completions API. No retraining required.\n\n---\n\n## Table of Contents\n- [Install & Setup](#install--setup)\n- [Core Mathematical Framework](#core-mathematical-framework)\n- [Understanding System Behavior](#understanding-system-behavior)\n- [Two Ways to Build Rolling Priors](#two-ways-to-build-rolling-priors)\n- [API Surface](#api-surface)\n- [Calibration & Validation](#calibration--validation)\n- [Practical Considerations](#practical-considerations)\n- [Project Layout](#project-layout)\n- [Deployment Options](#deployment-options)\n\n---\n\n## Install & Setup\n\n```bash\npip install --upgrade openai\nexport OPENAI_API_KEY=sk-...\n```\n\n> The module uses `openai>=1.0.0` and the Chat Completions API (e.g., `gpt-4o`, `gpt-4o-mini`).\n\n---\n\n## Core Mathematical Framework\n\n### The EDFL Principle\n\nLet the binary event $\\mathcal{A}$ be the thing you want to guarantee (e.g., **Answer** in decision mode, or **Correct** for factual accuracy).  \nBuild an ensemble of **content-weakened prompts** (the *rolling priors*) $\\{S_k\\}_{k=1}^m$. For the realized label $y$, estimate:\n\n- **Information budget:**  \n  $$\\bar{\\Delta} = \\tfrac{1}{m}\\sum_k \\mathrm{clip}_+(\\log P(y) - \\log S_k(y), B)$$\n  (one-sided clipping; default $B=12$ nats to prevent outliers while maintaining conservative bounds).\n\n- **Prior masses:** $q_k = S_k(\\mathcal{A})$, with:\n  - $\\bar{q}=\\tfrac{1}{m}\\sum_k q_k$ (average prior for EDFL bound)\n  - $q_{\\text{lo}}=\\min_k q_k$ (worst-case prior for SLA gating)\n\nBy EDFL, the achievable reliability is bounded by:  \n$$\\bar{\\Delta} \\ge \\mathrm{KL}(\\mathrm{Ber}(p) \\| \\mathrm{Ber}(\\bar{q})) \\Rightarrow p\\le p_{\\max}(\\bar{\\Delta},\\bar{q})$$\n\nThus the **hallucination risk** (error) is bounded by $\\overline{\\mathrm{RoH}} \\le 1 - p_{\\max}$.\n\n### Decision Rule (SLA Gating)\n\nFor target hallucination rate $h^*$:\n- **Bits-to-Trust:** $\\mathrm{B2T} = \\mathrm{KL}(\\mathrm{Ber}(1-h^*) \\| \\mathrm{Ber}(q_{\\text{lo}}))$\n- **Information Sufficiency Ratio:** $\\mathrm{ISR} = \\bar{\\Delta}/\\mathrm{B2T}$\n- **ANSWER** iff $\\mathrm{ISR}\\ge 1$ *and* $\\bar{\\Delta} \\ge \\mathrm{B2T} + \\text{margin}$ (default `margin≈0.2` nats)\n\n> **Why two priors?** The gate uses **worst-case** $q_{\\text{lo}}$ for strict SLA compliance. The RoH bound uses **average** $\\bar{q}$ per EDFL theory. This dual approach ensures conservative safety while providing realistic risk bounds.\n\n---\n\n## Understanding System Behavior\n\n### Expected Behavioral Patterns\n\nThe toolkit exhibits different behaviors across query types, which is **mathematically consistent** with the framework:\n\n#### Simple Arithmetic Queries\n**Observation:** May abstain despite apparent simplicity  \n**Explanation:**\n- Models often attempt answers even with masked numbers (pattern recognition)\n- This yields **low information lift** $\\bar{\\Delta} \\approx 0$ between full prompt and skeletons\n- Despite potentially low EDFL risk bound, worst-case prior gate triggers **abstention** (ISR < 1)\n\n#### Named-Entity Factoids\n**Observation:** Generally answered with confidence  \n**Explanation:**\n- Masking entities/dates substantially reduces answer propensity in skeletons\n- Restoring these yields **large** $\\bar{\\Delta}$ that clears B2T threshold\n- System **answers** with tight EDFL risk bound\n\n**This is not a bug but a feature**: The framework prioritizes safety through worst-case guarantees while providing realistic average-case bounds.\n\n### Mitigation Strategies\n\n1. **Switch Event Measurement**\n   - Use **Correct/Incorrect** instead of Answer/Refuse for factual QA\n   - Skeletons without key information rarely yield correct results → large $\\bar{\\Delta}$\n\n2. **Enhance Skeleton Weakening**\n   - Implement mask-aware decision head that refuses on redaction tokens\n   - Ensures skeletons have strictly lower \"Answer\" mass than full prompt\n\n3. **Calibration Adjustments**\n   - Relax $h^*$ slightly (e.g., 0.10 instead of 0.05) for higher answer rates\n   - Reduce margin for less conservative gating\n   - Increase sampling ($n=7-10$) for stability\n\n4. **Provide Evidence**\n   - Adding compact, relevant evidence increases $\\bar{\\Delta}$ while preserving bounds\n\n---\n\n## Two Ways to Build Rolling Priors\n\n### 1) Evidence-based (when you have context)\n\n- **Prompt** contains a field like `Evidence:` (or JSON keys)\n- **Skeletons** erase the evidence content but preserve st",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-27T02:14:17.837252"
  },
  {
    "basic_info": {
      "name": "SRPO",
      "full_name": "Tencent-Hunyuan/SRPO",
      "owner": "Tencent-Hunyuan",
      "description": "Directly Aligning the Full Diffusion Trajectory with Fine-Grained Human Preference",
      "url": "https://github.com/Tencent-Hunyuan/SRPO",
      "clone_url": "https://github.com/Tencent-Hunyuan/SRPO.git",
      "ssh_url": "git@github.com:Tencent-Hunyuan/SRPO.git",
      "homepage": "https://tencent.github.io/srpo-project-page/",
      "created_at": "2025-09-09T07:36:49Z",
      "updated_at": "2025-09-26T23:04:17Z",
      "pushed_at": "2025-09-16T06:21:00Z"
    },
    "stats": {
      "stars": 1014,
      "forks": 32,
      "watchers": 1014,
      "open_issues": 13,
      "size": 12251
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 712645,
        "Shell": 4058
      },
      "license": "Other",
      "topics": []
    },
    "content": {
      "readme": "<div align=“center” style=“font-family: charter;”>\n<h1 align=\"center\">Directly Aligning the Full Diffusion Trajectory with Fine-Grained Human Preference </h1>\n<div align=\"center\">\n  <a href='https://arxiv.org/abs/2509.06942'><img src='https://img.shields.io/badge/ArXiv-red?logo=arxiv'></a>  &nbsp;\n  <a href='https://huggingface.co/tencent/SRPO/'><img src='https://img.shields.io/badge/Model-blue?logo=huggingface'></a> &nbsp; \n  <a href='https://tencent.github.io/srpo-project-page/'><img src='https://img.shields.io/badge/%F0%9F%92%BB_Project-SRPO-blue'></a> &nbsp;\n</div>\n<div align=\"center\">\n  Xiangwei Shen<sup>1,2,3*</sup>,\n  <a href=\"https://scholar.google.com/citations?user=Lnr1FQEAAAAJ&hl=zh-CN\" target=\"_blank\"><b>Zhimin Li</b></a><sup>1*</sup>,\n  <a href=\"https://scholar.google.com.hk/citations?user=Fz3X5FwAAAAJ\" target=\"_blank\"><b>Zhantao Yang</b></a><sup>1</sup>, \n  <a href=\"https://shiyi-zh0408.github.io/\" target=\"_blank\"><b>Shiyi Zhang</b></a><sup>3</sup>,\n  Yingfang Zhang<sup>1</sup>,\n  Donghao Li<sup>1</sup>,\n  <br>\n  <a href=\"https://scholar.google.com/citations?user=VXQV5xwAAAAJ&hl=en\" target=\"_blank\"><b>Chunyu Wang</b></a><sup>1✝</sup>,\n  <a href=\"https://openreview.net/profile?id=%7EQinglin_Lu2\" target=\"_blank\"><b>Qinglin Lu</b></a><sup>1</sup>,\n  <a href=\"https://andytang15.github.io\" target=\"_blank\"><b>Yansong Tang</b></a><sup>3,✉️</sup>\n</div>\n<div align=\"center\">\n  <sup>1</sup>Hunyuan, Tencent \n  <br>\n  <sup>2</sup>School of Science and Engineering, The Chinese University of Hong Kong, Shenzhen \n  <br>\n  <sup>3</sup>Shenzhen International Graduate School, Tsinghua University \n  <br>\n  <sup>*</sup>Equal contribution \n  <sup>✝</sup>Project lead \n  <sup>✉️</sup>Corresponding author\n</div>\n\n![head](assets/head.jpg)\n\n## 🎉 Key Features\n1. **Direct Align**: We introduce a new sampling strategy for diffusion fine-tuning that can effectively restore highly noisy images, leading to an optimization process that is more stable and less computationally demanding, especially during the initial timesteps.\n2. **Faster Training**:   By rolling out only a single image and optimizing directly with analytical gradients—a key distinction from GRPO—our method achieves significant performance improvements for FLUX.1.dev in under 10 minutes of training. To further accelerate the process, our method supports replacing online rollouts entirely with a small dataset of real images; we find that fewer than 1500 images are sufficient to effectively train FLUX.1.dev.\n3. **Free of Reward Hacking**: We have improved the training strategy for method that direct backpropagation on reward signal (such as ReFL and DRaFT). Moreover, we directly regularize the model using negative rewards, without the need for KL divergence or a separate reward system. In our experiments, this approach achieves comparable performance with multiple different rewards, improving the perceptual quality of FLUX.1.dev without suffering from reward hacking issues, such as overfitting to color or oversaturation preferences.\n4. **Potential for Controllable Fine-tuning**: For the first time in online RL, we incorporate dynamically controllable text conditions, enabling on-the-fly adjustment of reward preference towards styles within the scope of the reward model.\n\n## 🔥 News\n\n- __[2025.9.12]__:  🎉 We released the complete training code. We also share tips and experiences to help you train your models. You’re welcome to discuss and ask questions in the issues! 💬✨\n- __[2025.9.12]__:  🎉 We provide a standard workflow—feel free to use it in ComfyUI.\n- __[2025.9.8]__:   🎉 We released the paper, checkpoint, inference code.\n\n## 📑 Open-source Plan\n- [X] The training code is under internal review and will be open-sourced by this weekend at the latest.\n- [ ] Release a quantized version for the FLUX community.\n- [ ] Extend support to other models.\n\n## 🛠️ Dependencies and Installation\n\n```bash\nconda create -n SRPO python=3.10.16 -y\nconda activate SRPO\nbash ./env_setup.sh \n```\n💡 The environment dependency is basically the same as DanceGRPO\n\n## 🤗 Download Models\n\n1. Model Cards\n\n|       Model       |                           Huggingface Download URL                                      |  \n|:-----------------:|:---------------------------------------------------------------------------------------:|\n|       SRPO        |           [diffusion_pytorch_model](https://huggingface.co/tencent/SRPO/tree/main)      |\n\n2. Download our `diffusion_pytorch_model.safetensors` in [https://huggingface.co/tencent/SRPO]\n```bash\nmkdir ./srpo\nhuggingface-cli login\nhuggingface-cli download --resume-download Tencent/SRPO diffusion_pytorch_model.safetensors --local-dir ./srpo/\n```\n3. Load your FLUX cache or use the `black-forest-labs/FLUX.1-dev`[https://huggingface.co/black-forest-labs/FLUX.1-dev]\n```bash\nmkdir ./data/flux\nhuggingface-cli login\nhuggingface-cli download --resume-download  black-forest-labs/FLUX.1-dev --local-dir ./data/flux\n```\n\n## 🔑 Inference\n\n### Using ComfyUI\n\nYou can use",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-27T02:14:18.982499"
  }
]