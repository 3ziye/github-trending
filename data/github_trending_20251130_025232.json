[
  {
    "basic_info": {
      "name": "llm-council",
      "full_name": "karpathy/llm-council",
      "owner": "karpathy",
      "description": "LLM Council works together to answer your hardest questions",
      "url": "https://github.com/karpathy/llm-council",
      "clone_url": "https://github.com/karpathy/llm-council.git",
      "ssh_url": "git@github.com:karpathy/llm-council.git",
      "homepage": "",
      "created_at": "2025-11-22T23:24:14Z",
      "updated_at": "2025-11-30T02:51:31Z",
      "pushed_at": "2025-11-22T23:35:21Z"
    },
    "stats": {
      "stars": 7812,
      "forks": 1203,
      "watchers": 7812,
      "open_issues": 44,
      "size": 262
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 24729,
        "JavaScript": 20694,
        "CSS": 9346,
        "Shell": 625,
        "HTML": 357
      },
      "license": null,
      "topics": []
    },
    "content": {
      "readme": "# LLM Council\n\n![llmcouncil](header.jpg)\n\nThe idea of this repo is that instead of asking a question to your favorite LLM provider (e.g. OpenAI GPT 5.1, Google Gemini 3.0 Pro, Anthropic Claude Sonnet 4.5, xAI Grok 4, eg.c), you can group them into your \"LLM Council\". This repo is a simple, local web app that essentially looks like ChatGPT except it uses OpenRouter to send your query to multiple LLMs, it then asks them to review and rank each other's work, and finally a Chairman LLM produces the final response.\n\nIn a bit more detail, here is what happens when you submit a query:\n\n1. **Stage 1: First opinions**. The user query is given to all LLMs individually, and the responses are collected. The individual responses are shown in a \"tab view\", so that the user can inspect them all one by one.\n2. **Stage 2: Review**. Each individual LLM is given the responses of the other LLMs. Under the hood, the LLM identities are anonymized so that the LLM can't play favorites when judging their outputs. The LLM is asked to rank them in accuracy and insight.\n3. **Stage 3: Final response**. The designated Chairman of the LLM Council takes all of the model's responses and compiles them into a single final answer that is presented to the user.\n\n## Vibe Code Alert\n\nThis project was 99% vibe coded as a fun Saturday hack because I wanted to explore and evaluate a number of LLMs side by side in the process of [reading books together with LLMs](https://x.com/karpathy/status/1990577951671509438). It's nice and useful to see multiple responses side by side, and also the cross-opinions of all LLMs on each other's outputs. I'm not going to support it in any way, it's provided here as is for other people's inspiration and I don't intend to improve it. Code is ephemeral now and libraries are over, ask your LLM to change it in whatever way you like.\n\n## Setup\n\n### 1. Install Dependencies\n\nThe project uses [uv](https://docs.astral.sh/uv/) for project management.\n\n**Backend:**\n```bash\nuv sync\n```\n\n**Frontend:**\n```bash\ncd frontend\nnpm install\ncd ..\n```\n\n### 2. Configure API Key\n\nCreate a `.env` file in the project root:\n\n```bash\nOPENROUTER_API_KEY=sk-or-v1-...\n```\n\nGet your API key at [openrouter.ai](https://openrouter.ai/). Make sure to purchase the credits you need, or sign up for automatic top up.\n\n### 3. Configure Models (Optional)\n\nEdit `backend/config.py` to customize the council:\n\n```python\nCOUNCIL_MODELS = [\n    \"openai/gpt-5.1\",\n    \"google/gemini-3-pro-preview\",\n    \"anthropic/claude-sonnet-4.5\",\n    \"x-ai/grok-4\",\n]\n\nCHAIRMAN_MODEL = \"google/gemini-3-pro-preview\"\n```\n\n## Running the Application\n\n**Option 1: Use the start script**\n```bash\n./start.sh\n```\n\n**Option 2: Run manually**\n\nTerminal 1 (Backend):\n```bash\nuv run python -m backend.main\n```\n\nTerminal 2 (Frontend):\n```bash\ncd frontend\nnpm run dev\n```\n\nThen open http://localhost:5173 in your browser.\n\n## Tech Stack\n\n- **Backend:** FastAPI (Python 3.10+), async httpx, OpenRouter API\n- **Frontend:** React + Vite, react-markdown for rendering\n- **Storage:** JSON files in `data/conversations/`\n- **Package Management:** uv for Python, npm for JavaScript\n",
      "default_branch": "master"
    },
    "fetched_at": "2025-11-30T02:52:32.773833"
  },
  {
    "basic_info": {
      "name": "omnilingual-asr",
      "full_name": "facebookresearch/omnilingual-asr",
      "owner": "facebookresearch",
      "description": "Omnilingual ASR Open-Source Multilingual SpeechRecognition for 1600+ Languages",
      "url": "https://github.com/facebookresearch/omnilingual-asr",
      "clone_url": "https://github.com/facebookresearch/omnilingual-asr.git",
      "ssh_url": "git@github.com:facebookresearch/omnilingual-asr.git",
      "homepage": null,
      "created_at": "2025-11-06T22:38:00Z",
      "updated_at": "2025-11-29T21:24:48Z",
      "pushed_at": "2025-11-19T18:07:58Z"
    },
    "stats": {
      "stars": 2316,
      "forks": 188,
      "watchers": 2316,
      "open_issues": 18,
      "size": 1026
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 295799
      },
      "license": "Other",
      "topics": []
    },
    "content": {
      "readme": "<div align=\"center\">\n  <img src=\"./omniASR_header.jpg\" alt=\"Header image with a collage of on-the-ground photos from the transcription gathering efforts in Pakistan and Liberia.\" width=\"100%\" />\n  <p><i>Photographs captured during corpus creation efforts in Pakistan and Liberia.</i></p>\n</div>\n\n# Omnilingual ASR: Open-Source Multilingual Speech Recognition for 1600+ Languages\n\nOmnilingual ASR is an open-source speech recognition system supporting over 1,600 languages â€” including hundreds never previously covered by any ASR technology. Designed for broad accessibility, it enables new languages to be added with just a few paired examples without requiring specialized expertise or large datasets. By combining scalable zero-shot learning with a flexible model family, Omnilingual ASR aims to make speech technology more inclusive and adaptable for communities and researchers worldwide.\n\n* [Huggingface Demo](https://huggingface.co/spaces/facebook/omniasr-transcriptions)\n* [Huggingface Dataset](https://huggingface.co/datasets/facebook/omnilingual-asr-corpus)\n* [Paper](https://ai.meta.com/research/publications/omnilingual-asr-open-source-multilingual-speech-recognition-for-1600-languages/)\n* [Blogpost](http://ai.meta.com/blog/omnilingual-asr-advancing-automatic-speech-recognition)\n\n<div align=\"center\">\n  <img src=\"./result_table.png\" alt=\"Performance results table\" width=\"100%\" />\n  <p><i>Our 7B-LLM-ASR system achieves state-of-the-art performance across 1,600+ languages, with character error rates (CER) below 10 for 78% of those languages.</i></p>\n</div>\n\n\n## Documentation\n\n### Quick Start\n- **[Installation & Basic Usage](#installation)** - Setup and first transcription\n- **[Inference Pipeline](src/omnilingual_asr/models/inference/README.md)** - Comprehensive transcription guide with batch processing, language conditioning, and context examples\n- **[Supported Languages](#supported-languages)** - View the complete list of 1600+ supported languages\n\n\n### Models & Architecture\n- **[Model Specifications](#model-architectures)** - Available models, parameters, and memory requirements\n- **[Architecture Overview](src/omnilingual_asr/models/README.md)** - Technical details on W2V, CTC, and LLM model families\n- **[Asset Management](src/omnilingual_asr/cards/README.md)** - Configuration system for models, tokenizers, and datasets\n\n### Training & Data Pipeline\n- **[Data Preparation](workflows/dataprep/README.md)** - End-to-end guide for multilingual dataset preparation, HuggingFace integration, and parquet processing\n- **[Training Recipes](workflows/recipes/wav2vec2/asr/README.md)** - Pre-configured workflows for CTC and LLM model training\n\n---\n\n## Installation\n\nThe models were developed using [fairseq2](https://github.com/facebookresearch/fairseq2), a research-focused sequence modeling toolkit. While we provide a **reference** inference pipeline that works across platforms, audio support requires [libsndfile](https://github.com/facebookresearch/fairseq2?tab=readme-ov-file#system-dependencies) (Mac: `brew install libsndfile`; Windows may need an additional [setup](https://github.com/facebookresearch/fairseq2?tab=readme-ov-file#installing-on-windows)).\n\n```bash\n# using pip\npip install omnilingual-asr\n\n# using uv\nuv add omnilingual-asr\n```\n\n## Inference\n\n```python\nfrom omnilingual_asr.models.inference.pipeline import ASRInferencePipeline\n\npipeline = ASRInferencePipeline(model_card=\"omniASR_LLM_7B\")\n\naudio_files = [\"/path/to/eng_audio1.flac\", \"/path/to/deu_audio2.wav\"]\nlang = [\"eng_Latn\", \"deu_Latn\"]\ntranscriptions = pipeline.transcribe(audio_files, lang=lang, batch_size=2)\n```\n\nMore details on running specific models can be found in the [src/omnilingual_asr/models/inference](/src/omnilingual_asr/models/inference/README.md) directory.\n\n> **âš ï¸ Important:** Currently only audio files shorter than 40 seconds are accepted for inference. We plan to add support for transcribing unlimited-length audio files shortly.\n\n### Supported Languages\n\nTo view the full list of 1600+ supported languages, you can access the language list [programmatically](/src/omnilingual_asr/models/wav2vec2_llama/lang_ids.py):\n\n```python\nfrom omnilingual_asr.models.wav2vec2_llama.lang_ids import supported_langs\n\n# Print all supported languages\nprint(f\"Total supported languages: {len(supported_langs)}\")\nprint(supported_langs)\n\n# Check if a specific language is supported\nif \"eng_Latn\" in supported_langs:\n    print(\"English (Latin script) is supported!\")\n```\n\nLanguages follow the format `{language_code}_{script}`, for example `eng_Latn` - English (Latin script), `cmn_Hans` - Mandarin Chinese (Simplified), ...\n\n### Using the HuggingFace Dataset ğŸ¤—\n\nWe provide a large-scale multilingual speech dataset on HuggingFace under CC-BY-4.0 License: [`facebook/omnilingual-asr-corpus`](https://huggingface.co/datasets/facebook/omnilingual-asr-corpus).\nThis dataset can be directly used with our inference pipeline for evaluation or testing:\n\n```bash\npip install \"omnilingual-asr[dat",
      "default_branch": "main"
    },
    "fetched_at": "2025-11-30T02:52:33.894596"
  },
  {
    "basic_info": {
      "name": "reader3",
      "full_name": "karpathy/reader3",
      "owner": "karpathy",
      "description": "Quick illustration of how one can easily read books together with LLMs. It's great and I highly recommend it.",
      "url": "https://github.com/karpathy/reader3",
      "clone_url": "https://github.com/karpathy/reader3.git",
      "ssh_url": "git@github.com:karpathy/reader3.git",
      "homepage": null,
      "created_at": "2025-11-18T02:37:00Z",
      "updated_at": "2025-11-30T02:40:07Z",
      "pushed_at": "2025-11-18T02:37:51Z"
    },
    "stats": {
      "stars": 2298,
      "forks": 264,
      "watchers": 2298,
      "open_issues": 10,
      "size": 271
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 13925,
        "HTML": 8921
      },
      "license": null,
      "topics": []
    },
    "content": {
      "readme": "# reader 3\n\n![reader3](reader3.png)\n\nA lightweight, self-hosted EPUB reader that lets you read through EPUB books one chapter at a time. This makes it very easy to copy paste the contents of a chapter to an LLM, to read along. Basically - get epub books (e.g. [Project Gutenberg](https://www.gutenberg.org/) has many), open them up in this reader, copy paste text around to your favorite LLM, and read together and along.\n\nThis project was 90% vibe coded just to illustrate how one can very easily [read books together with LLMs](https://x.com/karpathy/status/1990577951671509438). I'm not going to support it in any way, it's provided here as is for other people's inspiration and I don't intend to improve it. Code is ephemeral now and libraries are over, ask your LLM to change it in whatever way you like.\n\n## Usage\n\nThe project uses [uv](https://docs.astral.sh/uv/). So for example, download [Dracula EPUB3](https://www.gutenberg.org/ebooks/345) to this directory as `dracula.epub`, then:\n\n```bash\nuv run reader3.py dracula.epub\n```\n\nThis creates the directory `dracula_data`, which registers the book to your local library. We can then run the server:\n\n```bash\nuv run server.py\n```\n\nAnd visit [localhost:8123](http://localhost:8123/) to see your current Library. You can easily add more books, or delete them from your library by deleting the folder. It's not supposed to be complicated or complex.\n\n## License\n\nMIT",
      "default_branch": "master"
    },
    "fetched_at": "2025-11-30T02:52:35.034039"
  },
  {
    "basic_info": {
      "name": "RedInk",
      "full_name": "HisMax/RedInk",
      "owner": "HisMax",
      "description": "çº¢å¢¨ - åŸºäºğŸŒNano Banana ProğŸŒ çš„ä¸€ç«™å¼å°çº¢ä¹¦å›¾æ–‡ç”Ÿæˆå™¨ ã€Šä¸€å¥è¯ä¸€å¼ å›¾ç‰‡ç”Ÿæˆå°çº¢ä¹¦å›¾æ–‡ã€‹ Red Ink - A one-stop Xiaohongshu image-and-text generator based on the ğŸŒNano Banana ProğŸŒ, \"One Sentence, One Image: Generate Xiaohongshu Text and Images.\"",
      "url": "https://github.com/HisMax/RedInk",
      "clone_url": "https://github.com/HisMax/RedInk.git",
      "ssh_url": "git@github.com:HisMax/RedInk.git",
      "homepage": "",
      "created_at": "2025-11-25T10:12:54Z",
      "updated_at": "2025-11-30T02:45:20Z",
      "pushed_at": "2025-11-29T19:43:23Z"
    },
    "stats": {
      "stars": 1958,
      "forks": 402,
      "watchers": 1958,
      "open_issues": 1,
      "size": 19227
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 189067,
        "Vue": 117985,
        "TypeScript": 33936,
        "CSS": 23861,
        "Dockerfile": 1568,
        "HTML": 349
      },
      "license": "Other",
      "topics": []
    },
    "content": {
      "readme": "![](images/logo.png)\n\n---\n\n[![License: CC BY-NC-SA 4.0](https://img.shields.io/badge/License-CC%20BY--NC--SA%204.0-lightgrey.svg)](https://creativecommons.org/licenses/by-nc-sa/4.0/)\n[![Python 3.11+](https://img.shields.io/badge/python-3.11+-blue.svg)](https://www.python.org/downloads/)\n[![Vue 3](https://img.shields.io/badge/vue-3.x-green.svg)](https://vuejs.org/)\n\n# çº¢å¢¨ - å°çº¢ä¹¦AIå›¾æ–‡ç”Ÿæˆå™¨\n\n> è®©ä¼ æ’­ä¸å†éœ€è¦é—¨æ§›ï¼Œè®©åˆ›ä½œä»æœªå¦‚æ­¤ç®€å•\n\n![](images/index.gif)\n\n<p align=\"center\">\n  <em>çº¢å¢¨é¦–é¡µ</em>\n</p>\n\n<p align=\"center\">\n  <img src=\"images/showcase-grid.png\" alt=\"ä½¿ç”¨çº¢å¢¨ç”Ÿæˆçš„å„ç±»å°çº¢ä¹¦å°é¢\" width=\"600\"/>\n</p>\n\n<p align=\"center\">\n  <em>ä½¿ç”¨çº¢å¢¨ç”Ÿæˆçš„å„ç±»å°çº¢ä¹¦å°é¢ - AIé©±åŠ¨ï¼Œé£æ ¼ç»Ÿä¸€ï¼Œæ–‡å­—å‡†ç¡®</em>\n</p>\n\n\n\n## å†™åœ¨å‰é¢\n\nå‰æ®µæ—¶é—´é»˜å­åœ¨ Linux.do å‘äº†ä¸€ä¸ªç”¨ Nano banana Pro åš PPT çš„å¸–å­,æ”¶è·äº† 600 å¤šä¸ªèµã€‚å¾ˆå¤šäººç”¨ğŸŒNano banana Pro å»åšäº§å“å®£ä¼ å›¾ã€ç›´æ¥ç”Ÿæˆæ¼«ç”»ç­‰ç­‰ã€‚æˆ‘å°±åœ¨æƒ³:**ä¸ºä»€ä¹ˆä¸æ‹¿ğŸŒ2æ¥åšç‚¹æ›´åŠŸåˆ©ã€æ›´åˆºæ¿€çš„äº‹æƒ…?**\n\näºæ˜¯å°±æœ‰äº†è¿™ä¸ªé¡¹ç›®ã€‚ä¸€å¥è¯ä¸€å¼ å›¾ç‰‡ç”Ÿæˆå°çº¢ä¹¦å›¾æ–‡\n\n---\n\n## âœ¨ æ•ˆæœå±•ç¤º\n\n### è¾“å…¥ä¸€å¥è¯,å°±èƒ½ç”Ÿæˆå®Œæ•´çš„å°çº¢ä¹¦å›¾æ–‡\n\n#### æç¤ºè¯ï¼šç§‹å­£æ˜¾ç™½ç¾ç”²ï¼ˆæš—å¹¿ä¸€ä¸ªï¼šé»˜å­ç‰Œç¾ç”²ï¼‰ï¼Œå›¾ç‰‡ æ˜¯æˆ‘çš„å°çº¢ä¹¦ä¸»é¡µã€‚ç¬¦åˆæˆ‘çš„é£æ ¼ç”Ÿæˆ\n\n#### åŒæ—¶æˆ‘è¿˜æˆªå›¾äº†æˆ‘çš„å°çº¢ä¹¦ä¸»é¡µï¼ŒåŒ…æ‹¬æˆ‘çš„å¤´åƒï¼Œç­¾åï¼ŒèƒŒæ™¯ï¼Œå§“åä»€ä¹ˆçš„\n\n![ç¤ºä¾‹1](./images/example-1.png)\n\n#### ç„¶åç­‰å¾…10-20ç§’åï¼Œå°±ä¼šæœ‰æ¯ä¸€é¡µçš„å¤§çº²ï¼Œå¤§å®¶å¯ä»¥æ ¹æ®çš„è‡ªå·±çš„éœ€æ±‚å»è°ƒæ•´é¡µé¢é¡ºåºï¼ˆä¸å»ºè®®ï¼‰ï¼Œè‡ªå®šä¹‰æ¯ä¸€ä¸ªé¡µé¢çš„å†…å®¹ï¼ˆè¿™ä¸ªå¾ˆå»ºè®®ï¼‰\n\n![ç¤ºä¾‹2](./images/example-2.png)\n\n#### é¦–å…ˆç”Ÿæˆçš„æ˜¯å°é¢é¡µ\n\n![ç¤ºä¾‹3](./images/example-3.png)\n\n#### ç„¶åç¨ç­‰ä¸€ä¼šå„¿åï¼Œä¼šç”Ÿæˆåé¢çš„æ‰€æœ‰é¡µé¢ï¼ˆè¿™é‡Œæ˜¯å¹¶å‘ç”Ÿæˆçš„æ‰€æœ‰é¡µé¢ï¼ˆé»˜è®¤æ˜¯15ä¸ªï¼‰ï¼Œå¦‚æœå¤§å®¶çš„APIä¾›åº”å•†æ— æ³•æ”¯æŒé«˜å¹¶å‘çš„è¯ï¼Œè®°å¾—è¦å»æ”¹ä¸€ä¸‹è®¾ç½®ï¼‰\n\n![ç¤ºä¾‹4](./images/example-4.png)\n\n---\n\n## ğŸ—ï¸ æŠ€æœ¯æ¶æ„\n\n### åç«¯\n- **è¯­è¨€**: Python 3.11+\n- **æ¡†æ¶**: Flask\n- **AI æ¨¡å‹**:\n  - Gemini 3 (æ–‡æ¡ˆç”Ÿæˆ)\n  - ğŸŒNano banana Pro (å›¾ç‰‡ç”Ÿæˆ)\n- **åŒ…ç®¡ç†**: uv\n\n### å‰ç«¯\n- **æ¡†æ¶**: Vue 3 + TypeScript\n- **æ„å»º**: Vite\n- **çŠ¶æ€ç®¡ç†**: Pinia\n\n---\n\n## ğŸ“¦ å¦‚ä½•è‡ªå·±éƒ¨ç½²\n\n### æ–¹å¼ä¸€ï¼šDocker éƒ¨ç½²ï¼ˆæ¨èï¼‰\n\n**æœ€ç®€å•çš„éƒ¨ç½²æ–¹å¼ï¼Œä¸€è¡Œå‘½ä»¤å³å¯å¯åŠ¨ï¼š**\n\n```bash\ndocker run -d -p 12398:12398 -v ./history:/app/history -v ./output:/app/output histonemax/redink:latest\n```\n\nè®¿é—® http://localhost:12398ï¼Œåœ¨ Web ç•Œé¢çš„**è®¾ç½®é¡µé¢**é…ç½®ä½ çš„ API Key å³å¯ä½¿ç”¨ã€‚\n\n**ä½¿ç”¨ docker-composeï¼ˆå¯é€‰ï¼‰ï¼š**\n\nä¸‹è½½ [docker-compose.yml](https://github.com/HisMax/RedInk/blob/main/docker-compose.yml) åï¼š\n\n```bash\ndocker-compose up -d\n```\n\n**Docker éƒ¨ç½²è¯´æ˜ï¼š**\n- å®¹å™¨å†…ä¸åŒ…å«ä»»ä½• API Keyï¼Œéœ€è¦åœ¨ Web ç•Œé¢é…ç½®\n- ä½¿ç”¨ `-v ./history:/app/history` æŒä¹…åŒ–å†å²è®°å½•\n- ä½¿ç”¨ `-v ./output:/app/output` æŒä¹…åŒ–ç”Ÿæˆçš„å›¾ç‰‡\n- å¯é€‰ï¼šæŒ‚è½½è‡ªå®šä¹‰é…ç½®æ–‡ä»¶ `-v ./text_providers.yaml:/app/text_providers.yaml`\n\n---\n\n### æ–¹å¼äºŒï¼šæœ¬åœ°å¼€å‘éƒ¨ç½²\n\n**å‰ç½®è¦æ±‚ï¼š**\n- Python 3.11+\n- Node.js 18+\n- pnpm\n- uv\n\n### 1. å…‹éš†é¡¹ç›®\n```bash\ngit clone https://github.com/HisMax/RedInk.git\ncd RedInk\n```\n\n### 2. é…ç½® API æœåŠ¡\n\nå¤åˆ¶é…ç½®æ¨¡æ¿æ–‡ä»¶ï¼š\n```bash\ncp text_providers.yaml.example text_providers.yaml\ncp image_providers.yaml.example image_providers.yaml\n```\n\nç¼–è¾‘é…ç½®æ–‡ä»¶ï¼Œå¡«å…¥ä½ çš„ API Key å’ŒæœåŠ¡é…ç½®ã€‚ä¹Ÿå¯ä»¥å¯åŠ¨ååœ¨ Web ç•Œé¢çš„**è®¾ç½®é¡µé¢**è¿›è¡Œé…ç½®ã€‚\n\n### 3. å®‰è£…åç«¯ä¾èµ–\n```bash\nuv sync\n```\n\n### 4. å®‰è£…å‰ç«¯ä¾èµ–\n```bash\ncd frontend\npnpm install\n```\n\n### 5. å¯åŠ¨æœåŠ¡\n\n**å¯åŠ¨åç«¯:**\n```bash\nuv run python -m backend.app\n```\nè®¿é—®: http://localhost:12398\n\n**å¯åŠ¨å‰ç«¯:**\n```bash\ncd frontend\npnpm dev\n```\nè®¿é—®: http://localhost:5173\n\n---\n\n## ğŸ® ä½¿ç”¨æŒ‡å—\n\n### åŸºç¡€ä½¿ç”¨\n1. **è¾“å…¥ä¸»é¢˜**: åœ¨é¦–é¡µè¾“å…¥æƒ³è¦åˆ›ä½œçš„ä¸»é¢˜,å¦‚\"å¦‚ä½•åœ¨å®¶åšæ‹¿é“\"\n2. **ç”Ÿæˆå¤§çº²**: AI è‡ªåŠ¨ç”Ÿæˆ 6-9 é¡µçš„å†…å®¹å¤§çº²\n3. **ç¼–è¾‘ç¡®è®¤**: å¯ä»¥ç¼–è¾‘å’Œè°ƒæ•´æ¯ä¸€é¡µçš„æè¿°\n4. **ç”Ÿæˆå›¾ç‰‡**: ç‚¹å‡»ç”Ÿæˆ,å®æ—¶æŸ¥çœ‹è¿›åº¦\n5. **ä¸‹è½½ä½¿ç”¨**: ä¸€é”®ä¸‹è½½æ‰€æœ‰å›¾ç‰‡\n\n### è¿›é˜¶ä½¿ç”¨\n- **ä¸Šä¼ å‚è€ƒå›¾ç‰‡**: é€‚åˆå“ç‰Œæ–¹,ä¿æŒå“ç‰Œè§†è§‰é£æ ¼\n- **ä¿®æ”¹æè¿°è¯**: ç²¾ç¡®æ§åˆ¶æ¯ä¸€é¡µçš„å†…å®¹å’Œæ„å›¾\n- **é‡æ–°ç”Ÿæˆ**: å¯¹ä¸æ»¡æ„çš„é¡µé¢å•ç‹¬é‡æ–°ç”Ÿæˆ\n\n---\n\n## ğŸ”§ é…ç½®è¯´æ˜\n\n### é…ç½®æ–¹å¼\n\né¡¹ç›®æ”¯æŒä¸¤ç§é…ç½®æ–¹å¼ï¼š\n\n1. **Web ç•Œé¢é…ç½®ï¼ˆæ¨èï¼‰**ï¼šå¯åŠ¨æœåŠ¡åï¼Œåœ¨è®¾ç½®é¡µé¢å¯è§†åŒ–é…ç½®\n2. **YAML æ–‡ä»¶é…ç½®**ï¼šç›´æ¥ç¼–è¾‘é…ç½®æ–‡ä»¶\n\n### æ–‡æœ¬ç”Ÿæˆé…ç½®\n\né…ç½®æ–‡ä»¶: `text_providers.yaml`\n\n```yaml\n# å½“å‰æ¿€æ´»çš„æœåŠ¡å•†\nactive_provider: openai\n\nproviders:\n  # OpenAI å®˜æ–¹æˆ–å…¼å®¹æ¥å£\n  openai:\n    type: openai_compatible\n    api_key: sk-xxxxxxxxxxxxxxxxxxxx\n    base_url: https://api.openai.com/v1\n    model: gpt-4o\n\n  # Google Geminiï¼ˆåŸç”Ÿæ¥å£ï¼‰\n  gemini:\n    type: google_gemini\n    api_key: AIzaxxxxxxxxxxxxxxxxxxxxxxxxx\n    model: gemini-2.0-flash\n```\n\n### å›¾ç‰‡ç”Ÿæˆé…ç½®\n\né…ç½®æ–‡ä»¶: `image_providers.yaml`\n\n```yaml\n# å½“å‰æ¿€æ´»çš„æœåŠ¡å•†\nactive_provider: gemini\n\nproviders:\n  # Google Gemini å›¾ç‰‡ç”Ÿæˆ\n  gemini:\n    type: google_genai\n    api_key: AIzaxxxxxxxxxxxxxxxxxxxxxxxxx\n    model: gemini-3-pro-image-preview\n    high_concurrency: false  # é«˜å¹¶å‘æ¨¡å¼\n\n  # OpenAI å…¼å®¹æ¥å£\n  openai_image:\n    type: image_api\n    api_key: sk-xxxxxxxxxxxxxxxxxxxx\n    base_url: https://your-api-endpoint.com\n    model: dall-e-3\n    high_concurrency: false\n```\n\n### é«˜å¹¶å‘æ¨¡å¼è¯´æ˜\n\n- **å…³é—­ï¼ˆé»˜è®¤ï¼‰**ï¼šå›¾ç‰‡é€å¼ ç”Ÿæˆï¼Œé€‚åˆ GCP 300$ è¯•ç”¨è´¦å·æˆ–æœ‰é€Ÿç‡é™åˆ¶çš„ API\n- **å¼€å¯**ï¼šå›¾ç‰‡å¹¶è¡Œç”Ÿæˆï¼ˆæœ€å¤š15å¼ åŒæ—¶ï¼‰ï¼Œé€Ÿåº¦æ›´å¿«ï¼Œä½†éœ€è¦ API æ”¯æŒé«˜å¹¶å‘\n\nâš ï¸ **GCP 300$ è¯•ç”¨è´¦å·ä¸å»ºè®®å¯ç”¨é«˜å¹¶å‘**ï¼Œå¯èƒ½ä¼šè§¦å‘é€Ÿç‡é™åˆ¶å¯¼è‡´ç”Ÿæˆå¤±è´¥ã€‚\n\n---\n\n## âš ï¸ æ³¨æ„äº‹é¡¹\n\n1. **API é…é¢é™åˆ¶**:\n   - æ³¨æ„ Gemini å’Œå›¾ç‰‡ç”Ÿæˆ API çš„è°ƒç”¨é…é¢\n   - GCP è¯•ç”¨è´¦å·å»ºè®®å…³é—­é«˜å¹¶å‘æ¨¡å¼\n\n2. **ç”Ÿæˆæ—¶é—´**:\n   - å›¾ç‰‡ç”Ÿæˆéœ€è¦æ—¶é—´,è¯·è€å¿ƒç­‰å¾…ï¼ˆä¸è¦ç¦»å¼€é¡µé¢ï¼‰\n\n---\n\n## ğŸ¤ å‚ä¸è´¡çŒ®\n\næ¬¢è¿æäº¤ Issue å’Œ Pull Request!\n\nå¦‚æœè¿™ä¸ªé¡¹ç›®å¯¹ä½ æœ‰å¸®åŠ©,æ¬¢è¿ç»™ä¸ª Star â­\n\n### æœªæ¥è®¡åˆ’\n- [ ] æ”¯æŒæ›´å¤šå›¾ç‰‡æ ¼å¼ï¼Œä¾‹å¦‚ä¸€å¥è¯ç”Ÿæˆä¸€å¥—PPTä»€ä¹ˆçš„\n- [x] å†å²è®°å½•ç®¡ç†ä¼˜åŒ–\n- [ ] å¯¼å‡ºä¸ºå„ç§æ ¼å¼(PDFã€é•¿å›¾ç­‰)\n\n---\n\n## æ›´æ–°æ—¥å¿—\n\n### v1.4.0 (2025-11-30)\n- ğŸ—ï¸ åç«¯æ¶æ„é‡æ„ï¼šæ‹†åˆ†å•ä½“è·¯ç”±ä¸ºæ¨¡å—åŒ–è“å›¾ï¼ˆhistoryã€imagesã€generationã€outlineã€configï¼‰\n- ğŸ—ï¸ å‰ç«¯ç»„ä»¶é‡æ„ï¼šæå–å¯å¤ç”¨ç»„ä»¶ï¼ˆImageGalleryModalã€OutlineModalã€ShowcaseBackgroundç­‰ï¼‰\n- âœ¨ ä¼˜åŒ–é¦–é¡µè®¾è®¡ï¼Œç§»é™¤å†—ä½™å†…å®¹åŒºå—\n- âœ¨ èƒŒæ™¯å›¾ç‰‡é¢„åŠ è½½å’Œæ¸å…¥åŠ¨ç”»ï¼Œæå‡åŠ è½½ä½“éªŒ\n- âœ¨ å†å²è®°å½•æŒä¹…åŒ–æ”¯æŒï¼ˆDockeréƒ¨ç½²ï¼‰\n- ğŸ”§ ä¿®å¤å†å²è®°å½•é¢„è§ˆå’Œå¤§çº²æŸ¥çœ‹åŠŸèƒ½\n- ğŸ”§ ä¼˜åŒ–Modalç»„ä»¶å¯è§æ€§æ§åˆ¶\n- ğŸ§ª æ–°å¢65ä¸ªåç«¯å•å…ƒæµ‹è¯•\n\n### v1.3.0 (2025-11-26)\n- âœ¨ æ–°å¢ Docker æ”¯æŒï¼Œä¸€é”®éƒ¨ç½²\n- âœ¨ å‘å¸ƒå®˜æ–¹ Docker é•œåƒåˆ° Docker Hub: `histonemax/redink`\n- ğŸ”§ Flask è‡ªåŠ¨æ£€æµ‹å‰ç«¯æ„å»ºäº§ç‰©ï¼Œæ”¯æŒå•å®¹å™¨éƒ¨ç½²\n- ğŸ”§ Docker é•œåƒå†…ç½®ç©ºç™½é…ç½®æ¨¡æ¿ï¼Œä¿æŠ¤ API Key å®‰å…¨\n- ğŸ“ æ›´æ–° READMEï¼Œæ·»åŠ  Docker éƒ¨ç½²è¯´æ˜\n\n### v1.2.0 (2025-11-26)\n- âœ¨ æ–°å¢ç‰ˆæƒä¿¡æ¯å±•ç¤ºï¼Œæ‰€æœ‰é¡µé¢æ˜¾ç¤ºå¼€æºåè®®å’Œé¡¹ç›®é“¾æ¥\n- âœ¨ ä¼˜åŒ–å›¾ç‰‡é‡æ–°ç”ŸæˆåŠŸèƒ½ï¼Œæ”¯æŒå•å¼ å›¾ç‰‡é‡ç»˜\n- âœ¨ é‡æ–°ç”Ÿæˆå›¾ç‰‡æ—¶ä¿æŒé£æ ¼ä¸€è‡´ï¼Œä¼ é€’å®Œæ•´ä¸Šä¸‹æ–‡ï¼ˆå°é¢å›¾ã€å¤§çº²ã€ç”¨æˆ·è¾“å…¥ï¼‰\n- âœ¨ ä¿®å¤å›¾ç‰‡ç¼“å­˜é—®é¢˜ï¼Œé‡æ–°ç”Ÿæˆçš„å›¾ç‰‡ç«‹å³åˆ·æ–°æ˜¾ç¤º\n- âœ¨ ç»Ÿä¸€æ–‡æœ¬ç”Ÿæˆå®¢æˆ·ç«¯æ¥å£ï¼Œæ”¯æŒ Google Gemini å’Œ OpenAI å…¼å®¹æ¥å£è‡ªåŠ¨åˆ‡æ¢\n- âœ¨ æ–°å¢ Web ç•Œé¢é…ç½®åŠŸèƒ½ï¼Œå¯è§†åŒ–ç®¡ç† API æœåŠ¡å•†\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-11-30T02:52:36.148426"
  },
  {
    "basic_info": {
      "name": "xiaomi-miloco",
      "full_name": "XiaoMi/xiaomi-miloco",
      "owner": "XiaoMi",
      "description": "Xiaomi Miloco",
      "url": "https://github.com/XiaoMi/xiaomi-miloco",
      "clone_url": "https://github.com/XiaoMi/xiaomi-miloco.git",
      "ssh_url": "git@github.com:XiaoMi/xiaomi-miloco.git",
      "homepage": null,
      "created_at": "2025-11-06T13:01:59Z",
      "updated_at": "2025-11-30T01:56:14Z",
      "pushed_at": "2025-11-28T13:14:55Z"
    },
    "stats": {
      "stars": 1458,
      "forks": 92,
      "watchers": 1458,
      "open_issues": 66,
      "size": 23989
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 1050918,
        "JavaScript": 506494,
        "Shell": 90272,
        "C++": 83167,
        "Less": 71704,
        "HTML": 53752,
        "CSS": 9737,
        "Dockerfile": 5505,
        "C": 2948,
        "CMake": 1813
      },
      "license": "Other",
      "topics": []
    },
    "content": {
      "readme": "# Xiaomi Miloco\n\n**Xiaomi Local Copilot** is a future exploration solution for smart homes. Using Xiaomi Home cameras as the source of visual information and a self-developed LLM as its core, it connects all IoT devices throughout the house. Based on the development paradigm of LLM, it enables users to define various family needs and rules in natural language, achieving broader and more creative smart device integration.\n\n<div align=\"center\">\n\nEnglish | [ç®€ä½“ä¸­æ–‡](README.zh_Hans.md)\n\n</div>\n\n## News\n\n- [2025-11] Xiaomi Miloco Framework Open Source\n\n## Key Features\n\n1. New Interaction Paradigm: Based on the development paradigm of LLM, rule-setting and complex device command control can be completed through natural language interaction.\n2. New Use for Visual Data: Using camera data streams as a source of perceptual information, the LLM is used to analyze various home scene events contained in the visual data to respond to user queries.\n3. On-Device LLM: The home scene tasks are split into two stages: planning and visual understanding. It provides Xiaomi's self-developed on-device model to realize on-device video understanding and ensure family privacy and security.\n4. Xiaomi Home Ecosystem: It connects with the Xiaomi Home ecosystem, supports the retrieval and execution of Mi Home devices and scenes, and supports sending customized content for Xiao Home notifications.\n\n    <img src=\"assets/images/ai_center.jpg\" width=\"60%\" />\n\n## Quick Start\n\n### System Requirements\n\n- **Hardware Requirements**\n```Plain Text\nCPU: x64 architecture\nGraphics Card: NVIDIA 30 series and above, 8GB VRAM minimum (recommended 12GB and above)\nStorage: Recommended 16GB or more available space (for local model storage)\n```\n\n- **Software Requirements**\n```Plain Text\nOperating System:\n  - Linux: x64 architecture, recommended Ubuntu 22.04 and above LTS versions\n  - Windows: x64 architecture, recommended Windows 10 and above, requires WSL2 support\n  - macOS: Not currently supported\nDocker: Version 20.10 and above, requires docker compose support\nNVIDIA Driver: NVIDIA driver with CUDA support\nNVIDIA Container Toolkit: For Docker GPU support\n```\n\n### Install\n\n> **Note**: Please ensure your system meets the above hardware and software requirements. Windows systems need to enter the WSL environment.\n\n**Install with Docker**  \nOne-click installation via command line\n```bash\nbash -c \"$(wget -qO- https://xiaomi-miloco.cnbj1.mi-fds.com/xiaomi-miloco/install.sh)\"\n```\nOr download the source code first, then execute the one-click installation script:\n```bash\ngit clone https://github.com/XiaoMi/xiaomi-miloco.git\n\nbash scripts/install.sh\n```\nFor detailed installation steps, please refer to the [Docker Deployment Documentation](docs/environment-setup.md).\n\n**Install with source code**  \nFor source code installation steps, please refer to the [Development Guide](docs/development/developer-setup.md).\n\n## Usage Documentation\n\nPlease refer to the [Usage Documentation](docs/usage/README.md).\n\n## Contributing\n\nPlease refer to the [Contributing Guide](CONTRIBUTING.md).\n\n## License\n\nFor license details, please see [LICENSE.md](LICENSE.md).\n\n**Important Notice**: This project is limited to non-commercial use only. Without written authorization from Xiaomi Corporation, this project may not be used for developing applications, web services, or other forms of software.\n\n## Security Issues\n\nIf you discover potential security issues in this project, or believe you may have found a security issue, please notify the [Miloco Team](xiaomi-miloco@xiaomi.com) via our vulnerability reporting email. Please do not create public GitHub Issues.\n\n## Contact Us\n\n### Issue Reporting\n\nFor issue reporting, please participate through the following methods:\n- Submit a [GitHub Issue](https://github.com/XiaoMi/xiaomi-miloco/issues/new/)\n\n### Technical Discussion\n\n- GitHub [Discussions](https://github.com/XiaoMi/xiaomi-miloco/discussions/)\n- Project Discussion Group (WeChat):\n\n  <img src=\"assets/images/miloco_wechat_group_17.jpeg\" width=\"30%\" />\n  <img src=\"assets/images/miloco_wechat_15.jpeg\" width=\"30%\" />\n  <img src=\"assets/images/miloco_wechat_group_12.jpeg\" width=\"30%\" />\n\n\n\n### Join Us\n\nThe **Xiaomi Miloco** team is hiring. Send your resume to `xiaomi-miloco@xiaomi.com`, and it will be delivered directly to the project lead.\n\n## Acknowledgments\n\nThank you to the original team members who worked hard for Milocoï¼šzhaoyã€yangyongjieã€xxã€Changyuã€yykã€junhuiã€éƒ­å…´å®ã€47ã€afeiã€‚\n\nYour passion and talent are the fundamental driving force behind Miloco's continuous innovation and progress.\n\nSpecial thanks to:\n- The [llama.cpp](https://github.com/ggml-org/llama.cpp) open source project for providing inference backend capabilities\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-11-30T02:52:37.254494"
  },
  {
    "basic_info": {
      "name": "JiT",
      "full_name": "LTH14/JiT",
      "owner": "LTH14",
      "description": "PyTorch implementation of JiT https://arxiv.org/abs/2511.13720",
      "url": "https://github.com/LTH14/JiT",
      "clone_url": "https://github.com/LTH14/JiT.git",
      "ssh_url": "git@github.com:LTH14/JiT.git",
      "homepage": "",
      "created_at": "2025-11-10T22:37:40Z",
      "updated_at": "2025-11-29T16:48:08Z",
      "pushed_at": "2025-11-18T03:24:51Z"
    },
    "stats": {
      "stars": 1443,
      "forks": 68,
      "watchers": 1443,
      "open_issues": 18,
      "size": 67601
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 56577
      },
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "## Just image Transformer (JiT) for Pixel-space Diffusion\n\n[![arXiv](https://img.shields.io/badge/arXiv%20paper-2511.13720-b31b1b.svg)](https://arxiv.org/abs/2511.13720)&nbsp;\n\n<p align=\"center\">\n  <img src=\"demo/visual.jpg\" width=\"100%\">\n</p>\n\n\nThis is a PyTorch/GPU re-implementation of the paper [Back to Basics: Let Denoising Generative Models Denoise](https://arxiv.org/abs/2511.13720):\n\n```\n@article{li2025jit,\n  title={Back to Basics: Let Denoising Generative Models Denoise},\n  author={Li, Tianhong and He, Kaiming},\n  journal={arXiv preprint arXiv:2511.13720},\n  year={2025}\n}\n```\n\nJiT adopts a minimalist and self-contained design for pixel-level high-resolution image diffusion. \nThe original implementation was in JAX+TPU. This re-implementation is in PyTorch+GPU.\n\n<p align=\"center\">\n  <img src=\"demo/jit.jpg\" width=\"40%\">\n</p>\n\n### Dataset\nDownload [ImageNet](http://image-net.org/download) dataset, and place it in your `IMAGENET_PATH`.\n\n### Installation\n\nDownload the code:\n```\ngit clone https://github.com/LTH14/JiT.git\ncd JiT\n```\n\nA suitable [conda](https://conda.io/) environment named `jit` can be created and activated with:\n\n```\nconda env create -f environment.yaml\nconda activate jit\n```\n\nIf you get ```undefined symbol: iJIT_NotifyEvent``` when importing ```torch```, simply\n```\npip uninstall torch\npip install torch==2.5.1 --index-url https://download.pytorch.org/whl/cu124\n```\nCheck this [issue](https://github.com/conda/conda/issues/13812#issuecomment-2071445372) for more details.\n\n### Training\nThe below training scripts have been tested on 8 H200 GPUs.\n\nExample script for training JiT-B/16 on ImageNet 256x256 for 600 epochs:\n```\ntorchrun --nproc_per_node=8 --nnodes=1 --node_rank=0 \\\nmain_jit.py \\\n--model JiT-B/16 \\\n--proj_dropout 0.0 \\\n--P_mean -0.8 --P_std 0.8 \\\n--img_size 256 --noise_scale 1.0 \\\n--batch_size 128 --blr 5e-5 \\\n--epochs 600 --warmup_epochs 5 \\\n--gen_bsz 128 --num_images 50000 --cfg 2.9 --interval_min 0.1 --interval_max 1.0 \\\n--output_dir ${OUTPUT_DIR} --resume ${OUTPUT_DIR} \\\n--data_path ${IMAGENET_PATH} --online_eval\n```\n\nExample script for training JiT-B/32 on ImageNet 512x512 for 600 epochs:\n```\ntorchrun --nproc_per_node=8 --nnodes=1 --node_rank=0 \\\nmain_jit.py \\\n--model JiT-B/32 \\\n--proj_dropout 0.0 \\\n--P_mean -0.8 --P_std 0.8 \\\n--img_size 512 --noise_scale 2.0 \\\n--batch_size 128 --blr 5e-5 \\\n--epochs 600 --warmup_epochs 5 \\\n--gen_bsz 128 --num_images 50000 --cfg 2.9 --interval_min 0.1 --interval_max 1.0 \\\n--output_dir ${OUTPUT_DIR} --resume ${OUTPUT_DIR} \\\n--data_path ${IMAGENET_PATH} --online_eval\n```\n\nExample script for training JiT-H/16 on ImageNet 256x256 for 600 epochs:\n```\ntorchrun --nproc_per_node=8 --nnodes=1 --node_rank=0 \\\nmain_jit.py \\\n--model JiT-H/16 \\\n--proj_dropout 0.2 \\\n--P_mean -0.8 --P_std 0.8 \\\n--img_size 256 --noise_scale 1.0 \\\n--batch_size 128 --blr 5e-5 \\\n--epochs 600 --warmup_epochs 5 \\\n--gen_bsz 128 --num_images 50000 --cfg 2.2 --interval_min 0.1 --interval_max 1.0 \\\n--output_dir ${OUTPUT_DIR} --resume ${OUTPUT_DIR} \\\n--data_path ${IMAGENET_PATH} --online_eval\n```\n\n### Evaluation\n\nEvaluate a trained JiT:\n```\ntorchrun --nproc_per_node=8 --nnodes=1 --node_rank=0 \\\nmain_jit.py \\\n--model JiT-B/16 \\\n--img_size 256 --noise_scale 1.0 \\\n--gen_bsz 128 --num_images 50000 --cfg 2.9 --interval_min 0.1 --interval_max 1.0 \\\n--output_dir ${CKPT_DIR} --resume ${CKPT_DIR} \\\n--data_path ${IMAGENET_PATH} --evaluate_gen\n```\n\nWe use a customized [```torch-fidelity```](https://github.com/LTH14/torch-fidelity)\nto evaluate FID and IS against a reference image folder or statistics. You can use ```prepare_ref.py```\nto prepare the reference image folder, or directly use our pre-computed reference stats\nunder ```fid_stats```.\n\n### Acknowledgements\n\nWe thank Google TPU Research Cloud (TRC) for granting us access to TPUs, and the MIT\nORCD Seed Fund Grants for supporting GPU resources.\n\n### Contact\n\nIf you have any questions, feel free to contact me through email (tianhong@mit.edu). Enjoy!\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-11-30T02:52:38.358130"
  },
  {
    "basic_info": {
      "name": "crypto-trading-open",
      "full_name": "cryptocj520/crypto-trading-open",
      "owner": "cryptocj520",
      "description": "crypto-trading-open",
      "url": "https://github.com/cryptocj520/crypto-trading-open",
      "clone_url": "https://github.com/cryptocj520/crypto-trading-open.git",
      "ssh_url": "git@github.com:cryptocj520/crypto-trading-open.git",
      "homepage": null,
      "created_at": "2025-11-11T12:00:02Z",
      "updated_at": "2025-11-29T20:10:34Z",
      "pushed_at": "2025-11-11T12:03:28Z"
    },
    "stats": {
      "stars": 1276,
      "forks": 688,
      "watchers": 1276,
      "open_issues": 13,
      "size": 997
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 2825921,
        "Shell": 37998
      },
      "license": null,
      "topics": []
    },
    "content": {
      "readme": "# å¤šäº¤æ˜“æ‰€ç­–ç•¥è‡ªåŠ¨åŒ–ç³»ç»Ÿ\n\n**Multi-Exchange Strategy Automation System**\n\n## ğŸ¯ é¡¹ç›®ç®€ä»‹\n\nè¿™æ˜¯ä¸€ä¸ªä¼ä¸šçº§çš„å¤šäº¤æ˜“æ‰€åŠ å¯†è´§å¸è‡ªåŠ¨åŒ–äº¤æ˜“ç³»ç»Ÿï¼Œæä¾›é«˜æ€§èƒ½ã€é«˜å¯é æ€§çš„ç½‘æ ¼äº¤æ˜“ã€åˆ·é‡äº¤æ˜“ã€å¥—åˆ©ç›‘æ§å’Œå¸‚åœºç›‘æ§åŠŸèƒ½ã€‚ç³»ç»Ÿé‡‡ç”¨ä¸¥æ ¼çš„åˆ†å±‚æ¶æ„è®¾è®¡ï¼Œæ”¯æŒ Hyperliquidã€Backpackã€Lighterã€Binanceã€OKXã€EdgeX ç­‰å¤šä¸ªäº¤æ˜“æ‰€çš„å®Œæ•´é€‚é…ã€‚\n\n## ğŸ—ï¸ æ ¸å¿ƒç³»ç»Ÿæ¶æ„\n\n### ç³»ç»Ÿç»„ä»¶\n\n```\nå¤šäº¤æ˜“æ‰€ç­–ç•¥è‡ªåŠ¨åŒ–ç³»ç»Ÿ\nâ”œâ”€â”€ ğŸ“Š ç½‘æ ¼äº¤æ˜“ç³»ç»Ÿ (Grid Trading)\nâ”‚   â”œâ”€â”€ æ™®é€šç½‘æ ¼              # å›ºå®šä»·æ ¼åŒºé—´ç½‘æ ¼\nâ”‚   â”œâ”€â”€ é©¬ä¸ç½‘æ ¼              # é©¬ä¸æ ¼å°”é€’å¢ç­–ç•¥\nâ”‚   â”œâ”€â”€ ä»·æ ¼ç§»åŠ¨ç½‘æ ¼          # åŠ¨æ€è·Ÿéšä»·æ ¼\nâ”‚   â”œâ”€â”€ å‰¥å¤´çš®æ¨¡å¼            # å¿«é€Ÿæ­¢æŸç­–ç•¥\nâ”‚   â”œâ”€â”€ æ™ºèƒ½å‰¥å¤´çš®            # å¤šæ¬¡æ·±è·Œæ£€æµ‹\nâ”‚   â”œâ”€â”€ æœ¬é‡‘ä¿æŠ¤æ¨¡å¼          # è‡ªåŠ¨æ­¢æŸä¿æŠ¤\nâ”‚   â”œâ”€â”€ æ­¢ç›ˆæ¨¡å¼              # åˆ°è¾¾ç›®æ ‡è‡ªåŠ¨å¹³ä»“\nâ”‚   â””â”€â”€ ç°è´§é¢„ç•™ç®¡ç†          # ç°è´§å¸ç§é¢„ç•™\nâ”œâ”€â”€ ğŸ” ç½‘æ ¼æ³¢åŠ¨ç‡æ‰«æå™¨ (Grid Volatility Scanner)\nâ”‚   â”œâ”€â”€ è™šæ‹Ÿç½‘æ ¼æ¨¡æ‹Ÿ          # æ— éœ€å®é™…ä¸‹å•çš„æ¨¡æ‹Ÿç½‘æ ¼\nâ”‚   â”œâ”€â”€ å®æ—¶APRè®¡ç®—           # å‡†ç¡®é¢„æµ‹å¹´åŒ–æ”¶ç›Šç‡\nâ”‚   â”œâ”€â”€ ä»£å¸æ’è¡Œæ¦œ            # æŒ‰æ³¢åŠ¨ç‡å’ŒAPRæ’åº\nâ”‚   â”œâ”€â”€ æ™ºèƒ½è¯„çº§ç³»ç»Ÿ          # S/A/B/C/Dç­‰çº§è¯„ä¼°\nâ”‚   â””â”€â”€ ç»ˆç«¯ UI              # Rich å®æ—¶ç›‘æ§ç•Œé¢\nâ”œâ”€â”€ ğŸ’¹ åˆ·é‡äº¤æ˜“ç³»ç»Ÿ (Volume Maker)\nâ”‚   â”œâ”€â”€ æŒ‚å•æ¨¡å¼              # é™ä»·å•åˆ·é‡ï¼ˆBackpackï¼‰\nâ”‚   â””â”€â”€ å¸‚ä»·æ¨¡å¼              # å¸‚ä»·å•å¿«é€Ÿåˆ·é‡ï¼ˆLighterï¼‰\nâ”œâ”€â”€ ğŸ”„ å¥—åˆ©ç›‘æ§ç³»ç»Ÿ (Arbitrage Monitor)\nâ”‚   â”œâ”€â”€ ä»·æ ¼ç›‘æ§              # å®æ—¶ä»·æ ¼å·®ç›‘æ§\nâ”‚   â”œâ”€â”€ èµ„é‡‘è´¹ç‡ç›‘æ§          # è·¨äº¤æ˜“æ‰€è´¹ç‡å·®å¼‚\nâ”‚   â”œâ”€â”€ å¥—åˆ©æœºä¼šè¯†åˆ«          # ä»·å·®å’Œè´¹ç‡å¥—åˆ©\nâ”‚   â”œâ”€â”€ ç»ˆç«¯ UI              # Rich å®æ—¶ç›‘æ§ç•Œé¢\nâ”‚   â””â”€â”€ äº¤æ˜“å¯¹è‡ªåŠ¨å‘ç°        # å¤šäº¤æ˜“æ‰€äº¤æ˜“å¯¹åŒ¹é…\nâ”œâ”€â”€ ğŸ”” ä»·æ ¼æé†’ç³»ç»Ÿ (Price Alert)\nâ”‚   â”œâ”€â”€ ä»·æ ¼çªç ´ç›‘æ§          # ä»·æ ¼è§¦åŠç›®æ ‡æé†’\nâ”‚   â”œâ”€â”€ å¤šäº¤æ˜“æ‰€æ”¯æŒ          # æ”¯æŒæ‰€æœ‰æ¥å…¥çš„äº¤æ˜“æ‰€\nâ”‚   â”œâ”€â”€ ç»ˆç«¯ UI              # å®æ—¶ä»·æ ¼æ˜¾ç¤º\nâ”‚   â””â”€â”€ å£°éŸ³æé†’              # çªç ´æ—¶å£°éŸ³é€šçŸ¥\nâ”œâ”€â”€ ğŸ”— äº¤æ˜“æ‰€é€‚é…å±‚ (Exchange Adapters)\nâ”‚   â”œâ”€â”€ Hyperliquid é€‚é…å™¨    # æ°¸ç»­åˆçº¦ + ç°è´§\nâ”‚   â”œâ”€â”€ Backpack é€‚é…å™¨       # æ°¸ç»­åˆçº¦\nâ”‚   â”œâ”€â”€ Lighter é€‚é…å™¨        # æ°¸ç»­åˆçº¦ï¼ˆä½æ‰‹ç»­è´¹ï¼‰\nâ”‚   â”œâ”€â”€ Binance é€‚é…å™¨        # ç°è´§ + æ°¸ç»­åˆçº¦\nâ”‚   â”œâ”€â”€ OKX é€‚é…å™¨            # ç°è´§ + æ°¸ç»­åˆçº¦\nâ”‚   â”œâ”€â”€ EdgeX é€‚é…å™¨          # æ°¸ç»­åˆçº¦\nâ”‚   â””â”€â”€ ç»Ÿä¸€æ¥å£æ ‡å‡†          # æ ‡å‡†åŒ– API æ¥å£\nâ””â”€â”€ ğŸ›ï¸ åŸºç¡€è®¾æ–½å±‚ (Infrastructure)\n    â”œâ”€â”€ ä¾èµ–æ³¨å…¥å®¹å™¨          # DI å®¹å™¨ç®¡ç†\n    â”œâ”€â”€ äº‹ä»¶ç³»ç»Ÿ              # äº‹ä»¶é©±åŠ¨æ¶æ„\n    â”œâ”€â”€ æ—¥å¿—ç³»ç»Ÿ              # ç»“æ„åŒ–æ—¥å¿—\n    â”œâ”€â”€ é…ç½®ç®¡ç†              # YAML é…ç½®ç³»ç»Ÿ\n    â””â”€â”€ æ•°æ®èšåˆå™¨            # å¤šäº¤æ˜“æ‰€æ•°æ®èšåˆ\n```\n\n## ğŸš€ å¿«é€Ÿå¼€å§‹\n\n### ç³»ç»Ÿè¦æ±‚\n\n- Python 3.8+\n- æ”¯æŒçš„æ“ä½œç³»ç»Ÿï¼šLinuxã€macOSã€Windows\n- å¯é€‰ï¼štmuxï¼ˆç”¨äºå¤šè¿›ç¨‹ç®¡ç†ï¼‰\n\n### å®‰è£…ä¾èµ–\n\n```bash\n# å®‰è£… Python ä¾èµ–\npip install -r requirements.txt\n```\n\n### é…ç½® API å¯†é’¥\n\nåœ¨ `config/exchanges/` ç›®å½•ä¸‹é…ç½®å¯¹åº”äº¤æ˜“æ‰€çš„ API å¯†é’¥ï¼š\n\n```bash\nconfig/exchanges/\nâ”œâ”€â”€ hyperliquid_config.yaml   # Hyperliquid é…ç½®\nâ”œâ”€â”€ backpack_config.yaml       # Backpack é…ç½®\nâ”œâ”€â”€ lighter_config.yaml        # Lighter é…ç½®\nâ”œâ”€â”€ binance_config.yaml        # Binance é…ç½®\nâ”œâ”€â”€ okx_config.yaml            # OKX é…ç½®\nâ””â”€â”€ edgex_config.yaml          # EdgeX é…ç½®\n```\n\n### å¿«é€Ÿå¯åŠ¨å„ç³»ç»Ÿ\n\n#### ç½‘æ ¼äº¤æ˜“ç³»ç»Ÿ\n```bash\npython3 run_grid_trading.py config/grid/lighter-long-perp-btc.yaml\n```\n\n#### åˆ·é‡äº¤æ˜“ç³»ç»Ÿï¼ˆBackpackæŒ‚å•æ¨¡å¼ï¼‰\n```bash\npython3 run_volume_maker.py config/volume_maker/backpack_btc_volume_maker.yaml\n```\n\n#### åˆ·é‡äº¤æ˜“ç³»ç»Ÿï¼ˆLighterå¸‚ä»·æ¨¡å¼ï¼‰\n```bash\npython3 run_lighter_volume_maker.py config/volume_maker/lighter_volume_maker.yaml\n```\n\n#### å¥—åˆ©ç›‘æ§ç³»ç»Ÿ\n```bash\npython3 run_arbitrage_monitor.py\n```\n\n#### ä»·æ ¼æé†’ç³»ç»Ÿ\n```bash\npython3 run_price_alert.py config/price_alert/binance_alert.yaml\n```\n\n#### ç½‘æ ¼æ³¢åŠ¨ç‡æ‰«æå™¨\n```bash\npython3 grid_volatility_scanner/run_scanner.py\n```\n\n## ğŸ“‹ æ ¸å¿ƒåŠŸèƒ½è¯¦è§£\n\n### 1ï¸âƒ£ ç½‘æ ¼äº¤æ˜“ç³»ç»Ÿ\n\n#### åŠŸèƒ½ç‰¹æ€§\n\n- **å¤šç§ç½‘æ ¼æ¨¡å¼**ï¼šæ™®é€šç½‘æ ¼ã€é©¬ä¸ç½‘æ ¼ã€ä»·æ ¼ç§»åŠ¨ç½‘æ ¼\n- **æ™ºèƒ½ç­–ç•¥**ï¼šå‰¥å¤´çš®ã€æ™ºèƒ½å‰¥å¤´çš®ã€æœ¬é‡‘ä¿æŠ¤ã€æ­¢ç›ˆæ¨¡å¼\n- **å¥åº·æ£€æŸ¥**ï¼šè‡ªåŠ¨è®¢å•æ ¡éªŒå’Œä¿®å¤æœºåˆ¶\n- **ç»ˆç«¯ UI**ï¼šå®æ—¶ç›‘æ§ç•Œé¢ï¼Œæ˜¾ç¤ºæŒä»“ã€ç›ˆäºã€ç½‘æ ¼çŠ¶æ€\n- **ç°è´§æ”¯æŒ**ï¼šç°è´§é¢„ç•™ç®¡ç†ï¼ˆè‡ªåŠ¨ç»´æŒå¸ç§ä½™é¢ï¼‰\n- **å¤šäº¤æ˜“æ‰€**ï¼šæ”¯æŒ Hyperliquidã€Backpackã€Lighter\n\n#### é…ç½®æ–‡ä»¶ä½ç½®\n\n```\nconfig/grid/\nâ”œâ”€â”€ lighter_btc_perp_long.yaml              # Lighter BTC åšå¤š\nâ”œâ”€â”€ lighter_btc_perp_short.yaml             # Lighter BTC åšç©º\nâ”œâ”€â”€ hyperliquid_btc_perp_long.yaml          # Hyperliquid BTC åšå¤š\nâ”œâ”€â”€ hyperliquid_btc_perp_short.yaml         # Hyperliquid BTC åšç©º\nâ”œâ”€â”€ hyperliquid_btc_spot_long.yaml          # Hyperliquid ç°è´§åšå¤š\nâ”œâ”€â”€ backpack_capital_protection_long_btc.yaml   # Backpack BTC æœ¬é‡‘ä¿æŠ¤\nâ”œâ”€â”€ backpack_capital_protection_long_eth.yaml   # Backpack ETH æœ¬é‡‘ä¿æŠ¤\nâ”œâ”€â”€ backpack_capital_protection_long_sol.yaml   # Backpack SOL æœ¬é‡‘ä¿æŠ¤\nâ”œâ”€â”€ backpack_capital_protection_long_bnb.yaml   # Backpack BNB æœ¬é‡‘ä¿æŠ¤\nâ””â”€â”€ backpack_capital_protection_long_hype.yaml  # Backpack HYPE æœ¬é‡‘ä¿æŠ¤\n```\n\n#### å¯åŠ¨æ–¹å¼\n\n```bash\n# æ–¹å¼1ï¼šç›´æ¥å¯åŠ¨ï¼ˆæ¨èï¼‰\npython3 run_grid_trading.py config/grid/lighter_btc_perp_long.yaml\npython3 run_grid_trading.py config/grid/lighter_eth_perp_long.yaml\n\n# æ–¹å¼2ï¼šDEBUG æ¨¡å¼å¯åŠ¨ï¼ˆæŸ¥çœ‹è¯¦ç»†æ—¥å¿—ï¼‰\npython3 run_grid_trading.py config/grid/lighter_btc_perp_long.yaml --debug\n\n# æ–¹å¼3ï¼šä½¿ç”¨ Shell è„šæœ¬æ‰¹é‡å¯åŠ¨ï¼ˆtmuxï¼‰\n./scripts/start_all_grids.sh\n```\n\n#### æ ¸å¿ƒæ–‡ä»¶\n\n| æ–‡ä»¶è·¯å¾„ | è¯´æ˜ |\n|---------|------|\n| `run_grid_trading.py` | ç½‘æ ¼äº¤æ˜“ç³»ç»Ÿä¸»å¯åŠ¨è„šæœ¬ |\n| `core/services/grid/coordinator/grid_coordinator.py` | ç½‘æ ¼ç³»ç»Ÿåè°ƒå™¨ï¼ˆæ ¸å¿ƒé€»è¾‘ï¼‰ |\n| `core/services/grid/implementations/grid_engine_impl.py` | ç½‘æ ¼æ‰§è¡Œå¼•æ“ |\n| `core/services/grid/implementations/grid_strategy_impl.py` | ç½‘æ ¼ç­–ç•¥å®ç° |\n| `core/services/grid/implementations/position_tracker_impl.py` | æŒä»“è·Ÿè¸ªå™¨ |\n| `core/services/grid/implementations/order_health_checker.py` | è®¢å•å¥åº·æ£€æŸ¥å™¨ |\n| `core/services/grid/scalping/scalping_manager.py` | å‰¥å¤´çš®ç®¡ç†å™¨ |\n| `core/services/grid/scalping/smart_scalping_tracker.py` | æ™ºèƒ½å‰¥å¤´çš®è¿½è¸ªå™¨ |\n| `core/services/grid/capital_protection/capital_protection_manager.py` | æœ¬é‡‘ä¿æŠ¤ç®¡ç†å™¨ |\n| `core/services/grid/terminal_ui.py` | ç»ˆç«¯ UI ç•Œé¢ |\n\n### 2ï¸âƒ£ åˆ·é‡äº¤æ˜“ç³»ç»Ÿ\n\n#### åŠŸèƒ½ç‰¹æ€§\n\n- **åŒäº¤æ˜“æ¨¡å¼**ï¼šæŒ‚å•æ¨¡å¼ï¼ˆBackpackï¼‰ã€å¸‚ä»·æ¨¡å¼ï¼ˆLighterï¼‰\n- **ä¿¡å·æºæ”¯æŒ**ï¼šBackpack REST APIã€Hyperliquid WebSocket\n- **æ™ºèƒ½åˆ¤æ–­**ï¼šä¹°å–å•æ•°é‡å¯¹æ¯”ã€ä»·æ ¼å˜åŠ¨ç›‘æ§\n- **å®æ—¶ç»Ÿè®¡**ï¼šæˆäº¤é‡ã€",
      "default_branch": "main"
    },
    "fetched_at": "2025-11-30T02:52:39.454970"
  },
  {
    "basic_info": {
      "name": "karpathy",
      "full_name": "K-Dense-AI/karpathy",
      "owner": "K-Dense-AI",
      "description": "An agentic Machine Learning Engineer",
      "url": "https://github.com/K-Dense-AI/karpathy",
      "clone_url": "https://github.com/K-Dense-AI/karpathy.git",
      "ssh_url": "git@github.com:K-Dense-AI/karpathy.git",
      "homepage": "https://k-dense.ai",
      "created_at": "2025-11-16T22:39:26Z",
      "updated_at": "2025-11-30T00:31:21Z",
      "pushed_at": "2025-11-24T01:50:20Z"
    },
    "stats": {
      "stars": 1095,
      "forks": 123,
      "watchers": 1095,
      "open_issues": 1,
      "size": 22
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 10658
      },
      "license": "MIT License",
      "topics": [
        "agentic-ai",
        "automl",
        "machine-learning"
      ]
    },
    "content": {
      "readme": "# Karpathy\n\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n[![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg)](https://github.com/K-Dense-AI/karpathy/pulls)\n\nAn agentic Machine Learning Engineer that trains state-of-the-art ML models using Claude Code SDK and Google ADK. This is a very simple implemenation demonstraing the power of Claude Scientific Skills for machine learning.\n\n## Prerequisites\n\n- Python 3.13 or higher\n- [uv](https://github.com/astral-sh/uv) package manager\n- Claude Code installed and authenticated (see [installation guide](https://www.claude.com/product/claude-code))\n\n## Setup\n\n### 1. Clone the Repository\n\n```bash\ngit clone https://github.com/K-Dense-AI/karpathy.git\ncd karpathy\n```\n\n### 2. Install Dependencies\n\nInstall dependencies using `uv`:\n\n```bash\nuv sync\n```\n\n### 3. Environment Variables\n\nCreate a `.env` file in the `karpathy` directory with your API keys:\n\n```bash\nOPENROUTER_API_KEY=your_openrouter_api_key_here\nAGENT_MODEL=your_model_name_here\n```\n\nThe `OPENROUTER_API_KEY` is required for the agent to function properly.\n\nThis is the same environment variable that will be copied to the `sandbox` directory so the agents can use any API keys you provide here.\n\n## Quick Start\n\nRun the startup script to set up the sandbox and start the ADK web interface:\n\n```bash\npython start.py\n```\n\nThis automatically:\n1. Creates a `sandbox` directory with scientific skills from Claude Scientific Skills\n2. Sets up a Python virtual environment with ML packages (PyTorch, transformers, scikit-learn, etc.)\n3. Copies your `.env` file to the sandbox\n4. Starts the ADK web interface\n5. Navigate to **http://localhost:8000** in your browser\n6. Select `karpathy` in the top left under 'Select an agent'\n7. All outputs will be in the `sandbox` directory so continue to monitor that as you converse with the agent\n\n**Note:** Any files you want the agent to use (datasets, scripts, etc.) should be manually added to the `sandbox` directory.\n\n## Community\n\nJoin our K-Dense Slack community to connect with other users, share ideas, and get support:\n\n**[Join K-Dense Slack Community](https://join.slack.com/t/k-densecommunity/shared_invite/zt-3iajtyls1-EwmkwIZk0g_o74311Tkf5g)**\n\n## Claude Scientific Skills\n\nThis repository is designed to work with the **[Claude Scientific Skills](https://github.com/K-Dense-AI/claude-scientific-skills)** collection of ready-to-use scientific tools and workflows ([link](https://github.com/K-Dense-AI/claude-scientific-skills)). The `start.py` setup script creates a `sandbox` that includes scientific skills from this collection so the `karpathy` agent can leverage specialized ML libraries and scientific workflows. For full details on the skills themselves, see the upstream repositoryâ€™s README and documentation [here](https://github.com/K-Dense-AI/claude-scientific-skills).\n\n## Manual Usage\n\nTo set up the sandbox without starting the web interface:\n\n```bash\npython -m karpathy.utils\n```\n\n**Note:** Any files you want the agent to use (datasets, scripts, etc.) should be manually added to the `sandbox` directory.\n\nTo run the ADK web interface manually:\n\n```bash\nadk web\n```\n\nThen navigate to **http://localhost:8000** in your browser.\n\n## Enhanced ML Capabilities\n\nIf you want substantially more powerful ML capabilities through a multi-agentic system, sign up for [www.k-dense.ai](https://www.k-dense.ai). Currently in closed beta, launching publicly in December 2025.\n\n## Upcoming Features\n\n- **Modal sandbox integration** - Choose any type of compute you want\n- **K-Dense Web features** - We might make some features from K-Dense Web available here based on interest\n\n## Star History\n\n[![Star History Chart](https://api.star-history.com/svg?repos=K-Dense-AI/karpathy&type=date&legend=top-left)](https://www.star-history.com/#K-Dense-AI/karpathy&type=date&legend=top-left)\n\n## Disclaimer\n\nThis project is **not** endorsed by or affiliated with Andrej Karpathy. The name is used as a tribute and out of deep respect for his contributions to AI and technical leadership.",
      "default_branch": "main"
    },
    "fetched_at": "2025-11-30T02:52:40.565048"
  },
  {
    "basic_info": {
      "name": "code-mode",
      "full_name": "universal-tool-calling-protocol/code-mode",
      "owner": "universal-tool-calling-protocol",
      "description": "ğŸ”Œ Plug-and-play library to enable agents to call MCP and UTCP tools via code execution. ",
      "url": "https://github.com/universal-tool-calling-protocol/code-mode",
      "clone_url": "https://github.com/universal-tool-calling-protocol/code-mode.git",
      "ssh_url": "git@github.com:universal-tool-calling-protocol/code-mode.git",
      "homepage": "",
      "created_at": "2025-11-11T09:35:44Z",
      "updated_at": "2025-11-29T18:57:47Z",
      "pushed_at": "2025-11-29T13:59:35Z"
    },
    "stats": {
      "stars": 1075,
      "forks": 68,
      "watchers": 1075,
      "open_issues": 6,
      "size": 257
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 49996,
        "TypeScript": 41432,
        "JavaScript": 12971
      },
      "license": "Mozilla Public License 2.0",
      "topics": [
        "ai-agents",
        "codemode",
        "mcp",
        "model-context-protocol",
        "toolchain",
        "utcp"
      ]
    },
    "content": {
      "readme": "<div align=\"center\">\n<!-- <img alt=\"utcp code mode banner\" src=\"https://github.com/user-attachments/assets/77723130-ecbc-4d1d-9e9b-20f978882699\" width=\"80%\" style=\"margin: 20px auto;\"> -->\n\n<h1 align=\"center\">ğŸ¤– Code-Mode Library: First library for tool calls via code execution</h1>\n<p align=\"center\">\n    <a href=\"https://github.com/universal-tool-calling-protocol\">\n        <img src=\"https://img.shields.io/github/followers/universal-tool-calling-protocol?label=Follow%20Org&logo=github\" /></a>\n    <a href=\"https://img.shields.io/npm/dt/@utcp/code-mode\" title=\"PyPI Version\">\n        <img src=\"https://img.shields.io/npm/dt/@utcp/code-mode\"/></a>\n    <a href=\"https://github.com/universal-tool-calling-protocol/code-mode/blob/main/LICENSE\" alt=\"License\">\n        <img src=\"https://img.shields.io/github/license/universal-tool-calling-protocol/code-mode\" /></a>\n \n  [![npm](https://img.shields.io/npm/v/@utcp/code-mode)](https://www.npmjs.com/package/@utcp/code-mode)\n</p>\n</div>\n\n> Transform your AI agents from clunky tool callers into efficient code executors â€” in just 3 lines.\n\n## Why This Changes Everything\n\nLLMs excel at writing code but struggle with tool calls. Instead of exposing hundreds of tools directly, give them ONE tool that executes TypeScript code with access to your entire toolkit.\n\n[Apple](https://machinelearning.apple.com/research/codeact), [Cloudflare](https://blog.cloudflare.com/code-mode/), and [Anthropic](https://www.anthropic.com/engineering/code-execution-with-mcp) say that Code-Mode is a more efficient way to approach tool calling compared to the traditional dump function information and then extract a JSON for function calling.\n\n## Benchmarks\n\nIndependent [Python benchmark study](https://github.com/imran31415/codemode_python_benchmark) validates the performance claims with **$9,536/year cost savings** at 1,000 scenarios/day:\n\n| Scenario Complexity | Traditional | Code Mode | **Improvement** |\n|---------------------|-------------|-----------|----------------|\n| **Simple (2-3 tools)** | 3 iterations | 1 execution | **67% faster** |\n| **Medium (4-7 tools)** | 8 iterations | 1 execution | **75% faster** |\n| **Complex (8+ tools)** | 16 iterations | 1 execution | **88% faster** |\n\n### **Why Code Mode Dominates:**\n\n   **Batching Advantage** - Single code block replaces multiple API calls  \n   **Cognitive Efficiency** - LLMs excel at code generation vs. tool orchestration  \n   **Computational Efficiency** - No context re-processing between operations\n\n# Getting Started\n\n[<img width=\"2606\" height=\"1445\" alt=\"Frame 4 (4)\" src=\"https://github.com/user-attachments/assets/58ba26ab-6e77-459b-a59a-eeb60d711746\" />\n](https://www.youtube.com/watch?v=zsMjkPzmqhA)\n\n## Get Started in 3 Lines\n\n```typescript\nimport { CodeModeUtcpClient } from '@utcp/code-mode';\n\nconst client = await CodeModeUtcpClient.create();                    // 1. Initialize\nawait client.registerManual({ name: 'github', /* MCP config */ });  // 2. Add tools  \nconst { result } = await client.callToolChain(`/* TypeScript */`);   // 3. Execute code\n```\n\nThat's it. Your AI agent can now execute complex workflows in a single request instead of dozens.\n\n## What You Get\n\n### **Progressive Tool Discovery**\n```typescript\n// Agent discovers tools dynamically, loads only what it needs\nconst tools = await client.searchTools('github pull request');\n// Instead of 500 tool definitions â†’ 3 relevant tools\n```\n\n### **Natural Code Execution**  \n```typescript\nconst { result, logs } = await client.callToolChain(`\n  // Chain multiple operations in one request\n  const pr = await github.get_pull_request({ owner: 'microsoft', repo: 'vscode', pull_number: 1234 });\n  const comments = await github.get_pull_request_comments({ owner: 'microsoft', repo: 'vscode', pull_number: 1234 });\n  const reviews = await github.get_pull_request_reviews({ owner: 'microsoft', repo: 'vscode', pull_number: 1234 });\n  \n  // Process data efficiently in-sandbox\n  return {\n    title: pr.title,\n    commentCount: comments.length,\n    approvals: reviews.filter(r => r.state === 'APPROVED').length\n  };\n`);\n// Single API call replaces 15+ traditional tool calls\n```\n\n### **Auto-Generated TypeScript Interfaces**\n```typescript\nnamespace github {\n  interface get_pull_requestInput {\n    /** Repository owner */\n    owner: string;\n    /** Repository name */ \n    repo: string;\n    /** Pull request number */\n    pull_number: number;\n  }\n}\n```\n\n## Enterprise-Ready\n\n- **Secure VM Sandboxing** â€“ Node.js isolates prevent unauthorized access\n- **Timeout Protection** â€“ Configurable execution limits prevent runaway code  \n- **Complete Observability** â€“ Full console output capture and error handling\n- **Zero External Dependencies** â€“ Tools only accessible through registered UTCP/MCP servers\n- **Runtime Introspection** â€“ Dynamic interface discovery for adaptive workflows\n\nIf you're working at an enterprise, and need support, book a consultation [here](https://bevel.neetocal.com/meeting-with-ali).\n## Universal Protocol",
      "default_branch": "main"
    },
    "fetched_at": "2025-11-30T02:52:41.693193"
  },
  {
    "basic_info": {
      "name": "Video-Materials-AutoGEN-Workstation",
      "full_name": "Norsico/Video-Materials-AutoGEN-Workstation",
      "owner": "Norsico",
      "description": "ä¸€ä¸ªé›†å†…å®¹ç­–åˆ’ã€AIæ–‡æ¡ˆè‡ªåŠ¨ç”Ÿæˆã€TTS æ‰¹é‡è‡ªåŠ¨é…éŸ³ã€(AI)å›¾ç‰‡ç´ æåˆæˆã€ASRè‡ªåŠ¨æå–è¯­è¨€å­—å¹•è„šæœ¬ã€AIè‡ªç”±åˆ›ä½œäºä¸€ä½“çš„(çŸ­è§†é¢‘)ç”Ÿæˆå·¥ä½œç«™ã€‚æ–¹ä¾¿ç®¡ç†æ¯æœŸçš„è§†é¢‘é¡¹ç›®ã€‚",
      "url": "https://github.com/Norsico/Video-Materials-AutoGEN-Workstation",
      "clone_url": "https://github.com/Norsico/Video-Materials-AutoGEN-Workstation.git",
      "ssh_url": "git@github.com:Norsico/Video-Materials-AutoGEN-Workstation.git",
      "homepage": "",
      "created_at": "2025-11-18T13:04:59Z",
      "updated_at": "2025-11-30T01:35:23Z",
      "pushed_at": "2025-11-28T07:43:12Z"
    },
    "stats": {
      "stars": 972,
      "forks": 197,
      "watchers": 972,
      "open_issues": 1,
      "size": 58397
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 19833252,
        "JavaScript": 144399,
        "HTML": 45750,
        "CSS": 28271,
        "Batchfile": 215
      },
      "license": null,
      "topics": []
    },
    "content": {
      "readme": "# Video Material GEN Workstation\n\nä¸€ä¸ªé›†å†…å®¹ç­–åˆ’ã€AIæ–‡æ¡ˆè‡ªåŠ¨ç”Ÿæˆã€TTS æ‰¹é‡è‡ªåŠ¨é…éŸ³ã€(AI)å›¾ç‰‡ç´ æåˆæˆã€ASRè‡ªåŠ¨æå–è¯­è¨€å­—å¹•è„šæœ¬ã€AIè‡ªç”±åˆ›ä½œäºä¸€ä½“çš„(çŸ­è§†é¢‘)ç”Ÿæˆå·¥ä½œç«™ã€‚æ–¹ä¾¿ç®¡ç†æ¯æœŸçš„è§†é¢‘é¡¹ç›®ã€‚\n\n# â—æ­£åœ¨è€ƒè™‘ä½¿ç”¨æœ€æ–°LangGraphæ¶æ„é‡æ„è¯¥é¡¹ç›®ï¼Œæ•¬è¯·æœŸå¾…â—\n\n## åŠŸèƒ½é€Ÿè§ˆ\n\n- æ”¯æŒæŒ‰æ¨¡æ¿æ‰¹é‡ç”Ÿæˆè§†é¢‘é¡¹ç›®ï¼Œè„šæœ¬ã€å›¾ç‰‡ç´ æ(AI)ã€å­—å¹•å’ŒéŸ³é¢‘ä¸€é”®é½å¤‡ã€‚\n- Gemini + TTSåˆæˆï¼Œæ—¢èƒ½æ”¹å†™è„šæœ¬åˆèƒ½ç›´æ¥è¾“å‡º(å¸¦æƒ…ç»ªçš„)é…éŸ³ã€‚\n- å›¾æ–‡åˆ†è½¨ç®¡ç†ï¼Œå¯åœ¨å‰ç«¯éšæ—¶æ›¿æ¢å›¾ç‰‡ã€å­—å¹•æˆ–éŸ³é¢‘å¹¶é¢„è§ˆç»“æœã€‚\n\n## æ•°æ®å±•ç¤º\n\n![æŠ–éŸ³æŠ•æ”¾æ•°æ®](img/æ•°æ®.png)\n\n\n## å‰ç«¯ç•Œé¢\n\n![ç•Œé¢ 1](img/1.png)\n![ç•Œé¢ 2](img/2.png)\n![ç•Œé¢ 3](img/3.png)\n![ç•Œé¢ 4](img/4.png)\n![ç•Œé¢ 5](img/5.png)\n![ç•Œé¢ 6](img/6.png)\n\n## å¿«é€Ÿä¸Šæ‰‹\n\n1. å¤åˆ¶ `env.example.yaml` ä¸º `env.yaml`ï¼Œå¡«å…¥è‡ªå·±çš„ Gemini Keyã€Base URLã€æ¨¡å‹ã€TTS Key ä¸æç¤ºè¯ç­‰é…ç½®ï¼Œå¦åˆ™æ— æ³•è°ƒç”¨æ¥å£ã€‚\n2. ï¼ˆå¯é€‰ï¼‰åœ¨ `env.yaml` ä¸­è®¾ç½® `Default-Project-Root`ï¼Œç”¨äºå­˜æ”¾è‡ªåŠ¨ç”Ÿæˆçš„è„šæœ¬ã€éŸ³é¢‘ä¸å›¾ç‰‡æ–‡ä»¶ã€‚\n3. å®‰è£…ä¾èµ–ï¼š`npm install`ã€‚\n4. å¯åŠ¨æœåŠ¡ï¼š`npm start` æˆ–ç›´æ¥åŒå‡» `start.bat`ï¼Œé»˜è®¤è®¿é—®åœ°å€ä¸º `http://localhost:8765`ã€‚\n\n## åŠŸèƒ½ä»‹ç»\n\n1. **é¡¹ç›®æ€»è§ˆ**ï¼šä»¥å¡ç‰‡å½¢å¼ç®¡ç†æ‰¹é‡é¡¹ç›®ï¼Œæ˜¾ç¤ºè¾“å‡ºç›®å½•ã€åˆ›å»ºæ—¶é—´åŠåˆ é™¤åŠ¨ä½œï¼Œä¾¿äºå¿«é€Ÿå®šä½ã€‚\n2. **æ–‡æ¡ˆç”Ÿæˆ**ï¼šç»“æ„åŒ–å±•ç¤ºåœºæ™¯è„šæœ¬ï¼Œå¯å¤åˆ¶å•æ¡æˆ–æ•´æ®µæ–‡æ¡ˆï¼Œå·¦ä¾§å‹¾é€‰è”åŠ¨å³ä¾§æç¤ºè¯ã€‚\n3. **å­—å¹•è·å–**: éœ€é…åˆæˆ‘çš„å¦ä¸€ä¸ªé¡¹ç›®(n8n-http-tools): å¼€æºåœ°å€:[n8n-http-tools](https://github.com/Norsico/n8n-http-tools)\n4. **TTS åˆæˆ**ï¼šæ”¯æŒå•æ¡ä¸æ‰¹é‡ä¸¤ç§æ¨¡å¼ï¼Œè¾“å…¥åˆæˆæ–‡æœ¬ä¸æƒ…æ„Ÿæç¤ºå³å¯ç”Ÿæˆè¯­éŸ³ã€‚\n5. **å›¾ç‰‡ç”Ÿæˆ**ï¼šé›†ä¸­ç®¡ç†è§’è‰²æè¿°ã€åœºæ™¯æè¿°ç­‰æç¤ºè¯ï¼Œå‹¾é€‰åå³å¯æ‰¹é‡å¤åˆ¶åˆ°ç»˜å›¾ä»»åŠ¡ã€‚\n6. **ç«‹ç»˜/èƒŒæ™¯ç­‰ç”Ÿæˆ**ï¼šæä¾›æç¤ºè¯è¾“å…¥ã€å‚è€ƒå›¾ä¸Šä¼ ã€å®½é«˜æ¯”è®¾ç½®ä¸å†å²è®°å½•ï¼Œæ–¹ä¾¿éšæ—¶å¤ç”¨ç´ æã€‚\n7. **é€†å‘æ¥å£å®ç°ASRè‡ªåŠ¨æå–å‰ªè¾‘éœ€è¦çš„å­—å¹•æ–‡ä»¶**ï¼šåœ¨TTSåˆæˆç•Œé¢ä¸‹æ–¹ï¼Œæœ‰â€œå­—å¹•ç”Ÿæˆâ€åŠŸèƒ½ï¼Œç‚¹å‡»ä¸‹æ–¹çš„æŒ‰é’®å¯ä»¥æ‰“å¼€å­—å¹•ç”Ÿæˆå·¥å…·ã€‚æ­¤éƒ¨åˆ†ä»£ç ç”±å…¶å®ƒä½œè€…å¼€æºã€‚\n8. **å¸¸ç”¨æç¤ºè¯ä¸è‡ªç”±åˆ›ä½œ**ï¼šæ”¶è—é«˜é¢‘æç¤ºè¯å¹¶ä¸€é”®å¤åˆ¶ï¼ŒåŒæ—¶æä¾›è‡ªç”±åˆ›ä½œé¢æ¿è¿›è¡Œè‡ªå®šä¹‰ç»˜åˆ¶ã€‚\n\n### å…¶å®ƒåŠŸèƒ½æˆ‘å°±æ‡’å¾—ä¸€ä¸ªä¸€ä¸ªå†™äº†ï¼Œå…·ä½“æœ‰å•¥è‡ªå·±å¯ä»¥éƒ¨ç½²ä¸€ä¸‹å»ç©ï¼Œæ³¨æ„æ–‡æ¡ˆç”Ÿæˆè¿™é‡Œéœ€è¦é…åˆn8næ¥æ“ä½œï¼Œä¹‹å‰å†™çš„n8næ–‡ä»¶æ‰¾ä¸åˆ°äº†ï¼Œæ‰€ä»¥è¿™éƒ¨åˆ†å…¶å®å¯ä»¥å¿½ç•¥ï¼Œä¸»è¦å°±æ˜¯ä¸€ä¸ªç”¨äºç”Ÿæˆæ–‡æ¡ˆçš„è„šæœ¬AIæç¤ºè¯ä»¥åŠæˆ‘ä¸»é¡µå¦ä¸€ä¸ªä»“åº“ä¸­æœ‰çš„ä¸€ä¸ªå¼€æºçš„Bç«™è§†é¢‘å­—å¹•æå–å™¨ï¼ˆå½“ç„¶ç½‘ä¸Šä¹Ÿæœ‰ï¼‰ï¼ˆå‚è€ƒåˆ«äººé«˜æ’­æ”¾çš„è§†é¢‘è‡ªå·±å­¦èµ·æ¥ä¹Ÿä¼šå¿«å¾ˆå¤šï¼‰\n\n## æ¥ä¸‹æ¥å¦‚ä½•å¥½å¥½åˆ©ç”¨è¿™ä¸ªé¡¹ç›®è¿˜æ˜¯å¾—é è‡ªå·±ã€‚\n### å› ä¸ºä¸»è¦è¿˜æ˜¯åå‘ç®¡ç†ç”¨çš„ï¼ˆç®€å•æ¥è®²å°±æ˜¯åŠŸèƒ½ä¸ä¼šæœ‰ä½ æƒ³è±¡çš„é‚£ä¹ˆå®ç”¨ï¼‰ï¼Œè§†é¢‘å†…å®¹å¦‚ä½•å®šä¹‰ï¼Œå¦‚ä½•æ‰“é€ çˆ†æ¬¾è¿˜æ˜¯éœ€è¦åŠ¨è„‘å­ã€‚å½“ç„¶æœ¬é¡¹ç›®é‡Œé¢ä½¿ç”¨å›¾åƒç¼–è¾‘æ¨¡å‹çš„æ˜¯NanoBananaï¼Œæœ¬åœ°éƒ¨ç½²çš„AIStudioçš„åå‘ä»£ç†çš„æ¥å£ï¼Œç”¨æ¥ç”Ÿå›¾ç„¶åç»™Soraä¹Ÿæ˜¯ä¸é”™çš„ï¼Œèµ·ç æµ‹è¯•ä¸‹æ¥æ¯”è¾ƒç¨³å®šã€‚\n\n## Star History\n\n<a href=\"https://www.star-history.com/#Norsico/Video-Materials-AutoGEN-Workstation&type=date&legend=bottom-right\">\n <picture>\n   <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://api.star-history.com/svg?repos=Norsico/Video-Materials-AutoGEN-Workstation&type=date&theme=dark&legend=bottom-right\" />\n   <source media=\"(prefers-color-scheme: light)\" srcset=\"https://api.star-history.com/svg?repos=Norsico/Video-Materials-AutoGEN-Workstation&type=date&legend=bottom-right\" />\n   <img alt=\"Star History Chart\" src=\"https://api.star-history.com/svg?repos=Norsico/Video-Materials-AutoGEN-Workstation&type=date&legend=bottom-right\" />\n </picture>\n</a>\n\n## å…è´£å£°æ˜\n\n### é¡¹ç›®ä»…å…±å‚è€ƒäº¤æµå­¦ä¹ ä½¿ç”¨ï¼Œä¸å¯¹ä»»ä½•ä½¿ç”¨è€…äº§ç”Ÿçš„é—®é¢˜è´Ÿè´£\n\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-11-30T02:52:42.793448"
  },
  {
    "basic_info": {
      "name": "HunyuanVideo-1.5",
      "full_name": "Tencent-Hunyuan/HunyuanVideo-1.5",
      "owner": "Tencent-Hunyuan",
      "description": "HunyuanVideo-1.5: A leading lightweight video generation model",
      "url": "https://github.com/Tencent-Hunyuan/HunyuanVideo-1.5",
      "clone_url": "https://github.com/Tencent-Hunyuan/HunyuanVideo-1.5.git",
      "ssh_url": "git@github.com:Tencent-Hunyuan/HunyuanVideo-1.5.git",
      "homepage": "https://hunyuan.tencent.com/video/zh?tabIndex=0",
      "created_at": "2025-11-20T06:15:42Z",
      "updated_at": "2025-11-30T02:39:48Z",
      "pushed_at": "2025-11-29T08:04:44Z"
    },
    "stats": {
      "stars": 963,
      "forks": 66,
      "watchers": 963,
      "open_issues": 20,
      "size": 874
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 431482
      },
      "license": "Other",
      "topics": [
        "image-to-video",
        "text-to-video",
        "video-generation"
      ]
    },
    "content": {
      "readme": "[ä¸­æ–‡æ–‡æ¡£](./README_CN.md)\n\n# HunyuanVideo-1.5\n\n<div align=\"center\">\n\n<img src=\"./assets/logo.png\" alt=\"HunyuanVideo-1.5 Logo\" width=\"80%\">\n\n# ğŸ¬ HunyuanVideo-1.5: A leading lightweight video generation model\n\n</div>\n\n\n<div align=\"center\">\n<!-- <img src=\"./assets/banner.png\" alt=\"HunyuanVideo-1.5 Banner\" width=\"800\"> -->\n\n</div>\n\n\nHunyuanVideo-1.5 is a video generation model that delivers top-tier quality with only 8.3B parameters, significantly lowering the barrier to usage. It runs smoothly on consumer-grade GPUs, making it accessible for every developer and creator. This repository provides the implementation and tools needed to generate creative videos.\n\n\n<div align=\"center\">\n  <a href=\"https://hunyuan.tencent.com/video/zh?tabIndex=0\" target=\"_blank\"><img src=https://img.shields.io/badge/Official%20Site-333399.svg?logo=homepage height=22px></a>\n  <a href=https://huggingface.co/tencent/HunyuanVideo-1.5 target=\"_blank\"><img src=https://img.shields.io/badge/%F0%9F%A4%97%20Models-d96902.svg height=22px></a>\n  <a href=https://github.com/Tencent-Hunyuan/HunyuanVideo-1.5 target=\"_blank\"><img src= https://img.shields.io/badge/Page-bb8a2e.svg?logo=github height=22px></a>\n  <a href=\"https://arxiv.org/pdf/2511.18870\" target=\"_blank\"><img src=https://img.shields.io/badge/Report-b5212f.svg?logo=arxiv height=22px></a>\n  <a href=https://x.com/TencentHunyuan target=\"_blank\"><img src=https://img.shields.io/badge/Hunyuan-black.svg?logo=x height=22px></a>\n  <a href=\"https://github.com/Tencent-Hunyuan/HunyuanVideo-1.5/blob/main/assets/HunyuanVideo_1_5_Prompt_Handbook_EN.md\" target=\"_blank\"><img src=https://img.shields.io/badge/ğŸ“š-PromptHandBook-blue.svg?logo=book height=22px></a> <br/>\n  <a href=\"./ComfyUI/README.md\" target=\"_blank\"><img src=https://img.shields.io/badge/ComfyUI-blue.svg?logo=book height=22px></a>\n  <a href=\"https://github.com/ModelTC/LightX2V\" target=\"_blank\"><img src=https://img.shields.io/badge/LightX2V-yellow.svg?logo=book height=22px></a>\n  <a href=\"https://tusi.cn/models/933574988890423836\" target=\"_blank\"><img src=https://img.shields.io/badge/åå¸-purple.svg?logo=book height=22px></a>\n  <a href=\"https://tensor.art/models/933574988890423836\" target=\"_blank\"><img src=https://img.shields.io/badge/TensorArt-cyan.svg?logo=book height=22px></a>\n\n</div>\n\n\n<p align=\"center\">\n    ğŸ‘ Join our <a href=\"./assets/wechat.png\" target=\"_blank\">WeChat</a> and <a href=\"https://discord.gg/ehjWMqF5wY\">Discord</a> | \nğŸ’» <a href=\"https://hunyuan.tencent.com/video/zh?tabIndex=0\">Official website Try our model!</a>&nbsp&nbsp\n</p>\n\n## ğŸ”¥ğŸ”¥ğŸ”¥ News\n* ğŸ“š Training code is coming soon.\n* ğŸš€ Nov 27, 2025: We now support cache inference (deepcache, teacache, taylorcache), achieving significant speedup! Pull the latest code to try it. ğŸ”¥ğŸ”¥ğŸ”¥ğŸ†• \n* ğŸš€ Nov 24, 2025: We now support deepcache inference.\n* ğŸ‘‹ Nov 20, 2025: We release the inference code and model weights of HunyuanVideo-1.5.\n\n\n## ğŸ¥ Demo\n<div align=\"center\">\n  <video src=\"https://github.com/user-attachments/assets/d45ec78e-ea40-47f1-8d4d-f4d9a0682e2d\" width=\"60%\"> </video>\n</div>\n\n## ğŸ§© Community Contributions\n\nIf you develop/use HunyuanVideo-1.5 in your projects, welcome to let us know.\n\n- **ComfyUI** - [ComfyUI](https://github.com/comfyanonymous/ComfyUI): A powerful and modular diffusion model GUI with a graph/nodes interface. ComfyUI supports HunyuanVideo-1.5 with various engineering optimizations for fast inference. We provide a [ComfyUI Usage Guide](./ComfyUI/README.md) for HunyuanVideo-1.5.\n\n- **Community-implemented ComfyUI Plugin** - [comfyui_hunyuanvideo_1.5_plugin](https://github.com/yuanyuan-spec/comfyui_hunyuanvideo_1.5_plugin): A community-implemented ComfyUI plugin for HunyuanVideo-1.5, offering both simplified and complete node sets for quick usage or deep workflow customization, with built-in automatic model download support.\n\n- **LightX2V** - [LightX2V](https://github.com/ModelTC/LightX2V): A lightweight and efficient video generation framework that integrates HunyuanVideo-1.5, supporting multiple engineering acceleration techniques for fast inference.\n\n- **Wan2GP v9.62** - [Wan2GP](https://github.com/deepbeepmeep/Wan2GP): WanGP is a very low VRAM app (as low 6 GB of VRAM for Hunyuan Video 1.5) supports Lora Accelerator for a 8 steps generation and offers tools to facilitate Video Generation.\n\n- **ComfyUI-MagCache** - [ComfyUI-MagCache](https://github.com/Zehong-Ma/ComfyUI-MagCache): MagCache is a training-free caching approach that accelerates video generation by estimating fluctuating differences among model outputs across timesteps. It achieves 1.7x speedup for HunyuanVideo-1.5 with 20 inference steps.\n\n\n## ğŸ“‘ Open-source Plan\n- HunyuanVideo-1.5 (T2V/I2V)\n  - [x] Inference Code and checkpoints\n  - [x] ComfyUI Support\n  - [x] LightX2V Support\n  - [ ] Diffusers Support\n  - [ ] Release all model weights (Sparse attention, distill model, and SR models)\n\n## ğŸ“‹ Table of Contents\n- [ğŸ”¥ğŸ”¥ğŸ”¥ News](#-news)\n- [ğŸ¥ Demo](#-demo)\n- [ğŸ§© Community Contributions](#-community-contributions)\n- [ğŸ“‘ Open",
      "default_branch": "main"
    },
    "fetched_at": "2025-11-30T02:52:43.897199"
  },
  {
    "basic_info": {
      "name": "flux2",
      "full_name": "black-forest-labs/flux2",
      "owner": "black-forest-labs",
      "description": "Official inference repo for FLUX.2 models",
      "url": "https://github.com/black-forest-labs/flux2",
      "clone_url": "https://github.com/black-forest-labs/flux2.git",
      "ssh_url": "git@github.com:black-forest-labs/flux2.git",
      "homepage": null,
      "created_at": "2025-11-24T23:28:49Z",
      "updated_at": "2025-11-30T01:38:36Z",
      "pushed_at": "2025-11-28T19:28:31Z"
    },
    "stats": {
      "stars": 914,
      "forks": 38,
      "watchers": 914,
      "open_issues": 6,
      "size": 37542
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 84177
      },
      "license": "Apache License 2.0",
      "topics": []
    },
    "content": {
      "readme": "# FLUX.2\nby Black Forest Labs: https://bfl.ai.\n\nDocumentation for our API can be found here: [docs.bfl.ai](https://docs.bfl.ai/).\n\nThis repo contains minimal inference code to run image generation & editing with our FLUX.2 open-weight models.\n\n## `FLUX.2 [dev]`\n\n`FLUX.2 [dev]` is a 32B parameter flow matching transformer model capable of generating and editing (multiple) images. The model is released under the [FLUX.2-dev Non-Commercial License](model_licenses/LICENSE-FLUX-DEV) and can be found [here](https://huggingface.co/black-forest-labs/FLUX.2-dev).\n\nNote that the below script for `FLUX.2 [dev]` needs considerable amount of VRAM (H100-equivalent GPU). We partnered with Hugging Face to make quantized versions that run on consumer hardware; below you can find instructions on how to run it on a RTX 4090 with a remote text encoder, for other quantization sizes and combinations, check the [diffusers quantization guide here](docs/flux2_dev_hf.md).\n\n### Text-to-image examples\n\n![t2i-grid](assets/teaser_generation.png)\n\n### Editing examples\n\n![edit-grid](assets/teaser_editing.png)\n\n### Prompt upsampling\n\n`FLUX.2 [dev]` benefits significantly from prompt upsampling. The inference script below offers the option to use both local prompt upsampling with the same model we use for text encoding ([`Mistral-Small-3.2-24B-Instruct-2506`](https://huggingface.co/mistralai/Mistral-Small-3.2-24B-Instruct-2506)), or alternatively, use any model on [OpenRouter](https://openrouter.ai/) via an API call.\n\nSee the [upsampling guide](docs/flux2_with_prompt_upsampling.md) for additional details and guidance on when to use upsampling.\n\n## `FLUX.2` autoencoder\n\nThe FLUX.2 autoencoder has considerably improved over the [FLUX.1 autoencoder](https://huggingface.co/black-forest-labs/FLUX.1-dev/blob/main/ae.safetensors). The autoencoder is released under [Apache 2.0](https://huggingface.co/datasets/choosealicense/licenses/blob/main/markdown/apache-2.0.md) and can be found [here](https://huggingface.co/black-forest-labs/FLUX.2-dev/blob/main/ae.safetensors). For more information, see our [technical blogpost](https://bfl.ai/research/representation-comparison).\n\n## Local installation\n\nThe inference code was tested on GB200 and H100 (with CPU offloading).\n\n### GB200\n\nOn GB200, we tested `FLUX.2 [dev]` using CUDA 12.9 and Python 3.12.\n\n```bash\npython3.12 -m venv .venv\nsource .venv/bin/activate\npip install -e . --extra-index-url https://download.pytorch.org/whl/cu129 --no-cache-dir\n```\n\n### H100\n\nOn H100, we tested `FLUX.2 [dev]` using CUDA 12.6 and Python 3.10.\n\n```bash\npython3.10 -m venv .venv\nsource .venv/bin/activate\npip install -e . --extra-index-url https://download.pytorch.org/whl/cu126 --no-cache-dir\n```\n\n## Run the CLI\n\nBefore running the CLI, you may download the weights from [here](https://huggingface.co/black-forest-labs/FLUX.2-dev) and set the following environment variables.\n\n```bash\nexport FLUX2_MODEL_PATH=\"<flux2_path>\"\nexport AE_MODEL_PATH=\"<ae_path>\"\n```\n\nIf you don't set the environment variables, the weights will be downloaded\nautomatically.\n\nYou can start an interactive session with loaded weights by running the\nfollowing command. That will allow you to do both text to image generation as\nwell as editing one or multiple images.\n```bash\nexport PYTHONPATH=src\npython scripts/cli.py\n```\n\nOn H100, we additionally set the flag `--cpu_offloading True`.\n\n## Watermarking\n\nWe've added an option to embed invisible watermarks directly into the generated images\nvia the [invisible watermark library](https://github.com/ShieldMnt/invisible-watermark).\n\nAdditionally, we are recommending implementing a solution to mark the metadata of your outputs, such as [C2PA](https://c2pa.org/)\n\n## ğŸ§¨ Lower VRAM diffusers example\n\nThe below example should run on a RTX 4090. For more examples check the [diffusers quantization guide here](docs/flux2_dev_hf.md)\n\n```python\nimport torch\nfrom diffusers import Flux2Pipeline\nfrom diffusers.utils import load_image\nfrom huggingface_hub import get_token\nimport requests\nimport io\n\nrepo_id = \"diffusers/FLUX.2-dev-bnb-4bit\"\ndevice = \"cuda:0\"\ntorch_dtype = torch.bfloat16\n\ndef remote_text_encoder(prompts):\n    response = requests.post(\n        \"https://remote-text-encoder-flux-2.huggingface.co/predict\",\n        json={\"prompt\": prompts},\n        headers={\n            \"Authorization\": f\"Bearer {get_token()}\",\n            \"Content-Type\": \"application/json\"\n        }\n    )\n    prompt_embeds = torch.load(io.BytesIO(response.content))\n\n    return prompt_embeds.to(device)\n\npipe = Flux2Pipeline.from_pretrained(\n    repo_id, text_encoder=None, torch_dtype=torch_dtype\n).to(device)\n\nprompt = \"Realistic macro photograph of a hermit crab using a soda can as its shell, partially emerging from the can, captured with sharp detail and natural colors, on a sunlit beach with soft shadows and a shallow depth of field, with blurred ocean waves in the background. The can has the text `BFL Diffusers` on it and it has a color gradient that star",
      "default_branch": "main"
    },
    "fetched_at": "2025-11-30T02:52:45.005398"
  },
  {
    "basic_info": {
      "name": "HunyuanOCR",
      "full_name": "Tencent-Hunyuan/HunyuanOCR",
      "owner": "Tencent-Hunyuan",
      "description": null,
      "url": "https://github.com/Tencent-Hunyuan/HunyuanOCR",
      "clone_url": "https://github.com/Tencent-Hunyuan/HunyuanOCR.git",
      "ssh_url": "git@github.com:Tencent-Hunyuan/HunyuanOCR.git",
      "homepage": null,
      "created_at": "2025-11-18T04:06:24Z",
      "updated_at": "2025-11-30T02:52:01Z",
      "pushed_at": "2025-11-28T06:44:17Z"
    },
    "stats": {
      "stars": 777,
      "forks": 49,
      "watchers": 777,
      "open_issues": 25,
      "size": 73639
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 22360
      },
      "license": "Other",
      "topics": []
    },
    "content": {
      "readme": "<div align=\"center\">\n\n[ä¸­æ–‡é˜…è¯»](./README_zh.md)\n\n</div>\n\n<div align=\"center\">\n\n# HunyuanOCR\n\n</div>\n\n<p align=\"center\">\n <img src=\"./assets/hyocr-head-img.png\" width=\"80%\"/> <br>\n</p>\n\n\n<p align=\"center\">\n<a href=\"https://huggingface.co/spaces/tencent/HunyuanOCR\"><b>ğŸ¯ Demo</b></a> |\n<a href=\"https://huggingface.co/tencent/HunyuanOCR\"><b>ğŸ“¥ Model Download</b></a> |\n<a href=\"https://arxiv.org/abs/2511.19575\"><b>ğŸ“„ Technical Report</b></a>\n</p>\n\n## ğŸ¤ Join Our Community\n\n<div align=\"center\">\n\n| Wechat Discussion Group | Discord Group |\n| :---: | :---: |\n| <img src=\"./assets/qrcode_for_hunyuanocr_wechat.jpg\" width=\"150\"> | [Join HunyuanOCR Discord](https://discord.gg/XeD3p2MRDk) |\n\n</div>\n\n## ğŸ”¥ News\n- **[2025/11/28]** ğŸ› ï¸ We fixed vLLM inference bugs and hyperparameter configuration issues such as system prompt. It is recommended to use the latest vLLM installation steps and the [inference script](https://github.com/Tencent-Hunyuan/HunyuanOCR/blob/main/Hunyuan-OCR-master/Hunyuan-OCR-vllm/run_hy_ocr.py) for performance testing. Currently, there is still a certain accuracy difference between Transformers and the vLLM framework (we are working on fixing this).\n- **[2025/11/25]** ğŸ“ Inference code and model weights publicly available.\n\n\n## ğŸ“– Introduction\n**HunyuanOCR** stands as a leading end-to-end OCR expert VLM powered by Hunyuan's native multimodal architecture. With a remarkably lightweight 1B parameter design, it has achieved multiple state-of-the-art benchmarks across the industry. The model demonstrates mastery in **complex multilingual document parsing** while excelling in practical applications including **text spotting, open-field information extraction, video subtitle extraction, and photo translation**.\n\n\n## âœ¨ Key Features\n\n- ğŸ’ª **Efficient Lightweight Architecture**: Built on Hunyuan's native multimodal architecture and training strategy, achieving SOTA performance with only 1B parameters, significantly reducing deployment costs.\n\n- ğŸ“‘ **Comprehensive OCR Capabilities**: A single model covering classic OCR tasks including text detection and recognition, complex document parsing, open-field information extraction and video subtitle extraction, while supporting end-to-end photo translation and document QA.\n\n- ğŸš€ **Ultimate Usability**: Deeply embraces the \"end-to-end\" philosophy of large models - achieving SOTA results with single instruction and single inference, offering greater efficiency and convenience compared to industry cascade solutions.\n\n- ğŸŒ **Extensive Language Support**: Robust support for over 100 languages, excelling in both single-language and mixed-language scenarios across various document types.\n\n<div align=\"left\">\n  <img src=\"./assets/hyocr-pipeline-v1.png\" alt=\"HunyuanOCR framework\" width=\"80%\">\n</div>\n\n\n\n\n## ğŸ› ï¸ Dependencies and Installation\n\n### System Requirements\n- ğŸ–¥ï¸ Operating System: Linux\n- ğŸ Python: 3.12+ (recommended and tested)\n- âš¡ CUDA: 12.9\n- ğŸ”¥ PyTorch: 2.7.1\n- ğŸ® GPU: NVIDIA GPU with CUDA support\n- ğŸ§  GPU Memory: 20GB (for vLLM)\n- ğŸ’¾ Disk Space: 6GB\n\n## ğŸš€ Quick Start with vLLM (â­ Recommended)\n\n- **[HunyuanOCR Usage Guide](https://docs.vllm.ai/projects/recipes/en/latest/Tencent-Hunyuan/HunyuanOCR.html)**\n\n### Installation\n```bash\nuv venv hunyuanocr\nsource hunyuanocr/bin/activate\n\nuv pip install -U vllm --pre --extra-index-url https://wheels.vllm.ai/nightly\nuv pip install -r requirements.txt\n```\n\nNote: We suggest to install [cuda-compat-12-9](https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2404/x86_64/):\n```bash\nsudo dpkg -i cuda-compat-12-9_575.57.08-0ubuntu1_amd64.deb\necho 'export LD_LIBRARY_PATH=/usr/local/cuda-12.9/compat:$LD_LIBRARY_PATH' >> ~/.bashrc\nsource ~/.bashrc\n# verify cuda-compat-12-9\nls /usr/local/cuda-12.9/compat\n```\n\n### Model Deploy\n```bash\nvllm serve tencent/HunyuanOCR \\\n    --no-enable-prefix-caching \\\n    --mm-processor-cache-gb 0 \\\n    --gpu-memory-utilization 0.2\n```\n\n### Model Inference\n```python\nfrom vllm import LLM, SamplingParams\nfrom PIL import Image\nfrom transformers import AutoProcessor\n\ndef clean_repeated_substrings(text):\n    \"\"\"Clean repeated substrings in text\"\"\"\n    n = len(text)\n    if n<8000:\n        return text\n    for length in range(2, n // 10 + 1):\n        candidate = text[-length:] \n        count = 0\n        i = n - length\n        \n        while i >= 0 and text[i:i + length] == candidate:\n            count += 1\n            i -= length\n\n        if count >= 10:\n            return text[:n - length * (count - 1)]  \n\n    return text\n\nmodel_path = \"tencent/HunyuanOCR\"\nllm = LLM(model=model_path, trust_remote_code=True)\nprocessor = AutoProcessor.from_pretrained(model_path)\nsampling_params = SamplingParams(temperature=0, max_tokens=16384)\n\nimg_path = \"/path/to/image.jpg\"\nimg = Image.open(img_path)\nmessages = [\n    {\"role\": \"system\", \"content\": \"\"},\n    {\"role\": \"user\", \"content\": [\n        {\"type\": \"image\", \"image\": img_path},\n        {\"type\": \"text\", \"text\": \"æ£€æµ‹å¹¶è¯†åˆ«å›¾ç‰‡ä¸­çš„æ–‡å­—ï¼Œå°†æ–‡æœ¬åæ ‡æ ¼å¼åŒ–è¾“å‡ºã€‚\"}\n    ]}\n]\nprompt = processor.apply_chat_template(messag",
      "default_branch": "main"
    },
    "fetched_at": "2025-11-30T02:52:46.117435"
  },
  {
    "basic_info": {
      "name": "AgentEvolver",
      "full_name": "modelscope/AgentEvolver",
      "owner": "modelscope",
      "description": "AgentEvolver: Towards Efficient Self-Evolving Agent System",
      "url": "https://github.com/modelscope/AgentEvolver",
      "clone_url": "https://github.com/modelscope/AgentEvolver.git",
      "ssh_url": "git@github.com:modelscope/AgentEvolver.git",
      "homepage": "https://modelscope.github.io/AgentEvolver/",
      "created_at": "2025-11-13T08:09:51Z",
      "updated_at": "2025-11-29T22:18:15Z",
      "pushed_at": "2025-11-21T08:43:42Z"
    },
    "stats": {
      "stars": 727,
      "forks": 86,
      "watchers": 727,
      "open_issues": 6,
      "size": 15550
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 975095,
        "Shell": 11430,
        "Dockerfile": 4664
      },
      "license": "Apache License 2.0",
      "topics": [
        "agent",
        "agent-system",
        "llm",
        "reinforcement-learning",
        "self-evolving"
      ]
    },
    "content": {
      "readme": "<p align=\"center\">\n <img src=\"docs/img/logo.png\" alt=\"AgentEvolver Logo\" width=\"70%\">\n</p>\n<h2 align=\"center\">AgentEvolver: Towards Efficient Self-Evolving Agent System</h2>\n\n<!-- --- -->\n\n<p align=\"center\">\n  <!-- <a href=\"https://arxiv.org/abs/0000\"><img src=\"https://img.shields.io/badge/cs.MA-0000-B31C1C?logo=arxiv&logoColor=B31C1C\" alt=\"arxiv\"/></a> -->\n  <a href=\"https://www.python.org/\"><img src=\"https://img.shields.io/badge/python-3.11+-blue\" alt=\"Python Version\"></a>\n  <a href=\"./LICENSE\"><img src=\"https://img.shields.io/badge/license-Apache--2.0-black\" alt=\"License\"></a>\n  <a href=\"https://modelscope.github.io/AgentEvolver/\"><img src=\"https://img.shields.io/badge/docs-online-blue?logo=markdown\" alt=\"Documentation\"></a>\n  <a href=\"https://arxiv.org/abs/2511.10395\"><img src=\"https://img.shields.io/badge/arXiv-2511.10395-b31b1b.svg\" alt=\"arXiv\"></a>\n  <a href=\"https://deepwiki.com/modelscope/AgentEvolver\"><img src=\"https://deepwiki.com/badge.svg\" alt=\"deepwiki\"></a>\n  <a href=\"https://github.com/modelscope/AgentEvolver\"><img src=\"https://img.shields.io/github/stars/modelscope/AgentEvolver?style=social\" alt=\"GitHub Stars\"></a>\n</p>\n\n\n<!-- <p align=\"center\">\n  <strong>AgentEvolver: An Efficient Self-Evolving Agent System</strong><br>\n</p> -->\n\n**AgentEvolver** is an end-to-end, self-evolving training framework that unifies self-questioning, self-navigating, and self-attributing into a cohesive system. It empowers agents to autonomously\nimprove their capabilities, aiming for efficient, cost-effective, and continuous capability evolution.\n\n\n## ğŸ“° News\n\n- **[2025-11]** ğŸ“„ [The AgentEvolver Technical Report is now available](https://arxiv.org/abs/2511.10395), detailing the frameworkâ€™s architecture, methodology, and key findings.\n- **[2025-11]** ğŸ§© AgentEvolver v1 has been released now!\n\n\n## âœ¨ Why AgentEvolver\n\n\n\nğŸ§  AgentEvolver provides three **Self-Evolving Mechanisms** from Environment to Policy:\n\n- **Automatic Task Generation (Self-Questioning)** â€“ Explore the environment and autonomously create diverse tasks, eliminating costly manual dataset construction.\n- **Experience-guided Exploration (Self-Navigating)** â€“ Summarize and reuse cross-task experience, guiding higher-quality rollouts and improving exploration efficiency.\n- **Attribution-based Credit Assignment (Self-Attributing)** â€“ Process long trajectories to uncover the causal contribution of intermediate steps, enabling fine-grained and efficient policy optimization.\n\n<p align=\"center\">\n <img src=\"docs/img/flowchart.png\" alt=\"AgentEvolver Flowchart\" width=\"80%\">\n</p>\n\n\n\n\n## ğŸ”§ Architecture Design\nAgentEvolver adopts a service-oriented dataflow architecture, seamlessly integrating environment sandboxes, LLMs, and experience management into modular services.\n\n<p align=\"center\">\n <img src=\"docs/img/system.png\" alt=\"system framework\" width=\"80%\">\n</p>\n\n\n- **Environment Compatibility** â€“ Standardized interfaces for seamless integration with a wide range of external environments and tool APIs.\n- **Flexible Context Manager** â€“ Built-in utilities for managing multi-turn contexts and complex interaction logic, supporting diverse deployment scenarios.\n- **Modular & Extensible Architecture** â€“ Decoupled components allow easy customization, secondary development, and future algorithm upgrades.\n\n\n## ğŸŒŸ Benchmark Performance\n\nPerformance comparison on the AppWorld and BFCL-v3 benchmarks. AgentEvolver achieves superior results while using substantially fewer parameters than larger baseline models.\n\n<p align=\"center\">\n <img src=\"docs/img/performance.png\" alt=\"Benchmark Performance\" width=\"80%\">\n</p>\n\nPerformance on two benchmarks. Columns show avg@8 and best@8 for each benchmark, plus their averages (Avg.). All values are in percent (%). **Bolded numbers** highlight the best results.\n\n| **Model** | **Params** | **AppWorld** | | **BFCL v3** | | **Avg.** | |\n|:---|:---:|:---:|:---:|:---:|:---:|:---:|:---:|\n| | | avg@8 | best@8 | avg@8 | best@8 | avg@8 | best@8 |\n| Qwen2.5-7B | 7B | 1.8 | 5.6 | 29.8 | 42.4 | 15.8 | 24.0 |\n| +Questioning | 7B | 23.2 | 40.3 | 49.0 | 60.6 | 36.1 | 50.5 |\n| +Questioning&Navigating | 7B | 26.3 | 43.1 | 53.3 | 61.0 | 39.8 | 52.1 |\n| +Questioning&Attributing | 7B | 25.7 | 43.7 | 56.8 | 65.3 | 41.3 | 54.5 |\n| **AgentEvolver (overall)** | **7B** | **32.4** | **51.2** | **57.9** | **69.0** | **45.2** | **60.1** |\n| | | | | | | | |\n| Qwen2.5-14B | 14B | 18.0 | 31.4 | 41.6 | 54.1 | 29.8 | 42.8 |\n| +Questioning | 14B | 44.3 | 65.5 | 60.3 | 72.1 | 52.3 | 68.8 |\n| +Questioning&Navigating | 14B | 45.4 | 65.3 | 62.8 | 74.5 | 54.1 | 69.9 |\n| +Questioning&Attributing | 14B | 47.8 | 65.6 | 64.9 | 76.3 | 56.4 | 71.0 |\n| **AgentEvolver (overall)** | **14B** | **48.7** | **69.4** | **66.5** | **76.7** | **57.6** | **73.1** |\n\n\n## ğŸš€ Quick Start\n### Step 1. Basic Dependency Installation\n\nMake sure you have **conda** and **cuda toolkit** installed.\n\nThen, set up the training environment by running the script\n\n```bash\nbash install.sh\n```\n\n\n### Step 2. Setup Env-Serv",
      "default_branch": "main"
    },
    "fetched_at": "2025-11-30T02:52:47.212202"
  },
  {
    "basic_info": {
      "name": "Mini-Agent",
      "full_name": "MiniMax-AI/Mini-Agent",
      "owner": "MiniMax-AI",
      "description": "A minimal yet professional single agent demo project that showcases the core execution pipeline and production-grade features of agents.",
      "url": "https://github.com/MiniMax-AI/Mini-Agent",
      "clone_url": "https://github.com/MiniMax-AI/Mini-Agent.git",
      "ssh_url": "git@github.com:MiniMax-AI/Mini-Agent.git",
      "homepage": "https://www.minimax.io/",
      "created_at": "2025-10-31T03:56:27Z",
      "updated_at": "2025-11-30T02:01:10Z",
      "pushed_at": "2025-11-18T10:26:15Z"
    },
    "stats": {
      "stars": 699,
      "forks": 98,
      "watchers": 699,
      "open_issues": 6,
      "size": 4282
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 810622,
        "JavaScript": 45621,
        "HTML": 20844,
        "Shell": 15390,
        "PowerShell": 5180
      },
      "license": "MIT License",
      "topics": [
        "agent",
        "llm",
        "minimax"
      ]
    },
    "content": {
      "readme": "# Mini Agent\n\nEnglish | [ä¸­æ–‡](./README_CN.md)\n\n**Mini Agent** is a minimal yet professional demo project that showcases the best practices for building agents with the MiniMax M2 model. Leveraging an Anthropic-compatible API, it fully supports interleaved thinking to unlock M2's powerful reasoning capabilities for long, complex tasks.\n\nThis project comes packed with features designed for a robust and intelligent agent development experience:\n\n*   âœ… **Full Agent Execution Loop**: A complete and reliable foundation with a basic toolset for file system and shell operations.\n*   âœ… **Persistent Memory**: An active **Session Note Tool** ensures the agent retains key information across multiple sessions.\n*   âœ… **Intelligent Context Management**: Automatically summarizes conversation history to handle contexts up to a configurable token limit, enabling infinitely long tasks.\n*   âœ… **Claude Skills Integration**: Comes with 15 professional skills for documents, design, testing, and development.\n*   âœ… **MCP Tool Integration**: Natively supports MCP for tools like knowledge graph access and web search.\n*   âœ… **Comprehensive Logging**: Detailed logs for every request, response, and tool execution for easy debugging.\n*   âœ… **Clean & Simple Design**: A beautiful CLI and a codebase that is easy to understand, making it the perfect starting point for building advanced agents.\n\n## Table of Contents\n\n- [Mini Agent](#mini-agent)\n  - [Table of Contents](#table-of-contents)\n  - [Quick Start](#quick-start)\n    - [1. Get API Key](#1-get-api-key)\n    - [2. Choose Your Usage Mode](#2-choose-your-usage-mode)\n      - [ğŸš€ Quick Start Mode (Recommended for Beginners)](#-quick-start-mode-recommended-for-beginners)\n      - [ğŸ”§ Development Mode](#-development-mode)\n  - [ACP \\& Zed Editor Integration(optional)](#acp--zed-editor-integrationoptional)\n  - [Usage Examples](#usage-examples)\n    - [Task Execution](#task-execution)\n    - [Using a Claude Skill (e.g., PDF Generation)](#using-a-claude-skill-eg-pdf-generation)\n    - [Web Search \\& Summarization (MCP Tool)](#web-search--summarization-mcp-tool)\n  - [Testing](#testing)\n    - [Quick Run](#quick-run)\n    - [Test Coverage](#test-coverage)\n  - [Troubleshooting](#troubleshooting)\n    - [SSL Certificate Error](#ssl-certificate-error)\n    - [Module Not Found Error](#module-not-found-error)\n  - [Related Documentation](#related-documentation)\n  - [Contributing](#contributing)\n  - [License](#license)\n  - [References](#references)\n\n## Quick Start\n\n### 1. Get API Key\n\nMiniMax provides both global and China platforms. Choose based on your network environment:\n\n| Version    | Platform                                                       | API Base                   |\n| ---------- | -------------------------------------------------------------- | -------------------------- |\n| **Global** | [https://platform.minimax.io](https://platform.minimax.io)     | `https://api.minimax.io`   |\n| **China**  | [https://platform.minimaxi.com](https://platform.minimaxi.com) | `https://api.minimaxi.com` |\n\n**Steps to get API Key:**\n1. Visit the corresponding platform to register and login\n2. Go to **Account Management > API Keys**\n3. Click **\"Create New Key\"**\n4. Copy and save it securely (key is only shown once)\n\n> ğŸ’¡ **Tip**: Remember the API Base address corresponding to your chosen platform, you'll need it for configuration\n\n### 2. Choose Your Usage Mode\n\n**Prerequisites: Install uv**\n\nBoth usage modes require uv. If you don't have it installed:\n\n```bash\n# macOS/Linux/WSL\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n\n# Windows (PowerShell)\npython -m pip install --user pipx\npython -m pipx ensurepath\n# Restart PowerShell after installation\n\n# After installation, restart your terminal or run:\nsource ~/.bashrc  # or ~/.zshrc (macOS/Linux)\n```\n\nWe offer two usage modes - choose based on your needs:\n\n#### ğŸš€ Quick Start Mode (Recommended for Beginners)\n\nPerfect for users who want to quickly try Mini Agent without cloning the repository or modifying code.\n\n**Installation:**\n\n```bash\n# 1. Install directly from GitHub\nuv tool install git+https://github.com/MiniMax-AI/Mini-Agent.git\n\n# 2. Run setup script (automatically creates config files)\n# macOS/Linux:\ncurl -fsSL https://raw.githubusercontent.com/MiniMax-AI/Mini-Agent/main/scripts/setup-config.sh | bash\n\n# Windows (PowerShell):\nInvoke-WebRequest -Uri \"https://raw.githubusercontent.com/MiniMax-AI/Mini-Agent/main/scripts/setup-config.ps1\" -OutFile \"$env:TEMP\\setup-config.ps1\"\npowershell -ExecutionPolicy Bypass -File \"$env:TEMP\\setup-config.ps1\"\n```\n\n> ğŸ’¡ **Tip**: If you want to develop locally or modify code, use \"Development Mode\" below\n\n**Configuration:**\n\nThe setup script creates config files in `~/.mini-agent/config/`. Edit the config file:\n\n```bash\n# Edit config file\nnano ~/.mini-agent/config/config.yaml\n```\n\nFill in your API Key and corresponding API Base:\n\n```yaml\napi_key: \"YOUR_API_KEY_HERE\"          # API Key from step 1\napi_base: \"https://api.minimax.io\"  # Global\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-11-30T02:52:48.315429"
  },
  {
    "basic_info": {
      "name": "autonomous-researcher",
      "full_name": "mshumer/autonomous-researcher",
      "owner": "mshumer",
      "description": null,
      "url": "https://github.com/mshumer/autonomous-researcher",
      "clone_url": "https://github.com/mshumer/autonomous-researcher.git",
      "ssh_url": "git@github.com:mshumer/autonomous-researcher.git",
      "homepage": null,
      "created_at": "2025-11-19T17:55:50Z",
      "updated_at": "2025-11-29T19:26:00Z",
      "pushed_at": "2025-11-24T22:27:41Z"
    },
    "stats": {
      "stars": 686,
      "forks": 148,
      "watchers": 686,
      "open_issues": 3,
      "size": 337
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 123938,
        "TypeScript": 117696,
        "JavaScript": 3352,
        "CSS": 3158,
        "HTML": 694,
        "Dockerfile": 654
      },
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "# AI Researcher\n[![Twitter Follow](https://img.shields.io/twitter/follow/mattshumer_?style=social)](https://twitter.com/mattshumer_)\n\n[Be the first to know when I publish new AI builds + demos!](https://tally.so/r/w2M17p)\n\nAn autonomous AI researcher. It takes a research objective, breaks it into experiments, spins up separate agents with access to their own GPUs to run these experiments, and delivers a paper-style writeup with findings.\n\n## How it Works\n- Decomposes your prompt into experiments and assigns them to specialist researcher agents.\n- Each agent can launch GPU-enabled sandboxes to train models/run inference/etc., evaluate, and collect evidence.\n- Based on the results of these experiments, the orchestrator can decide to finalize, or run more experiments.\n- The orchestrator goes over all of the results and turns them into a coherent \"paper\".\n\n## Run it (web notebook, one command)\nThe fastest way to use it:\n```\npython run_app.py\n```\nThis installs missing deps, starts the API + frontend, and opens the notebook. If Google/Modal keys arenâ€™t set, the UI will prompt you and save them locally before the run starts.\n\n## Keys Needed\n- **LLM key** (at least one):\n  - Google AI Studio: `GOOGLE_API_KEY` (for Gemini 3 Pro)\n  - Anthropic: `ANTHROPIC_API_KEY` (for Claude Opus 4.5)\n- **Modal tokens**: `MODAL_TOKEN_ID` and `MODAL_TOKEN_SECRET` (for GPU sandboxes)\n- Add them to `.env` in the repo root, or paste them into the web prompt when asked.\n\n## Model Selection\nChoose between **Gemini 3 Pro** and **Claude Opus 4.5** from the dropdown in the web UI, or via CLI with `--model`.\n\n## Optional CLI\nPrefer the terminal?\n```\npython -m venv venv && source venv/bin/activate\npip install -r requirements.txt\npython main.py \"Does label smoothing improve ViT-Base on CIFAR-10?\" --mode single --gpu any --model gemini-3-pro-preview\n```\nOrchestrator (multi-agent):\n```\npython main.py \"Characterize scaling laws for sparse attention transformers\" \\\n  --mode orchestrator --num-agents 3 --max-rounds 3 --max-parallel 2 --gpu any\n```\nDry run:\n```\npython main.py \"Sanity check the pipeline\" --mode orchestrator --test-mode\n```\n\n## Deploy to Railway\n\n[![Deploy on Railway](https://railway.app/button.svg)](https://railway.app/new/template?template=https://github.com/mattshumer/ai-researcher&referralCode=mattshumer)\n\n**Steps:**\n1. Click the button above (or go to Railway and select \"Deploy from GitHub repo\")\n2. Connect your GitHub account and select this repo (or your fork)\n3. Railway will automatically detect the Dockerfile and build the app\n4. Once deployed, open the app URL and enter your API keys in the UI\n\n**Optional environment variables** (if you want server-side defaults):\n- `GOOGLE_API_KEY` - Google AI Studio key for Gemini 3 Pro\n- `ANTHROPIC_API_KEY` - Anthropic key for Claude Opus 4.5\n- `MODAL_TOKEN_ID` and `MODAL_TOKEN_SECRET` - For GPU sandboxes\n\nNote: Users can also enter their own keys directly in the web UI without setting environment variables.\n\n## Status/Contribution\nThis is a super-early, experimental harness. There are a number of improvements to be worked out (i.e. dataset sharing between agents, key management, etc.), literature search, that would make this way more capable. If anyone wants to add these in, feel free!",
      "default_branch": "main"
    },
    "fetched_at": "2025-11-30T02:52:49.429559"
  },
  {
    "basic_info": {
      "name": "codexmcp",
      "full_name": "GuDaStudio/codexmcp",
      "owner": "GuDaStudio",
      "description": "Enable seamless collaboration between Claude Code and Codex, transforming from a single agent to multiple agents for significantly enhanced productivity!",
      "url": "https://github.com/GuDaStudio/codexmcp",
      "clone_url": "https://github.com/GuDaStudio/codexmcp.git",
      "ssh_url": "git@github.com:GuDaStudio/codexmcp.git",
      "homepage": "https://code.guda.studio",
      "created_at": "2025-11-05T16:46:30Z",
      "updated_at": "2025-11-29T20:34:56Z",
      "pushed_at": "2025-11-24T06:18:21Z"
    },
    "stats": {
      "stars": 681,
      "forks": 40,
      "watchers": 681,
      "open_issues": 18,
      "size": 2098
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 10044
      },
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "![è¿™æ˜¯å›¾ç‰‡](./images/title.png)\n\n<div align=\"center\">\n\n\n**è®© Claude Code ä¸ Codex æ— ç¼åä½œ** \n\n[![License: MIT](https://img.shields.io/badge/License-MIT-blue.svg)](https://opensource.org/licenses/MIT) [![Python Version](https://img.shields.io/badge/python-3.12+-blue.svg)](https://www.python.org/downloads/) [![MCP Compatible](https://img.shields.io/badge/MCP-Compatible-green.svg)](https://modelcontextprotocol.io)[![Share](https://img.shields.io/badge/share-000000?logo=x&logoColor=white)](https://x.com/intent/tweet?text=CodexMCPï¼šè®©%20Claude%20Code%20ä¸%20Codex%20æ— ç¼åä½œ%20https://github.com/GuDaStudio/codexmcp%20%23AI%20%23Coding%20%23MCP) [![Share](https://img.shields.io/badge/share-1877F2?logo=facebook&logoColor=white)](https://www.facebook.com/sharer/sharer.php?u=https://github.com/GuDaStudio/codexmcp) [![Share](https://img.shields.io/badge/share-FF4500?logo=reddit&logoColor=white)](https://www.reddit.com/submit?title=CodexMCPï¼šè®©%20Claude%20Code%20ä¸%20Codex%20æ— ç¼åä½œ&url=https://github.com/GuDaStudio/codexmcp) [![Share](https://img.shields.io/badge/share-0088CC?logo=telegram&logoColor=white)](https://t.me/share/url?url=https://github.com/GuDaStudio/codexmcp&text=CodexMCPï¼šè®©%20Claude%20Code%20ä¸%20Codex%20æ— ç¼åä½œ)\n\n\nâ­ åœ¨GitHubä¸Šç»™æˆ‘ä»¬ç‚¹æ˜Ÿ~æ‚¨çš„æ”¯æŒå¯¹æˆ‘ä»¬æ„ä¹‰é‡å¤§ï¼ ğŸ™ğŸ˜Š\n\n[English](./docs/README_EN.md) | ç®€ä½“ä¸­æ–‡\n\n</div>\n\n---\n\n## ä¸€ã€é¡¹ç›®ç®€ä»‹ \n\nåœ¨å½“å‰ AI è¾…åŠ©ç¼–ç¨‹ç”Ÿæ€ä¸­ï¼Œ**Claude Code** æ“…é•¿æ¶æ„è®¾è®¡ä¸å…¨å±€æ€è€ƒï¼Œè€Œ **Codex** åœ¨ä»£ç ç”Ÿæˆä¸ç»†èŠ‚ä¼˜åŒ–ä¸Šè¡¨ç°å“è¶Šã€‚**CodexMCP** ä½œä¸ºä¸¤è€…ä¹‹é—´çš„æ¡¥æ¢ï¼Œé€šè¿‡ MCP åè®®è®©å®ƒä»¬ä¼˜åŠ¿äº’è¡¥ï¼š\n\n- **Claude Code**ï¼šè´Ÿè´£éœ€æ±‚åˆ†æã€æ¶æ„è§„åˆ’ã€ä»£ç é‡æ„\n- **Codex**ï¼šè´Ÿè´£ç®—æ³•å®ç°ã€bug å®šä½ã€ä»£ç å®¡æŸ¥\n- **CodexMCP**ï¼šç®¡ç†ä¼šè¯ä¸Šä¸‹æ–‡ï¼Œæ”¯æŒå¤šè½®å¯¹è¯ä¸å¹¶è¡Œä»»åŠ¡\n\nç›¸æ¯”å®˜æ–¹ Codex MCP å®ç°ï¼ŒCodexMCP å¼•å…¥äº†**ä¼šè¯æŒä¹…åŒ–**ã€**å¹¶è¡Œæ‰§è¡Œ**å’Œ**æ¨ç†è¿½è¸ª**ç­‰ä¼ä¸šçº§ç‰¹æ€§ï¼Œè®© AI ç¼–ç¨‹åŠ©æ‰‹ä¹‹é—´çš„åä½œæ›´åŠ æ™ºèƒ½é«˜æ•ˆã€‚CodexMCP ä¸å®˜æ–¹ Codex MCP åŒºåˆ«ä¸€è§ˆï¼š\n\n\n| ç‰¹æ€§ | å®˜æ–¹ç‰ˆ | CodexMCP |\n|------|--------|----------|\n| åŸºæœ¬ Codex è°ƒç”¨ | âˆš | âˆš |\n| å¤šè½®å¯¹è¯ | Ã— | âˆš |\n| æ¨ç†è¯¦æƒ…è¿½è¸ª | Ã— | âˆš |\n| å¹¶è¡Œä»»åŠ¡æ”¯æŒ | Ã—  | âˆš  |\n| é”™è¯¯å¤„ç† | Ã—  | âˆš  |\n\n\n---\n\n## äºŒã€å¿«é€Ÿå¼€å§‹\n\n### 0. å‰ç½®è¦æ±‚\n\nè¯·ç¡®ä¿æ‚¨å·²æˆåŠŸ**å®‰è£…**å’Œ**é…ç½®**claude codeä¸codexä¸¤ä¸ªç¼–ç¨‹å·¥å…·ã€‚\n- [Claude Code å®‰è£…æŒ‡å—](https://docs.claude.com/docs/claude-code)\n- [Codex CLI å®‰è£…æŒ‡å—](https://developers.openai.com/codex/quickstart)\n\n\nè¯·ç¡®ä¿æ‚¨å·²æˆåŠŸå®‰è£…[uvå·¥å…·](https://docs.astral.sh/uv/getting-started/installation/)ï¼š\n\n- Windows\n  åœ¨Powershellä¸­è¿è¡Œä»¥ä¸‹å‘½ä»¤ï¼š\n  ```\n  powershell -ExecutionPolicy ByPass -c \"irm https://astral.sh/uv/install.ps1 | iex\"\n  ```\n\n- Linux/macOS\n  ä½¿ç”¨curl/wgetä¸‹è½½å¹¶å®‰è£…:\n  ```\n  curl -LsSf https://astral.sh/uv/install.sh | sh #ä½¿ç”¨curl\n\n  wget -qO- https://astral.sh/uv/install.sh | sh #ä½¿ç”¨wget\n  ```\n\n**æ³¨æ„ï¼Œæˆ‘ä»¬æåŠ›æ¨èWindowsç”¨æˆ·åœ¨WSLä¸­è¿è¡Œæœ¬é¡¹ç›®ï¼**\n\n<!-- å¦‚æœæ‚¨æ­£åœ¨ä¸ºè®¢é˜…å’Œé…ç½®è€Œå¿§æ„ï¼Œæˆ‘ä»¬éå¸¸æ¬¢è¿æ‚¨[ç§¯æè”ç³»æˆ‘ä»¬](https://cc.guda.studio)ã€‚ -->\n\n### 1. å®‰è£…æ­¥éª¤\n\n**1.1** ç§»é™¤å®˜æ–¹ Codex MCPï¼ˆå¦‚æœå·²å®‰è£…ï¼‰ã€‚\n\n```bash\nclaude mcp remove codex\n```\n\n**1.2** å®‰è£… CodexMCPã€‚\n\n```bash\nclaude mcp add codex -s user --transport stdio -- uvx --from git+https://github.com/GuDaStudio/codexmcp.git codexmcp\n```\n\n**1.3** éªŒè¯å®‰è£…ã€‚åœ¨ **ç»ˆç«¯** ä¸­è¿è¡Œï¼š\n\n```\nclaude mcp list\n```\n\n> [!IMPORTANT]\n> å¦‚æœçœ‹åˆ°å¦‚ä¸‹æè¿°ï¼Œè¯´æ˜å®‰è£…æˆåŠŸï¼\n>  `codex: uvx --from git+https://github.com/GuDaStudio/codexmcp.git codexmcp - âœ“ Connected` \n\n**1.4** å¯é€‰æ‹©é»˜è®¤å…è®¸claude codeè‡ªåŠ¨ä¸codexäº¤äº’ï¼Œåœ¨`~/.claude/settings.json`æ·»åŠ  `mcp__codex__codex` allowé¡¹\n![](images/image.png)\n\n### 2. é…ç½®claude codeæç¤ºè¯ï¼ˆå¯é€‰ï¼‰\n\nä¸ºä½¿claude codeæ›´å¥½å®Œæˆä¸codexçš„ååŒå·¥ä½œï¼Œæˆ‘ä»¬**æåŠ›æ¨èæ‚¨åœ¨ `~/.claude/CLAUDE.md`ä¸­æ·»åŠ ä»¥ä¸‹å†…å®¹**\n\n<details>\n<summary>å±•å¼€æŸ¥çœ‹prompt</summary>\n\n\n```\n## Core Instruction for CodeX MCP\n\nåœ¨ä»»ä½•æ—¶åˆ»ï¼Œä½ å¿…é¡»æ€è€ƒå½“å‰è¿‡ç¨‹å¯ä»¥å¦‚ä½•ä¸codexè¿›è¡Œåä½œï¼Œå¦‚ä½•è°ƒç”¨Codex ä¸ºä½ æä¾›çš„MCPå·¥å…·ä½œä¸ºä½ å®¢è§‚å…¨é¢åˆ†æçš„ä¿éšœã€‚\nå…¶ä¸­ä½ **åŠ¡å¿…æ‰§è¡Œ**ä»¥ä¸‹å‡ ä¸ªæ­¥éª¤ï¼š\n**1** åœ¨ä½ å¯¹ç”¨æˆ·éœ€æ±‚å½¢æˆåˆæ­¥åˆ†æåï¼Œå°†ç”¨æˆ·éœ€æ±‚ã€åˆå§‹æ€è·¯å‘ŠçŸ¥codexï¼Œå¹¶è¦æ±‚å…¶å®Œå–„éœ€æ±‚åˆ†æå’Œå®æ–½è®¡åˆ’ã€‚\n**2** åœ¨å®æ–½å…·ä½“ç¼–ç ä»»åŠ¡å‰ï¼Œ**å¿…é¡»å‘codexç´¢è¦ä»£ç å®ç°åŸå‹ï¼ˆè¦æ±‚codexä»…ç»™å‡ºunified diff patchï¼Œä¸¥ç¦å¯¹ä»£ç åšä»»ä½•çœŸå®ä¿®æ”¹ï¼‰**ã€‚åœ¨è·å–ä»£ç åŸå‹åï¼Œä½ **åªèƒ½ä»¥æ­¤ä¸ºé€»è¾‘å‚è€ƒï¼Œå†æ¬¡å¯¹ä»£ç ä¿®æ”¹è¿›è¡Œé‡å†™**ï¼Œå½¢æˆä¼ä¸šç”Ÿäº§çº§åˆ«ã€å¯è¯»æ€§æé«˜ã€å¯ç»´æŠ¤æ€§æé«˜çš„ä»£ç åï¼Œæ‰èƒ½å®æ–½å…·ä½“ç¼–ç¨‹ä¿®æ”¹ä»»åŠ¡ã€‚\n**3** æ— è®ºä½•æ—¶ï¼Œåªè¦å®Œæˆåˆ‡å®ç¼–ç è¡Œä¸ºåï¼Œ**å¿…é¡»ç«‹å³ä½¿ç”¨codex reviewä»£ç æ”¹åŠ¨å’Œå¯¹åº”éœ€æ±‚å®Œæˆç¨‹åº¦**ã€‚\n**4** codexåªèƒ½ç»™å‡ºå‚è€ƒï¼Œä½ **å¿…é¡»æœ‰è‡ªå·±çš„æ€è€ƒï¼Œç”šè‡³éœ€è¦å¯¹codexçš„å›ç­”æå‡ºç½®ç–‘**ã€‚å°½ä¿¡ä¹¦åˆ™ä¸å¦‚æ— ä¹¦ï¼Œä½ ä¸codexçš„æœ€ç»ˆä½¿å‘½éƒ½æ˜¯è¾¾æˆç»Ÿä¸€ã€å…¨é¢ã€ç²¾å‡†çš„æ„è§ï¼Œæ‰€ä»¥ä½ ä»¬å¿…é¡»ä¸æ–­äº‰è¾©å·²æ‰¾åˆ°é€šå‘çœŸç†çš„å”¯ä¸€é€”å¾„ã€‚\n\n\n## Codex Tool Invocation Specification\n\n 1. å·¥å…·æ¦‚è¿°\n\n  codex MCP æä¾›äº†ä¸€ä¸ªå·¥å…· `codex`ï¼Œç”¨äºæ‰§è¡Œ AI è¾…åŠ©çš„ç¼–ç ä»»åŠ¡ã€‚è¯¥å·¥å…·**é€šè¿‡ MCP åè®®è°ƒç”¨**ï¼Œæ— éœ€ä½¿ç”¨å‘½ä»¤è¡Œã€‚\n\n  2. å·¥å…·å‚æ•°\n\n  **å¿…é€‰**å‚æ•°ï¼š\n  - PROMPT (string): å‘é€ç»™ codex çš„ä»»åŠ¡æŒ‡ä»¤\n  - cd (Path): codex æ‰§è¡Œä»»åŠ¡çš„å·¥ä½œç›®å½•æ ¹è·¯å¾„\n\n  å¯é€‰å‚æ•°ï¼š\n  - sandbox (string): æ²™ç®±ç­–ç•¥ï¼Œå¯é€‰å€¼ï¼š\n    - \"read-only\" (é»˜è®¤): åªè¯»æ¨¡å¼ï¼Œæœ€å®‰å…¨\n    - \"workspace-write\": å…è®¸åœ¨å·¥ä½œåŒºå†™å…¥\n    - \"danger-full-access\": å®Œå…¨è®¿é—®æƒé™\n  - SESSION_ID (UUID | null): ç”¨äºç»§ç»­ä¹‹å‰çš„ä¼šè¯ä»¥ä¸codexè¿›è¡Œå¤šè½®äº¤äº’ï¼Œé»˜è®¤ä¸º Noneï¼ˆå¼€å¯æ–°ä¼šè¯ï¼‰\n  - skip_git_repo_check (boolean): æ˜¯å¦å…è®¸åœ¨é Git ä»“åº“ä¸­è¿è¡Œï¼Œé»˜è®¤ False\n  - return_all_messages (boolean): æ˜¯å¦è¿”å›æ‰€æœ‰æ¶ˆæ¯ï¼ˆåŒ…æ‹¬æ¨ç†ã€å·¥å…·è°ƒç”¨ç­‰ï¼‰ï¼Œé»˜è®¤ False\n  - image (List[Path] | null): é™„åŠ ä¸€ä¸ªæˆ–å¤šä¸ªå›¾ç‰‡æ–‡ä»¶åˆ°åˆå§‹æç¤ºè¯ï¼Œé»˜è®¤ä¸º None\n  - model (string | null): æŒ‡å®šä½¿ç”¨çš„æ¨¡å‹ï¼Œé»˜è®¤ä¸º Noneï¼ˆä½¿ç”¨ç”¨æˆ·é»˜è®¤é…ç½®ï¼‰\n  - yolo (boolean | null): æ— éœ€å®¡æ‰¹è¿è¡Œæ‰€æœ‰å‘½ä»¤ï¼ˆè·³è¿‡æ²™ç®±ï¼‰ï¼Œé»˜è®¤ False\n  - profile (string | null): ä» `~/.codex/config.toml` åŠ è½½çš„é…ç½®æ–‡ä»¶åç§°ï¼Œé»˜è®¤ä¸º Noneï¼ˆä½¿ç”¨ç”¨æˆ·é»˜è®¤é…ç½®ï¼‰\n\n  è¿”å›å€¼ï¼š\n  {\n    \"success\": true,\n    \"SESSION_ID\": \"uuid-string\",\n    \"agent_messages\": \"agentå›å¤çš„æ–‡æœ¬å†…å®¹\",\n    \"all_messages\": []  // ä»…å½“ return_all_messages=True æ—¶åŒ…å«\n  }\n  æˆ–å¤±è´¥æ—¶ï¼š\n  {\n    \"success\": false,\n    \"error\": \"é”™è¯¯ä¿¡æ¯\"\n  }\n\n  3. ä½¿ç”¨æ–¹å¼\n\n  å¼€å¯æ–°å¯¹è¯ï¼š\n  - ä¸ä¼  SESSION_ID å‚æ•°ï¼ˆæˆ–ä¼  Noneï¼‰\n  - å·¥å…·ä¼šè¿”å›æ–°çš„ SESSION_ID ç”¨äºåç»­å¯¹è¯\n\n  ç»§ç»­ä¹‹å‰çš„å¯¹è¯ï¼š\n  - å°†ä¹‹å‰è¿”å›çš„ SESSION_ID ä½œä¸ºå‚æ•°ä¼ å…¥\n  - åŒä¸€ä¼šè¯çš„ä¸Šä¸‹æ–‡ä¼šè¢«ä¿ç•™\n\n  4. è°ƒç”¨è§„èŒƒ\n\n  **å¿…é¡»éµå®ˆ**ï¼š\n  - æ¯æ¬¡è°ƒç”¨ codex å·¥å…·æ—¶ï¼Œå¿…é¡»ä¿å­˜è¿”å›çš„ SESSION_IDï¼Œä»¥ä¾¿åç»­ç»§ç»­å¯¹è¯\n  - cd å‚æ•°å¿…é¡»æŒ‡å‘å­˜åœ¨çš„ç›®å½•ï¼Œå¦åˆ™å·¥å…·ä¼šé™é»˜å¤±è´¥\n  - ä¸¥ç¦codexå¯¹ä»£ç è¿›è¡Œå®é™…ä¿®æ”¹ï¼Œä½¿ç”¨ sandbox=\"read-only\" ä»¥é¿å…æ„å¤–ï¼Œå¹¶è¦æ±‚codexä»…ç»™å‡ºunified diff patchå³",
      "default_branch": "main"
    },
    "fetched_at": "2025-11-30T02:52:50.542718"
  },
  {
    "basic_info": {
      "name": "codeassist",
      "full_name": "gensyn-ai/codeassist",
      "owner": "gensyn-ai",
      "description": "A completely private and local AI coding assistant, developed by Gensyn. It helps you practice programming problems and train a novel assistant to help you code.",
      "url": "https://github.com/gensyn-ai/codeassist",
      "clone_url": "https://github.com/gensyn-ai/codeassist.git",
      "ssh_url": "git@github.com:gensyn-ai/codeassist.git",
      "homepage": null,
      "created_at": "2025-10-31T14:03:09Z",
      "updated_at": "2025-11-30T00:37:26Z",
      "pushed_at": "2025-11-13T14:23:42Z"
    },
    "stats": {
      "stars": 680,
      "forks": 120,
      "watchers": 680,
      "open_issues": 14,
      "size": 23696
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 500590,
        "TypeScript": 265570,
        "CSS": 6326,
        "Dockerfile": 4578,
        "JavaScript": 2247,
        "Nix": 842,
        "Shell": 157
      },
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "\n# CodeAssist - AI Programming Assistant\n\nCodeAssist is a completely private and local AI coding assistant, developed by Gensyn. It helps you practice programming problems and train a novel assistant to help you code.\n\nUnlike typical code assistants, CodeAssist writes directly in your editor as you work. Every keystroke - whether you type, fix, delete, or leave its output untouched - becomes a learning signal. Over time, it adapts to your habits and style, acting more like an apprentice learning from your craft than a tool following commands.\n\n[Docs](https://docs.gensyn.ai/testnet/codeassist) | [Tutorial](https://docs.gensyn.ai/testnet/codeassist/using-codeassist) | [Leaderboard](https://dashboard.gensyn.ai/?application=CodeAssist)\n\n# Installation\n\nGet started with installing CodeAssist.\n\n## Docker\n\nInstall [Docker](https://docs.docker.com/engine/install/) on your system, according to the instructions for your machine.\n\n## Python\n\nPython is required to run the main script that handles your environment. We require a version no older than 3.10.\n\n## UV\n\nUV is required to manage the dependencies of the main script. It can be installed with the following steps:\n\n### MacOS\n\n```bash\nbrew install uv\n```\n\n### Linux (or alternate MacOS install, for those without Brew)\n\n```bash\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n```\n\n# Downloading the Code\n\nTo download the code, simply clone the repository:\n\n```bash\ngit clone https://github.com/gensyn-ai/codeassist.git\ncd codeassist\n```\n\n# Running\n\nTo run CodeAssist, simply execute the following command:\n\n```bash\nuv run run.py\n```\n\n## HuggingFace Token\n\nTo start CodeAssist, you will need to have a HuggingFace token. Follow [these instructions](https://huggingface.co/docs/hub/en/security-tokens) and generate a token with `Write` access.\n\n## Web UI\n\nAfter the script is running, your browser should open automatically but if it doesn't, open a window and go to [localhost:3000](http://localhost:3000) to open CodeAssist.\n\nWhen the web UI loads, you'll see a login modal where you can log in with email (which sends a one-time passcode) or with Google. After logging in for the first time, your local credentials will be stored in `persistent-data/auth/userKeyMap.json`.\n\nOnce logged in, you can select Easy, Medium, or Hard problems from the sidebar. CodeAssist will begin recording an episode. Every click, keystroke, edit, or deletion is logged as training feedback.\n\nWhen you stop typing, CodeAssist takes initiative. It writes directly into your file without any pop-ups or confirmations. Whether you accept, modify, or remove its edits, each interaction contributes to the modelâ€™s understanding of your preferences.\n\n### Tips & Tricks\n\n- Use `Shift+Space` or click the Pause Assistant button to temporarily stop the assistant. The first keystroke after pausing will unpause it.\n- Keep your cursor near the section you're working on, as CodeAssist inserts code relative to your cursor position.\n- When the assistant produces a \"No-Op\" (does nothing), it's waiting for you. This is intentional and signals it's your turn to act.\n\n## Training\n\nCodeAssist continuously records your interactions while the web UI is running. To complete an episode and train your model, press `Ctrl+C` in the terminal where CodeAssist is running.\n\nYou do not need to successfully solve a LeetCode problem to train the model. You can stop recording the episode by leaving the CodeAssist web UI, returning to the terminal CodeAssist is running in, and using the `ctrl+c` command to start training.\n\nDuring training, CodeAssist will:\n- Compare your edits to the assistant's actions\n- Calculate rewards and penalties based on your interactions\n- Update your local model checkpoint\n- Store new model weights under `persistent-data/trainer/models`\n- Upload your trained model to Hugging Face (if a valid token is provided)\n\nAfter training completes (which takes a few minutes depending on your system), you can restart CodeAssist to use your updated model trained on your most recent episode.\n\n### Best Practices\n\n- **Be patient**: The assistant watches your typing and timing. Working too quickly or aggressively correcting can neutralize training efficacy.\n- **Treat it as a collaborator**: Let it naturally interject code and keep useful code around briefly before editing or removing.\n- **Don't delete everything instantly**: If you delete everything it writes right away, you're teaching it to stop acting altogether.\n- **Record multiple varied problems**: Diversify its learning signals by working on different problems.\n- **Expect gradual improvement**: Early episodes may feel inconsistent. Improvement becomes clearer after 4-5 episodes of training.\n\n# Troubleshooting\n\n## Exception: Container <container-name> is unhealthy.\n\nThis occurs when a container fails to boot. You can view the logs by running `docker logs <container-name>`. Please review and upload the logs when creating a new issue.\n\n## Error connecting to Docker daemon\n\n```\n2025-09-04 ",
      "default_branch": "main"
    },
    "fetched_at": "2025-11-30T02:52:51.643659"
  },
  {
    "basic_info": {
      "name": "lejepa",
      "full_name": "rbalestr-lab/lejepa",
      "owner": "rbalestr-lab",
      "description": null,
      "url": "https://github.com/rbalestr-lab/lejepa",
      "clone_url": "https://github.com/rbalestr-lab/lejepa.git",
      "ssh_url": "git@github.com:rbalestr-lab/lejepa.git",
      "homepage": null,
      "created_at": "2025-11-11T14:29:00Z",
      "updated_at": "2025-11-29T16:43:20Z",
      "pushed_at": "2025-11-20T16:38:29Z"
    },
    "stats": {
      "stars": 676,
      "forks": 51,
      "watchers": 676,
      "open_issues": 13,
      "size": 4034
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 274017
      },
      "license": "Other",
      "topics": []
    },
    "content": {
      "readme": "# LeJEPA\n**Lean Joint-Embedding Predictive Architecture (LeJEPA): Provable and Scalable Self-Supervised Learning Without the Heuristics**\n[GitHub Repository](https://github.com/rbalestr-lab/lejepa)  \n[arXiv:2511.08544](https://arxiv.org/abs/2511.08544)\n---\n\nRush to our [minimal working example](MINIMAL.md) to see a full-fledge working example (ViT, inet).\n\n## Demo\n\n<img src=\"eval/output1.gif\" controls width=\"400\">\n<img src=\"eval/output2.gif\" controls width=\"400\">\n<img src=\"eval/output3.gif\" controls width=\"400\">\n<table>\n  <tr>\n    <td><img src=\"eval/n01818515_919_original.png\" width=\"200\"/></td>\n    <td><img src=\"eval/n01818515_919_pca.png\" width=\"200\"/></td>\n  </tr>\n  <tr>\n    <td><img src=\"eval/n01818515_14304_original.png\" width=\"200\"/></td>\n    <td><img src=\"eval/n01818515_14304_pca.png\" width=\"200\"/></td>\n  </tr>\n</table>\n\n| shots | model                  | params | pretrain | epochs | DTD      | aircr.   | cars     | cifar10  | cifar100 | flowers102 | food     | pets     | avg.    |\n|-------|------------------------|--------|----------|--------|----------|----------|----------|----------|----------|------------|----------|----------|---------|\n| 1     | LeJEPA ViT-L           | 304M   | IN-1K    | 100    | **33.21**| 9.37     | 3.40     | 51.65    | 27.01    | 48.53      | 17.14    | 46.11    | 29.55   |\n| 1     | LeJEPA ConvNeXtV2-H    | 660M   | IN-1K    | 100    | 32.15    | 8.07     | 4.28     | 50.95    | **31.48**| **48.74**  | **17.95**| **58.98**| **31.58**|\n| 1     | I-JEPA ViT-H           | 632M   | IN-1K    | 300    | 27.71    | **9.86** | **4.33** | **56.52**| 30.58    | 44.69      | 14.53    | 53.38    | 30.20   |\n| 10    | LeJEPA ViT-L           | 304M   | IN-1K    | 100    | **64.72**| **35.25**| 22.25    | 85.15    | 59.77    | **92.53**  | **50.90**| 77.00    | **60.95**|\n| 10    | LeJEPA ConvNeXtV2-H    | 660M   | IN-1K    | 100    | 61.84    | 30.67    | **24.46**| 85.74    | 63.29    | 91.78      | 49.32    | 78.53    | 60.70   |\n| 10    | I-JEPA ViT-H           | 632M   | IN-1K    | 300    | 57.68    | 33.82    | 21.96    | **88.77**| **66.42**| 88.24      | 43.97    | **83.23**| 60.51   |\n| all   | LeJEPA ViT-L           | 304M   | IN-1K    | 100    | **78.30**| 57.01    | **57.28**| 96.50    | 83.71    | **91.21**  | **82.05**| 89.74    | **79.48**|\n| all   | LeJEPA ConvNeXtV2-H    | 660M   | IN-1K    | 100    | 76.60    | 52.99    | 54.88    | 96.15    | 81.34    | 91.11      | 77.64    | 89.76    | 77.56   |\n| all   | I-JEPA ViT-H           | 632M   | IN-1K    | 300    | 73.32    | **56.61**| 54.47    | **97.54**| **86.42**| 86.47      | 81.02    | **92.11**| 78.50   |\n\n## Overview\nLeJEPA is a lean, scalable, and theoretically grounded framework for self-supervised representation learning, based on Joint-Embedding Predictive Architectures (JEPAs). LeJEPA introduces **Sketched Isotropic Gaussian Regularization (SIGReg)**, a novel objective that constrains learned embeddings to an optimal isotropic Gaussian distribution, minimizing downstream prediction risk.\n**Key Features:**\n- Single trade-off hyperparameter\n- Linear time and memory complexity\n- Stable training across architectures and domains\n- Heuristics-free implementation (no stop-gradient, teacherâ€“student, or schedulers)\n- Distributed training-friendly codebase (~50 lines of core code)\n- State-of-the-art results across 10+ datasets and 60+ architectures\n---\n\n## GOTO hyperparameters\n\n\nOur data augmentation strategy follows a multi-crop approach inspired by DINO, where we generate multiple views of each image at different scales to encourage the model to learn both global semantic information and local fine-grained features.\n\n### Data augmentation and views\n\nEach training image is augmented to produce **2 global views** and **6 local views** with different spatial scales but the same set of color and geometric transformations:\n| **Global Views** | **Local Views** |\n|------------------|-----------------|\n| **RandomResizedCrop**<br>- Resolution: 224x224<br>- Scale: (0.3, 1.0)<br>- Covers 30-100% of the image | **RandomResizedCrop**<br>- Resolution: 98x98<br>- Scale: (0.05, 0.3)<br>- Covers 5-30% of the image |\n| **RandomHorizontalFlip** (p=0.5) | **RandomHorizontalFlip** (p=0.5) |\n| **ColorJitter** (p=0.8)<br>- Brightness: 0.4<br>- Contrast: 0.4<br>- Saturation: 0.2<br>- Hue: 0.1 | **ColorJitter** (p=0.8)<br>- Brightness: 0.4<br>- Contrast: 0.4<br>- Saturation: 0.2<br>- Hue: 0.1 |\n| **RandomGrayscale** (p=0.2) | **RandomGrayscale** (p=0.2) |\n| **GaussianBlur** (p=0.5) | **GaussianBlur** (p=0.5) |\n| **RandomSolarize** (p=0.2, threshold=128) | **RandomSolarize** (p=0.2, threshold=128) |\n| **Normalization** (mean, std) | **Normalization** (mean, std) |\n\n\nThe key difference between global and local views is the **cropping scale**: global views capture larger portions of the image to learn high-level semantics, while local views focus on smaller regions to learn fine-grained local patterns. All other augmentations are applied iden",
      "default_branch": "main"
    },
    "fetched_at": "2025-11-30T02:52:52.775785"
  },
  {
    "basic_info": {
      "name": "dia2",
      "full_name": "nari-labs/dia2",
      "owner": "nari-labs",
      "description": "TTS model capable of streaming conversational audio in realtime.",
      "url": "https://github.com/nari-labs/dia2",
      "clone_url": "https://github.com/nari-labs/dia2.git",
      "ssh_url": "git@github.com:nari-labs/dia2.git",
      "homepage": "",
      "created_at": "2025-11-17T19:04:12Z",
      "updated_at": "2025-11-30T00:36:53Z",
      "pushed_at": "2025-11-29T00:51:56Z"
    },
    "stats": {
      "stars": 656,
      "forks": 50,
      "watchers": 656,
      "open_issues": 2,
      "size": 4528
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 116055
      },
      "license": "Apache License 2.0",
      "topics": [
        "open-weight",
        "text-to-speech"
      ]
    },
    "content": {
      "readme": "![Banner](banner.gif)\n\n<div align=\"center\">\n  <a href=\"https://huggingface.co/nari-labs/Dia2-2B\"><img src=\"https://img.shields.io/badge/HF%20Repo-Dia2--2B-orange?style=for-the-badge\"></a>\n  <a href=\"https://discord.gg/bJq6vjRRKv\"><img src=\"https://img.shields.io/badge/Discord-Join%20Chat-7289DA?logo=discord&style=for-the-badge\"></a>\n  <a href=\"https://github.com/nari-labs/dia2/blob/main/LICENSE\"><img src=\"https://img.shields.io/badge/License-Apache_2.0-blue.svg?style=for-the-badge\"></a>\n</div>\n\n\n**Dia2** is a **streaming dialogue TTS model** created by Nari Labs.\n\nThe model does not need the entire text to produce the audio, and can start generating as the first few words are given as input. You can condition the output on audio, enabling natural conversations in realtime.\n\nWe provide model checkpoints (1B, 2B) and inference code to accelerate research. The model only supports up to 2 minutes of generation in English.\n\nâš ï¸ Quality and voices vary per generation, as the model is not fine-tuned on a specific voice. Use with prefix or fine-tune in order to obtain stable output.\n\nTry it now on Hugging Face [Spaces](https://huggingface.co/spaces/nari-labs/Dia2-2B)\n\n## Upcoming\n\n- Bonsai (JAX) implementation\n- Dia2 TTS Server: Real streaming support\n- Sori: Dia2-powered speech-to-speech engine written in Rust\n\n## Quickstart\n\n> **Requirement** â€” install [uv](https://docs.astral.sh/uv/) and use CUDA 12.8+\n> drivers. All commands below run through `uv run â€¦` as a rule.\n\n1. **Install dependencies (one-time):**\n   ```bash\n   uv sync\n   ```\n2. **Prepare a script:** edit `input.txt` using `[S1]` / `[S2]` speaker tags.\n3. **Generate audio:**\n   ```bash\n   uv run -m dia2.cli \\\n     --hf nari-labs/Dia2-2B \\\n     --input input.txt \\\n     --cfg 6.0 --temperature 0.8 \\\n     --cuda-graph --verbose \\\n     output.wav\n   ```\n   The first run downloads weights/tokenizer/Mimi. The CLI auto-selects CUDA when available (otherwise CPU) and defaults to bfloat16 precisionâ€”override with `--device` / `--dtype` if needed.\n4. **Conditional Generation (recommended for stable use):**\n   ```bash\n   uv run -m dia2.cli \\\n     --hf nari-labs/Dia2-2B \\\n     --input input.txt \\\n     --prefix-speaker-1 example_prefix1.wav \\\n     --prefix-speaker-2 example_prefix2.wav \\\n     --cuda-graph --verbose \\\n     output_conditioned.wav\n   ```\n   Condition the generation on previous conversational context in order to generate natural output for your speech-to-speech system. For example, place the voice of your assistant as prefix speaker 1, place user's audio input as prefix speaker 2, and generate the response to user's input.\n\n   Whisper is used to transcribe each prefix file, which takes additional time. We include example prefix files as `example_prefix1.wav` and `example_prefix2.wav` (both files are output created by the model).\n6. **Gradio for Easy Usage**\n   ```bash\n   uv run gradio_app.py\n   ```\n\n### Programmatic Usage\n```python\nfrom dia2 import Dia2, GenerationConfig, SamplingConfig\n\ndia = Dia2.from_repo(\"nari-labs/Dia2-2B\", device=\"cuda\", dtype=\"bfloat16\")\nconfig = GenerationConfig(\n    cfg_scale=2.0,\n    audio=SamplingConfig(temperature=0.8, top_k=50),\n    use_cuda_graph=True,\n)\nresult = dia.generate(\"[S1] Hello Dia2!\", config=config, output_wav=\"hello.wav\", verbose=True)\n```\nGeneration runs until the runtime config's `max_context_steps` (1500, 2 minutes)\nor until EOS is detected. `GenerationResult` includes audio tokens, waveform tensor,\nand word timestamps relative to Mimiâ€™s ~12.5 Hz frame rate.\n\n## Hugging Face\n\n| Variant | Repo |\n| --- | --- |\n| Dia2-1B | [`nari-labs/Dia2-1B`](https://huggingface.co/nari-labs/Dia2-1B)\n| Dia2-2B | [`nari-labs/Dia2-2B`](https://huggingface.co/nari-labs/Dia2-2B)\n\n## License & Attribution\n\nLicensed under [Apache 2.0](LICENSE). All third-party assets (Kyutai Mimi codec, etc.) retain their original licenses.\n\n## Disclaimer\n\nThis project offers a high-fidelity speech generation model intended for research and educational use. The following uses are **strictly forbidden**:\n\n- **Identity Misuse**: Do not produce audio resembling real individuals without permission.\n- **Deceptive Content**: Do not use this model to generate misleading content (e.g. fake news)\n- **Illegal or Malicious Use**: Do not use this model for activities that are illegal or intended to cause harm.\n\nBy using this model, you agree to uphold relevant legal standards and ethical responsibilities. We **are not responsible** for any misuse and firmly oppose any unethical usage of this technology.\n\n## Acknowledgements\n- We thank the [TPU Research Cloud](https://sites.research.google/trc/about/) program for providing compute for training.\n- Our work was heavily inspired by [KyutaiTTS](https://kyutai.org/next/tts) and [Sesame](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice)\n\n---\nQuestions? Join our [Discord](https://discord.gg/bJq6vjRRKv) or open an issue.\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-11-30T02:52:53.885936"
  }
]