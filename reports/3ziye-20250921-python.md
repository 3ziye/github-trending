## ğŸ“ çƒ­ç‚¹é¡¹ç›®-20250921-python

<div style="background-color: #f5f5f5; padding: 15px; border-radius: 8px; margin: 20px 0;">
- ğŸ¤– æœ¬æŠ¥å‘ŠåŸºäº GitHub API è‡ªåŠ¨ç”Ÿæˆ
- ğŸ“… æ•°æ®æ›´æ–°æ—¶é—´: 2025-09-21 10:27:10 (ä¸Šæµ·æ—¶åŒº)
- ğŸŒŸ æ˜Ÿæ ‡æ•°ç­‰ç»Ÿè®¡ä¿¡æ¯ä¸ºç”Ÿæˆæ—¶çš„å®æ—¶æ•°æ®
- ğŸ“š é¡¹ç›®ä¿¡æ¯æ¥æºäºå„é¡¹ç›®çš„ README æ–‡æ¡£
- ğŸ’¡ çƒ­åº¦æŒ‡æ•°è®¡ç®—æ–¹å¼: æ˜Ÿæ ‡æ•° + Forkæ•° Ã— 0.5
- æœ¬æŠ¥å‘Šç”± ä¸‰å­å¶å¼€æº github-trendingé¡¹ç›®åˆ†æå·¥å…·è‡ªåŠ¨ç”Ÿæˆ
</div>

## ğŸ”— ç›¸å…³é“¾æ¥

- [GitHub API æ–‡æ¡£](https://docs.github.com/en/rest)
- [é¡¹ç›®æ•°æ®è·å–å™¨æºç ](https://github.com/3ziye/github-trending)

---

# çƒ­ç‚¹é¡¹ç›®-20250921-python

<div align="center">
ğŸ“Š <strong>ç”Ÿæˆæ—¶é—´</strong>: 2025å¹´09æœˆ21æ—¥ 10:27  â€¢  
ğŸ¯ <strong>é¡¹ç›®æ•°é‡</strong>: 20 ä¸ª  â€¢  
â±ï¸ <strong>çƒ­åº¦æ—¶é—´</strong>: æœˆæ¦œ  â€¢  
ğŸ”¥ <strong>æ•°æ®æ¥æº</strong>: GitHub API
</div>

---

## ğŸš€ çƒ­é—¨é¡¹ç›®è¯¦æƒ…

### 1. map-anything

**ğŸ”¥ çƒ­åº¦æŒ‡æ•°**: 1,257

**é¡¹ç›®åç§°**: [facebookresearch/map-anything](https://github.com/facebookresearch/map-anything)
**é¡¹ç›®æè¿°**: MapAnything: Universal Feed-Forward Metric 3D Reconstruction
**ä¸»è¦è¯­è¨€**: Python
**è®¸å¯è¯**: Apache License 2.0
**â­ æ˜Ÿæ ‡æ•°**: 1,233
**ğŸ´ Fork æ•°**: 48
**ğŸ‘€ å…³æ³¨æ•°**: 1,233
**ğŸ› å¼€æ”¾é—®é¢˜**: 13
**æœ€åæ›´æ–°**: 2025-09-21 (9åˆ†é’Ÿå‰)
**ğŸ“– é¡¹ç›®æ–‡æ¡£**: [README](3ziye-20250921-python/1-map-anything-Readme.md)

**ğŸ  é¡¹ç›®ä¸»é¡µ**: æ— 

**ğŸ“‚ å…‹éš†åœ°å€**: `https://github.com/facebookresearch/map-anything.git`

**ğŸ’» æŠ€æœ¯æ ˆ**: **ä¸»è¦è¯­è¨€**: Python

#### ğŸ·ï¸ é¡¹ç›®æ ‡ç­¾
- `3d-reconstruction`
- `ai`
- `calibration`
- `depth-completion`
- `depth-estimation`
- `image-to-3d`
- `multi-view-stereo`
- `robotics`

#### ğŸ¯ æ ¸å¿ƒç‰¹æ€§
- [Quick Start](#quick-start)
- [Interactive Demos](#interactive-demos)

#### ğŸ¨ é€‚ç”¨åœºæ™¯
- æ•°æ®ç§‘å­¦ä¸åˆ†æ
- æœºå™¨å­¦ä¹ é¡¹ç›®
- Web åº”ç”¨å¼€å‘

---

### 2. multi-agent-coding-system

**ğŸ”¥ çƒ­åº¦æŒ‡æ•°**: 1,257

**é¡¹ç›®åç§°**: [Danau5tin/multi-agent-coding-system](https://github.com/Danau5tin/multi-agent-coding-system)
**é¡¹ç›®æè¿°**: Reached #13 on Stanford's Terminal Bench leaderboard. Orchestrator, explorer & coder agents working together with intell...
**ä¸»è¦è¯­è¨€**: Python
**è®¸å¯è¯**: None
**â­ æ˜Ÿæ ‡æ•°**: 1,185
**ğŸ´ Fork æ•°**: 145
**ğŸ‘€ å…³æ³¨æ•°**: 1,185
**ğŸ› å¼€æ”¾é—®é¢˜**: 2
**æœ€åæ›´æ–°**: 2025-09-20 (12å°æ—¶å‰)
**ğŸ“– é¡¹ç›®æ–‡æ¡£**: [README](3ziye-20250921-python/2-multi-agent-coding-system-Readme.md)

**ğŸ  é¡¹ç›®ä¸»é¡µ**: æ— 

**ğŸ“‚ å…‹éš†åœ°å€**: `https://github.com/Danau5tin/multi-agent-coding-system.git`

**ğŸ’» æŠ€æœ¯æ ˆ**: **ä¸»è¦è¯­è¨€**: Python

#### ğŸ¯ æ ¸å¿ƒç‰¹æ€§
- Over the weekend, quite unexpectedly, I made a multi-agent AI system that places slightly higher than Claude Code on Stanford's TerminalBench leaderboard (13th place).
- This AI system consists of an orchestration agent that dispatches multiple explorer and coder agents to do all the work.
- The orchestrator explicitly defines what knowledge artifacts subagents must return, then reuses and synthesises these artifacts across future tasks - creating compound intelligence where each action builds meaningfully on previous discoveries.

#### ğŸ¨ é€‚ç”¨åœºæ™¯
- æ•°æ®ç§‘å­¦ä¸åˆ†æ
- æœºå™¨å­¦ä¹ é¡¹ç›®
- Web åº”ç”¨å¼€å‘

---

### 3. HunyuanWorld-Voyager

**ğŸ”¥ çƒ­åº¦æŒ‡æ•°**: 1,176

**é¡¹ç›®åç§°**: [Tencent-Hunyuan/HunyuanWorld-Voyager](https://github.com/Tencent-Hunyuan/HunyuanWorld-Voyager)
**é¡¹ç›®æè¿°**: Voyager is an interactive RGBD video generation model conditioned on camera input, and supports real-time 3D reconstruct...
**ä¸»è¦è¯­è¨€**: Python
**è®¸å¯è¯**: Other
**â­ æ˜Ÿæ ‡æ•°**: 1,139
**ğŸ´ Fork æ•°**: 74
**ğŸ‘€ å…³æ³¨æ•°**: 1,139
**ğŸ› å¼€æ”¾é—®é¢˜**: 14
**æœ€åæ›´æ–°**: 2025-09-20 (1å¤©å‰)
**ğŸ“– é¡¹ç›®æ–‡æ¡£**: [README](3ziye-20250921-python/3-HunyuanWorld-Voyager-Readme.md)

**ğŸ  é¡¹ç›®ä¸»é¡µ**: https://3d-models.hunyuan.tencent.com/world/

**ğŸ“‚ å…‹éš†åœ°å€**: `https://github.com/Tencent-Hunyuan/HunyuanWorld-Voyager.git`

**ğŸ’» æŠ€æœ¯æ ˆ**: **ä¸»è¦è¯­è¨€**: Python

#### ğŸ·ï¸ é¡¹ç›®æ ‡ç­¾
- `3d`
- `3d-generation`
- `aigc`
- `hunyuan3d`
- `image-to-3d`
- `image-to-video`
- `scene-generation`
- `world-model`

#### ğŸ¯ æ ¸å¿ƒç‰¹æ€§
- Sep 2, 2025: ğŸ‘‹ We release the code and model weights of HunyuanWorld-Voyager. [Download](ckpts/README.md).
- Video Reconstruction

#### ğŸ¨ é€‚ç”¨åœºæ™¯
- æ•°æ®ç§‘å­¦ä¸åˆ†æ
- æœºå™¨å­¦ä¹ é¡¹ç›®
- Web åº”ç”¨å¼€å‘

---

### 4. USO

**ğŸ”¥ çƒ­åº¦æŒ‡æ•°**: 1,140

**é¡¹ç›®åç§°**: [bytedance/USO](https://github.com/bytedance/USO)
**é¡¹ç›®æè¿°**: ğŸ”¥ğŸ”¥ Open-sourced unified customization model
**ä¸»è¦è¯­è¨€**: Python
**è®¸å¯è¯**: Apache License 2.0
**â­ æ˜Ÿæ ‡æ•°**: 1,107
**ğŸ´ Fork æ•°**: 67
**ğŸ‘€ å…³æ³¨æ•°**: 1,107
**ğŸ› å¼€æ”¾é—®é¢˜**: 24
**æœ€åæ›´æ–°**: 2025-09-21 (9å°æ—¶å‰)
**ğŸ“– é¡¹ç›®æ–‡æ¡£**: [README](3ziye-20250921-python/4-USO-Readme.md)

**ğŸ  é¡¹ç›®ä¸»é¡µ**: https://bytedance.github.io/USO/

**ğŸ“‚ å…‹éš†åœ°å€**: `https://github.com/bytedance/USO.git`

**ğŸ’» æŠ€æœ¯æ ˆ**: **ä¸»è¦è¯­è¨€**: Python

#### ğŸ¯ æ ¸å¿ƒç‰¹æ€§
**2025.08.28** ğŸ”¥ The [demo](https://huggingface.co/spaces/bytedance-research/USO) of USO is released. Try it Now! âš¡ï¸
**2025.08.28** ğŸ”¥ Update fp8 mode as a primary low vmemory usage support (please scroll down). Gift for consumer-grade GPU users. The peak Vmemory usage is ~16GB now.
**2025.08.27** ğŸ”¥ The [inference code](https://github.com/bytedance/USO) and [model](https://huggingface.co/bytedance-research/USO) of USO are released.
**2025.08.27** ğŸ”¥ The [project page](https://bytedance.github.io/USO) of USO is created.
**2025.08.27** ğŸ”¥ The [technical report](https://arxiv.org/abs/2508.18966) of USO is released.

#### ğŸ¨ é€‚ç”¨åœºæ™¯
- æ•°æ®ç§‘å­¦ä¸åˆ†æ
- æœºå™¨å­¦ä¹ é¡¹ç›®
- Web åº”ç”¨å¼€å‘

---

### 5. VoxCPM

**ğŸ”¥ çƒ­åº¦æŒ‡æ•°**: 1,021

**é¡¹ç›®åç§°**: [OpenBMB/VoxCPM](https://github.com/OpenBMB/VoxCPM)
**é¡¹ç›®æè¿°**: VoxCPM: Tokenizer-Free TTS for Context-Aware Speech Generation and True-to-Life Voice Cloning
**ä¸»è¦è¯­è¨€**: Python
**è®¸å¯è¯**: Apache License 2.0
**â­ æ˜Ÿæ ‡æ•°**: 974
**ğŸ´ Fork æ•°**: 94
**ğŸ‘€ å…³æ³¨æ•°**: 974
**ğŸ› å¼€æ”¾é—®é¢˜**: 20
**æœ€åæ›´æ–°**: 2025-09-21 (8åˆ†é’Ÿå‰)
**ğŸ“– é¡¹ç›®æ–‡æ¡£**: [README](3ziye-20250921-python/5-VoxCPM-Readme.md)

**ğŸ  é¡¹ç›®ä¸»é¡µ**: æ— 

**ğŸ“‚ å…‹éš†åœ°å€**: `https://github.com/OpenBMB/VoxCPM.git`

**ğŸ’» æŠ€æœ¯æ ˆ**: **ä¸»è¦è¯­è¨€**: Python

#### ğŸ¯ æ ¸å¿ƒç‰¹æ€§
- [2025.09.16] ğŸ”¥ ğŸ”¥ ğŸ”¥  We Open Source the VoxCPM-0.5B [weights](https://huggingface.co/openbmb/VoxCPM-0.5B)!
- [2025.09.16] ğŸ‰ ğŸ‰ ğŸ‰  We Provide the [Gradio PlayGround](https://huggingface.co/spaces/OpenBMB/VoxCPM-Demo) for VoxCPM-0.5B, try it now!
**High-Efficiency Synthesis** - VoxCPM supports streaming synthesis with a Real-Time Factor (RTF) as low as 0.17 on a consumer-grade NVIDIA RTX 4090 GPU, making it possible for real-time applications.

#### ğŸ¨ é€‚ç”¨åœºæ™¯
- æ•°æ®ç§‘å­¦ä¸åˆ†æ
- æœºå™¨å­¦ä¹ é¡¹ç›®
- Web åº”ç”¨å¼€å‘

---

### 6. SRPO

**ğŸ”¥ çƒ­åº¦æŒ‡æ•°**: 893

**é¡¹ç›®åç§°**: [Tencent-Hunyuan/SRPO](https://github.com/Tencent-Hunyuan/SRPO)
**é¡¹ç›®æè¿°**: Directly Aligning the Full Diffusion Trajectory with Fine-Grained Human Preference
**ä¸»è¦è¯­è¨€**: Python
**è®¸å¯è¯**: Other
**â­ æ˜Ÿæ ‡æ•°**: 882
**ğŸ´ Fork æ•°**: 22
**ğŸ‘€ å…³æ³¨æ•°**: 882
**ğŸ› å¼€æ”¾é—®é¢˜**: 10
**æœ€åæ›´æ–°**: 2025-09-21 (24åˆ†é’Ÿå‰)
**ğŸ“– é¡¹ç›®æ–‡æ¡£**: [README](3ziye-20250921-python/6-SRPO-Readme.md)

**ğŸ  é¡¹ç›®ä¸»é¡µ**: https://tencent.github.io/srpo-project-page/

**ğŸ“‚ å…‹éš†åœ°å€**: `https://github.com/Tencent-Hunyuan/SRPO.git`

**ğŸ’» æŠ€æœ¯æ ˆ**: **ä¸»è¦è¯­è¨€**: Python

#### ğŸ¨ é€‚ç”¨åœºæ™¯
- æ•°æ®ç§‘å­¦ä¸åˆ†æ
- æœºå™¨å­¦ä¹ é¡¹ç›®
- Web åº”ç”¨å¼€å‘

---

### 7. VibeVoice-ComfyUI

**ğŸ”¥ çƒ­åº¦æŒ‡æ•°**: 919

**é¡¹ç›®åç§°**: [Enemyx-net/VibeVoice-ComfyUI](https://github.com/Enemyx-net/VibeVoice-ComfyUI)
**é¡¹ç›®æè¿°**: A comprehensive ComfyUI integration for Microsoft's VibeVoice text-to-speech model, enabling high-quality single and mul...
**ä¸»è¦è¯­è¨€**: Python
**è®¸å¯è¯**: MIT License
**â­ æ˜Ÿæ ‡æ•°**: 849
**ğŸ´ Fork æ•°**: 141
**ğŸ‘€ å…³æ³¨æ•°**: 849
**ğŸ› å¼€æ”¾é—®é¢˜**: 17
**æœ€åæ›´æ–°**: 2025-09-21 (1å°æ—¶å‰)
**ğŸ“– é¡¹ç›®æ–‡æ¡£**: [README](3ziye-20250921-python/7-VibeVoice-ComfyUI-Readme.md)

**ğŸ  é¡¹ç›®ä¸»é¡µ**: æ— 

**ğŸ“‚ å…‹éš†åœ°å€**: `https://github.com/Enemyx-net/VibeVoice-ComfyUI.git`

**ğŸ’» æŠ€æœ¯æ ˆ**: **ä¸»è¦è¯­è¨€**: Python

#### ğŸ·ï¸ é¡¹ç›®æ ‡ç­¾
- `ai-audio`
- `ai-tts`
- `ai-voice`
- `ai-voice-clone`
- `ai-voice-clonining`
- `comfyui-custom-node`
- `comfyui-custom-nodes-text-to-speech`
- `comfyui-nodes`

#### ğŸ¯ æ ¸å¿ƒç‰¹æ€§
- ### Core Functionality
- ğŸ¤ **Single Speaker TTS**: Generate natural speech with optional voice cloning
- ğŸ‘¥ **Multi-Speaker Conversations**: Support for up to 4 distinct speakers
- ğŸ¯ **Voice Cloning**: Clone voices from audio samples
- ğŸ“ **Text File Loading**: Load scripts from text files
- ğŸ“š **Automatic Text Chunking**: Handles long texts seamlessly with configurable chunk size
- â¸ï¸ **Custom Pause Tags**: Insert silences with `[pause]` and `[pause:ms]` tags (wrapper feature)
- ğŸ”„ **Node Chaining**: Connect multiple VibeVoice nodes for complex workflows

#### ğŸ¨ é€‚ç”¨åœºæ™¯
- æ•°æ®ç§‘å­¦ä¸åˆ†æ
- æœºå™¨å­¦ä¹ é¡¹ç›®
- Web åº”ç”¨å¼€å‘

---

### 8. SpikingBrain-7B

**ğŸ”¥ çƒ­åº¦æŒ‡æ•°**: 881

**é¡¹ç›®åç§°**: [BICLab/SpikingBrain-7B](https://github.com/BICLab/SpikingBrain-7B)
**é¡¹ç›®æè¿°**: æš‚æ— æè¿°
**ä¸»è¦è¯­è¨€**: Python
**è®¸å¯è¯**: None
**â­ æ˜Ÿæ ‡æ•°**: 832
**ğŸ´ Fork æ•°**: 99
**ğŸ‘€ å…³æ³¨æ•°**: 832
**ğŸ› å¼€æ”¾é—®é¢˜**: 7
**æœ€åæ›´æ–°**: 2025-09-21 (1å°æ—¶å‰)
**ğŸ“– é¡¹ç›®æ–‡æ¡£**: [README](3ziye-20250921-python/8-SpikingBrain-7B-Readme.md)

**ğŸ  é¡¹ç›®ä¸»é¡µ**: æ— 

**ğŸ“‚ å…‹éš†åœ°å€**: `https://github.com/BICLab/SpikingBrain-7B.git`

**ğŸ’» æŠ€æœ¯æ ˆ**: **ä¸»è¦è¯­è¨€**: Python

#### ğŸ¯ æ ¸å¿ƒç‰¹æ€§
**Decoupled codebase**: Backend-specific code remains independent, keeping the vLLM core cleaner.
**Reduced maintenance cost**: vLLM developers can focus on general functionality without being affected by backend-specific implementations.
**Faster integration**: New backends can be integrated quickly and evolve independently with less engineering effort.

#### ğŸ¨ é€‚ç”¨åœºæ™¯
- æ•°æ®ç§‘å­¦ä¸åˆ†æ
- æœºå™¨å­¦ä¹ é¡¹ç›®
- Web åº”ç”¨å¼€å‘

---

### 9. cc-sessions

**ğŸ”¥ çƒ­åº¦æŒ‡æ•°**: 856

**é¡¹ç›®åç§°**: [GWUDCAP/cc-sessions](https://github.com/GWUDCAP/cc-sessions)
**é¡¹ç›®æè¿°**: An opinionated extension set for Claude Code (hooks, subagents, commands, task/git management infrastructure)
**ä¸»è¦è¯­è¨€**: Python
**è®¸å¯è¯**: None
**â­ æ˜Ÿæ ‡æ•°**: 824
**ğŸ´ Fork æ•°**: 65
**ğŸ‘€ å…³æ³¨æ•°**: 824
**ğŸ› å¼€æ”¾é—®é¢˜**: 31
**æœ€åæ›´æ–°**: 2025-09-21 (3å°æ—¶å‰)
**ğŸ“– é¡¹ç›®æ–‡æ¡£**: [README](3ziye-20250921-python/9-cc-sessions-Readme.md)

**ğŸ  é¡¹ç›®ä¸»é¡µ**: æ— 

**ğŸ“‚ å…‹éš†åœ°å€**: `https://github.com/GWUDCAP/cc-sessions.git`

**ğŸ’» æŠ€æœ¯æ ˆ**: **ä¸»è¦è¯­è¨€**: Python

#### ğŸ¯ æ ¸å¿ƒç‰¹æ€§
- ğŸ’­ The LLM programmer hype gave you a nerd chub
- ğŸ˜¬ The people hyping LLM programming made your nerd chub crawl back into your body <br> <sup>_(are you ready to 'scale your impact', dog?)_</sup>
- ğŸ¤® You held your nose and downloaded Cursor/added Cline or Roo Code/npm installed Claude Code
- `'NEVER use class components.'`
- `'ALWAYS use the existing auth middleware.'`
- `'DO NOT refactor unrelated code.'`
- `'REMEMBER we use PostgreSQL.'`

#### ğŸ¨ é€‚ç”¨åœºæ™¯
- æ•°æ®ç§‘å­¦ä¸åˆ†æ
- æœºå™¨å­¦ä¹ é¡¹ç›®
- Web åº”ç”¨å¼€å‘

---

### 10. checkpoint-engine

**ğŸ”¥ çƒ­åº¦æŒ‡æ•°**: 746

**é¡¹ç›®åç§°**: [MoonshotAI/checkpoint-engine](https://github.com/MoonshotAI/checkpoint-engine)
**é¡¹ç›®æè¿°**: Checkpoint-engine is a simple middleware to update model weights in LLM inference engines
**ä¸»è¦è¯­è¨€**: Python
**è®¸å¯è¯**: MIT License
**â­ æ˜Ÿæ ‡æ•°**: 722
**ğŸ´ Fork æ•°**: 48
**ğŸ‘€ å…³æ³¨æ•°**: 722
**ğŸ› å¼€æ”¾é—®é¢˜**: 3
**æœ€åæ›´æ–°**: 2025-09-21 (3å°æ—¶å‰)
**ğŸ“– é¡¹ç›®æ–‡æ¡£**: [README](3ziye-20250921-python/10-checkpoint-engine-Readme.md)

**ğŸ  é¡¹ç›®ä¸»é¡µ**: æ— 

**ğŸ“‚ å…‹éš†åœ°å€**: `https://github.com/MoonshotAI/checkpoint-engine.git`

**ğŸ’» æŠ€æœ¯æ ˆ**: **ä¸»è¦è¯­è¨€**: Python

#### ğŸ¯ æ ¸å¿ƒç‰¹æ€§
**Broadcast**: Used when a large number of inference instances need to update weights in synchronous. This is the fastest implementation and should be used as the default update method. See `_update_per_bucket`.

#### ğŸ¨ é€‚ç”¨åœºæ™¯
- æ•°æ®ç§‘å­¦ä¸åˆ†æ
- æœºå™¨å­¦ä¹ é¡¹ç›®
- Web åº”ç”¨å¼€å‘

---

### 11. batch_invariant_ops

**ğŸ”¥ çƒ­åº¦æŒ‡æ•°**: 691

**é¡¹ç›®åç§°**: [thinking-machines-lab/batch_invariant_ops](https://github.com/thinking-machines-lab/batch_invariant_ops)
**é¡¹ç›®æè¿°**: æš‚æ— æè¿°
**ä¸»è¦è¯­è¨€**: Python
**è®¸å¯è¯**: MIT License
**â­ æ˜Ÿæ ‡æ•°**: 671
**ğŸ´ Fork æ•°**: 40
**ğŸ‘€ å…³æ³¨æ•°**: 671
**ğŸ› å¼€æ”¾é—®é¢˜**: 3
**æœ€åæ›´æ–°**: 2025-09-21 (3å°æ—¶å‰)
**ğŸ“– é¡¹ç›®æ–‡æ¡£**: [README](3ziye-20250921-python/11-batch_invariant_ops-Readme.md)

**ğŸ  é¡¹ç›®ä¸»é¡µ**: æ— 

**ğŸ“‚ å…‹éš†åœ°å€**: `https://github.com/thinking-machines-lab/batch_invariant_ops.git`

**ğŸ’» æŠ€æœ¯æ ˆ**: **ä¸»è¦è¯­è¨€**: Python

#### ğŸ¯ æ ¸å¿ƒç‰¹æ€§
- `torch.mm()` - Matrix multiplication
- `torch.addmm()` - Matrix multiplication with bias addition
- `torch.log_softmax()` - Log-softmax activation
- `torch.mean()` - Mean computation along specified dimensions

#### ğŸ¨ é€‚ç”¨åœºæ™¯
- æ•°æ®ç§‘å­¦ä¸åˆ†æ
- æœºå™¨å­¦ä¹ é¡¹ç›®
- Web åº”ç”¨å¼€å‘

---

### 12. PromptEnhancer

**ğŸ”¥ çƒ­åº¦æŒ‡æ•°**: 668

**é¡¹ç›®åç§°**: [Hunyuan-PromptEnhancer/PromptEnhancer](https://github.com/Hunyuan-PromptEnhancer/PromptEnhancer)
**é¡¹ç›®æè¿°**: PromptEnhancer is a prompt-rewriting tool, refining prompts into clearer, structured versions for better image generatio...
**ä¸»è¦è¯­è¨€**: Python
**è®¸å¯è¯**: Other
**â­ æ˜Ÿæ ‡æ•°**: 642
**ğŸ´ Fork æ•°**: 52
**ğŸ‘€ å…³æ³¨æ•°**: 642
**ğŸ› å¼€æ”¾é—®é¢˜**: 4
**æœ€åæ›´æ–°**: 2025-09-21 (5åˆ†é’Ÿå‰)
**ğŸ“– é¡¹ç›®æ–‡æ¡£**: [README](3ziye-20250921-python/12-PromptEnhancer-Readme.md)

**ğŸ  é¡¹ç›®ä¸»é¡µ**: https://hunyuan-promptenhancer.github.io/

**ğŸ“‚ å…‹éš†åœ°å€**: `https://github.com/Hunyuan-PromptEnhancer/PromptEnhancer.git`

**ğŸ’» æŠ€æœ¯æ ˆ**: **ä¸»è¦è¯­è¨€**: Python

#### ğŸ·ï¸ é¡¹ç›®æ ‡ç­¾
- `hunyuan`
- `hunyuan-image`
- `prompt`
- `prompt-engineering`
- `prompt-enhancer`
- `vlm`

#### ğŸ¨ é€‚ç”¨åœºæ™¯
- æ•°æ®ç§‘å­¦ä¸åˆ†æ
- æœºå™¨å­¦ä¹ é¡¹ç›®
- Web åº”ç”¨å¼€å‘

---

### 13. AI-Video-Transcriber

**ğŸ”¥ çƒ­åº¦æŒ‡æ•°**: 661

**é¡¹ç›®åç§°**: [wendy7756/AI-Video-Transcriber](https://github.com/wendy7756/AI-Video-Transcriber)
**é¡¹ç›®æè¿°**:  Transcribe and summarize video content using AI. Open-source, multi-platform, and supports multiple languages.
**ä¸»è¦è¯­è¨€**: Python
**è®¸å¯è¯**: Apache License 2.0
**â­ æ˜Ÿæ ‡æ•°**: 620
**ğŸ´ Fork æ•°**: 83
**ğŸ‘€ å…³æ³¨æ•°**: 620
**ğŸ› å¼€æ”¾é—®é¢˜**: 1
**æœ€åæ›´æ–°**: 2025-09-21 (2å°æ—¶å‰)
**ğŸ“– é¡¹ç›®æ–‡æ¡£**: [README](3ziye-20250921-python/13-AI-Video-Transcriber-Readme.md)

**ğŸ  é¡¹ç›®ä¸»é¡µ**: æ— 

**ğŸ“‚ å…‹éš†åœ°å€**: `https://github.com/wendy7756/AI-Video-Transcriber.git`

**ğŸ’» æŠ€æœ¯æ ˆ**: **ä¸»è¦è¯­è¨€**: Python

#### ğŸ·ï¸ é¡¹ç›®æ ‡ç­¾
- `aitool`
- `tiktok`
- `transcribe`
- `videototext`
- `youtube`

#### ğŸ¯ æ ¸å¿ƒç‰¹æ€§
- ğŸ¥ **Multi-Platform Support**: Works with YouTube, Tiktok, Bilibili, and 30+ more
- ğŸ—£ï¸ **Intelligent Transcription**: High-accuracy speech-to-text using Faster-Whisper
- ğŸ¤– **AI Text Optimization**: Automatic typo correction, sentence completion, and intelligent paragraphing
- ğŸŒ **Multi-Language Summaries**: Generate intelligent summaries in multiple languages
- âš¡ **Real-Time Progress**: Live progress tracking and status updates
- âš™ï¸ **Conditional Translation**: When the selected summary language differs from the detected transcript language, the system auto-translates with GPTâ€‘4o
- ğŸ“± **Mobile-Friendly**: Perfect support for mobile devices
- Python 3.8+

#### ğŸ¨ é€‚ç”¨åœºæ™¯
- æ•°æ®ç§‘å­¦ä¸åˆ†æ
- æœºå™¨å­¦ä¹ é¡¹ç›®
- Web åº”ç”¨å¼€å‘

---

### 14. Genie

**ğŸ”¥ çƒ­åº¦æŒ‡æ•°**: 634

**é¡¹ç›®åç§°**: [High-Logic/Genie](https://github.com/High-Logic/Genie)
**é¡¹ç›®æè¿°**: GPT-SoVITS ONNX Inference Engine & Model Converter
**ä¸»è¦è¯­è¨€**: Python
**è®¸å¯è¯**: MIT License
**â­ æ˜Ÿæ ‡æ•°**: 619
**ğŸ´ Fork æ•°**: 31
**ğŸ‘€ å…³æ³¨æ•°**: 619
**ğŸ› å¼€æ”¾é—®é¢˜**: 2
**æœ€åæ›´æ–°**: 2025-09-21 (8å°æ—¶å‰)
**ğŸ“– é¡¹ç›®æ–‡æ¡£**: [README](3ziye-20250921-python/14-Genie-Readme.md)

**ğŸ  é¡¹ç›®ä¸»é¡µ**: æ— 

**ğŸ“‚ å…‹éš†åœ°å€**: `https://github.com/High-Logic/Genie.git`

**ğŸ’» æŠ€æœ¯æ ˆ**: **ä¸»è¦è¯­è¨€**: Python

#### ğŸ·ï¸ é¡¹ç›®æ ‡ç­¾
- `gpt-sovits`
- `text-to-speech`
- `tts`
- `vits`
- `voice-clone`
- `voice-cloning`

#### ğŸ¯ æ ¸å¿ƒç‰¹æ€§
**âœ… Supported Model Version:** GPT-SoVITS V2
**âœ… Supported Language:** Japanese
**âœ… Supported Python Version:** >= 3.9
**[â¡ï¸ Watch the demo video (Chinese)](https://www.bilibili.com/video/BV1d2hHzJEz9)**

#### ğŸ¨ é€‚ç”¨åœºæ™¯
- æ•°æ®ç§‘å­¦ä¸åˆ†æ
- æœºå™¨å­¦ä¹ é¡¹ç›®
- Web åº”ç”¨å¼€å‘

---

### 15. HunyuanImage-2.1

**ğŸ”¥ çƒ­åº¦æŒ‡æ•°**: 616

**é¡¹ç›®åç§°**: [Tencent-Hunyuan/HunyuanImage-2.1](https://github.com/Tencent-Hunyuan/HunyuanImage-2.1)
**é¡¹ç›®æè¿°**: HunyuanImage-2.1: An Efficient Diffusion Model for High-Resolution (2K) Text-to-Image Generationâ€‹
**ä¸»è¦è¯­è¨€**: Python
**è®¸å¯è¯**: Other
**â­ æ˜Ÿæ ‡æ•°**: 595
**ğŸ´ Fork æ•°**: 42
**ğŸ‘€ å…³æ³¨æ•°**: 595
**ğŸ› å¼€æ”¾é—®é¢˜**: 16
**æœ€åæ›´æ–°**: 2025-09-20 (14å°æ—¶å‰)
**ğŸ“– é¡¹ç›®æ–‡æ¡£**: [README](3ziye-20250921-python/15-HunyuanImage-2.1-Readme.md)

**ğŸ  é¡¹ç›®ä¸»é¡µ**: https://hunyuan.tencent.com/image/en?tabIndex=0

**ğŸ“‚ å…‹éš†åœ°å€**: `https://github.com/Tencent-Hunyuan/HunyuanImage-2.1.git`

**ğŸ’» æŠ€æœ¯æ ˆ**: **ä¸»è¦è¯­è¨€**: Python

#### ğŸ·ï¸ é¡¹ç›®æ ‡ç­¾
- `aigc`
- `diffusion-models`
- `diffusion-transformer`
- `image-generation`
- `text-to-image`

#### ğŸ¯ æ ¸å¿ƒç‰¹æ€§
- September 18, 2025: âœ¨ Try the [PromptEnhancer-32B model](https://huggingface.co/PromptEnhancer/PromptEnhancer-32B) for higher-quality prompt enhancement!â€‹.
- September 18, 2025: âœ¨ [ComfyUI workflow of HunyuanImage-2.1](https://github.com/KimbingNg/ComfyUI-HunyuanImage2.1) is available now!
- September 16, 2025: ğŸ‘‘ We achieved the Top1 on Arena's leaderboard for text-to-image open-source models. [Leaderboard](https://artificialanalysis.ai/text-to-image/arena/leaderboard-text)
- September 12, 2025: ğŸš€ Released FP8 quantized models! Making it possible to generate 2K images with only 24GB GPU memory!
- September 8, 2025: ğŸš€ Released inference code and model weights for HunyuanImage-2.1.

#### ğŸ¨ é€‚ç”¨åœºæ™¯
- æ•°æ®ç§‘å­¦ä¸åˆ†æ
- æœºå™¨å­¦ä¹ é¡¹ç›®
- Web åº”ç”¨å¼€å‘

---

### 16. youtu-graphrag

**ğŸ”¥ çƒ­åº¦æŒ‡æ•°**: 631

**é¡¹ç›®åç§°**: [TencentCloudADP/youtu-graphrag](https://github.com/TencentCloudADP/youtu-graphrag)
**é¡¹ç›®æè¿°**: Official repository of Youtu-GraphRAG: Vertically Unified Agents for Graph Retrieval-Augmented Complex Reasoning
**ä¸»è¦è¯­è¨€**: Python
**è®¸å¯è¯**: Other
**â­ æ˜Ÿæ ‡æ•°**: 592
**ğŸ´ Fork æ•°**: 78
**ğŸ‘€ å…³æ³¨æ•°**: 592
**ğŸ› å¼€æ”¾é—®é¢˜**: 32
**æœ€åæ›´æ–°**: 2025-09-21 (30åˆ†é’Ÿå‰)
**ğŸ“– é¡¹ç›®æ–‡æ¡£**: [README](3ziye-20250921-python/16-youtu-graphrag-Readme.md)

**ğŸ  é¡¹ç›®ä¸»é¡µ**: https://arxiv.org/abs/2508.19855

**ğŸ“‚ å…‹éš†åœ°å€**: `https://github.com/TencentCloudADP/youtu-graphrag.git`

**ğŸ’» æŠ€æœ¯æ ˆ**: **ä¸»è¦è¯­è¨€**: Python

#### ğŸ·ï¸ é¡¹ç›®æ ‡ç­¾
- `agent`
- `graph`
- `graphrag`
- `llm`
- `rag`

#### ğŸ¯ æ ¸å¿ƒç‰¹æ€§
- ğŸŒ± **Seed Graph Schema**: Introduces targeted entity types, relations, and attribute types to bound automatic extraction agents
- ğŸ“ˆ **Scalable Schema Expansion**: Continuously expands schemas for

#### ğŸ¨ é€‚ç”¨åœºæ™¯
- æ•°æ®ç§‘å­¦ä¸åˆ†æ
- æœºå™¨å­¦ä¹ é¡¹ç›®
- Web åº”ç”¨å¼€å‘

---

### 17. FireRedTTS2

**ğŸ”¥ çƒ­åº¦æŒ‡æ•°**: 618

**é¡¹ç›®åç§°**: [FireRedTeam/FireRedTTS2](https://github.com/FireRedTeam/FireRedTTS2)
**é¡¹ç›®æè¿°**: Long-form streaming TTS system for multi-speaker dialogue generation
**ä¸»è¦è¯­è¨€**: Python
**è®¸å¯è¯**: Apache License 2.0
**â­ æ˜Ÿæ ‡æ•°**: 590
**ğŸ´ Fork æ•°**: 56
**ğŸ‘€ å…³æ³¨æ•°**: 590
**ğŸ› å¼€æ”¾é—®é¢˜**: 13
**æœ€åæ›´æ–°**: 2025-09-21 (44åˆ†é’Ÿå‰)
**ğŸ“– é¡¹ç›®æ–‡æ¡£**: [README](3ziye-20250921-python/17-FireRedTTS2-Readme.md)

**ğŸ  é¡¹ç›®ä¸»é¡µ**: æ— 

**ğŸ“‚ å…‹éš†åœ°å€**: `https://github.com/FireRedTeam/FireRedTTS2.git`

**ğŸ’» æŠ€æœ¯æ ˆ**: **ä¸»è¦è¯­è¨€**: Python

#### ğŸ¯ æ ¸å¿ƒç‰¹æ€§
**Long Conversational Speech Generation**: It currently supports 3 minutes dialogues with 4 speakers and can be easily scaled to longer conversations
**Multilingual Support**: It supports multiple languages including English, Chinese, Japanese, Korean, French, German, and Russian. Support zero-shot voice cloning for cross-lingual and code-switching scenarios.
**Strong Stability**ï¼šOur model achieves high similarity and low WER/CER in both monologue and dialogue tests.
**Random Timbre Generation**:Useful for creating ASR/speech interaction data.
- [2025/09/12] ğŸ”¥ *

#### ğŸ¨ é€‚ç”¨åœºæ™¯
- æ•°æ®ç§‘å­¦ä¸åˆ†æ
- æœºå™¨å­¦ä¹ é¡¹ç›®
- Web åº”ç”¨å¼€å‘

---

### 18. HuMo

**ğŸ”¥ çƒ­åº¦æŒ‡æ•°**: 593

**é¡¹ç›®åç§°**: [Phantom-video/HuMo](https://github.com/Phantom-video/HuMo)
**é¡¹ç›®æè¿°**: HuMo: Human-Centric Video Generation via Collaborative Multi-Modal Conditioning
**ä¸»è¦è¯­è¨€**: Python
**è®¸å¯è¯**: Apache License 2.0
**â­ æ˜Ÿæ ‡æ•°**: 570
**ğŸ´ Fork æ•°**: 46
**ğŸ‘€ å…³æ³¨æ•°**: 570
**ğŸ› å¼€æ”¾é—®é¢˜**: 11
**æœ€åæ›´æ–°**: 2025-09-21 (8å°æ—¶å‰)
**ğŸ“– é¡¹ç›®æ–‡æ¡£**: [README](3ziye-20250921-python/18-HuMo-Readme.md)

**ğŸ  é¡¹ç›®ä¸»é¡µ**: https://phantom-video.github.io/HuMo/

**ğŸ“‚ å…‹éš†åœ°å€**: `https://github.com/Phantom-video/HuMo.git`

**ğŸ’» æŠ€æœ¯æ ˆ**: **ä¸»è¦è¯­è¨€**: Python

#### ğŸ¯ æ ¸å¿ƒç‰¹æ€§
- A Best-Practice Guide for HuMo will be released soon. Stay tuned.
- Sep 17, 2025: ğŸ”¥ğŸ”¥ [ComfyUI](https://github.com/comfyanonymous/ComfyUI/pull/9903) officially supports HuMo-1.7B!
- Sep 13, 2025: ğŸ”¥ğŸ”¥ The 17B model is merged into [ComfyUI-Wan](https://github.com/kijai/ComfyUI-WanVideoWrapper), which can be run on a NVIDIA 3090 GPU. Thank [kijai](https://github.com/kijai) for the update!
- Sep 10, 2025: ğŸ”¥ğŸ”¥ We release the [17B weights](https://huggingface.co/bytedance-research/HuMo/tree/main/HuMo-17B) and inference codes.
- Sep 9, 2025: We release the [Project Page](https://phantom-video.github.io/HuMo/) and [Technique Report](https://arxiv.org/abs/2509.08519/) of **HuMo**.

#### ğŸ¨ é€‚ç”¨åœºæ™¯
- æ•°æ®ç§‘å­¦ä¸åˆ†æ
- æœºå™¨å­¦ä¹ é¡¹ç›®
- Web åº”ç”¨å¼€å‘

---

### 19. DeepMCPAgent

**ğŸ”¥ çƒ­åº¦æŒ‡æ•°**: 597

**é¡¹ç›®åç§°**: [cryxnet/DeepMCPAgent](https://github.com/cryxnet/DeepMCPAgent)
**é¡¹ç›®æè¿°**: Model-agnostic plug-n-play LangChain/LangGraph agents powered entirely by MCP tools over HTTP/SSE.
**ä¸»è¦è¯­è¨€**: Python
**è®¸å¯è¯**: Apache License 2.0
**â­ æ˜Ÿæ ‡æ•°**: 557
**ğŸ´ Fork æ•°**: 81
**ğŸ‘€ å…³æ³¨æ•°**: 557
**ğŸ› å¼€æ”¾é—®é¢˜**: 0
**æœ€åæ›´æ–°**: 2025-09-21 (2å°æ—¶å‰)
**ğŸ“– é¡¹ç›®æ–‡æ¡£**: [README](3ziye-20250921-python/19-DeepMCPAgent-Readme.md)

**ğŸ  é¡¹ç›®ä¸»é¡µ**: https://cryxnet.github.io/DeepMCPAgent/

**ğŸ“‚ å…‹éš†åœ°å€**: `https://github.com/cryxnet/DeepMCPAgent.git`

**ğŸ’» æŠ€æœ¯æ ˆ**: **ä¸»è¦è¯­è¨€**: Python

#### ğŸ·ï¸ é¡¹ç›®æ ‡ç­¾
- `agent-framework`
- `agentic-ai`
- `agents`
- `ai`
- `ai-agents`
- `ai-framework`
- `artificial-intelligence`
- `autonomous-agents`

#### ğŸ¯ æ ¸å¿ƒç‰¹æ€§
- ğŸ”Œ **Zero manual tool wiring** â€” tools are discovered dynamically from MCP servers (HTTP/SSE)
- ğŸŒ **External APIs welcome** â€” connect to remote MCP servers (with headers/auth)
- ğŸ§  **Model-agnostic** â€” pass any LangChain chat model instance (OpenAI, Anthropic, Ollama, Groq, local, â€¦)
- âš¡ **DeepAgents (optional)** â€” if installed, you get a deep agent loop; otherwise robust LangGraph ReAct fallback
- ğŸ› ï¸ **Typed tool args** â€” JSON-Schema â†’ Pydantic â†’ LangChain `BaseTool` (typed, validated calls)
- ğŸ§ª **Quality bar** â€” mypy (strict), ruff, pytest, GitHub Actions, docs
- `dev` â†’ linting, typing, tests
- `docs` â†’ MkDocs + Material + mkdocstrings

#### ğŸ¨ é€‚ç”¨åœºæ™¯
- æ•°æ®ç§‘å­¦ä¸åˆ†æ
- æœºå™¨å­¦ä¹ é¡¹ç›®
- Web åº”ç”¨å¼€å‘

---

### 20. Hunyuan-MT

**ğŸ”¥ çƒ­åº¦æŒ‡æ•°**: 565

**é¡¹ç›®åç§°**: [Tencent-Hunyuan/Hunyuan-MT](https://github.com/Tencent-Hunyuan/Hunyuan-MT)
**é¡¹ç›®æè¿°**: æš‚æ— æè¿°
**ä¸»è¦è¯­è¨€**: Python
**è®¸å¯è¯**: Other
**â­ æ˜Ÿæ ‡æ•°**: 543
**ğŸ´ Fork æ•°**: 45
**ğŸ‘€ å…³æ³¨æ•°**: 543
**ğŸ› å¼€æ”¾é—®é¢˜**: 18
**æœ€åæ›´æ–°**: 2025-09-21 (7å°æ—¶å‰)
**ğŸ“– é¡¹ç›®æ–‡æ¡£**: [README](3ziye-20250921-python/20-Hunyuan-MT-Readme.md)

**ğŸ  é¡¹ç›®ä¸»é¡µ**: æ— 

**ğŸ“‚ å…‹éš†åœ°å€**: `https://github.com/Tencent-Hunyuan/Hunyuan-MT.git`

**ğŸ’» æŠ€æœ¯æ ˆ**: **ä¸»è¦è¯­è¨€**: Python

#### ğŸ¯ æ ¸å¿ƒç‰¹æ€§
- In the WMT25 competition, the model achieved first place in 30 out of the 31 language categories it participated in.
- Hunyuan-MT-7B achieves industry-leading performance among models of comparable scale
- Hunyuan-MT-Chimera-7B is the industryâ€™s first open-source translation ensemble model, elevating translation quality to a new level
- 2025.9.1 We have open-sourced  **Hunyuan-MT-7B** , **Hunyuan-MT-Chimera-7B** on Hugging Face.

#### ğŸ¨ é€‚ç”¨åœºæ™¯
- æ•°æ®ç§‘å­¦ä¸åˆ†æ
- æœºå™¨å­¦ä¹ é¡¹ç›®
- Web åº”ç”¨å¼€å‘

---