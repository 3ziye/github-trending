[
  {
    "basic_info": {
      "name": "nanochat",
      "full_name": "karpathy/nanochat",
      "owner": "karpathy",
      "description": "The best ChatGPT that $100 can buy.",
      "url": "https://github.com/karpathy/nanochat",
      "clone_url": "https://github.com/karpathy/nanochat.git",
      "ssh_url": "git@github.com:karpathy/nanochat.git",
      "homepage": "",
      "created_at": "2025-10-13T13:46:35Z",
      "updated_at": "2025-11-09T02:01:09Z",
      "pushed_at": "2025-11-05T21:08:37Z"
    },
    "stats": {
      "stars": 36142,
      "forks": 4215,
      "watchers": 36142,
      "open_issues": 48,
      "size": 167
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 327694,
        "HTML": 20192,
        "Rust": 16627,
        "Shell": 13520
      },
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "# nanochat\n\n![nanochat logo](dev/nanochat.png)\n\n> The best ChatGPT that $100 can buy.\n\nThis repo is a full-stack implementation of an LLM like ChatGPT in a single, clean, minimal, hackable, dependency-lite codebase. nanochat is designed to run on a single 8XH100 node via scripts like [speedrun.sh](speedrun.sh), that run the entire pipeline start to end. This includes tokenization, pretraining, finetuning, evaluation, inference, and web serving over a simple UI so that you can talk to your own LLM just like ChatGPT. nanochat will become the capstone project of the course LLM101n being developed by Eureka Labs.\n\n## Talk to it\n\nTo get a sense of the endpoint of this repo, you can currently find [nanochat d32](https://github.com/karpathy/nanochat/discussions/8) hosted on [nanochat.karpathy.ai](https://nanochat.karpathy.ai/). \"d32\" means that this model has 32 layers in the Transformer neural network. This model has 1.9 billion parameters, it was trained on 38 billion tokens by simply running the single script [run1000.sh](run1000.sh), and the total cost of training was ~$800 (about 33 hours training time on 8XH100 GPU node). While today this is enough to outperform GPT-2 of 2019, it falls dramatically short of modern Large Language Models like GPT-5. When talking to these micro models, you'll see that they make a lot of mistakes, they are a little bit naive and silly and they hallucinate a ton, a bit like children. It's kind of amusing. But what makes nanochat unique is that it is fully yours - fully configurable, tweakable, hackable, and trained by you from start to end. To train and talk to your own, we turn to...\n\n## Quick start\n\nThe fastest way to feel the magic is to run the speedrun script [speedrun.sh](speedrun.sh), which trains and inferences the $100 tier of nanochat. On an 8XH100 node at $24/hr, this gives a total run time of about 4 hours. Boot up a new 8XH100 GPU box from your favorite provider (e.g. I use and like [Lambda](https://lambda.ai/service/gpu-cloud)), and kick off the training script:\n\n```bash\nbash speedrun.sh\n```\n\nAlternatively, since the script runs for 4 hours, I like to launch it like this inside a new screen session `speedrun` (and also log output to `speedrun.log`):\n\n```bash\nscreen -L -Logfile speedrun.log -S speedrun bash speedrun.sh\n```\n\nSee the [screen cheatsheet](https://gist.github.com/jctosta/af918e1618682638aa82) if you are less familiar. You can watch it go inside the screen session, or detach with `Ctrl-a d` and `tail speedrun.log` to view progress. Now wait 4 hours. Once it's done, you can talk to your LLM via the ChatGPT-like web UI. Make sure again that your local uv virtual environment is active (run `source .venv/bin/activate`), and serve it:\n\n```bash\npython -m scripts.chat_web\n```\n\nAnd then visit the URL shown. Make sure to access it correctly, e.g. on Lambda use the public IP of the node you're on, followed by the port, so for example [http://209.20.xxx.xxx:8000/](http://209.20.xxx.xxx:8000/), etc. Then talk to your LLM as you'd normally talk to ChatGPT! Get it to write stories or poems. Ask it to tell you who you are to see a hallucination. Ask it why the sky is blue. Or why it's green. The speedrun is a 4e19 FLOPs capability model so it's a bit like talking to a kindergartener :).\n\n---\n\n<img width=\"2672\" height=\"1520\" alt=\"image\" src=\"https://github.com/user-attachments/assets/ed39ddf8-2370-437a-bedc-0f39781e76b5\" />\n\n---\n\nYou can also `cat report.md` file which appeared in the project directory and contains the \"report card\" of the run, i.e. a bunch of evaluations and metrics. At the very end, you'll see a summary table, for example:\n\n---\n\n- Characters: 333,989\n- Lines: 8,304\n- Files: 44\n- Tokens (approx): 83,497\n- Dependencies (uv.lock lines): 2,004\n\n| Metric          | BASE     | MID      | SFT      | RL       |\n|-----------------|----------|----------|----------|----------|\n| CORE            | 0.2219   | -        | -        | -        |\n| ARC-Challenge   | -        | 0.2875   | 0.2807   | -        |\n| ARC-Easy        | -        | 0.3561   | 0.3876   | -        |\n| GSM8K           | -        | 0.0250   | 0.0455   | 0.0758   |\n| HumanEval       | -        | 0.0671   | 0.0854   | -        |\n| MMLU            | -        | 0.3111   | 0.3151   | -        |\n| ChatCORE        | -        | 0.0730   | 0.0884   | -        |\n\nTotal wall clock time: 3h51m\n\n---\n\n(Your table might be missing the RL number by default). For a lot more information around the speedrun script and what to look for and expect, please refer to the walkthrough that I posted in Discussions of the repo: [\"Introducing nanochat: The best ChatGPT that $100 can buy\"](https://github.com/karpathy/nanochat/discussions/1).\n\n## Bigger models\n\nUnsurprisingly, $100 is not enough to train a highly performant ChatGPT clone. In fact, LLMs are famous for their multi-million dollar capex. For our purposes, I think there are two more scales of interest. First is the ~$300 tier d26 model (i.e. depth=26) that trains in ~1",
      "default_branch": "master"
    },
    "fetched_at": "2025-11-09T02:30:12.001164"
  },
  {
    "basic_info": {
      "name": "DeepSeek-OCR",
      "full_name": "deepseek-ai/DeepSeek-OCR",
      "owner": "deepseek-ai",
      "description": "Contexts Optical Compression",
      "url": "https://github.com/deepseek-ai/DeepSeek-OCR",
      "clone_url": "https://github.com/deepseek-ai/DeepSeek-OCR.git",
      "ssh_url": "git@github.com:deepseek-ai/DeepSeek-OCR.git",
      "homepage": null,
      "created_at": "2025-10-17T06:14:27Z",
      "updated_at": "2025-11-09T02:17:11Z",
      "pushed_at": "2025-10-25T02:43:18Z"
    },
    "stats": {
      "stars": 19917,
      "forks": 1440,
      "watchers": 19917,
      "open_issues": 210,
      "size": 7948
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 113538
      },
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "<!-- markdownlint-disable first-line-h1 -->\n<!-- markdownlint-disable html -->\n<!-- markdownlint-disable no-duplicate-header -->\n\n\n<div align=\"center\">\n  <img src=\"assets/logo.svg\" width=\"60%\" alt=\"DeepSeek AI\" />\n</div>\n\n\n<hr>\n<div align=\"center\">\n  <a href=\"https://www.deepseek.com/\" target=\"_blank\">\n    <img alt=\"Homepage\" src=\"assets/badge.svg\" />\n  </a>\n  <a href=\"https://huggingface.co/deepseek-ai/DeepSeek-OCR\" target=\"_blank\">\n    <img alt=\"Hugging Face\" src=\"https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-DeepSeek%20AI-ffc107?color=ffc107&logoColor=white\" />\n  </a>\n\n</div>\n\n<div align=\"center\">\n\n  <a href=\"https://discord.gg/Tc7c45Zzu5\" target=\"_blank\">\n    <img alt=\"Discord\" src=\"https://img.shields.io/badge/Discord-DeepSeek%20AI-7289da?logo=discord&logoColor=white&color=7289da\" />\n  </a>\n  <a href=\"https://twitter.com/deepseek_ai\" target=\"_blank\">\n    <img alt=\"Twitter Follow\" src=\"https://img.shields.io/badge/Twitter-deepseek_ai-white?logo=x&logoColor=white\" />\n  </a>\n\n</div>\n\n\n\n<p align=\"center\">\n  <a href=\"https://huggingface.co/deepseek-ai/DeepSeek-OCR\"><b>üì• Model Download</b></a> |\n  <a href=\"https://github.com/deepseek-ai/DeepSeek-OCR/blob/main/DeepSeek_OCR_paper.pdf\"><b>üìÑ Paper Link</b></a> |\n  <a href=\"https://arxiv.org/abs/2510.18234\"><b>üìÑ Arxiv Paper Link</b></a> |\n</p>\n\n<h2>\n<p align=\"center\">\n  <a href=\"\">DeepSeek-OCR: Contexts Optical Compression</a>\n</p>\n</h2>\n\n<p align=\"center\">\n<img src=\"assets/fig1.png\" style=\"width: 1000px\" align=center>\n</p>\n<p align=\"center\">\n<a href=\"\">Explore the boundaries of visual-text compression.</a>       \n</p>\n\n## Release\n- [2025/10/23]üöÄüöÄüöÄ DeepSeek-OCR is now officially supported in upstream [vLLM](https://docs.vllm.ai/projects/recipes/en/latest/DeepSeek/DeepSeek-OCR.html#installing-vllm). Thanks to the [vLLM](https://github.com/vllm-project/vllm) team for their help.\n- [2025/10/20]üöÄüöÄüöÄ We release DeepSeek-OCR, a model to investigate the role of vision encoders from an LLM-centric viewpoint.\n\n## Contents\n- [Install](#install)\n- [vLLM Inference](#vllm-inference)\n- [Transformers Inference](#transformers-inference)\n  \n\n\n\n\n## Install\n>Our environment is cuda11.8+torch2.6.0.\n1. Clone this repository and navigate to the DeepSeek-OCR folder\n```bash\ngit clone https://github.com/deepseek-ai/DeepSeek-OCR.git\n```\n2. Conda\n```Shell\nconda create -n deepseek-ocr python=3.12.9 -y\nconda activate deepseek-ocr\n```\n3. Packages\n\n- download the vllm-0.8.5 [whl](https://github.com/vllm-project/vllm/releases/tag/v0.8.5) \n```Shell\npip install torch==2.6.0 torchvision==0.21.0 torchaudio==2.6.0 --index-url https://download.pytorch.org/whl/cu118\npip install vllm-0.8.5+cu118-cp38-abi3-manylinux1_x86_64.whl\npip install -r requirements.txt\npip install flash-attn==2.7.3 --no-build-isolation\n```\n**Note:** if you want vLLM and transformers codes to run in the same environment, you don't need to worry about this installation error like: vllm 0.8.5+cu118 requires transformers>=4.51.1\n\n## vLLM-Inference\n- VLLM:\n>**Note:** change the INPUT_PATH/OUTPUT_PATH and other settings in the DeepSeek-OCR-master/DeepSeek-OCR-vllm/config.py\n```Shell\ncd DeepSeek-OCR-master/DeepSeek-OCR-vllm\n```\n1. image: streaming output\n```Shell\npython run_dpsk_ocr_image.py\n```\n2. pdf: concurrency ~2500tokens/s(an A100-40G)\n```Shell\npython run_dpsk_ocr_pdf.py\n```\n3. batch eval for benchmarks\n```Shell\npython run_dpsk_ocr_eval_batch.py\n```\n\n**[2025/10/23] The version of upstream [vLLM](https://docs.vllm.ai/projects/recipes/en/latest/DeepSeek/DeepSeek-OCR.html#installing-vllm):**\n\n```shell\nuv venv\nsource .venv/bin/activate\n# Until v0.11.1 release, you need to install vLLM from nightly build\nuv pip install -U vllm --pre --extra-index-url https://wheels.vllm.ai/nightly\n```\n\n```python\nfrom vllm import LLM, SamplingParams\nfrom vllm.model_executor.models.deepseek_ocr import NGramPerReqLogitsProcessor\nfrom PIL import Image\n\n# Create model instance\nllm = LLM(\n    model=\"deepseek-ai/DeepSeek-OCR\",\n    enable_prefix_caching=False,\n    mm_processor_cache_gb=0,\n    logits_processors=[NGramPerReqLogitsProcessor]\n)\n\n# Prepare batched input with your image file\nimage_1 = Image.open(\"path/to/your/image_1.png\").convert(\"RGB\")\nimage_2 = Image.open(\"path/to/your/image_2.png\").convert(\"RGB\")\nprompt = \"<image>\\nFree OCR.\"\n\nmodel_input = [\n    {\n        \"prompt\": prompt,\n        \"multi_modal_data\": {\"image\": image_1}\n    },\n    {\n        \"prompt\": prompt,\n        \"multi_modal_data\": {\"image\": image_2}\n    }\n]\n\nsampling_param = SamplingParams(\n            temperature=0.0,\n            max_tokens=8192,\n            # ngram logit processor args\n            extra_args=dict(\n                ngram_size=30,\n                window_size=90,\n                whitelist_token_ids={128821, 128822},  # whitelist: <td>, </td>\n            ),\n            skip_special_tokens=False,\n        )\n# Generate output\nmodel_outputs = llm.generate(model_input, sampling_param)\n\n# Print output\nfor output in model_outputs:\n    print(output.outputs[0].text)\n```\n## ",
      "default_branch": "main"
    },
    "fetched_at": "2025-11-09T02:30:13.271271"
  },
  {
    "basic_info": {
      "name": "AI-Trader",
      "full_name": "HKUDS/AI-Trader",
      "owner": "HKUDS",
      "description": "\"AI-Trader: Can AI Beat the Market?\" Live Trading Bench: https://hkuds.github.io/AI-Trader/",
      "url": "https://github.com/HKUDS/AI-Trader",
      "clone_url": "https://github.com/HKUDS/AI-Trader.git",
      "ssh_url": "git@github.com:HKUDS/AI-Trader.git",
      "homepage": "",
      "created_at": "2025-10-23T12:45:00Z",
      "updated_at": "2025-11-09T02:04:36Z",
      "pushed_at": "2025-11-08T17:45:43Z"
    },
    "stats": {
      "stars": 9125,
      "forks": 1334,
      "watchers": 9125,
      "open_issues": 40,
      "size": 13134
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 247389,
        "Shell": 3328
      },
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "<div align=\"center\">\n\n# üöÄ AI-Trader: Can AI Beat the Market?\n\n[![Python](https://img.shields.io/badge/Python-3.10+-blue.svg)](https://python.org)\n[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)\n[![GitHub stars](https://img.shields.io/github/stars/HKUDS/AI-Trader?style=social)](https://github.com/HKUDS/AI-Trader)\n[![Feishu](https://img.shields.io/badge/üí¨Feishu-Group-blue?style=flat)](./Communication.md) \n[![WeChat](https://img.shields.io/badge/WeChat-Group-green?style=flat&logo=wechat)](./Communication.md)\n\n**AI agents battle for supremacy in NASDAQ 100 and SSE 50 markets. Zero human input. Pure competition.**\n\n## üèÜ Current Championship Leaderboard üèÜ \n[*Click Here: AI Live Trading*](https://ai4trade.ai)\n\n</div>\n\n---\n## üéâ Weekly Update\n\nWe're excited to announce the following major updates completed this week:\n\n### üìà Market Expansion\n- ‚úÖ **A-Share Market Support** - Extended our trading capabilities to include Chinese A-share markets, expanding our global market coverage.\n\n### ‚è∞ Enhanced Trading Capabilities\n- ‚úÖ **Hourly Trading Support** - We've upgraded from daily to hourly trading intervals, enabling more precise and responsive market participation with granular timing control.\n\n### üé® User Experience Improvements\n- ‚úÖ **Live Trading Dashboard** - Introduced real-time visualization of all agent trading activities, providing comprehensive oversight of market operations.\n\n- ‚úÖ **Agent Reasoning Display** - Implemented complete transparency into AI decision-making processes, featuring detailed reasoning chains that show how each trading decision is formed.\n\n- ‚úÖ **Interactive Leaderboard** - Launched a dynamic performance ranking system with live updates, allowing users to track and compare agent performance in real-time.\n\n---\n\n## **How to use this dataset**\n\nIt's simple! \n\nYou just need to submit a PR that includes at least: `./agent/{your_strategy}.py` (you can inherit from Basemodel to create your strategy!), `./configs/{yourconfig}`, and instructions on how to run your strategy. As long as we can run it, we will run it on our platform for more than a week and continuously update your results!\n\n---\n\n<div align=\"center\">\n\n[üöÄ Quick Start](#-quick-start) ‚Ä¢ [üìà Performance Analysis](#-performance-analysis) ‚Ä¢ [üõ†Ô∏è Configuration Guide](#-configuration-guide) ‚Ä¢ [‰∏≠ÊñáÊñáÊ°£](README_CN.md)\n\n</div>\n\n\n## üåü Project Introduction\n\n> **AI-Trader enables five distinct AI models, each employing unique investment strategies, to compete autonomously in the same market and determine which can generate the highest profits in NASDAQ 100 or SSE 50 trading!**\n\n### üéØ Core Features\n\n- ü§ñ **Fully Autonomous Decision-Making**: AI agents perform 100% independent analysis, decision-making, and execution without human intervention\n- üõ†Ô∏è **Pure Tool-Driven Architecture**: Built on MCP toolchain, enabling AI to complete all trading operations through standardized tool calls\n- üèÜ **Multi-Model Competition Arena**: Deploy multiple AI models (GPT, Claude, Qwen, etc.) for competitive trading\n- üìä **Real-Time Performance Analytics**: Comprehensive trading records, position monitoring, and profit/loss analysis\n- üîç **Intelligent Market Intelligence**: Integrated Jina search for real-time market news and financial reports\n- ‚ö° **MCP Toolchain Integration**: Modular tool ecosystem based on Model Context Protocol\n- üîå **Extensible Strategy Framework**: Support for third-party strategies and custom AI agent integration\n- ‚è∞ **Historical Replay Capability**: Time-period replay functionality with automatic future information filtering\n\n---\n\n### üéÆ Trading Environment\nEach AI model starts with $10,000 or 100,000¬• to trade NASDAQ 100 stocks or SSE 50 stocks in a controlled environment with real market data and historical replay capabilities.\n\n- üí∞ **Initial Capital**: $10,000 USD or 100,000¬• CNY starting balance\n- üìà **Trading Universe**: NASDAQ 100 component stocks (top 100 technology stocks) or SSE 50 component stocks\n- ‚è∞ **Trading Schedule**: Weekday market hours with historical simulation support\n- üìä **Data Integration**: Alpha Vantage API combined with Jina AI market intelligence\n- üîÑ **Time Management**: Historical period replay with automated future information filtering\n\n---\n\n### üß† Agentic Trading Capabilities\nAI agents operate with complete autonomy, conducting market research, making trading decisions, and continuously evolving their strategies without human intervention.\n\n- üì∞ **Autonomous Market Research**: Intelligent retrieval and filtering of market news, analyst reports, and financial data\n- üí° **Independent Decision Engine**: Multi-dimensional analysis driving fully autonomous buy/sell execution\n- üìù **Comprehensive Trade Logging**: Automated documentation of trading rationale, execution details, and portfolio changes\n- üîÑ **Adaptive Strategy Evolution**: Self-optimizing algorithms that adjust based on market performance feedback\n\n---\n\n### üèÅ Competition Rules\nAll AI models compete under identical conditions with the same capital, data ac",
      "default_branch": "main"
    },
    "fetched_at": "2025-11-09T02:30:14.557634"
  },
  {
    "basic_info": {
      "name": "Skill_Seekers",
      "full_name": "yusufkaraaslan/Skill_Seekers",
      "owner": "yusufkaraaslan",
      "description": "Convert documentation websites, GitHub repositories, and PDFs into Claude AI skills with automatic conflict detection",
      "url": "https://github.com/yusufkaraaslan/Skill_Seekers",
      "clone_url": "https://github.com/yusufkaraaslan/Skill_Seekers.git",
      "ssh_url": "git@github.com:yusufkaraaslan/Skill_Seekers.git",
      "homepage": "",
      "created_at": "2025-10-17T14:43:48Z",
      "updated_at": "2025-11-09T02:28:03Z",
      "pushed_at": "2025-11-06T22:25:41Z"
    },
    "stats": {
      "stars": 3604,
      "forks": 362,
      "watchers": 3604,
      "open_issues": 118,
      "size": 696
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 682739,
        "Shell": 8958
      },
      "license": "MIT License",
      "topics": [
        "ai-tools",
        "ast-parser",
        "automation",
        "claude-ai",
        "claude-skills",
        "code-analysis",
        "conflict-detection",
        "documentation",
        "documentation-generator",
        "github",
        "github-scraper",
        "mcp",
        "mcp-server",
        "multi-source",
        "ocr",
        "pdf",
        "python",
        "web-scraping"
      ]
    },
    "content": {
      "readme": "[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/yusufkaraaslan-skill-seekers-badge.png)](https://mseep.ai/app/yusufkaraaslan-skill-seekers)\n\n# Skill Seeker\n\n[![Version](https://img.shields.io/badge/version-2.0.0-blue.svg)](https://github.com/yusufkaraaslan/Skill_Seekers/releases/tag/v2.0.0)\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)\n[![MCP Integration](https://img.shields.io/badge/MCP-Integrated-blue.svg)](https://modelcontextprotocol.io)\n[![Tested](https://img.shields.io/badge/Tests-299%20Passing-brightgreen.svg)](tests/)\n[![Project Board](https://img.shields.io/badge/Project-Board-purple.svg)](https://github.com/users/yusufkaraaslan/projects/2)\n\n**Automatically convert documentation websites, GitHub repositories, and PDFs into Claude AI skills in minutes.**\n\n> üìã **[View Development Roadmap & Tasks](https://github.com/users/yusufkaraaslan/projects/2)** - 134 tasks across 10 categories, pick any to contribute!\n\n## What is Skill Seeker?\n\nSkill Seeker is an automated tool that transforms documentation websites, GitHub repositories, and PDF files into production-ready [Claude AI skills](https://www.anthropic.com/news/skills). Instead of manually reading and summarizing documentation, Skill Seeker:\n\n1. **Scrapes** multiple sources (docs, GitHub repos, PDFs) automatically\n2. **Analyzes** code repositories with deep AST parsing\n3. **Detects** conflicts between documentation and code implementation\n4. **Organizes** content into categorized reference files\n5. **Enhances** with AI to extract best examples and key concepts\n6. **Packages** everything into an uploadable `.zip` file for Claude\n\n**Result:** Get comprehensive Claude skills for any framework, API, or tool in 20-40 minutes instead of hours of manual work.\n\n## Why Use This?\n\n- üéØ **For Developers**: Create skills from documentation + GitHub repos with conflict detection\n- üéÆ **For Game Devs**: Generate skills for game engines (Godot docs + GitHub, Unity, etc.)\n- üîß **For Teams**: Combine internal docs + code repositories into single source of truth\n- üìö **For Learners**: Build comprehensive skills from docs, code examples, and PDFs\n- üîç **For Open Source**: Analyze repos to find documentation gaps and outdated examples\n\n## Key Features\n\n### üåê Documentation Scraping\n- ‚úÖ **llms.txt Support** - Automatically detects and uses LLM-ready documentation files (10x faster)\n- ‚úÖ **Universal Scraper** - Works with ANY documentation website\n- ‚úÖ **Smart Categorization** - Automatically organizes content by topic\n- ‚úÖ **Code Language Detection** - Recognizes Python, JavaScript, C++, GDScript, etc.\n- ‚úÖ **8 Ready-to-Use Presets** - Godot, React, Vue, Django, FastAPI, and more\n\n### üìÑ PDF Support (**v1.2.0**)\n- ‚úÖ **Basic PDF Extraction** - Extract text, code, and images from PDF files\n- ‚úÖ **OCR for Scanned PDFs** - Extract text from scanned documents\n- ‚úÖ **Password-Protected PDFs** - Handle encrypted PDFs\n- ‚úÖ **Table Extraction** - Extract complex tables from PDFs\n- ‚úÖ **Parallel Processing** - 3x faster for large PDFs\n- ‚úÖ **Intelligent Caching** - 50% faster on re-runs\n\n### üêô GitHub Repository Scraping (**v2.0.0**)\n- ‚úÖ **Deep Code Analysis** - AST parsing for Python, JavaScript, TypeScript, Java, C++, Go\n- ‚úÖ **API Extraction** - Functions, classes, methods with parameters and types\n- ‚úÖ **Repository Metadata** - README, file tree, language breakdown, stars/forks\n- ‚úÖ **GitHub Issues & PRs** - Fetch open/closed issues with labels and milestones\n- ‚úÖ **CHANGELOG & Releases** - Automatically extract version history\n- ‚úÖ **Conflict Detection** - Compare documented APIs vs actual code implementation\n- ‚úÖ **MCP Integration** - Natural language: \"Scrape GitHub repo facebook/react\"\n\n### üîÑ Unified Multi-Source Scraping (**NEW - v2.0.0**)\n- ‚úÖ **Combine Multiple Sources** - Mix documentation + GitHub + PDF in one skill\n- ‚úÖ **Conflict Detection** - Automatically finds discrepancies between docs and code\n- ‚úÖ **Intelligent Merging** - Rule-based or AI-powered conflict resolution\n- ‚úÖ **Transparent Reporting** - Side-by-side comparison with ‚ö†Ô∏è warnings\n- ‚úÖ **Documentation Gap Analysis** - Identifies outdated docs and undocumented features\n- ‚úÖ **Single Source of Truth** - One skill showing both intent (docs) and reality (code)\n- ‚úÖ **Backward Compatible** - Legacy single-source configs still work\n\n### ü§ñ AI & Enhancement\n- ‚úÖ **AI-Powered Enhancement** - Transforms basic templates into comprehensive guides\n- ‚úÖ **No API Costs** - FREE local enhancement using Claude Code Max\n- ‚úÖ **MCP Server for Claude Code** - Use directly from Claude Code with natural language\n\n### ‚ö° Performance & Scale\n- ‚úÖ **Async Mode** - 2-3x faster scraping with async/await (use `--async` flag)\n- ‚úÖ **Large Documentation Support** - Handle 10K-40K+ page docs with intelligent splitting\n- ‚úÖ **Router/Hub Skills** - Intelligent routing to specialized sub-skills\n-",
      "default_branch": "development"
    },
    "fetched_at": "2025-11-09T02:30:15.803440"
  },
  {
    "basic_info": {
      "name": "dexter",
      "full_name": "virattt/dexter",
      "owner": "virattt",
      "description": "An autonomous agent for deep financial research",
      "url": "https://github.com/virattt/dexter",
      "clone_url": "https://github.com/virattt/dexter.git",
      "ssh_url": "git@github.com:virattt/dexter.git",
      "homepage": null,
      "created_at": "2025-10-14T21:02:00Z",
      "updated_at": "2025-11-08T18:23:07Z",
      "pushed_at": "2025-11-05T23:32:24Z"
    },
    "stats": {
      "stars": 2733,
      "forks": 348,
      "watchers": 2733,
      "open_issues": 8,
      "size": 105
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 67362,
        "JavaScript": 228
      },
      "license": null,
      "topics": []
    },
    "content": {
      "readme": "# Dexter ü§ñ\n\nDexter is an autonomous financial research agent that thinks, plans, and learns as it works. It performs analysis using task planning, self-reflection, and real-time market data. Think Claude Code, but built specifically for financial research.\n\n\n<img width=\"979\" height=\"651\" alt=\"Screenshot 2025-10-14 at 6 12 35‚ÄØPM\" src=\"https://github.com/user-attachments/assets/5a2859d4-53cf-4638-998a-15cef3c98038\" />\n\n## Overview\n\nDexter takes complex financial questions and turns them into clear, step-by-step research plans. It runs those tasks using live market data, checks its own work, and refines the results until it has a confident, data-backed answer.  \n\n**Key Capabilities:**\n- **Intelligent Task Planning**: Automatically decomposes complex queries into structured research steps\n- **Autonomous Execution**: Selects and executes the right tools to gather financial data\n- **Self-Validation**: Checks its own work and iterates until tasks are complete\n- **Real-Time Financial Data**: Access to income statements, balance sheets, and cash flow statements\n- **Safety Features**: Built-in loop detection and step limits to prevent runaway execution\n\n[![Twitter Follow](https://img.shields.io/twitter/follow/virattt?style=social)](https://twitter.com/virattt)\n\n### Prerequisites\n\n- Python 3.10 or higher\n- [uv](https://github.com/astral-sh/uv) package manager\n- OpenAI API key (get [here](https://platform.openai.com/api-keys))\n- Financial Datasets API key (get [here](https://financialdatasets.ai))\n\n### Installation\n\n1. Clone the repository:\n```bash\ngit clone https://github.com/virattt/dexter.git\ncd dexter\n```\n\n2. Install dependencies with uv:\n```bash\nuv sync\n```\n\n3. Set up your environment variables:\n```bash\n# Copy the example environment file\ncp env.example .env\n\n# Edit .env and add your API keys\n# OPENAI_API_KEY=your-openai-api-key\n# FINANCIAL_DATASETS_API_KEY=your-financial-datasets-api-key\n```\n\n### Usage\n\nRun Dexter in interactive mode:\n```bash\nuv run dexter-agent\n```\n\n### Example Queries\n\nTry asking Dexter questions like:\n- \"What was Apple's revenue growth over the last 4 quarters?\"\n- \"Compare Microsoft and Google's operating margins for 2023\"\n- \"Analyze Tesla's cash flow trends over the past year\"\n- \"What is Amazon's debt-to-equity ratio based on recent financials?\"\n\nDexter will automatically:\n1. Break down your question into research tasks\n2. Fetch the necessary financial data\n3. Perform calculations and analysis\n4. Provide a comprehensive, data-rich answer\n\n## Architecture\n\nDexter uses a multi-agent architecture with specialized components:\n\n- **Planning Agent**: Analyzes queries and creates structured task lists\n- **Action Agent**: Selects appropriate tools and executes research steps\n- **Validation Agent**: Verifies task completion and data sufficiency\n- **Answer Agent**: Synthesizes findings into comprehensive responses\n\n## Project Structure\n\n```\ndexter/\n‚îú‚îÄ‚îÄ src/\n‚îÇ   ‚îú‚îÄ‚îÄ dexter/\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ agent.py      # Main agent orchestration logic\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ model.py      # LLM interface\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ tools.py      # Financial data tools\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ prompts.py    # System prompts for each component\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ schemas.py    # Pydantic models\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ utils/        # Utility functions\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ cli.py        # CLI entry point\n‚îú‚îÄ‚îÄ pyproject.toml\n‚îî‚îÄ‚îÄ uv.lock\n```\n\n## Configuration\n\nDexter supports configuration via the `Agent` class initialization:\n\n```python\nfrom dexter.agent import Agent\n\nagent = Agent(\n    max_steps=20,              # Global safety limit\n    max_steps_per_task=5       # Per-task iteration limit\n)\n```\n\n## How to Contribute\n\n1. Fork the repository\n2. Create a feature branch\n3. Commit your changes\n4. Push to the branch\n5. Create a Pull Request\n\n**Important**: Please keep your pull requests small and focused.  This will make it easier to review and merge.\n\n\n## License\n\nThis project is licensed under the MIT License.\n\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-11-09T02:30:17.053453"
  },
  {
    "basic_info": {
      "name": "kimi-cli",
      "full_name": "MoonshotAI/kimi-cli",
      "owner": "MoonshotAI",
      "description": "Kimi CLI is your next CLI agent.",
      "url": "https://github.com/MoonshotAI/kimi-cli",
      "clone_url": "https://github.com/MoonshotAI/kimi-cli.git",
      "ssh_url": "git@github.com:MoonshotAI/kimi-cli.git",
      "homepage": null,
      "created_at": "2025-10-15T12:58:03Z",
      "updated_at": "2025-11-09T02:30:07Z",
      "pushed_at": "2025-11-09T02:28:14Z"
    },
    "stats": {
      "stars": 2544,
      "forks": 209,
      "watchers": 2544,
      "open_issues": 49,
      "size": 1525
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 523984,
        "Makefile": 3545
      },
      "license": "Apache License 2.0",
      "topics": []
    },
    "content": {
      "readme": "# Kimi CLI\n\n[![Commit Activity](https://img.shields.io/github/commit-activity/w/MoonshotAI/kimi-cli)](https://github.com/MoonshotAI/kimi-cli/graphs/commit-activity)\n[![Checks](https://img.shields.io/github/check-runs/MoonshotAI/kimi-cli/main)](https://github.com/MoonshotAI/kimi-cli/actions)\n[![Version](https://img.shields.io/pypi/v/kimi-cli)](https://pypi.org/project/kimi-cli/)\n[![Downloads](https://img.shields.io/pypi/dw/kimi-cli)](https://pypistats.org/packages/kimi-cli)\n[![Ask DeepWiki](https://deepwiki.com/badge.svg)](https://deepwiki.com/MoonshotAI/kimi-cli)\n\n[‰∏≠Êñá](https://www.kimi.com/coding/docs/kimi-cli.html)\n\nKimi CLI is a new CLI agent that can help you with your software development tasks and terminal operations.\n\n> [!IMPORTANT]\n> Kimi CLI is currently in technical preview.\n\n## Key features\n\n- Shell-like UI and raw shell command execution\n- Zsh integration\n- [Agent Client Protocol] support\n- MCP support\n- And more to come...\n\n[Agent Client Protocol]: https://github.com/agentclientprotocol/agent-client-protocol\n\n## Installation\n\n> [!IMPORTANT]\n> Kimi CLI currently only supports macOS and Linux. Windows support is coming soon.\n\nKimi CLI is published as a Python package on PyPI. We highly recommend installing it with [uv](https://docs.astral.sh/uv/). If you have not installed uv yet, please follow the instructions [here](https://docs.astral.sh/uv/getting-started/installation/) to install it first.\n\nOnce uv is installed, you can install Kimi CLI with:\n\n```sh\nuv tool install --python 3.13 kimi-cli\n```\n\nRun `kimi --help` to check if Kimi CLI is installed successfully.\n\n> [!IMPORTANT]\n> Due to the security checks on macOS, the first time you run `kimi` command may take 10 seconds or more depending on your system environment.\n\n## Upgrading\n\nUpgrade Kimi CLI to the latest version with:\n\n```sh\nuv tool upgrade kimi-cli --no-cache\n```\n\n## Usage\n\nRun `kimi` command in the directory you want to work on, then send `/setup` to setup Kimi CLI:\n\n![](./docs/images/setup.png)\n\nAfter setup, Kimi CLI will be ready to use. You can send `/help` to get more information.\n\n## Features\n\n### Shell mode\n\nKimi CLI is not only a coding agent, but also a shell. You can switch the mode by pressing `Ctrl-X`. In shell mode, you can directly run shell commands without leaving Kimi CLI.\n\n> [!NOTE]\n> Built-in shell commands like `cd` are not supported yet.\n\n### Zsh integration\n\nYou can use Kimi CLI together with Zsh, to empower your shell experience with AI agent capabilities.\n\nInstall the [zsh-kimi-cli](https://github.com/MoonshotAI/zsh-kimi-cli) plugin via:\n\n```sh\ngit clone https://github.com/MoonshotAI/zsh-kimi-cli.git \\\n  ${ZSH_CUSTOM:-~/.oh-my-zsh/custom}/plugins/kimi-cli\n```\n\n> [!NOTE]\n> If you are using a plugin manager other than Oh My Zsh, you may need to refer to the plugin's README for installation instructions.\n\nThen add `kimi-cli` to your Zsh plugin list in `~/.zshrc`:\n\n```sh\nplugins=(... kimi-cli)\n```\n\nAfter restarting Zsh, you can switch to agent mode by pressing `Ctrl-X`.\n\n### ACP support\n\nKimi CLI supports [Agent Client Protocol] out of the box. You can use it together with any ACP-compatible editor or IDE.\n\nFor example, to use Kimi CLI with [Zed](https://zed.dev/), add the following configuration to your `~/.config/zed/settings.json`:\n\n```json\n{\n  \"agent_servers\": {\n    \"Kimi CLI\": {\n      \"command\": \"kimi\",\n      \"args\": [\"--acp\"],\n      \"env\": {}\n    }\n  }\n}\n```\n\nThen you can create Kimi CLI threads in Zed's agent panel.\n\n### Using MCP tools\n\nKimi CLI supports the well-established MCP config convention. For example:\n\n```json\n{\n  \"mcpServers\": {\n    \"context7\": {\n      \"url\": \"https://mcp.context7.com/mcp\",\n      \"headers\": {\n        \"CONTEXT7_API_KEY\": \"YOUR_API_KEY\"\n      }\n    },\n    \"chrome-devtools\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"chrome-devtools-mcp@latest\"]\n    }\n  }\n}\n```\n\nRun `kimi` with `--mcp-config-file` option to connect to the specified MCP servers:\n\n```sh\nkimi --mcp-config-file /path/to/mcp.json\n```\n\n## Development\n\nTo develop Kimi CLI, run:\n\n```sh\ngit clone https://github.com/MoonshotAI/kimi-cli.git\ncd kimi-cli\n\nmake prepare  # prepare the development environment\n```\n\nThen you can start working on Kimi CLI.\n\nRefer to the following commands after you make changes:\n\n```sh\nuv run kimi  # run Kimi CLI\n\nmake format  # format code\nmake check  # run linting and type checking\nmake test  # run tests\nmake help  # show all make targets\n```\n\n## Contributing\n\nWe welcome contributions to Kimi CLI! Please refer to [CONTRIBUTING.md](./CONTRIBUTING.md) for more information.\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-11-09T02:30:18.315365"
  },
  {
    "basic_info": {
      "name": "DeepAnalyze",
      "full_name": "ruc-datalab/DeepAnalyze",
      "owner": "ruc-datalab",
      "description": "DeepAnalyze is the first agentic LLM for autonomous data science.",
      "url": "https://github.com/ruc-datalab/DeepAnalyze",
      "clone_url": "https://github.com/ruc-datalab/DeepAnalyze.git",
      "ssh_url": "git@github.com:ruc-datalab/DeepAnalyze.git",
      "homepage": "https://ruc-deepanalyze.github.io",
      "created_at": "2025-10-11T11:19:21Z",
      "updated_at": "2025-11-09T02:23:38Z",
      "pushed_at": "2025-11-08T09:00:42Z"
    },
    "stats": {
      "stars": 1756,
      "forks": 222,
      "watchers": 1756,
      "open_issues": 14,
      "size": 23098
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 5372510,
        "Shell": 220988,
        "Jupyter Notebook": 61761,
        "Jinja": 3956,
        "Dockerfile": 3011,
        "Makefile": 997,
        "Batchfile": 764,
        "Scheme": 403
      },
      "license": "MIT License",
      "topics": [
        "agent",
        "agentic",
        "agentic-ai",
        "ai",
        "ai-scientist",
        "chatbot",
        "chatgpt",
        "data",
        "data-analysis",
        "data-engineering",
        "data-science",
        "data-visualization",
        "database",
        "gpt",
        "llama",
        "llm",
        "qwen",
        "science",
        "structured-data",
        "vllm"
      ]
    },
    "content": {
      "readme": "<p align=\"center\" width=\"100%\">\n<img src=\"assets/logo.png\" alt=\"DeepAnalyze\" style=\"width: 60%; min-width: 300px; display: block; margin: auto;\">\n</p>\n\n# DeepAnalyze: Agentic Large Language Models for Autonomous Data Science\n[![arXiv](https://img.shields.io/badge/arXiv-2510.16872-b31b1b.svg?logo=arXiv)](https://arxiv.org/abs/2510.16872)\n[![homepage](https://img.shields.io/badge/%F0%9F%8C%90%20Homepage%20-DeepAnalyze%20Cases-blue.svg)](https://ruc-deepanalyze.github.io/)\n[![model](https://img.shields.io/badge/%F0%9F%A4%97%20Huggingface%20-DeepAnalyze--8B-orange.svg)](https://huggingface.co/RUC-DataLab/DeepAnalyze-8B)\n[![data](https://img.shields.io/badge/%F0%9F%93%9A%20Datasets%20-DataScience--Instruct--500K-darkgreen.svg)](https://huggingface.co/datasets/RUC-DataLab/DataScience-Instruct-500K)\n[![star](https://img.shields.io/github/stars/ruc-datalab/DeepAnalyze?style=social&label=Code+Stars)](https://github.com/ruc-datalab/DeepAnalyze)\n![Badge](https://hitscounter.dev/api/hit?url=https%3A%2F%2Fgithub.com%2Fruc-datalab%2FDeepAnalyze&label=Visitors&icon=graph-up&color=%23dc3545&message=&style=flat&tz=UTC)  [![wechat](https://img.shields.io/badge/WeChat-%E5%8A%A0%E5%85%A5DeepAnalyze%E4%BA%A4%E6%B5%81%E8%AE%A8%E8%AE%BA%E7%BE%A4-black?logo=wechat&logoColor=07C160)](./assets/wechat.jpg) \n\n[![twitter](https://img.shields.io/badge/@Brian%20Roemmele-gray?logo=x&logoColor=white&labelColor=black)](https://x.com/BrianRoemmele/status/1981015483823571352) [![twitter](https://img.shields.io/badge/@Dr%20Singularity-gray?logo=x&logoColor=white&labelColor=black)](https://x.com/Dr_Singularity/status/1981010771338498241) [![twitter](https://img.shields.io/badge/@Gorden%20Sun-gray?logo=x&logoColor=white&labelColor=black)](https://x.com/Gorden_Sun/status/1980573407386423408) [![twitter](https://img.shields.io/badge/@AIGCLINK-gray?logo=x&logoColor=white&labelColor=black)](https://x.com/aigclink/status/1980554517126246642) [![twitter](https://img.shields.io/badge/@Python%20Developer-gray?logo=x&logoColor=white&labelColor=black)](https://x.com/Python_Dv/status/1980667557318377871) [![twitter](https://img.shields.io/badge/@meng%20shao-gray?logo=x&logoColor=white&labelColor=black)](https://x.com/shao__meng/status/1980623242114314531) \n\n\n> **Authors**: **[Shaolei Zhang](https://zhangshaolei1998.github.io/), [Ju Fan*](http://iir.ruc.edu.cn/~fanj/), [Meihao Fan](https://scholar.google.com/citations?user=9RTm2qoAAAAJ), [Guoliang Li](https://dbgroup.cs.tsinghua.edu.cn/ligl/), [Xiaoyong Du](http://info.ruc.edu.cn/jsky/szdw/ajxjgcx/jsjkxyjsx1/js2/7374b0a3f58045fc9543703ccea2eb9c.htm)**\n>\n> Renmin University of China, Tsinghua University\n\n\n**DeepAnalyze** is the first agentic LLM for autonomous data science. It can autonomously complete a wide range of data-centric tasks without human intervention, supporting:\n- üõ† **Entire data science pipeline**: Automatically perform any data science tasks such as data preparation, analysis, modeling, visualization, and report generation.\n- üîç **Open-ended data research**: Conduct deep research on diverse data sources, including structured data (Databases, CSV, Excel), semi-structured data (JSON, XML, YAML), and unstructured data (TXT, Markdown), and finally produce analyst-grade research reports.\n- üìä **Fully open-source**: The [model](https://huggingface.co/RUC-DataLab/DeepAnalyze-8B), [code](https://github.com/ruc-datalab/DeepAnalyze), [training data](https://huggingface.co/datasets/RUC-DataLab/DataScience-Instruct-500K), and [demo](https://huggingface.co/RUC-DataLab/DeepAnalyze-8B) of DeepAnalyze are all open-sourced, allowing you to deploy or extend your own data analysis assistant.\n\n<p align=\"center\" width=\"100%\">\n<img src=\"./assets/deepanalyze.jpg\" alt=\"deepanalyze\" style=\"width: 70%; min-width: 300px; display: block; margin: auto;\">\n</p>\n\n\n## üî• News\n- **[2025.11.08]**: DeepAnalyze is now accessible through the JupyterUI, building based on [jupyter-mcp-server](https://github.com/datalayer/jupyter-mcp-server). Thanks to the contributor [@ChengJiale150](https://github.com/ChengJiale150).\n- **[2025.10.28]**: We welcome all contributions, including improving the DeepAnalyze and sharing use cases (see [`CONTRIBUTION.md`](CONTRIBUTION.md)). All merged PRs will be listed as contributors.\n- **[2025.10.27]**: DeepAnalyze has attracted widespread attention, gaining **1K+** GitHub stars and **200K+** Twitter views within a week.\n- **[2025.10.21]**: DeepAnalyze's [paper](https://arxiv.org/abs/2510.16872), [code](https://github.com/ruc-datalab/DeepAnalyze), [model](https://huggingface.co/RUC-DataLab/DeepAnalyze-8B), [training data](https://huggingface.co/datasets/RUC-DataLab/DataScience-Instruct-500K) are released!\n\n## üñ• Demo\n\n### WebUI\n\nhttps://github.com/user-attachments/assets/04184975-7ee7-4ae0-8761-7a7550c5c8fe\n<p align=\"center\" width=\"100%\">\nUpload the data, DeepAnalyze can perform data-oriented deep research üîç and any data-centric tasks üõ†\n</p>\n\n- Clone this repo and download [DeepAnalyze-8B](https://huggingf",
      "default_branch": "main"
    },
    "fetched_at": "2025-11-09T02:30:19.590091"
  },
  {
    "basic_info": {
      "name": "PokeeResearchOSS",
      "full_name": "Pokee-AI/PokeeResearchOSS",
      "owner": "Pokee-AI",
      "description": "Pokee Deep Research Model Open Source Repo",
      "url": "https://github.com/Pokee-AI/PokeeResearchOSS",
      "clone_url": "https://github.com/Pokee-AI/PokeeResearchOSS.git",
      "ssh_url": "git@github.com:Pokee-AI/PokeeResearchOSS.git",
      "homepage": null,
      "created_at": "2025-10-17T07:15:18Z",
      "updated_at": "2025-11-08T10:25:13Z",
      "pushed_at": "2025-10-22T06:57:29Z"
    },
    "stats": {
      "stars": 1621,
      "forks": 1011,
      "watchers": 1621,
      "open_issues": 2,
      "size": 1835
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 386590,
        "Shell": 3417
      },
      "license": "Apache License 2.0",
      "topics": []
    },
    "content": {
      "readme": "This repository hosts Pokee‚Äôs state-of-the-art 7B DeepResearch Agent, which integrates web search and content reading capabilities to answer complex questions using the most up-to-date information available online.\n\n*We also offer an API hosting our proprietary deep research agent, which is up to 75% cheaper than OpenAI, Gemini, and Perplexity. It delivers comprehensive, citation-rich research reports with no hidden costs and no API key management required. (For more information about the API, visit [pokee.ai/deepresearch-preview](https://pokee.ai/deepresearch-preview))*\n\n<div align=\"center\">\n\n[![GitHub Stars](https://img.shields.io/github/stars/Pokee-AI/PokeeResearchOSS?style=social)](https://github.com/Pokee-AI/PokeeResearchOSS)\n[![arXiv](https://img.shields.io/badge/arXiv-2510.15862-b31b1b.svg)](https://arxiv.org/pdf/2510.15862)\n[![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Model-yellow)](https://huggingface.co/PokeeAI/pokee_research_7b)\n\n[![LinkedIn](https://img.shields.io/badge/LinkedIn-Follow-blue?style=social&logo=linkedin)](https://linkedin.com/company/pokee-ai)\n[![X (Twitter)](https://img.shields.io/badge/X-Follow-1DA1F2?style=social&logo=x)](https://x.com/pokee_ai)\n[![Discord](https://img.shields.io/badge/Discord-Join-5865F2?style=social&logo=discord)](https://discord.gg/VJXWQvyd)\n[![WeChat](https://img.shields.io/badge/WeChat-Join-07C160?style=social&logo=wechat)](https://i.postimg.cc/wv099v5w/wechat-group-pokee.jpg)\n\n<img src=\"Logo.png\" alt=\"Pokee AI Logo\" width=\"200\"/>\n<p style=\"text-align:center;\">\n  <a href=\"https://pokee.ai\" target=\"_blank\" rel=\"noopener noreferrer\"><strong>pokee.ai</strong></a>\n</p>\n\n</div>\n\n# PokeeResearch-7B Agent\n\nPokee's state-of-the-art 7B DeepResearch Agent that leverages web search and content reading capabilities to answer complex questions using the most up-to-date information available online.\n<div align=\"center\">\n<img src=\"hle_gaia_bro.png\" alt=\"HLE, GAIA and BrowseComp Performance\" width=\"600\"/>\n</div>\n<div align=\"center\">\n<img src=\"qa.png\" alt=\"7 QA Benchmark Performance\" width=\"800\"/>\n</div>\n\n## üöÄ Features\n\n- **Multi-turn Research**: Performs iterative web searches and content analysis\n- **Tool Integration**: Seamlessly integrates web search, content reading, and browsing tools\n- **Comprehensive Evaluation**: Includes benchmark evaluation across multiple QA datasets\n- **High Performance**: Achieves superior results on complex reasoning tasks\n- **Scalable Architecture**: Built on efficient 7B parameter model for optimal performance\n\n\n## üìã Requirements\n\n### Hardware\n- **Compute Node**: We tested the code on a single 80GB A100 GPU (GPUs with less memory may also work, though we have not tested them). Using multiple GPUs can further accelerate inference. For reference, the driver version is 570.133.20 and the CUDA toolkit version is 12.8.\n\n### Software\n- **Docker**: Environment to run the code will be provided as a docker image.\n\n### API Keys\nYou will need the following API keys:\n- **Serper API**: For web search functionality\n- **Jina API**: For web content reading and extraction\n- **Gemini API**: For content summarization and result evaluation\n- **HuggingFace Token**: For downloading the model from HuggingFace\n\n## üõ†Ô∏è Quick Start\n\n### 1. Environment Setup\nWe provide a Docker image for easy deployment:\n```bash\ndocker pull verlai/verl:app-verl0.5-transformers4.55.4-sglang0.4.10.post2-mcore0.13.0-te2.2\ndocker create --runtime=nvidia --gpus all --net=host --shm-size=\"80g\"  -v .:/workspace/ --name pokeeresearch verlai/verl:app-verl0.5-transformers4.55.4-sglang0.4.10.post2-mcore0.13.0-te2.2 sleep infinity\ndocker start pokeeresearch\ndocker exec -it pokeeresearch  bash\nssh-keygen -t ed25519 -C <USER_NAME>\n# copy /root/.ssh/id_ed25519.pub to github ssh keys\ngit clone git@github.com:Pokee-AI/PokeeResearchOSS.git --recursive\ncd PokeeResearchOSS\npip install colorlog\npip install -U google-genai\nhf auth login # enter your huggingface token, the tokens needs to have permission to use Pokee AI's models\ncd verl\npip install -e .\ncd ..\n```\n\nCreate a `.env` file in the project root and add your API keys:\n```bash\nSERPER_API_KEY=your_serper_api_key_here\nJINA_API_KEY=your_jina_api_key_here\nGEMINI_API_KEY=your_gemini_api_key_here\n```\n\n### 2. Modify ```run.sh``` to use more than one GPUs (optional)\nRunning the experiment with more GPUs is faster. By default the experiment uses one GPU.\nIf you want to use more GPUs, simply modify \n```bash\ntrainer.n_gpus_per_node=1 \\\n```\nin ```run.sh``` to \n```bash\ntrainer.n_gpus_per_node=<NUM_GPUS_TO_USE> \\\n```\n### 3. Run Benchmark Evaluation\n\n**Step 1: Start the Tool Server**\n```bash\npython start_tool_server.py \\\n--port <PORT_NUMBER> \\ # to specify the port to listen to (default 8888)\n--enable-cache # to enable caching tool results (recommended to save api credits)\n```\n\n**Step 2: Run the Evaluation**\n\nStart a new terminal, then run the experiment.\n```bash\ndocker exec -it pokeeresearch bash\ncd PokeeResearchOSS\nbash run.sh\n```\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-11-09T02:30:20.841069"
  },
  {
    "basic_info": {
      "name": "pico-banana-400k",
      "full_name": "apple/pico-banana-400k",
      "owner": "apple",
      "description": null,
      "url": "https://github.com/apple/pico-banana-400k",
      "clone_url": "https://github.com/apple/pico-banana-400k.git",
      "ssh_url": "git@github.com:apple/pico-banana-400k.git",
      "homepage": null,
      "created_at": "2025-10-21T21:15:35Z",
      "updated_at": "2025-11-09T02:09:07Z",
      "pushed_at": "2025-10-28T20:51:33Z"
    },
    "stats": {
      "stars": 1574,
      "forks": 67,
      "watchers": 1574,
      "open_issues": 6,
      "size": 7260
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 3369
      },
      "license": "Other",
      "topics": []
    },
    "content": {
      "readme": "# üçå Pico-Banana-400K: A Large-Scale Dataset for Text-Guided Image Editing\n\n<font size=7><div align='center' > [[üìñ Paper](https://www.arxiv.org/pdf/2510.19808)]  </div></font>\n\n**Pico-Banana-400K** is a large-scale dataset of **~400K text‚Äìimage‚Äìedit triplets** designed to advance research in **text-guided image editing**.  \nEach example contains:\n- an **original image** (from [Open Images](https://storage.googleapis.com/openimages/web/factsfigures.html)),  \n- a **human-like edit instruction**, and  \n- the **edited result** generated by Nano-Banana and verified by Gemini-2.5-Pro.\n\nThe dataset spans **35 edit operations** across **8 semantic categories**, covering diverse transformations‚Äîfrom low-level color adjustments to high-level object, scene, and stylistic edits.\n\n---\n\n\n\n## üß© Key Features\n\n| Feature | Description |\n|----------|-------------|\n| **Total Samples** | ~257K single-turn text‚Äìimage‚Äìedit triplets for SFT, ~56K single-turn text-image(positive) - image(negative)-edit for preference learning, and ~72K multi-turn texts-images-edits for multi-turn applications|\n| **Source** | [Open Images](https://storage.googleapis.com/openimages/web/factsfigures.html) |\n| **Edit Operations** | 35 across 8 semantic categories |\n| **Categories** | Pixel & Photometric, Object-Level, Scene Composition, Stylistic, Text & Symbol, Human-Centric, Scale & Perspective, Spatial/Layout |\n| **Image Resolution** | 512‚Äì1024 px |\n| **Prompt Generator** | [Gemini-2.5-Flash](https://deepmind.google/models/gemini/flash/) |\n| **Editing Model** | Nano-Banana |\n| **Self-Evaluation** | Automated judging pipeline using Gemini-2.5-Pro for edit quality |\n\n---\n\n## üèóÔ∏è Dataset Construction\n\nPico-Banana-400K is built using a **two-stage multimodal generation pipeline**:\n\n1. **Instruction Generation**  \n   Each Open Images sample is passed to *Gemini-2.5-Flash*, which writes concise, natural-language editing instructions grounded in visible content. We also provide short instructions summarized by Qwen-2.5-Instruct-7B. \n   Example:  \n   ```json\n   {\n     \"instruction\": \"Change the red car to blue.\"\n   }\n   \n\n2. **Editing + Self-Evaluation**\n   The Nano-Banana model performs the edit, then automatically evaluates the result using a structured quality prompt that measures:\n   Instruction Compliance (40%)\n   Editing Realism (25%)\n   Preservation Balance (20%)\n   Technical Quality (15%)\n   Only edits scoring above a strict threshold (~0.7) are labeled as successful, forming the main dataset; the remaining ~56K are retained as failure cases for robustness and preference learning.\n\n## üìä Dataset Statistics\n\n**Nano-Banana-400K** contains **~400K image editing data**, covering a wide visual and semantic range drawn from real-world imagery.\n\n---\n\n### üß≠ Category Distribution\n\n| Category | Description | Percentage |\n|:----------|:-------------|:------------:|\n| **Object-Level Semantic** | Add, remove, replace, or relocate objects | **35%** |\n| **Scene Composition & Multi-Subject** | Contextual and environmental transformations | **20%** |\n| **Human-Centric** | Edits involving clothing, expression, or appearance | **18%** |\n| **Stylistic** | Domain and artistic style transfer | **10%** |\n| **Text & Symbol** | Edits involving visible text, signs, or symbols | **8%** |\n| **Pixel & Photometric** | Brightness, contrast, and tonal adjustments | **5%** |\n| **Scale & Perspective** | Zoom, viewpoint, or framing changes | **2%** |\n| **Spatial / Layout** | Outpainting, composition, or canvas extension | **2%** |\n\n---\n\n### üìÇ Data Composition\n\n- **Single-Turn SFT samples (successful edits):** ~257K  \n- **Single-Turn Preference samples (failure cases):** ~56K\n- **Multi-Turn SFT samples (successful cases):** ~72K  \n- **Gemini-generated instructions:** concise, natural, and image-aware\n- **Edit coverage:** 35 edit types across 8 semantic categories  \n- **Image diversity:** includes humans, objects, text-rich scenes, etc from Open Images  \n\n---\n\n### üñºÔ∏è Visualization\n\nBelow are representative examples from different categories:\n\n| Category | Example |\n|:----------|:---------|\n| Object-Level | ‚ÄúReplace the red apple with a green one.‚Äù |\n| Scene Composition | ‚ÄúAdd sunlight streaming through the window.‚Äù |\n| Human-Centric | ‚ÄúChange the person‚Äôs expression to smiling.‚Äù |\n| Text & Symbol | ‚ÄúUppercase the text on the billboard.‚Äù |\n| Stylistic | ‚ÄúConvert the image to a Van Gogh painting style.‚Äù |\n\n---\n\nPico-Banana-400K provides both **breadth** (diverse edit operations) and **depth** (quality-controlled multimodal supervision), making it a strong foundation for training and evaluating text-guided image editing models.\n\n## üß† Applications\n\n**Pico-Banana-400K** serves as a versatile resource for advancing controllable and instruction-aware image editing.  \nBeyond single-step editing, the dataset enables **multi-turn, conversational editing** and **reward-based training paradigms**.\n\n\n\n## üì¶ Dataset Download Guide\n\nThe **Pico-Banana-400K** dataset is hosted on Apple‚Äôs public CDN.  \nYou ",
      "default_branch": "main"
    },
    "fetched_at": "2025-11-09T02:30:22.113051"
  },
  {
    "basic_info": {
      "name": "wechat-selkies",
      "full_name": "nickrunning/wechat-selkies",
      "owner": "nickrunning",
      "description": "Âü∫‰∫éSelkiesÁöÑLinuxÁΩëÈ°µÁâàÂæÆ‰ø°/QQÔºåÊîØÊåÅÊú¨Âú∞‰∏≠ÊñáËæìÂÖ•Ê≥ïÔºåÊîØÊåÅAMD64ÂíåARM64„ÄÇ",
      "url": "https://github.com/nickrunning/wechat-selkies",
      "clone_url": "https://github.com/nickrunning/wechat-selkies.git",
      "ssh_url": "git@github.com:nickrunning/wechat-selkies.git",
      "homepage": "https://hub.docker.com/r/nickrunning/wechat-selkies",
      "created_at": "2025-10-12T10:03:40Z",
      "updated_at": "2025-11-09T02:23:39Z",
      "pushed_at": "2025-10-29T04:59:28Z"
    },
    "stats": {
      "stars": 1514,
      "forks": 124,
      "watchers": 1514,
      "open_issues": 10,
      "size": 140
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 14807,
        "Dockerfile": 4168,
        "Shell": 1770
      },
      "license": "MIT License",
      "topics": [
        "docker",
        "qq",
        "vnc",
        "web",
        "wechat"
      ]
    },
    "content": {
      "readme": "# WeChat Selkies\n\n[![GitHub Stars](https://img.shields.io/github/stars/nickrunning/wechat-selkies?style=flat-square&logo=github&color=yellow)](https://github.com/nickrunning/wechat-selkies/stargazers)\n[![GitHub Forks](https://img.shields.io/github/forks/nickrunning/wechat-selkies?style=flat-square&logo=github&color=blue)](https://github.com/nickrunning/wechat-selkies/network/members)\n[![GitHub Issues](https://img.shields.io/github/issues/nickrunning/wechat-selkies?style=flat-square&logo=github&color=red)](https://github.com/nickrunning/wechat-selkies/issues)\n[![GitHub License](https://img.shields.io/github/license/nickrunning/wechat-selkies?style=flat-square&color=green)](https://github.com/nickrunning/wechat-selkies/blob/master/LICENSE)\n[![Docker Pulls](https://img.shields.io/docker/pulls/nickrunning/wechat-selkies?style=flat-square&logo=docker&color=blue)](https://hub.docker.com/r/nickrunning/wechat-selkies)\n[![Docker Image Size](https://img.shields.io/docker/image-size/nickrunning/wechat-selkies?style=flat-square&logo=docker&color=orange)](https://hub.docker.com/r/nickrunning/wechat-selkies)\n[![GitHub Release](https://img.shields.io/github/v/release/nickrunning/wechat-selkies?style=flat-square&logo=github&include_prereleases)](https://github.com/nickrunning/wechat-selkies/releases)\n[![GitHub Workflow Status](https://img.shields.io/github/actions/workflow/status/nickrunning/wechat-selkies/docker.yml?style=flat-square&logo=github-actions&label=build)](https://github.com/nickrunning/wechat-selkies/actions)\n[![GitHub Last Commit](https://img.shields.io/github/last-commit/nickrunning/wechat-selkies?style=flat-square&logo=github&color=purple)](https://github.com/nickrunning/wechat-selkies/commits)\n\n‰∏≠Êñá | [English](README_en.md)\n\nÂü∫‰∫é Docker ÁöÑÂæÆ‰ø°/QQ Linux ÂÆ¢Êà∑Á´ØÔºå‰ΩøÁî® Selkies WebRTC ÊäÄÊúØÊèê‰æõÊµèËßàÂô®ËÆøÈóÆÊîØÊåÅ„ÄÇ\n\n## È°πÁõÆÁÆÄ‰ªã\n\nÊú¨È°πÁõÆÂ∞ÜÂÆòÊñπÂæÆ‰ø°/QQ Linux ÂÆ¢Êà∑Á´ØÂ∞ÅË£ÖÂú® Docker ÂÆπÂô®‰∏≠ÔºåÈÄöËøá Selkies ÊäÄÊúØÂÆûÁé∞Âú®ÊµèËßàÂô®‰∏≠Áõ¥Êé•‰ΩøÁî®ÂæÆ‰ø°/QQÔºåÊó†ÈúÄÂú®Êú¨Âú∞ÂÆâË£ÖÂæÆ‰ø°/QQ ÂÆ¢Êà∑Á´Ø„ÄÇÈÄÇÁî®‰∫éÊúçÂä°Âô®ÈÉ®ÁΩ≤„ÄÅËøúÁ®ãÂäûÂÖ¨Á≠âÂú∫ÊôØ„ÄÇ\n\n## ÂçáÁ∫ßÊ≥®ÊÑè‰∫ãÈ°π\n\n> Â¶ÇÊûúÂçáÁ∫ßÂêéÈÉ®ÂàÜÂäüËÉΩÁº∫Â§±ÔºåËØ∑ÂÖàÊ∏ÖÁ©∫Êú¨Âú∞ÊåÇËΩΩÁõÆÂΩï‰∏ãÁöÑopenboxÁõÆÂΩï(Â¶Ç`./config/.config/openbox`)„ÄÇ\n\n## ÂäüËÉΩÁâπÊÄß\n\n- üåê **ÊµèËßàÂô®ËÆøÈóÆ**ÔºöÈÄöËøá Web ÊµèËßàÂô®Áõ¥Êé•‰ΩøÁî®ÂæÆ‰ø°ÔºåÊó†ÈúÄÊú¨Âú∞ÂÆâË£Ö\n- üê≥ **DockerÂåñÈÉ®ÁΩ≤**ÔºöÁÆÄÂçïÁöÑÂÆπÂô®ÂåñÈÉ®ÁΩ≤ÔºåÁéØÂ¢ÉÈöîÁ¶ª\n- üîí **Êï∞ÊçÆÊåÅ‰πÖÂåñ**ÔºöÊîØÊåÅÈÖçÁΩÆÂíåËÅäÂ§©ËÆ∞ÂΩïÊåÅ‰πÖÂåñÂ≠òÂÇ®\n- üé® **‰∏≠ÊñáÊîØÊåÅ**ÔºöÂÆåÊï¥ÁöÑ‰∏≠ÊñáÂ≠ó‰ΩìÂíåÊú¨Âú∞ÂåñÊîØÊåÅÔºåÊîØÊåÅÊú¨Âú∞‰∏≠ÊñáËæìÂÖ•Ê≥ï\n- üñºÔ∏è **ÂõæÁâáÂ§çÂà∂**ÔºöÊîØÊåÅÈÄöËøá‰æßËæπÊ†èÈù¢ÊùøÂºÄÂêØÂõæÁâáÂ§çÂà∂\n- üìÅ **Êñá‰ª∂‰º†Ëæì**ÔºöÊîØÊåÅÈÄöËøá‰æßËæπÊ†èÈù¢ÊùøËøõË°åÊñá‰ª∂‰º†Ëæì\n- üñ•Ô∏è **AMD64ÂíåARM64Êû∂ÊûÑÊîØÊåÅ**ÔºöÂÖºÂÆπ‰∏ªÊµÅCPUÊû∂ÊûÑ\n- üîß **Á°¨‰ª∂Âä†ÈÄü**ÔºöÂèØÈÄâÁöÑ GPU Á°¨‰ª∂Âä†ÈÄüÊîØÊåÅ\n- ü™ü **Á™óÂè£ÂàáÊç¢Âô®**ÔºöÂ∑¶‰∏äËßíÂ¢ûÂä†ÂàáÊç¢ÊÇ¨ÊµÆÁ™óÔºåÊñπ‰æøÂàáÊç¢Âà∞ÂêéÂè∞Á™óÂè£Ôºå‰∏∫ÂêéÁª≠Ê∑ªÂä†ÂÖ∂ÂÆÉÂäüËÉΩÂÅöÂü∫Á°Ä\n- ü§ñ **Ëá™Âä®ÂêØÂä®**ÔºöÂèØÈÖçÁΩÆËá™Âä®ÂêØÂä®ÂæÆ‰ø°ÂíåQQÂÆ¢Êà∑Á´ØÔºàÂèØÈÄâÔºâ\n\n## Êà™ÂõæÂ±ïÁ§∫\n![ÂæÆ‰ø°Êà™Âõæ](./docs/images/wechat-selkies-1.jpg)\n![QQÊà™Âõæ](./docs/images/wechat-selkies-2.jpg)\n\n## Âø´ÈÄüÂºÄÂßã\n\n### ÁéØÂ¢ÉË¶ÅÊ±Ç\n\n- Docker\n- Docker Compose\n- ÊîØÊåÅWebRTCÁöÑÁé∞‰ª£ÊµèËßàÂô®ÔºàChrome„ÄÅFirefox„ÄÅSafariÁ≠âÔºâ\n\n### Âø´ÈÄüÈÉ®ÁΩ≤\n\n1. **Áõ¥Êé•‰ΩøÁî®Â∑≤ÊûÑÂª∫ÁöÑÈïúÂÉèËøõË°åÂø´ÈÄüÈÉ®ÁΩ≤**\n\nGitHub Container RegistryÈïúÂÉèÔºö\n```bash\ndocker run -it -p 3001:3001 -v ./config:/config --device /dev/dri:/dev/dri ghcr.io/nickrunning/wechat-selkies:latest\n```\n\nDocker HubÈïúÂÉèÔºö\n```bash\ndocker run -it -p 3001:3001 -v ./config:/config --device /dev/dri:/dev/dri nickrunning/wechat-selkies:latest\n```\n\n2. **ËÆøÈóÆÂæÆ‰ø°**\n   \n   Âú®ÊµèËßàÂô®‰∏≠ËÆøÈóÆÔºö`https://localhost:3001` Êàñ `https://<ÊúçÂä°Âô®IP>:3001`\n   > **Ê≥®ÊÑèÔºö** Êò†Â∞Ñ3000Á´ØÂè£Áî®‰∫éHTTPËÆøÈóÆÔºå3001Á´ØÂè£Áî®‰∫éHTTPSËÆøÈóÆÔºåÂª∫ËÆÆ‰ΩøÁî®HTTPS„ÄÇ\n\n### docker-compose ÈÉ®ÁΩ≤\n1. **ÂàõÂª∫È°πÁõÆÁõÆÂΩïÂπ∂ËøõÂÖ•**\n   ```bash\n   mkdir wechat-selkies\n   cd wechat-selkies\n   ```\n2. **ÂàõÂª∫ docker-compose.yml Êñá‰ª∂**\n   ```yaml\n    services:\n      wechat-selkies:\n        image: nickrunning/wechat-selkies:latest    # or ghcr.io/nickrunning/wechat-selkies:latest\n        container_name: wechat-selkies\n        ports:\n          - \"3000:3000\"       # http port\n          - \"3001:3001\"       # https port\n        restart: unless-stopped\n        volumes:\n          - ./config:/config\n        devices:\n          - /dev/dri:/dev/dri # optional, for hardware acceleration\n        environment:\n          - PUID=1000                    # user ID\n          - PGID=100                     # group ID\n          - TZ=Asia/Shanghai             # timezone\n          - LC_ALL=zh_CN.UTF-8           # locale\n          - AUTO_START_WECHAT=true       # default is true\n          - AUTO_START_QQ=false          # default is false\n          # - CUSTOM_USER=<Your Name>      # recommended to set a custom user name\n          # - PASSWORD=<Your Password>     # recommended to set a password for selkies web ui\n    ```\n3. **ÂêØÂä®ÊúçÂä°**\n   ```bash\n   docker-compose up -d\n   ```\n\n### Ê∫êÁ†ÅÈÉ®ÁΩ≤\n\n1. **ÂÖãÈöÜÈ°πÁõÆ**\n   ```bash\n   git clone https://github.com/nickrunning/wechat-selkies.git\n   cd wechat-selkies\n   ```\n\n2. **ÂêØÂä®ÊúçÂä°**\n   ```bash\n   docker-compose up -d\n   ```\n\n3. **ËÆøÈóÆÂæÆ‰ø°**\n\n   Âú®ÊµèËßàÂô®‰∏≠ËÆøÈóÆÔºö`https://localhost:3001` Êàñ `https://<ÊúçÂä°Âô®IP>:3001`\n\n### ÈÖçÁΩÆËØ¥Êòé\n\nÊõ¥Â§öËá™ÂÆö‰πâÈÖçÁΩÆËØ∑ÂèÇËÄÉ [Selkies Base Images from LinuxServer](https://github.com/linuxserver/docker-baseimage-selkies)„ÄÇ\n\n#### Docker Hub Êé®ÈÄÅÈÖçÁΩÆ\n\nÊú¨È°πÁõÆÊîØÊåÅÂêåÊó∂Êé®ÈÄÅÂà∞ GitHub Container Registry Âíå Docker Hub„ÄÇÂ¶ÇÈúÄÂêØÁî® Docker Hub Êé®ÈÄÅÂäüËÉΩÔºåËØ∑Âú®‰ªìÂ∫ì‰∏ãÊ∑ªÂä†Environment SecretsÂíåEnvironment Variables:\n\n**Environment Secrets:**\n* DOCKERHUB_USERNAME: ‰Ω†ÁöÑ Docker Hub Áî®Êà∑Âêç\n* DOCKERHUB_TOKEN: ‰Ω†ÁöÑ Docker Hub Access Token\n**Environment Variables:**\n* ENABLE_DOCKERHUB: ËÆæÁΩÆ‰∏∫ `true` Êù•ÂêØÁî® Docker Hub Êé®ÈÄÅ\n\n#### ÁéØÂ¢ÉÂèòÈáèÈÖçÁΩÆ\n\nÂú® `docker-compose.yml` ‰∏≠ÂèØ‰ª•ÈÖçÁΩÆ‰ª•‰∏ãÁéØÂ¢ÉÂèòÈáèÔºö\n\n| ÂèòÈáèÂêç | ÈªòËÆ§ÂÄº | ËØ¥Êòé |\n|--------|--------|------|\n| `TITLE` | `WeChat Selkies` |",
      "default_branch": "master"
    },
    "fetched_at": "2025-11-09T02:30:23.374795"
  },
  {
    "basic_info": {
      "name": "Emu3.5",
      "full_name": "baaivision/Emu3.5",
      "owner": "baaivision",
      "description": "Native Multimodal Models are World Learners",
      "url": "https://github.com/baaivision/Emu3.5",
      "clone_url": "https://github.com/baaivision/Emu3.5.git",
      "ssh_url": "git@github.com:baaivision/Emu3.5.git",
      "homepage": "",
      "created_at": "2025-10-29T13:40:19Z",
      "updated_at": "2025-11-08T19:41:47Z",
      "pushed_at": "2025-11-07T10:03:31Z"
    },
    "stats": {
      "stars": 1180,
      "forks": 42,
      "watchers": 1180,
      "open_issues": 20,
      "size": 23792
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 225034
      },
      "license": "Apache License 2.0",
      "topics": []
    },
    "content": {
      "readme": "<div align='center'>\n<h1>Emu3.5: Native Multimodal Models are World Learners</h1>\n\nEmu3.5 Team, BAAI\n\n[Project Page](https://emu.world/) | [ü§óHF Models](https://huggingface.co/collections/BAAI/emu35) | [Paper](https://arxiv.org/pdf/2510.26583)\n</div>\n\n\n<div align='center'>\n<img src=\"./assets/arch.png\" class=\"interpolation-image\" alt=\"arch.\" height=\"100%\" width=\"100%\" />\n</div>\n\n\n<div align='center'>\n<img src=\"./assets/co.png\" class=\"interpolation-image\" alt=\"arch.\" height=\"90%\" width=\"90%\" />\n</div>\n\n\n|  üîπ | **Core Concept**                         | **Description**                                                                                                                            |\n| :-: | :--------------------------------------- | :----------------------------------------------------------------------------------------------------------------------------------------- |\n|  üß† | **Unified World Modeling**               | Predicts the **next state jointly across vision and language**, enabling coherent **world modeling** and **generation**.              |\n|  üß© | **End-to-End Pretraining**               | Trained with a **unified next-token prediction** objective over **interleaved vision‚Äìlanguage sequences**.                                 |\n|  üìö | **Over 10T+ Multimodal Tokens**               | Pre-trained on **over 10 trillion interleaved tokens** from **video frames** and **transcripts**, capturing **spatiotemporal structure**.       |\n|  üîÑ | **Native Multimodal I/O**                | Processes and generates **interleaved visual‚Äìtext sequences** without **modality adapters** or **task-specific heads**.                    |\n|  üéØ | **RL Post-Training**                     | Large-scale **reinforcement learning** enhances **reasoning**, **compositionality**, and **generation quality**.                           |\n|  ‚ö°  | **Discrete Diffusion Adaptation (DiDA)** | Converts **sequential decoding ‚Üí bidirectional parallel prediction**, achieving **‚âà20√ó faster inference without performance loss**.      |\n| üñºÔ∏è | **Versatile Generation**                 | Excels in **long-horizon vision‚Äìlanguage generation**, **any-to-image (X2I)** synthesis, and **text-rich image creation**.                 |\n|  üåê | **Generalizable World Modeling**         | Enables **spatiotemporally consistent world exploration**, and **open-world embodied manipulation** across diverse scenarios.          |\n|  üèÜ | **Performance Benchmark**                | Matches **Gemini 2.5 Flash Image (Nano Banana)** on **image generation/editing**, and **outperforms** on **interleaved generation tasks**. |\n\n\n\n## Table of Contents\n\n1. [Model & Weights](#1-model--weights)\n2. [Quick Start](#2-quick-start)\n3. [Schedule](#3-schedule)\n4. [Citation](#4-citation)\n\n## 1. Model & Weights\n\n| Model name               | HF Weight |\n| ------------------------ | --------- |\n| Emu3.5               | [ü§ó HF link](https://huggingface.co/BAAI/Emu3.5/tree/main) |\n| Emu3.5-Image                | [ü§ó HF link](https://huggingface.co/BAAI/Emu3.5-Image/tree/main) |\n| Emu3.5-VisionTokenizer     | [ü§ó HF link](https://huggingface.co/BAAI/Emu3.5-VisionTokenizer/tree/main) |\n\n\n*Note:*  \n- **Emu3.5** supports general-purpose multimodal predictions, including interleaved image-text generation and single-image generation (T2I/X2I) tasks.\n- **Emu3.5-Image** is a model focused on T2I/X2I tasks for best performance on these scenarios.\n- Both models are pure next-token predictors without DiDA acceleration (each image may take several minutes to generate).  \n- ‚ö° **Stay tuned for DiDA-accelerated weights.**\n\n> üí° **Usage tip:**  \n> For **interleaved image-text generation**, use **Emu3.5**.  \n> For **single-image generation** (T2I and X2I), use **Emu3.5-Image** for the best quality.\n\n\n\n## 2. Quick Start\n\n### Environment Setup\n\n```bash\n# Python 3.10 or higher is required.\ngit clone https://github.com/baaivision/Emu3.5\ncd Emu3.5\npip install -r requirements.txt\npip install flash_attn==2.8.3 --no-build-isolation\n```\n### Configuration\n\nEdit `configs/config.py` to set:\n\n- Paths: `model_path`, `vq_path`\n- Task template: `task_type in {t2i, x2i, howto, story, explore, vla}`\n- Input image: `use_image` (True to provide reference images, controls <|IMAGE|> token); set `reference_image` in each prompt to specify the image path. For x2i task, we recommand using `reference_image` as a list containing single/multiple image paths to be compatible with multi-image input.\n- Sampling: `sampling_params` (classifier_free_guidance, temperature, top_k/top_p, etc.)\n- Aspect Ratio (for t2i task): `aspect_ratio` (\"4:3\", \"21:9\", \"1:1\", \"auto\" etc..)\n\n### Run Inference\n\n```bash\npython inference.py --cfg configs/config.py\n```\n\n\n#### Example Configurations by Task\nBelow are example commands for different tasks.\nMake sure to set CUDA_VISIBLE_DEVICES according to your available GPUs.\n\n\n```bash\n# üñºÔ∏è Text-to-Image (T2I) task\nCUDA_VISIBLE_DEVICES=0 python inference.py --cfg configs/example_config_t2i.py\n\n# üîÑ Any-to-Image (X2I) t",
      "default_branch": "main"
    },
    "fetched_at": "2025-11-09T02:30:24.638800"
  },
  {
    "basic_info": {
      "name": "LongCat-Video",
      "full_name": "meituan-longcat/LongCat-Video",
      "owner": "meituan-longcat",
      "description": null,
      "url": "https://github.com/meituan-longcat/LongCat-Video",
      "clone_url": "https://github.com/meituan-longcat/LongCat-Video.git",
      "ssh_url": "git@github.com:meituan-longcat/LongCat-Video.git",
      "homepage": null,
      "created_at": "2025-10-25T06:49:49Z",
      "updated_at": "2025-11-08T21:02:45Z",
      "pushed_at": "2025-11-04T10:07:37Z"
    },
    "stats": {
      "stars": 1084,
      "forks": 95,
      "watchers": 1084,
      "open_issues": 8,
      "size": 1300750
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 328127
      },
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "# LongCat-Video\n\n<div align=\"center\">\n  <img src=\"assets/longcat-video_logo.svg\" width=\"45%\" alt=\"LongCat-Video\" />\n</div>\n<hr>\n\n<div align=\"center\" style=\"line-height: 1;\">\n  <a href='https://meituan-longcat.github.io/LongCat-Video/'><img src='https://img.shields.io/badge/Project-Page-green'></a>\n  <a href='https://arxiv.org/abs/2510.22200'><img src='https://img.shields.io/badge/Technique-Report-red'></a>\n  <a href='https://huggingface.co/meituan-longcat/LongCat-Video'><img src='https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Model-blue'></a>\n</div>\n\n<div align=\"center\" style=\"line-height: 1;\">\n  <a href='https://github.com/meituan-longcat/LongCat-Flash-Chat/blob/main/figures/wechat_official_accounts.png'><img src='https://img.shields.io/badge/WeChat-LongCat-brightgreen?logo=wechat&logoColor=white'></a>  \n  <a href='https://x.com/Meituan_LongCat'><img src='https://img.shields.io/badge/Twitter-LongCat-white?logo=x&logoColor=white'></a>\n</div>\n\n<div align=\"center\" style=\"line-height: 1;\">\n  <a href='LICENSE'><img src='https://img.shields.io/badge/License-MIT-f5de53?&color=f5de53'></a>\n</div>\n\n## Model Introduction\nWe introduce LongCat-Video, a foundational video generation model with 13.6B parameters, delivering strong performance across *Text-to-Video*, *Image-to-Video*, and *Video-Continuation* generation tasks. It particularly excels in efficient and high-quality long video generation, representing our first step toward world models.\n\n### Key Features\n- üåü **Unified architecture for multiple tasks**: LongCat-Video unifies *Text-to-Video*, *Image-to-Video*, and *Video-Continuation* tasks within a single video generation framework. It natively supports all these tasks with a single model and consistently delivers strong performance across each individual task.\n- üåü **Long video generation**: LongCat-Video is natively pretrained on *Video-Continuation* tasks, enabling it to produce minutes-long videos without color drifting or quality degradation.\n- üåü **Efficient inference**: LongCat-Video generates $720p$, $30fps$ videos within minutes by employing a coarse-to-fine generation strategy along both the temporal and spatial axes. Block Sparse Attention further enhances efficiency, particularly at high resolutions\n- üåü **Strong performance with multi-reward RLHF**: Powered by multi-reward Group Relative Policy Optimization (GRPO), comprehensive evaluations on both internal and public benchmarks demonstrate that LongCat-Video achieves performance comparable to leading open-source video generation models as well as the latest commercial solutions.\n\nFor more detail, please refer to the comprehensive [***LongCat-Video Technical Report***](https://arxiv.org/abs/2510.22200).\n\n## üé• Teaser Video\n\n<div align=\"center\">\n  <video src=\"https://github.com/user-attachments/assets/00fa63f0-9c4e-461a-a79e-c662ad596d7d\" width=\"2264\" height=\"384\"> </video>\n</div>\n\n## Quick Start\n\n### Installation\n\nClone the repo:\n\n```shell\ngit clone --single-branch --branch main https://github.com/meituan-longcat/LongCat-Video\ncd LongCat-Video\n```\n\nInstall dependencies:\n\n```shell\n# create conda environment\nconda create -n longcat-video python=3.10\nconda activate longcat-video\n\n# install torch (configure according to your CUDA version)\npip install torch==2.6.0+cu124 torchvision==0.21.0+cu124 torchaudio==2.6.0 --index-url https://download.pytorch.org/whl/cu124\n\n# install flash-attn-2\npip install ninja \npip install psutil \npip install packaging \npip install flash_attn==2.7.4.post1\n\n# install other requirements\npip install -r requirements.txt\n```\n\nFlashAttention-2 is enabled in the model config by default; you can also change the model config (\"./weights/LongCat-Video/dit/config.json\") to use FlashAttention-3 or xformers once installed.\n\n### Model Download\n\n| Models | Download Link |\n| --- | --- |\n| LongCat-Video | ü§ó [Huggingface](https://huggingface.co/meituan-longcat/LongCat-Video) |\n\nDownload models using huggingface-cli:\n```shell\npip install \"huggingface_hub[cli]\"\nhuggingface-cli download meituan-longcat/LongCat-Video --local-dir ./weights/LongCat-Video\n```\n\n### Run Text-to-Video\n\n```shell\n# Single-GPU inference\ntorchrun run_demo_text_to_video.py --checkpoint_dir=./weights/LongCat-Video --enable_compile\n\n# Multi-GPU inference\ntorchrun --nproc_per_node=2 run_demo_text_to_video.py --context_parallel_size=2 --checkpoint_dir=./weights/LongCat-Video --enable_compile\n```\n\n### Run Image-to-Video\n\n```shell\n# Single-GPU inference\ntorchrun run_demo_image_to_video.py --checkpoint_dir=./weights/LongCat-Video --enable_compile\n\n# Multi-GPU inference\ntorchrun --nproc_per_node=2 run_demo_image_to_video.py --context_parallel_size=2 --checkpoint_dir=./weights/LongCat-Video --enable_compile\n```\n\n### Run Video-Continuation\n\n```shell\n# Single-GPU inference\ntorchrun run_demo_video_continuation.py --checkpoint_dir=./weights/LongCat-Video --enable_compile\n\n# Multi-GPU inference\ntorchrun --nproc_per_node=2 run_demo_video_continuation.py --context_parallel_size=2 -",
      "default_branch": "main"
    },
    "fetched_at": "2025-11-09T02:30:25.885537"
  },
  {
    "basic_info": {
      "name": "DeekSeek-OCR---Dockerized-API",
      "full_name": "Bogdanovich77/DeekSeek-OCR---Dockerized-API",
      "owner": "Bogdanovich77",
      "description": null,
      "url": "https://github.com/Bogdanovich77/DeekSeek-OCR---Dockerized-API",
      "clone_url": "https://github.com/Bogdanovich77/DeekSeek-OCR---Dockerized-API.git",
      "ssh_url": "git@github.com:Bogdanovich77/DeekSeek-OCR---Dockerized-API.git",
      "homepage": null,
      "created_at": "2025-10-21T23:30:09Z",
      "updated_at": "2025-11-09T00:29:29Z",
      "pushed_at": "2025-10-22T19:32:31Z"
    },
    "stats": {
      "stars": 990,
      "forks": 109,
      "watchers": 990,
      "open_issues": 10,
      "size": 117
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 160119,
        "Batchfile": 2335,
        "Dockerfile": 2214
      },
      "license": null,
      "topics": []
    },
    "content": {
      "readme": "# DeepSeek-OCR: PDF to Markdown Converter\n\nA powerful OCR solution that converts PDF documents to Markdown format using DeepSeek-OCR with FastAPI backend. This project provides both a batch processing script and a REST API for flexible document conversion.\n\n## üöÄ Quick Start\n\n### Option 1: Batch Processing with pdf_to_markdown_processor.py\n\n1. Place your PDF files in the `data/` directory\n2. Ensure the DeepSeek-OCR API is running (see Docker setup below)\n3. Run the processor:\n\n```bash\npython pdf_to_markdown_processor.py\n```\n\n### Option 2: REST API with Docker Backend\n\n1. Build and start the Docker container\n2. Use the API endpoints to process documents\n3. Integrate with your applications\n\n---\n\n## üìã Prerequisites\n\n### Hardware Requirements\n- **NVIDIA GPU** with CUDA 11.8+ support\n- **GPU Memory**: Minimum 12GB VRAM (Model takes ~9GB)\n- **System RAM**: Minimum 32GB (recommended: 64GB+)\n- **Storage**: 50GB+ free space for model and containers\n\n### Software Requirements\n- **Python 3.8+** (for local processing)\n- **Docker** 20.10+ with GPU support\n- **Docker Compose** 2.0+\n- **NVIDIA Container Toolkit** installed\n- **CUDA 11.8** compatible drivers\n\n---\n\n## üê≥ Docker Backend Setup\n\n### 1. Download Model Weights\n\nCreate a directory for model weights and download the DeepSeek-OCR model:\n\n```bash\n# Create models directory\nmkdir -p models\n\n# Download using Hugging Face CLI\npip install huggingface_hub\nhuggingface-cli download deepseek-ai/DeepSeek-OCR --local-dir models/deepseek-ai/DeepSeek-OCR\n\n# Or using git\ngit clone https://huggingface.co/deepseek-ai/DeepSeek-OCR models/deepseek-ai/DeepSeek-OCR\n```\n\n### 2. Build and Run the Docker Container\n\n#### Windows Users\n\n```cmd\nREM Build the Docker image\nbuild.bat\n\nREM Start the service\ndocker-compose up -d\n\nREM Check logs\ndocker-compose logs -f deepseek-ocr\n```\n\n#### Linux/macOS Users\n\n```bash\n# Build the Docker image\ndocker-compose build\n\n# Start the service\ndocker-compose up -d\n\n# Check logs\ndocker-compose logs -f deepseek-ocr\n```\n\n### 3. Verify Installation\n\n```bash\n# Health check\ncurl http://localhost:8000/health\n\n# Expected response:\n{\n  \"status\": \"healthy\",\n  \"model_loaded\": true,\n  \"model_path\": \"/app/models/deepseek-ai/DeepSeek-OCR\",\n  \"cuda_available\": true,\n  \"cuda_device_count\": 1\n}\n```\n\n---\n\n## üìÑ PDF Processing Scripts\n\nThis project provides several PDF processing scripts, each designed for different use cases. All scripts scan the `data/` directory for PDF files and convert them to Markdown format with different prompts and post-processing options.\n\n### Output Naming Convention\n\nAll processors append a suffix to the output filename to indicate the processing method used:\n- **-MD.md**: Markdown conversion (preserves document structure)\n- **-OCR.md**: Plain OCR extraction (raw text without formatting)\n- **-CUSTOM.md**: Custom prompt processing (uses prompt from YAML file)\n\nFor example, processing `document.pdf` will create:\n- `document-MD.md` (markdown processors)\n- `document-OCR.md` (OCR processor)\n- `document-CUSTOM.md` (custom prompt processors)\n\n---\n\n### 1. pdf_to_markdown_processor.py\n\n**Purpose**: Basic PDF to Markdown conversion using the standard markdown prompt\n\n**Features**:\n- Uses prompt: `'<image>\\n<|grounding|>Convert the document to markdown.'`\n- Converts PDFs to structured Markdown format\n- Simple processing without image extraction\n- Outputs files with `-MD.md` suffix\n\n**Usage**:\n```bash\n# Place PDF files in the data directory\ncp your_document.pdf data/\n\n# Run the processor\npython pdf_to_markdown_processor.py\n\n# Check results\nls data/*-MD.md\n```\n\n---\n\n### 2. pdf_to_markdown_processor_enhanced.py\n\n**Purpose**: Enhanced PDF to Markdown conversion with post-processing\n\n**Features**:\n- Uses the same markdown prompt as the basic version\n- **Post-processing features**:\n  - Image extraction and saving to `data/images/` folder\n  - Special token cleanup\n  - Reference processing for layout information\n  - Content cleaning and formatting\n- Outputs files with `-MD.md` suffix\n\n**Usage**:\n```bash\n# Place PDF files in the data directory\ncp your_document.pdf data/\n\n# Run the enhanced processor\npython pdf_to_markdown_processor_enhanced.py\n\n# Check results (including extracted images)\nls data/*-MD.md\nls data/images/\n```\n\n---\n\n### 3. pdf_to_ocr_enhanced.py\n\n**Purpose**: Plain OCR text extraction without markdown formatting\n\n**Features**:\n- Uses OCR prompt: `'<image>\\nFree OCR.'`\n- Extracts raw text without markdown structure\n- Includes the same post-processing features as the enhanced markdown processor\n- Outputs files with `-OCR.md` suffix\n\n**Usage**:\n```bash\n# Place PDF files in the data directory\ncp your_document.pdf data/\n\n# Run the OCR processor\npython pdf_to_ocr_enhanced.py\n\n# Check results\nls data/*-OCR.md\n```\n\n---\n\n### 4. pdf_to_custom_prompt.py\n\n**Purpose**: PDF processing with custom prompts (raw output)\n\n**Features**:\n- Uses custom prompt loaded from `custom_prompt.yaml`\n- Returns raw model response without post-processing\n- Ideal for testing and debugging ",
      "default_branch": "main"
    },
    "fetched_at": "2025-11-09T02:30:27.142560"
  },
  {
    "basic_info": {
      "name": "World-in-Novel-View",
      "full_name": "tianrun-chen/World-in-Novel-View",
      "owner": "tianrun-chen",
      "description": "Scaling Novel View Synthesis for Static and Dynamic Scenes",
      "url": "https://github.com/tianrun-chen/World-in-Novel-View",
      "clone_url": "https://github.com/tianrun-chen/World-in-Novel-View.git",
      "ssh_url": "git@github.com:tianrun-chen/World-in-Novel-View.git",
      "homepage": null,
      "created_at": "2025-10-17T14:43:19Z",
      "updated_at": "2025-11-08T07:36:23Z",
      "pushed_at": "2025-10-26T15:22:16Z"
    },
    "stats": {
      "stars": 870,
      "forks": 0,
      "watchers": 870,
      "open_issues": 0,
      "size": 9996
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 187502,
        "Shell": 4789
      },
      "license": null,
      "topics": []
    },
    "content": {
      "readme": "# World-in-Novel-View\n\n[![Stars](https://img.shields.io/github/stars/tianrun-chen/World-in-Novel-View?style=social)](https://github.com/tianrun-chen/World-in-Novel-View/stargazers)\n[![Python](https://img.shields.io/badge/Python-3.8%2B-blue)](https://www.python.org/)\n\n**Scaling Novel View Synthesis for Static and Dynamic Scenes**\n\n![(im3.gif)](https://github.com/tianrun-chen/World-in-Novel-View/blob/main/im3.gif)\n\n- **Distributed Training**: FSDP and DDP support for large-scale training\n- **Mixed Precision**: Automatic mixed precision (AMP) for faster training\n---\nTrain and Inference with Ascend 910b NPU\nTorch.NPU is required„ÄÇ\n\n\n## Project Structure\n```\n.\n‚îú‚îÄ‚îÄ geope_core/              # Core GeoPE implementation\n‚îÇ   ‚îú‚îÄ‚îÄ torch.py             # PyTorch GeoPE attention\n‚îÇ   ‚îî‚îÄ‚îÄ utils/               # Utility modules\n‚îÇ       ‚îú‚îÄ‚îÄ config.py        # Configuration utilities\n‚îÇ       ‚îú‚îÄ‚îÄ functional.py    # Functional utilities\n‚îÇ       ‚îú‚îÄ‚îÄ mha.py           # Multi-head attention\n‚îÇ       ‚îú‚îÄ‚îÄ runner.py        # Training runner\n‚îÇ       ‚îî‚îÄ‚îÄ transformer.py   # Transformer components\n‚îú‚îÄ‚îÄ src/                     # Application code\n‚îÇ   ‚îú‚îÄ‚îÄ geope_attention/     # GeoPE attention wrapper\n‚îÇ   ‚îú‚îÄ‚îÄ geope_utils/         # GeoPE utilities wrapper\n‚îÇ   ‚îú‚îÄ‚îÄ nvs_models/          # Novel view synthesis models\n‚îÇ   ‚îú‚îÄ‚îÄ nvs_data/            # Data loading and preprocessing\n‚îÇ   ‚îî‚îÄ‚îÄ nvs_training/        # Training and evaluation\n‚îú‚îÄ‚îÄ tests/                   # Unit tests\n‚îî‚îÄ‚îÄ scripts/                 # Utility scripts\n```\n### Training\n\n```bash\n# Single NPU training\npython -m src.main konet\n\n# Multi-NPU training with FSDP\ntorchrun --standalone --nproc-per-node=4 -m src.main konet-fsdp\n```\n\n\n\n# Dataset: KOKONI-WorldVID-1A\n\nKOKONI-WorldVID-1A is a large-scale video dataset designed for **Novel View Synthesis** research. It contains over **10,000 unique videos** sourced from **Bilibili**, one of China's leading video-sharing platforms.\n\n## üí° Dataset Highlights\n\nUnlike most existing novel view synthesis datasets, KOKONI-WorldVID-1A provides videos from real-world, diverse scenarios with a unique data domain. These videos are created by a wide range of content creators, covering everything from static landscapes and object displays to dynamic human activities and lifestyle recordings.\n\n- **Data Source**: All videos are sourced from Bilibili, providing the research community with a perspective distinct from Western-dominated datasets.\n- **Scale**: Contains over 10,000 unique videos, offering sufficient data for deep learning model training.\n- **Content Diversity**: Videos encompass a wide variety of content, helping to improve model generalization in complex real-world scenarios.\n- **Static & Dynamic**: The dataset includes both static and partially dynamic videos. For static scene videos, we additionally provide human-screened static segments to facilitate more fine-grained model training and evaluation.\n\n## üìä Dataset Statistics\n\n| Category | Count | Description |\n|----------|-------|-------------|\n| **Total Videos** | 10,000 | Unique videos from Bilibili |\n| **Static Videos** | ~5,000 | Videos with static scenes and annotated segments |\n| **Dynamic Videos** | ~5,000 | Videos with dynamic content (e.g., walking, movement) |\n| **Data Domain** | Chinese UGC | User-generated content from China |\n| **Application** | Novel View Synthesis | Training and evaluation of NVS models |\n\n## üìÇ Data Structure\n\nThe dataset is organized using two CSV files that contain metadata for all videos:\n\n### 1. `static.csv` - Static Scene Videos\n\nContains videos with static scenes and human-annotated time segments.\n\n**Format:**\n```\nÂ∫èÂè∑,URL,ËßÜÈ¢ëÊ†áÈ¢ò,ÈùôÊÄÅÂºÄÂßãÊó∂Èó¥1,ÈùôÊÄÅÁªìÊùüÊó∂Èó¥1,ÈùôÊÄÅÂºÄÂßãÊó∂Èó¥2,ÈùôÊÄÅÁªìÊùüÊó∂Èó¥2\n1,https://www.bilibili.com/video/BV1xx411c7mD,Beautiful Landscape,00:10,00:35,01:20,02:15\n2,https://www.bilibili.com/video/BV1Ab411q7yH,Object Display,00:00,00:45,,\n```\n\n**Columns:**\n- `Â∫èÂè∑` (Index): Sequential number\n- `URL`: Bilibili video URL\n- `ËßÜÈ¢ëÊ†áÈ¢ò` (Video Title): Original video title\n- `ÈùôÊÄÅÂºÄÂßãÊó∂Èó¥1` (Static Start Time 1): Start time of first static segment (format: MM:SS or HH:MM:SS)\n- `ÈùôÊÄÅÁªìÊùüÊó∂Èó¥1` (Static End Time 1): End time of first static segment\n- `ÈùôÊÄÅÂºÄÂßãÊó∂Èó¥2` (Static Start Time 2): Start time of second static segment (optional)\n- `ÈùôÊÄÅÁªìÊùüÊó∂Èó¥2` (Static End Time 2): End time of second static segment (optional)\n\n**Note:** Additional segment pairs may exist (ÈùôÊÄÅÂºÄÂßãÊó∂Èó¥3, ÈùôÊÄÅÁªìÊùüÊó∂Èó¥3, etc.)\n\n### 2. `walk.csv` - Dynamic Scene Videos\n\nContains videos with dynamic content such as walking, movement, or changing scenes.\n\n**Format:**\n```\nÂ∫èÂè∑,URL,ËßÜÈ¢ëÊ†áÈ¢ò\n1,https://www.bilibili.com/video/BV1yZ4y1u7fA,City Walk Tour\n2,https://www.bilibili.com/video/BV1Hx411v7iP,Campus Walking\n```\n\n**Columns:**\n- `Â∫èÂè∑` (Index): Sequential number\n- `URL`: Bilibili video URL\n- `ËßÜÈ¢ëÊ†áÈ¢ò` (Video Title): Original video title\n\n## üì• Download & Usage\n\nWe provide a Python script (`download_videos.py`) to help users batch download videos from the dataset. Please follow these steps:\n\n### 1. Install Dependencies\n\n```bash\npip install you-get pandas\n```\n\nOr alternativel",
      "default_branch": "main"
    },
    "fetched_at": "2025-11-09T02:30:28.412486"
  },
  {
    "basic_info": {
      "name": "tiny8",
      "full_name": "sql-hkr/tiny8",
      "owner": "sql-hkr",
      "description": "A tiny CPU simulator written in Python",
      "url": "https://github.com/sql-hkr/tiny8",
      "clone_url": "https://github.com/sql-hkr/tiny8.git",
      "ssh_url": "git@github.com:sql-hkr/tiny8.git",
      "homepage": "https://sql-hkr.github.io/tiny8/",
      "created_at": "2025-10-20T16:28:30Z",
      "updated_at": "2025-11-08T20:19:31Z",
      "pushed_at": "2025-11-01T06:46:18Z"
    },
    "stats": {
      "stars": 808,
      "forks": 18,
      "watchers": 808,
      "open_issues": 5,
      "size": 1685
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 225911,
        "Dockerfile": 332
      },
      "license": "MIT License",
      "topics": [
        "8-bit-computer",
        "assembler",
        "visualization"
      ]
    },
    "content": {
      "readme": "# Tiny8\n\n[![PyPI version](https://img.shields.io/pypi/v/tiny8)](https://pypi.org/project/tiny8/)\n[![License](https://img.shields.io/github/license/sql-hkr/tiny8)](LICENSE)\n[![Python versions](https://img.shields.io/pypi/pyversions/tiny8)](https://pypi.org/project/tiny8/)\n[![CI](https://github.com/sql-hkr/tiny8/actions/workflows/ci.yml/badge.svg)](https://github.com/sql-hkr/tiny8/actions/workflows/ci.yml)\n[![codecov](https://codecov.io/github/sql-hkr/tiny8/graph/badge.svg?token=OBM58R8MCL)](https://codecov.io/github/sql-hkr/tiny8)\n\n> **An educational 8-bit CPU simulator with interactive visualization**\n\nTiny8 is a lightweight and educational toolkit for exploring the fundamentals of computer architecture through hands-on assembly programming and real-time visualization. Designed for learning and experimentation, it features an AVR-inspired 8-bit CPU with 32 registers, a rich instruction set, and powerful debugging tools ‚Äî all with zero heavy dependencies.\n\n\n<div align=\"center\">\n  <img src=\"https://github.com/user-attachments/assets/ffbcb2c4-2c3a-469f-b7b7-e6e86eb374da\" alt=\"Animated bubble sort visualization\" width=\"600\">\n  <p><em>Real-time visualization of a bubble sort algorithm executing on Tiny8</em></p>\n</div>\n\n## ‚ú® Features\n\n### üéØ **Interactive Terminal Debugger**\n<img width=\"600\" src=\"https://github.com/user-attachments/assets/0bbd4382-806e-4b5a-af0b-54d83417fcfb\" alt=\"CLI visualizer screenshot\">\n\n- **Vim-style navigation**: Step through execution with intuitive keyboard controls\n- **Change highlighting**: See exactly what changed at each step (registers, flags, memory)\n- **Advanced search**: Find instructions, track register/memory changes, locate PC addresses\n- **Marks and bookmarks**: Set and jump to important execution points\n- **Vertical scrolling**: Handle programs with large memory footprints\n\n### üé¨ **Graphical Animation**\n- Generate high-quality GIF/MP4 videos of program execution\n- Visualize register evolution, memory access patterns, and flag changes\n- Perfect for presentations, documentation, and learning materials\n\n### üèóÔ∏è **Complete 8-bit Architecture**\n- **32 general-purpose registers** (R0-R31)\n- **8-bit ALU** with arithmetic, logical, and bit manipulation operations\n- **Status register (SREG)** with 8 condition flags\n- **2KB address space** for unified memory and I/O\n- **Stack operations** with dedicated stack pointer\n- **AVR-inspired instruction set** with 60+ instructions\n\n### üìö **Educational Focus**\n- Clean, readable Python implementation\n- Comprehensive examples (Fibonacci, bubble sort, factorial, and more)\n- Step-by-step execution traces for debugging\n- Full API documentation and instruction set reference\n\n## üöÄ Quick Start\n\n### Installation\n\n```bash\npip install tiny8\n```\n\n### Your First Program\n\nCreate `fibonacci.asm`:\n```asm\n; Fibonacci Sequence Calculator\n; Calculates the 10th Fibonacci number (F(10) = 55)\n; F(0) = 0, F(1) = 1, F(n) = F(n-1) + F(n-2)\n;\n; Results stored in registers:\n; R16 and R17 hold the two most recent Fibonacci numbers\n\n    ldi r16, 0          ; F(0) = 0\n    ldi r17, 1          ; F(1) = 1\n    ldi r18, 9          ; Counter: 9 more iterations to reach F(10)\n\nloop:\n    add r16, r17        ; F(n) = F(n-1) + F(n-2)\n    mov r19, r16        ; Save result temporarily\n    mov r16, r17        ; Shift: previous = current\n    mov r17, r19        ; Shift: current = new result\n    dec r18             ; Decrement counter\n    brne loop           ; Continue if counter != 0\n\ndone:\n    jmp done            ; Infinite loop at end\n```\n\nRun it:\n```bash\ntiny8 fibonacci.asm # Interactive debugger\ntiny8 fibonacci.asm -m ani -o fibonacci.gif # Generate animation\n```\n\n### Python API\n\n```python\nfrom tiny8 import CPU, assemble_file\n\nasm = assemble_file(\"fibonacci.asm\")\ncpu = CPU()\ncpu.load_program(asm)\ncpu.run(max_steps=1000)\n\nprint(f\"Result: R17 = {cpu.read_reg(17)}\")  # Final Fibonacci number\n```\n\n## üí° Why Tiny8?\n\n**For Students** ‚Äî Write assembly, see immediate results with visual feedback. Understand how each instruction affects CPU state without abstractions.\n\n**For Educators** ‚Äî Interactive demonstrations, easy assignment creation, and generate animations for lectures.\n\n**For Hobbyists** ‚Äî Rapid algorithm prototyping at the hardware level with minimal overhead and an extensible, readable codebase.\n\n## üìñ Documentation\n\n- [**Full Documentation**](https://sql-hkr.github.io/tiny8/) ‚Äî Complete API reference and guides\n- [**Instruction Set Reference**](#instruction-set-reference) ‚Äî All 60+ instructions\n- [**CLI Guide**](#interactive-cli-controls) ‚Äî Terminal debugger keyboard shortcuts\n- [**Examples**](#examples) ‚Äî Sample programs with explanations\n- [**Contributing**](CONTRIBUTING.md) ‚Äî Guidelines for contributors\n\n## üéÆ Interactive CLI Controls\n\nThe terminal-based debugger provides powerful navigation and inspection capabilities.\n\n### Navigation & Playback\n\n- `l` / `h` or `‚Üí` / `‚Üê` ‚Äî Step forward/backward\n- `w` / `b` ‚Äî Jump ¬±10 steps\n- `0` / `$` ‚Äî Jump to first/last step\n- `Space` ‚Äî Play/p",
      "default_branch": "main"
    },
    "fetched_at": "2025-11-09T02:30:29.773591"
  },
  {
    "basic_info": {
      "name": "AITradeGame",
      "full_name": "chadyi/AITradeGame",
      "owner": "chadyi",
      "description": null,
      "url": "https://github.com/chadyi/AITradeGame",
      "clone_url": "https://github.com/chadyi/AITradeGame.git",
      "ssh_url": "git@github.com:chadyi/AITradeGame.git",
      "homepage": "",
      "created_at": "2025-10-20T07:23:51Z",
      "updated_at": "2025-11-08T16:41:38Z",
      "pushed_at": "2025-11-03T09:07:01Z"
    },
    "stats": {
      "stars": 805,
      "forks": 245,
      "watchers": 805,
      "open_issues": 0,
      "size": 21963
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 68175,
        "JavaScript": 40008,
        "CSS": 15588,
        "HTML": 15410,
        "Dockerfile": 161
      },
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "# AITradeGame - Open Source AI Trading Simulator\n\n[English](README.md) | [‰∏≠Êñá](README_ZH.md)\n\n[![Python 3.9+](https://img.shields.io/badge/python-3.9+-blue.svg)](https://www.python.org/downloads/)\n[![Flask](https://img.shields.io/badge/flask-3.0+-green.svg)](https://flask.palletsprojects.com/)\n[![License](https://img.shields.io/badge/license-MIT-blue.svg)](LICENSE)\n\nAITradeGame is an AI trading simulator that supports both local and online versions.\n\nProvides an online version with interactive features and leaderboards.\n\nLocal version stores all data on your computer, no cloud storage, no tracking.\n\nIncludes a Windows one-click standalone executable that runs without installation.\n\n## Features\n\n### Desktop Version (Local)\n\nAI-driven trading strategies based on large language models, compatible with OpenAI, DeepSeek, Claude, and other models. Leveraged portfolio management with ECharts visualizations. 100% privacy with all data stored in local database. Trading fee configuration supported to simulate real trading environment.\n\n**Latest Features:**\n- API Provider Management: Unified management of multiple AI service provider API configurations\n- Smart Model Selection: Automatically fetch available model lists for each provider\n- Aggregated View: View aggregated assets and performance comparison across all models\n- System Settings: Configurable trading frequency and fee rates\n\n### Online Version (Public)\n\nLeaderboard functionality to compete with AI enthusiasts worldwide. Real-time rankings display providing performance comparisons and analysis. Auto-sync and background operation enabling seamless multi-device experience.\n\n## Quick Start\n\n### Try Online Version\n\nLaunch the online version at https://aitradegame.com without any installation.\n\n### Desktop Version\n\nDownload AITradeGame.exe from GitHub releases. Double-click the executable to run. The interface will open automatically. Start adding AI models and begin trading.\n\nAlternatively, clone the repository from GitHub. Install dependencies with pip install -r requirements.txt. Run the application with python app.py and visit http://localhost:5000.\n\n### Docker Deployment\n\nYou can also run AITradeGame using Docker:\n\n**Using docker-compose (recommended):**\n```bash\n# Build and start the container\ndocker-compose up -d\n\n# Access the application at http://localhost:5000\n```\n\n**Using docker directly:**\n```bash\n# Build the image\ndocker build -t aitradegame .\n\n# Run the container\ndocker run -d -p 5000:5000 -v $(pwd)/data:/app/data aitradegame\n\n# Access the application at http://localhost:5000\n```\n\nThe data directory will be created automatically to store the SQLite database. To stop the container, run `docker-compose down`.\n\n## Configuration\n\n### API Provider Setup\nFirst, add AI service providers:\n1. Click the \"API Provider\" button\n2. Enter provider name, API URL, and API key\n3. Manually input available models or click \"Fetch Models\" to auto-fetch\n4. Click save to complete configuration\n\n### Adding Trading Models\nAfter configuring providers, add trading models:\n1. Click the \"Add Model\" button\n2. Select a configured API provider\n3. Choose a specific model from the dropdown\n4. Enter display name and initial capital\n5. Click submit to start trading\n\n### System Settings\nClick the \"Settings\" button to configure:\n- Trading Frequency: Control AI decision interval (1-1440 minutes)\n- Trading Fee Rate: Commission rate per trade (default 0.1%)\n\n## Supported AI Models\n\nSupports all OpenAI-compatible APIs. This includes OpenAI models like gpt-4 and gpt-3.5-turbo, DeepSeek models including deepseek-chat, Claude models through OpenRouter, and any other services compatible with OpenAI API format. More protocols are being added.\n\n## Usage\n\nStart the server by running AITradeGame.exe or python app.py. Add AI model configuration through the web interface at http://localhost:5000. The system automatically begins trading simulation based on your configuration. Trading fees are charged for each open and close position according to the set rate, ensuring AI strategies operate under realistic cost conditions.\n\n## Privacy and Security\n\nAll data is stored in the AITradeGame.db SQLite file in the same directory as the executable. No external servers are contacted except your specified AI API endpoints. No user accounts or login required - everything runs locally.\n\n## Development\n\nDevelopment requires Python 3.9 or later. Internet connection is needed for market data and AI API calls.\n\nInstall all dependencies with: pip install -r requirements.txt\n\n## Contributing\n\nCommunity contributions are welcome.\n\n## Disclaimer\n\nThis is a simulated trading platform for testing AI models and strategies. It is not real trading and no actual money is involved. Always conduct your own research and analysis before making investment decisions. No warranties are provided regarding trading outcomes or AI performance.\n\n## Links\n\nOnline version with leaderboard and social features: https://aitradegame.com\n\nDesktop bui",
      "default_branch": "main"
    },
    "fetched_at": "2025-11-09T02:30:31.041038"
  },
  {
    "basic_info": {
      "name": "OpenAIglasses_for_Navigation",
      "full_name": "AI-FanGe/OpenAIglasses_for_Navigation",
      "owner": "AI-FanGe",
      "description": "a open framework for blind navigation based on esp32",
      "url": "https://github.com/AI-FanGe/OpenAIglasses_for_Navigation",
      "clone_url": "https://github.com/AI-FanGe/OpenAIglasses_for_Navigation.git",
      "ssh_url": "git@github.com:AI-FanGe/OpenAIglasses_for_Navigation.git",
      "homepage": null,
      "created_at": "2025-10-20T10:03:03Z",
      "updated_at": "2025-11-09T01:53:48Z",
      "pushed_at": "2025-11-01T06:47:27Z"
    },
    "stats": {
      "stars": 765,
      "forks": 219,
      "watchers": 765,
      "open_issues": 6,
      "size": 23596
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 530632,
        "JavaScript": 73288,
        "C++": 38358,
        "HTML": 9871,
        "Shell": 5462,
        "Batchfile": 4340,
        "CSS": 3712,
        "C": 1665,
        "Dockerfile": 1337
      },
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "# AI Êô∫ËÉΩÁõ≤‰∫∫ÁúºÈïúÁ≥ªÁªü ü§ñüëì\n\n<div align=\"center\">\n\n‰∏Ä‰∏™Èù¢ÂêëËßÜÈöú‰∫∫Â£´ÁöÑÊô∫ËÉΩÂØºËà™‰∏éËæÖÂä©Á≥ªÁªüÔºåÈõÜÊàê‰∫ÜÁõ≤ÈÅìÂØºËà™„ÄÅËøáÈ©¨Ë∑ØËæÖÂä©„ÄÅÁâ©ÂìÅËØÜÂà´„ÄÅÂÆûÊó∂ËØ≠Èü≥‰∫§‰∫íÁ≠âÂäüËÉΩ„ÄÇ  Êú¨È°πÁõÆ‰ªÖ‰∏∫‰∫§ÊµÅÂ≠¶‰π†‰ΩøÁî®ÔºåËØ∑ÂãøÁõ¥Êé•ÁªôËßÜÈöú‰∫∫Áæ§‰ΩøÁî®„ÄÇÊú¨È°πÁõÆÂÜÖ‰ªÖÂåÖÂê´‰ª£Á†ÅÔºåÊ®°ÂûãÂú∞ÂùÄÔºöhttps://www.modelscope.cn/models/archifancy/AIGlasses_for_navigation  „ÄÇ‰∏ãËΩΩÂêéÂ≠òÊîæÂú®/model Êñá‰ª∂Â§π\n\n[ÂäüËÉΩÁâπÊÄß](#ÂäüËÉΩÁâπÊÄß) ‚Ä¢ [Âø´ÈÄüÂºÄÂßã](#Âø´ÈÄüÂºÄÂßã) ‚Ä¢ [Á≥ªÁªüÊû∂ÊûÑ](#Á≥ªÁªüÊû∂ÊûÑ) ‚Ä¢ [‰ΩøÁî®ËØ¥Êòé](#‰ΩøÁî®ËØ¥Êòé) ‚Ä¢ [ÂºÄÂèëÊñáÊ°£](#ÂºÄÂèëÊñáÊ°£)\n\n</div>\n\n---\n<img width=\"2481\" height=\"3508\" alt=\"1\" src=\"https://github.com/user-attachments/assets/e8dec4a6-8fa6-4d94-bd66-4e9864b67daf\" />\n<img width=\"2480\" height=\"3508\" alt=\"2\" src=\"https://github.com/user-attachments/assets/bc7d1aac-a9e9-4ef8-9d67-224708d0c9fd\" />\n<img width=\"2481\" height=\"3508\" alt=\"4\" src=\"https://github.com/user-attachments/assets/6dd19750-57af-4560-a007-9a7059956b53\" />\n\n## üìã ÁõÆÂΩï\n\n- [ÂäüËÉΩÁâπÊÄß](#ÂäüËÉΩÁâπÊÄß)\n- [Á≥ªÁªüË¶ÅÊ±Ç](#Á≥ªÁªüË¶ÅÊ±Ç)\n- [Âø´ÈÄüÂºÄÂßã](#Âø´ÈÄüÂºÄÂßã)\n- [Á≥ªÁªüÊû∂ÊûÑ](#Á≥ªÁªüÊû∂ÊûÑ)\n- [‰ΩøÁî®ËØ¥Êòé](#‰ΩøÁî®ËØ¥Êòé)\n- [ÈÖçÁΩÆËØ¥Êòé](#ÈÖçÁΩÆËØ¥Êòé)\n- [ÂºÄÂèëÊñáÊ°£](#ÂºÄÂèëÊñáÊ°£)\n\n## ‚ú® ÂäüËÉΩÁâπÊÄß\n\n### üö∂ Áõ≤ÈÅìÂØºËà™Á≥ªÁªü\n- **ÂÆûÊó∂Áõ≤ÈÅìÊ£ÄÊµã**ÔºöÂü∫‰∫é YOLO ÂàÜÂâ≤Ê®°ÂûãÂÆûÊó∂ËØÜÂà´Áõ≤ÈÅì\n- **Êô∫ËÉΩËØ≠Èü≥ÂºïÂØº**ÔºöÊèê‰æõÁ≤æÂáÜÁöÑÊñπÂêëÊåáÂºïÔºàÂ∑¶ËΩ¨„ÄÅÂè≥ËΩ¨„ÄÅÁõ¥Ë°åÁ≠âÔºâ\n- **ÈöúÁ¢çÁâ©Ê£ÄÊµã‰∏éÈÅøÈöú**ÔºöËá™Âä®ËØÜÂà´ÂâçÊñπÈöúÁ¢çÁâ©Âπ∂ËßÑÂàíÈÅøÈöúË∑ØÁ∫ø\n- **ËΩ¨ÂºØÊ£ÄÊµã**ÔºöËá™Âä®ËØÜÂà´ÊÄ•ËΩ¨ÂºØÂπ∂ÊèêÂâçÊèêÈÜí\n- **ÂÖâÊµÅÁ®≥ÂÆö**Ôºö‰ΩøÁî® Lucas-Kanade ÂÖâÊµÅÁÆóÊ≥ïÁ®≥ÂÆöÊé©Á†ÅÔºåÂáèÂ∞ëÊäñÂä®\n\n### üö¶ ËøáÈ©¨Ë∑ØËæÖÂä©\n- **ÊñëÈ©¨Á∫øËØÜÂà´**ÔºöÂÆûÊó∂Ê£ÄÊµãÊñëÈ©¨Á∫ø‰ΩçÁΩÆÂíåÊñπÂêë\n- **Á∫¢ÁªøÁÅØËØÜÂà´**ÔºöÂü∫‰∫éÈ¢úËâ≤ÂíåÂΩ¢Áä∂ÁöÑÁ∫¢ÁªøÁÅØÁä∂ÊÄÅÊ£ÄÊµã\n- **ÂØπÈΩêÂºïÂØº**ÔºöÂºïÂØºÁî®Êà∑ÂØπÂáÜÊñëÈ©¨Á∫ø‰∏≠ÂøÉ\n- **ÂÆâÂÖ®ÊèêÈÜí**ÔºöÁªøÁÅØÊó∂ËØ≠Èü≥ÊèêÁ§∫ÂèØ‰ª•ÈÄöË°å\n\n### üîç Áâ©ÂìÅËØÜÂà´‰∏éÊü•Êâæ\n- **Êô∫ËÉΩÁâ©ÂìÅÊêúÁ¥¢**ÔºöËØ≠Èü≥Êåá‰ª§Êü•ÊâæÁâ©ÂìÅÔºàÂ¶Ç\"Â∏ÆÊàëÊâæ‰∏Ä‰∏ãÁ∫¢Áâõ\"Ôºâ\n- **ÂÆûÊó∂ÁõÆÊ†áËøΩË∏™**Ôºö‰ΩøÁî® YOLO-E ÂºÄÊîæËØçÊ±áÊ£ÄÊµã + ByteTrack ËøΩË∏™\n- **ÊâãÈÉ®ÂºïÂØº**ÔºöÁªìÂêà MediaPipe ÊâãÈÉ®Ê£ÄÊµãÔºåÂºïÂØºÁî®Êà∑ÊâãÈÉ®Èù†ËøëÁâ©ÂìÅ\n- **ÊäìÂèñÊ£ÄÊµã**ÔºöÊ£ÄÊµãÊâãÈÉ®Êè°ÊåÅÂä®‰ΩúÔºåÁ°ÆËÆ§Áâ©ÂìÅÂ∑≤ÊãøÂà∞\n- **Â§öÊ®°ÊÄÅÂèçÈ¶à**ÔºöËßÜËßâÊ†áÊ≥® + ËØ≠Èü≥ÂºïÂØº + Â±Ö‰∏≠ÊèêÁ§∫\n\n### üéôÔ∏è ÂÆûÊó∂ËØ≠Èü≥‰∫§‰∫í\n- **ËØ≠Èü≥ËØÜÂà´ÔºàASRÔºâ**ÔºöÂü∫‰∫éÈòøÈáå‰∫ë DashScope Paraformer ÂÆûÊó∂ËØ≠Èü≥ËØÜÂà´\n- **Â§öÊ®°ÊÄÅÂØπËØù**ÔºöQwen-Omni-Turbo ÊîØÊåÅÂõæÂÉè+ÊñáÊú¨ËæìÂÖ•ÔºåËØ≠Èü≥ËæìÂá∫\n- **Êô∫ËÉΩÊåá‰ª§Ëß£Êûê**ÔºöËá™Âä®ËØÜÂà´ÂØºËà™„ÄÅÊü•Êâæ„ÄÅÂØπËØùÁ≠â‰∏çÂêåÁ±ªÂûãÊåá‰ª§\n- **‰∏ä‰∏ãÊñáÊÑüÁü•**ÔºöÂú®‰∏çÂêåÊ®°Âºè‰∏ãÊô∫ËÉΩËøáÊª§Êó†ÂÖ≥Êåá‰ª§\n\n### üìπ ËßÜÈ¢ë‰∏éÈü≥È¢ëÂ§ÑÁêÜ\n- **ÂÆûÊó∂ËßÜÈ¢ëÊµÅ**ÔºöWebSocket Êé®ÊµÅÔºåÊîØÊåÅÂ§öÂÆ¢Êà∑Á´ØÂêåÊó∂ËßÇÁúã\n- **Èü≥ËßÜÈ¢ëÂêåÊ≠•ÂΩïÂà∂**ÔºöËá™Âä®‰øùÂ≠òÂ∏¶Êó∂Èó¥Êà≥ÁöÑÂΩïÂÉèÂíåÈü≥È¢ëÊñá‰ª∂\n- **IMU Êï∞ÊçÆËûçÂêà**ÔºöÊé•Êî∂ ESP32 ÁöÑ IMU Êï∞ÊçÆÔºåÊîØÊåÅÂßøÊÄÅ‰º∞ËÆ°\n- **Â§öË∑ØÈü≥È¢ëÊ∑∑Èü≥**ÔºöÊîØÊåÅÁ≥ªÁªüËØ≠Èü≥„ÄÅAI ÂõûÂ§ç„ÄÅÁéØÂ¢ÉÈü≥ÂêåÊó∂Êí≠Êîæ\n\n### üé® ÂèØËßÜÂåñ‰∏é‰∫§‰∫í\n- **Web ÂÆûÊó∂ÁõëÊéß**ÔºöÊµèËßàÂô®Á´ØÂÆûÊó∂Êü•ÁúãÂ§ÑÁêÜÂêéÁöÑËßÜÈ¢ëÊµÅ\n- **IMU 3D ÂèØËßÜÂåñ**ÔºöThree.js ÂÆûÊó∂Ê∏≤ÊüìËÆæÂ§áÂßøÊÄÅ\n- **Áä∂ÊÄÅÈù¢Êùø**ÔºöÊòæÁ§∫ÂØºËà™Áä∂ÊÄÅ„ÄÅÊ£ÄÊµã‰ø°ÊÅØ„ÄÅFPS Á≠â\n- **‰∏≠ÊñáÂèãÂ•Ω**ÔºöÊâÄÊúâÁïåÈù¢ÂíåËØ≠Èü≥‰ΩøÁî®‰∏≠ÊñáÔºåÊîØÊåÅËá™ÂÆö‰πâÂ≠ó‰Ωì\n\n## üíª Á≥ªÁªüË¶ÅÊ±Ç\n\n### Á°¨‰ª∂Ë¶ÅÊ±Ç\n- **ÂºÄÂèë/ÊúçÂä°Âô®Á´Ø**Ôºö\n  - CPU: Intel i5 Êàñ‰ª•‰∏äÔºàÊé®Ëçê i7/i9Ôºâ\n  - GPU: NVIDIA GPUÔºàÊîØÊåÅ CUDA 11.8+ÔºåÊé®Ëçê RTX 3060 Êàñ‰ª•‰∏äÔºâ\n  - ÂÜÖÂ≠ò: 8GB RAMÔºàÊé®Ëçê 16GBÔºâ\n  - Â≠òÂÇ®: 10GB ÂèØÁî®Á©∫Èó¥\n\n- **ÂÆ¢Êà∑Á´ØËÆæÂ§á**ÔºàÂèØÈÄâÔºâÔºö\n  - ESP32-CAM ÊàñÂÖ∂‰ªñÊîØÊåÅ WebSocket ÁöÑÊëÑÂÉèÂ§¥\n  - È∫¶ÂÖãÈ£éÔºàÁî®‰∫éËØ≠Èü≥ËæìÂÖ•Ôºâ\n  - Êâ¨Â£∞Âô®/ËÄ≥Êú∫ÔºàÁî®‰∫éËØ≠Èü≥ËæìÂá∫Ôºâ\n\n### ËΩØ‰ª∂Ë¶ÅÊ±Ç\n- **Êìç‰ΩúÁ≥ªÁªü**: Windows 10/11, Linux (Ubuntu 20.04+), macOS 10.15+\n- **Python**: 3.9 - 3.11\n- **CUDA**: 11.8 ÊàñÊõ¥È´òÁâàÊú¨ÔºàGPU Âä†ÈÄüÂøÖÈúÄÔºâ\n- **ÊµèËßàÂô®**: Chrome 90+, Firefox 88+, Edge 90+ÔºàÁî®‰∫é Web ÁõëÊéßÔºâ\n\n### API ÂØÜÈí•\n- **ÈòøÈáå‰∫ë DashScope API Key**ÔºàÂøÖÈúÄÔºâÔºö\n  - Áî®‰∫éËØ≠Èü≥ËØÜÂà´ÔºàASRÔºâÂíå Qwen-Omni ÂØπËØù\n  - Áî≥ËØ∑Âú∞ÂùÄÔºöhttps://dashscope.console.aliyun.com/\n\n## üöÄ Âø´ÈÄüÂºÄÂßã\n\n### 1. ÂÖãÈöÜÈ°πÁõÆ\n\n```bash\ngit clone https://github.com/yourusername/aiglass.git\ncd aiglass/rebuild1002\n```\n\n### 2. ÂÆâË£Ö‰æùËµñ\n\n#### ÂàõÂª∫ËôöÊãüÁéØÂ¢ÉÔºàÊé®ËçêÔºâ\n```bash\npython -m venv venv\n# Windows\nvenv\\Scripts\\activate\n# Linux/macOS\nsource venv/bin/activate\n```\n\n#### ÂÆâË£Ö Python ÂåÖ\n```bash\npip install -r requirements.txt\n```\n\n#### ÂÆâË£Ö CUDA Âíå cuDNNÔºàGPU Âä†ÈÄüÔºâ\nËØ∑ÂèÇËÄÉ [NVIDIA CUDA Toolkit ÂÆâË£ÖÊåáÂçó](https://developer.nvidia.com/cuda-downloads)\n\n### 3. ‰∏ãËΩΩÊ®°ÂûãÊñá‰ª∂\n\nÂ∞Ü‰ª•‰∏ãÊ®°ÂûãÊñá‰ª∂ÊîæÂÖ• `model/` ÁõÆÂΩïÔºö\n\n| Ê®°ÂûãÊñá‰ª∂ | Áî®ÈÄî | Â§ßÂ∞è | ‰∏ãËΩΩÈìæÊé• |\n|---------|------|------|---------|\n| `yolo-seg.pt` | Áõ≤ÈÅìÂàÜÂâ≤ | ~50MB | [ÂæÖË°•ÂÖÖ] |\n| `yoloe-11l-seg.pt` | ÂºÄÊîæËØçÊ±áÊ£ÄÊµã | ~80MB | [ÂæÖË°•ÂÖÖ] |\n| `shoppingbest5.pt` | Áâ©ÂìÅËØÜÂà´ | ~30MB | [ÂæÖË°•ÂÖÖ] |\n| `trafficlight.pt` | Á∫¢ÁªøÁÅØÊ£ÄÊµã | ~20MB | [ÂæÖË°•ÂÖÖ] |\n| `hand_landmarker.task` | ÊâãÈÉ®Ê£ÄÊµã | ~15MB | [MediaPipe Models](https://developers.google.com/mediapipe/solutions/vision/hand_landmarker#models) |\n\n### 4. ÈÖçÁΩÆ API ÂØÜÈí•\n\nÂàõÂª∫ `.env` Êñá‰ª∂Ôºö\n\n```bash\n# .env\nDASHSCOPE_API_KEY=your_api_key_here\n```\n\nÊàñÂú®‰ª£Á†Å‰∏≠Áõ¥Êé•‰øÆÊîπÔºà‰∏çÊé®ËçêÔºâÔºö\n```python\n# app_main.py, line 50\nAPI_KEY = \"your_api_key_here\"\n```\n\n### 5. ÂêØÂä®Á≥ªÁªü\n\n```bash\npython app_main.py\n```\n\nÁ≥ªÁªüÂ∞ÜÂú® `http://0.0.0.0:8081` ÂêØÂä®ÔºåÊâìÂºÄÊµèËßàÂô®ËÆøÈóÆÂç≥ÂèØÁúãÂà∞ÂÆûÊó∂ÁõëÊéßÁïåÈù¢„ÄÇ\n\n### 6. ËøûÊé•ËÆæÂ§áÔºàÂèØÈÄâÔºâ\n\nÂ¶ÇÊûú‰ΩøÁî® ESP32-CAMÔºåËØ∑Ôºö\n1. ÁÉßÂΩï `compile/compile.ino` Âà∞ ESP32\n2. ‰øÆÊîπ WiFi ÈÖçÁΩÆÔºåËøûÊé•Âà∞Âêå‰∏ÄÁΩëÁªú\n3. ESP32 Ëá™Âä®ËøûÊé•Âà∞ WebSocket Á´ØÁÇπ\n\n## üèóÔ∏è Á≥ªÁªüÊû∂ÊûÑ\n\n### Êï¥‰ΩìÊû∂ÊûÑ\n\n```\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ                        ÂÆ¢Êà∑Á´ØÂ±Ç                              ‚îÇ\n‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îÇ\n‚îÇ  ‚îÇ  ESP32-CAM   ‚îÇ  ‚îÇ   ÊµèËßàÂô®      ‚îÇ  ‚îÇ   ÁßªÂä®Á´Ø      ‚îÇ      ‚îÇ\n‚îÇ  ‚îÇ  (ËßÜÈ¢ë/Èü≥È¢ë)  ‚îÇ  ‚îÇ  (ÁõëÊéßÁïåÈù¢)   ‚îÇ  ‚îÇ  (ËØ≠Èü≥ÊéßÂà∂)   ‚îÇ      ‚îÇ\n‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n          ‚îÇ WebSocket        ‚îÇ HTTP/WS          ‚îÇ WebSocket\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ         ‚îÇ                  ‚îÇ                  ‚îÇ              ‚îÇ\n‚îÇ    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ\n‚îÇ    ‚îÇ         FastAPI ‰∏ªÊúçÂä° (app_main.py)              ‚îÇ    ‚îÇ\n‚îÇ    ‚îÇ  - WebSocket Ë∑ØÁî±ÁÆ°ÁêÜ                              ‚îÇ    ‚îÇ\n‚îÇ    ‚îÇ  - Èü≥ËßÜÈ¢ëÊµÅÂàÜÂèë                                     ‚îÇ    ‚îÇ\n‚îÇ    ‚îÇ  - Áä∂ÊÄÅÁÆ°ÁêÜ‰∏éÂçèË∞É                                   ‚îÇ    ‚îÇ\n‚îÇ    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ\n‚îÇ         ‚îÇ                ‚îÇ                ‚îÇ                  ‚îÇ\n‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îÇ\n‚îÇ  ‚îÇ ASR Ê®°Âùó     ‚îÇ  ‚îÇ Omni ÂØπËØù   ‚îÇ  ‚îÇ Èü≥È¢ëÊí≠Êîæ     ‚îÇ         ‚îÇ\n‚îÇ  ‚îÇ (asr_core)   ‚îÇ  ‚îÇ(omni_client)‚îÇ  ‚îÇ(audio_player)‚îÇ         ‚îÇ\n‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îÇ\n‚îÇ                                                               ‚îÇ\n‚îÇ         Â∫îÁî®Â±Ç                         ",
      "default_branch": "main"
    },
    "fetched_at": "2025-11-09T02:30:32.337245"
  },
  {
    "basic_info": {
      "name": "FlashVSR",
      "full_name": "OpenImagingLab/FlashVSR",
      "owner": "OpenImagingLab",
      "description": "Towards Real-Time Diffusion-Based Streaming Video Super-Resolution ‚Äî An efficient one-step diffusion framework for streaming VSR with locality-constrained sparse attention and a tiny conditional decoder.",
      "url": "https://github.com/OpenImagingLab/FlashVSR",
      "clone_url": "https://github.com/OpenImagingLab/FlashVSR.git",
      "ssh_url": "git@github.com:OpenImagingLab/FlashVSR.git",
      "homepage": "https://zhuang2002.github.io/FlashVSR/",
      "created_at": "2025-10-14T12:20:30Z",
      "updated_at": "2025-11-08T20:28:21Z",
      "pushed_at": "2025-11-05T10:13:13Z"
    },
    "stats": {
      "stars": 727,
      "forks": 55,
      "watchers": 727,
      "open_issues": 24,
      "size": 53685
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 2471075
      },
      "license": "Apache License 2.0",
      "topics": [
        "diffusion-models",
        "video-super-resolution"
      ]
    },
    "content": {
      "readme": "# ‚ö° FlashVSR\n\n**Towards Real-Time Diffusion-Based Streaming Video Super-Resolution**\n\n**Authors:** Junhao Zhuang, Shi Guo, Xin Cai, Xiaohui Li, Yihao Liu, Chun Yuan, Tianfan Xue\n\n<a href='http://zhuang2002.github.io/FlashVSR'><img src='https://img.shields.io/badge/Project-Page-Green'></a> &nbsp;\n<a href=\"https://huggingface.co/JunhaoZhuang/FlashVSR\"><img src=\"https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Model%20(v1)-blue\"></a> &nbsp;\n<a href=\"https://huggingface.co/JunhaoZhuang/FlashVSR-v1.1\"><img src=\"https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Model%20(v1.1)-blue\"></a> &nbsp;\n<a href=\"https://huggingface.co/datasets/JunhaoZhuang/VSR-120K\"><img src=\"https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Dataset-orange\"></a> &nbsp;\n<a href=\"https://arxiv.org/abs/2510.12747\"><img src=\"https://img.shields.io/badge/arXiv-2510.12747-b31b1b.svg\"></a>\n\n**Your star means a lot for us to develop this project!** :star:\n\n<img src=\"./examples/WanVSR/assets/teaser.png\" />\n\n---\n\n### üåü Abstract\n\nDiffusion models have recently advanced video restoration, but applying them to real-world video super-resolution (VSR) remains challenging due to high latency, prohibitive computation, and poor generalization to ultra-high resolutions. Our goal in this work is to make diffusion-based VSR practical by achieving **efficiency, scalability, and real-time performance**. To this end, we propose **FlashVSR**, the first diffusion-based one-step streaming framework towards real-time VSR. **FlashVSR runs at ‚àº17 FPS for 768 √ó 1408 videos on a single A100 GPU** by combining three complementary innovations: (i) a train-friendly three-stage distillation pipeline that enables streaming super-resolution, (ii) locality-constrained sparse attention that cuts redundant computation while bridging the train‚Äìtest resolution gap, and (iii) a tiny conditional decoder that accelerates reconstruction without sacrificing quality. To support large-scale training, we also construct **VSR-120K**, a new dataset with 120k videos and 180k images. Extensive experiments show that FlashVSR scales reliably to ultra-high resolutions and achieves **state-of-the-art performance with up to ‚àº12√ó speedup** over prior one-step diffusion VSR models.\n\n---\n\n### üì∞ News\n\n- **Nov 2025 ‚Äî üéâ [FlashVSR v1.1](https://huggingface.co/JunhaoZhuang/FlashVSR-v1.1) released:** enhanced stability + fidelity  \n- **Oct 2025 ‚Äî [FlashVSR v1](https://huggingface.co/JunhaoZhuang/FlashVSR)  (initial release)**: Inference code and model weights are available now! üéâ  \n- **Bug Fix (October 21, 2025):** Fixed `local_attention_mask` update logic to prevent artifacts when switching between different aspect ratios during continuous inference.  \n- **Coming Soon:** Dataset release (**VSR-120K**) for large-scale training.\n\n---\n\n### üì¢ Important Quality Note (ComfyUI & other third-party implementations)\n\nFirst of all, huge thanks to the community for the fast adoption, feedback, and contributions to FlashVSR! üôå  \nDuring community testing, we noticed that some third-party implementations of FlashVSR (e.g. early ComfyUI versions) do **not include our Locality-Constrained Sparse Attention (LCSA)** module and instead fall back to **dense attention**. This may lead to **noticeable quality degradation**, especially at higher resolutions.  \nCommunity discussion: https://github.com/kijai/ComfyUI-WanVideoWrapper/issues/1441\n\nBelow is a comparison example provided by a community member:\n\n| Fig.1 ‚Äì LR Input Video | Fig.2 ‚Äì 3rd-party (no LCSA) | Fig.3 ‚Äì Official FlashVSR |\n|------------------|-----------------------------------------------|--------------------------------------|\n| <video src=\"https://github.com/user-attachments/assets/ea12a191-48d5-47c0-a8e5-e19ad13581a9\" controls width=\"260\"></video> | <video src=\"https://github.com/user-attachments/assets/c8e53bd5-7eca-420d-9cc6-2b9c06831047\" controls width=\"260\"></video> | <video src=\"https://github.com/user-attachments/assets/a4d80618-d13d-4346-8e37-38d2fabf9827\" controls width=\"260\"></video> |\n\n‚úÖ The **official FlashVSR pipeline (this repository)**:\n- **Better preserves fine structures and details**\n- **Effectively avoids texture aliasing and visual artifacts**\n\nWe are also working on a **version that does not rely on the Block-Sparse Attention library** while keeping **the same output quality**; this alternative may run slower than the optimized original implementation.\n\nThanks again to the community for actively testing and helping improve FlashVSR together! üöÄ\n\n---\n\n### üìã TODO\n\n- ‚úÖ Release inference code and model weights  \n- ‚¨ú Release dataset (VSR-120K)\n\n---\n\n### üöÄ Getting Started\n\nFollow these steps to set up and run **FlashVSR** on your local machine:\n\n> ‚ö†Ô∏è **Note:** This project is primarily designed and optimized for **4√ó video super-resolution**.  \n> We **strongly recommend** using the **4√ó SR setting** to achieve better results and stability. ‚úÖ\n\n#### 1Ô∏è‚É£ Clone the Repository\n\n```bash\ngit clone https://github.com/OpenImaging",
      "default_branch": "main"
    },
    "fetched_at": "2025-11-09T02:30:33.593538"
  },
  {
    "basic_info": {
      "name": "HunyuanWorld-Mirror",
      "full_name": "Tencent-Hunyuan/HunyuanWorld-Mirror",
      "owner": "Tencent-Hunyuan",
      "description": "Fast and Universal 3D reconstruction model for versatile tasks",
      "url": "https://github.com/Tencent-Hunyuan/HunyuanWorld-Mirror",
      "clone_url": "https://github.com/Tencent-Hunyuan/HunyuanWorld-Mirror.git",
      "ssh_url": "git@github.com:Tencent-Hunyuan/HunyuanWorld-Mirror.git",
      "homepage": "https://3d-models.hunyuan.tencent.com/world/",
      "created_at": "2025-10-16T13:47:15Z",
      "updated_at": "2025-11-08T21:37:23Z",
      "pushed_at": "2025-11-06T14:28:38Z"
    },
    "stats": {
      "stars": 705,
      "forks": 49,
      "watchers": 705,
      "open_issues": 7,
      "size": 15380
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 1340565,
        "Cuda": 396957,
        "C++": 146235,
        "Shell": 16888
      },
      "license": "Other",
      "topics": [
        "3d",
        "3d-reconstruction",
        "aigc",
        "hunyuan",
        "hunyuan3d",
        "image-to-3d",
        "scene-generation",
        "world-model",
        "world-models"
      ]
    },
    "content": {
      "readme": "[‰∏≠ÊñáÈòÖËØª](README_zh.md)\n# **HunyuanWorld-Mirror**\n\n<p align=\"center\">\n  <img src=\"assets/teaser.jpg\" width=\"95%\" alt=\"HunyuanWorld-Mirror Teaser\">\n</p>\n\n<p align=\"center\">\n<a href='https://3d-models.hunyuan.tencent.com/world/'><img src='https://img.shields.io/badge/Project-Page-green'></a>\n<a href='https://3d-models.hunyuan.tencent.com/world/worldMirror1_0/HYWorld_Mirror_Tech_Report.pdf'><img src='https://img.shields.io/badge/Technique-Report-red'></a>\n<a href='https://huggingface.co/tencent/HunyuanWorld-Mirror'><img src='https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Model-blue'></a>\n<a href='https://huggingface.co/spaces/tencent/HunyuanWorld-Mirror'><img src='https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Demo-orange'></a>\n<a href=https://discord.gg/dNBrdrGGMa target=\"_blank\"><img src= https://img.shields.io/badge/Discord-white.svg?logo=discord height=22px></a>\n  <a href=https://x.com/TencentHunyuan target=\"_blank\"><img src=https://img.shields.io/badge/Hunyuan-black.svg?logo=x height=22px></a>\n<p align=\"center\">\n\n\n\nHunyuanWorld-Mirror is a versatile feed-forward model for comprehensive 3D geometric prediction. It integrates diverse geometric priors (**camera poses**, **calibrated intrinsics**, **depth maps**) and simultaneously generates various 3D representations (**point clouds**, **multi-view depths**, **camera parameters**, **surface normals**, **3D Gaussians**) in a single forward pass.\n\n\n\nhttps://github.com/user-attachments/assets/146a9a25-5eb7-4400-aa09-5b58e1d10a5e\n\n\n\n\n## üî•üî•üî• Updates\n* **[Nov 7, 2025]**: üöÄüöÄüöÄ We release the training and evaluation code. See [Training Instructions](#ü§ñ-training) and [Evaluation Instructions](#üìä-evaluation).\n* **[Oct 22, 2025]**: We release the inference code and model weights. [Download Pretrained Model](https://huggingface.co/tencent/HunyuanWorld-Mirror).\n\n> Join our **[Wechat](#)** and **[Discord](https://discord.gg/dNBrdrGGMa)** group to discuss and find help from us.\n\n| Wechat Group                                     | Xiaohongshu                                           | X                                           | Discord                                           |\n|--------------------------------------------------|-------------------------------------------------------|---------------------------------------------|---------------------------------------------------|\n| <img src=\"assets/qrcode/wechat.png\"  height=140> | <img src=\"assets/qrcode/xiaohongshu.png\"  height=140> | <img src=\"assets/qrcode/x.png\"  height=140> | <img src=\"assets/qrcode/discord.png\"  height=140> | \n\n\n## Table of Contents\n\n- [Introduction](#‚òØÔ∏è-hunyuanworld-mirror-introduction)\n  - [Architecture](#architecture)\n- [Installation](#üõ†Ô∏è-dependencies-and-installation)\n- [Quick Start](#üéÆ-quick-start)\n  - [Online Demo](#online-demo)\n  - [Local Demo](#local-demo)\n- [Download Pretrained Models](#üì¶-download-pretrained-models)\n- [Inference with Images & Priors](#üöÄ-inference-with-images--priors)\n  - [Example Code Snippet](#example-code-snippet)\n  - [Output Format](#output-format)\n  - [Inference with More Functions](#inference-with-more-functions)\n- [Post 3DGS Optimization (Optional)](#üéØ-post-3dgs-optimization-optional)\n  - [Install Dependencies](#install-dependencies)\n  - [Optimization](#optimization)\n- [Performance](#üîÆ-performance)\n  - [Point Cloud Reconstruction](#point-cloud-reconstruction)\n  - [Novel View Synthesis](#novel-view-synthesis)\n  - [Boost of Geometric Priors](#boost-of-geometric-priors)\n- [Training](#ü§ñ-training)\n  - [Training Data Preparation](#training-data-preparation)\n  - [Install Dependencies](#install-dependencies)\n  - [Training Commands](#training-commands)\n  - [Fine-tuning Commands](#fine-tuning-commands)\n- [Evaluation](#üìä-evaluation)\n  - [Evaluation Data Preparation](#evaluation-data-preparation)\n  - [Install Dependencies](#install-dependencies)\n  - [Evaluation Commands](#evaluation-commands)\n    - [1. Point Map Reconstruction](#1-point-map-reconstruction)\n    - [2. Surface Normal Estimation](#2-surface-normal-estimation)\n    - [3. Novel View Synthesis](#3-novel-view-synthesis)\n    - [4. Depth Estimation](#4-depth-estimation)\n    - [5. Camera Pose Estimation](#5-camera-pose-estimation)\n- [Open-Source Plan](#üìë-open-source-plan)\n- [BibTeX](#üîó-bibtex)\n- [Contact](#üìß-contact)\n- [Acknowledgments](#acknowledgements)\n\n\n## ‚òØÔ∏è **HunyuanWorld-Mirror Introduction**\n\n### Architecture\nHunyuanWorld-Mirror consists of two key components:\n\n**(1) Multi-Modal Prior Prompting**: A mechanism that embeds diverse prior modalities,\nincluding calibrated intrinsics, camera pose, and depth, into the feed-forward model. Given any subset of the available priors, we utilize several lightweight encoding layers to convert each modality into structured tokens.\n\n**(2) Universal Geometric Prediction**: A unified architecture capable of handling\nthe full spectrum of 3D reconstruction tasks from camera and depth estimation to point map regression, surface normal estimation, and novel view s",
      "default_branch": "main"
    },
    "fetched_at": "2025-11-09T02:30:34.847503"
  },
  {
    "basic_info": {
      "name": "ds",
      "full_name": "huojichuanqi/ds",
      "owner": "huojichuanqi",
      "description": "Áî®deepseekÂÅö‰∫§ÊòìÔºàÊêûÁùÄÁé©ÔºåÊ≥®ÊÑèÈ£éÈô©Ôºâ",
      "url": "https://github.com/huojichuanqi/ds",
      "clone_url": "https://github.com/huojichuanqi/ds.git",
      "ssh_url": "git@github.com:huojichuanqi/ds.git",
      "homepage": null,
      "created_at": "2025-10-20T07:05:47Z",
      "updated_at": "2025-11-08T19:31:39Z",
      "pushed_at": "2025-11-01T10:45:20Z"
    },
    "stats": {
      "stars": 692,
      "forks": 389,
      "watchers": 692,
      "open_issues": 18,
      "size": 27
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 94612
      },
      "license": null,
      "topics": []
    },
    "content": {
      "readme": "#### ‰∏™‰∫∫ÂñúÊ¨¢Áé©ÈªëÁÆ±ÊñáÂåñÔºå‰Ω†‰ª¨‰∏ç‰∏ÄÊ†∑ÔºåÂà´‰∏äÂ§¥„ÄÇ\n#### ÊêûËøô‰∏™ÁõÆÁöÑÊòØÂÖàÁ°ÆÂÆödsÁöÑ‰∏Ä‰∫õ‰∏úË•øÔºå‰∏çÊòØÂïÜ‰∏öÂåñ‰∫ßÂìÅÔºå‰∏ª‰ΩìÊÄùË∑ØÊòØÂõ¥ÁªïËØïÈ™åËØÅ‰º™ÂéªÁöÑ\n###  ÂÖ≥Ê≥®Êé®Áâπ‰∫ÜËß£ÊÄùË∑ØÊù•ÈæôÂéªËÑâÔºöhttps://x.com/huojichuanqi\n####  ÁõÆÂâçÊúÄÊúâ‰ª∑ÂÄºÁöÑÂ∞±ÊòØds+ÊåáÊ†áÊñπÊ°àÔºå‰ΩÜÊòØÂü∫Á°ÄÁâàÊú¨ÊòØÊú¨Âú∞ÁÆóÂ•ΩÁªô‰ªñÔºåÊàë‰ª¨Ê≠£Âú®Â∞ùËØïÁî®dsÁõ¥Êé•ÂàÜÊûêÊåáÊ†áÔºå‰ΩÜÊòØÊïàÊûúÊ≤°ÁúãÂá∫Êù•\n### Ê≥®ÊÑè‰∏∫‰∫ÜÁÆÄÂåñÈÄªËæëÔºåËÆ∞ÂæóÊîπ ÂçïÂêëÊåÅ‰ªì  ÂçïÂêëÊåÅ‰ªì ÂçïÂêëÊåÅ‰ªì \n\n\n\n## ÈÖçÁΩÆÂÜÖÂÆπ\n\n### ÈÖçÁΩÆÊñá‰ª∂Âª∫Âú®Á≠ñÁï•Ê†πÁõÆÂΩï\n\n### Êñá‰ª∂ÂêçÂ≠ó    .env\n\n####  DEEPSEEK_API_KEY= ‰Ω†ÁöÑdeepseek  apiÂØÜÈí•\n\n####  BINANCE_API_KEY=\n\n####  BINANCE_SECRET=\n\n####  OKX_API_KEY=\n\n####  OKX_SECRET=\n\n#### OKX_PASSWORD=\n\n###  ËßÜÈ¢ëÊïôÁ®ãÔºöhttps://www.youtube.com/watch?v=Yv-AMVaWUVg\n###  ÈÖçÂêàÂàÜÊ°£ÁßªÂä®Ê≠¢ÁõàÊ≠¢ÊçüÔºöhttps://youtu.be/-vfeyqUkuzY\n\n### ÂáÜÂ§á‰∏ÄÂè∞ubuntuÊúçÂä°Âô® Êé®ËçêÈòøÈáå‰∫ë È¶ôÊ∏ØÊàñËÄÖÊñ∞Âä†Âù° ËΩª‰∫ëÊúçÂä°Âô®\n\n\n#### wget https://repo.anaconda.com/archive/Anaconda3-2024.10-1-Linux-x86_64.sh\n\n#### bash Anaconda3-2024.10-1-Linux-x86_64.sh\n\n#### source /root/anaconda3/etc/profile.d/conda.sh \n#### echo \". /root/anaconda3/etc/profile.d/conda.sh\" >> ~/.bashrc\n\n\n\n\n#### conda create -n ds python=3.10\n\n#### conda activate ds\n\n#### pip install -r requirements.txt\n\n\n\n#### apt-get update Êõ¥Êñ∞ÈïúÂÉèÊ∫ê\n\n\n#### apt-get upgrade ÂøÖË¶ÅÂ∫ìÁöÑ‰∏Ä‰∏™ÂçáÁ∫ß\n\n\n#### apt install npm ÂÆâË£Önpm\n\n\n#### npm install pm2 -g ‰ΩøÁî®npmÂÆâË£Öpm2\n\n#### conda create -n trail3 python=4.10\n\n###### ÊâìËµèÂú∞ÂùÄÔºàTRC20ÔºâÔºöTUunBuqQ1ZDYt9WrA3ZarndFPQgefXqZAM",
      "default_branch": "main"
    },
    "fetched_at": "2025-11-09T02:30:36.102582"
  }
]