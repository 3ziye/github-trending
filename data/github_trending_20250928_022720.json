[
  {
    "basic_info": {
      "name": "map-anything",
      "full_name": "facebookresearch/map-anything",
      "owner": "facebookresearch",
      "description": "MapAnything: Universal Feed-Forward Metric 3D Reconstruction",
      "url": "https://github.com/facebookresearch/map-anything",
      "clone_url": "https://github.com/facebookresearch/map-anything.git",
      "ssh_url": "git@github.com:facebookresearch/map-anything.git",
      "homepage": "",
      "created_at": "2025-09-04T14:37:36Z",
      "updated_at": "2025-09-28T01:09:34Z",
      "pushed_at": "2025-09-25T18:12:18Z"
    },
    "stats": {
      "stars": 1759,
      "forks": 82,
      "watchers": 1759,
      "open_issues": 23,
      "size": 6043
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 1727145,
        "Shell": 157713
      },
      "license": "Apache License 2.0",
      "topics": [
        "3d-reconstruction",
        "ai",
        "calibration",
        "depth-completion",
        "depth-estimation",
        "image-to-3d",
        "multi-view-stereo",
        "robotics",
        "sfm"
      ]
    },
    "content": {
      "readme": "<div align=\"center\">\n<h1>MapAnything: Universal Feed-Forward Metric <br>3D Reconstruction</h1>\n<a href=\"https://map-anything.github.io/assets/MapAnything.pdf\"><img src=\"https://img.shields.io/badge/Paper-blue\" alt=\"Paper\"></a>\n<a href=\"https://arxiv.org/abs/2509.13414\"><img src=\"https://img.shields.io/badge/arXiv-2509.13414-b31b1b\" alt=\"arXiv\"></a>\n<a href=\"https://map-anything.github.io/\"><img src=\"https://img.shields.io/badge/Project_Page-green\" alt=\"Project Page\"></a>\n<a href=\"https://huggingface.co/spaces/facebook/map-anything\"><img src='https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Demo-blue'></a>\n<br>\n<br>\n<strong>\n<a href=\"https://nik-v9.github.io/\">Nikhil Keetha<sup>1,2</sup></a>\n&nbsp;&nbsp;\n<a href=\"https://sirwyver.github.io/\">Norman M√ºller<sup>1</sup></a>\n&nbsp;&nbsp;\n<a href=\"https://demuc.de/\">Johannes Sch√∂nberger<sup>1</sup></a>\n&nbsp;&nbsp;\n<a href=\"https://www.linkedin.com/in/lorenzoporzi\">Lorenzo Porzi<sup>1</sup></a>\n&nbsp;&nbsp;\n<a href=\"https://infinity1096.github.io/\">Yuchen Zhang<sup>2</sup></a>\n<br>\n<a href=\"https://tobiasfshr.github.io/\">Tobias Fischer<sup>1</sup></a>\n&nbsp;&nbsp;\n<a href=\"https://www.linkedin.com/in/arno-knapitsch\">Arno Knapitsch<sup>1</sup></a>\n&nbsp;&nbsp;\n<a href=\"https://www.linkedin.com/in/duncan-zauss\">Duncan Zauss<sup>1</sup></a>\n&nbsp;&nbsp;\n<a href=\"https://ethanweber.me/\">Ethan Weber<sup>1</sup></a>\n&nbsp;&nbsp;\n<a href=\"https://www.linkedin.com/in/nelsonantunes7\">Nelson Antunes<sup>1</sup></a>\n<br>\n<a href=\"https://x.com/jonathonluiten?lang=en\">Jonathon Luiten<sup>1</sup></a>\n&nbsp;&nbsp;\n<a href=\"https://m.lopezantequera.com/\">Manuel Lopez-Antequera<sup>1</sup></a>\n&nbsp;&nbsp;\n<a href=\"https://scholar.google.com/citations?user=484sccEAAAAJ\">Samuel Rota Bul√≤<sup>1</sup></a>\n&nbsp;&nbsp;\n<a href=\"https://richardt.name/\">Christian Richardt<sup>1</sup></a>\n<br>\n<a href=\"https://www.cs.cmu.edu/~deva/\">Deva Ramanan<sup>2</sup></a>\n&nbsp;&nbsp;\n<a href=\"https://theairlab.org/team/sebastian/\">Sebastian Scherer<sup>2</sup></a>\n&nbsp;&nbsp;\n<a href=\"https://www.linkedin.com/in/peter-kontschieder-2a6410134\">Peter Kontschieder<sup>1</sup></a>\n<br>\n<br>\n<sup>1</sup> Meta &nbsp;&nbsp;\n<sup>2</sup> Carnegie Mellon University\n</strong>\n\n</div>\n\n## Overview\n\nMapAnything is a simple, end-to-end trained transformer model that directly regresses the factored metric 3D geometry of a scene given various types of inputs (images, calibration, poses, or depth). A single feed-forward model supports over 12 different 3D reconstruction tasks including multi-image sfm, multi-view stereo, monocular metric depth estimation, registration, depth completion and more.\n\n![Overview](./assets/teaser.png)\n\n## Table of Contents\n\n- [Quick Start](#quick-start)\n  - [Installation](#installation)\n  - [Image-Only Inference](#image-only-inference)\n  - [Multi-Modal Inference](#multi-modal-inference)\n- [Interactive Demos](#interactive-demos)\n  - [Online Demo](#online-demo)\n  - [Local Gradio Demo](#local-gradio-demo)\n  - [Rerun Demo](#rerun-demo)\n- [COLMAP & GSplat Support](#colmap--gsplat-support)\n  - [Exporting to COLMAP Format](#exporting-to-colmap-format)\n  - [Integration with Gaussian Splatting](#integration-with-gaussian-splatting)\n- [Data Processing for Training & Benchmarking](#data-processing-for-training--benchmarking)\n- [Training](#training)\n- [Benchmarking](#benchmarking)\n- [Code License](#code-license)\n- [Models](#models)\n- [Building Blocks for MapAnything](#building-blocks-for-mapanything)\n- [Acknowledgments](#acknowledgments)\n- [Citation](#citation)\n\n## Quick Start\n\n### Installation\n\n```bash\ngit clone https://github.com/facebookresearch/map-anything.git\ncd map-anything\n\n# Create and activate conda environment\nconda create -n mapanything python=3.12 -y\nconda activate mapanything\n\n# Optional: Install torch, torchvision & torchaudio specific to your system\n# Install MapAnything\npip install -e .\n\n# For all optional dependencies\n# See pyproject.toml for more details\npip install -e \".[all]\"\npre-commit install\n```\n\nNote that we don't pin a specific version of PyTorch or CUDA in our requirements. Please feel free to install PyTorch based on your specific system.\n\n### Image-Only Inference\n\nFor metric 3D reconstruction from images without additional geometric inputs:\n\n```python\n# Optional config for better memory efficiency\nimport os\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n\n# Required imports\nimport torch\nfrom mapanything.models import MapAnything\nfrom mapanything.utils.image import load_images\n\n# Get inference device\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# Init model - This requries internet access or the huggingface hub cache to be pre-downloaded\n# For Apache 2.0 license model, use \"facebook/map-anything-apache\"\nmodel = MapAnything.from_pretrained(\"facebook/map-anything\").to(device)\n\n# Load and preprocess images from a folder or list of paths\nimages = \"path/to/your/images/\"  # or [\"path/to/img1.jpg\", \"path/to/img2.jpg\", ...]\nviews = loa",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-28T02:27:21.571026"
  },
  {
    "basic_info": {
      "name": "VoxCPM",
      "full_name": "OpenBMB/VoxCPM",
      "owner": "OpenBMB",
      "description": "VoxCPM: Tokenizer-Free TTS for Context-Aware Speech Generation and True-to-Life Voice Cloning",
      "url": "https://github.com/OpenBMB/VoxCPM",
      "clone_url": "https://github.com/OpenBMB/VoxCPM.git",
      "ssh_url": "git@github.com:OpenBMB/VoxCPM.git",
      "homepage": "",
      "created_at": "2025-09-16T03:41:49Z",
      "updated_at": "2025-09-28T02:25:33Z",
      "pushed_at": "2025-09-23T06:12:14Z"
    },
    "stats": {
      "stars": 1445,
      "forks": 148,
      "watchers": 1445,
      "open_issues": 22,
      "size": 1454
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 112327
      },
      "license": "Apache License 2.0",
      "topics": []
    },
    "content": {
      "readme": "## üéôÔ∏è VoxCPM: Tokenizer-Free TTS for Context-Aware Speech Generation and True-to-Life Voice Cloning\n\n\n[![Project Page](https://img.shields.io/badge/Project%20Page-GitHub-blue)](https://github.com/OpenBMB/VoxCPM/) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-OpenBMB-yellow)](https://huggingface.co/openbmb/VoxCPM-0.5B) [![ModelScope](https://img.shields.io/badge/ModelScope-OpenBMB-purple)](https://modelscope.cn/models/OpenBMB/VoxCPM-0.5B)  [![Live Playground](https://img.shields.io/badge/Live%20PlayGround-Demo-orange)](https://huggingface.co/spaces/OpenBMB/VoxCPM-Demo) [![Samples](https://img.shields.io/badge/Page-Samples-red)](https://openbmb.github.io/VoxCPM-demopage)\n\n\n\n<div align=\"center\">\n  <img src=\"assets/voxcpm_logo.png\" alt=\"VoxCPM Logo\" width=\"40%\">\n</div>\n\n<div align=\"center\">\n\nüëã Contact us on [WeChat](assets/wechat.png)\n\n</div>\n\n## News \n* [2025.09.16] üî• üî• üî•  We Open Source the VoxCPM-0.5B [weights](https://huggingface.co/openbmb/VoxCPM-0.5B)!\n* [2025.09.16] üéâ üéâ üéâ  We Provide the [Gradio PlayGround](https://huggingface.co/spaces/OpenBMB/VoxCPM-Demo) for VoxCPM-0.5B, try it now! \n\n## Overview\n\nVoxCPM is a novel tokenizer-free Text-to-Speech (TTS) system that redefines realism in speech synthesis. By modeling speech in a continuous space, it overcomes the limitations of discrete tokenization and enables two flagship capabilities: context-aware speech generation and true-to-life zero-shot voice cloning.\n\nUnlike mainstream approaches that convert speech to discrete tokens, VoxCPM uses an end-to-end diffusion autoregressive architecture that directly generates continuous speech representations from text. Built on [MiniCPM-4](https://huggingface.co/openbmb/MiniCPM4-0.5B) backbone, it achieves implicit semantic-acoustic decoupling through hierachical language modeling and FSQ constraints, greatly enhancing both expressiveness and generation stability.\n\n<div align=\"center\">\n  <img src=\"assets/voxcpm_model.png\" alt=\"VoxCPM Model Architecture\" width=\"90%\">\n</div>\n\n\n###  üöÄ Key Features\n- **Context-Aware, Expressive Speech Generation** - VoxCPM comprehends text to infer and generate appropriate prosody, delivering speech with remarkable expressiveness and natural flow. It spontaneously adapts speaking style based on content, producing highly fitting vocal expression trained on a massive 1.8 million-hour bilingual corpus.\n- **True-to-Life Voice Cloning** - With only a short reference audio clip, VoxCPM performs accurate zero-shot voice cloning, capturing not only the speaker‚Äôs timbre but also fine-grained characteristics such as accent, emotional tone, rhythm, and pacing to create a faithful and natural replica.\n- **High-Efficiency Synthesis** - VoxCPM supports streaming synthesis with a Real-Time Factor (RTF) as low as 0.17 on a consumer-grade NVIDIA RTX 4090 GPU, making it possible for real-time applications.\n\n\n\n\n\n##  Quick Start\n\n### üîß Install from PyPI\n``` sh\npip install voxcpm\n```\n### 1.  Model Download (Optional)\nBy default, when you first run the script, the model will be downloaded automatically, but you can also download the model in advance.\n- Download VoxCPM-0.5B\n    ```\n    from huggingface_hub import snapshot_download\n    snapshot_download(\"openbmb/VoxCPM-0.5B\")\n    ```\n- Download ZipEnhancer and SenseVoice-Small. We use ZipEnhancer to enhance speech prompts and SenseVoice-Small for speech prompt ASR in the web demo.\n    ```\n    from modelscope import snapshot_download\n    snapshot_download('iic/speech_zipenhancer_ans_multiloss_16k_base')\n    snapshot_download('iic/SenseVoiceSmall')\n    ```\n\n### 2. Basic Usage\n```python\nimport soundfile as sf\nimport numpy as np\nfrom voxcpm import VoxCPM\n\nmodel = VoxCPM.from_pretrained(\"openbmb/VoxCPM-0.5B\")\n\n# Non-streaming\nwav = model.generate(\n    text=\"VoxCPM is an innovative end-to-end TTS model from ModelBest, designed to generate highly expressive speech.\",\n    prompt_wav_path=None,      # optional: path to a prompt speech for voice cloning\n    prompt_text=None,          # optional: reference text\n    cfg_value=2.0,             # LM guidance on LocDiT, higher for better adherence to the prompt, but maybe worse\n    inference_timesteps=10,   # LocDiT inference timesteps, higher for better result, lower for fast speed\n    normalize=True,           # enable external TN tool\n    denoise=True,             # enable external Denoise tool\n    retry_badcase=True,        # enable retrying mode for some bad cases (unstoppable)\n    retry_badcase_max_times=3,  # maximum retrying times\n    retry_badcase_ratio_threshold=6.0, # maximum length restriction for bad case detection (simple but effective), it could be adjusted for slow pace speech\n)\n\nsf.write(\"output.wav\", wav, 16000)\nprint(\"saved: output.wav\")\n\n# Streaming\nchunks = []\nfor chunk in model.generate_streaming(\n    text = \"Streaming text to speech is easy with VoxCPM!\",\n    # supports same args as above\n):\n    chunks.append(chunk)\nwav = np.concatenate(chunks)\n\nsf.write(\"output_streaming.",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-28T02:27:22.851625"
  },
  {
    "basic_info": {
      "name": "multi-agent-coding-system",
      "full_name": "Danau5tin/multi-agent-coding-system",
      "owner": "Danau5tin",
      "description": "Reached #13 on Stanford's Terminal Bench leaderboard. Orchestrator, explorer & coder agents working together with intelligent context sharing.",
      "url": "https://github.com/Danau5tin/multi-agent-coding-system",
      "clone_url": "https://github.com/Danau5tin/multi-agent-coding-system.git",
      "ssh_url": "git@github.com:Danau5tin/multi-agent-coding-system.git",
      "homepage": "",
      "created_at": "2025-08-30T19:23:45Z",
      "updated_at": "2025-09-27T06:51:58Z",
      "pushed_at": "2025-09-07T16:01:23Z"
    },
    "stats": {
      "stars": 1199,
      "forks": 145,
      "watchers": 1199,
      "open_issues": 2,
      "size": 849
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 139397,
        "Shell": 483
      },
      "license": null,
      "topics": []
    },
    "content": {
      "readme": "# ü§ì Orchestrator: A multi-agent AI coder. Reached #13 on Stanford's TerminalBench. Open sourced!\n\nTL;DR:\n- Over the weekend, quite unexpectedly, I made a multi-agent AI system that places slightly higher than Claude Code on Stanford's TerminalBench leaderboard (13th place).\n- This AI system consists of an orchestration agent that dispatches multiple explorer and coder agents to do all the work.\n- The orchestrator explicitly defines what knowledge artifacts subagents must return, then reuses and synthesises these artifacts across future tasks - creating compound intelligence where each action builds meaningfully on previous discoveries.\n\n![Orchestrator with claude-sonnet-4 on standford's terminal bench](./readme_imgs/orchestrator-sonnet-4-stanford-terminal-bench-leaderboard.png)\n\n## How the System Works\n\n![System architecture overview](readme_imgs/orch_agent_sys_arch.png)\n\nThe orchestrator acts as the brain of the operation - it receives the user's task but never touches code directly. Instead, it:\n\n1. **Analyses** the task and breaks it into focused subtasks\n2. **Dispatches** explorer agents to understand the system\n3. **Delegates** implementation work to coder agents with precise instructions\n4. **Verifies** all changes through additional explorer agents\n5. **Maintains** the context store with all discovered knowledge\n\nThe orchestrator's lack of direct code access forces proper delegation and verification patterns, leading to more strategic solutions.\n\nFor a full breakdown of this project's code structure, see [here](./PROJECT_STRUCTURE.md)\n\n\n## üìà Evaluation Results\n\n### Performance on TerminalBench\n\n[Terminal bench](https://www.tbench.ai/) is a brilliant benchmark created by Stanford and [Laude Institute](https://www.laude.org/) to quantify agents' ability to complete complex tasks in the terminal. My Orchestrator system achieved **13th place** on the leaderboard, demonstrating competitive performance against leading AI coding assistants.\n\nI ran the Orchestrator evaluations with both Claude-4-Sonnet and also Qwen3-Coder-480B-A35B:\n\n![Performance comparison chart](readme_imgs/perf_chart.png)\n![Orchestrator with qwen-3-coder on standford's terminal bench](./readme_imgs/orchestrator-qwen-3-coder-stanford-terminal-bench-leaderboard.png)\n\nThis image shows Qwen-3-Coder performance on the benchmark. The screenshot towards the top of this README shows Sonnet-4 performance.\n\n### Cost & Efficiency\n\nOne of the most striking results is the amount of tokens used by Sonnet-4 as opposed to Qwen3-Coder.\n\nThe below table shows the total tokens (input and output included) processed across the TerminalBench evaluation run (5 attempts at 80 tasks = 400 trajectories).\n\n| Model | Success Rate | Total Evaluation Cost | Token Usage |\n|-------|--------------|------------|-------------|\n| **Claude Sonnet-4** | 37.0% | $263.56* | 93.2M tokens |\n| **Qwen-3-Coder** | 19.7% | $217.83 | 14.7M tokens |\n\n*Claude Sonnet-4 costs reflect heavy caching usage, reducing actual API costs\n\n\n## ü§ñ The Agents\n\nWhile all agents use the same underlying LLM, each operates with its own context window, specialised system message, and distinct toolset. This creates functionally different agents optimised for their specific roles.\n\n### üéØ Orchestrator Agent\n[System message](./src/agents/system_msgs/md_files/orchestrator_sys_msg_v0.1.md)\n**Role:** Strategic coordinator and persistent intelligence layer  \n**Capabilities:** Task decomposition, context management, subagent delegation  \n**Tools:** Task creation, subagent launching, context store management  \n**Restrictions:** Cannot read or modify code directly - operates purely at architectural level  \n\nThe orchestrator maintains the complete picture across all tasks, tracking discoveries and progress. It crafts precise task descriptions that explicitly specify what contexts subagents should return, ensuring focused and valuable information gathering.\n\n**Trust Calibration Strategy:**  \nThe orchestrator employs adaptive delegation based on task complexity:\n- **Low Complexity Tasks**: Grants extremely high autonomy to the coder agent for simple modifications and bug fixes\n- **Medium/Large Tasks**: Maintains strong trust but uses iterative decomposition - breaking complex problems into atomic, verifiable steps\n- **Verification Philosophy**: Uses explorer agents liberally to verify progress, especially when tasks involve critical functionality\n\n\n### üîç Explorer Agent \n[System message](./src/agents/system_msgs/md_files/explorer_sys_msg_v0.1.md) \n**Role:** Read-only investigation and verification specialist  \n**Capabilities:** System exploration, code analysis, test execution, verification  \n**Tools:** File reading, search operations (grep/glob), bash commands, temporary script creation  \n**Restrictions:** Cannot modify existing files - strictly read-only operations  \n\nExplorers gather intelligence about the codebase, verify implementations, and discover system behaviors. They create knowledge artifacts that eliminat",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-28T02:27:24.061006"
  },
  {
    "basic_info": {
      "name": "SRPO",
      "full_name": "Tencent-Hunyuan/SRPO",
      "owner": "Tencent-Hunyuan",
      "description": "Directly Aligning the Full Diffusion Trajectory with Fine-Grained Human Preference",
      "url": "https://github.com/Tencent-Hunyuan/SRPO",
      "clone_url": "https://github.com/Tencent-Hunyuan/SRPO.git",
      "ssh_url": "git@github.com:Tencent-Hunyuan/SRPO.git",
      "homepage": "https://tencent.github.io/srpo-project-page/",
      "created_at": "2025-09-09T07:36:49Z",
      "updated_at": "2025-09-28T01:55:38Z",
      "pushed_at": "2025-09-16T06:21:00Z"
    },
    "stats": {
      "stars": 1021,
      "forks": 32,
      "watchers": 1021,
      "open_issues": 13,
      "size": 12251
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 712645,
        "Shell": 4058
      },
      "license": "Other",
      "topics": []
    },
    "content": {
      "readme": "<div align=‚Äúcenter‚Äù style=‚Äúfont-family: charter;‚Äù>\n<h1 align=\"center\">Directly Aligning the Full Diffusion Trajectory with Fine-Grained Human Preference </h1>\n<div align=\"center\">\n  <a href='https://arxiv.org/abs/2509.06942'><img src='https://img.shields.io/badge/ArXiv-red?logo=arxiv'></a>  &nbsp;\n  <a href='https://huggingface.co/tencent/SRPO/'><img src='https://img.shields.io/badge/Model-blue?logo=huggingface'></a> &nbsp; \n  <a href='https://tencent.github.io/srpo-project-page/'><img src='https://img.shields.io/badge/%F0%9F%92%BB_Project-SRPO-blue'></a> &nbsp;\n</div>\n<div align=\"center\">\n  Xiangwei Shen<sup>1,2,3*</sup>,\n  <a href=\"https://scholar.google.com/citations?user=Lnr1FQEAAAAJ&hl=zh-CN\" target=\"_blank\"><b>Zhimin Li</b></a><sup>1*</sup>,\n  <a href=\"https://scholar.google.com.hk/citations?user=Fz3X5FwAAAAJ\" target=\"_blank\"><b>Zhantao Yang</b></a><sup>1</sup>, \n  <a href=\"https://shiyi-zh0408.github.io/\" target=\"_blank\"><b>Shiyi Zhang</b></a><sup>3</sup>,\n  Yingfang Zhang<sup>1</sup>,\n  Donghao Li<sup>1</sup>,\n  <br>\n  <a href=\"https://scholar.google.com/citations?user=VXQV5xwAAAAJ&hl=en\" target=\"_blank\"><b>Chunyu Wang</b></a><sup>1‚úù</sup>,\n  <a href=\"https://openreview.net/profile?id=%7EQinglin_Lu2\" target=\"_blank\"><b>Qinglin Lu</b></a><sup>1</sup>,\n  <a href=\"https://andytang15.github.io\" target=\"_blank\"><b>Yansong Tang</b></a><sup>3,‚úâÔ∏è</sup>\n</div>\n<div align=\"center\">\n  <sup>1</sup>Hunyuan, Tencent‚ÄÉ\n  <br>\n  <sup>2</sup>School of Science and Engineering, The Chinese University of Hong Kong, Shenzhen‚ÄÉ\n  <br>\n  <sup>3</sup>Shenzhen International Graduate School, Tsinghua University‚ÄÉ\n  <br>\n  <sup>*</sup>Equal contribution‚ÄÉ\n  <sup>‚úù</sup>Project lead‚ÄÉ\n  <sup>‚úâÔ∏è</sup>Corresponding author\n</div>\n\n![head](assets/head.jpg)\n\n## üéâ Key Features\n1. **Direct Align**: We introduce a new sampling strategy for diffusion fine-tuning that can effectively restore highly noisy images, leading to an optimization process that is more stable and less computationally demanding, especially during the initial timesteps.\n2. **Faster Training**:   By rolling out only a single image and optimizing directly with analytical gradients‚Äîa key distinction from GRPO‚Äîour method achieves significant performance improvements for FLUX.1.dev in under 10 minutes of training. To further accelerate the process, our method supports replacing online rollouts entirely with a small dataset of real images; we find that fewer than 1500 images are sufficient to effectively train FLUX.1.dev.\n3. **Free of Reward Hacking**: We have improved the training strategy for method that direct backpropagation on reward signal (such as ReFL and DRaFT). Moreover, we directly regularize the model using negative rewards, without the need for KL divergence or a separate reward system. In our experiments, this approach achieves comparable performance with multiple different rewards, improving the perceptual quality of FLUX.1.dev without suffering from reward hacking issues, such as overfitting to color or oversaturation preferences.\n4. **Potential for Controllable Fine-tuning**: For the first time in online RL, we incorporate dynamically controllable text conditions, enabling on-the-fly adjustment of reward preference towards styles within the scope of the reward model.\n\n## üî• News\n\n- __[2025.9.12]__:  üéâ We released the complete training code. We also share tips and experiences to help you train your models. You‚Äôre welcome to discuss and ask questions in the issues! üí¨‚ú®\n- __[2025.9.12]__:  üéâ We provide a standard workflow‚Äîfeel free to use it in ComfyUI.\n- __[2025.9.8]__:   üéâ We released the paper, checkpoint, inference code.\n\n## üìë Open-source Plan\n- [X] The training code is under internal review and will be open-sourced by this weekend at the latest.\n- [ ] Release a quantized version for the FLUX community.\n- [ ] Extend support to other models.\n\n## üõ†Ô∏è Dependencies and Installation\n\n```bash\nconda create -n SRPO python=3.10.16 -y\nconda activate SRPO\nbash ./env_setup.sh \n```\nüí° The environment dependency is basically the same as DanceGRPO\n\n## ü§ó Download Models\n\n1. Model Cards\n\n|       Model       |                           Huggingface Download URL                                      |  \n|:-----------------:|:---------------------------------------------------------------------------------------:|\n|       SRPO        |           [diffusion_pytorch_model](https://huggingface.co/tencent/SRPO/tree/main)      |\n\n2. Download our `diffusion_pytorch_model.safetensors` in [https://huggingface.co/tencent/SRPO]\n```bash\nmkdir ./srpo\nhuggingface-cli login\nhuggingface-cli download --resume-download Tencent/SRPO diffusion_pytorch_model.safetensors --local-dir ./srpo/\n```\n3. Load your FLUX cache or use the `black-forest-labs/FLUX.1-dev`[https://huggingface.co/black-forest-labs/FLUX.1-dev]\n```bash\nmkdir ./data/flux\nhuggingface-cli login\nhuggingface-cli download --resume-download  black-forest-labs/FLUX.1-dev --local-dir ./data/flux\n```\n\n## üîë Inference\n\n### Using ComfyUI\n\nYou can use",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-28T02:27:25.268407"
  },
  {
    "basic_info": {
      "name": "SpikingBrain-7B",
      "full_name": "BICLab/SpikingBrain-7B",
      "owner": "BICLab",
      "description": null,
      "url": "https://github.com/BICLab/SpikingBrain-7B",
      "clone_url": "https://github.com/BICLab/SpikingBrain-7B.git",
      "ssh_url": "git@github.com:BICLab/SpikingBrain-7B.git",
      "homepage": null,
      "created_at": "2025-09-03T10:46:46Z",
      "updated_at": "2025-09-28T00:43:25Z",
      "pushed_at": "2025-09-25T02:30:39Z"
    },
    "stats": {
      "stars": 977,
      "forks": 120,
      "watchers": 977,
      "open_issues": 5,
      "size": 7282
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 328351,
        "Dockerfile": 695
      },
      "license": null,
      "topics": []
    },
    "content": {
      "readme": "# SpikingBrainÔºöSpiking Brain-inspired Large Models\r\n\r\nüìÑ Technical Report: [Chinese](SpikingBrain_Report_Chi.pdf) | [English](SpikingBrain_Report_Eng.pdf)  \r\nüöÄ Arxiv: [arXiv:2509.05276](https://www.arxiv.org/abs/2509.05276)  \r\nüß© Models: [Available Models](#available-models)   \r\nüîó Demo: [OpenBayesË¥ùÂºèËÆ°ÁÆó](https://openbayes.com/console/public/tutorials/eKBhv3jUkWw)    \r\n\r\n---\r\n\r\n## About SpikingBrain\r\n\r\nInspired by brain mechanisms, **SpikingBrain** integrates **hybrid efficient attention**, **MoE modules**, and **spike encoding** into its architecture, supported by a universal conversion pipeline compatible with the open-source model ecosystem. This enables continual pre-training with less than 2\\% of the data while achieving performance comparable to mainstream open-source models. We further adapt frameworks, operators, parallel strategies, and communication primitives for **non-NVIDIA (MetaX) clusters**, ensuring stable large-scale training and inference. SpikingBrain achieves over 100√ó speedup in TTFT for 4M-token sequences, while spiking delivers over 69\\% sparsity at the micro level. Combined with macro-level MoE sparsity, these advances provide valuable guidance for the design of next-generation neuromorphic chips.\r\n\r\n![](assets/fig1.png)\r\n\r\n---\r\n\r\n## Project Structure\r\nThis repository provides the full implementation and weights of **SpikingBrain-7B**, including the **HuggingFace version**, **vLLM inference version**, and **quantized version**, enabling flexible deployment and research across different scenarios.\r\n\r\n```\r\nSpikingBrain-7B/\r\n‚îú‚îÄ‚îÄ hf_7B_model/ # HuggingFace version\r\n‚îú‚îÄ‚îÄ run_model/   # Model run examples\r\n‚îú‚îÄ‚îÄ vllm_hymeta/ # vLLM plugins and inference support\r\n‚îú‚îÄ‚îÄ W8ASpike/    # Quantized inference version\r\n‚îú‚îÄ‚îÄ setup.py\r\n‚îú‚îÄ‚îÄ requirements.txt \r\n‚îî‚îÄ‚îÄ README.md \r\n```\r\n\r\n--- \r\n\r\n## vLLM-HyMeta\r\n\r\n**vllm-hymeta** is the plugin adaptation of HyMeta (Hybrid Models built on MetaX GPUs) for the [vLLM inference framework](https://github.com/vllm-project/vllm/tree/main), providing efficient inference support on NVIDIA GPUs.\r\n\r\nBy leveraging the [plugins mechanism](https://blog.vllm.ai/2025/05/12/hardware-plugin.html) in vLLM, hardware backends can be integrated in a modular fashion, bringing the following benefits:\r\n\r\n- **Decoupled codebase**: Backend-specific code remains independent, keeping the vLLM core cleaner.\r\n\r\n- **Reduced maintenance cost**: vLLM developers can focus on general functionality without being affected by backend-specific implementations.\r\n\r\n- **Faster integration**: New backends can be integrated quickly and evolve independently with less engineering effort.\r\n\r\n### Container Deployment (NVIDIA)\r\n```bash\r\ngit clone https://github.com/BICLab/SpikingBrain-7B.git\r\ncd SpikingBrain-7B\r\ndocker build -t spiking-brain:7b-v1.0 .\r\n```\r\n```bash\r\ndocker run -itd \\\r\n    --entrypoint /bin/bash \\\r\n    --network host \\\r\n    --name <container_name> \\\r\n    --shm-size 160g \\\r\n    --gpus all \\\r\n    --privileged \\\r\n    -v /host_path:/container_path \\\r\n    spiking-brain:7b-v1.0\r\n```\r\n\r\nRecommended environment for installing **vllm-hymeta** on NVIDIA GPUs:\r\n\r\n```makefile\r\ntorch==2.7.1\r\ntransformers==4.55.2\r\ntriton==3.3.1\r\nflash_attn==2.7.3\r\nflash-linear-attention==0.1\r\nvllm==0.10.0\r\nscipy\r\npyyaml\r\ndecorator\r\nsetuptools\r\nsetuptools-scm\r\n```\r\n\r\n### Run with vLLM\r\n\r\nYou can serve a model with vLLM in the simplest way using the following command:\r\n\r\n```bash\r\nvllm serve <your_model_path> \\\r\n  --served-model-name <model_name> \\\r\n  --gpu-memory-utilization <ratio> \\\r\n  --block-size <size> \\\r\n  --dtype bfloat16 \\\r\n  --port <port_number>\r\n```\r\n\r\nYou may also set `--tensor-parallel-size` and `--pipeline-parallel-size` when launching if you want to run with multiple GPUs. \r\n\r\n---\r\n\r\n## W8ASpike\r\n\r\n**W8ASpike** is the quantized inference version of SpikingBrain-7B, aiming to reduce inference cost under low-precision settings and explore the potential of Spiking Neural Networks (SNNs).\r\n\r\nThe current implementation adopts **pseudo-spiking**, where activations are approximated as spike-like signals at the tensor level, rather than true asynchronous event-driven spiking on neuromorphic hardware.\r\n\r\n- **Pseudo-spiking**: Efficient approximation at the tensor level, suitable for prototyping and research.\r\n\r\n- **True-spiking**: Requires asynchronous hardware and event-driven operator support, which is beyond the scope of this repository.\r\n\r\nThe activation spike encoding process here is inspired by the pseudo-spiking interfaces from [BICLab/Int2Spike](https://github.com/BICLab/Int2Spike). For additional PyTorch-based spiking interfaces, please refer to the Int2Spike library.\r\n\r\n---\r\n\r\n## Available Models\r\nThe model weights are hosted on **ModelScope**. Please select the appropriate version based on your needs:\r\n\r\n- **Pre-trained model (7B):** https://www.modelscope.cn/models/Panyuqi/V1-7B-base\r\n- **Chat model (7B-SFT):** https://www.modelscope.cn/models/Panyuqi/V1-7B-sft-s3-reasoning\r\n- **Quantized weights (7B-W8ASpike):** http",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-28T02:27:26.507176"
  },
  {
    "basic_info": {
      "name": "PromptEnhancer",
      "full_name": "Hunyuan-PromptEnhancer/PromptEnhancer",
      "owner": "Hunyuan-PromptEnhancer",
      "description": "PromptEnhancer is a prompt-rewriting tool, refining prompts into clearer, structured versions for better image generation.",
      "url": "https://github.com/Hunyuan-PromptEnhancer/PromptEnhancer",
      "clone_url": "https://github.com/Hunyuan-PromptEnhancer/PromptEnhancer.git",
      "ssh_url": "git@github.com:Hunyuan-PromptEnhancer/PromptEnhancer.git",
      "homepage": "https://hunyuan-promptenhancer.github.io/",
      "created_at": "2025-09-09T08:56:47Z",
      "updated_at": "2025-09-28T02:23:44Z",
      "pushed_at": "2025-09-26T06:21:52Z"
    },
    "stats": {
      "stars": 898,
      "forks": 76,
      "watchers": 898,
      "open_issues": 6,
      "size": 11897
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 28529,
        "Shell": 379
      },
      "license": "Other",
      "topics": [
        "hunyuan",
        "hunyuan-image",
        "prompt",
        "prompt-engineering",
        "prompt-enhancer",
        "vlm"
      ]
    },
    "content": {
      "readme": "<div align=\"center\">\n\n# PromptEnhancer: A Simple Approach to Enhance Text-to-Image Models via Chain-of-Thought Prompt Rewriting\n\n[**Linqing Wang**](https://scholar.google.com/citations?hl=en&view_op=list_works&gmla=AH8HC4z9rmDHYjp5o28xKk8U4ddD_n7BuMnk8UZFP-jygFBtHUSz6pf-5FP32B_yKMpRU9VpDY3iT8eM0zORHA&user=Hy12lcEAAAAJ) ¬∑ \n[**Ximing Xing**](https://ximinng.github.io/) ¬∑ \n[**Yiji Cheng**](https://scholar.google.com/citations?user=Plo8ZSYAAAAJ&hl=en) ¬∑ \nZhiyuan Zhao ¬∑ \nDonghao Li ¬∑\nTiankai Hang ¬∑\nZhenxi Li ¬∑\n[**Jiale Tao**](https://scholar.google.com/citations?user=WF5DPWkAAAAJ&hl=en) ¬∑ \n[**QiXun Wang**](https://github.com/wangqixun) ¬∑ \n[**Ruihuang Li**](https://scholar.google.com/citations?user=8CfyOtQAAAAJ&hl=en) ¬∑ \nComi Chen ¬∑\nXin Li ¬∑ \n[**Mingrui Wu**](https://scholar.google.com/citations?user=sbCKwnYAAAAJ&hl=en) ¬∑ \nXinchi Deng ¬∑ \nShuyang Gu ¬∑ \n[**Chunyu Wang**](https://scholar.google.com/citations?user=VXQV5xwAAAAJ&hl=en)<sup>‚Ä†</sup> ¬∑ \n[**Qinglin Lu**](https://luqinglin.weebly.com/)<sup>*</sup>\n\nTencent Hunyuan\n\n<sup>‚Ä†</sup>Project Lead ¬∑ <sup>*</sup>Corresponding Author\n\n</div>\n\n<p align=\"center\">\n  <a href=\"https://www.arxiv.org/abs/2509.04545\"><img src=\"https://img.shields.io/badge/Paper-arXiv:2509.04545-red?logo=arxiv\" alt=\"arXiv\"></a>\n  <a href=\"https://zhuanlan.zhihu.com/p/1949013083109459515\"><img src=\"https://img.shields.io/badge/Áü•‰πé-ÊäÄÊúØËß£ËØª-0084ff?logo=zhihu\" alt=\"Zhihu\"></a>\n  <a href=\"https://huggingface.co/tencent/HunyuanImage-2.1/tree/main/reprompt\"><img src=\"https://img.shields.io/badge/Model-PromptEnhancer_7B-blue?logo=huggingface\" alt=\"HuggingFace Model\"></a>\n  <!-- <a href=\"https://huggingface.co/PromptEnhancer/PromptEnhancer-32B\"><img src=\"https://img.shields.io/badge/Model-PromptEnhancer_32B-blue?logo=huggingface\" alt=\"HuggingFace Model\"></a> -->\n  <a href=\"https://huggingface.co/datasets/PromptEnhancer/T2I-Keypoints-Eval\"><img src=\"https://img.shields.io/badge/Benchmark-T2I_Keypoints_Eval-blue?logo=huggingface\" alt=\"T2I-Keypoints-Eval Dataset\"></a>\n  <a href=\"https://hunyuan-promptenhancer.github.io/\"><img src=\"https://img.shields.io/badge/Homepage-PromptEnhancer-1abc9c?logo=homeassistant&logoColor=white\" alt=\"Homepage\"></a>\n  <a href=\"https://github.com/Tencent-Hunyuan/HunyuanImage-2.1\"><img src=\"https://img.shields.io/badge/Code-HunyuanImage2.1-2ecc71?logo=github\" alt=\"HunyuanImage2.1 Code\"></a>\n  <a href=\"https://huggingface.co/tencent/HunyuanImage-2.1\"><img src=\"https://img.shields.io/badge/Model-HunyuanImage2.1-3498db?logo=huggingface\" alt=\"HunyuanImage2.1 Model\"></a>\n  <a href=https://x.com/TencentHunyuan target=\"_blank\"><img src=https://img.shields.io/badge/Hunyuan-black.svg?logo=x height=22px></a>\n</p>\n\n---\n\n<p align=\"center\">\n  <img src=\"assets/teaser-1.png\" alt=\"PromptEnhancer Teaser\"/>\n</p>\n\n## Overview\n\nHunyuan-PromptEnhancer is a prompt rewriting utility. It restructures an input prompt while preserving the original intent, producing clearer, layered, and logically consistent prompts suitable for downstream image generation or similar tasks.\n\n- Preserves intent across key elements (subject/action/quantity/style/layout/relations/attributes/text, etc.).\n- Encourages a \"global‚Äìdetails‚Äìsummary\" narrative, describing primary elements first, then secondary/background elements, ending with a concise style/type summary.\n- Robust output parsing with graceful fallback: prioritizes `<answer>...</answer>`; if missing, removes `<think>...</think>` and extracts clean text; otherwise falls back to the original input.\n- Configurable inference parameters (temperature, top_p, max_new_tokens) for balancing determinism and diversity.\n\n## üî•üî•üî•Updates\n\n- [2025-09-22] üöÄ Thanks @mradermacher for adding **GGUF model support** for efficient inference with quantized models!\n- [2025-09-18] ‚ú® Try the [PromptEnhancer-32B](https://huggingface.co/PromptEnhancer/PromptEnhancer-32B) for higher-quality prompt enhancement!\n- [2025-09-16] Release [T2I-Keypoints-Eval dataset](https://huggingface.co/datasets/PromptEnhancer/T2I-Keypoints-Eval).\n- [2025-09-07] Release [PromptEnhancer-7B model](https://huggingface.co/tencent/HunyuanImage-2.1/tree/main/reprompt).\n- [2025-09-07] Release [technical report](https://arxiv.org/abs/2509.04545).\n\n## Prerequisites\n\n- **Python**: 3.8 or higher\n- **CUDA**: 11.8+ (recommended for GPU acceleration)\n- **Storage**: At least 20GB free space for models\n- **Memory**: 8GB+ RAM (16GB+ recommended for 32B models)\n\n## Installation\n\n### Option 1: Standard Installation (Recommended for most users)\n```bash\npip install -r requirements.txt\n```\n\n### Option 2: GGUF Installation (For quantized models with CUDA support)\n```bash\nchmod +x script/install_gguf.sh && ./script/install_gguf.sh\n```\n\n> **üí° Tip**: Choose GGUF installation if you want faster inference with lower memory usage, especially for the 32B model.\n\n## Model Download\n\n### üéØ Quick Start (Recommended)\nFor most users, we recommend starting with the **PromptEnhancer-7B** model:\n\n```bash\n# Download PromptEnhancer-7B (13GB) - Best balance of",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-28T02:27:27.751827"
  },
  {
    "basic_info": {
      "name": "batch_invariant_ops",
      "full_name": "thinking-machines-lab/batch_invariant_ops",
      "owner": "thinking-machines-lab",
      "description": null,
      "url": "https://github.com/thinking-machines-lab/batch_invariant_ops",
      "clone_url": "https://github.com/thinking-machines-lab/batch_invariant_ops.git",
      "ssh_url": "git@github.com:thinking-machines-lab/batch_invariant_ops.git",
      "homepage": null,
      "created_at": "2025-09-10T00:08:08Z",
      "updated_at": "2025-09-27T20:07:39Z",
      "pushed_at": "2025-09-10T16:58:34Z"
    },
    "stats": {
      "stars": 745,
      "forks": 51,
      "watchers": 745,
      "open_issues": 3,
      "size": 8
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 18886
      },
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "# Batch Invariant Ops\n\nA companion library release to https://thinkingmachines.ai/blog/defeating-nondeterminism-in-llm-inference/. This library contains some batch-invariant kernels as well as an example of achieving deterministic vLLM inference.\n\n## Overview\n\nThis library primarily leverages torch.Library to sub out existing PyTorch kernels with \"batch-invariant\" ones. This allows many existing PyTorch models to use the batch-invariant ops with low overhead and non-intrusive code changes.\n\n## Installation\n\n```bash\npip install -e .\n```\n\n## Quick Start\n\n```python\nimport torch\nfrom batch_invariant_ops import set_batch_invariant_mode\n\n# Enable batch-invariant mode\nwith set_batch_invariant_mode():\n    # Your inference code here\n    model = YourModel()\n    output = model(input_tensor)\n```\n\n## Testing Batch-Invariance\n\nThe following example shows how batch size can affect results in standard PyTorch:\n\n```python\nimport torch\nfrom batch_invariant_ops import set_batch_invariant_mode\ntorch.set_default_device('cuda')\n\n# Just to get the logging out of the way haha\nwith set_batch_invariant_mode(True):\n    pass\n\ndef test_batch_invariance():\n    B, D = 2048, 4096\n    a = torch.linspace(-100, 100, B*D).reshape(B, D)\n    b = torch.linspace(-100, 100, D*D).reshape(D, D)\n    \n    # Method 1: Matrix-vector multiplication (batch size 1)\n    out1 = torch.mm(a[:1], b)\n    \n    # Method 2: Matrix-matrix multiplication, then slice (full batch)\n    out2 = torch.mm(a, b)[:1]\n    \n    # Check if results are identical\n    diff = (out1 - out2).abs().max()\n    print(f\"Difference: {diff.item()}\")\n    return diff.item() == 0\n\n# Test with standard PyTorch (likely to show differences)\nprint(\"Standard PyTorch:\")\nwith set_batch_invariant_mode(False):\n    is_deterministic = test_batch_invariance()\n    print(f\"Deterministic: {is_deterministic}\")\n\n# Test with batch-invariant operations\nprint(\"\\nBatch-Invariant Mode:\")\nwith set_batch_invariant_mode(True):\n    is_deterministic = test_batch_invariance()\n    print(f\"Deterministic: {is_deterministic}\")\n\n```\n\n## Deterministic Inference in vLLM\n`deterministic_vllm_inference.py` shows an proof of concept of validating that vLLM can be made deterministic with a minor upstream PR to use this library. Without the upstream PR, we see that out of 1000 random length 100 completions we see 18 unique samples. After the upstream PR, there is only one unique sample.\n\n## Supported Operations\n\n### Matrix Operations\n- `torch.mm()` - Matrix multiplication\n- `torch.addmm()` - Matrix multiplication with bias addition\n\n### Activation Functions\n- `torch.log_softmax()` - Log-softmax activation\n\n### Reduction Operations\n- `torch.mean()` - Mean computation along specified dimensions\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-28T02:27:28.969815"
  },
  {
    "basic_info": {
      "name": "checkpoint-engine",
      "full_name": "MoonshotAI/checkpoint-engine",
      "owner": "MoonshotAI",
      "description": "Checkpoint-engine is a simple middleware to update model weights in LLM inference engines",
      "url": "https://github.com/MoonshotAI/checkpoint-engine",
      "clone_url": "https://github.com/MoonshotAI/checkpoint-engine.git",
      "ssh_url": "git@github.com:MoonshotAI/checkpoint-engine.git",
      "homepage": "",
      "created_at": "2025-09-08T08:04:20Z",
      "updated_at": "2025-09-28T00:16:05Z",
      "pushed_at": "2025-09-22T14:26:01Z"
    },
    "stats": {
      "stars": 742,
      "forks": 47,
      "watchers": 742,
      "open_issues": 5,
      "size": 301
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 55357
      },
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "# Checkpoint Engine\nCheckpoint-engine is a simple middleware to update model weights in LLM inference engines -- a critical step in reinforcement learning.\nWe provide an efficient and lightweight implementation for inplace weight update:\nupdating our [Kimi-K2](https://github.com/MoonshotAI/Kimi-K2) model (1 Trillion parameters) across thousands of GPUs takes about 20s.\n\n\n<div align=\"center\">\n  <picture>\n      <img src=\"figures/checkpoint-engine.png\" width=\"80%\" alt=\"ckpt-engine\">\n  </picture>\n</div>\n\n## Architecture\n\nThe core weight update logic is in `ParameterServer` class, a service colocated with inference engines. It provides two implementations of weight update: Broadcast and P2P.\n\n- **Broadcast**: Used when a large number of inference instances need to update weights in synchronous. This is the fastest implementation and should be used as the default update method. See `_update_per_bucket`.\n- **P2P**: Used when new inference instances are dynamically added (due to restarts or dynamic availability) while the existing instances are already serving requests. Under this scenario, to avoid affecting the workloads on existing instances, we use the [`mooncake-transfer-engine`](https://github.com/kvcache-ai/Mooncake?tab=readme-ov-file#use-python-package) to P2P send weights from CPUs in existing instances to GPUs in new instances. See `_update_per_bucket_p2p`.\n\n### Optimized Weight Broadcast\nIn the *Broadcast* implementation, the checkpoint-engine holds references to sharded weights in CPU memory, and need to efficiently broadcast them to a cluster of inference instances, often under a different sharding pattern.\nWe arrange the data transfer into 3 stages:\n1. H2D: moving weights to GPU memory. These weights may come from disk or the training engine.\n2. broadcast: broadcast among checkpoint engine workers; the data results in a CUDA IPC buffer shared with inference engine.\n3. reload: inference engine decides what subset of weights to copy from the broadcasted data.\n\nCheckpoint-engine orchestrates the entire transfer process. It first gathers necessary metadata to create a plan, including deciding the proper bucket size for data transfer.\nIt then executes the transfer, where it controls the inference engine through a ZeroMQ socket. To maximize performance, it organizes the data transfers into a pipeline with overlapped communication and copy, illustrated below. The details can be found in [Kimi-K2 Technical Report](https://arxiv.org/abs/2507.20534).\n\n\n<div align=\"center\">\n  <picture>\n      <img src=\"figures/pipeline.png\" width=\"80%\" alt=\"pipeline\">\n  </picture>\n</div>\n\nPipelining naturally requires more GPU memory. When memory is not enough, checkpoint-engine will fallback to serial execution.\n\n## Benchmark\n\n| Model                                | Device Info  | GatherMetas | Update (Broadcast) | Update (P2P)            |\n| :----------------------------------- | :----------- | :---------- |:-------------------| :---------------------- |\n| GLM-4.5-Air (BF16)                   | 8xH800 TP8  | 0.17s       | 3.94s (1.42GiB)    | 8.83s (4.77GiB)         |\n| Qwen3-235B-A22B-Instruct-2507 (BF16) | 8xH800 TP8  | 0.46s       | 6.75s (2.69GiB)    | 16.47s (4.05GiB)        |\n| DeepSeek-V3.1 (FP8)                  | 16xH20 TP16  | 1.44s       | 12.22s (2.38GiB)   | 25.77s (3.61GiB)        |\n| Kimi-K2-Instruct (FP8)               | 16xH20 TP16  | 1.81s       | 15.45s (2.93GiB)   | 36.24s (4.46GiB)        |\n| DeepSeek-V3.1 (FP8)                  | 256xH20 TP16 | 1.40s       | 13.88s (2.54GiB)   | 33.30s (3.86 GiB) |\n| Kimi-K2-Instruct (FP8)               | 256xH20 TP16 | 1.88s       | 21.50s (2.99GiB)   | 34.49s (4.57 GiB) |\n\nAll results above are tested by [`examples/update.py`](./examples/update.py) and use [vLLM v0.10.2rc1](https://github.com/vllm-project/vllm/tree/v0.10.2rc1) as inference engine. Some notes:\n\n* FP8 test needs additional vLLM patches, see [FP8 quantization](#fp8-quantization).\n* Device Info: we tested various combination of devices and parallelism setups. For example, a 256-GPU TP16 setup means that we deploy 16 vLLM instances, each with 16-way tensor parallelism.\n* Since update duration is related to IPC bucket size, we provide the bucket size in the table.\n* The P2P time were tested for updating no more than two nodes (16 GPUs) (`ParameterServer.update(ranks=range(0, 16))`) out of the entire cluster.\n\n## Installation\n\nUse the fastest broadcast implementation\n\n```Bash\npip install checkpoint-engine\n```\n\nUse the flexible P2P implementation, notice this will install `mooncake-transfer-engine` to support RDMA transfer between different ranks.\n\n```Bash\npip install 'checkpoint-engine[p2p]'\n```\n\nIf set `NCCL_IB_HCA` env, checkpoint-engine will use it to auto select net devices for different ranks. If not set, it will read all RDMA devices and try to divide them into each rank.\n\n## Getting Started\n\nPrepare an H800 or H20 machine with 8 GPUs with latest vLLM. Be sure to include [/collective_rpc API endpoint](ht",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-28T02:27:30.184075"
  },
  {
    "basic_info": {
      "name": "FireRedTTS2",
      "full_name": "FireRedTeam/FireRedTTS2",
      "owner": "FireRedTeam",
      "description": "Long-form streaming TTS system for multi-speaker dialogue generation",
      "url": "https://github.com/FireRedTeam/FireRedTTS2",
      "clone_url": "https://github.com/FireRedTeam/FireRedTTS2.git",
      "ssh_url": "git@github.com:FireRedTeam/FireRedTTS2.git",
      "homepage": null,
      "created_at": "2025-09-02T06:25:28Z",
      "updated_at": "2025-09-28T01:21:17Z",
      "pushed_at": "2025-09-17T12:29:15Z"
    },
    "stats": {
      "stars": 714,
      "forks": 85,
      "watchers": 714,
      "open_issues": 15,
      "size": 777
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 124201
      },
      "license": "Apache License 2.0",
      "topics": []
    },
    "content": {
      "readme": "<div align=\"center\">\n    <h1>\n    FireRedTTS-2\n    </h1>\n    <p>\n    Official PyTorch code for <br>\n    <b><em>FireRedTTS-2: Towards Long Conversational Speech Generation for Podcast and Chatbot</em></b>\n    </p>\n    <p>\n    <!-- <img src=\"assets/XiaoHongShu_Logo.png\" alt=\"Institution 4\" style=\"width: 102px; height: 48px;\"> -->\n    <img src=\"assets/FireRedTTS_Logo.png\" alt=\"FireRedTTS_Logo\" style=\"width: 248px; height: 68px;\">\n    </p>\n    <p>\n    </p>\n    <a href=\"https://arxiv.org/abs/2509.02020\"><img src=\"https://img.shields.io/badge/Paper-ArXiv-red\" alt=\"technical report\"></a>\n    <a href=\"https://fireredteam.github.io/demos/firered_tts_2/\"><img src=\"https://img.shields.io/badge/Demo-Page-lightgrey\" alt=\"version\"></a>\n    <a href=\"https://huggingface.co/FireRedTeam/FireRedTTS2\"><img src=\"https://img.shields.io/badge/Hugging%20Face-Model%20Page-yellow\" alt=\"HF-model\"></a>\n    <a href=\"https://github.com/FireRedTeam/FireRedTTS\"><img src=\"https://img.shields.io/badge/License-Apache%202.0-blue.svg\" alt=\"Apache-2.0\"></a>\n</div>\n\n## Overview\n\nFireRedTTS‚Äë2 is a long-form streaming TTS system for **multi-speaker dialogue generation**, delivering stable, natural speech with reliable speaker switching and context-aware prosody.\n\n## Highlightüî•\n\n- **Long Conversational Speech Generation**: It currently supports 3 minutes dialogues with 4 speakers and can be easily scaled to longer conversations\nwith more speakers by extending training corpus.\n- **Multilingual Support**: It supports multiple languages including English, Chinese, Japanese, Korean, French, German, and Russian. Support zero-shot voice cloning for cross-lingual and code-switching scenarios.\n- **Ultra-Low Latency**: Building on the new **12.5Hz streaming** speech tokenizer, we employ a dual-transformer architecture that operates on a text‚Äìspeech interleaved sequence, enabling flexible sentence-bysentence generation and reducing first-packet latencyÔºåSpecifically, on an L20 GPU, our first-packet latency as low as 140ms while maintaining high-quality audio output.\n- **Strong Stability**ÔºöOur model achieves high similarity and low WER/CER in both monologue and dialogue tests.\n- **Random Timbre Generation**:Useful for creating ASR/speech interaction data.\n\n## Demo Examples\n\n**Random Timbre Generation & Multilingual Support**\n<div align=\"center\">\n\n<https://github.com/user-attachments/assets/804e9e67-fb15-4557-9715-43cd46a1b3e8>\n\n</div>\n\n**Zero-Shot Podcast Generation**\n<div align=\"center\">\n\n<https://github.com/user-attachments/assets/e68b1b7e-1329-47bb-a16f-8589cf227579>\n\n</div>\n\n**Speaker-Specific Finetuned Podcast Generation**\n\n‚ö†Ô∏è Speaker voices: hosts \"ËÇ•Êù∞\" and \"ÊÉ†Â≠ê\" from the podcast \"ËÇ•ËØùËøûÁØá\". Use without authorization is forbidden.\n\n‚ö†Ô∏è Â£∞Èü≥Êù•Ê∫êÔºöÊí≠ÂÆ¢ \"ËÇ•ËØùËøûÁØá\" ‰∏ªÊí≠ \"ËÇ•Êù∞\" Âíå \"ÊÉ†Â≠ê\"ÔºåÊú™ÁªèÊéàÊùÉ‰∏çËÉΩ‰ΩøÁî®„ÄÇ\n<div align=\"center\">\n\n<https://github.com/user-attachments/assets/21f626cb-eaf4-4f5c-920c-3d5d4c8cfa8b>\n\n</div>\n\nFor more examples, see [demo page](https://fireredteam.github.io/demos/firered_tts_2/).\n\n## News\n\n- [2025/09/12] üî• **We have added a UI tool to the dialogue generation.**\n- [2025/09/08] üî• We release the [pre-trained checkpoints](https://huggingface.co/FireRedTeam/FireRedTTS2) and inference code.\n- [2025/09/02] üî• We release the [technical report](https://arxiv.org/abs/2509.02020) and [demo page](https://fireredteam.github.io/demos/firered_tts_2/)\n\n## Roadmap\n\n- [x] 2025/09\n  - [x] Release the pre-trained checkpoints and inference code.\n  - [x] Add web UI tool.\n\n- [ ] 2025/10\n  - [ ] Release a base model with enhanced multilingual support.\n  - [ ] **Provide fine-tuning code & tutorial for specific dialogue/multilingual data.**\n  - [ ] **End-to-end text-to-blog pipeline.**\n\n## Install & Model Download\n\n### Clone and install\n\n- **Clone the repo**\n\n    ``` sh\n    git clone https://github.com/FireRedTeam/FireRedTTS2.git\n    cd FireRedTTS2\n    ```\n\n- **Create Conda env**:\n\n    ``` sh\n    conda create --name fireredtts2 python==3.11\n    conda activate fireredtts2\n\n    # Step 1. PyTorch Installation (if required)\n    pip install torch==2.7.1 torchvision==0.22.1 torchaudio==2.7.1 --index-url https://download.pytorch.org/whl/cu126\n\n    # Step 2. Install Dependencies\n    pip install -e .\n    pip install -r requirements.txt\n    ```\n\n- **Model download**\n\n    ```sh\n    git lfs install\n    git clone https://huggingface.co/FireRedTeam/FireRedTTS2 pretrained_models/FireRedTTS2\n    ```\n\n## Basic Usage\n\n**Dialogue Generation with Web UI**\n\nGenerate dialogue through an easy-to-use web interface that supports both voice cloning and randomized voices.\n\n```sh\npython gradio_demo.py --pretrained-dir \"./pretrained_models/FireRedTTS2\"\n```\n\n<div align=\"center\">\n\n<p>\n<img src=\"assets/gradio.png\" alt=\"FireRedTTS_Logo\" style=\"width: 997px; height: 515px;\">\n</p>\n\n</div>\n\n**Dialogue Generation**\n\n```python\nimport os\nimport sys\nimport torch\nimport torchaudio\nfrom fireredtts2.fireredtts2 import FireRedTTS2\n\ndevice = \"cuda\"\n\nfireredtts2 = FireRedTTS2(\n    pretrained_dir=\"./pretrained_models/FireRedTTS2\",\n    gen_ty",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-28T02:27:31.405133"
  },
  {
    "basic_info": {
      "name": "youtu-graphrag",
      "full_name": "TencentCloudADP/youtu-graphrag",
      "owner": "TencentCloudADP",
      "description": "Official repository of Youtu-GraphRAG: Vertically Unified Agents for Graph Retrieval-Augmented Complex Reasoning",
      "url": "https://github.com/TencentCloudADP/youtu-graphrag",
      "clone_url": "https://github.com/TencentCloudADP/youtu-graphrag.git",
      "ssh_url": "git@github.com:TencentCloudADP/youtu-graphrag.git",
      "homepage": "https://arxiv.org/abs/2508.19855",
      "created_at": "2025-09-01T02:52:13Z",
      "updated_at": "2025-09-28T01:50:26Z",
      "pushed_at": "2025-09-25T09:37:33Z"
    },
    "stats": {
      "stars": 704,
      "forks": 95,
      "watchers": 704,
      "open_issues": 45,
      "size": 38407
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 326863,
        "HTML": 83430,
        "Shell": 2521,
        "Dockerfile": 856
      },
      "license": "Other",
      "topics": [
        "agent",
        "graph",
        "graphrag",
        "llm",
        "rag"
      ]
    },
    "content": {
      "readme": "<div align=\"center\">\n\n# <img src=\"assets/logo.svg\" alt=\"Youtu-agent Logo\" height=\"26px\"> Youtu-GraphRAG: <br>Vertically Unified Agents for Graph Retrieval-Augmented Complex Reasoning\n\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)\n[![Paper](https://img.shields.io/badge/Paper-Latest-blue.svg)](https://arxiv.org/abs/2508.19855)\n[![WeChat Community](https://img.shields.io/badge/Community-WeChat-32CD32)](assets/wechat_qr.png)\n[![Discord Community](https://img.shields.io/badge/Community-Discord-8A2BE2)](https://discord.gg/QjqhkHQVVM)\n<a href=https://deepwiki.com/TencentCloudADP/youtu-graphrag><img src=https://img.shields.io/badge/DeepWiki-Tencent-blue.svg></a>\n[![GitHub stars](https://img.shields.io/github/stars/TencentCloudADP/youtu-graphrag?style=social)](https://github.com/TencentCloudADP/youtu-graphrag)\n\n*üöÄ Revolutionary framework moving Pareto Frontier with 33.6% lower token cost and 16.62% higher accuracy over SOTA baselines*\n\n[üîñ ‰∏≠ÊñáÁâà](README-CN.md) ‚Ä¢ [üîñ Êó•Êú¨Ë™û](README-JA.md) ‚Ä¢ [‚≠ê Contributions](#contributions) ‚Ä¢ [üìä Benchmarks](https://huggingface.co/datasets/Youtu-Graph/AnonyRAG) ‚Ä¢ [üöÄ Getting Started](#quickstart)\n\n</div>\n\n## üéØ Brief Introduction\n**Youtu-GraphRAG** is a vertically unified agentic paradigm that jointly connects the entire framework as an intricate integration based on graph schema. We allow seamless domain transfer with minimal intervention on the graph schema, providing insights of the next evolutionary GraphRAG paradigm for real-world applications with remarkable adaptability.\n\n<img src=\"assets/logo.png\" alt=\"Youtu-GrapHRAG Logo\" width=\"140\" align=\"left\" style=\"margin-right:20px;\">\n\n\n### üé® When and Why to use Youtu-GraphRAG\n\nüîó Multi-hop Reasoning/Summarization/Conclusion: Complex questions requiring multi-step reasoning<br>\nüìö Knowledge-Intensive Tasks: Questions dependent on large amounts of structured/private/domain knowledge<br>\nüåê Domain Scalability: Easily support encyclopedias, academic papers, commercial/private knowledge base and other domains with minimal intervention on the schema<br><br>\n\n\n## üèóÔ∏è Framework Architecture\n\n<div align=\"center\">\n<img src=\"assets/framework.png\" alt=\"Youtu-GraphRAG Framework Architecture\" width=\"95%\"/><br>\nA sketched overview of our proposed framework Youtu-GraphRAG.\n</div>\n\n## üì≤ Interactive interface\nThis [video](https://youtu.be/fVUsgClHqwc) walks through the main features of the project.\n<div align=\"center\">\n<img src=\"assets/graph_demo.png\" alt=\"Graph Construction\" width=\"45.9%\"/>\n<img src=\"assets/retrieval_demo.png\" alt=\"Retrieval\" width=\"49.4%\"/>\n</div>\n\n<!-- <div align=\"center\"> -->\n\n\n\n<!-- </div> -->\n\n\n\n\n<a id=\"contributions\"></a>\n## üöÄ Contributions and Novelty\n\nBased on our unified agentic paradigm for Graph Retrieval-Augmented Generation (GraphRAG), Youtu-GraphRAG introduces several key innovations that jointly connect the entire framework as an intricate integration:\n\n\n<strong>üèóÔ∏è 1. Schema-Guided Hierarchical Knowledge Tree Construction</strong>\n\n- üå± **Seed Graph Schema**: Introduces targeted entity types, relations, and attribute types to bound automatic extraction agents\n- üìà **Scalable Schema Expansion**: Continuously expands schemas for adaptability over unseen domains\n- üè¢ **Four-Level Architecture**: \n  - **Level 1 (Attributes)**: Entity property information\n  - **Level 2 (Relations)**: Entity relationship triples\n  - **Level 3 (Keywords)**: Keyword indexing\n  - **Level 4 (Communities)**: Hierarchical community structure\n- ‚ö° **Quick Adaptation to industrial applications**: We allow seamless domain transfer with minimal intervention on the schema\n\n\n<strong>üå≥ 2. Dually-Perceived Community Detection</strong>\n\n- üî¨ **Novel Community Detection Algorithm**: Fuses structural topology with subgraph semantics for comprehensive knowledge organization\n- üìä **Hierarchical Knowledge Tree**: Naturally yields a structure supporting both top-down filtering and bottom-up reasoning that performs better than traditional Leiden and Louvain algorithms\n- üìù **Community Summaries**: LLM-enhanced community summarization for higher-level knowledge abstraction\n\n<div align=\"center\">\n<img src=\"assets/comm.png\" alt=\"Youtu-GraphRAG Community Detection\" width=\"60%\"/>\n</div>\n\n<strong>ü§ñ 3. Agentic Retrieval</strong>\n\n- üéØ **Schema-Aware Decomposition**: Interprets the same graph schema to transform complex queries into tractable and parallel sub-queries\n- üîÑ **Iterative Reflection**: Performs reflection for more advanced reasoning through IRCoT (Iterative Retrieval Chain of Thought)\n\n<div align=\"center\">\n<img src=\"assets/agent.png\" alt=\"Youtu-GraphRAG Agentic Decomposer\" width=\"50%\"/>\n</div>\n\n<strong>üß† 4. Advanced Construction and Reasoning Capabilities for real-world deployment</strong>\n\n- üéØ **Performance Enhancement**: Less token costs and higher accuracy with optimized prompting, indexing and retrieval strategies\n- ü§π‚Äç‚ôÄÔ∏è **User friendly visualization**: In ```output/graphs/```, the four-level knowledge tree supports visualization with neo4j impor",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-28T02:27:32.639248"
  },
  {
    "basic_info": {
      "name": "MiMo-Audio",
      "full_name": "XiaomiMiMo/MiMo-Audio",
      "owner": "XiaomiMiMo",
      "description": "MiMo-Audio: Audio Language Models are Few-Shot Learners",
      "url": "https://github.com/XiaomiMiMo/MiMo-Audio",
      "clone_url": "https://github.com/XiaomiMiMo/MiMo-Audio.git",
      "ssh_url": "git@github.com:XiaomiMiMo/MiMo-Audio.git",
      "homepage": "https://xiaomimimo.github.io/MiMo-Audio-Demo/",
      "created_at": "2025-09-19T00:46:49Z",
      "updated_at": "2025-09-28T01:59:00Z",
      "pushed_at": "2025-09-20T19:03:26Z"
    },
    "stats": {
      "stars": 692,
      "forks": 62,
      "watchers": 692,
      "open_issues": 24,
      "size": 6029
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 223792
      },
      "license": "Apache License 2.0",
      "topics": []
    },
    "content": {
      "readme": "<div align=\"center\">\n  <picture>\n    <source srcset=\"https://github.com/XiaomiMiMo/MiMo-VL/raw/main/figures/Xiaomi_MiMo_darkmode.png?raw=true\" media=\"(prefers-color-scheme: dark)\">\n    <img src=\"https://github.com/XiaomiMiMo/MiMo-VL/raw/main/figures/Xiaomi_MiMo.png?raw=true\" width=\"60%\" alt=\"Xiaomi-MiMo\" />\n  </picture>\n</div>\n\n<h3 align=\"center\">\n  <b>\n    <span>‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ</span>\n    <br/>\n    MiMo Audio: Audio Language Models are Few-Shot Learners\n    <br/>\n    <span>‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ</span>\n    <br/>\n  </b>\n</h3>\n\n<br/>\n\n<div align=\"center\" style=\"line-height: 1;\">\n  |\n  <a href=\"https://huggingface.co/collections/XiaomiMiMo/mimo-audio-68cc7202692c27dae881cce0\" target=\"_blank\">ü§ó HuggingFace</a>\n  &nbsp;|\n  <a href=\"https://github.com/XiaomiMiMo/MiMo-Audio/blob/main/MiMo-Audio-Technical-Report.pdf\" target=\"_blank\">üìÑ Paper</a>\n  &nbsp;|\n  <a href=\"https://xiaomimimo.github.io/MiMo-Audio-Demo\" target=\"_blank\">üì∞ Blog</a>\n  &nbsp;|\n  <a href=\"https://huggingface.co/spaces/XiaomiMiMo/mimo_audio_chat\" target=\"_blank\">üî• Online Demo</a>\n  &nbsp;|\n  <a href=\"https://github.com/XiaomiMiMo/MiMo-Audio-Eval\" target=\"_blank\">üìä MiMo-Audio-Eval</a>\n  &nbsp;|\n\n  <br/>\n</div>\n\n<br/>\n\n## Introduction\n\nExisting audio language models typically rely on task-specific fine-tuning to accomplish particular audio tasks. In contrast, humans are able to generalize to new audio tasks with only a few examples or simple instructions. GPT-3 has shown that scaling next-token prediction pretraining enables strong generalization capabilities in text, and we believe this paradigm is equally applicable to the audio domain. By scaling MiMo-Audio's pretraining data to over one hundred million of hours, we observe the emergence of few-shot learning capabilities across a diverse set of audio tasks. We develop a systematic evaluation of these capabilities and find that MiMo-Audio-7B-Base achieves SOTA performance on both speech intelligence and audio understanding benchmarks among open-source models. Beyond standard metrics, MiMo-Audio-7B-Base generalizes to tasks absent from its training data, such as voice conversion, style transfer, and speech editing. MiMo-Audio-7B-Base also demonstrates powerful speech continuation capabilities, capable of generating highly realistic talk shows, recitations, livestreaming and debates. At the post-training stage, we curate a diverse instruction-tuning corpus and introduce thinking mechanisms into both audio understanding and generation. MiMo-Audio-7B-Instruct achieves open-source SOTA on audio understanding benchmarks, spoken dialogue benchmarks and instruct-TTS evaluations, approaching or surpassing closed-source models.\n\n\n![Results](assets/Results.png)\n\n\n\n## Architecture\n### MiMo-Audio-Tokenizer\nMiMo-Audio-Tokenizer is a 1.2B-parameter Transformer operating at 25 Hz. It employs an eight-layer RVQ stack to generate 200 tokens per second. By jointly optimizing semantic and reconstruction objectives, we train MiMo-Audio-Tokenizer from scratch on a 10-million-hour corpus, achieving superior reconstruction quality and facilitating downstream language modeling.\n\n![Tokenizer](assets/tokenizer.png)\n\nMiMo-Audio couples a patch encoder, an LLM, and a patch decoder to improve modeling efficiency for high-rate sequences and bridge the length mismatch between speech and text. The patch encoder aggregates four consecutive time steps of RVQ tokens into a single patch, downsampling the sequence to a 6.25 Hz representation for the LLM. The patch decoder autoregressively generates the full 25 Hz RVQ token sequence via a delayed-generation scheme.\n### MiMo-Audio\n![Arch](assets/architecture.png)\n\n##  Explore MiMo-Audio Now! üöÄüöÄüöÄ\n- üéß **Try the Hugging Face demo:** [MiMo-Audio Demo](https://huggingface.co/spaces/XiaomiMiMo/mimo_audio_chat)\n- üì∞ **Read the Official Blog:** [MiMo-Audio Blog](https://xiaomimimo.github.io/MiMo-Audio-Demo)\n- üìÑ **Dive into the Technical Report:** [MiMo-Audio Technical Report](https://github.com/XiaomiMiMo/MiMo-Audio/blob/main/MiMo-Audio-Technical-Report.pdf)\n\n\n## Model Download\n| Models   | ü§ó Hugging Face |\n|-------|-------|\n| MiMo-Audio-Tokenizer | [XiaomiMiMo/MiMo-Audio-Tokenizer](https://huggingface.co/XiaomiMiMo/MiMo-Audio-Tokenizer) |\n| MiMo-Audio-7B-Base | [XiaomiMiMo/MiMo-Audio-7B-Base](https://huggingface.co/XiaomiMiMo/MiMo-Audio-7B-Base) |\n| MiMo-Audio-7B-Instruct | [XiaomiMiMo/MiMo-Audio-7B-Instruct](https://huggingface.co/XiaomiMiMo/MiMo-Audio-7B-Instruct) |\n\n\n```bash\npip install huggingface-hub\n\nhf download XiaomiMiMo/MiMo-Audio-Tokenizer --local-dir ./models/MiMo-Audio-Tokenizer\nhf download XiaomiMiMo/MiMo-Audio-7B-Base --local-dir ./models/MiMo-Audio-7B-Base\nhf download XiaomiMiMo/MiMo-Audio-7B-Instruct --local-dir ./models/MiMo-Audio-7B-Instruct\n```\n\n## Getting Started\n\nSpin up the MiMo-Audio demo in minutes with the built-in Gradio app.\n\n### Prerequisites (Linux)\n\n* Python 3.12\n* CUDA >= 12.0\n\n### Installation\n\n```bash\ngit clone https",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-28T02:27:33.894314"
  },
  {
    "basic_info": {
      "name": "HuMo",
      "full_name": "Phantom-video/HuMo",
      "owner": "Phantom-video",
      "description": "HuMo: Human-Centric Video Generation via Collaborative Multi-Modal Conditioning",
      "url": "https://github.com/Phantom-video/HuMo",
      "clone_url": "https://github.com/Phantom-video/HuMo.git",
      "ssh_url": "git@github.com:Phantom-video/HuMo.git",
      "homepage": "https://phantom-video.github.io/HuMo/",
      "created_at": "2025-09-09T06:18:13Z",
      "updated_at": "2025-09-27T21:02:20Z",
      "pushed_at": "2025-09-23T13:30:47Z"
    },
    "stats": {
      "stars": 624,
      "forks": 54,
      "watchers": 624,
      "open_issues": 7,
      "size": 101093
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 315221,
        "Shell": 4876
      },
      "license": "Apache License 2.0",
      "topics": []
    },
    "content": {
      "readme": "<div align=\"center\">\n<h1> HuMo: Human-Centric Video Generation via Collaborative Multi-Modal Conditioning </h1>\n\n<a href=\"https://arxiv.org/abs/2509.08519\"><img src=\"https://img.shields.io/badge/arXiv%20paper-2509.08519-b31b1b.svg\"></a>\n<a href=\"https://phantom-video.github.io/HuMo/\"><img src=\"https://img.shields.io/badge/Project_page-More_visualizations-green\"></a>\n<a href=\"https://huggingface.co/bytedance-research/HuMo\"><img src=\"https://img.shields.io/static/v1?label=%F0%9F%A4%97%20Hugging%20Face&message=Model&color=orange\"></a>\n\n[Liyang Chen](https://scholar.google.com/citations?user=jk6jWXgAAAAJ&hl)<sup> * </sup>, [Tianxiang Ma](https://tianxiangma.github.io/)<sup> * </sup>, [Jiawei Liu](https://scholar.google.com/citations?user=X21Fz-EAAAAJ), [Bingchuan Li](https://scholar.google.com/citations?user=ac5Se6QAAAAJ)<sup> &dagger; </sup>, <br>[Zhuowei Chen](https://scholar.google.com/citations?user=ow1jGJkAAAAJ), [Lijie Liu](https://liulj13.github.io/), [Xu He](https://scholar.google.com/citations?user=KMrFk2MAAAAJ&hl), [Gen Li](https://scholar.google.com/citations?user=wqA7EIoAAAAJ), [Qian He](https://scholar.google.com/citations?user=9rWWCgUAAAAJ), [Zhiyong Wu](https://scholar.google.com/citations?user=7Xl6KdkAAAAJ)<sup> ¬ß </sup><br>\n<sup> * </sup>Equal contribution, <sup> &dagger; </sup>Project lead, <sup> ¬ß </sup>Corresponding author  \nTsinghua University | Intelligent Creation Team, ByteDance\n\n</div>\n\n<p align=\"center\">\n<img src=\"assets/teaser.png\" width=95%>\n<p>\n\n## üî• Latest News\n\n* A Best-Practice Guide for HuMo will be released soon. Stay tuned.\n* Sep 17, 2025: üî•üî• [ComfyUI](https://github.com/comfyanonymous/ComfyUI/pull/9903) officially supports HuMo-1.7B!\n* Sep 16, 2025: üî•üî• We release the [1.7B weights](https://huggingface.co/bytedance-research/HuMo/tree/main/HuMo-1.7B), which generate a 480P video in 8 minutes on a 32G GPU. The visual quality is lower than that of the 17B model, but the audio-visual sync remains nearly unaffected.\n* Sep 13, 2025: üî•üî• The 17B model is merged into [ComfyUI-Wan](https://github.com/kijai/ComfyUI-WanVideoWrapper), which can be run on a NVIDIA 3090 GPU. Thank [kijai](https://github.com/kijai) for the update!\n* Sep 10, 2025: üî•üî• We release the [17B weights](https://huggingface.co/bytedance-research/HuMo/tree/main/HuMo-17B) and inference codes.\n* Sep 9, 2025: We release the [Project Page](https://phantom-video.github.io/HuMo/) and [Technique Report](https://arxiv.org/abs/2509.08519/) of **HuMo**.\n\n\n## ‚ú® Key Features\nHuMo is a unified, human-centric video generation framework designed to produce high-quality, fine-grained, and controllable human videos from multimodal inputs‚Äîincluding text, images, and audio. It supports strong text prompt following, consistent subject preservation, synchronized audio-driven motion.\n\n> - **‚Äã‚ÄãVideoGen from Text-Image**‚Äã‚Äã - Customize character appearance, clothing, makeup, props, and scenes using text prompts combined with reference images.\n> - **‚Äã‚ÄãVideoGen from Text-Audio**‚Äã‚Äã - Generate audio-synchronized videos solely from text and audio inputs, removing the need for image references and enabling greater creative freedom.\n> - **‚Äã‚ÄãVideoGen from Text-Image-Audio**‚Äã‚Äã - Achieve the higher level of customization and control by combining text, image, and audio guidance.\n\n## üìë Todo List\n- [x] Release Paper\n- [x] Checkpoint of HuMo-17B\n- [x] Checkpoint of HuMo-1.7B\n- [x] Inference Codes\n  - [ ] Text-Image Input\n  - [x] Text-Audio Input\n  - [x] Text-Image-Audio Input\n- [x] Multi-GPU Inference\n- [ ] Best-Practice Guide of HuMo for Movie-Level Generation\n- [ ] Checkpoint for Longer Generation\n- [ ] Prompts to Generate Demo of ***Faceless Thrones***\n- [ ] Training Data\n\n## ‚ö°Ô∏è Quickstart\n\n### Installation\n```\nconda create -n humo python=3.11\nconda activate humo\npip install torch==2.5.1 torchvision==0.20.1 torchaudio==2.5.1 --index-url https://download.pytorch.org/whl/cu124\npip install flash_attn==2.6.3\npip install -r requirements.txt\nconda install -c conda-forge ffmpeg\n```\n\n### Model Preparation\n| Models       | Download Link                                                                                                                                           |    Notes                      |\n|--------------|---------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------|\n| HuMo-17B      | ü§ó [Huggingface](https://huggingface.co/bytedance-research/HuMo/tree/main/HuMo-17B)   | Supports 480P & 720P \n| HuMo-1.7B | ü§ó [Huggingface](https://huggingface.co/bytedance-research/HuMo/tree/main/HuMo-1.7B) | Lightweight on 32G GPU\n| HuMo-Longer | ü§ó [Huggingface](https://huggingface.co/bytedance-research/HuMo) | Longer generation to be released in Oct.\n| Wan-2.1 | ü§ó [Huggingface](https://huggingface.co/Wan-AI/Wan2.1-T2V-1.3B) | VAE & Text encoder\n| Whisper-large-v3 |      ü§ó [Huggingface](https://huggingface.co/openai/whisper-large-v3)          | ",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-28T02:27:35.105298"
  },
  {
    "basic_info": {
      "name": "HunyuanImage-2.1",
      "full_name": "Tencent-Hunyuan/HunyuanImage-2.1",
      "owner": "Tencent-Hunyuan",
      "description": "HunyuanImage-2.1: An Efficient Diffusion Model for High-Resolution (2K) Text-to-Image Generation‚Äã",
      "url": "https://github.com/Tencent-Hunyuan/HunyuanImage-2.1",
      "clone_url": "https://github.com/Tencent-Hunyuan/HunyuanImage-2.1.git",
      "ssh_url": "git@github.com:Tencent-Hunyuan/HunyuanImage-2.1.git",
      "homepage": "https://hunyuan.tencent.com/image/en?tabIndex=0",
      "created_at": "2025-09-04T06:46:19Z",
      "updated_at": "2025-09-27T21:00:37Z",
      "pushed_at": "2025-09-23T14:08:39Z"
    },
    "stats": {
      "stars": 615,
      "forks": 45,
      "watchers": 615,
      "open_issues": 11,
      "size": 23171
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 250779
      },
      "license": "Other",
      "topics": [
        "aigc",
        "diffusion-models",
        "diffusion-transformer",
        "image-generation",
        "text-to-image"
      ]
    },
    "content": {
      "readme": "\n[‰∏≠ÊñáÈòÖËØª](./README_CN.md)\n\n<p align=\"center\">\n  <img src=\"./assets/logo.png\"  height=100>\n</p>\n\n<div align=\"center\">\n\n# HunyuanImage-2.1: An Efficient Diffusion Model for High-Resolution (2K) Text-to-Image Generation‚Äã\n\n</div>\n\n\n<p align=\"center\"> &nbsp&nbspü§ó <a href=\"https://huggingface.co/tencent/HunyuanImage-2.1\">HuggingFace</a>&nbsp&nbsp | \nüíª <a href=\"https://hunyuan.tencent.com/modelSquare/home/play?modelId=286&from=/visual\">Official website(ÂÆòÁΩë) Try our model!</a>&nbsp&nbsp\n</p>\n\n\n\n<p align=\"center\">\n    üëè Join our <a href=\"assets/WECHAT.md\" target=\"_blank\">WeChat</a> and <a href=\"https://discord.gg/ehjWMqF5wY\">Discord</a>\n</p>\n\n\n-----\n\nThis repo contains PyTorch model definitions, pretrained weights and inference/sampling code for our HunyuanImage-2.1. You can <span style=\"color:red\">**directly try our model**</span> on [Official website(ÂÆòÁΩë)](https://hunyuan.tencent.com/modelSquare/home/play?modelId=286&from=/visual) and find more visualizations on our [project page](https://hunyuan.tencent.com/image/en?tabIndex=0).\n\n\n<div align=\"center\">\n  <img src=\"./assets/demo.jpg\" width=100% alt=\"HunyuanImage 2.1 Demo\">\n</div>\n\n## üî•üî•üî• Latest Updates\n- September 18, 2025: ‚ú® Try the [PromptEnhancer-32B model](https://huggingface.co/PromptEnhancer/PromptEnhancer-32B) for higher-quality prompt enhancement!‚Äã.\n- September 18, 2025: ‚ú® [ComfyUI workflow of HunyuanImage-2.1](https://github.com/KimbingNg/ComfyUI-HunyuanImage2.1) is available now!\n- September 16, 2025: üëë We achieved the Top1 on Arena's leaderboard for text-to-image open-source models. [Leaderboard](https://artificialanalysis.ai/text-to-image/arena/leaderboard-text)\n- September 12, 2025: üöÄ Released FP8 quantized models! Making it possible to generate 2K images with only 24GB GPU memory!\n- September 8, 2025: üöÄ Released inference code and model weights for HunyuanImage-2.1.\n\n## Introduction\nWe are excited to introduce **HunyuanImage-2.1**, a 17B text-to-image model that is capable of generating **2K (2048 √ó 2048) resolution** images. \n\n<!-- Leveraging an extensive dataset and structured captions involving multiple expert models, we significantly enhance text-image alignment capabilities. The model employs a highly expressive VAE with a (32 √ó 32) spatial compression ratio, substantially reducing computational costs. -->\n\nOur architecture consists of two stages:\n1. **‚ÄãBase text-to-image Model**:‚Äã‚Äã The first stage is a text-to-image model that utilizes two text encoders: a multimodal large language model (MLLM) to improve image-text alignment, and a multi-language, character-aware encoder to enhance text rendering across various languages. \n2. **Refiner Model**: The second stage introduces a refiner model that further enhances image quality and clarity, while minimizing artifacts. \n<!-- \nAdditionally, we developed the PromptEnhancer module to further boost model performance, and employed meanflow distillation for efficient inference. HunyuanImage-2.1 demonstrates robust semantic alignment and cross-scenario generalization, leading to improved consistency between text and image, enhanced control of scene details, character poses, and expressions, and the ability to generate multiple objects with distinct descriptions. -->\n\nüëë We achieved the **Top1** on Arena's leaderboard for text-to-image open-source models.\n\n<div align=\"center\">\n  <img src=\"./assets/leaderboard.png\" width=70% alt=\"HunyuanImage 2.1 Demo\">\n</div>\n \n\n## üéâ HunyuanImage-2.1 Key Features\n\n- **High-Quality Generation**: Efficiently produces ultra-high-definition (2K) images with cinematic composition.\n- **Multilingual Support**: Provides native support for both Chinese and English prompts.\n- **Advanced Architecture**: Built on a multi-modal, single- and dual-stream combined DiT (Diffusion Transformer) backbone.\n- **Glyph-Aware Processing**: Utilizes ByT5's text rendering capabilities for improved text generation accuracy.\n- **Flexible Aspect Ratios**: Supports a variety of image aspect ratios (1:1, 16:9, 9:16, 4:3, 3:4, 3:2, 2:3).\n- **Prompt Enhancement**: Automatically rewrites prompts to improve descriptive accuracy and visual quality.\n\n\n\n\n\n\n## üìú System Requirements\n\n**Hardware and OS Requirements:**\n- NVIDIA GPU with CUDA support.\n\n  **Minimum requrement for now:** 24 GB GPU memory for 2048x2048 image generation.\n  \n  > **Note:** The memory requirements above are measured with model CPU offloading and FP8 quantization enabled. If your GPU has sufficient memory, you may disable offloading for improved inference speed.\n- Supported operating system: Linux.\n\n\n## üõ†Ô∏è Dependencies and Installation\n\n1. Clone the repository:\n```bash\ngit clone https://github.com/Tencent-Hunyuan/HunyuanImage-2.1.git\ncd HunyuanImage-2.1\n```\n\n2. Install dependencies:\n```bash\npip install -r requirements.txt\npip install flash-attn==2.7.3 --no-build-isolation\n```\n\n## üß± Download Pretrained Models\n\nThe details of download pretrained models are shown [here](ckpts/checkpoints-download.md).\n\n## üîë Usage\n\n### Prompt Enhancement\n\nPrompt enhan",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-28T02:27:36.320224"
  },
  {
    "basic_info": {
      "name": "ml-simplefold",
      "full_name": "apple/ml-simplefold",
      "owner": "apple",
      "description": null,
      "url": "https://github.com/apple/ml-simplefold",
      "clone_url": "https://github.com/apple/ml-simplefold.git",
      "ssh_url": "git@github.com:apple/ml-simplefold.git",
      "homepage": null,
      "created_at": "2025-09-23T03:08:49Z",
      "updated_at": "2025-09-28T02:23:27Z",
      "pushed_at": "2025-09-27T04:47:33Z"
    },
    "stats": {
      "stars": 602,
      "forks": 25,
      "watchers": 602,
      "open_issues": 9,
      "size": 1251
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 662611,
        "Jupyter Notebook": 11558
      },
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "\n<h1 align=\"center\"><strong>SimpleFold: Folding Proteins is Simpler than You Think</strong></h1>\n\n\n<div align=\"center\">\n\nThis github repository accompanies the research paper, [*SimpleFold: Folding Proteins is Simpler than You Think*](https://arxiv.org/abs/2509.18480) (Arxiv 2025).\n\n*Yuyang Wang, Jiarui Lu, Navdeep Jaitly, Joshua M. Susskind, Miguel Angel Bautista*\n\n[[`Paper`](https://arxiv.org/abs/2509.18480)]  [[`BibTex`](#citation)]\n\n<img src=\"assets/intro.png\" width=\"750\">\n\n</div>\n\n\n## Introduction\n\nWe introduce SimpleFold, the first flow-matching based protein folding model that solely uses general purpose transformer layers. SimpleFold does not rely on expensive modules like triangle attention or pair representation biases, and is trained via a generative flow-matching objective. We scale SimpleFold to 3B parameters and train it on more than 8.6M distilled protein structures together with experimental PDB data. To the best of our knowledge, SimpleFold is the largest scale folding model ever developed. On standard folding benchmarks, SimpleFold-3B model achieves competitive performance compared to state-of-the-art baselines. Due to its generative training objective, SimpleFold also demonstrates strong performance in ensemble prediction. SimpleFold challenges the reliance on complex domain-specific architectures designs in folding, highlighting an alternative yet important avenue of progress in protein structure prediction.\n\n</div>\n\n\n## Installation\n\nTo install `simplefold` package from github repository, run\n```\ngit clone https://github.com/apple/ml-simplefold.git\ncd ml-simplefold\nconda create -n simplefold python=3.10\npython -m pip install -U pip build; pip install -e .\n```\nIf you want to use MLX backend on Apple silicon: \n```\npip install mlx==0.28.0\npip install git+https://github.com/facebookresearch/esm.git\n```\n\n## Example \n\nWe provide a jupyter notebook [`sample.ipynb`](sample.ipynb) to predict protein structures from example protein sequences. \n\n## Inference\n\nOnce you have `simplefold` package installed, you can predict the protein structure from target fasta file(s) via the following command line. We provide support for both [PyTorch](https://pytorch.org/) and [MLX](https://mlx-framework.org/) (recommended for Apple hardware) backends in inference. \n```\nsimplefold \\\n    --simplefold_model simplefold_100M \\  # specify folding model in simplefold_100M/360M/700M/1.1B/1.6B/3B\n    --num_steps 500 --tau 0.01 \\        # specify inference setting\n    --nsample_per_protein 1 \\           # number of generated conformers per target\n    --plddt \\                           # output pLDDT\n    --fasta_path [FASTA_PATH] \\         # path to the target fasta directory or file\n    --output_dir [OUTPUT_DIR] \\         # path to the output directory\n    --backend [mlx, torch]              # choose from MLX and PyTorch for inference backend \n```\n\n## Evaluation\n\nWe provide predicted structures from SimpleFold of different model sizes:\n```\nhttps://ml-site.cdn-apple.com/models/simplefold/cameo22_predictions.zip # predicted structures of CAMEO22\nhttps://ml-site.cdn-apple.com/models/simplefold/casp14_predictions.zip  # predicted structures of CASP14\nhttps://ml-site.cdn-apple.com/models/simplefold/apo_predictions.zip     # predicted structures of Apo\nhttps://ml-site.cdn-apple.com/models/simplefold/codnas_predictions.zip  # predicted structures of Fold-switch (CoDNaS)\n```\nWe use the docker image of [openstructure](https://git.scicore.unibas.ch/schwede/openstructure/) 2.9.1 to evaluate generated structures for folding tasks (i.e., CASP14/CAMEO22). Once having the docker image enabled, you can run evaluation via:\n```\npython src/simplefold/evaluation/analyze_folding.py \\\n    --data_dir [PATH_TO_TARGET_MMCIF] \\\n    --sample_dir [PATH_TO_PREDICTED_MMCIF] \\\n    --out_dir [PATH_TO_OUTPUT] \\\n    --max-workers [NUMBER_OF_WORKERS]\n```\nTo evaluate results of two-state prediction (i.e., Apo/CoDNaS), one need to compile the [TMsore](https://zhanggroup.org/TM-score/TMscore.cpp) and then run evaluation via:\n```\npython src/simplefold/evaluation/analyze_two_state.py \\ \n    --data_dir [PATH_TO_TARGET_DATA_DIRECTORY] \\\n    --sample_dir [PATH_TO_PREDICTED_PDB] \\\n    --tm_bin [PATH_TO_TMscore_BINARY] \\\n    --task apo \\ # choose from apo and codnas\n    --nsample 5\n```\n\n## Train\n\nYou can also train or tune SimpleFold on your end. Instructions below include details for SimpleFold training. \n\n### Data preparation\n\n#### Training targets\n\nSimpleFold is trained on joint datasets including experimental structures from [PDB](https://www.rcsb.org/), as well as distilled predictions from [AFDB SwissProt](https://alphafold.ebi.ac.uk/download#swissprot-section) and [AFESM](https://afesm.foldseek.com/). Target lists of filtered SwissProt and AFESM targets thta are used in our training can be found:\n```\nhttps://ml-site.cdn-apple.com/models/simplefold/swissprot_list.csv # list of filted SwissProt (~270K targets)\nhttps://ml-site.cdn-apple.com/models/simplefold/af",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-28T02:27:37.569188"
  },
  {
    "basic_info": {
      "name": "DeepMCPAgent",
      "full_name": "cryxnet/DeepMCPAgent",
      "owner": "cryxnet",
      "description": "Model-agnostic plug-n-play LangChain/LangGraph agents powered entirely by MCP tools over HTTP/SSE.",
      "url": "https://github.com/cryxnet/DeepMCPAgent",
      "clone_url": "https://github.com/cryxnet/DeepMCPAgent.git",
      "ssh_url": "git@github.com:cryxnet/DeepMCPAgent.git",
      "homepage": "https://cryxnet.github.io/DeepMCPAgent/",
      "created_at": "2025-08-29T13:49:43Z",
      "updated_at": "2025-09-28T01:07:15Z",
      "pushed_at": "2025-09-08T18:15:58Z"
    },
    "stats": {
      "stars": 591,
      "forks": 89,
      "watchers": 591,
      "open_issues": 1,
      "size": 179
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 29117
      },
      "license": "Apache License 2.0",
      "topics": [
        "agent-framework",
        "agentic-ai",
        "agents",
        "ai",
        "ai-agents",
        "ai-framework",
        "artificial-intelligence",
        "autonomous-agents",
        "deep-agents",
        "developer-tools",
        "langchain",
        "langgraph",
        "llm-agents",
        "mcp",
        "opensource-agents",
        "python",
        "react-agents"
      ]
    },
    "content": {
      "readme": "<!-- Banner / Title -->\n<div align=\"center\">\n  <img src=\"docs/images/icon.png\" width=\"120\" alt=\"DeepMCPAgent Logo\"/>\n\n  <h1>ü§ñ DeepMCPAgent</h1>\n  <p><strong>Model-agnostic LangChain/LangGraph agents powered entirely by <a href=\"https://modelcontextprotocol.io/\">MCP</a> tools over HTTP/SSE.</strong></p>\n\n  <!-- Badges -->\n  <p>\n    <a href=\"https://cryxnet.github.io/DeepMCPAgent\">\n      <img alt=\"Docs\" src=\"https://img.shields.io/badge/docs-latest-brightgreen.svg\">\n    </a>\n    <a href=\"#\"><img alt=\"Python\" src=\"https://img.shields.io/badge/Python-3.10%2B-blue.svg\"></a>\n    <a href=\"#\"><img alt=\"License\" src=\"https://img.shields.io/badge/License-Apache%202.0-blue.svg\"></a>\n    <a href=\"#\"><img alt=\"Status\" src=\"https://img.shields.io/badge/status-beta-orange.svg\"></a>\n\n<p>\n  <a href=\"https://www.producthunt.com/products/deep-mcp-agents?utm_source=badge-featured&utm_medium=badge&utm_source=badge-deep-mcp-agents\" target=\"_blank\">\n    <img src=\"https://api.producthunt.com/widgets/embed-image/v1/featured.svg?post_id=1011071&theme=light\" alt=\"Deep MCP Agents on Product Hunt\" style=\"width: 250px; height: 54px;\" width=\"250\" height=\"54\" />\n  </a>\n</p> \n  </p>\n\n  <p>\n    <em>Discover MCP tools dynamically. Bring your own LangChain model. Build production-ready agents‚Äîfast.</em>\n  </p>\n\n  <p>\n    üìö <a href=\"https://cryxnet.github.io/deepmcpagent/\">Documentation</a> ‚Ä¢ üõ† <a href=\"https://github.com/cryxnet/deepmcpagent/issues\">Issues</a>\n  </p>\n</div>\n\n<hr/>\n\n## ‚ú® Why DeepMCPAgent?\n\n- üîå **Zero manual tool wiring** ‚Äî tools are discovered dynamically from MCP servers (HTTP/SSE)\n- üåê **External APIs welcome** ‚Äî connect to remote MCP servers (with headers/auth)\n- üß† **Model-agnostic** ‚Äî pass any LangChain chat model instance (OpenAI, Anthropic, Ollama, Groq, local, ‚Ä¶)\n- ‚ö° **DeepAgents (optional)** ‚Äî if installed, you get a deep agent loop; otherwise robust LangGraph ReAct fallback\n- üõ†Ô∏è **Typed tool args** ‚Äî JSON-Schema ‚Üí Pydantic ‚Üí LangChain `BaseTool` (typed, validated calls)\n- üß™ **Quality bar** ‚Äî mypy (strict), ruff, pytest, GitHub Actions, docs\n\n> **MCP first.** Agents shouldn‚Äôt hardcode tools ‚Äî they should **discover** and **call** them. DeepMCPAgent builds that bridge.\n\n---\n\n## üöÄ Installation\n\nInstall from [PyPI](https://pypi.org/project/deepmcpagent/):\n\n```bash\npip install \"deepmcpagent[deep]\"\n```\n\nThis installs DeepMCPAgent with **DeepAgents support (recommended)** for the best agent loop.\nOther optional extras:\n\n- `dev` ‚Üí linting, typing, tests\n- `docs` ‚Üí MkDocs + Material + mkdocstrings\n- `examples` ‚Üí dependencies used by bundled examples\n\n```bash\n# install with deepagents + dev tooling\npip install \"deepmcpagent[deep,dev]\"\n```\n\n‚ö†Ô∏è If you‚Äôre using **zsh**, remember to quote extras:\n\n```bash\npip install \"deepmcpagent[deep,dev]\"\n```\n\n---\n\n## üöÄ Quickstart\n\n### 1) Start a sample MCP server (HTTP)\n\n```bash\npython examples/servers/math_server.py\n```\n\nThis serves an MCP endpoint at: **[http://127.0.0.1:8000/mcp](http://127.0.0.1:8000/mcp)**\n\n### 2) Run the example agent (with fancy console output)\n\n```bash\npython examples/use_agent.py\n```\n\n**What you‚Äôll see:**\n\n![screenshot](/docs/images/screenshot_output.png)\n\n---\n\n## üßë‚Äçüíª Bring-Your-Own Model (BYOM)\n\nDeepMCPAgent lets you pass **any LangChain chat model instance** (or a provider id string if you prefer `init_chat_model`):\n\n```python\nimport asyncio\nfrom deepmcpagent import HTTPServerSpec, build_deep_agent\n\n# choose your model:\n# from langchain_openai import ChatOpenAI\n# model = ChatOpenAI(model=\"gpt-4.1\")\n\n# from langchain_anthropic import ChatAnthropic\n# model = ChatAnthropic(model=\"claude-3-5-sonnet-latest\")\n\n# from langchain_community.chat_models import ChatOllama\n# model = ChatOllama(model=\"llama3.1\")\n\nasync def main():\n    servers = {\n        \"math\": HTTPServerSpec(\n            url=\"http://127.0.0.1:8000/mcp\",\n            transport=\"http\",    # or \"sse\"\n            # headers={\"Authorization\": \"Bearer <token>\"},\n        ),\n    }\n\n    graph, _ = await build_deep_agent(\n        servers=servers,\n        model=model,\n        instructions=\"Use MCP tools precisely.\"\n    )\n\n    out = await graph.ainvoke({\"messages\":[{\"role\":\"user\",\"content\":\"add 21 and 21 with tools\"}]})\n    print(out)\n\nasyncio.run(main())\n```\n\n> Tip: If you pass a **string** like `\"openai:gpt-4.1\"`, we‚Äôll call LangChain‚Äôs `init_chat_model()` for you (and it will read env vars like `OPENAI_API_KEY`). Passing a **model instance** gives you full control.\n\n---\n\n## üñ•Ô∏è CLI (no Python required)\n\n```bash\n# list tools from one or more HTTP servers\ndeepmcpagent list-tools \\\n  --http name=math url=http://127.0.0.1:8000/mcp transport=http \\\n  --model-id \"openai:gpt-4.1\"\n\n# interactive agent chat (HTTP/SSE servers only)\ndeepmcpagent run \\\n  --http name=math url=http://127.0.0.1:8000/mcp transport=http \\\n  --model-id \"openai:gpt-4.1\"\n```\n\n> The CLI accepts **repeated** `--http` blocks; add `header.X=Y` pairs for auth:\n>\n> ```\n> --http name=ext url=https://api.example.com/mcp transport=http header.Authorization=\"Bearer TOKEN",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-28T02:27:38.790168"
  },
  {
    "basic_info": {
      "name": "Qwen3-ASR-Toolkit",
      "full_name": "QwenLM/Qwen3-ASR-Toolkit",
      "owner": "QwenLM",
      "description": "Official Python toolkit for the Qwen3-ASR API. Parallel high‚Äëthroughput calls, robust long‚Äëaudio transcription, multi‚Äësample‚Äërate support.",
      "url": "https://github.com/QwenLM/Qwen3-ASR-Toolkit",
      "clone_url": "https://github.com/QwenLM/Qwen3-ASR-Toolkit.git",
      "ssh_url": "git@github.com:QwenLM/Qwen3-ASR-Toolkit.git",
      "homepage": "",
      "created_at": "2025-09-16T09:03:49Z",
      "updated_at": "2025-09-28T02:02:20Z",
      "pushed_at": "2025-09-22T10:38:51Z"
    },
    "stats": {
      "stars": 569,
      "forks": 49,
      "watchers": 569,
      "open_issues": 4,
      "size": 15
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 17592
      },
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "# Qwen3-ASR-Toolkit\n\n[![PyPI version](https://badge.fury.io/py/qwen3-asr-toolkit.svg)](https://badge.fury.io/py/qwen3-asr-toolkit)\n[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://www.python.org/downloads/)\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n\nAn advanced, high-performance Python command-line toolkit for using the **Qwen-ASR API** (formerly Qwen3-ASR-Flash). This implementation overcomes the API's 3-minute audio length limitation by intelligently splitting long audio/video files and processing them in parallel, enabling rapid transcription of hours-long content.\n\n## üöÄ Key Features\n\n-   **Break the 3-Minute Limit**: Seamlessly transcribe audio and video files of any length by bypassing the official API's duration constraint.\n-   **Smart Audio Splitting**: Utilizes **Voice Activity Detection (VAD)** to split audio into meaningful chunks at natural silent pauses. This ensures that words and sentences are not awkwardly cut off.\n-   **High-Speed Parallel Processing**: Leverages multi-threading to send audio chunks to the Qwen-ASR API concurrently, dramatically reducing the total transcription time for long files.\n-   **Intelligent Post-Processing**: Automatically detects and removes common ASR **hallucinations and repetitive artifacts** for cleaner, more accurate transcripts.\n-   **SRT Subtitle Generation**: Automatically create timestamped **`.srt` subtitle files** based on VAD segments, perfect for adding captions to video content.\n-   **Automatic Audio Resampling**: Automatically converts audio from any sample rate and channel count to the 16kHz mono format required by the Qwen-ASR API. You can use any audio file without worrying about pre-processing.\n-   **Universal Media Support**: Supports virtually any audio and video format (e.g., `.mp4`, `.mov`, `.mkv`, `.mp3`, `.wav`, `.m4a`) thanks to its reliance on FFmpeg.\n-   **Simple & Easy to Use**: A straightforward command-line interface allows you to get started with just a single command.\n\n## ‚öôÔ∏è How It Works\n\nThis tool follows a robust pipeline to deliver fast and accurate transcriptions for long-form media:\n\n1.  **Media Loading**: The script first loads your media file, whether it's a **local file or a remote URL**.\n2.  **VAD-based Chunking**: It analyzes the audio stream using Voice Activity Detection (VAD) to identify silent segments.\n3.  **Intelligent Splitting**: The audio is then split into smaller chunks based on the detected silences. Each chunk's duration is managed to stay under the 3-minute API limit, with a **user-configurable target length (defaulting to 120 seconds)**, preventing mid-sentence cuts.\n4.  **Parallel API Calls**: A thread pool is initiated to upload and process these chunks concurrently using the DashScope Qwen-ASR API.\n5.  **Result Aggregation & Cleaning**: The transcribed text segments from all chunks are collected, re-ordered, and then **post-processed to remove detected repetitions and hallucinations**.\n6.  **Output Generation**: The final, cleaned transcription is printed to the console and saved to a `.txt` file. **Optionally, a timestamped `.srt` subtitle file can also be generated.**\n\n## üèÅ Getting Started\n\nFollow these steps to set up and run the project on your local machine.\n\n### Prerequisites\n\n-   Python 3.8 or higher.\n-   **FFmpeg**: The script requires FFmpeg to be installed on your system to handle media files.\n    -   **Ubuntu/Debian**: `sudo apt update && sudo apt install ffmpeg`\n    -   **macOS**: `brew install ffmpeg`\n    -   **Windows**: Download from the [official FFmpeg website](https://ffmpeg.org/download.html) and add it to your system's PATH.\n-   **DashScope API Key**: You need an API key from Alibaba Cloud's DashScope.\n    -   You can obtain one from the [DashScope Console](https://dashscope.console.aliyun.com/apiKey). If you are calling the API services of Tongyi Qwen for the first time, you can follow the tutorial on [this website](https://help.aliyun.com/zh/model-studio/first-api-call-to-qwen) to create your own API Key.\n    -   For better security and convenience, it is **highly recommended** to set your API key as an environment variable named `DASHSCOPE_API_KEY`. The script will automatically use it, and you won't need to pass the `--api-key` argument in the command.\n\n        **On Linux/macOS:**\n        ```bash\n        export DASHSCOPE_API_KEY=\"your_api_key_here\"\n        ```\n        *(To make this permanent, add the line to your `~/.bashrc`, `~/.zshrc`, or `~/.profile` file.)*\n\n        **On Windows (Command Prompt):**\n        ```cmd\n        set DASHSCOPE_API_KEY=\"your_api_key_here\"\n        ```\n\n        **On Windows (PowerShell):**\n        ```powershell\n        $env:DASHSCOPE_API_KEY=\"your_api_key_here\"\n        ```\n        *(For a permanent setting on Windows, search for \"Edit the system environment variables\" in the Start Menu and add `DASHSCOPE_API_KEY` to your user variables.)*\n\n### Installation\n\nWe recommen",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-28T02:27:40.010474"
  },
  {
    "basic_info": {
      "name": "unifolm-world-model-action",
      "full_name": "unitreerobotics/unifolm-world-model-action",
      "owner": "unitreerobotics",
      "description": null,
      "url": "https://github.com/unitreerobotics/unifolm-world-model-action",
      "clone_url": "https://github.com/unitreerobotics/unifolm-world-model-action.git",
      "ssh_url": "git@github.com:unitreerobotics/unifolm-world-model-action.git",
      "homepage": null,
      "created_at": "2025-09-12T13:51:15Z",
      "updated_at": "2025-09-27T14:13:59Z",
      "pushed_at": "2025-09-23T08:58:00Z"
    },
    "stats": {
      "stars": 566,
      "forks": 44,
      "watchers": 566,
      "open_issues": 5,
      "size": 123058
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 814096,
        "Shell": 3231
      },
      "license": "Other",
      "topics": []
    },
    "content": {
      "readme": "# UnifoLM-WMA-0: A World-Model-Action (WMA) Framework under UnifoLM Family\n<p style=\"font-size: 1.2em;\">\n    <a href=\"https://unigen-x.github.io/unifolm-world-model-action.github.io\"><strong>Project Page</strong></a> | \n    <a href=\"https://huggingface.co/collections/unitreerobotics/unifolm-wma-0-68ca23027310c0ca0f34959c\"><strong>Models</strong></a> |\n    <a href=\"https://huggingface.co/unitreerobotics/datasets\"><strong>Dataset</strong></a> \n  </p>\n<div align=\"center\">\n  <p align=\"right\">\n    <span> üåéEnglish </span> | <a href=\"README_cn.md\"> üá®üá≥‰∏≠Êñá </a>\n  </p>\n</div>\n<div align=\"justify\">\n    <b>UnifoLM-WMA-0</b> is Unitree‚Äòs open-source world-model‚Äìaction architecture spanning multiple types of robotic embodiments, designed specifically for general-purpose robot learning. Its core component is a world-model capable of understanding the physical interactions between robots and the environments. This world-model provides two key functions: (a) <b>Simulation Engine</b> ‚Äì operates as an interactive simulator to generate synthetic data for robot learning; (b) <b>Policy Enhancement</b> ‚Äì connects with an action head and, by predicting future interaction processes with the world-model, further optimizes decision-making performance.\n</div>\n\n## ü¶æ Real-Robot Demonstrations\n| <img src=\"assets/gifs/real_z1_stackbox.gif\" style=\"border:none;box-shadow:none;margin:0;padding:0;\" /> | <img src=\"assets/gifs/real_dual_stackbox.gif\" style=\"border:none;box-shadow:none;margin:0;padding:0;\" /> |\n|:---:|:---:|\n| <img src=\"assets/gifs/real_cleanup_pencils.gif\" style=\"border:none;box-shadow:none;margin:0;padding:0;\" /> | <img src=\"assets/gifs/real_g1_pack_camera.gif\" style=\"border:none;box-shadow:none;margin:0;padding:0;\" /> |\n\n**Note: the top-right window shows the world model‚Äôs prediction of future action videos.**\n\n## üî• News\n\n* Sep 22, 2025: üöÄ We released the deployment code for assisting experiments with [Unitree](https://www.unitree.com/) robots.\n* Sep 15, 2025: üöÄ We released the training and inference code along with the model weights of [**UnifoLM-WMA-0**](https://huggingface.co/collections/unitreerobotics/unifolm-wma-0-68ca23027310c0ca0f34959c).\n\n## üìë Opensource Plan\n- [x] Training \n- [x] Inference\n- [x] Checkpoints\n- [x] Deployment\n\n## ‚öôÔ∏è  Installation\n```\nconda create -n unifolm-wma python==3.10.18\nconda activate unifolm-wma\n\nconda install pinocchio=3.2.0 -c conda-forge -y\nconda install ffmpeg=7.1.1 -c conda-forge\n\ngit clone --recurse-submodules https://github.com/unitreerobotics/unifolm-world-model-action.git\n\n# If you already downloaded the repo:\ncd unifolm-world-model-action\ngit submodule update --init --recursive\n\npip install -e .\n\ncd external/dlimp\npip install -e .\n```\n## üß∞ Model Checkpoints\n| Model | Description | Link|\n|---------|-------|------|\n|$\\text{UnifoLM-WMA-0}_{Base}$| Fintuned on [Open-X](https://robotics-transformer-x.github.io/) dataset. | [HuggingFace](https://huggingface.co/unitreerobotics/UnifoLM-WMA-0-Base)|\n|$\\text{UnifoLM-WMA-0}_{Dual}$| Fintuned on five [Unitree opensource dataset](https://huggingface.co/collections/unitreerobotics/g1-dex1-datasets-68bae98bf0a26d617f9983ab) in both decision-making and simulation modes. | [HuggingFace](https://huggingface.co/unitreerobotics/UnifoLM-WMA-0-Dual)|\n\n## üõ¢Ô∏è Dataset\nIn our experiments, we consider the following three opensource dataset:\n| Dataset | Robot | Link |\n|---------|-------|------|\n|Z1_StackBox| [Unitree Z1](https://www.unitree.com/z1)|[Huggingface](https://huggingface.co/datasets/unitreerobotics/Z1_StackBox_Dataset/tree/v2.1)|\n|Z1_DualArm_StackBox|[Unitree Z1](https://www.unitree.com/z1)|[Huggingface](https://huggingface.co/datasets/unitreerobotics/Z1_Dual_Dex1_StackBox_Dataset/tree/v2.1)|\n|Z1_DualArm_StackBox_V2|[Unitree Z1](https://www.unitree.com/z1)|[Huggingface](https://huggingface.co/datasets/unitreerobotics/Z1_Dual_Dex1_StackBox_Dataset_V2/tree/v2.1)|\n|Z1_DualArm_Cleanup_Pencils|[Unitree Z1](https://www.unitree.com/z1)|[Huggingface](https://huggingface.co/datasets/unitreerobotics/Z1_Dual_Dex1_CleanupPencils_Dataset/tree/v2.1)|\n|G1_Pack_Camera|[Unitree G1](https://www.unitree.com/g1)|[Huggingface](https://huggingface.co/datasets/unitreerobotics/G1_Dex1_MountCameraRedGripper_Dataset/tree/v2.1)|\n\nTo train on your own dataset, first to have the data following the [Huggingface LeRobot V2.1](https://github.com/huggingface/lerobot) dataset format. Assume the dataset‚Äôs source directory structure is as follows:\n```\nsource_dir/\n    ‚îú‚îÄ‚îÄ dataset1_name\n    ‚îú‚îÄ‚îÄ dataset2_name\n    ‚îú‚îÄ‚îÄ dataset3_name\n    ‚îî‚îÄ‚îÄ ...\n```\nThen, convert a dataset to the required format using the command below:\n```python\ncd prepare_data\npython prepare_training_data.py \\\n    --source_dir /path/to/your/source_dir \\\n    --target_dir /path/to/save/the/converted/data \\\n    --dataset_name \"dataset1_name\" \\\n    --robot_name \"a tag of the robot in the dataset\" # e.g, Unitree Z1 Robot Arm or Unitree G1 Robot with Gripper.\n```\nThe resulting data structure (Note: model training only supports in",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-28T02:27:41.233059"
  },
  {
    "basic_info": {
      "name": "Lumina-DiMOO",
      "full_name": "Alpha-VLLM/Lumina-DiMOO",
      "owner": "Alpha-VLLM",
      "description": "Lumina-DiMOO - An Open-Sourced Multi-Modal Large Diffusion Language Model",
      "url": "https://github.com/Alpha-VLLM/Lumina-DiMOO",
      "clone_url": "https://github.com/Alpha-VLLM/Lumina-DiMOO.git",
      "ssh_url": "git@github.com:Alpha-VLLM/Lumina-DiMOO.git",
      "homepage": "https://synbol.github.io/Lumina-DiMOO/",
      "created_at": "2025-09-09T18:19:24Z",
      "updated_at": "2025-09-28T02:22:18Z",
      "pushed_at": "2025-09-26T03:23:30Z"
    },
    "stats": {
      "stars": 564,
      "forks": 31,
      "watchers": 564,
      "open_issues": 2,
      "size": 55173
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 146667
      },
      "license": "Apache License 2.0",
      "topics": [
        "diffusion-large-language-model",
        "discrete-diffusion-models",
        "unified-multimodal-understanding-and-generation"
      ]
    },
    "content": {
      "readme": "<p align=\"center\">\n <img src=\"./assets/Lumina-DiMOO.png\" width=\"20%\"/>\n</p>\n\n<div align=\"center\">\n <h1> Lumina-DiMOO: An Omni Diffusion Large Language Model for Multi-Modal Generation and Understanding </h1>\n\n  [[üìë Technical Report ](https://github.com/Alpha-VLLM/Lumina-DiMOO/blob/main/Technical-Report.pdf)] &emsp; [[üåê Project Page (Demo & Benchmark)](https://synbol.github.io/Lumina-DiMOO/)] &emsp; [[ü§ó Model ](https://huggingface.co/Alpha-VLLM/Lumina-DiMOO)]\n \n <b>¬πShanghai AI Laboratory, ¬≤Shanghai Innovation Institute, ¬≥Shanghai Jiao Tong University, ‚Å¥Nanjing University </b>\n \n <b>‚ÅµThe University of Sydney, ‚Å∂The Chinese University of Hong Kong, ‚Å∑Tsinghua University</b>\n\n <img src=\"./assets/teaser.png\" width=\"95%\"/>\n</div>\n\n## üìö Introduction \nWe introduce Lumina-DiMOO, an omni foundational model for seamless multimodal generation and understanding. Lumina-DiMOO is distinguished by four key innovations:\n\n - **Unified Discrete Diffusion Architecture:** Lumina-DiMOO sets itself apart from prior unified models by utilizing a fully discrete diffusion modeling to handle inputs and outputs across various modalities.\n - **Versatile Multimodal Capabilities:** Lumina-DiMOO supports a broad spectrum of multimodal tasks, including text-to-image generation (allowing for arbitrary and high-resolution), image-to-image generation (e.g., image editing, subject-driven generation, and image inpainting, etc.), alongside advanced image understanding.\n\n - **Higher Sampling Efficiency:** Compared to previous AR or hybrid AR-diffusion paradigms, Lumina-DiMOO demonstrates remarkable sampling efficiency. Additionally, we design a bespoke caching method to further speed up the sampling speed by 2x.\n\n - **Superior Performance:** Lumina-DiMOO achieves state-of-the-art performance on multiple benchmarks, surpassing existing open-source unified multimodal models, setting a new standard in the field.\n\n\n   \n <img src=\"./assets/architecture.png\" width=\"100%\"/>\n\n\n## üî• News\n- **[2025-09-25]** üéâüéâüéâ We have released the Technical Report.\n- **[2025-09-20]** üéâüéâüéâ In the latest [UniGenBench Leaderboard](https://huggingface.co/spaces/CodeGoat24/UniGenBench_Leaderboard)(maintained by Tencent Hunyuan Team), Lumina-DiMOO's generation evaluation ranks 1st ü•á among all open-source unified models. \n- **[2025-09-12]** üéâüéâüéâ We have open-sourced Image Inpainting & Extrapolation code.\n- **[2025-09-11]** üéâüéâüéâ We have open-sourced the Max Logit-based Cache solution, offering a 2x speed improvement for sampling.\n- **[2025-09-10]** üéâüéâüéâ We release the initial version of **Lumina-DiMOO**, including:\n  - üéØ Model Checkpoints on [HuggingFace](https://huggingface.co/Alpha-VLLM/Lumina-DiMOO)!\n  - üéØ Text-to-Image & Image-to-Image Generation Inference code!\n  - üéØ Image Understanding Inference Code!\n  - üéØ Website & Demo on [Project Page](https://synbol.github.io/Lumina-DiMOO/)!\n\n## üìù Open-Source Plan\n - [x] Image Inpainting & Extrapolation Code\n - [x] Fast Sampling with Max Logit-based Cache\n - [ ] Gradio Demo\n - [ ] Bechmark Evaluation Code\n - [ ] Fine-Tuning Code\n - [ ] Self-GRPO Training Code\n - [x] Technical Report\n\n## üìΩÔ∏è Qualitative Results\nHere we present some comparative generation results with other models. **For additional visualization results, please see our [Project Page](https://synbol.github.io/Lumina-DiMOO/).**\n<details open>\n  <summary>Text-to-Image Comparison</summary>\n  <img src=\"./assets/demo_t2i.png\" width=\"100%\"/>\n<!--   <details open>\n  <summary>Effects of Max Logit-Based Cache (A800 GPU, 1536x768 resolution)</summary>\n  Without Cache: Latency: 58.2 s; Peak GPU Memory: 38.9 GiB\n  <img src=\"./assets/nocache.png\" width=\"80%\"/>\n\n\n  With Cache: Latency: 32.2 s; Peak GPU Memory: 45.9 GiB\n  <img src=\"./assets/cache.png\" width=\"80%\"/>\n</details> -->\n</details>\n\n<details close>\n  <summary>Image Editing Comparison</summary>\n  <img src=\"./assets/demo_editing.png\" width=\"100%\"/>\n</details>\n\n<details close>\n  <summary>Controllable & Subject-Driven Generation Comparison</summary>\n  <img src=\"./assets/qualitative_control_subject.png\" width=\"100%\"/>\n</details>\n\n<details close>\n  <summary>Image Inpainting & Extrapolation</summary>\n  <img src=\"./assets/demo_inpainting.jpg\" width=\"100%\"/>\n</details>\n\n\n## üìä Quantitative Performance\n<details open>\n  <summary>GenEval Benchmark</summary>\n  <img src=\"./assets/GenEval_benchmark.png\" width=\"100%\"/>\n</details>\n\n\n<details close>\n  <summary>DPG Benchmark</summary>\n  <img src=\"./assets/DPG_benchmark.png\" width=\"100%\"/>\n</details>\n\n<details close>\n  <summary>OneIG-EN Benchmark</summary>\n  <img src=\"./assets/OneIG-EN_benchmark.png\" width=\"100%\"/>\n</details>\n\n\n<details close>\n  <summary>TIIF Benchmark</summary>\n  <img src=\"./assets/TIIF_benchmark.png\" width=\"100%\"/>\n</details>\n\n<details close>\n  <summary>Image-to-Image Benchmark</summary>\n  <img src=\"./assets/i2i_benchmark.png\" width=\"100%\"/>\n</details>\n\n<details close>\n  <summary>Image Understanding Benchmark</summary>\n  <img src=\"./assets/understanding_benchmark.png\" width=\"100%\"/>\n</",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-28T02:27:42.449840"
  },
  {
    "basic_info": {
      "name": "VibeVoice",
      "full_name": "vibevoice-community/VibeVoice",
      "owner": "vibevoice-community",
      "description": "VibeVoice: Expressive, longform conversational speech synthesis. (Community fork)",
      "url": "https://github.com/vibevoice-community/VibeVoice",
      "clone_url": "https://github.com/vibevoice-community/VibeVoice.git",
      "ssh_url": "git@github.com:vibevoice-community/VibeVoice.git",
      "homepage": "https://discord.gg/ZDEYTTRxWG",
      "created_at": "2025-09-04T15:53:19Z",
      "updated_at": "2025-09-28T00:36:37Z",
      "pushed_at": "2025-09-27T01:32:48Z"
    },
    "stats": {
      "stars": 519,
      "forks": 202,
      "watchers": 519,
      "open_issues": 5,
      "size": 40022
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 329655
      },
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "> [!IMPORTANT]\n> This is a community-maintained fork of VibeVoice. Following the removal of the official VibeVoice repository, this fork serves to preserve the codebase and maintain accessibility for the community while also introducing additional functionality (such as unofficial training/fine-tuning implementations)\n\n## üéôÔ∏è VibeVoice: A Frontier Long Conversational Text-to-Speech Model\n\n[![Project Page](https://img.shields.io/badge/Project-Page-blue)](https://microsoft.github.io/VibeVoice)\n[![Hugging Face](https://img.shields.io/badge/Hugging_Face-Models-orange?logo=huggingface)](https://huggingface.co/vibevoice)\n[![Technical Report](https://img.shields.io/badge/Technical-Report-red)](https://arxiv.org/pdf/2508.19205)\n[![Colab](https://img.shields.io/badge/Colab-Demo-orange?logo=googlecolab)](https://colab.research.google.com/github/vibevoice-community/VibeVoice/blob/main/demo/VibeVoice_colab.ipynb)\n\n## Community\n\n**Join the unofficial Discord community: https://discord.gg/ZDEYTTRxWG** - share samples, ask questions, discuss fine-tuning, etc.\n\n## Overview\n\nVibeVoice is a novel framework designed for generating **expressive**, **long-form**, **multi-speaker** conversational audio, such as podcasts, from text. It addresses significant challenges in traditional Text-to-Speech (TTS) systems, particularly in scalability, speaker consistency, and natural turn-taking.\n\nA core innovation of VibeVoice is its use of continuous speech tokenizers (Acoustic and Semantic) operating at an ultra-low frame rate of 7.5 Hz. These tokenizers efficiently preserve audio fidelity while significantly boosting computational efficiency for processing long sequences. VibeVoice employs a [next-token diffusion](https://arxiv.org/abs/2412.08635) framework, leveraging a Large Language Model (LLM) to understand textual context and dialogue flow, and a diffusion head to generate high-fidelity acoustic details.\n\nThe model can synthesize speech up to **90 minutes** long with up to **4 distinct speakers**, surpassing the typical 1-2 speaker limits of many prior models.\n\nFine-tuning is now supported, which is incredibly powerful. You can adapt VibeVoice to a new language or a new voice - [try it out](https://github.com/vibevoice-community/VibeVoice/blob/main/FINETUNING.md)\n\n## [Examples](./EXAMPLES.md)\n\n## Evaluation\n\n<p align=\"left\">\n  <img src=\"Figures/MOS-preference.png\" alt=\"MOS Preference Results\" height=\"260px\">\n  <img src=\"Figures/VibeVoice.jpg\" alt=\"VibeVoice Overview\" height=\"250px\" style=\"margin-right: 10px;\">\n</p>\n\n\n## Updates\n\n- **[2025-09-05]** Microsoft repo restored (without code) with statement about responsible AI use.\n- **[2025-09-04]** Community backup created after Microsoft removed original repo and models.\n- **[2025-08-26]** The [VibeVoice-7B](https://huggingface.co/vibevoice/VibeVoice-7B) model weights are open-sourced!\n- **[2025-08-28]** [Colab Notebook](https://colab.research.google.com/github/microsoft-community/VibeVoice/blob/main/demo/VibeVoice_colab.ipynb) available. Only VibeVoice-1.5B is supported due to GPU memory limitations.\n\n## Roadmap\n\n- [x] Unofficial/community training code\n- [ ] HF Transformers integration ([PR](https://github.com/huggingface/transformers/pull/40546))\n- [ ] VibePod: End-to-end solution that creates podcasts from documents, webpages, or even a simple topic.\n\n## Model Zoo\n\n| Model | Context Length | Generation Length |  Weight |\n|-------|----------------|----------|----------|\n| VibeVoice-1.5B | 64K | ~90 min | [HF link](https://huggingface.co/vibevoice/VibeVoice-1.5B) |\n| VibeVoice-Large| 32K | ~45 min | [HF link](https://huggingface.co/vibevoice/VibeVoice-7B) |\n\n## Installation\n\n```bash\ngit clone https://github.com/vibevoice-community/VibeVoice.git\ncd VibeVoice/\n\nuv pip install -e .\n```\n\n## Usage\n\n### üö® Tips\n\nWe observed users may encounter occasional instability when synthesizing Chinese speech. We recommend:\n\n- Using English punctuation even for Chinese text, preferably only commas and periods.\n- Using the Large model variant, which is considerably more stable.\n- If you found the generated voice speak too fast. Please try to chunk your text with multiple speaker turns with same speaker label.\n\nWe'd like to thank [PsiPi](https://huggingface.co/PsiPi) for sharing an interesting way for emotion control. Details can be found via [discussion #12](https://huggingface.co/microsoft/VibeVoice-1.5B/discussions/12).\n\n**Option 1: Launch Gradio demo**\n\n```bash\npython demo/gradio_demo.py --model_path vibevoice/VibeVoice-1.5B --share\n# or python demo/gradio_demo.py --model_path vibevoice/VibeVoice-7B --share\n# optionally add --checkpoint_path path/to/checkpoint to load a fine-tuned adapter\n# use the in-app \"Disable voice cloning\" setting (Advanced Settings) to skip speaker conditioning\n```\n\n**Option 2: Inference from files directly**\n\n```bash\n# We provide some LLM generated example scripts under demo/text_examples/ for demo\n# 1 speaker\npython demo/inference_from_file.py --model_path vibevoice/VibeVoi",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-28T02:27:43.747250"
  },
  {
    "basic_info": {
      "name": "Lucy-Edit-ComfyUI",
      "full_name": "DecartAI/Lucy-Edit-ComfyUI",
      "owner": "DecartAI",
      "description": null,
      "url": "https://github.com/DecartAI/Lucy-Edit-ComfyUI",
      "clone_url": "https://github.com/DecartAI/Lucy-Edit-ComfyUI.git",
      "ssh_url": "git@github.com:DecartAI/Lucy-Edit-ComfyUI.git",
      "homepage": null,
      "created_at": "2025-09-17T13:24:52Z",
      "updated_at": "2025-09-28T01:59:02Z",
      "pushed_at": "2025-09-22T13:38:59Z"
    },
    "stats": {
      "stars": 518,
      "forks": 54,
      "watchers": 518,
      "open_issues": 5,
      "size": 11267
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 12846
      },
      "license": null,
      "topics": []
    },
    "content": {
      "readme": "# Lucy Edit - ComfyUI\n\n<p align=\"center\">\n  <img src=\"assets/logo.png\" width=\"680\" alt=\"Lucy Edit Dev Logo\"/>\n</p>\n\n<p align=\"center\">\n  üß™ <a href=\"http://github.com/DecartAI/lucy-edit-comfyui\"><b>GitHub</b></a>\n  &nbsp;|&nbsp; ü§ó <a href=\"https://huggingface.co/decart-ai/Lucy-Edit-Dev\">Huggingface</a>\n  &nbsp;|&nbsp; üìñ <a href=\"https://platform.decart.ai\">Playground</a>\n  &nbsp;|&nbsp; üìë <a href=\"https://d2drjpuinn46lb.cloudfront.net/Lucy_Edit__High_Fidelity_Text_Guided_Video_Editing.pdf\">Technical Report</a>\n  &nbsp;|&nbsp; üí¨ <a href=\"https://discord.gg/decart\">Discord</a>\n</p>\n\n---\n\n<img width=\"2559\" height=\"812\" alt=\"image\" src=\"https://github.com/user-attachments/assets/291f41d2-f4a4-4d36-a0cf-f73a05fd0a0c\" />\n\n\n<div align=\"center\">\n\n<table>\n<tr>\n<td align=\"center\">\n  <video src=\"https://github.com/user-attachments/assets/5084db41-be23-47a2-97a2-4f6bf7229809\" width=\"100%\" controls>\n    Your browser does not support the video tag.\n  </video>\n  <br/>\n  <em>Put the woman in gothic black jeans and leather jacket and crop top under it.</em>\n</td>\n<td align=\"center\">\n  <video src=\"https://github.com/user-attachments/assets/f72e58e1-f00b-45a7-a2d4-28bea2aad11c\" width=\"100%\" controls>\n    Your browser does not support the video tag.\n  </video>\n  <br/>\n  <em>1.2) Put her in a clown outfit.</em>\n</td>\n<td align=\"center\">\n  <video src=\"https://github.com/user-attachments/assets/51263d11-66e9-4bdc-a41d-b59ee628332d\" width=\"100%\" controls>\n    Your browser does not support the video tag.\n  </video>\n  <br/>\n  <em>1.3) Put the woman in a red bikini with an open thick coat above it.</em>\n</td>\n</tr>\n</table>\n</div>\n\n\n**Lucy Edit** is a **video editing** model that performs **instruction-guided edits** on videos using free-text prompts ‚Äî it supports a variety of edits, such as **clothing & accessory changes**, **character changes**, **object insertions**, and **scene replacements** while preserving the motion and composition perfectly.\n\n- üèÉ‚Äç‚ôÇÔ∏è **Motion Preservation** - preserves the motion and composition of videos perfectly, allowing precise edits.\n- üéØ **Edit reliability** ‚Äî edits are more robust when compared to common inference time methods.\n- üß¢ **Wardrobe & accessories** ‚Äî change outfits, add glasses/earrings/hats/etc.\n- üßå **Character Changes** ‚Äî replace characters with monsters, animals and known characters. (e.g., \"Replace the person with a polar bear\")\n- üó∫Ô∏è **Scenery swap** ‚Äî move the scene (e.g., \"transform the scene into a 2D cartoon,\")  \n- üìù **Pure text instructions** ‚Äî no finetuning, no masks required for common edits  \n\n---\n\n## üõ†Ô∏è Quickstart\n\n### Installation\n\n1. Clone this repo into custom_nodes folder.\n1. Install dependencies: pip install -r requirements.txt\n\n### Download Model Weights\n\n1. Download the appropriate weights for your setup:\n\n   * **FP16 weights**:  \n     https://huggingface.co/decart-ai/Lucy-Edit-Dev-ComfyUI/resolve/main/lucy-edit-dev-cui-fp16.safetensors\n\n   * **FP32 weights**:  \n     https://huggingface.co/decart-ai/Lucy-Edit-Dev-ComfyUI/resolve/main/lucy-edit-dev-cui.safetensors\n\n2. Place the weights under: `models/diffusion_models/`\n\n### Usage\nPlease refer to the \"Prompting Guidelines & Supported Edits\" section for the best experience.\n\n#### Lucy Edit Pro (API)\n1. Load the workflow from `examples/basic-api-lucy-edit.json`.\n1. Get an api key from: https://platform.decart.ai/.\n\n\n#### Lucy Edit Dev (Local)\n1. Load the workflow from `examples/basic-lucy-edit-dev.json`\n\n## üé¨ Demos\n\n<div align=\"center\">\n### Sample 1\n<table>\n<tr>\n<td align=\"center\">\n  <video src=\"https://github.com/user-attachments/assets/0ac94178-ce03-4e9d-9326-676fe6146bc6\" width=\"100%\" controls>\n    Your browser does not support the video tag.\n  </video>\n  <br/>\n  <em>1.1) Replace the man with an alien wearing the same leather jacket.</em>\n</td>\n<td align=\"center\">\n  <video src=\"https://github.com/user-attachments/assets/78275b81-04b4-4ee7-afa2-79fdcf54b688\" width=\"100%\" controls>\n    Your browser does not support the video tag.\n  </video>\n  <br/>\n  <em>1.2) Replace the man witha polar bear.</em>\n</td>\n<td align=\"center\">\n  <video src=\"https://github.com/user-attachments/assets/3ad89caa-8b89-4322-a1ef-e92df45c907a\" width=\"100%\" controls>\n    Your browser does not support the video tag.\n  </video>\n  <br/>\n  <em>1.3) Make it snow.</em>\n</td>\n</tr>\n</table>\n\n### Sample 2\n<table>\n<tr>\n<td align=\"center\">\n  <video src=\"https://github.com/user-attachments/assets/443c36a8-dfc9-4a11-8873-4ed4985753ee\" width=\"100%\" controls>\n    Your browser does not support the video tag.\n  </video>\n  <br/>\n  <em>2.1) Replace the woman with Harley Quinn with full make up and a shirt with \"Daddy's Lil Monster\" written on it.</em>\n</td>\n<td align=\"center\">\n  <video src=\"https://github.com/user-attachments/assets/e9654e91-e0f4-479e-8632-d567178ea72f\" width=\"100%\" controls>\n    Your browser does not support the video tag.\n  </video>\n  <br/>\n  <em>2.2) Replace the girl with a lego character.</em>\n</td>\n<td align=\"center\">\n  <video src=\"https://github.com/",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-28T02:27:44.978995"
  }
]