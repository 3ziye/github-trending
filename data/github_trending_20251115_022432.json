[
  {
    "basic_info": {
      "name": "DeepSeek-OCR",
      "full_name": "deepseek-ai/DeepSeek-OCR",
      "owner": "deepseek-ai",
      "description": "Contexts Optical Compression",
      "url": "https://github.com/deepseek-ai/DeepSeek-OCR",
      "clone_url": "https://github.com/deepseek-ai/DeepSeek-OCR.git",
      "ssh_url": "git@github.com:deepseek-ai/DeepSeek-OCR.git",
      "homepage": null,
      "created_at": "2025-10-17T06:14:27Z",
      "updated_at": "2025-11-15T02:15:35Z",
      "pushed_at": "2025-10-25T02:43:18Z"
    },
    "stats": {
      "stars": 20447,
      "forks": 1723,
      "watchers": 20447,
      "open_issues": 234,
      "size": 7948
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 113538
      },
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "<!-- markdownlint-disable first-line-h1 -->\n<!-- markdownlint-disable html -->\n<!-- markdownlint-disable no-duplicate-header -->\n\n\n<div align=\"center\">\n  <img src=\"assets/logo.svg\" width=\"60%\" alt=\"DeepSeek AI\" />\n</div>\n\n\n<hr>\n<div align=\"center\">\n  <a href=\"https://www.deepseek.com/\" target=\"_blank\">\n    <img alt=\"Homepage\" src=\"assets/badge.svg\" />\n  </a>\n  <a href=\"https://huggingface.co/deepseek-ai/DeepSeek-OCR\" target=\"_blank\">\n    <img alt=\"Hugging Face\" src=\"https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-DeepSeek%20AI-ffc107?color=ffc107&logoColor=white\" />\n  </a>\n\n</div>\n\n<div align=\"center\">\n\n  <a href=\"https://discord.gg/Tc7c45Zzu5\" target=\"_blank\">\n    <img alt=\"Discord\" src=\"https://img.shields.io/badge/Discord-DeepSeek%20AI-7289da?logo=discord&logoColor=white&color=7289da\" />\n  </a>\n  <a href=\"https://twitter.com/deepseek_ai\" target=\"_blank\">\n    <img alt=\"Twitter Follow\" src=\"https://img.shields.io/badge/Twitter-deepseek_ai-white?logo=x&logoColor=white\" />\n  </a>\n\n</div>\n\n\n\n<p align=\"center\">\n  <a href=\"https://huggingface.co/deepseek-ai/DeepSeek-OCR\"><b>üì• Model Download</b></a> |\n  <a href=\"https://github.com/deepseek-ai/DeepSeek-OCR/blob/main/DeepSeek_OCR_paper.pdf\"><b>üìÑ Paper Link</b></a> |\n  <a href=\"https://arxiv.org/abs/2510.18234\"><b>üìÑ Arxiv Paper Link</b></a> |\n</p>\n\n<h2>\n<p align=\"center\">\n  <a href=\"\">DeepSeek-OCR: Contexts Optical Compression</a>\n</p>\n</h2>\n\n<p align=\"center\">\n<img src=\"assets/fig1.png\" style=\"width: 1000px\" align=center>\n</p>\n<p align=\"center\">\n<a href=\"\">Explore the boundaries of visual-text compression.</a>       \n</p>\n\n## Release\n- [2025/10/23]üöÄüöÄüöÄ DeepSeek-OCR is now officially supported in upstream [vLLM](https://docs.vllm.ai/projects/recipes/en/latest/DeepSeek/DeepSeek-OCR.html#installing-vllm). Thanks to the [vLLM](https://github.com/vllm-project/vllm) team for their help.\n- [2025/10/20]üöÄüöÄüöÄ We release DeepSeek-OCR, a model to investigate the role of vision encoders from an LLM-centric viewpoint.\n\n## Contents\n- [Install](#install)\n- [vLLM Inference](#vllm-inference)\n- [Transformers Inference](#transformers-inference)\n  \n\n\n\n\n## Install\n>Our environment is cuda11.8+torch2.6.0.\n1. Clone this repository and navigate to the DeepSeek-OCR folder\n```bash\ngit clone https://github.com/deepseek-ai/DeepSeek-OCR.git\n```\n2. Conda\n```Shell\nconda create -n deepseek-ocr python=3.12.9 -y\nconda activate deepseek-ocr\n```\n3. Packages\n\n- download the vllm-0.8.5 [whl](https://github.com/vllm-project/vllm/releases/tag/v0.8.5) \n```Shell\npip install torch==2.6.0 torchvision==0.21.0 torchaudio==2.6.0 --index-url https://download.pytorch.org/whl/cu118\npip install vllm-0.8.5+cu118-cp38-abi3-manylinux1_x86_64.whl\npip install -r requirements.txt\npip install flash-attn==2.7.3 --no-build-isolation\n```\n**Note:** if you want vLLM and transformers codes to run in the same environment, you don't need to worry about this installation error like: vllm 0.8.5+cu118 requires transformers>=4.51.1\n\n## vLLM-Inference\n- VLLM:\n>**Note:** change the INPUT_PATH/OUTPUT_PATH and other settings in the DeepSeek-OCR-master/DeepSeek-OCR-vllm/config.py\n```Shell\ncd DeepSeek-OCR-master/DeepSeek-OCR-vllm\n```\n1. image: streaming output\n```Shell\npython run_dpsk_ocr_image.py\n```\n2. pdf: concurrency ~2500tokens/s(an A100-40G)\n```Shell\npython run_dpsk_ocr_pdf.py\n```\n3. batch eval for benchmarks\n```Shell\npython run_dpsk_ocr_eval_batch.py\n```\n\n**[2025/10/23] The version of upstream [vLLM](https://docs.vllm.ai/projects/recipes/en/latest/DeepSeek/DeepSeek-OCR.html#installing-vllm):**\n\n```shell\nuv venv\nsource .venv/bin/activate\n# Until v0.11.1 release, you need to install vLLM from nightly build\nuv pip install -U vllm --pre --extra-index-url https://wheels.vllm.ai/nightly\n```\n\n```python\nfrom vllm import LLM, SamplingParams\nfrom vllm.model_executor.models.deepseek_ocr import NGramPerReqLogitsProcessor\nfrom PIL import Image\n\n# Create model instance\nllm = LLM(\n    model=\"deepseek-ai/DeepSeek-OCR\",\n    enable_prefix_caching=False,\n    mm_processor_cache_gb=0,\n    logits_processors=[NGramPerReqLogitsProcessor]\n)\n\n# Prepare batched input with your image file\nimage_1 = Image.open(\"path/to/your/image_1.png\").convert(\"RGB\")\nimage_2 = Image.open(\"path/to/your/image_2.png\").convert(\"RGB\")\nprompt = \"<image>\\nFree OCR.\"\n\nmodel_input = [\n    {\n        \"prompt\": prompt,\n        \"multi_modal_data\": {\"image\": image_1}\n    },\n    {\n        \"prompt\": prompt,\n        \"multi_modal_data\": {\"image\": image_2}\n    }\n]\n\nsampling_param = SamplingParams(\n            temperature=0.0,\n            max_tokens=8192,\n            # ngram logit processor args\n            extra_args=dict(\n                ngram_size=30,\n                window_size=90,\n                whitelist_token_ids={128821, 128822},  # whitelist: <td>, </td>\n            ),\n            skip_special_tokens=False,\n        )\n# Generate output\nmodel_outputs = llm.generate(model_input, sampling_param)\n\n# Print output\nfor output in model_outputs:\n    print(output.outputs[0].text)\n```\n## ",
      "default_branch": "main"
    },
    "fetched_at": "2025-11-15T02:24:33.676026"
  },
  {
    "basic_info": {
      "name": "toon",
      "full_name": "toon-format/toon",
      "owner": "toon-format",
      "description": "üéí Token-Oriented Object Notation (TOON) ‚Äì Compact, human-readable, schema-aware JSON for LLM prompts. Spec, benchmarks, TypeScript SDK.",
      "url": "https://github.com/toon-format/toon",
      "clone_url": "https://github.com/toon-format/toon.git",
      "ssh_url": "git@github.com:toon-format/toon.git",
      "homepage": "https://toonformat.dev",
      "created_at": "2025-10-22T18:17:32Z",
      "updated_at": "2025-11-15T02:18:15Z",
      "pushed_at": "2025-11-14T17:48:21Z"
    },
    "stats": {
      "stars": 15389,
      "forks": 612,
      "watchers": 15389,
      "open_issues": 14,
      "size": 1436
    },
    "tech_info": {
      "language": "TypeScript",
      "languages": {
        "TypeScript": 254186,
        "JavaScript": 230
      },
      "license": "MIT License",
      "topics": [
        "data-format",
        "llm",
        "serialization",
        "tokenization"
      ]
    },
    "content": {
      "readme": "![TOON logo with step‚Äëby‚Äëstep guide](./.github/og.png)\n\n# Token-Oriented Object Notation (TOON)\n\n[![CI](https://github.com/toon-format/toon/actions/workflows/ci.yml/badge.svg)](https://github.com/toon-format/toon/actions)\n[![npm version](https://img.shields.io/npm/v/@toon-format/toon.svg)](https://www.npmjs.com/package/@toon-format/toon)\n[![SPEC v2.0](https://img.shields.io/badge/spec-v2.0-lightgray)](https://github.com/toon-format/spec)\n[![npm downloads (total)](https://img.shields.io/npm/dt/@toon-format/toon.svg)](https://www.npmjs.com/package/@toon-format/toon)\n[![License: MIT](https://img.shields.io/badge/license-MIT-blue.svg)](./LICENSE)\n\n**Token-Oriented Object Notation** is a compact, human-readable format for serializing JSON data in LLM prompts. It represents the same objects, arrays, and primitives as JSON, but in a syntax that minimizes tokens and makes structure easy for models to follow.\n\nTOON combines YAML's indentation-based structure for nested objects with a CSV-style tabular layout for uniform arrays. TOON's sweet spot is **uniform arrays of objects** (multiple fields per row, same structure across items), achieving CSV-like compactness while adding explicit structure that helps LLMs parse and validate data reliably. For deeply nested or non-uniform data, JSON may be more efficient.\n\nThe similarity to CSV is intentional: CSV is simple and ubiquitous, and TOON aims to keep that familiarity while remaining a lossless, drop-in representation of JSON for Large Language Models.\n\nThink of it as a translation layer: use JSON programmatically, and encode it as TOON for LLM input.\n\n> [!TIP]\n> TOON is production-ready, but also an idea in progress. Nothing's set in stone ‚Äì help shape where it goes by contributing to the [spec](https://github.com/toon-format/spec) or sharing feedback.\n\n## Table of Contents\n\n- [Why TOON?](#why-toon)\n- [Key Features](#key-features)\n- [When Not to Use TOON](#when-not-to-use-toon)\n- [Benchmarks](#benchmarks)\n- [Playgrounds](#playgrounds)\n- [üìã Full Specification](https://github.com/toon-format/spec/blob/main/SPEC.md)\n- [Installation & Quick Start](#installation--quick-start)\n- [CLI](#cli)\n- [Format Overview](#format-overview)\n- [API](#api)\n- [Using TOON in LLM Prompts](#using-toon-in-llm-prompts)\n- [Notes and Limitations](#notes-and-limitations)\n- [Syntax Cheatsheet](#syntax-cheatsheet)\n- [Other Implementations](#other-implementations)\n\n## Why TOON?\n\nAI is becoming cheaper and more accessible, but larger context windows allow for larger data inputs as well. **LLM tokens still cost money** ‚Äì and standard JSON is verbose and token-expensive:\n\n```json\n{\n  \"users\": [\n    { \"id\": 1, \"name\": \"Alice\", \"role\": \"admin\" },\n    { \"id\": 2, \"name\": \"Bob\", \"role\": \"user\" }\n  ]\n}\n```\n\nYAML conveys the same infromation with **fewer tokens**:\n\n```yaml\nusers:\n  - id: 1\n    name: Alice\n    role: admin\n  - id: 2\n    name: Bob\n    role: user\n```\n\nTOON conveys the same information with **even fewer tokens**:\n\n```\nusers[2]{id,name,role}:\n  1,Alice,admin\n  2,Bob,user\n```\n\n## Key Features\n\n- üí∏ **Token-efficient:** typically 30-60% fewer tokens on large uniform arrays vs formatted JSON[^1]\n- ü§ø **LLM-friendly guardrails:** explicit lengths and fields enable validation\n- üç± **Minimal syntax:** removes redundant punctuation (braces, brackets, most quotes)\n- üìê **Indentation-based structure:** like YAML, uses whitespace instead of braces\n- üß∫ **Tabular arrays:** declare keys once, stream data as rows\n- üîó **Optional key folding:** collapses single-key wrapper chains into dotted paths (e.g., `data.metadata.items`) to reduce indentation and tokens\n\n[^1]: For flat tabular data, CSV is more compact. TOON adds minimal overhead to provide explicit structure and validation that improves LLM reliability.\n\n## When Not to Use TOON\n\nTOON excels with uniform arrays of objects, but there are cases where other formats are better:\n\n- **Deeply nested or non-uniform structures** (tabular eligibility ‚âà 0%): JSON-compact often uses fewer tokens. Example: complex configuration objects with many nested levels.\n- **Semi-uniform arrays** (~40‚Äì60% tabular eligibility): Token savings diminish. Prefer JSON if your pipelines already rely on it.\n- **Pure tabular data**: CSV is smaller than TOON for flat tables. TOON adds minimal overhead (~5-10%) to provide structure (array length declarations, field headers, delimiter scoping) that improves LLM reliability.\n- **Latency-critical applications**: If end-to-end response time is your top priority, benchmark on your exact setup. Some deployments (especially local/quantized models like Ollama) may process compact JSON faster despite TOON's lower token count. Measure TTFT, tokens/sec, and total time for both formats and use whichever is faster.\n\nSee [benchmarks](#benchmarks) for concrete comparisons across different data structures.\n\n## Benchmarks\n\nBenchmarks are organized into two tracks to ensure fair comparisons:\n\n- **Mixed-Structure Track**: Datasets with nested or semi-uniform structu",
      "default_branch": "main"
    },
    "fetched_at": "2025-11-15T02:24:35.003889"
  },
  {
    "basic_info": {
      "name": "AI-Trader",
      "full_name": "HKUDS/AI-Trader",
      "owner": "HKUDS",
      "description": "\"AI-Trader: Can AI Beat the Market?\" Live Trading Bench: https://ai4trade.ai",
      "url": "https://github.com/HKUDS/AI-Trader",
      "clone_url": "https://github.com/HKUDS/AI-Trader.git",
      "ssh_url": "git@github.com:HKUDS/AI-Trader.git",
      "homepage": "https://ai4trade.ai",
      "created_at": "2025-10-23T12:45:00Z",
      "updated_at": "2025-11-14T22:15:56Z",
      "pushed_at": "2025-11-14T11:49:24Z"
    },
    "stats": {
      "stars": 9362,
      "forks": 1423,
      "watchers": 9362,
      "open_issues": 48,
      "size": 15329
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 312770,
        "Shell": 4908
      },
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "\n\n<div align=\"center\">\n  <picture>\n      <img src=\"./assets/AI-Trader-log.png\" width=\"20%\" style=\"border: none; box-shadow: none;\">\n  </picture>\n</div >\n\n<div align=\"center\">\n\n# üöÄ AI-Trader: Can AI Beat the Market?\n\n[![Python](https://img.shields.io/badge/Python-3.10+-blue.svg)](https://python.org)\n[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)\n[![GitHub stars](https://img.shields.io/github/stars/HKUDS/AI-Trader?style=social)](https://github.com/HKUDS/AI-Trader)\n[![Feishu](https://img.shields.io/badge/üí¨Feishu-Group-blue?style=flat)](./Communication.md) \n[![WeChat](https://img.shields.io/badge/WeChat-Group-green?style=flat&logo=wechat)](./Communication.md)\n\n**AI agents battle for supremacy in NASDAQ 100, SSE 50, and cryptocurrency markets. Zero human input. Pure competition.**\n\n## üèÜ Current Championship Leaderboard üèÜ \n[*Click Here: AI Live Trading*](https://ai4trade.ai)\n\n</div>\n\n---\n## Friends of AI-Trader: Other Interesting Projects\n- [TradeTrap](https://github.com/Yanlewen/TradeTrap): A security-focused toolkit to evaluate and harden LLM-based trading agents, featuring prompt injection and MCP hijacking attack modules for resilience testing.\n\n- [RockAlpha](https://rockalpha.rockflow.ai/): The investment arena launched by RockFlow. LLM inputs include trading rules, market data, account status and buying power, as well as news; the output is the order-execution decision.\n\n- [TwinMarket](https://github.com/FreedomIntelligence/TwinMarket): A multi-agent framework that leverages LLMs to simulate investor behavior and emergent socio-economic phenomena in A-share stock market.\n---\n## üéâ Weekly Update\n\n### üìà Market Expansion\n- ‚úÖ **A-Share Market Support** - Extended our trading capabilities to include Chinese A-share markets, expanding our global market coverage.\n- ‚úÖ **Cryptocurrency Market Support** - Added support for trading major cryptocurrencies including Bitcoin, Ethereum, and 8 other leading digital assets.\n\n### ‚è∞ Enhanced Trading Capabilities\n- ‚úÖ **Hourly Trading Support** - We've upgraded from daily to hourly trading intervals, enabling more precise and responsive market participation with granular timing control.\n\n### üé® User Experience Improvements\n- ‚úÖ **Live Trading Dashboard** - Introduced real-time visualization of all agent trading activities: https://ai4trade.ai.\n\n- ‚úÖ **Agent Reasoning Display** - Implemented complete transparency into AI decision-making processes, featuring detailed reasoning chains that show how each trading decision is formed.\n\n- ‚úÖ **Interactive Leaderboard** - Launched a dynamic performance ranking system with live updates, allowing users to track and compare agent performance in real-time.\n\n- ‚è∞ **Important Notice** - To maintain a well-managed repository, we no longer upload runtime data to the repo, as it would make it very bloated. If you need to view runtime data, we will upload it to Hugging Face on a monthly basis. You can view real-time runtime data here: https://ai4trade.ai.\n---\n\n## **How to use this dataset**\n\nIt's simple! \n\nYou just need to submit a PR that includes at least: `./agent/{your_strategy}.py` (you can inherit from Basemodel to create your strategy!), `./configs/{yourconfig}`, and instructions on how to run your strategy. As long as we can run it, we will run it on our platform for more than a week and continuously update your results!\n\n---\n\n<div align=\"center\">\n\n[üöÄ Quick Start](#-quick-start) ‚Ä¢ [üìà Performance Analysis](#-performance-analysis) ‚Ä¢ [üõ†Ô∏è Configuration Guide](#-configuration-guide) ‚Ä¢ [‰∏≠ÊñáÊñáÊ°£](README_CN.md)\n\n</div>\n\n\n## üåü Project Introduction\n\n> **AI-Trader enables five distinct AI models, each employing unique investment strategies, to compete autonomously in the same market and determine which can generate the highest profits in NASDAQ 100, SSE 50, or cryptocurrency trading!**\n\n### üéØ Core Features\n\n- ü§ñ **Fully Autonomous Decision-Making**: AI agents perform 100% independent analysis, decision-making, and execution without human intervention\n- üõ†Ô∏è **Pure Tool-Driven Architecture**: Built on MCP toolchain, enabling AI to complete all trading operations through standardized tool calls\n- üèÜ **Multi-Model Competition Arena**: Deploy multiple AI models (GPT, Claude, Qwen, etc.) for competitive trading\n- üìä **Real-Time Performance Analytics**: Comprehensive trading records, position monitoring, and profit/loss analysis\n- üîç **Intelligent Market Intelligence**: Integrated Jina search for real-time market news and financial reports\n- ‚ö° **MCP Toolchain Integration**: Modular tool ecosystem based on Model Context Protocol\n- üîå **Extensible Strategy Framework**: Support for third-party strategies and custom AI agent integration\n- ‚è∞ **Historical Replay Capability**: Time-period replay functionality with automatic future information filtering\n\n---\n\n### üéÆ Trading Environment\nEach AI model starts with $10,000, 100,000¬•, or 50,000 USDT to trade NASDAQ 100 stocks, SSE 50 stocks, or major cryptocurrencies in a controlled environment with real mar",
      "default_branch": "main"
    },
    "fetched_at": "2025-11-15T02:24:36.296573"
  },
  {
    "basic_info": {
      "name": "Valdi",
      "full_name": "Snapchat/Valdi",
      "owner": "Snapchat",
      "description": "Valdi is a cross-platform UI framework that delivers native performance without sacrificing developer velocity.",
      "url": "https://github.com/Snapchat/Valdi",
      "clone_url": "https://github.com/Snapchat/Valdi.git",
      "ssh_url": "git@github.com:Snapchat/Valdi.git",
      "homepage": "https://discord.gg/uJyNEeYX2U",
      "created_at": "2025-11-06T17:33:28Z",
      "updated_at": "2025-11-15T02:20:14Z",
      "pushed_at": "2025-11-15T00:26:59Z"
    },
    "stats": {
      "stars": 8927,
      "forks": 276,
      "watchers": 8927,
      "open_issues": 34,
      "size": 82156
    },
    "tech_info": {
      "language": "C++",
      "languages": {
        "C++": 8511254,
        "TypeScript": 4846036,
        "JavaScript": 2334883,
        "Swift": 1833358,
        "C": 1666128,
        "Kotlin": 1177109,
        "Objective-C": 1014696,
        "Objective-C++": 562611,
        "Starlark": 417975,
        "Java": 44024,
        "Shell": 35915,
        "Smarty": 6857,
        "HTML": 4271,
        "Python": 3905,
        "Pug": 645,
        "Makefile": 426,
        "Dockerfile": 235,
        "Linker Script": 171,
        "CSS": 140,
        "Go": 86,
        "SCSS": 20
      },
      "license": "Other",
      "topics": [
        "android",
        "cross-platform",
        "ios",
        "typescript"
      ]
    },
    "content": {
      "readme": "# Valdi\n\n[![License: MIT](https://img.shields.io/badge/License-MIT-blue.svg)](./LICENSE.md)\n[![Platforms](https://img.shields.io/badge/platform-iOS%20%7C%20Android%20%7C%20macOS-lightgrey)](./docs/INSTALL.md)\n[![Status](https://img.shields.io/badge/status-beta-yellow)]()\n[![Discord](https://img.shields.io/discord/1285677307163574322?color=7289da&label=Discord&logo=discord&logoColor=white)](https://discord.gg/uJyNEeYX2U)\n[![TypeScript](https://img.shields.io/badge/TypeScript-5.x-blue?logo=typescript)](https://www.typescriptlang.org/)\n[![Documentation](https://img.shields.io/badge/docs-available-brightgreen)](./docs/README.md)\n[![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg)](./CONTRIBUTING.md)\n\n> [!NOTE]\n> **Beta Status:** Valdi has been widely used in Snap's production apps for the last 8 years. We're calling this a beta because our tools and documentation need more battle testing in the open source world. Valdi will exit beta when we're happy with the developer experience.\n\n**Valdi is a cross-platform UI framework that delivers native performance without sacrificing developer velocity.** Write your UI once in declarative TypeScript, and it compiles directly to native views on iOS, Android, and macOS‚Äîno web views, no JavaScript bridges. \n\n## Quick Example\n\nA basic Valdi component:\n\n```tsx\nimport { Component } from 'valdi_core/src/Component';\n\nclass HelloWorld extends Component {\n  onRender() {\n    const message = 'Hello World! üëª';\n    <view backgroundColor='#FFFC00' padding={30}>\n      <label color='black' value={message} />\n    </view>;\n  }\n}\n```\n\n<p align=\"center\">\n  <img src=\"./docs/docs/assets/start-about/IMG_1445.jpg\" width=\"400\" alt=\"Hello World example running on iOS\" />\n</p>\n\n## Quick Links\n\n- [Getting Started Guide](./docs/INSTALL.md)\n- [Documentation](./docs/README.md)\n- [Codelabs](./docs/docs/start-code-lab.md)\n- [API Reference](./docs/api/api-quick-reference.md)\n- [Frequently Asked Questions](./docs/docs/faq.md)\n- [Component Library](https://github.com/Snapchat/Valdi_Widgets)\n\n## Why Choose Valdi?\n\nValdi is a cross-platform UI framework designed to solve the fundamental problem of cross-platform development: velocity vs. runtime performance. For 8 years, it has powered a large portion of Snap's production apps.\n\n### True Native Performance\n\nUnlike frameworks that rely on web views or JavaScript bridges, Valdi compiles declaratively rendered TypeScript components into platform-native views. Valdi also includes several other performance advantages:\n\n- **[Automatic view recycling](./docs/docs/performance-view-recycling.md)** - Global view pooling system reuses native views across all screens, dramatically reducing inflation latency\n- **Optimized component rendering** - Components re-render independently without triggering parent re-renders, enabling fast incremental updates\n- **Optimized layout engine** - C++ layout engine runs on the main thread with minimal marshalling overhead\n- **Viewport-aware rendering** - Only visible views are inflated, making infinite scrolling performant by default\n\nLearn more in our [Performance Optimization Guide](./docs/docs/performance-optimization.md).\n\n### Developer Experience Built for Speed\n\nValdi eliminates the traditional compile-test-debug cycle that slows native development:\n\n- **Instant hot reload** - See changes in milliseconds on iOS, Android, or desktop without recompiling\n- **[Full VSCode debugging](./docs/docs/workflow-hermes-debugger.md)** - Set breakpoints, inspect variables, profile performance, and capture heap dumps directly in VSCode\n- **Familiar syntax** - TSX components with TypeScript for type safety\n\n### Flexible Adoption Model\n\nValdi integrates easily into existing apps - start small and scale as needed:\n\n- **[Embed Valdi in native](./docs/docs/native-bindings.md)** - Drop Valdi components into existing UIKit or Android view hierarchies\n- **[Embed native in Valdi](./docs/docs/native-customviews.md)** - Use platform-specific views within Valdi layouts via `<custom-view>`\n- **[Polyglot modules](./docs/docs/native-polyglot.md)** - Write performance-critical code in C++, Swift, Kotlin, or Objective-C with type-safe bindings to TypeScript\n- **[Full-stack architecture](./docs/docs/advanced-full-stack.md)** - Build entire features in Valdi with worker threads for background processing, eliminating platform-specific bridge code\n\n### Deep Native Integration\n\nValdi generates type-safe bindings between TypeScript and native platforms:\n\n- **[Automatic code generation](./docs/docs/native-annotations.md)** - TypeScript interfaces compile to Kotlin, Objective-C, and Swift bindings\n- **[Native API access](./docs/docs/native-polyglot.md)** - Direct access to platform APIs and third-party native libraries through polyglot modules\n- **Bidirectional communication** - Pass complex data structures and callbacks between TypeScript and native code safely\n- **[Native protobuf support](./docs/docs/advanced-protobuf.md)** - Seamless integration ",
      "default_branch": "main"
    },
    "fetched_at": "2025-11-15T02:24:37.595189"
  },
  {
    "basic_info": {
      "name": "nofx",
      "full_name": "NoFxAiOS/nofx",
      "owner": "NoFxAiOS",
      "description": "NOFX: Defining the Next-Generation AI Trading Operating System. A multi-exchange Al trading platform(Binance/Hyperliquid/Aster) with multi-Ai competition(deepseek/qwen/claude)self-evolution, and real-time dashboard",
      "url": "https://github.com/NoFxAiOS/nofx",
      "clone_url": "https://github.com/NoFxAiOS/nofx.git",
      "ssh_url": "git@github.com:NoFxAiOS/nofx.git",
      "homepage": "https://nofxai.com",
      "created_at": "2025-10-28T07:17:53Z",
      "updated_at": "2025-11-15T02:10:58Z",
      "pushed_at": "2025-11-15T02:10:54Z"
    },
    "stats": {
      "stars": 7558,
      "forks": 1932,
      "watchers": 7558,
      "open_issues": 324,
      "size": 43585
    },
    "tech_info": {
      "language": "Go",
      "languages": {
        "Go": 697606,
        "TypeScript": 647332,
        "Shell": 86050,
        "CSS": 13770,
        "JavaScript": 4014,
        "HTML": 1062
      },
      "license": "GNU Affero General Public License v3.0",
      "topics": [
        "agentic-ai",
        "agentictrading",
        "ai",
        "ai-trading",
        "aitradingos",
        "nof1ai",
        "trading"
      ]
    },
    "content": {
      "readme": "# ü§ñ NOFX - Agentic Trading OS\n\n[![Go Version](https://img.shields.io/badge/Go-1.21+-00ADD8?style=flat&logo=go)](https://golang.org/)\n[![React](https://img.shields.io/badge/React-18+-61DAFB?style=flat&logo=react)](https://reactjs.org/)\n[![TypeScript](https://img.shields.io/badge/TypeScript-5.0+-3178C6?style=flat&logo=typescript)](https://www.typescriptlang.org/)\n[![License](https://img.shields.io/badge/License-AGPL--3.0-blue.svg)](LICENSE)\n[![Backed by Amber.ac](https://img.shields.io/badge/Backed%20by-Amber.ac-orange.svg)](https://amber.ac)\n\n**Languages:** [English](README.md) | [‰∏≠Êñá](docs/i18n/zh-CN/README.md) | [–£–∫—Ä–∞—ó–Ω—Å—å–∫–∞](docs/i18n/uk/README.md) | [–†—É—Å—Å–∫–∏–π](docs/i18n/ru/README.md) | [Êó•Êú¨Ë™û](docs/i18n/ja/README.md)\n\n**üìö Documentation:** [Docs Home](docs/README.md) | [Getting Started](docs/getting-started/README.md) | [Prompt Writing Guide](docs/prompt-guide.md) ([‰∏≠Êñá](docs/prompt-guide.zh-CN.md)) | [Changelog](CHANGELOG.md) | [Contributing](CONTRIBUTING.md) | [Security](SECURITY.md)\n\n---\n\n## üìë Table of Contents\n\n- [üöÄ Universal AI Trading Operating System](#-universal-ai-trading-operating-system)\n- [üë• Developer Community](#-developer-community)\n- [üÜï What's New](#-whats-new-latest-update)\n- [üì∏ Screenshots](#-screenshots)\n- [‚ú® Current Implementation](#-current-implementation---crypto-markets)\n- [üîÆ Roadmap](#-roadmap---universal-market-expansion)\n- [üèóÔ∏è Technical Architecture](#Ô∏è-technical-architecture)\n- [üí∞ Register Binance Account](#-register-binance-account-save-on-fees)\n- [üî∑ Register Hyperliquid Account](#-using-hyperliquid-exchange)\n- [üî∂ Register Aster DEX Account](#-using-aster-dex-exchange)\n- [üöÄ Quick Start](#-quick-start)\n- [üìñ AI Decision Flow](#-ai-decision-flow)\n- [üß† AI Self-Learning](#-ai-self-learning-example)\n- [üìä Web Interface Features](#-web-interface-features)\n- [üéõÔ∏è API Endpoints](#Ô∏è-api-endpoints)\n- [‚ö†Ô∏è Important Risk Warnings](#Ô∏è-important-risk-warnings)\n- [üõ†Ô∏è Common Issues](#Ô∏è-common-issues)\n- [üìà Performance Tips](#-performance-optimization-tips)\n- [üîÑ Changelog](#-changelog)\n- [üìÑ License](#-license)\n- [ü§ù Contributing](#-contributing)\n\n---\n\n## üöÄ Universal AI Trading Operating System\n\n**NOFX** is a **universal Agentic Trading OS** built on a unified architecture. We've successfully closed the loop in crypto markets: **\"Multi-Agent Decision ‚Üí Unified Risk Control ‚Üí Low-Latency Execution ‚Üí Live/Paper Account Backtesting\"**, and are now expanding this same technology stack to **stocks, futures, options, forex, and all financial markets**.\n\n### üéØ Core Features\n\n- **Universal Data & Backtesting Layer**: Cross-market, cross-timeframe, cross-exchange unified representation and factor library, accumulating transferable \"strategy memory\"\n- **Multi-Agent Self-Play & Self-Evolution**: Strategies automatically compete and select the best, continuously iterating based on account-level PnL and risk constraints\n- **Integrated Execution & Risk Control**: Low-latency routing, slippage/risk control sandbox, account-level limits, one-click market switching\n\n### üè¢ Backed by [Amber.ac](https://amber.ac)\n\n### üë• Core Team\n\n- **Tinkle** - [@Web3Tinkle](https://x.com/Web3Tinkle)\n\n### üíº Seed Funding Round Open\n\nWe are currently raising our **seed round**.\n\n**For investment inquiries**, please DM **Tinkle** via Twitter.\n\n---\n\n> ‚ö†Ô∏è **Risk Warning**: This system is experimental. AI auto-trading carries significant risks. Strongly recommended for learning/research purposes or testing with small amounts only!\n\n## üë• Developer Community\n\nJoin our Telegram developer community to discuss, share ideas, and get support:\n\n**üí¨ [NOFX Developer Community](https://t.me/nofx_dev_community)**\n\n---\n\n## üÜï What's New (Latest Update)\n\n### üöÄ Multi-Exchange Support!\n\nNOFX now supports **three major exchanges**: Binance, Hyperliquid, and Aster DEX!\n\n#### **Hyperliquid Exchange**\n\nA high-performance decentralized perpetual futures exchange!\n\n**Key Features:**\n- ‚úÖ Full trading support (long/short, leverage, stop-loss/take-profit)\n- ‚úÖ Automatic precision handling (order size & price)\n- ‚úÖ Unified trader interface (seamless exchange switching)\n- ‚úÖ Support for both mainnet and testnet\n- ‚úÖ No API keys needed - just your Ethereum private key\n\n**New Workflow:**\n1. **Configure AI Models**: Add your DeepSeek/Qwen API keys through the web interface\n2. **Configure Exchanges**: Set up Binance/Hyperliquid API credentials\n3. **Create Traders**: Combine any AI model with any exchange to create custom traders\n4. **Monitor & Control**: Start/stop traders and monitor performance in real-time\n\n**Why This Update?**\n- üéØ **User-Friendly**: No more editing JSON files or server restarts\n- üîß **Flexible**: Mix and match different AI models with different exchanges\n- üìä **Scalable**: Create unlimited trader combinations\n- üîí **Secure**: Database storage with proper data management\n\nSee [Quick Start](#-quick-start) for the new setup process!\n\n#### **Aster DEX Exchange** (NEW! v2.0.2)\n\nA Binance-compatible decentralized perpetual futures exchange!\n\n**Key Features:**\n- ‚úÖ Binance-",
      "default_branch": "dev"
    },
    "fetched_at": "2025-11-15T02:24:38.914581"
  },
  {
    "basic_info": {
      "name": "claude-code-infrastructure-showcase",
      "full_name": "diet103/claude-code-infrastructure-showcase",
      "owner": "diet103",
      "description": "Examples of my Claude Code infrastructure with skill auto-activation, hooks, and agents",
      "url": "https://github.com/diet103/claude-code-infrastructure-showcase",
      "clone_url": "https://github.com/diet103/claude-code-infrastructure-showcase.git",
      "ssh_url": "git@github.com:diet103/claude-code-infrastructure-showcase.git",
      "homepage": null,
      "created_at": "2025-10-30T03:12:16Z",
      "updated_at": "2025-11-15T02:20:26Z",
      "pushed_at": "2025-10-31T01:41:31Z"
    },
    "stats": {
      "stars": 6579,
      "forks": 836,
      "watchers": 6579,
      "open_issues": 13,
      "size": 214
    },
    "tech_info": {
      "language": "Shell",
      "languages": {
        "Shell": 19297,
        "JavaScript": 12798
      },
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "# Claude Code Infrastructure Showcase\n\n**A curated reference library of production-tested Claude Code infrastructure.**\n\nBorn from 6 months of real-world use managing a complex TypeScript microservices project, this showcase provides the patterns and systems that solved the \"skills don't activate automatically\" problem and scaled Claude Code for enterprise development.\n\n> **This is NOT a working application** - it's a reference library. Copy what you need into your own projects.\n\n---\n\n## What's Inside\n\n**Production-tested infrastructure for:**\n- ‚úÖ **Auto-activating skills** via hooks\n- ‚úÖ **Modular skill pattern** (500-line rule with progressive disclosure)\n- ‚úÖ **Specialized agents** for complex tasks\n- ‚úÖ **Dev docs system** that survives context resets\n- ‚úÖ **Comprehensive examples** using generic blog domain\n\n**Time investment to build:** 6 months of iteration\n**Time to integrate into your project:** 15-30 minutes\n\n---\n\n## Quick Start - Pick Your Path\n\n### ü§ñ Using Claude Code to Integrate?\n\n**Claude:** Read [`CLAUDE_INTEGRATION_GUIDE.md`](CLAUDE_INTEGRATION_GUIDE.md) for step-by-step integration instructions tailored for AI-assisted setup.\n\n### üéØ I want skill auto-activation\n\n**The breakthrough feature:** Skills that actually activate when you need them.\n\n**What you need:**\n1. The skill-activation hooks (2 files)\n2. A skill or two relevant to your work\n3. 15 minutes\n\n**üëâ [Setup Guide: .claude/hooks/README.md](.claude/hooks/README.md)**\n\n### üìö I want to add ONE skill\n\nBrowse the [skills catalog](.claude/skills/) and copy what you need.\n\n**Available:**\n- **backend-dev-guidelines** - Node.js/Express/TypeScript patterns\n- **frontend-dev-guidelines** - React/TypeScript/MUI v7 patterns\n- **skill-developer** - Meta-skill for creating skills\n- **route-tester** - Test authenticated API routes\n- **error-tracking** - Sentry integration patterns\n\n**üëâ [Skills Guide: .claude/skills/README.md](.claude/skills/README.md)**\n\n### ü§ñ I want specialized agents\n\n10 production-tested agents for complex tasks:\n- Code architecture review\n- Refactoring assistance\n- Documentation generation\n- Error debugging\n- And more...\n\n**üëâ [Agents Guide: .claude/agents/README.md](.claude/agents/README.md)**\n\n---\n\n## What Makes This Different?\n\n### The Auto-Activation Breakthrough\n\n**Problem:** Claude Code skills just sit there. You have to remember to use them.\n\n**Solution:** UserPromptSubmit hook that:\n- Analyzes your prompts\n- Checks file context\n- Automatically suggests relevant skills\n- Works via `skill-rules.json` configuration\n\n**Result:** Skills activate when you need them, not when you remember them.\n\n### Production-Tested Patterns\n\nThese aren't theoretical examples - they're extracted from:\n- ‚úÖ 6 microservices in production\n- ‚úÖ 50,000+ lines of TypeScript\n- ‚úÖ React frontend with complex data grids\n- ‚úÖ Sophisticated workflow engine\n- ‚úÖ 6 months of daily Claude Code use\n\nThe patterns work because they solved real problems.\n\n### Modular Skills (500-Line Rule)\n\nLarge skills hit context limits. The solution:\n\n```\nskill-name/\n  SKILL.md                  # <500 lines, high-level guide\n  resources/\n    topic-1.md              # <500 lines each\n    topic-2.md\n    topic-3.md\n```\n\n**Progressive disclosure:** Claude loads main skill first, loads resources only when needed.\n\n---\n\n## Repository Structure\n\n```\n.claude/\n‚îú‚îÄ‚îÄ skills/                 # 5 production skills\n‚îÇ   ‚îú‚îÄ‚îÄ backend-dev-guidelines/  (12 resource files)\n‚îÇ   ‚îú‚îÄ‚îÄ frontend-dev-guidelines/ (11 resource files)\n‚îÇ   ‚îú‚îÄ‚îÄ skill-developer/         (7 resource files)\n‚îÇ   ‚îú‚îÄ‚îÄ route-tester/\n‚îÇ   ‚îú‚îÄ‚îÄ error-tracking/\n‚îÇ   ‚îî‚îÄ‚îÄ skill-rules.json    # Skill activation configuration\n‚îú‚îÄ‚îÄ hooks/                  # 6 hooks for automation\n‚îÇ   ‚îú‚îÄ‚îÄ skill-activation-prompt.*  (ESSENTIAL)\n‚îÇ   ‚îú‚îÄ‚îÄ post-tool-use-tracker.sh   (ESSENTIAL)\n‚îÇ   ‚îú‚îÄ‚îÄ tsc-check.sh        (optional, needs customization)\n‚îÇ   ‚îî‚îÄ‚îÄ trigger-build-resolver.sh  (optional)\n‚îú‚îÄ‚îÄ agents/                 # 10 specialized agents\n‚îÇ   ‚îú‚îÄ‚îÄ code-architecture-reviewer.md\n‚îÇ   ‚îú‚îÄ‚îÄ refactor-planner.md\n‚îÇ   ‚îú‚îÄ‚îÄ frontend-error-fixer.md\n‚îÇ   ‚îî‚îÄ‚îÄ ... 7 more\n‚îî‚îÄ‚îÄ commands/               # 3 slash commands\n    ‚îú‚îÄ‚îÄ dev-docs.md\n    ‚îî‚îÄ‚îÄ ...\n\ndev/\n‚îî‚îÄ‚îÄ active/                 # Dev docs pattern examples\n    ‚îî‚îÄ‚îÄ public-infrastructure-repo/\n```\n\n---\n\n## Component Catalog\n\n### üé® Skills (5)\n\n| Skill | Lines | Purpose | Best For |\n|-------|-------|---------|----------|\n| [**skill-developer**](.claude/skills/skill-developer/) | 426 | Creating and managing skills | Meta-development |\n| [**backend-dev-guidelines**](.claude/skills/backend-dev-guidelines/) | 304 | Express/Prisma/Sentry patterns | Backend APIs |\n| [**frontend-dev-guidelines**](.claude/skills/frontend-dev-guidelines/) | 398 | React/MUI v7/TypeScript | React frontends |\n| [**route-tester**](.claude/skills/route-tester/) | 389 | Testing authenticated routes | API testing |\n| [**error-tracking**](.claude/skills/error-tracking/) | ~250 | Sentry integration | Error monitoring |\n\n**All skills follow the modular pattern** - main",
      "default_branch": "main"
    },
    "fetched_at": "2025-11-15T02:24:40.199697"
  },
  {
    "basic_info": {
      "name": "Skill_Seekers",
      "full_name": "yusufkaraaslan/Skill_Seekers",
      "owner": "yusufkaraaslan",
      "description": "Convert documentation websites, GitHub repositories, and PDFs into Claude AI skills with automatic conflict detection",
      "url": "https://github.com/yusufkaraaslan/Skill_Seekers",
      "clone_url": "https://github.com/yusufkaraaslan/Skill_Seekers.git",
      "ssh_url": "git@github.com:yusufkaraaslan/Skill_Seekers.git",
      "homepage": "",
      "created_at": "2025-10-17T14:43:48Z",
      "updated_at": "2025-11-15T01:29:36Z",
      "pushed_at": "2025-11-12T20:30:22Z"
    },
    "stats": {
      "stars": 3993,
      "forks": 426,
      "watchers": 3993,
      "open_issues": 118,
      "size": 770
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 693757,
        "Shell": 8958
      },
      "license": "MIT License",
      "topics": [
        "ai-tools",
        "ast-parser",
        "automation",
        "claude-ai",
        "claude-skills",
        "code-analysis",
        "conflict-detection",
        "documentation",
        "documentation-generator",
        "github",
        "github-scraper",
        "mcp",
        "mcp-server",
        "multi-source",
        "ocr",
        "pdf",
        "python",
        "web-scraping"
      ]
    },
    "content": {
      "readme": "[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/yusufkaraaslan-skill-seekers-badge.png)](https://mseep.ai/app/yusufkaraaslan-skill-seekers)\n\n# Skill Seeker\n\n[![Version](https://img.shields.io/badge/version-2.0.0-blue.svg)](https://github.com/yusufkaraaslan/Skill_Seekers/releases/tag/v2.0.0)\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)\n[![MCP Integration](https://img.shields.io/badge/MCP-Integrated-blue.svg)](https://modelcontextprotocol.io)\n[![Tested](https://img.shields.io/badge/Tests-379%20Passing-brightgreen.svg)](tests/)\n[![Project Board](https://img.shields.io/badge/Project-Board-purple.svg)](https://github.com/users/yusufkaraaslan/projects/2)\n[![PyPI version](https://badge.fury.io/py/skill-seekers.svg)](https://pypi.org/project/skill-seekers/)\n[![PyPI - Downloads](https://img.shields.io/pypi/dm/skill-seekers.svg)](https://pypi.org/project/skill-seekers/)\n[![PyPI - Python Version](https://img.shields.io/pypi/pyversions/skill-seekers.svg)](https://pypi.org/project/skill-seekers/)\n\n**Automatically convert documentation websites, GitHub repositories, and PDFs into Claude AI skills in minutes.**\n\n> üìã **[View Development Roadmap & Tasks](https://github.com/users/yusufkaraaslan/projects/2)** - 134 tasks across 10 categories, pick any to contribute!\n\n## What is Skill Seeker?\n\nSkill Seeker is an automated tool that transforms documentation websites, GitHub repositories, and PDF files into production-ready [Claude AI skills](https://www.anthropic.com/news/skills). Instead of manually reading and summarizing documentation, Skill Seeker:\n\n1. **Scrapes** multiple sources (docs, GitHub repos, PDFs) automatically\n2. **Analyzes** code repositories with deep AST parsing\n3. **Detects** conflicts between documentation and code implementation\n4. **Organizes** content into categorized reference files\n5. **Enhances** with AI to extract best examples and key concepts\n6. **Packages** everything into an uploadable `.zip` file for Claude\n\n**Result:** Get comprehensive Claude skills for any framework, API, or tool in 20-40 minutes instead of hours of manual work.\n\n## Why Use This?\n\n- üéØ **For Developers**: Create skills from documentation + GitHub repos with conflict detection\n- üéÆ **For Game Devs**: Generate skills for game engines (Godot docs + GitHub, Unity, etc.)\n- üîß **For Teams**: Combine internal docs + code repositories into single source of truth\n- üìö **For Learners**: Build comprehensive skills from docs, code examples, and PDFs\n- üîç **For Open Source**: Analyze repos to find documentation gaps and outdated examples\n\n## Key Features\n\n### üåê Documentation Scraping\n- ‚úÖ **llms.txt Support** - Automatically detects and uses LLM-ready documentation files (10x faster)\n- ‚úÖ **Universal Scraper** - Works with ANY documentation website\n- ‚úÖ **Smart Categorization** - Automatically organizes content by topic\n- ‚úÖ **Code Language Detection** - Recognizes Python, JavaScript, C++, GDScript, etc.\n- ‚úÖ **8 Ready-to-Use Presets** - Godot, React, Vue, Django, FastAPI, and more\n\n### üìÑ PDF Support (**v1.2.0**)\n- ‚úÖ **Basic PDF Extraction** - Extract text, code, and images from PDF files\n- ‚úÖ **OCR for Scanned PDFs** - Extract text from scanned documents\n- ‚úÖ **Password-Protected PDFs** - Handle encrypted PDFs\n- ‚úÖ **Table Extraction** - Extract complex tables from PDFs\n- ‚úÖ **Parallel Processing** - 3x faster for large PDFs\n- ‚úÖ **Intelligent Caching** - 50% faster on re-runs\n\n### üêô GitHub Repository Scraping (**v2.0.0**)\n- ‚úÖ **Deep Code Analysis** - AST parsing for Python, JavaScript, TypeScript, Java, C++, Go\n- ‚úÖ **API Extraction** - Functions, classes, methods with parameters and types\n- ‚úÖ **Repository Metadata** - README, file tree, language breakdown, stars/forks\n- ‚úÖ **GitHub Issues & PRs** - Fetch open/closed issues with labels and milestones\n- ‚úÖ **CHANGELOG & Releases** - Automatically extract version history\n- ‚úÖ **Conflict Detection** - Compare documented APIs vs actual code implementation\n- ‚úÖ **MCP Integration** - Natural language: \"Scrape GitHub repo facebook/react\"\n\n### üîÑ Unified Multi-Source Scraping (**NEW - v2.0.0**)\n- ‚úÖ **Combine Multiple Sources** - Mix documentation + GitHub + PDF in one skill\n- ‚úÖ **Conflict Detection** - Automatically finds discrepancies between docs and code\n- ‚úÖ **Intelligent Merging** - Rule-based or AI-powered conflict resolution\n- ‚úÖ **Transparent Reporting** - Side-by-side comparison with ‚ö†Ô∏è warnings\n- ‚úÖ **Documentation Gap Analysis** - Identifies outdated docs and undocumented features\n- ‚úÖ **Single Source of Truth** - One skill showing both intent (docs) and reality (code)\n- ‚úÖ **Backward Compatible** - Legacy single-source configs still work\n\n### ü§ñ AI & Enhancement\n- ‚úÖ **AI-Powered Enhancement** - Transforms basic templates into comprehensive guides\n- ‚úÖ **No API Costs** - FREE local enhancement using Claude Code Max\n- ‚úÖ **MCP Server for",
      "default_branch": "development"
    },
    "fetched_at": "2025-11-15T02:24:41.495562"
  },
  {
    "basic_info": {
      "name": "awesome-claude-skills",
      "full_name": "ComposioHQ/awesome-claude-skills",
      "owner": "ComposioHQ",
      "description": "A curated list of awesome Claude Skills, resources, and tools for customizing Claude AI workflows",
      "url": "https://github.com/ComposioHQ/awesome-claude-skills",
      "clone_url": "https://github.com/ComposioHQ/awesome-claude-skills.git",
      "ssh_url": "git@github.com:ComposioHQ/awesome-claude-skills.git",
      "homepage": "",
      "created_at": "2025-10-17T07:15:01Z",
      "updated_at": "2025-11-15T02:18:45Z",
      "pushed_at": "2025-11-12T03:24:23Z"
    },
    "stats": {
      "stars": 3453,
      "forks": 420,
      "watchers": 3453,
      "open_issues": 1,
      "size": 3147
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 566237,
        "JavaScript": 37795,
        "Shell": 11441
      },
      "license": null,
      "topics": [
        "anthropic",
        "anthropic-ai",
        "anthropic-skills",
        "awesome",
        "awesome-lists",
        "claude",
        "claude-4",
        "claude-4-5-sonnet",
        "claude-4-opus",
        "claude-api",
        "claude-code",
        "claude-desktop",
        "claude-skills",
        "claude-skills-hub",
        "skills"
      ]
    },
    "content": {
      "readme": "<h1 align=\"center\">Awesome Claude Skills</h1>\n\n<p align=\"center\">\n<a href=\"https://composio.dev/?utm_source=Github&utm_medium=Youtube&utm_campaign=2025-11&utm_content=AwesomeSkills\">\n  <img width=\"1280\" height=\"640\" alt=\"Composio banner\" src=\"https://github.com/user-attachments/assets/adb3f57a-2706-4329-856f-059a32059d48\">\n</a>\n\n\n</p>\n\n<p align=\"center\">\n  <a href=\"https://awesome.re\">\n    <img src=\"https://awesome.re/badge.svg\" alt=\"Awesome\" />\n  </a>\n  <a href=\"https://makeapullrequest.com\">\n    <img src=\"https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat-square\" alt=\"PRs Welcome\" />\n  </a>\n  <a href=\"https://www.apache.org/licenses/LICENSE-2.0\">\n    <img src=\"https://img.shields.io/badge/License-Apache_2.0-blue.svg?style=flat-square\" alt=\"License: Apache-2.0\" />\n  </a>\n</p>\n\nA curated list of practical Claude Skills for enhancing productivity across Claude.ai, Claude Code, and the Claude API.\n\n\n> If you want your skills to take actions across 500+ apps, wire them up with [Composio](https://composio.dev/?utm_source=Github&utm_medium=Youtube&utm_campaign=2025-11&utm_content=AwesomeSkills)\n\n\n## Contents\n\n- [What Are Claude Skills?](#what-are-claude-skills)\n- [Skills](#skills)\n  - [Document Processing](#document-processing)\n  - [Development & Code Tools](#development--code-tools)\n  - [Data & Analysis](#data--analysis)\n  - [Business & Marketing](#business--marketing)\n  - [Communication & Writing](#communication--writing)\n  - [Creative & Media](#creative--media)\n  - [Productivity & Organization](#productivity--organization)\n  - [Collaboration & Project Management](#collaboration--project-management)\n  - [Security & Systems](#security--systems)\n- [Getting Started](#getting-started)\n- [Creating Skills](#creating-skills)\n- [Contributing](#contributing)\n- [Resources](#resources)\n- [License](#license)\n\n## What Are Claude Skills?\n\nClaude Skills are customizable workflows that teach Claude how to perform specific tasks according to your unique requirements. Skills enable Claude to execute tasks in a repeatable, standardized manner across all Claude platforms.\n\n## Skills\n\n### Document Processing\n\n- [docx](https://github.com/anthropics/skills/tree/main/document-skills/docx) - Create, edit, analyze Word docs with tracked changes, comments, formatting.\n- [pdf](https://github.com/anthropics/skills/tree/main/document-skills/pdf) - Extract text, tables, metadata, merge & annotate PDFs.\n- [pptx](https://github.com/anthropics/skills/tree/main/document-skills/pptx) - Read, generate, and adjust slides, layouts, templates.\n- [xlsx](https://github.com/anthropics/skills/tree/main/document-skills/xlsx) - Spreadsheet manipulation: formulas, charts, data transformations.\n- [Markdown to EPUB Converter](https://github.com/smerchek/claude-epub-skill) - Converts markdown documents and chat summaries into professional EPUB ebook files. *By [@smerchek](https://github.com/smerchek)*\n\n### Development & Code Tools\n\n- [artifacts-builder](https://github.com/anthropics/skills/tree/main/artifacts-builder) - Suite of tools for creating elaborate, multi-component claude.ai HTML artifacts using modern frontend web technologies (React, Tailwind CSS, shadcn/ui).\n- [aws-skills](https://github.com/zxkane/aws-skills) - AWS development with CDK best practices, cost optimization MCP servers, and serverless/event-driven architecture patterns.\n- [Changelog Generator](./changelog-generator/) - Automatically creates user-facing changelogs from git commits by analyzing history and transforming technical commits into customer-friendly release notes.\n- [Claude Code Terminal Title](https://github.com/bluzername/claude-code-terminal-title) - Gives each Claud-Code terminal window a dynamic title that describes the work being done so you don't lose track of what window is doing what.\n- [D3.js Visualization](https://github.com/chrisvoncsefalvay/claude-d3js-skill) - Teaches Claude to produce D3 charts and interactive data visualizations. *By [@chrisvoncsefalvay](https://github.com/chrisvoncsefalvay)*\n- [FFUF Web Fuzzing](https://github.com/jthack/ffuf_claude_skill) - Integrates the ffuf web fuzzer so Claude can run fuzzing tasks and analyze results for vulnerabilities. *By [@jthack](https://github.com/jthack)*\n- [finishing-a-development-branch](https://github.com/obra/superpowers/tree/main/skills/finishing-a-development-branch) - Guides completion of development work by presenting clear options and handling chosen workflow.\n- [iOS Simulator](https://github.com/conorluddy/ios-simulator-skill) - Enables Claude to interact with iOS Simulator for testing and debugging iOS applications. *By [@conorluddy](https://github.com/conorluddy)*\n- [MCP Builder](./mcp-builder/) - Guides creation of high-quality MCP (Model Context Protocol) servers for integrating external APIs and services with LLMs using Python or TypeScript.\n- [move-code-quality-skill](https://github.com/1NickPappas/move-code-quality-skill) - Analyzes Move language packages against the official Move",
      "default_branch": "master"
    },
    "fetched_at": "2025-11-15T02:24:42.758379"
  },
  {
    "basic_info": {
      "name": "nof0",
      "full_name": "wquguru/nof0",
      "owner": "wquguru",
      "description": "NOF0 - ÂºÄÊ∫êÁöÑ AI ‰∫§ÊòìÁ´ûÊäÄÂú∫",
      "url": "https://github.com/wquguru/nof0",
      "clone_url": "https://github.com/wquguru/nof0.git",
      "ssh_url": "git@github.com:wquguru/nof0.git",
      "homepage": "https://nof0.wqu.guru",
      "created_at": "2025-10-22T16:30:45Z",
      "updated_at": "2025-11-15T02:10:01Z",
      "pushed_at": "2025-11-06T14:41:46Z"
    },
    "stats": {
      "stars": 2702,
      "forks": 428,
      "watchers": 2702,
      "open_issues": 14,
      "size": 6126
    },
    "tech_info": {
      "language": "Go",
      "languages": {
        "Go": 711679,
        "TypeScript": 240568,
        "Python": 17418,
        "CSS": 7723,
        "JavaScript": 6050,
        "Makefile": 5532,
        "PLpgSQL": 3276,
        "Shell": 2688,
        "Dockerfile": 739
      },
      "license": null,
      "topics": []
    },
    "content": {
      "readme": "# NOF0 - ÂºÄÊ∫êÁöÑ AI ‰∫§ÊòìÁ´ûÊäÄÂú∫\n\n<div align=\"center\">\n\n[![Next.js](https://img.shields.io/badge/Next.js-000000?style=flat&logo=nextdotjs&logoColor=white)](https://nextjs.org/)\n[![React](https://img.shields.io/badge/React-20232A?style=flat&logo=react&logoColor=61DAFB)](https://reactjs.org/)\n[![Go](https://img.shields.io/badge/Go-00ADD8?style=flat&logo=go&logoColor=white)](https://go.dev/)\n[![Go-Zero](https://img.shields.io/badge/Go--Zero-000000?style=flat&logo=go&logoColor=white)](https://go-zero.dev/)\n[![ZenMux](https://img.shields.io/badge/ZenMux-LLM-000000)](https://zenmux.ai?utm_source=nof0)\n\n\n</div>\n\n<div align=\"center\">\n\n[![Hyperliquid](https://img.shields.io/badge/Hyperliquid-DEX-000000)](https://hyperliquid.xyz/)\n\n</div>\n\n<div align=\"center\">\n\n[![Documentation](https://img.shields.io/badge/Documentation-GitBook-3884FF?style=flat&logo=gitbook&logoColor=white)](https://wquguru.gitbook.io/nof0)\n[![Join Telegram Group](https://img.shields.io/badge/Telegram-nof0__ai-26A5E4?style=flat&logo=telegram&logoColor=white)](https://t.me/nof0_ai)\n[![Follow @wquguru](https://img.shields.io/badge/Follow-@wquguru-1DA1F2?style=flat&logo=x&logoColor=white)](https://twitter.com/intent/follow?screen_name=wquguru)\n\n</div>\n\n<div align=\"center\">\n\n![ÂâçÁ´ØËøõÂ∫¶](https://img.shields.io/badge/ÂâçÁ´ØËøõÂ∫¶-100%25-success?style=flat-square)\n![ÂêéÁ´ØËøõÂ∫¶](https://img.shields.io/badge/ÂêéÁ´ØËøõÂ∫¶-70%25-yellow?style=flat-square)\n![AI‰∫§ÊòìÂºïÊìéËøõÂ∫¶](https://img.shields.io/badge/AI‰∫§ÊòìÂºïÊìéËøõÂ∫¶-80%25-yellowgreen?style=flat-square)\n\n</div>\n\n\n> **ÂºÄÁÆ±Âç≥Áî®ÁöÑ LLM/Agentic Trading È°πÁõÆ**\n>\n> ÂÆåÊï¥Â§çÂàª [NOF1.ai](https://nof1.ai) Alpha ArenaÔºåËÆ© AI + Crypto Ëµ∞ÂêëÂ§ß‰ºóËßÜÈáé\n\n**Áî®ÁúüÂÆûÊï∞ÊçÆÂíåÊ∏ÖÊô∞ÂèØËßÜÂåñÔºåÂõûÁ≠î\"Âì™‰∏™Ê®°ÂûãÊõ¥‰ºöËµö\"ÁöÑÊú¥Á¥†ÈóÆÈ¢ò**\n\n## È°πÁõÆÁÆÄ‰ªã\n\nNOF0 ÊòØ‰∏Ä‰∏™ËÆ©Â§ö‰∏™ AI Ê®°ÂûãÂú®ÁúüÂÆûÂä†ÂØÜË¥ßÂ∏ÅÂ∏ÇÂú∫‰∏≠ËøõË°å‰∫§ÊòìÁ´ûËµõÁöÑÂπ≥Âè∞„ÄÇ\n\n**Ê†∏ÂøÉÁâπÊÄß**:\n\n- ÊØè‰∏™ AI LLM / Agent ‰ªé $10,000 ÂêØÂä®ËµÑÈáëÂºÄÂßã\n- ÂÆûÊó∂Â±ïÁ§∫ÊØè‰∏™Ê®°ÂûãÁöÑÁõà‰∫èË°®Áé∞\n- ÂÆåÊï¥ÂºÄÊ∫êÂ§çÂàª nof1.ai ÁöÑÂäüËÉΩ\n- ËÆ©‰ªª‰Ωï‰∫∫ÈÉΩËÉΩÈÉ®ÁΩ≤Ëá™Â∑±ÁöÑ AI ‰∫§ÊòìÁ´ûÊäÄÂú∫\n\n## Ê†∏ÂøÉÁêÜÂøµ\n\nNOF0 ‰∏çÊòØ‰º†ÁªüÁöÑÂõûÊµãÂ∑•ÂÖ∑ÔºåËÄåÊòØ‰∏Ä‰∏™ **‰ª• Prompt ‰∏∫‰∏≠ÂøÉÁöÑ‰∫§ÊòìÁ´ûÊäÄÂú∫**Ôºö\n\n- **ÂÆûÁõòÁ´ûÊäÄÔºå‰∏çÊòØÂõûÊµãÂ∑•ÂÖ∑** - Áî®ÁúüÂÆûÁõà‰∫èÈ™åËØÅÁ≠ñÁï•ÔºåÊåÅÁª≠ÂØπÊäóËøáÂ∫¶ÊãüÂêà\n- **Á´ûÊäÄÂú∫ (Arena)Ôºå‰∏çÊòØÂçï‰∏ÄÊ®°Âûã** - ‰∏ÄÈîÆÈÉ®ÁΩ≤Âü∫Á°ÄËÆæÊñΩÔºå‰∏ìÊ≥® Prompt Á≠ñÁï•Êú¨Ë∫´\n- **‰ª• Prompt ‰∏∫‰∏≠ÂøÉ** - ËÆ©Á≠ñÁï•ÂêåÂè∞Á´ûÊäÄÔºåÁî®Êï∞ÊçÆÂõûÁ≠îÔºöÂì™‰∏™Ê®°ÂûãÊõ¥‰ºöËµöÔºü\n\n### Ê†∏ÂøÉÂ∑•‰ΩúÊµÅ\n\n```\n[ÊÄùËÄÉÁ≠ñÁï•] ‚Üí [Êí∞ÂÜôPrompt] ‚Üí [ÂÆûÁõò‰∫§Êòì] ‚Üí [PNLÊéíË°å] ‚Üí [Ëø≠‰ª£Prompt]\n     ‚Üë                                                      ‚Üì\n     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n```\n\n‰ªé $10,000 ÂêØÂä®ËµÑÈáëÂºÄÂßãÔºåÂÆûÊó∂ÁúãÊùøÂ±ïÁ§∫ÊâÄÊúâ Prompt-LLM Agent ÁöÑÁúüÂÆûË°®Áé∞„ÄÇ\n\n**[Êü•ÁúãÂÆåÊï¥ËÆæËÆ°ÂéüÂàô](go/docs/principles.md)** - ‰∫ÜËß£ÊØè‰∏™ÁêÜÂøµËÉåÂêéÁöÑÊÄùËÄÉ\n\n### ÂºÄÂèëËøõÂ∫¶\n\n1. ÂâçÁ´ØÔºö100% ÂèØÁã¨Á´ãËøêË°åÔºå‰∏ç‰æùËµñÂêéÁ´Ø\n2. ÂêéÁ´ØÔºö70% Ê†∏ÂøÉÂäüËÉΩÂºÄÂèë‰∏≠\n3. ÊâßË°åÂºïÊìéÔºàÂü∫‰∫éAIÂ∑•‰ΩúÊµÅÔºâ80% Á®≥ÂÆöÊÄßÊèêÂçá‰∏≠\n\n## È°πÁõÆÁªìÊûÑ\n\n```\nnof0/\n‚îú‚îÄ‚îÄ web/          # [ÂâçÁ´Ø] Next.js + React + Recharts\n‚îú‚îÄ‚îÄ go/           # [ÂêéÁ´Ø] Go-Zero + REST API\n‚îÇ   ‚îî‚îÄ‚îÄ pkg/      # Ê†∏ÂøÉ‰∏öÂä°ÂåÖ\n‚îÇ       ‚îú‚îÄ‚îÄ executor/   # AI Êï∞ÊçÆÊµÅ‰∏éÂ∑•‰ΩúÊµÅÂºïÊìé\n‚îÇ       ‚îú‚îÄ‚îÄ llm/        # LLM Êèê‰æõÂïÜÂ∞ÅË£Ö\n‚îÇ       ‚îú‚îÄ‚îÄ manager/    # Á≠ñÁï•ÁÆ°ÁêÜÂô®\n‚îÇ       ‚îú‚îÄ‚îÄ exchange/   # ‰∫§ÊòìÊâÄÊé•Âè£\n‚îÇ       ‚îú‚îÄ‚îÄ market/     # Â∏ÇÂú∫Êï∞ÊçÆ\n‚îÇ       ‚îî‚îÄ‚îÄ prompt/     # Prompt Ê®°Êùø\n‚îî‚îÄ‚îÄ mcp/          # [MCPÊï∞ÊçÆ] MCPÊµèËßàÂô®Êà™Âõæ„ÄÅJSONÈùôÊÄÅÊï∞ÊçÆÁ≠â\n```\n\n## Âø´ÈÄüÂºÄÂßã\n\n### 1. ÂàùÂßãÂåñÈ°πÁõÆ\n\nÂÖãÈöÜÈ°πÁõÆÂêéÔºåÈÖçÁΩÆ Git Ëá™Âä®ÈÄíÂΩíÂ§ÑÁêÜÂ≠êÊ®°ÂùóÔºö\n\n```bash\ngit clone <repo>\ncd nof0\ngit config submodule.recurse true\n```\n\n> Ê≠§Âêé `git pull` ‰ºöËá™Âä®Êõ¥Êñ∞Â≠êÊ®°ÂùóÔºàÂåÖÊã¨ `go/etc/prompts/base`ÔºâÔºåÊó†ÈúÄÊâãÂä®ÊâßË°å `git submodule update`\n\n### 2. ÂêØÂä®ÂâçÁ´Ø\n\n```bash\ncd web\nnpm install\nnpm run dev\n```\n\nËÆøÈóÆ `http://localhost:3000`\n\n**ÂâçÁ´ØÊ†∏ÂøÉÁâπÊÄß**:\n\n- Ë¥¶Êà∑ÊÄªËµÑ‰∫ßÊõ≤Á∫ø\n- ÊåÅ‰ªìÊÉÖÂÜµ\n- Êàê‰∫§Á∫™ÂΩï\n- Ê®°ÂûãÂØπËØùÔºàModel ChatÔºâ\n- ÊéíË°åÊ¶ú\n- Ê®°ÂûãËØ¶ÊÉÖ\n\n### 3. ÂêØÂä®ÂêéÁ´Ø\n\n> Â∞öÊú™ÂºÄÂèëÂÆåÊØïÔºåÊ¨¢ËøéÂä†ÂÖ•tgÁæ§Ëé∑ÂèñÂºÄÂèëËøõÂ∫¶ÈÄöÁü•Ôºöhttps://t.me/nof0_ai\n\n## ÊäÄÊúØÊ†à\n\n### ÂâçÁ´Ø (web/)\n\n| Á±ªÂà´   | ÊäÄÊúØÈÄâÂûã                               | ËØ¥Êòé              |\n|------|------------------------------------|-----------------|\n| Ê°ÜÊû∂   | Next.js 15 + React 19 + TypeScript | ÂÖ®Ê†àÊ°ÜÊû∂ + Á±ªÂûãÂÆâÂÖ®     |\n| ÂõæË°®   | Recharts                           | Ëá™ÂÆö‰πâÂõæ‰æã‰∏éÊú´Á´ØÊ†áËÆ∞      |\n| Áä∂ÊÄÅÁÆ°ÁêÜ | Zustand                            | ËΩªÈáèÁ∫ßÁä∂ÊÄÅÁÆ°ÁêÜ         |\n| Ê†∑ÂºèÁ≥ªÁªü | CSS Variables                      | ÈÅøÂÖç SSR/CSR Ê∞¥ÂêàÂ∑ÆÂºÇ |\n\n**ÊäÄÊúØ‰∫ÆÁÇπ**:\n\n- Âú® `src/lib/model/meta.ts` Áªü‰∏ÄÈÖçÁΩÆÂìÅÁâåËâ≤‰∏éÁôΩËâ≤Áâà Logo\n- `globals.css` ‰ΩøÁî® CSS ÂèòÈáèÈ©±Âä®‰∏ªÈ¢òÔºà`--panel-bg`„ÄÅ`--muted-text`„ÄÅ`--axis-tick` Á≠âÔºâ\n- ÂºÄÂèëËßÑËåÉÔºöÂèÇËÄÉ `web/docs/theme.md`ÔºåÈÅøÂÖç `isDark` ÂàÜÊîØÂà§Êñ≠\n\n### ÂêéÁ´Ø (go/)\n\n| Á±ªÂà´   | ÊäÄÊúØÈÄâÂûã    | ËØ¥Êòé          |\n|------|---------|-------------|\n| Ê°ÜÊû∂   | Go-Zero | ÂæÆÊúçÂä°Ê°ÜÊû∂       |\n\n> ËØ¶ÁªÜÊñáÊ°£ËßÅ [go/README.md](go/README.md)\n\n## Êï∞ÊçÆÂø´ÁÖßÂ∑•ÂÖ∑\n\n‰∏ÄÈîÆ‰∏ãËΩΩ nof1.ai ÁöÑ‰∏äÊ∏∏Êé•Âè£ÂéüÂßãÊï∞ÊçÆÔºåÁ¶ªÁ∫ø‰øùÂ≠òÔºö\n\n```bash\ncd web\nnpm run snapshot:nof1\n```\n\n**ËæìÂá∫ËØ¥Êòé**:\n\n- **ÁîüÊàêÁõÆÂΩï**: `snapshots/nof1/<ISOÊó∂Èó¥Êà≥>/*.json` ‰∏é `index.json`\n- **ÂåÖÂê´Êï∞ÊçÆ**:\n    - crypto-pricesÔºàÂä†ÂØÜË¥ßÂ∏Å‰ª∑Ê†ºÔºâ\n    - positionsÔºàÊåÅ‰ªìÊÉÖÂÜµÔºâ\n    - tradesÔºàÊàê‰∫§Á∫™ÂΩïÔºâ\n    - account-totalsÔºàË¥¶Êà∑ÊÄªÂÄºÔºâ\n    - since-inception-valuesÔºàÁ¥ØËÆ°Êî∂ÁõäÔºâ\n    - leaderboardÔºàÊéíË°åÊ¶úÔºâ\n    - analyticsÔºàÂàÜÊûêÊï∞ÊçÆÔºâ\n    - conversationsÔºàÊ®°ÂûãÂØπËØùÔºâ\n- **ÁâàÊú¨ÊéßÂà∂**: ÈªòËÆ§‰∏çÊèê‰∫§Âà∞‰ªìÂ∫ìÔºàËßÅ `.gitignore`Ôºâ\n\n## Áõ∏ÂÖ≥ËµÑÊ∫ê\n\n- [ÂÆåÊï¥ÊñáÊ°£](https://wquguru.gitbook.io/nof0) - GitBook Âú®Á∫øÊñáÊ°£\n- [NOF1 ÂÆòÊñπÁΩëÁ´ô](https://nof1.ai/) - ÂéüÁâà Alpha Arena\n- [ÂêéÁ´ØÂÆåÊï¥ÊñáÊ°£](go/README.md) - Go ÊúçÂä°ËØ¶ÁªÜËØ¥Êòé\n- [Go-Zero Ê°ÜÊû∂](https://go-zero.dev/) - ÂæÆÊúçÂä°Ê°ÜÊû∂ÊñáÊ°£\n\n## ËÆ∏ÂèØËØÅ\n\nMIT License\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-11-15T02:24:44.029351"
  },
  {
    "basic_info": {
      "name": "surf",
      "full_name": "deta/surf",
      "owner": "deta",
      "description": "Personal AI Notebooks. Organize files & webpages and generate notes from them. Open source, local & open data, open model choice (incl. local).",
      "url": "https://github.com/deta/surf",
      "clone_url": "https://github.com/deta/surf.git",
      "ssh_url": "git@github.com:deta/surf.git",
      "homepage": "https://deta.surf",
      "created_at": "2025-10-20T15:09:57Z",
      "updated_at": "2025-11-14T23:15:38Z",
      "pushed_at": "2025-11-14T14:46:03Z"
    },
    "stats": {
      "stars": 2671,
      "forks": 189,
      "watchers": 2671,
      "open_issues": 20,
      "size": 274866
    },
    "tech_info": {
      "language": "TypeScript",
      "languages": {
        "TypeScript": 1877432,
        "Svelte": 1614432,
        "Rust": 675921,
        "JavaScript": 39662,
        "SCSS": 39395,
        "CSS": 18112,
        "HTML": 4089,
        "NSIS": 3077,
        "Handlebars": 972,
        "Shell": 369
      },
      "license": "Apache License 2.0",
      "topics": [
        "claude",
        "deepseek",
        "gemma",
        "knowledge-base",
        "knowledge-management",
        "llm",
        "local",
        "local-llm",
        "ollama",
        "openai",
        "productivity",
        "rust",
        "svelte",
        "typescript"
      ]
    },
    "content": {
      "readme": "<div align=\"center\">\n  \n![splash](./docs/assets/repo-header.png)\n\n[**Website**](https://deta.surf) - [**Discord**](https://deta.surf/discord)\n\n</div>\n\n<br>\n\n# Deta Surf: Your AI Notebook\n\nDeta Surf is an AI notebook that brings all your files and the web directly into your stream of thought.\n\nIt‚Äôs meant for simultaneous research and thinking that minimizes the grunt work: manually searching, opening windows & tabs, scrolling, copying and pasting into a document editor.\n\nSurf is primarily built in Svelte, TypeScript and Rust, runs on MacOS, Windows & Linux, stores data locally in open formats, and is open source.\n\n![split](./docs/assets/split-note.webp)\n\n## Motivation\n\nMost applications are focused on a single task, or a single media type: notes, websites, or PDFs. Real thinking requires juggling media across sources to make connections and synthesize ideas. We want to help people think better, across all their media.\n\nSurf is built to be personal and open, in service of the user. This means local first data, open data formats, open source, and openness with respect to AI models. [Read more](https://deta.surf/motivation).\n\n## Installation\n\nCheckout the [GitHub releases](https://github.com/deta/surf/releases) for the latest stable version of Surf for MacOS, Windows and Linux.\n\nYou can also download Surf with some managed & additional features (e.g.¬†AI) from the [Deta website](https://deta.surf). That version is subject to different terms.\n\nFor building from source and local development, see [CONTRIBUTING.md](CONTRIBUTING.md).\n\n## TL;DR - Things to try\n\n- _YouTube Notes_: visit a YouTube video and ask a question\n- _PDF Notes_: open a PDF and ask a question\n- _Create an applet_: use the \"app generation\" tool and ask for an app\n- _Notes that search the web_: use the \"web search\" tool and ask a question with \"search\" in it\n\n## Features\n\n### Multi-Media Library & Notebooks\n\n![notebooks](./docs/assets/readme/notebook-grid.png)\n\nStore almost any media in a private library on your computer, in an open and transparent format.\n\n- Support for local files, sites & links from the web (YouTube, Tweets & more), or create media directly in Surf.\n- Organize this library into Notebooks.\n- Open and use much of your library offline.\n- Use your library to power Surf‚Äôs AI features.\n\nSurf's library is built on a local storage engine called SFFS (Surf Flat File System), which stores data in open and transparent formats.\n\n[Details on the library](/docs/LIBRARY.md).\n\n### Smart Notes\n\n![smart-notes](./docs/assets/readme/smart-notes.png)\n\nExplore and think across your digital stuff without opening up a bunch of windows, clicking, scrolling and copying & pasting into your document (or chatbot).\n\n- `@-mention` and auto-generate from any tab, website or any resource in your [library](./docs/LIBRARY.md).\n- Trigger [web searches](./docs/SMART_NOTES.md#web-search) to do research, and bring the results back in your notes.\n- Integrated [citations](./docs/SMART_NOTES.md#citations) deeplinked to original sources, whether a section on a webpage, a timestamp in a video, or a page in a PDF.\n- Generate interactive applications without writing code using [Surflets](./docs/Surflets.md).\n- Paste in images, tables or data from other applications and have Surf understand and incorporate them.\n- Use rich formating, code blocks, to-do lists and more in your notes.\n\n[Read more](/docs/SMART_NOTES.md).\n\n### Tabs, Split View & Sidebar\n\n![split](./docs/assets/another-split.webp)\n\nSurf is built around tabs, split view and a sidebar for easy navigation.\n\n- Open local notes, files or web pages in tabs.\n- Split view allows you to view and interact with multiple resources side by side.\n- The sidebar provides quick access to your Notebooks & notes.\n\n### Surflets (App Generation)\n\n![surflets](./docs/assets/readme/surflets.png)\n\nSurf can code interactive applets to help you visualize, understand or explore concepts or data that are aided with code.\n\n[Read more](./docs/SURFLETS.md).\n\n### AI\n\n![models.png](./docs/assets/readme/models.png)\n\n[Surf‚Äôs notes](./docs/SMART_NOTES.md) and [Surflets](./docs/SURFLETS.md) are powered by large language models of your choice.\n\n- Bring your own key for popular models\n- Add a cloud model\n- Use Local Language Models\n\n[Read more](./docs/AI_MODELS.md).\n\n### Shortcuts\n\nFind the most common shortcuts [here](./docs/SHORTCUTS.md).\n\n## Security\n\n_To report a security concern, please see_ https://github.com/deta/surf/security/policy\n\n## Contributing\n\nSee [CONTRIBUTING.md](CONTRIBUTING.md) for details on contributing to the project and an overview of the codebase.\n\n## Code of Conduct\n\nSee [CODE_OF_CONDUCT.md](CODE_OF_CONDUCT.md) for details on our code of conduct.\n\n## License\n\nThe source code for this project is licensed under the Apache 2.0 license, with the following exceptions:\n\n1. Our patch for the @ghostery/adblocker-electron package is licensed under the Mozilla Public License 2.0 (MPL-2.0), consistent with the upstream project's licensing.",
      "default_branch": "main"
    },
    "fetched_at": "2025-11-15T02:24:45.308492"
  },
  {
    "basic_info": {
      "name": "react-native-godot",
      "full_name": "borndotcom/react-native-godot",
      "owner": "borndotcom",
      "description": "React Native Godot - Embed Godot Engine in React Native apps",
      "url": "https://github.com/borndotcom/react-native-godot",
      "clone_url": "https://github.com/borndotcom/react-native-godot.git",
      "ssh_url": "git@github.com:borndotcom/react-native-godot.git",
      "homepage": "",
      "created_at": "2025-11-01T10:54:51Z",
      "updated_at": "2025-11-14T18:04:14Z",
      "pushed_at": "2025-11-07T13:56:23Z"
    },
    "stats": {
      "stars": 2369,
      "forks": 96,
      "watchers": 2369,
      "open_issues": 6,
      "size": 43456
    },
    "tech_info": {
      "language": "C++",
      "languages": {
        "C++": 96704,
        "Objective-C++": 44758,
        "TypeScript": 31195,
        "Java": 29411,
        "JavaScript": 13042,
        "C": 6699,
        "Ruby": 4547,
        "Objective-C": 4216,
        "Python": 4120,
        "Shell": 3800,
        "GDScript": 3456,
        "Kotlin": 2881,
        "CMake": 2267,
        "Swift": 1232
      },
      "license": "MIT License",
      "topics": [
        "android",
        "godot",
        "godot-engine",
        "ios",
        "react-native"
      ]
    },
    "content": {
      "readme": "![Cover-21](https://github.com/user-attachments/assets/770e4972-84f7-433e-87db-6391601256ba)\nBorn React Native Godot\n-----------------------\n\nReact Native Godot allows embedding the Godot Engine into React Native applications.\n\nBorn React Native Godot was created by [Born](https://born.com) and developed by [Migeran](https://migeran.com), in close collaboration between the two teams.\n\n# Main Features\n\n* Supports Android and iOS, built on [LibGodot](https://github.com/migeran/libgodot).\n* Stable implementation serving millions of users in [Born](https://born.com)'s applications.\n* Supports starting, stopping and restarting the Godot Engine. [(docs)](#initialize-the-godot-instance)\n* When restarting, the engine can be reconfigured, so a different Godot app may be loaded each time. [(docs)](#stop-the-godot-instance)\n* It is also possible to pause and resume the running Godot instance. [(docs)](#pause-the-godot-instance)\n* Godot is running on a separate thread, so it does not affect the main thread of the application nor the React Native JavaScript thread. [(docs)](#threading-and-javascript-in-react-native)\n* The Godot main window and any subwindows created by the Godot app may be embedded into the React Native application either on the same screen, or on separate screens (see [example app](example/)).\n* The whole Godot API is accessible from TypeScript / JavaScript. It is possible to instantiate objects, call methods, get and set properties, attach JS functions to signals, provide JS functions as callables to Godot methods ... etc. [(docs)](#godot-api-usage)\n\n<p align=\"center\">\n  <video src=\"https://github.com/user-attachments/assets/33266f05-d733-4c1d-ab49-edaaf426e3e1\" width=\"600\" controls></video>\n</p>\n\n# Getting Started with the Example App\n\nThe [example app](example/) shows the main features of React Native Godot in action.\n\n## Install Prerequisites\n\nDuring development we use [ASDF](https://asdf-vm.com/) to manage most external dependencies required for React Native development, like Node, Java, Gradle or Ruby. If you also use ASDF, just run:\n\n```sh\nasdf install\n```\n    \nThis will make sure that all the dependencies are the same like in our environment. Otherwise you may also install React Native prerequisites using any other method.\n\n## Export the Godot app\n\nRun the following scripts for either platform you plan to test (or both):\n\n```sh\ncd example\n./export_godot_GodotTest.sh android\n./export_godot_GodotTest.sh ios\n./export_godot_GodotTest2.sh android\n./export_godot_GodotTest2.sh ios\n```\n\nThe script is configured to look for Godot in the standard system wide installation folder on macOS. If your Godot is installed elsewhere, or you are on Linux, just point the `GODOT_EDITOR`\nenvironment variable to your Godot editor prior to running the above scripts:\n\n```sh\nexport GODOT_EDITOR=/path/to/godot_editor\n```\n\n## Configure and download LibGodot\n\n```sh\ncd example\nyarn\nyarn download-prebuilt\n```\n\nThese commands will resolve all the React Native and other dependencies from npm. The second one will download the prebuilt LibGodot release from GitHub.\n\n## Run on the iOS Simulator\n\n```sh\ncd example/ios\nbundle install\nbundle exec pod install\ncd ..\nyarn ios\n```\n\n## Run on the Android Emulator\n\n```sh\ncd example\nyarn android\n```\n\n## Use your native IDEs\n\nYou may use Xcode and Android Studio the same way as with any other project. Just open:\n\n* ``ios/GodotTest.xcworkspace`` from Xcode\n* ``android`` from Android Studio\n\n> [!note]\n> If you are using ASDF to manage your Java and Node dependencies, you should start Android Studio from under the `react-native-godot` (or `example`) folder, so it can find these tools. For example on macOS:\n\n```sh\ncd example\nopen -a \"Android Studio\"\n```\n\n## Convenience script for dependency management\n\nThere is an `update_deps.sh` script included in the example app's folder. It will execute all the setup commands for both iOS and Android in one step, so you may start your work immediately.\n\n```sh\ncd example\n./update_deps.sh\nyarn ios # or yarn android\n```\n\n# Your first React Native Godot App\n\nBorn React Native Godot is distributed on npm.\n\nJust follow these steps to add it to your React Native application:\n\n## Update `package.json`\n\n```sh\nyarn add @borndotcom/react-native-godot\n```\n\n## Download the prebuilt LibGodot packages\n\nThe LibGodot packages used by React Native Godot are not distributed on npm. Instead, they are downloaded separately by issuing the following command:\n\n```sh\nyarn download-prebuilt\n```\n\nThis way React Native Godot can be updated independently from LibGodot, and also local, customized builds of LibGodot are supported.\n\n## Import React Native Godot in your App code\n\n```typescript\nimport { RTNGodot, RTNGodotView, runOnGodotThread } from \"@borndotcom/react-native-godot\";\n```\n\n## Add the Godot View to your view, e.g.\n\n```tsx\nconst App = () => {\n  return (\n    <View>\n      <RTNGodotView style={...}/>\n    </View>\n  );\n};\n```\n\nIf no `windowName` property is specified, that view is fo",
      "default_branch": "master"
    },
    "fetched_at": "2025-11-15T02:24:46.593108"
  },
  {
    "basic_info": {
      "name": "awesome-claude-skills",
      "full_name": "BehiSecc/awesome-claude-skills",
      "owner": "BehiSecc",
      "description": "A curated list of Claude Skills.",
      "url": "https://github.com/BehiSecc/awesome-claude-skills",
      "clone_url": "https://github.com/BehiSecc/awesome-claude-skills.git",
      "ssh_url": "git@github.com:BehiSecc/awesome-claude-skills.git",
      "homepage": null,
      "created_at": "2025-10-17T15:05:35Z",
      "updated_at": "2025-11-15T02:18:16Z",
      "pushed_at": "2025-11-14T06:26:44Z"
    },
    "stats": {
      "stars": 2192,
      "forks": 150,
      "watchers": 2192,
      "open_issues": 1,
      "size": 36
    },
    "tech_info": {
      "language": null,
      "languages": {},
      "license": null,
      "topics": []
    },
    "content": {
      "readme": "# Awesome Claude Skills\n\n## üìö Table of Contents  \n- [Document Skills](#-document-skills)  \n- [Development & Code Tools](#-development--code-tools)  \n- [Data & Analysis](#-data--analysis)  \n- [Scientific & Research Tools](#-scientific--research-tools)  \n- [Writing & Research](#-writing--research)  \n- [Learning & Knowledge](#-learning--knowledge)  \n- [Media & Content](#-media--content)  \n- [Collaboration & Project Management](#-collaboration--project-management)  \n- [Security & Web Testing](#-security--web-testing)  \n- [Utility & Automation](#-utility--automation)\n\n\n\n## üìÑ Document Skills  \n- [docx](https://github.com/anthropics/skills/tree/main/document-skills/docx) - Create, edit, analyze Word docs with tracked changes, comments, formatting.  \n- [pdf](https://github.com/anthropics/skills/tree/main/document-skills/pdf) - Extract text, tables, metadata, merge & annotate PDFs.  \n- [pptx](https://github.com/anthropics/skills/tree/main/document-skills/pptx) - Read, generate, and adjust slides, layouts, templates.  \n- [xlsx](https://github.com/anthropics/skills/tree/main/document-skills/xlsx) - Spreadsheet manipulation: formulas, charts, data transformations.  \n\n\n\n## üõ† Development & Code Tools\n- [artifacts-builder](https://github.com/anthropics/skills/tree/main/artifacts-builder) - Suite of tools for creating elaborate, multi-component claude.ai HTML artifacts using modern frontend web technologies (React, Tailwind CSS, shadcn/ui).\n- [test-driven-development](https://github.com/obra/superpowers/tree/main/skills/test-driven-development) - Use when implementing any feature or bugfix, before writing implementation code\n- [using-git-worktrees](https://github.com/obra/superpowers/blob/main/skills/using-git-worktrees/) - Creates isolated git worktrees with smart directory selection and safety verification.\n- [finishing-a-development-branch](https://github.com/obra/superpowers/tree/main/skills/finishing-a-development-branch) - Guides completion of development work by presenting clear options and handling chosen workflow.\n- [pypict-claude-skill](https://github.com/omkamal/pypict-claude-skill) - Design comprehensive test cases using PICT (Pairwise Independent Combinatorial Testing) for requirements or code, generating optimized test suites with pairwise coverage.\n- [aws-skills](https://github.com/zxkane/aws-skills) - AWS development with CDK best practices, cost optimization MCP servers, and serverless/event-driven architecture patterns.\n- [move-code-quality-skill](https://github.com/1NickPappas/move-code-quality-skill) - Analyzes Move language packages against the official Move Book Code Quality Checklist for Move 2024 Edition compliance and best practices.\n- [claude-code-terminal-title](https://github.com/bluzername/claude-code-terminal-title) - Gives each Claud Code terminal window a dynamic title that describes the work being done so you don't lose track of what terminal window is doing what.\n\n\n\n## üìä Data & Analysis  \n- [root-cause-tracing](https://github.com/obra/superpowers/tree/main/skills/root-cause-tracing) - Use when errors occur deep in execution and you need to trace back to find the original trigger \n- [csv-data-summarizer-claude-skill](https://github.com/coffeefuelbump/csv-data-summarizer-claude-skill) - Automatically analyzes CSVs: columns, distributions, missing data, correlations.\n\n\n\n## üî¨ Scientific & Research Tools\n- [scientific-databases](https://github.com/K-Dense-AI/claude-scientific-skills/tree/main/scientific-databases) - Access to 26 scientific databases including PubMed, PubChem, UniProt, ChEMBL, and AlphaFold DB.\n- [scientific-integrations](https://github.com/K-Dense-AI/claude-scientific-skills/tree/main/scientific-integrations) - Platform integrations for lab automation and workflow management (Benchling, DNAnexus, Opentrons, and more).\n- [scientific-packages](https://github.com/K-Dense-AI/claude-scientific-skills/tree/main/scientific-packages) - 58 specialized Python packages for bioinformatics, cheminformatics, machine learning, and data analysis.\n- [scientific-thinking](https://github.com/K-Dense-AI/claude-scientific-skills/tree/main/scientific-thinking) - Analysis tools and document processing for scientific writing, visualization, and methodology.\n\n\n\n## ‚úçÔ∏è Writing & Research  \n- [article-extractor](https://github.com/michalparkola/tapestry-skills-for-claude-code/tree/main/article-extractor) - Extract full article text and metadata from web pages.\n- [content-research-writer](https://github.com/ComposioHQ/awesome-claude-skills/tree/master/content-research-writer) - Assists in writing high-quality content by conducting research, adding citations, improving hooks, iterating on outlines, and providing real-time feedback on each section\n-  [internal-comms](https://github.com/anthropics/skills/tree/main/internal-comms) - Create internal communications\t(status reports, leadership updates, etc)\n- [brainstorming](https://github.com/obra/superpowers/tree/main/skills/brainstorming) - Transform rough ide",
      "default_branch": "main"
    },
    "fetched_at": "2025-11-15T02:24:47.891777"
  },
  {
    "basic_info": {
      "name": "awesome-claude-skills",
      "full_name": "travisvn/awesome-claude-skills",
      "owner": "travisvn",
      "description": "A curated list of awesome Claude Skills, resources, and tools for customizing Claude AI workflows ‚Äî particularly Claude Code",
      "url": "https://github.com/travisvn/awesome-claude-skills",
      "clone_url": "https://github.com/travisvn/awesome-claude-skills.git",
      "ssh_url": "git@github.com:travisvn/awesome-claude-skills.git",
      "homepage": "",
      "created_at": "2025-10-16T20:42:39Z",
      "updated_at": "2025-11-15T01:24:18Z",
      "pushed_at": "2025-11-14T00:25:01Z"
    },
    "stats": {
      "stars": 1975,
      "forks": 120,
      "watchers": 1975,
      "open_issues": 2,
      "size": 153
    },
    "tech_info": {
      "language": null,
      "languages": {},
      "license": null,
      "topics": [
        "agentic-coding",
        "anthropic",
        "awesome",
        "awesome-list",
        "awesome-lists",
        "claude",
        "claude-ai",
        "claude-code",
        "claude-desktop",
        "claude-skills",
        "claudeskills"
      ]
    },
    "content": {
      "readme": "<p align=\"center\">\n  <a href=\"https://github.com/travisvn/awesome-claude-skills\">\n    <img alt=\"Awesome Claude Skills\" src=\"https://pc0o4oduww.ufs.sh/f/crfz5GypRfo0lI4924gMSJKLY6297aVP0zZpilXBvqTbDyrs\"/>\n  </a>\n</p>\n\n# Awesome Claude Skills\n\n[![Awesome](https://awesome.re/badge.svg)](https://awesome.re)\n[![Last Updated](https://img.shields.io/badge/updated-Nov%202025-green.svg)]()\n[![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg)](CONTRIBUTING.md)\n[![License](https://img.shields.io/badge/license-CC0--1.0-blue.svg)](LICENSE)\n\n> A curated list of awesome Claude Skills, resources, and tools for customizing Claude AI workflows\n\n**Claude Skills** teach Claude how to **perform tasks in a repeatable way**\n\nThey are specialized folders containing instructions, scripts, and resources that Claude dynamically discovers and loads when relevant to tasks.\n\n### How Skills Work\n\nSkills employ a **progressive disclosure architecture** for efficiency:\n\n1. **Metadata loading** (~100 tokens): Claude scans available Skills to identify relevant matches\n2. **Full instructions** (<5k tokens): Load when Claude determines the Skill applies\n3. **Bundled resources**: Files and executable code load only as needed\n\nThis design allows multiple Skills to remain available without overwhelming Claude's context window.\n\n## üöÄ Quick Start\n\n### Claude Code\n\n```bash\n/plugin marketplace add anthropics/skills\n```\n\n### Claude Desktop\n\n[Enable Skills here](https://claude.ai/settings/capabilities)\n\n## üõ†Ô∏è Installation & Setup\n\n### Claude.ai Web Interface\n\n1. Go to [Settings > Capabilities](https://claude.ai/settings/capabilities)\n2. Enable Skills toggle\n3. Browse available skills or upload custom skills\n4. **For Team/Enterprise**: Admin must enable Skills organization-wide first\n\n### Claude Code CLI\n\n```bash\n# Install skills from marketplace\n/plugin marketplace add anthropics/skills\n\n# Or install from local directory\n/plugin add /path/to/skill-directory\n```\n\n### Claude API\n\nSkills are accessible via the `/v1/skills` API endpoint. See the [Skills API documentation](https://docs.claude.com/en/api/skills) for detailed integration examples.\n\n```python\nimport anthropic\n\nclient = anthropic.Client(api_key=\"your-api-key\")\n# See API docs for full implementation details\n```\n\n## üéØ Official Skills\n\n### Document Skills\n\nSkills for working with complex file formats:\n\n- **[docx](https://github.com/anthropics/skills/tree/main/document-skills/docx)** - Create, edit, and analyze Word documents with support for tracked changes, comments, formatting preservation, and text extraction\n- **[pdf](https://github.com/anthropics/skills/tree/main/document-skills/pdf)** - Comprehensive PDF manipulation toolkit for extracting text and tables, creating new PDFs, merging/splitting documents, and handling forms\n- **[pptx](https://github.com/anthropics/skills/tree/main/document-skills/pptx)** - Create, edit, and analyze PowerPoint presentations with support for layouts, templates, charts, and automated slide generation\n- **[xlsx](https://github.com/anthropics/skills/tree/main/document-skills/xlsx)** - Create, edit, and analyze Excel spreadsheets with support for formulas, formatting, data analysis, and visualization\n\n### Design & Creative\n\n- **[algorithmic-art](https://github.com/anthropics/skills/tree/main/algorithmic-art)** - Create generative art using p5.js with seeded randomness, flow fields, and particle systems\n- **[canvas-design](https://github.com/anthropics/skills/tree/main/canvas-design)** - Design beautiful visual art in .png and .pdf formats using design philosophies\n- **[slack-gif-creator](https://github.com/anthropics/skills/tree/main/slack-gif-creator)** - Create animated GIFs optimized for Slack's size constraints\n\n### Development\n\n- **[artifacts-builder](https://github.com/anthropics/skills/tree/main/artifacts-builder)** - Build complex claude.ai HTML artifacts using React, Tailwind CSS, and shadcn/ui components\n- **[mcp-builder](https://github.com/anthropics/skills/tree/main/mcp-builder)** - Guide for creating high-quality MCP servers to integrate external APIs and services\n- **[webapp-testing](https://github.com/anthropics/skills/tree/main/webapp-testing)** - Test local web applications using Playwright for UI verification and debugging\n\n### Communication\n\n- **[brand-guidelines](https://github.com/anthropics/skills/tree/main/brand-guidelines)** - Apply Anthropic's official brand colors and typography to artifacts\n- **[internal-comms](https://github.com/anthropics/skills/tree/main/internal-comms)** - Write internal communications like status reports, newsletters, and FAQs\n\n### Skill Creation\n\n- **[skill-creator](https://github.com/anthropics/skills/tree/main/skill-creator)** - Interactive skill creation tool that guides you through building new skills with Q&A\n\n## üåü Community Skills\n\n> [!Warning]\n> Skills can execute arbitrary code in Claude's environment.\n> \n> See [Security & Best Practices](#-security--best-practices) for more information\n\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-11-15T02:24:49.232149"
  },
  {
    "basic_info": {
      "name": "alt-sendme",
      "full_name": "tonyantony300/alt-sendme",
      "owner": "tonyantony300",
      "description": "Share files and directories anywhere - Local or Global",
      "url": "https://github.com/tonyantony300/alt-sendme",
      "clone_url": "https://github.com/tonyantony300/alt-sendme.git",
      "ssh_url": "git@github.com:tonyantony300/alt-sendme.git",
      "homepage": "https://www.iroh.computer/sendme",
      "created_at": "2025-10-17T09:08:25Z",
      "updated_at": "2025-11-15T02:23:05Z",
      "pushed_at": "2025-11-14T11:38:30Z"
    },
    "stats": {
      "stars": 1909,
      "forks": 87,
      "watchers": 1909,
      "open_issues": 7,
      "size": 65748
    },
    "tech_info": {
      "language": "TypeScript",
      "languages": {
        "TypeScript": 102265,
        "Rust": 92819,
        "CSS": 4383,
        "JavaScript": 2472,
        "HTML": 569
      },
      "license": "GNU Affero General Public License v3.0",
      "topics": [
        "encryption",
        "file-sharing",
        "file-transfer",
        "ftp",
        "iroh",
        "localsend-alternative",
        "open-source",
        "p2p",
        "p2p-network",
        "privacy",
        "scp",
        "sftp",
        "tauri",
        "wetransfer-alternative"
      ]
    },
    "content": {
      "readme": "<div align=\"center\">\n\n# File transfer doesn't need to be complicated\n\n</div>\n\n![AltSendme Header](assets/header.png)\n\n<div align=\"center\">\n\n![AltSendme working demo](assets/animation.gif)\n\n</div>\n\n<div align=\"center\">\n\n![Version][badge-version]\n![Website][badge-website]\n![Platforms][badge-platforms]\n[![Sponsor][badge-sponsor]](https://github.com/sponsors/tonyantony300)\n[![Hire developer][badge-hire]](mailto:tnyantny@protonmail.com?subject=Looking%20to%20hire)\n\n\n</div>\n\n\n\nA free and open-source file transfer tool that harnesses the power of [cutting-edge peer-to-peer networking](https://www.iroh.computer), letting you transfer files directly without storing them on cloud servers.\n\nWhy rely on WeTransfer, Dropbox, or Google Drive when you can reliably and easily transfer files directly, end-to-end encrypted and without revealing any personal information?\n\n\n## Features\n\n- **Send anywhere** ‚Äì Works seamlessly on local networks or across continents.\n- **Peer-to-peer direct transfer** ‚Äì Send files straight between devices, with no cloud servers or intermediaries.\n- [**End-to-end encryption**](https://www.iroh.computer/docs/faq#how-secure-is-irohs-end-to-end-encryption) ‚Äì Always-on protection with QUIC + TLS 1.3 for forward and backward secrecy.\n- **No accounts or personal info** ‚Äì Transfer files without sign-ups or exposing private data.\n- [**Transfer anything**](https://www.iroh.computer/proto/iroh-blobs) ‚Äì Send files or directories of any size, verified with BLAKE3-based integrity checks.\n- **Resumable transfers** ‚Äì Interrupted downloads automatically resume where they left off.\n- **Fast & reliable** ‚Äì Capable of saturating multi-gigabit connections for lightning-fast transfers.\n- [**NAT traversal via QUIC**](https://www.iroh.computer/docs/faq#does-iroh-use-relay-servers) ‚Äì Secure, low-latency connections using QUIC hole punching with encrypted relay fallback.\n- **CLI integration** ‚Äì Interoperable with the [Sendme CLI](https://www.iroh.computer/sendme).\n- **Mobile & web** ‚Äì Coming soon.\n- **Free & open source** ‚Äì No upload costs, no size limits, and fully community-driven.\n\n\n\n## Installation\n\nThe easiest way to get started is by downloading one of the following versions for your respective operating system:\n\n<table>\n  <tr>\n    <td><b>Platform</b></td>\n    <td><b>Download</b></td>\n  </tr>\n  <tr>\n    <td><b>Windows</b></td>\n    <td><a href='https://github.com/tonyantony300/alt-sendme/releases/download/v0.2.1/AltSendme_0.2.0_x64-setup.exe'>AltSendme.exe</a></td>\n  </tr>\n  <tr>\n    <td><b>macOS</b></td>\n    <td><a href='https://github.com/tonyantony300/alt-sendme/releases/download/v0.2.1/AltSendme_0.2.0_universal.dmg'>AltSendme.dmg</a></td>\n  <tr>\n    <td><b>Linux </b></td>\n    <td><a href='https://github.com/tonyantony300/alt-sendme/releases/download/v0.2.1/AltSendme_0.2.0_amd64.deb'>AltSendme.deb</a></td>\n  </tr>\n</table>\n\n\n*For running on macOS, see [macOS Installation (Unsigned App)](#macos-installation) below.\n\n\nMore download options in [GitHub Releases](https://github.com/tonyantony300/alt-sendme/releases).\n\n\n\n### macOS Installation \n\nAlt-Sendme is currently distributed **without code signing**. When you first open it, macOS will show a security warning saying the application is damaged. Here is how you can run it:\n\n\n**Remove Quarantine Flag (Terminal)**\n\n1. Open Spotlight Search (‚åò+space) - search for terminal and open\n2. Paste the following command and hit enter\n\n```bash\ncd /Applications && xattr -dr com.apple.quarantine AltSendme.app\n```\n\nThe app is open source and safe - the warning is just macOS's gatekeeper for unsigned applications. Code signing will be added in future releases.\n\n\n## CLI Compatibility\n\nThe AltSendme uses the same core networking library as the [sendme](https://www.iroh.computer/sendme), so:\n- Tickets generated by CLI can be used in desktop\n- Tickets generated by desktop can be used in CLI\n- Both versions can send/receive to each other\n\n\n\n## Supported Languages\n\n\n- üá´üá∑ French\n- üáπüá≠ Thai\n- üá©üá™ German\n- üá®üá≥ Chinese\n- üáØüáµ Japanese\n- üá∑üá∫ Russian\n\n\n\n\n## Development\n\nIf you want to contribute or run the app from source:\n\n### Prerequisites\n\n- Rust 1.81+\n- Node.js 18+\n- npm or yarn\n\n### Running in Development\n\n1. **Install frontend dependencies**:\n   ```bash\n   cd web-app\n   npm install\n   ```\n\n2. **Run the desktop app**:\n   ```bash\n   cd src-tauri\n   cargo tauri dev\n   ```\n\nThis will start the app with hot reload enabled for both frontend and backend changes.\n\n\n### Building Locally\n\n\n 1. **Build stage**:\n   ```bash\n   cd src-tauri\n   cargo tauri build --no-bundle\n   ```\n 2. **Run**:\n\n  ```bash\n   cd src-tauri/target/release\n   ./alt-sendme        # macOS or Linux\n   alt-sendme.exe      # Windows\n   ```\n\n\n## License\n\nAGPL-3.0\n\n## Privacy Policy\n\nSee [PRIVACY.md](PRIVACY.md) for information about how AltSendme handles your data and privacy.\n\n[![Sponsor](https://img.shields.io/badge/sponsor-30363D?style=for-the-badge&logo=GitHub-Sponsors&logoColor=#EA4AAA)](https://github.com/sponsors/tonyantony300) [!",
      "default_branch": "main"
    },
    "fetched_at": "2025-11-15T02:24:50.510876"
  },
  {
    "basic_info": {
      "name": "deepseek-ocr.rs",
      "full_name": "TimmyOVO/deepseek-ocr.rs",
      "owner": "TimmyOVO",
      "description": "Rust implementation of DeepSeek-OCR with OpenAI-compatible server & CLI No Python environment needed - just download and run.",
      "url": "https://github.com/TimmyOVO/deepseek-ocr.rs",
      "clone_url": "https://github.com/TimmyOVO/deepseek-ocr.rs.git",
      "ssh_url": "git@github.com:TimmyOVO/deepseek-ocr.rs.git",
      "homepage": "",
      "created_at": "2025-10-25T13:42:10Z",
      "updated_at": "2025-11-14T20:34:49Z",
      "pushed_at": "2025-11-14T19:02:49Z"
    },
    "stats": {
      "stars": 1868,
      "forks": 142,
      "watchers": 1868,
      "open_issues": 6,
      "size": 1071
    },
    "tech_info": {
      "language": "Rust",
      "languages": {
        "Rust": 814831,
        "Python": 62963,
        "Dockerfile": 1057
      },
      "license": "Apache License 2.0",
      "topics": [
        "candle",
        "ocr",
        "ocr-recognition",
        "openai",
        "rust"
      ]
    },
    "content": {
      "readme": "# deepseek-ocr.rs üöÄ\n\nRust implementation of the DeepSeek-OCR inference stack with a fast CLI and an OpenAI-compatible HTTP server. The workspace packages multiple OCR backends, prompt tooling, and a serving layer so you can build document understanding pipelines that run locally on CPU, Apple Metal, or (alpha) NVIDIA CUDA GPUs.\n\n> ‰∏≠ÊñáÊñáÊ°£ËØ∑Áúã [README_CN.md](README_CN.md)„ÄÇ  \n\n> Want ready-made binaries? Latest macOS (Metal-enabled) and Windows bundles live in the [build-binaries workflow artifacts](https://github.com/TimmyOVO/deepseek-ocr.rs/actions/workflows/build-binaries.yml). Grab them from the newest green run.\n\n## Choosing a Model üî¨\n\n| Model | Memory footprint* | Best on | When to pick it |\n| --- | --- | --- | --- |\n| **DeepSeek‚ÄëOCR** | **‚âà6.3‚ÄØGB** FP16 weights, **‚âà13‚ÄØGB** RAM/VRAM with cache & activations (512-token budget) | Apple Silicon + Metal (FP16), high-VRAM NVIDIA GPUs, 32‚ÄØGB+ RAM desktops | Highest accuracy, SAM+CLIP global/local context, MoE DeepSeek‚ÄëV2 decoder (3‚ÄØB params, ~570‚ÄØM active per token). Use when latency is secondary to quality. |\n| **PaddleOCR‚ÄëVL** | **‚âà4.7‚ÄØGB** FP16 weights, **‚âà9‚ÄØGB** RAM/VRAM with cache & activations | 16‚ÄØGB laptops, CPU-only boxes, mid-range GPUs | Dense 0.9‚ÄØB Ernie decoder with SigLIP vision tower. Faster startup, lower memory, great for batch jobs or lightweight deployments. |\n\n\\*Measured from the default FP16 safetensors. Runtime footprint varies with sequence length.\n\nGuidance:\n\n- **Need maximum fidelity, multi-region reasoning, or already have 16‚Äì24‚ÄØGB VRAM?** Use **DeepSeek‚ÄëOCR**. The hybrid SAM+CLIP tower plus DeepSeek‚ÄëV2 MoE decoder handles complex layouts best, but expect higher memory/latency.\n- **Deploying to CPU-only nodes, 16‚ÄØGB laptops, or latency-sensitive services?** Choose **PaddleOCR‚ÄëVL**. Its dense Ernie decoder (18 layers, hidden 1024) activates fewer parameters per token and keeps memory under 10‚ÄØGB while staying close in quality on most docs.\n\n## Why Rust? üí°\n\nThe original DeepSeek-OCR ships as a Python + Transformers stack‚Äîpowerful, but hefty to deploy and awkward to embed. Rewriting the pipeline in Rust gives us:\n\n- Smaller deployable artifacts with zero Python runtime or conda baggage.\n- Memory-safe, thread-friendly infrastructure that blends into native Rust backends.\n- Unified tooling (CLI + server) running on Candle + Rocket without the Python GIL overhead.\n- Drop-in compatibility with OpenAI-style clients while tuned for single-turn OCR prompts.\n\n## Technical Stack ‚öôÔ∏è\n\n- **Candle** for tensor compute, with Metal and CUDA backends and FlashAttention support.\n- **Rocket** + async streaming for OpenAI-compatible `/v1/responses` and `/v1/chat/completions`.\n- **tokenizers** (upstream DeepSeek release) wrapped by `crates/assets` for deterministic caching via Hugging Face and ModelScope mirrors.\n- **Pure Rust vision/prompt pipeline** shared by CLI and server to avoid duplicated logic.\n\n## Advantages over the Python Release ü•∑\n\n- Faster cold-start on Apple Silicon, lower RSS, and native binary distribution.\n- Deterministic dual-source (Hugging Face + ModelScope) asset download + verification built into the workspace.\n- Automatic single-turn chat compaction so OCR outputs stay stable even when clients send history.\n- Ready-to-use OpenAI compatibility for tools like Open WebUI without adapters.\n\n## Highlights ‚ú®\n\n- **One repo, two entrypoints** ‚Äì a batteries-included CLI for batch jobs and a Rocket-based server that speaks `/v1/responses` and `/v1/chat/completions`.\n- **Works out of the box** ‚Äì pulls model weights, configs, and tokenizer from whichever of Hugging Face or ModelScope responds fastest on first run.\n- **Optimised for Apple Silicon** ‚Äì optional Metal backend with FP16 execution for real-time OCR on laptops.\n- **CUDA (alpha)** ‚Äì experimental support via `--features cuda` + `--device cuda --dtype f16`; expect rough edges while we finish kernel coverage.\n- **Intel MKL (preview)** ‚Äì faster BLAS on x86 via `--features mkl` (install Intel oneMKL beforehand).\n- **OpenAI client compatibility** ‚Äì drop-in replacement for popular SDKs; the server automatically collapses chat history to the latest user turn for OCR-friendly prompts.\n\n## Quick Start üèÅ\n\n### Prerequisites\n\n- Rust 1.78+ (edition 2024 support)\n- Git\n- Optional: Apple Silicon running macOS 13+ for Metal acceleration\n- Optional: CUDA 12.2+ toolkit + driver for experimental NVIDIA GPU acceleration on Linux/Windows\n- Optional: Intel oneAPI MKL for preview x86 acceleration (see below)\n- (Recommended) Hugging Face account with `HF_TOKEN` when pulling from the `deepseek-ai/DeepSeek-OCR` repo (ModelScope is used automatically when it‚Äôs faster/reachable).\n\n### Clone the Workspace\n\n```bash\ngit clone https://github.com/TimmyOVO/deepseek-ocr.rs.git\ncd deepseek-ocr.rs\ncargo fetch\n```\n\n### Model Assets\n\nThe first invocation of the CLI or server downloads the config, tokenizer, and `model-00001-of-000001.safetensors` (~6.3GB) into `DeepSeek-OCR/`. To prefetch manually:\n\n```bash\ncargo run -p deepseek-o",
      "default_branch": "master"
    },
    "fetched_at": "2025-11-15T02:24:51.822755"
  },
  {
    "basic_info": {
      "name": "OpenMemory",
      "full_name": "CaviraOSS/OpenMemory",
      "owner": "CaviraOSS",
      "description": "Add long-term memory to any AI in minutes. Self-hosted, open, and framework-free.",
      "url": "https://github.com/CaviraOSS/OpenMemory",
      "clone_url": "https://github.com/CaviraOSS/OpenMemory.git",
      "ssh_url": "git@github.com:CaviraOSS/OpenMemory.git",
      "homepage": "https://openmemory.cavira.app",
      "created_at": "2025-10-19T16:12:49Z",
      "updated_at": "2025-11-15T02:16:44Z",
      "pushed_at": "2025-11-13T17:17:41Z"
    },
    "stats": {
      "stars": 1867,
      "forks": 204,
      "watchers": 1867,
      "open_issues": 1,
      "size": 1019
    },
    "tech_info": {
      "language": "TypeScript",
      "languages": {
        "TypeScript": 521603,
        "JavaScript": 90362,
        "Python": 31947,
        "Makefile": 5382,
        "Dockerfile": 2100,
        "CSS": 1533
      },
      "license": "Apache License 2.0",
      "topics": [
        "ai",
        "ai-agents",
        "ai-infrastructure",
        "ai-memory",
        "artificial-intelligence",
        "cognitive-architecture",
        "embeddings",
        "gemini",
        "llm",
        "long-term-memory",
        "memory",
        "memory-engine",
        "memory-retrieval",
        "ollama",
        "openai",
        "openmemory",
        "rag",
        "supermemory",
        "vector-database"
      ]
    },
    "content": {
      "readme": "<img width=\"1577\" height=\"781\" alt=\"image\" src=\"https://github.com/user-attachments/assets/3baada32-1111-4c2c-bf13-558f2034e511\" />\n\n# OpenMemory\n\nLong-term memory for AI systems. Open source, self-hosted, and explainable.\n\n‚ö†Ô∏è **Upgrading from v1.1?** Multi-user tenant support requires database migration. See [MIGRATION.md](./MIGRATION.md) for upgrade instructions.\n\n[VS Code Extension](https://marketplace.visualstudio.com/items?itemName=Nullure.openmemory-vscode) ‚Ä¢ [Report Bug](https://github.com/caviraOSS/openmemory/issues) ‚Ä¢ [Request Feature](https://github.com/caviraOSS/openmemor/issues) ‚Ä¢ [Discord server](https://discord.gg/P7HaRayqTh)\n\n---\n\n## 1. Overview\n\nOpenMemory gives AI systems persistent memory. It stores what matters, recalls it when needed, and explains why it matters.\n\nUnlike traditional vector databases, OpenMemory uses a cognitive architecture. It organizes memories by type (semantic, episodic, procedural, emotional, reflective), tracks importance over time, and builds associations between related memories.\n\n### Key Features\n\n- **Multi-sector memory** - Different memory types for different content\n- **Automatic decay** - Memories fade naturally unless reinforced\n- **Graph associations** - Memories link to related memories\n- **Temporal knowledge graph** - Time-aware relationships with fact evolution and historical reasoning\n- **Pattern recognition** - Finds and consolidates similar memories\n- **User isolation** - Each user gets a separate memory space\n- **Local or cloud** - Run with your own embeddings or use OpenAI/Gemini\n- **Framework agnostic** - Works with any LLM or agent system\n- **Migration** - Easily migrate from Mem0, Zep and Supermemory.\n\n### Uses\n\n**We are featuring projects that use OpenMemory here. To get your project displayed, please email nullureq@gmail.com**\n\n### VS Code Extension\n\nThe OpenMemory extension tracks your coding activity and gives AI assistants access to your project history.\n\n**[Get it on VS Code Marketplace](https://marketplace.visualstudio.com/items?itemName=Nullure.openmemory-vscode)**\n\nWorks with GitHub Copilot, Cursor, Claude Desktop, Windsurf, and any MCP-compatible AI.\n\nFeatures:\n\n- Tracks file edits, saves, and opens\n- Compresses context to reduce token usage by 30-70%\n- Query responses under 80ms\n- Supports Direct HTTP and MCP protocol modes\n- Zero configuration required\n\n### Architecture\n\nOpenMemory uses Hierarchical Memory Decomposition (HMD):\n\n- One canonical node per memory (no duplication)\n- Multiple embeddings per memory (one per sector)\n- Single-waypoint linking between memories\n- Composite similarity scoring across sectors\n\nThis approach improves recall accuracy while reducing costs.\n\n---\n\n## 2. Competitor Comparison\n\n| **Feature / Metric**                     | **OpenMemory (Our Tests ‚Äì Nov 2025)**                       | **Zep (Their Benchmarks)**         | **Supermemory (Their Docs)**    | **Mem0 (Their Tests)**        | **OpenAI Memory**          | **LangChain Memory**        | **Vector DBs (Chroma / Weaviate / Pinecone)** |\n| ---------------------------------------- | ----------------------------------------------------------- | ---------------------------------- | ------------------------------- | ----------------------------- | -------------------------- | --------------------------- | --------------------------------------------- |\n| **Open-source License**                  | ‚úÖ Apache 2.0                                               | ‚úÖ Apache 2.0                      | ‚úÖ Source available (GPL-like)  | ‚úÖ Apache 2.0                 | ‚ùå Closed                  | ‚úÖ Apache 2.0               | ‚úÖ Varies (OSS + Cloud)                       |\n| **Self-hosted / Local**                  | ‚úÖ Full (Local / Docker / MCP) tested ‚úì                     | ‚úÖ Local + Cloud SDK               | ‚ö†Ô∏è Mostly managed cloud tier    | ‚úÖ Self-hosted ‚úì              | ‚ùå No                      | ‚úÖ Yes (in your stack)      | ‚úÖ Chroma / Weaviate ‚ùå Pinecone (cloud)      |\n| **Per-user namespacing (`user_id`)**     | ‚úÖ Built-in (`user_id` linking added)                       | ‚úÖ Sessions / Users API            | ‚ö†Ô∏è Multi-tenant via API key     | ‚úÖ Explicit `user_id` field ‚úì | ‚ùå Internal only           | ‚úÖ Namespaces via LangGraph | ‚úÖ Collection-per-user schema                 |\n| **Architecture**                         | HSG v3 (Hierarchical Semantic Graph + Decay + Coactivation) | Flat embeddings + Postgres + FAISS | Graph + Embeddings              | Flat vector store             | Proprietary cache          | Context memory utils        | Vector index (ANN)                            |\n| **Avg Response Time (100k nodes)**       | **115 ms avg (measured)**                                   | 310 ms (docs)                      | 200‚Äì340 ms (on-prem/cloud)      | ~250 ms                       | 300 ms (observed)          | 200 ms (avg)                | 160 ms (avg)                                  |\n| **Throughput (QPS)**                     | **338 QPS avg (8 workers, ",
      "default_branch": "main"
    },
    "fetched_at": "2025-11-15T02:24:53.102193"
  },
  {
    "basic_info": {
      "name": "open-agent-builder",
      "full_name": "firecrawl/open-agent-builder",
      "owner": "firecrawl",
      "description": "üî• Visual workflow builder for AI agents powered by Firecrawl - drag-and-drop web scraping pipelines with real-time execution",
      "url": "https://github.com/firecrawl/open-agent-builder",
      "clone_url": "https://github.com/firecrawl/open-agent-builder.git",
      "ssh_url": "git@github.com:firecrawl/open-agent-builder.git",
      "homepage": null,
      "created_at": "2025-10-16T15:34:46Z",
      "updated_at": "2025-11-14T20:25:58Z",
      "pushed_at": "2025-10-20T15:15:47Z"
    },
    "stats": {
      "stars": 1845,
      "forks": 333,
      "watchers": 1845,
      "open_issues": 10,
      "size": 1104
    },
    "tech_info": {
      "language": "TypeScript",
      "languages": {
        "TypeScript": 1583973,
        "CSS": 48176,
        "JavaScript": 4757
      },
      "license": null,
      "topics": []
    },
    "content": {
      "readme": "# Open Agent Builder\n\n<p align=\"center\">\n  <img src=\"https://media4.giphy.com/media/v1.Y2lkPTc5MGI3NjExcGNoY25xY2ptZTZtcDN6czBmdXJ2dnpkdWVjcXlqNXNhdjgyZXpkaiZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/tWtopK29eXAbvaDpi5/giphy.gif\" alt=\"Demo\" width=\"100%\" />\n</p>\n\n<div align=\"center\">\n\n**Build, test, and deploy AI agent workflows with a visual no-code interface**\n\n[![MIT License](https://img.shields.io/badge/License-MIT-green.svg)](https://choosealicense.com/licenses/mit/)\n[![Firecrawl](https://img.shields.io/badge/Powered%20by-Firecrawl-orange)](https://firecrawl.dev)\n\n[Documentation](#documentation) ‚Ä¢ [Examples](#example-workflows)\n\n</div>\n\n---\n\n## What is Open Agent Builder?\n\nOpen Agent Builder is a visual workflow builder for creating AI agent pipelines powered by [Firecrawl](https://firecrawl.dev). Design complex agent workflows with a drag-and-drop interface, then execute them with real-time streaming updates.\n\n**Perfect for:**\n- Web scraping and data extraction workflows\n- Multi-step AI agent pipelines\n- Automated research and content generation\n- Data transformation and analysis\n- Web automation with human-in-the-loop approvals\n\n> **Note:** This project is actively under development. Some features are still in progress and we welcome contributions and PRs!\n\n---\n\n## Key Features\n\n### Visual Workflow Builder\n- **Drag-and-drop interface** for building agent workflows\n- **Real-time execution** with streaming updates\n- **8 core node types**: Start, Agent, MCP Tools, Transform, If/Else, While Loop, User Approval, End\n- **Template library** with pre-built workflows\n- **MCP protocol support** for extensible tool integration\n\n### Powered by Firecrawl\n- **Native Firecrawl integration** for web scraping and searching\n\n### Enterprise Features\n- **LangGraph execution engine** for reliable state management\n- **Clerk authentication** for secure multi-user access\n- **Convex database** for persistent storage\n- **API endpoints** for programmatic execution\n- **Human-in-the-loop** approvals for sensitive operations\n\n---\n\n## Tech Stack\n\n| Technology | Purpose |\n|-----------|---------|\n| **[Firecrawl](https://firecrawl.dev)** | Web scraping API for converting websites into LLM-ready data |\n| **[Next.js 16 (canary)](https://nextjs.org/)** | React framework with App Router for frontend and API routes |\n| **[TypeScript](https://www.typescriptlang.org/)** | Type-safe development across the stack |\n| **[LangGraph](https://github.com/langchain-ai/langgraph)** | Workflow orchestration engine with state management, conditional routing, and human-in-the-loop support |\n| **[Convex](https://convex.dev)** | Real-time database with automatic reactivity for workflows, executions, and user data |\n| **[Clerk](https://clerk.com)** | Authentication and user management with JWT integration |\n| **[Tailwind CSS](https://tailwindcss.com/)** | Utility-first CSS framework for responsive UI |\n| **[React Flow](https://reactflow.dev/)** | Visual workflow builder canvas with drag-and-drop nodes |\n| **[Anthropic](https://www.anthropic.com/)** | Claude AI integration with native MCP support (Claude Haiku 4.5 & Sonnet 4.5) |\n| **[OpenAI](https://platform.openai.com/)** | gpt-5 integration (MCP support coming soon) |\n| **[Groq](https://groq.com/)** | Fast inference for open models (MCP support coming soon) |\n| **[E2B](https://e2b.dev)** | Sandboxed code execution for secure transform nodes |\n| **[Vercel](https://vercel.com)** | Deployment platform with edge functions |\n\n---\n\n## Prerequisites\n\nBefore you begin, you'll need:\n\n1. **Node.js 18+** installed on your machine\n2. **Firecrawl API key** (Required for web scraping) - [Get one here](https://firecrawl.dev)\n3. **Convex account** - [Sign up free](https://convex.dev)\n4. **Clerk account** - [Sign up free](https://clerk.com)\n\n> **Note:** LLM API keys can be added directly in the UI via Settings ‚Üí API Keys after setup. For MCP tool support, Anthropic Claude (Haiku 4.5 or Sonnet 4.5) is currently recommended as the default option.\n\n---\n\n## Installation & Setup\n\n### 1. Clone the Repository\n\n```bash\ngit clone https://github.com/firecrawl/open-agent-builder.git\ncd open-agent-builder\nnpm install\n```\n\n### 2. Set Up Convex (Database)\n\nConvex handles all workflow and execution data persistence.\n\n```bash\n# Install Convex CLI globally\nnpm install -g convex\n\n# Initialize Convex project\nnpx convex dev\n```\n\nThis will:\n- Open your browser to create/link a Convex project\n- Generate a `NEXT_PUBLIC_CONVEX_URL` in your `.env.local`\n- Start the Convex development server\n\nKeep the Convex dev server running in a separate terminal.\n\n### 3. Set Up Clerk (Authentication)\n\nClerk provides secure user authentication and management.\n\n1. Go to [clerk.com](https://clerk.com) and create a new application\n2. In your Clerk dashboard:\n   - Go to **API Keys**\n   - Copy your keys\n3. Go to **JWT Templates** ‚Üí **Convex**:\n   - Click \"Apply\"\n   - Copy the issuer URL\n\nAdd to your `.env.local`:\n\n```bash\n# Clerk Authentication\nNEXT_PUBLIC_CLERK_PUBLI",
      "default_branch": "main"
    },
    "fetched_at": "2025-11-15T02:24:54.385013"
  },
  {
    "basic_info": {
      "name": "omnilingual-asr",
      "full_name": "facebookresearch/omnilingual-asr",
      "owner": "facebookresearch",
      "description": "Omnilingual ASR Open-Source Multilingual SpeechRecognition for 1600+ Languages",
      "url": "https://github.com/facebookresearch/omnilingual-asr",
      "clone_url": "https://github.com/facebookresearch/omnilingual-asr.git",
      "ssh_url": "git@github.com:facebookresearch/omnilingual-asr.git",
      "homepage": null,
      "created_at": "2025-11-06T22:38:00Z",
      "updated_at": "2025-11-15T00:40:54Z",
      "pushed_at": "2025-11-13T20:34:22Z"
    },
    "stats": {
      "stars": 1810,
      "forks": 133,
      "watchers": 1810,
      "open_issues": 14,
      "size": 1021
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 295799
      },
      "license": "Other",
      "topics": []
    },
    "content": {
      "readme": "<div align=\"center\">\n  <img src=\"./omniASR_header.jpg\" alt=\"Header image with a collage of on-the-ground photos from the transcription gathering efforts in Pakistan and Liberia.\" width=\"100%\" />\n  <p><i>Photographs captured during corpus creation efforts in Pakistan and Liberia.</i></p>\n</div>\n\n# Omnilingual ASR: Open-Source Multilingual Speech Recognition for 1600+ Languages\n\nOmnilingual ASR is an open-source speech recognition system supporting over 1,600 languages ‚Äî including hundreds never previously covered by any ASR technology. Designed for broad accessibility, it enables new languages to be added with just a few paired examples without requiring specialized expertise or large datasets. By combining scalable zero-shot learning with a flexible model family, Omnilingual ASR aims to make speech technology more inclusive and adaptable for communities and researchers worldwide.\n\n* [Huggingface Demo](https://huggingface.co/spaces/facebook/omniasr-transcriptions)\n* [Huggingface Dataset](https://huggingface.co/datasets/facebook/omnilingual-asr-corpus)\n* [Paper](https://ai.meta.com/research/publications/omnilingual-asr-open-source-multilingual-speech-recognition-for-1600-languages/)\n* [Blogpost](http://ai.meta.com/blog/omnilingual-asr-advancing-automatic-speech-recognition)\n\n<div align=\"center\">\n  <img src=\"./result_table.png\" alt=\"Performance results table\" width=\"100%\" />\n  <p><i>Our 7B-LLM-ASR system achieves state-of-the-art performance across 1,600+ languages, with character error rates (CER) below 10 for 78% of those languages.</i></p>\n</div>\n\n\n## Documentation\n\n### Quick Start\n- **[Installation & Basic Usage](#installation)** - Setup and first transcription\n- **[Inference Pipeline](src/omnilingual_asr/models/inference/README.md)** - Comprehensive transcription guide with batch processing, language conditioning, and context examples\n- **[Supported Languages](#supported-languages)** - View the complete list of 1600+ supported languages\n\n\n### Models & Architecture\n- **[Model Specifications](#model-architectures)** - Available models, parameters, and memory requirements\n- **[Architecture Overview](src/omnilingual_asr/models/README.md)** - Technical details on W2V, CTC, and LLM model families\n- **[Asset Management](src/omnilingual_asr/cards/README.md)** - Configuration system for models, tokenizers, and datasets\n\n### Training & Data Pipeline\n- **[Data Preparation](workflows/dataprep/README.md)** - End-to-end guide for multilingual dataset preparation, HuggingFace integration, and parquet processing\n- **[Training Recipes](workflows/recipes/wav2vec2/asr/README.md)** - Pre-configured workflows for CTC and LLM model training\n\n---\n\n## Installation\n\nThe models were developed using [fairseq2](https://github.com/facebookresearch/fairseq2), a research-focused sequence modeling toolkit. While we provide a **reference** inference pipeline that works across platforms, audio support requires [libsndfile](https://github.com/facebookresearch/fairseq2?tab=readme-ov-file#system-dependencies) (Mac: `brew install libsndfile`; Windows may need an additional [setup](https://github.com/facebookresearch/fairseq2?tab=readme-ov-file#installing-on-windows)).\n\n```bash\n# using pip\npip install omnilingual-asr\n\n# using uv\nuv add omnilingual-asr\n```\n\n## Inference\n\n```python\nfrom omnilingual_asr.models.inference.pipeline import ASRInferencePipeline\n\npipeline = ASRInferencePipeline(model_card=\"omniASR_LLM_7B\")\n\naudio_files = [\"/path/to/eng_audio1.flac\", \"/path/to/deu_audio2.wav\"]\nlang = [\"eng_Latn\", \"deu_Latn\"]\ntranscriptions = pipeline.transcribe(audio_files, lang=lang, batch_size=2)\n```\n\nMore details on running specific models can be found in the [src/omnilingual_asr/models/inference](/src/omnilingual_asr/models/inference/README.md) directory.\n\n> **‚ö†Ô∏è Important:** Currently only audio files shorter than 40 seconds are accepted for inference. We plan to add support for transcribing unlimited-length audio files shortly.\n\n### Supported Languages\n\nTo view the full list of 1600+ supported languages, you can access the language list [programmatically](/src/omnilingual_asr/models/wav2vec2_llama/lang_ids.py):\n\n```python\nfrom omnilingual_asr.models.wav2vec2_llama.lang_ids import supported_langs\n\n# Print all supported languages\nprint(f\"Total supported languages: {len(supported_langs)}\")\nprint(supported_langs)\n\n# Check if a specific language is supported\nif \"eng_Latn\" in supported_langs:\n    print(\"English (Latin script) is supported!\")\n```\n\nLanguages follow the format `{language_code}_{script}`, for example `eng_Latn` - English (Latin script), `cmn_Hans` - Mandarin Chinese (Simplified), ...\n\n### Using the HuggingFace Dataset ü§ó\n\nWe provide a large-scale multilingual speech dataset on HuggingFace under CC-BY-4.0 License: [`facebook/omnilingual-asr-corpus`](https://huggingface.co/datasets/facebook/omnilingual-asr-corpus).\nThis dataset can be directly used with our inference pipeline for evaluation or testing:\n\n```bash\npip install \"omnilingual-asr[dat",
      "default_branch": "main"
    },
    "fetched_at": "2025-11-15T02:24:55.685576"
  },
  {
    "basic_info": {
      "name": "espectre",
      "full_name": "francescopace/espectre",
      "owner": "francescopace",
      "description": "üõú ESPectre üëª  - Motion detection system based on Wi-Fi spectre analysis (CSI), with Home Assistant integration.",
      "url": "https://github.com/francescopace/espectre",
      "clone_url": "https://github.com/francescopace/espectre.git",
      "ssh_url": "git@github.com:francescopace/espectre.git",
      "homepage": "",
      "created_at": "2025-10-26T10:41:51Z",
      "updated_at": "2025-11-15T01:42:14Z",
      "pushed_at": "2025-11-14T01:00:50Z"
    },
    "stats": {
      "stars": 1786,
      "forks": 136,
      "watchers": 1786,
      "open_issues": 2,
      "size": 6787
    },
    "tech_info": {
      "language": "C",
      "languages": {
        "C": 1344660,
        "C++": 42172,
        "Shell": 21284,
        "Python": 6670,
        "CMake": 1766
      },
      "license": "GNU General Public License v3.0",
      "topics": [
        "csi",
        "diy",
        "esp-32",
        "espectre",
        "home-assistant",
        "motion-detection",
        "wifi",
        "wifi-sensing"
      ]
    },
    "content": {
      "readme": "![License](https://img.shields.io/badge/license-GPLv3-blue.svg)\n![C](https://img.shields.io/badge/C-ESP--IDF-orange.svg)\n![Platform](https://img.shields.io/badge/platform-ESP32--S3-red.svg)\n![Status](https://img.shields.io/badge/status-experimental-orange.svg)\n\n# üõú ESPectre üëª\n\n**Motion detection system based on Wi-Fi spectre analysis (CSI), with Home Assistant integration.**\n\n**‚ö†Ô∏è Disclaimer**: This is an experimental project for educational and research purposes. The author assumes no responsibility for misuse or damage resulting from the use of this system. Use responsibly and in compliance with applicable laws.\n\n\n---\n\n## üìë Table of Contents\n\n- [In 3 Points](#-in-3-points)\n- [Mathematical Approach](#-mathematical-approach)\n- [What You Need](#-what-you-need)\n- [Quick Start](#-quick-start)\n- [How It Works](#-how-it-works-simple-version)\n- [What You Can Do With It](#-what-you-can-do-with-it)\n- [Sensor Placement Guide](#-where-to-place-the-sensor)\n- [System Architecture](#Ô∏è-system-architecture)\n- [FAQ](#-faq-for-beginners)\n- [Security and Privacy](#-security-and-privacy)\n- [Technical Deep Dive](#-technical-deep-dive)\n- [Future Evolutions](#-future-evolutions-ai-approach)\n- [References](#-references)\n- [License](#-license)\n- [Author](#-author)\n\n---\n\n## üéØ In 3 Points\n\n1. **What it does**: Detects movement at home using Wi-Fi (no cameras, no microphones)\n2. **What you need**: A ~‚Ç¨10 device (ESP32-S3) + Home Assistant or MQTT server + ESP-IDF development tools\n3. **Setup time**: 30-45 minutes (first time, including ESP-IDF setup)\n\n---\n\n## üî¨ Mathematical Approach\n\n**This project currently does NOT use Machine Learning models.** Instead, it employs a **mathematical approach** that extracts **10 features** from CSI (Channel State Information) data using statistical and signal processing techniques.\n\n### Key Points\n\n- ‚úÖ **No ML training required**: Works out-of-the-box with mathematical algorithms\n- ‚úÖ **10 extracted features**: Statistical, spatial, and temporal features\n- ‚úÖ **Real-time processing**: Low latency detection on ESP32-S3 hardware\n- ‚úÖ **Foundation for ML**: These features can serve as the basis for collecting labeled datasets to train ML models for advanced tasks (people counting, activity recognition, gesture detection)\n\nThe mathematical approach provides excellent movement detection without the complexity of ML model training, while the extracted features offer a solid foundation for future ML-based enhancements.\n\n---\n\n## üõí What You Need\n\n### Hardware (Total: ~‚Ç¨10)\n\n- ‚úÖ **2.4GHz Wi-Fi Router** (the one you already have at home works fine)\n- ‚úÖ **ESP32-S3 DevKit bundle with external antennas** (~‚Ç¨10) - Available on Amazon, AliExpress, or electronics stores\n\n![3 x ESP32-S3 DevKit bundle with external antennas](images/home_lab.jpg)\n*ESP32-S3 DevKit with external antennas (recommended for better reception)*\n\n### Software (All Free)\n\n- ‚úÖ **MQTT Broker** (required for operation):\n  - **Home Assistant** with built-in MQTT broker (on Raspberry Pi, PC, NAS, or cloud)\n  - OR standalone **Mosquitto** MQTT server (can run on any device, including Raspberry Pi)\n- ‚úÖ **ESP-IDF v6.1** (development framework for building firmware)\n\n### Required Skills\n\n- ‚úÖ **Basic command line knowledge** required for building and flashing firmware\n- ‚ùå **NO** router configuration needed\n- ‚úÖ Follow the setup guide in SETUP.md\n\n---\n\n## üöÄ Quick Start\n\n**Setup time**: ~30-45 minutes (first time)  \n**Difficulty**: Intermediate (requires ESP-IDF setup)\n\n1. **Setup & Installation**: Follow the complete guide in [SETUP.md](SETUP.md)\n2. **Calibration & Tuning**: Optimize for your environment with [CALIBRATION.md](CALIBRATION.md)\n\n---\n\n## üìñ How It Works (Simple Version)\n\nWhen someone moves in a room, they \"disturb\" the Wi-Fi waves traveling between the router and the sensor. It's like when you move your hand in front of a flashlight and see the shadow change.\n\nThe ESP32-S3 device \"listens\" to these changes and understands if there's movement.\n\n### Advantages\n\n- ‚úÖ **No cameras** (total privacy)\n- ‚úÖ **No wearables needed** (no bracelets or sensors to wear)\n- ‚úÖ **Works through walls** (Wi-Fi passes through walls)\n- ‚úÖ **Very cheap** (~‚Ç¨10 total)\n\n<details>\n<summary>üìö Technical Explanation (click to expand)</summary>\n\n### What is CSI (Channel State Information)?\n\n**Channel State Information (CSI)** represents the physical characteristics of the wireless communication channel between transmitter and receiver. Unlike simple RSSI (Received Signal Strength Indicator), CSI provides rich, multi-dimensional data about the radio channel.\n\n#### What CSI Captures\n\n**Per-subcarrier information:**\n- **Amplitude**: Signal strength for each OFDM subcarrier (up to 64)\n- **Phase**: Phase shift of each subcarrier\n- **Frequency response**: How the channel affects different frequencies\n\n**Environmental effects:**\n- **Multipath propagation**: Reflections from walls, furniture, objects\n- **Doppler shifts**: Changes caused by movement\n- **Temporal variations**: How the chan",
      "default_branch": "main"
    },
    "fetched_at": "2025-11-15T02:24:56.981280"
  },
  {
    "basic_info": {
      "name": "MiniMax-M2",
      "full_name": "MiniMax-AI/MiniMax-M2",
      "owner": "MiniMax-AI",
      "description": "MiniMax-M2, a model built for Max coding & agentic workflows.",
      "url": "https://github.com/MiniMax-AI/MiniMax-M2",
      "clone_url": "https://github.com/MiniMax-AI/MiniMax-M2.git",
      "ssh_url": "git@github.com:MiniMax-AI/MiniMax-M2.git",
      "homepage": "https://www.minimax.io/",
      "created_at": "2025-10-25T21:28:29Z",
      "updated_at": "2025-11-15T00:35:13Z",
      "pushed_at": "2025-11-13T08:12:36Z"
    },
    "stats": {
      "stars": 1682,
      "forks": 121,
      "watchers": 1682,
      "open_issues": 22,
      "size": 1220
    },
    "tech_info": {
      "language": null,
      "languages": {},
      "license": "Other",
      "topics": [
        "large-language-models",
        "llm"
      ]
    },
    "content": {
      "readme": "<div align=\"center\">\n  <picture>\n    <source srcset=\"figures/MiniMaxLogo-Dark.png\" media=\"(prefers-color-scheme: dark)\">\n      <img src=\"figures/MiniMaxLogo-Light.png\" width=\"60%\" alt=\"MiniMax\">\n    </source>\n  </picture>\n</div>\n<hr>\n\n<div align=\"center\" style=\"line-height: 1.4; font-size:16px; margin-top: 30px;\">\n  Join Our \n  <a href=\"https://github.com/MiniMax-AI/MiniMax-AI.github.io/blob/main/images/wechat-qrcode.jpeg\" target=\"_blank\" style=\"font-size:17px; margin: 2px;\">\n    üí¨ WeChat\n  </a> | \n  <a href=\"https://discord.com/invite/hvvt8hAye6\" target=\"_blank\" style=\"font-size:17px; margin: 2px;\">\n    üß© Discord\n  </a> \n  community.\n</div>\n<div align=\"center\" style=\"line-height: 1.2; font-size:16px;\">\n  <a href=\"https://agent.minimax.io/\" target=\"_blank\" style=\"display: inline-block; margin: 4px;\">\n    MiniMax Agent\n  </a> | \n  <a href=\"https://platform.minimax.io/docs/guides/text-generation\" target=\"_blank\" style=\"display: inline-block; margin: 4px;\">\n    ‚ö°Ô∏è API (Now Free for a limited time!)\n  </a> | \n  <a href=\"https://github.com/MiniMax-AI/MiniMax-MCP\" style=\"display: inline-block; margin: 4px;\">\n    MCP\n  </a> |\n  <a href=\"https://www.minimax.io\" target=\"_blank\" style=\"display: inline-block; margin: 4px;\">\n    MiniMax Website\n  </a> \n</div>\n<div align=\"center\" style=\"lline-height: 1.2; font-size:16px; margin-bottom: 30px;\">\n  <a href=\"https://huggingface.co/MiniMaxAI\" target=\"_blank\" style=\"margin: 2px;\">\n    ü§ó Hugging Face \n  </a> | \n  <a href=\"https://github.com/MiniMax-AI/MiniMax-M2\" target=\"_blank\" style=\"margin: 2px;\">\n    üêô GitHub\n  </a> | \n  <a href=\"https://www.modelscope.cn/organization/MiniMax\" target=\"_blank\" style=\"margin: 2px;\">\n    ü§ñÔ∏è ModelScope\n  </a> | \n  <a href=\"https://github.com/MiniMax-AI/MiniMax-M2/blob/main/LICENSE\" style=\"margin: 2px;\">\n    üìÑ License: MIT\n  </a>\n</div>\n\n# Meet MiniMax-M2\n\nToday, we release and open source MiniMax-M2, a **Mini** model built for **Max** coding & agentic workflows.\n\n**MiniMax-M2** redefines efficiency for agents. It's a compact, fast, and cost-effective MoE model (230 billion total parameters with 10 billion active parameters) built for elite performance in coding and agentic tasks, all while maintaining powerful general intelligence. With just 10 billion activated parameters, MiniMax-M2 provides the sophisticated, end-to-end tool use performance expected from today's leading models, but in a streamlined form factor that makes deployment and scaling easier than ever.\n\n<p align=\"center\">\n  <img width=\"100%\" src=\"figures/Bench.png\">\n</p>\n\n---\n\n## Highlights\n\n**Superior Intelligence**. According to benchmarks from Artificial Analysis, MiniMax-M2 demonstrates highly competitive general intelligence across mathematics, science, instruction following, coding, and agentic tool use. **Its composite score ranks #1 among open-source models globally**.\n\n**Advanced Coding**. Engineered for end-to-end developer workflows, MiniMax-M2 excels at multi-file edits, coding-run-fix loops, and test-validated repairs. Strong performance on Terminal-Bench and (Multi-)SWE-Bench‚Äìstyle tasks demonstrates practical effectiveness in terminals, IDEs, and CI across languages.\n\n**Agent Performance**. MiniMax-M2 plans and executes complex, long-horizon toolchains across shell, browser, retrieval, and code runners. In BrowseComp-style evaluations, it consistently locates hard-to-surface sources, maintains evidence traceable, and gracefully recovers from flaky steps.\n\n**Efficient Design**. With 10 billion activated parameters (230 billion in total), MiniMax-M2 delivers lower latency, lower cost, and higher throughput for interactive agents and batched sampling‚Äîperfectly aligned with the shift toward highly deployable models that still shine on coding and agentic tasks.\n\n---\n\n## Coding & Agentic Benchmarks\n\nThese comprehensive evaluations test real-world end-to-end coding and agentic tool use: editing real repos, executing commands, browsing the web, and delivering functional solutions. Performance on this suite correlates with day-to-day developer experience in terminals, IDEs, and CI.\n\n| **Benchmark** | **MiniMax-M2** | **Claude Sonnet 4** | **Claude Sonnet 4.5** | **Gemini 2.5 Pro** | **GPT-5 (thinking)** | **GLM-4.6** | **Kimi K2 0905** | **DeepSeek-V3.2** |\n|-----------|------------|-----------------|-------------------|-----------------|------------------|---------|---------------|----------------|\n| **SWE-bench Verified** | 69.4 | 72.7 * | 77.2 * | 63.8 * | 74.9 * | 68 * | 69.2 * | 67.8 * |\n| **Multi-SWE-Bench** | 36.2 | 35.7 * | 44.3 | / | / | 30 | 33.5 | 30.6 |\n| **SWE-bench Multilingual** | 56.5 | 56.9 * | 68 | / | / | 53.8 | 55.9 * | 57.9 * |\n| **Terminal-Bench** | 46.3 | 36.4 * | 50 * | 25.3 * | 43.8 * | 40.5 * | 44.5 * | 37.7 * |\n| **ArtifactsBench** | 66.8 | 57.3* | 61.5 | 57.7* | 73* | 59.8 | 54.2 | 55.8 |\n| **BrowseComp** | 44 | 12.2 | 19.6 | 9.9 | 54.9* | 45.1* | 14.1 | 40.1* |\n| **BrowseComp-zh** | 48.5 | 29.1 | 40.8 | 32.2 | 65 | 49.5 | 28.8 | 47.9* |\n| **GAIA (",
      "default_branch": "main"
    },
    "fetched_at": "2025-11-15T02:24:58.278998"
  }
]