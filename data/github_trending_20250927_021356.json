[
  {
    "basic_info": {
      "name": "Awesome-Nano-Banana-images",
      "full_name": "PicoTrex/Awesome-Nano-Banana-images",
      "owner": "PicoTrex",
      "description": "A curated collection of fun and creative examples generated with Nano BananağŸŒ, Gemini-2.5-flash-image based model. We also release Nano-consistent-150K openly to support the community's development of image generation and unified models(click to website to see our blog)",
      "url": "https://github.com/PicoTrex/Awesome-Nano-Banana-images",
      "clone_url": "https://github.com/PicoTrex/Awesome-Nano-Banana-images.git",
      "ssh_url": "git@github.com:PicoTrex/Awesome-Nano-Banana-images.git",
      "homepage": "https://picotrex.github.io/Awesome-Nano-Banana-images/",
      "created_at": "2025-08-28T17:03:09Z",
      "updated_at": "2025-09-27T02:00:51Z",
      "pushed_at": "2025-09-24T09:59:16Z"
    },
    "stats": {
      "stars": 12382,
      "forks": 1266,
      "watchers": 12382,
      "open_issues": 11,
      "size": 182957
    },
    "tech_info": {
      "language": null,
      "languages": {},
      "license": "Apache License 2.0",
      "topics": [
        "awesome",
        "gemini-2-5-flash-image",
        "nano-banana"
      ]
    },
    "content": {
      "readme": "<div align=\"center\">\n\n<img src=\"images/logo.jpg\"  alt=\"è¾“å…¥å›¾ç‰‡\"> \n\n[![License: CC BY 4.0](https://img.shields.io/badge/License-CC_BY_4.0-lightgrey.svg)](LICENSE)\n[![Chinese](https://img.shields.io/badge/Chinese-Click_to_View-orange)](README.md)\n[![English](https://img.shields.io/badge/English-Click_to_View-yellow)](README_en.md)\n[![Japanese](https://img.shields.io/badge/æ—¥æœ¬èª-ã‚¯ãƒªãƒƒã‚¯ã—ã¦è¡¨ç¤º-green)](README_ja.md)\n[![Korean](https://img.shields.io/badge/í•œêµ­ì–´-ëˆŒëŸ¬ì„œ_ë³´ê¸°-blue)](README_kr.md)\n[![Spanish](https://img.shields.io/badge/EspaÃ±ol-Ver_TraducciÃ³n-blueviolet)](README_es.md)\n[![Turkish](https://img.shields.io/badge/TÃ¼rkÃ§e-GÃ¶rÃ¼ntÃ¼lemek_iÃ§in_TÄ±klayÄ±n-red)](README_tr.md)\n\n</div>\n\n> [!NOTE]\n> æˆ‘ä»¬æå‡º Nano-consistent-150kâ€”â€”é¦–ä¸ªåŸºäº Nano-Banana æ„å»ºã€è§„æ¨¡è¶…è¿‡ 150k çš„é«˜è´¨é‡æ•°æ®é›†ï¼Œä¸“ä¸ºåœ¨å¤šæ ·è€Œå¤æ‚çš„ç¼–è¾‘åœºæ™¯ä¸­ä¿æŒäººç‰©èº«ä»½ä¸€è‡´æ€§è€Œè®¾è®¡ã€‚å…¶ä¸€å¤§ç‰¹ç‚¹æ˜¯å“è¶Šçš„èº«ä»½ä¸€è‡´æ€§ï¼šé’ˆå¯¹åŒä¸€äººåƒï¼Œæˆ‘ä»¬åœ¨å¤šç§ä»»åŠ¡ä¸æŒ‡ä»¤ä¸‹æä¾›äº† 35 ç§ä»¥ä¸Šä¸åŒçš„ç¼–è¾‘ç»“æœã€‚ä»¥ä¸€è‡´çš„äººç‰©èº«ä»½ä¸ºé”šç‚¹ï¼Œè¯¥æ•°æ®é›†ä½¿å¾—å›´ç»•åŒä¸€ä¸»ä½“åœ¨å¤šç§ç¼–è¾‘ä»»åŠ¡ã€æŒ‡ä»¤ä¸æ¨¡æ€ä¹‹é—´æ— ç¼è¡”æ¥çš„äº¤é”™ï¼ˆinterleavedï¼‰æ•°æ®æ„å»ºæˆä¸ºå¯èƒ½ã€‚\n<a href='https://picotrex.github.io/Awesome-Nano-Banana-images/'><img src='https://img.shields.io/badge/ğŸŒ Website-Blog-orange' height=\"25\"></a>\n<a href='https://huggingface.co/datasets/Yejy53/Nano-consistent-150k'><img src='https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Dataset-yellow' height=\"25\"></a>\n\n## ğŸŒ Introduction\n\næ¬¢è¿æ¥åˆ° Nano-banana ç²¾é€‰å›¾ç‰‡åº“ï¼ğŸ¤— \n\n**æˆ‘ä»¬æ”¶é›†äº†Nano-bananaåœ¨å„ä¸ªä»»åŠ¡åœºæ™¯ä¸‹ç”Ÿæˆçš„ä»¤äººæƒŠè‰³çš„å›¾ç‰‡å’Œæç¤ºè¯**ï¼Œå…¨æ–¹ä½å±•ç¤ºGoogleåœ¨å›¾åƒç”Ÿæˆä¸ç¼–è¾‘çš„æ— é™å¯èƒ½ã€‚å¸Œæœ›èƒ½å¸®åŠ©ä½ æ›´å¥½åœ°äº†è§£Nano-bananaã€‚å¿«ä¸€èµ·è§£é” Nano-banana çš„å¤šå›¾èåˆä¸åˆ›æ„ç¼–è¾‘åŠ›é‡å§ï¼âœ¨\n\nè¿™äº›æ¡ˆä¾‹ä¸»è¦æ¥æºäºTwitter/ X ğŸ¦ã€å°çº¢ä¹¦ğŸ“•ç­‰è‡ªåª’ä½“å¹³å°ã€‚\n\nå–œæ¬¢å°±ç‚¹ â­ Star æ”¶è—èµ·æ¥å§ï¼\n\n## ğŸ“° News\n\n- **2025å¹´9æœˆ24æ—¥ï¼š** 5ï¸âƒ£ ç¬¬äº”æ¬¡ä»“åº“æ›´æ–°\n- **2025å¹´9æœˆ18æ—¥ï¼š** æˆ‘ä»¬å‘å¸ƒäº† [**Nano-consistent-150k**](https://picotrex.github.io/Awesome-Nano-Banana-images/) æ•°æ®é›†\n- **2025å¹´9æœˆ16æ—¥ï¼š** 4ï¸âƒ£ ç¬¬å››æ¬¡ä»“åº“æ›´æ–°\n- **2025å¹´9æœˆ9æ—¥ï¼š** 3ï¸âƒ£ ç¬¬ä¸‰æ¬¡ä»“åº“æ›´æ–°\n- **2025å¹´9æœˆ3æ—¥ï¼š** 2ï¸âƒ£ ç¬¬äºŒæ¬¡ä»“åº“æ›´æ–°\n- **2025å¹´8æœˆ28æ—¥ï¼š** ğŸ‰ 1ï¸âƒ£ ${\\color{red} ç¬¬ä¸€æ¬¡\\ Awesome-Nano-Banana-images \\ æ›´æ–°!}$\n\n## ğŸ“‘ Menu\n\n- [ğŸŒ Introduction](#-introduction)\n- [ğŸ“° News](#-news)\n- [ğŸ“‘ Menu](#-menu)\n- [ğŸ–¼ï¸ ä¾‹å­](#ï¸-ä¾‹å­)\n  - [ä¾‹ 1: æ’ç”»å˜æ‰‹åŠï¼ˆby @ZHO\\_ZHO\\_ZHOï¼‰](#ä¾‹-1-æ’ç”»å˜æ‰‹åŠby-zho_zho_zho)\n  - [ä¾‹ 2: æ ¹æ®åœ°å›¾ç®­å¤´ç”Ÿæˆåœ°é¢è§†è§’å›¾ç‰‡ï¼ˆby @tokuminï¼‰](#ä¾‹-2-æ ¹æ®åœ°å›¾ç®­å¤´ç”Ÿæˆåœ°é¢è§†è§’å›¾ç‰‡by-tokumin)\n  - [ä¾‹ 3: çœŸå®ä¸–ç•Œçš„ARä¿¡æ¯åŒ–ï¼ˆby @bilawalsidhuï¼‰](#ä¾‹-3-çœŸå®ä¸–ç•Œçš„arä¿¡æ¯åŒ–by-bilawalsidhu)\n  - [ä¾‹ 4: åˆ†ç¦»å‡º3Då»ºç­‘/åˆ¶ä½œç­‰è·æ¨¡å‹ï¼ˆby @Zieeettï¼‰](#ä¾‹-4-åˆ†ç¦»å‡º3då»ºç­‘åˆ¶ä½œç­‰è·æ¨¡å‹by-zieeett)\n  - [ä¾‹ 5: ä¸åŒæ—¶ä»£è‡ªå·±çš„ç…§ç‰‡ï¼ˆby @AmirMushichï¼‰](#ä¾‹-5-ä¸åŒæ—¶ä»£è‡ªå·±çš„ç…§ç‰‡by-amirmushich)\n  - [ä¾‹ 6: å¤šå‚è€ƒå›¾åƒç”Ÿæˆï¼ˆby @MrDavids1ï¼‰](#ä¾‹-6-å¤šå‚è€ƒå›¾åƒç”Ÿæˆby-mrdavids1)\n  - [ä¾‹ 7: è‡ªåŠ¨ä¿®å›¾ï¼ˆby @op7418ï¼‰](#ä¾‹-7-è‡ªåŠ¨ä¿®å›¾by-op7418)\n  - [ä¾‹ 8: æ‰‹ç»˜å›¾æ§åˆ¶å¤šè§’è‰²å§¿æ€ï¼ˆby @op7418ï¼‰](#ä¾‹-8-æ‰‹ç»˜å›¾æ§åˆ¶å¤šè§’è‰²å§¿æ€by-op7418)\n  - [ä¾‹ 9: è·¨è§†è§’å›¾åƒç”Ÿæˆï¼ˆby @op7418ï¼‰](#ä¾‹-9-è·¨è§†è§’å›¾åƒç”Ÿæˆby-op7418)\n  - [ä¾‹ 10: å®šåˆ¶äººç‰©è´´çº¸ï¼ˆby @op7418ï¼‰](#ä¾‹-10-å®šåˆ¶äººç‰©è´´çº¸by-op7418)\n  - [ä¾‹ 11: åŠ¨æ¼«è½¬çœŸäººCoserï¼ˆby @ZHO\\_ZHO\\_ZHOï¼‰](#ä¾‹-11-åŠ¨æ¼«è½¬çœŸäººcoserby-zho_zho_zho)\n  - [ä¾‹ 12: ç”Ÿæˆè§’è‰²è®¾å®šï¼ˆby @ZHO\\_ZHO\\_ZHOï¼‰](#ä¾‹-12-ç”Ÿæˆè§’è‰²è®¾å®šby-zho_zho_zho)\n  - [ä¾‹ 13: è‰²å¡çº¿ç¨¿ä¸Šè‰²ï¼ˆby @ZHO\\_ZHO\\_ZHOï¼‰](#ä¾‹-13-è‰²å¡çº¿ç¨¿ä¸Šè‰²by-zho_zho_zho)\n  - [ä¾‹ 14: æ–‡ç« ä¿¡æ¯å›¾ï¼ˆby @é»„å»ºåŒå­¦ï¼‰](#ä¾‹-14-æ–‡ç« ä¿¡æ¯å›¾by-é»„å»ºåŒå­¦)\n  - [ä¾‹ 15: æ›´æ¢å¤šç§å‘å‹ï¼ˆby @balconychyï¼‰](#ä¾‹-15-æ›´æ¢å¤šç§å‘å‹by-balconychy)\n  - [ä¾‹ 16: æ¨¡å‹æ ‡æ³¨è®²è§£å›¾ï¼ˆby @berryxia\\_aiï¼‰](#ä¾‹-16-æ¨¡å‹æ ‡æ³¨è®²è§£å›¾by-berryxia_ai)\n  - [ä¾‹ 17: å®šåˆ¶å¤§ç†çŸ³é›•å¡‘ï¼ˆby @umesh\\_aiï¼‰](#ä¾‹-17-å®šåˆ¶å¤§ç†çŸ³é›•å¡‘by-umesh_ai)\n  - [ä¾‹ 18: æ ¹æ®é£Ÿæåšèœï¼ˆby @Gdgtifyï¼‰](#ä¾‹-18-æ ¹æ®é£Ÿæåšèœby-gdgtify)\n  - [ä¾‹ 19: æ•°å­¦é¢˜æ¨ç†ï¼ˆby @Gorden Sunï¼‰](#ä¾‹-19-æ•°å­¦é¢˜æ¨ç†by-gorden-sun)\n  - [ä¾‹ 20: æ—§ç…§ç‰‡ä¸Šè‰²ï¼ˆby @GeminiAppï¼‰](#ä¾‹-20-æ—§ç…§ç‰‡ä¸Šè‰²by-geminiapp)\n  - [ä¾‹ 21: OOTDç©¿æ­ï¼ˆby @302.AIï¼‰](#ä¾‹-21-ootdç©¿æ­by-302ai)\n  - [ä¾‹ 22: äººç‰©æ¢è¡£ï¼ˆby @skiranoï¼‰](#ä¾‹-22-äººç‰©æ¢è¡£by-skirano)\n  - [ä¾‹ 23: å¤šè§†å›¾ç»“æœç”Ÿæˆï¼ˆby @Error\\_HTTP\\_404ï¼‰](#ä¾‹-23-å¤šè§†å›¾ç»“æœç”Ÿæˆby-error_http_404)\n  - [ä¾‹ 24: ç”µå½±åˆ†é•œï¼ˆby @GeminiAppï¼‰](#ä¾‹-24-ç”µå½±åˆ†é•œby-geminiapp)\n  - [ä¾‹ 25: äººç‰©å§¿åŠ¿ä¿®æ”¹ï¼ˆby @arrakis\\_aiï¼‰](#ä¾‹-25-äººç‰©å§¿åŠ¿ä¿®æ”¹by-arrakis_ai)\n  - [ä¾‹ 26: çº¿ç¨¿å›¾ç”Ÿæˆå›¾åƒï¼ˆby @ZHO\\_ZHO\\_ZHOï¼‰](#ä¾‹-26-çº¿ç¨¿å›¾ç”Ÿæˆå›¾åƒby-zho_zho_zho)\n  - [ä¾‹ 27: ä¸ºå›¾åƒæ·»åŠ æ°´å°ï¼ˆby @AiMacheteï¼‰](#ä¾‹-27-ä¸ºå›¾åƒæ·»åŠ æ°´å°by-aimachete)\n  - [ä¾‹ 28: çŸ¥è¯†æ¨ç†ç”Ÿæˆå›¾åƒï¼ˆby @icreatelifeï¼‰](#ä¾‹-28-çŸ¥è¯†æ¨ç†ç”Ÿæˆå›¾åƒby-icreatelife)\n  - [ä¾‹ 29: çº¢ç¬”æ‰¹æ³¨ï¼ˆby @AiMacheteï¼‰](#ä¾‹-29-çº¢ç¬”æ‰¹æ³¨by-aimachete)\n  - [ä¾‹ 30: çˆ†ç‚¸çš„é£Ÿç‰©ï¼ˆby @icreatelifeï¼‰](#ä¾‹-30-çˆ†ç‚¸çš„é£Ÿç‰©by-icreatelife)\n  - [ä¾‹ 31: åˆ¶ä½œæ¼«ç”»ä¹¦ï¼ˆby @icreatelifeï¼‰](#ä¾‹-31-åˆ¶ä½œæ¼«ç”»ä¹¦by-icreatelife)\n  - [ä¾‹ 32: åŠ¨ä½œäººå¶ï¼ˆby @icreatelifeï¼‰](#ä¾‹-32-åŠ¨ä½œäººå¶by-icreatelife)\n  - [ä¾‹ 33: åœ°å›¾ç”Ÿæˆç­‰è·å»ºç­‘ï¼ˆby @demishassabisï¼‰](#ä¾‹-33-åœ°å›¾ç”Ÿæˆç­‰è·å»ºç­‘by-demishassabis)\n  - [ä¾‹ 34: å‚è€ƒå›¾æ§åˆ¶äººç‰©è¡¨æƒ…ï¼ˆby @ZHO\\_ZHO\\_ZHOï¼‰](#ä¾‹-34-å‚è€ƒå›¾æ§åˆ¶äººç‰©è¡¨æƒ…by-zho_zho_zho)\n  - [ä¾‹ 35: æ’ç”»ç»˜ç”»è¿‡ç¨‹å››æ ¼ï¼ˆby @ZHO\\_ZHO\\_ZHOï¼‰](#ä¾‹-35-æ’ç”»ç»˜ç”»è¿‡ç¨‹å››æ ¼by-zho_zho_zho)\n  - [ä¾‹ 36: è™šæ‹Ÿè¯•å¦†ï¼ˆby @ZHO\\_ZHO\\_ZHOï¼‰](#ä¾‹-36-è™šæ‹Ÿè¯•å¦†by-zho_zho_zho)\n  - [ä¾‹ 37: å¦†é¢åˆ†æï¼ˆby @ZHO\\_ZHO\\_ZHOï¼‰](#ä¾‹-37-å¦†é¢åˆ†æby-zho_zho_zho)\n  - [ä¾‹ 38: Googleåœ°å›¾è§†è§’ä¸‹çš„ä¸­åœŸä¸–ç•Œï¼ˆby @TechHalloï¼‰](#ä¾‹-38-googleåœ°å›¾è§†è§’ä¸‹çš„ä¸­åœŸä¸–ç•Œby-techhallo)\n  - [ä¾‹ 39: å°åˆ·æ’ç”»ç”Ÿæˆï¼ˆby @Umeshï¼‰](#ä¾‹-39-å°åˆ·æ’ç”»ç”Ÿæˆby-umesh)\n  - [ä¾‹ 40: è¶…å¤šäººç‰©å§¿åŠ¿ç”Ÿæˆï¼ˆby @tapehead\\_Labï¼‰](#ä¾‹-40-è¶…å¤šäººç‰©å§¿åŠ¿ç”Ÿæˆby-tapehead_lab)\n  - [ä¾‹ 41: ç‰©å“åŒ…è£…ç”Ÿæˆï¼ˆby @ZHO\\_ZHO\\_ZHOï¼‰](#ä¾‹-41-ç‰©å“åŒ…è£…ç”Ÿæˆby-zho_zho_zho)\n  - [ä¾‹ 42: å åŠ æ»¤é•œ/æè´¨ï¼ˆby @ZHO\\_ZHO\\_ZHOï¼‰](#ä¾‹-42-å åŠ æ»¤é•œæè´¨by-zho_zho_zho)\n  - [ä¾‹ 43: æ§åˆ¶äººç‰©è„¸å‹ï¼ˆby @ZHO\\_ZHO\\_ZHOï¼‰](#ä¾‹-43-æ§åˆ¶äººç‰©è„¸å‹by-zho_zho_zho)\n  - [ä¾‹ 44: å…‰å½±æ§åˆ¶ï¼ˆby @ZHO\\_ZHO\\_ZHOï¼‰](#ä¾‹-44-å…‰å½±æ§åˆ¶by-zho_zho_zho)\n  - [ä¾‹ 45: ä¹é«˜ç©å…·å°äººï¼ˆby @ZHO\\_ZHO\\_ZHOï¼‰](#ä¾‹-45-ä¹é«˜ç©å…·å°äººby-zho_zho_zho)\n  - [ä¾‹ 46: é«˜è¾¾æ¨¡å‹å°äººï¼ˆby @ZHO\\_ZHO\\_ZHOï¼‰](#ä¾‹-46-é«˜è¾¾æ¨¡å‹å°äººby-zho_zho_zho)\n  - [ä¾‹ 47: ç¡¬ä»¶æ‹†è§£å›¾ï¼ˆby @AIimaginedï¼‰](#ä¾‹-47-ç¡¬ä»¶æ‹†è§£å›¾by-aiimagined)\n  - [ä¾‹ 48: é£Ÿç‰©å¡è·¯é‡Œæ ‡æ³¨ï¼ˆby @icreatelifeï¼‰](#ä¾‹-48-é£Ÿç‰©å¡è·¯é‡Œæ ‡æ³¨by-icreatelife)\n  - [ä¾‹ 49: æå–ä¿¡æ¯å¹¶æ”¾ç½®é€æ˜å›¾å±‚ï¼ˆby @nglprzï¼‰](#ä¾‹-49-æå–ä¿¡æ¯å¹¶æ”¾ç½®é€æ˜å›¾å±‚by-nglprz)\n  - [ä¾‹ 50: å›¾åƒå¤–æ‰©ä¿®å¤ï¼ˆby @bwabbageï¼‰](#ä¾‹-50-å›¾åƒå¤–æ‰©ä¿®å¤",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-27T02:13:57.073998"
  },
  {
    "basic_info": {
      "name": "term.everything",
      "full_name": "mmulet/term.everything",
      "owner": "mmulet",
      "description": "Run any GUI app in the terminalâ—",
      "url": "https://github.com/mmulet/term.everything",
      "clone_url": "https://github.com/mmulet/term.everything.git",
      "ssh_url": "git@github.com:mmulet/term.everything.git",
      "homepage": "",
      "created_at": "2025-09-07T02:52:48Z",
      "updated_at": "2025-09-27T00:50:27Z",
      "pushed_at": "2025-09-26T22:01:08Z"
    },
    "stats": {
      "stars": 6033,
      "forks": 131,
      "watchers": 6033,
      "open_issues": 17,
      "size": 53552
    },
    "tech_info": {
      "language": "TypeScript",
      "languages": {
        "TypeScript": 219331,
        "C++": 39626,
        "JavaScript": 2607,
        "Meson": 2286,
        "Shell": 1981,
        "C": 314
      },
      "license": "GNU Affero General Public License v3.0",
      "topics": [
        "alacritty",
        "cli",
        "foss",
        "iterm2",
        "kitty",
        "linux",
        "ssh",
        "terminal",
        "wayland",
        "wayland-compositor"
      ]
    },
    "content": {
      "readme": "\n\n\n<table>\n  <tr>\n    <td valign=\"middle\">\n      <img width=\"128\" height=\"128\" alt=\"icon2\" src=\"./resources/icon.png\" />\n    </td>\n    <td><h1>Term.Everythingâ—</h1></td>\n    <td><a href=\"https://github.com/mmulet/term.everything/releases\">Download the beta test now!</a></td>\n    <td><a href=\"./resources/HowIDidIt.md\">HowIDidIt.md</a></td>\n  </tr>\n  <tr>\n    <td></td>\n    <td>Works on both x11 and Wayland host systems.</td>\n    <td></td>\n    <td></td>\n  </tr>\n</table>\n\n## Run every GUI app in the terminal!\n\n![warp_into_terminal0001-0195](./resources/graphics/warp_in_2.gif)\n\n## Even over ssh!\nBehold as I play a [video game in a font](https://github.com/mmulet/font-game-engine) in a web browser in a terminal transmitted over ssh (with one hand tied behind my back)!\n\n![ssh_example](./resources/graphics/ssh_example.gif)\n\n### Read about how it works!\nCheck out [HowIDidIt.md](./resources/HowIDidIt.md)\n\n## More Examples\nThe quality of the window is limited to the number of rows and columns in your\nterminal. If you increase the resolution (ctrl - in alacritty, check your\nterminal) the quality will go up, (but performance may go down).\n\nHere I open up the Wing It! movie, and increase the quality until I get both\na good frame rate and resolution:\n\n![increase resolution](./resources/graphics/show_increase_res.gif)\n\n----------------\n\nIf your terminal supports images (like [kitty](https://sw.kovidgoyal.net/kitty/)\nor [iterm2](https://iterm2.com/)) you can render windows at full resolution\n(performance may degrade).\n\nIn this example, on my mac, I open iTerm2 ssh into ubuntu and open firefox\nat full resolution:\n\n![full_resultion](resources/graphics/full_resultion.gif)\n\n------------\n\nI feel like every single day I hear about another terminal file viewer. I say, stop making terminal file viewers because you can just use the file viewer you already have! In your terminal!\n\n![file_manager](./resources/graphics/file_manager.gif)\n\n-------------\n\nTerminal in a terminal in a terminal in a terminal in a terminal.... it's terminals all the way down.\n![terminal_in_terminal](./resources/graphics/terminal_in_terminal.gif)\n\n-------------\nWith only a small amount hacking, it can run Doom (shareware episode)!\n\n![Doom](./resources/graphics/doom.gif)\n------\nRun an entire Desktop in your terminal!\n[@ismail-yilmaz](https://github.com/ismail-yilmaz) is running Firefox, on [KDE Neon](https://neon.kde.org) in a [VM](https://gitlab.gnome.org/GNOME/gnome-boxes) on [Bobcat](https://github.com/ismail-yilmaz/Bobcat)\n![Desktop in VM](./resources/graphics/desktop_in_vm.gif)\n\nAnd this isn't even full resolution! Checkout the [full vid in in the discussions](https://github.com/mmulet/term.everything/discussions/16#discussioncomment-14390137)\n\n## About\n`term.everythingâ—` is a Linux CLI program to run GUI windows in your terminal. Specifically, `term.everythingâ—` is a built-from-scratch [Wayland](https://wiki.archlinux.org/title/Wayland) compositor that outputs to a terminal rather than your monitor.\n\n>Don't know what Wayland is or just want to know more about how this works? Then, head over to [HowIDidIt.md](./resources/HowIDidIt.md) where I will explain how everything works in detail.\n\n## Try it out!\n[Download the beta test now!](https://github.com/mmulet/term.everything/releases)\n\n## Roadmap\n1. [x] Term some things <--- This is where we are at\n  - Some apps or (even most apps) may fail to launch or even crash! Please create [an issue]( https://github.com/mmulet/term.everything/issues) if you have problems.\n2. [ ] Term most things\n3. [ ] Term everythingâ—\n\n## Help and Usage\nCheck out the [help file here](./resources/help.md) for a usage guide on how to use `term.everythingâ—`\n\n## Contributing\nterm.everythingâ— is written in developer friendly [Typescript](https://www.typescriptlang.org/) using the [bun](https://bun.com/) engine, with a just a smidge of C++.\nSee [./Contributing.md](./Contributing.md).\n\n## Legal:\n\nterm.everythingâ— copyright 2025 Late for Dinner Studios, LLC\n---\nFontemon copyright 2021 Late for Dinner Studios, LLC\n---\nWing It! movie is licensed under the Creative Commons Attribution 4.0 license\n[Wing it licensing page](https://studio.blender.org/projects/wing-it/pages/licensing/)\nAttribution:\n(CC) Blender Foundation | studio.blender.org\n---\nDoom shareware episode is copyright 1993 id Software\n---\n\n## Bonus:\nThis is Gwerm the Term Worm.\n\n![this is gwern](./resources/graphics/this_is_gwern.gif)\n\nHe is doing okay. Thanks for asking.\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-27T02:13:58.239272"
  },
  {
    "basic_info": {
      "name": "chrome-devtools-mcp",
      "full_name": "ChromeDevTools/chrome-devtools-mcp",
      "owner": "ChromeDevTools",
      "description": "Chrome DevTools for coding agents",
      "url": "https://github.com/ChromeDevTools/chrome-devtools-mcp",
      "clone_url": "https://github.com/ChromeDevTools/chrome-devtools-mcp.git",
      "ssh_url": "git@github.com:ChromeDevTools/chrome-devtools-mcp.git",
      "homepage": "https://npmjs.org/package/chrome-devtools-mcp",
      "created_at": "2025-09-11T10:39:55Z",
      "updated_at": "2025-09-27T02:11:34Z",
      "pushed_at": "2025-09-26T14:04:06Z"
    },
    "stats": {
      "stars": 3554,
      "forks": 187,
      "watchers": 3554,
      "open_issues": 27,
      "size": 1322
    },
    "tech_info": {
      "language": "TypeScript",
      "languages": {
        "TypeScript": 195000,
        "JavaScript": 5815
      },
      "license": "Apache License 2.0",
      "topics": [
        "browser",
        "chrome",
        "chrome-devtools",
        "debugging",
        "devtools",
        "mcp",
        "mcp-server",
        "puppeteer"
      ]
    },
    "content": {
      "readme": "# Chrome DevTools MCP\n\n[![npm chrome-devtools-mcp package](https://img.shields.io/npm/v/chrome-devtools-mcp.svg)](https://npmjs.org/package/chrome-devtools-mcp)\n\n`chrome-devtools-mcp` lets your coding agent (such as Gemini, Claude, Cursor or Copilot)\ncontrol and inspect a live Chrome browser. It acts as a Model-Context-Protocol\n(MCP) server, giving your AI coding assistant access to the full power of\nChrome DevTools for reliable automation, in-depth debugging, and performance analysis.\n\n## Key features\n\n- **Get performance insights**: Uses [Chrome\n  DevTools](https://github.com/ChromeDevTools/devtools-frontend) to record\n  traces and extract actionable performance insights.\n- **Advanced browser debugging**: Analyze network requests, take screenshots and\n  check the browser console.\n- **Reliable automation**. Uses\n  [puppeteer](https://github.com/puppeteer/puppeteer) to automate actions in\n  Chrome and automatically wait for action results.\n\n## Disclaimers\n\n`chrome-devtools-mcp` exposes content of the browser instance to the MCP clients\nallowing them to inspect, debug, and modify any data in the browser or DevTools.\nAvoid sharing sensitive or personal information that you don't want to share with\nMCP clients.\n\n## Requirements\n\n- [Node.js 22.12.0](https://nodejs.org/) or newer.\n- [Chrome](https://www.google.com/chrome/) current stable version or newer.\n- [npm](https://www.npmjs.com/).\n\n## Getting started\n\nAdd the following config to your MCP client:\n\n```json\n{\n  \"mcpServers\": {\n    \"chrome-devtools\": {\n      \"command\": \"npx\",\n      \"args\": [\"chrome-devtools-mcp@latest\"]\n    }\n  }\n}\n```\n\n> [!NOTE]  \n> Using `chrome-devtools-mcp@latest` ensures that your MCP client will always use the latest version of the Chrome DevTools MCP server.\n\n### MCP Client configuration\n\n<details>\n  <summary>Claude Code</summary>\n    Use the Claude Code CLI to add the Chrome DevTools MCP server (<a href=\"https://docs.anthropic.com/en/docs/claude-code/mcp\">guide</a>):\n\n```bash\nclaude mcp add chrome-devtools npx chrome-devtools-mcp@latest\n```\n\n</details>\n\n<details>\n  <summary>Cline</summary>\n  Follow https://docs.cline.bot/mcp/configuring-mcp-servers and use the config provided above.\n</details>\n\n<details>\n  <summary>Codex</summary>\n  Follow the <a href=\"https://github.com/openai/codex/blob/main/docs/advanced.md#model-context-protocol-mcp\">configure MCP guide</a>\n  using the standard config from above. You can also install the Chrome DevTools MCP server using the Codex CLI:\n\n```bash\ncodex mcp add chrome-devtools -- npx chrome-devtools-mcp@latest\n```\n\n</details>\n\n<details>\n  <summary>Copilot / VS Code</summary>\n  Follow the MCP install <a href=\"https://code.visualstudio.com/docs/copilot/chat/mcp-servers#_add-an-mcp-server\">guide</a>,\n  with the standard config from above. You can also install the Chrome DevTools MCP server using the VS Code CLI:\n  \n  ```bash\n  code --add-mcp '{\"name\":\"chrome-devtools\",\"command\":\"npx\",\"args\":[\"chrome-devtools-mcp@latest\"]}'\n  ```\n</details>\n\n<details>\n  <summary>Cursor</summary>\n\n**Click the button to install:**\n\n[<img src=\"https://cursor.com/deeplink/mcp-install-dark.svg\" alt=\"Install in Cursor\">](https://cursor.com/en/install-mcp?name=chrome-devtools&config=eyJjb21tYW5kIjoibnB4IGNocm9tZS1kZXZ0b29scy1tY3BAbGF0ZXN0In0%3D)\n\n**Or install manually:**\n\nGo to `Cursor Settings` -> `MCP` -> `New MCP Server`. Use the config provided above.\n\n</details>\n\n<details>\n  <summary>Gemini CLI</summary>\nInstall the Chrome DevTools MCP server using the Gemini CLI.\n\n**Project wide:**\n\n```bash\ngemini mcp add chrome-devtools npx chrome-devtools-mcp@latest\n```\n\n**Globally:**\n\n```bash\ngemini mcp add -s user chrome-devtools npx chrome-devtools-mcp@latest\n```\n\nAlternatively, follow the <a href=\"https://github.com/google-gemini/gemini-cli/blob/main/docs/tools/mcp-server.md#how-to-set-up-your-mcp-server\">MCP guide</a> and use the standard config from above.\n\n</details>\n\n<details>\n  <summary>Gemini Code Assist</summary>\n  Follow the <a href=\"https://cloud.google.com/gemini/docs/codeassist/use-agentic-chat-pair-programmer#configure-mcp-servers\">configure MCP guide</a>\n  using the standard config from above.\n</details>\n\n<details>\n  <summary>JetBrains AI Assistant & Junie</summary>\n\nGo to `Settings | Tools | AI Assistant | Model Context Protocol (MCP)` -> `Add`. Use the config provided above.\nThe same way chrome-devtools-mcp can be configured for JetBrains Junie in `Settings | Tools | Junie | MCP Settings` -> `Add`. Use the config provided above.\n\n</details>\n\n### Your first prompt\n\nEnter the following prompt in your MCP Client to check if everything is working:\n\n```\nCheck the performance of https://developers.chrome.com\n```\n\nYour MCP client should open the browser and record a performance trace.\n\n> [!NOTE]  \n> The MCP server will start the browser automatically once the MCP client uses a tool that requires a running browser instance. Connecting to the Chrome DevTools MCP server on its own will not automatically start the browser.\n\n## To",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-27T02:13:59.382856"
  },
  {
    "basic_info": {
      "name": "LidAngleSensor",
      "full_name": "samhenrigold/LidAngleSensor",
      "owner": "samhenrigold",
      "description": "tfw when you when your lid when uhh angle your lid sensor",
      "url": "https://github.com/samhenrigold/LidAngleSensor",
      "clone_url": "https://github.com/samhenrigold/LidAngleSensor.git",
      "ssh_url": "git@github.com:samhenrigold/LidAngleSensor.git",
      "homepage": "https://samhenri.gold",
      "created_at": "2025-09-06T19:07:20Z",
      "updated_at": "2025-09-26T21:53:57Z",
      "pushed_at": "2025-09-08T21:30:26Z"
    },
    "stats": {
      "stars": 3334,
      "forks": 124,
      "watchers": 3334,
      "open_issues": 31,
      "size": 1486
    },
    "tech_info": {
      "language": "Objective-C",
      "languages": {
        "Objective-C": 49586
      },
      "license": "Apache License 2.0",
      "topics": []
    },
    "content": {
      "readme": "# Lid Angle Sensor\n\nHi, Iâ€™m Sam Gold. Did you know that you have ~rights~ a lid angle sensor in your MacBook? [The ~Constitution~ human interface device utility says you do.](https://youtu.be/wqnHtGgVAUE?t=21)\n\nThis is a little utility that shows the angle from the sensor and, optionally, plays a wooden door creaking sound if you adjust it reeaaaaaal slowly.\n\n## FAQ\n\n**What is a lid angle sensor?**\n\nDespite what the name would have you believe, it is a sensor that detects the angle of the lid.\n\n**Which devices have a lid angle sensor?**\n\nIt was introduced with the 2019 16-inch MacBook Pro. If your laptop is newer, you probably have it. [People have reported](https://github.com/samhenrigold/LidAngleSensor/issues/13) that it **does not work on M1 devices**, I have not yet figured out a fix.\n\n**My laptop should have it, why doesn't it show up?**\n\nI've only tested this on my M4 MacBook Pro and have hard-coded it to look for a specific sensor. If that doesn't work, try running [this script](https://gist.github.com/samhenrigold/42b5a92d1ee8aaf2b840be34bff28591) and report the output in [an issue](https://github.com/samhenrigold/LidAngleSensor/issues/new/choose).\n\nKnown problematic models:\n\n- M1 MacBook Air\n- M1 MacBook Pro\n\n**Can I use this on my iMac?**\n\n~~Not yet tested. Feel free to slam your computer into your desk and make a PR with your results.~~\n\n[It totally works](https://github.com/samhenrigold/LidAngleSensor/issues/33). If it doesn't work for you, try slamming your computer harder?\n\n**Why?**\n\nA lot of free time. I'm open to full-time work in NYC or remote. I'm a designer/design-engineer. https://samhenri.gold\n\n**No I mean like why does my laptop need to know the exact angle of its lid?**\n\nOh. I don't know.\n\n**Can I contribute?**\n\nI guess.\n\n**Why does it say it's by Lisa?**\n\nI signed up for my developer account when I was a kid, used my mom's name, and now it's stuck that way forever and I can't change it. That's life.\n\n**How come the audio feels kind of...weird?**\n\nI'm bad at audio.\n\n**Where did the sound effect come from?**\n\nLEGO Batman 3: Beyond Gotham. But you knew that already.\n\n**Can I turn off the sound?**\n\nYes, never click \"Start Audio\". But this energy isn't encouraged.\n\n## Building\n\nAccording to [this issue](https://github.com/samhenrigold/LidAngleSensor/issues/12), building requires having Xcode installed. I've only tested this on Xcode 26. YMMV.\n\n## Installation\n\nVia Homebrew:\n\n```shell\nbrew install lidanglesensor\n```\n\n## Related projects\n\n- [Python library that taps into this sensor](https://github.com/tcsenpai/pybooklid)\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-27T02:14:00.538423"
  },
  {
    "basic_info": {
      "name": "RustGPT",
      "full_name": "tekaratzas/RustGPT",
      "owner": "tekaratzas",
      "description": "An transformer based LLM. Written completely in Rust",
      "url": "https://github.com/tekaratzas/RustGPT",
      "clone_url": "https://github.com/tekaratzas/RustGPT.git",
      "ssh_url": "git@github.com:tekaratzas/RustGPT.git",
      "homepage": null,
      "created_at": "2025-09-13T22:05:55Z",
      "updated_at": "2025-09-27T01:59:14Z",
      "pushed_at": "2025-09-25T13:50:46Z"
    },
    "stats": {
      "stars": 2690,
      "forks": 215,
      "watchers": 2690,
      "open_issues": 4,
      "size": 191
    },
    "tech_info": {
      "language": "Rust",
      "languages": {
        "Rust": 62444
      },
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "# ğŸ¦€ Rust LLM from Scratch\n\n[![Rust](https://github.com/tekaratzas/RustGPT/actions/workflows/rust.yml/badge.svg)](https://github.com/tekaratzas/RustGPT/actions/workflows/rust.yml)\n\nhttps://github.com/user-attachments/assets/ec4a4100-b03a-4b3c-a7d6-806ea54ed4ed\n\nA complete **Large Language Model implementation in pure Rust** with no external ML frameworks. Built from the ground up using only `ndarray` for matrix operations.\n\n## ğŸš€ What This Is\n\nThis project demonstrates how to build a transformer-based language model from scratch in Rust, including:\n- **Pre-training** on factual text completion\n- **Instruction tuning** for conversational AI\n- **Interactive chat mode** for testing\n- **Full backpropagation** with gradient clipping\n- **Modular architecture** with clean separation of concerns\n\n## âŒ What This Isn't\n\nThis is not a production grade LLM. It is so far away from the larger models.\n\nThis is just a toy project that demonstrates how these models work under the hood.\n\n## ğŸ” Key Files to Explore\n\nStart with these two core files to understand the implementation:\n\n- **[`src/main.rs`](src/main.rs)** - Training pipeline, data preparation, and interactive mode\n- **[`src/llm.rs`](src/llm.rs)** - Core LLM implementation with forward/backward passes and training logic\n\n## ğŸ—ï¸ Architecture\n\nThe model uses a **transformer-based architecture** with the following components:\n\n```\nInput Text â†’ Tokenization â†’ Embeddings â†’ Transformer Blocks â†’ Output Projection â†’ Predictions\n```\n\n### Project Structure\n\n```\nsrc/\nâ”œâ”€â”€ main.rs              # ğŸ¯ Training pipeline and interactive mode\nâ”œâ”€â”€ llm.rs               # ğŸ§  Core LLM implementation and training logic\nâ”œâ”€â”€ lib.rs               # ğŸ“š Library exports and constants\nâ”œâ”€â”€ transformer.rs       # ğŸ”„ Transformer block (attention + feed-forward)\nâ”œâ”€â”€ self_attention.rs    # ğŸ‘€ Multi-head self-attention mechanism\nâ”œâ”€â”€ feed_forward.rs      # âš¡ Position-wise feed-forward networks\nâ”œâ”€â”€ embeddings.rs        # ğŸ“Š Token embedding layer\nâ”œâ”€â”€ output_projection.rs # ğŸ° Final linear layer for vocabulary predictions\nâ”œâ”€â”€ vocab.rs            # ğŸ“ Vocabulary management and tokenization\nâ”œâ”€â”€ layer_norm.rs       # ğŸ§® Layer normalization\nâ””â”€â”€ adam.rs             # ğŸƒ Adam optimizer implementation\n\ntests/\nâ”œâ”€â”€ llm_test.rs         # Tests for core LLM functionality\nâ”œâ”€â”€ transformer_test.rs # Tests for transformer blocks\nâ”œâ”€â”€ self_attention_test.rs # Tests for attention mechanisms\nâ”œâ”€â”€ feed_forward_test.rs # Tests for feed-forward layers\nâ”œâ”€â”€ embeddings_test.rs  # Tests for embedding layers\nâ”œâ”€â”€ vocab_test.rs       # Tests for vocabulary handling\nâ”œâ”€â”€ adam_test.rs        # Tests for optimizer\nâ””â”€â”€ output_projection_test.rs # Tests for output layer\n```\n\n## ğŸ§ª What The Model Learns\n\nThe implementation includes two training phases:\n\n1. **Pre-training**: Learns basic world knowledge from factual statements\n   - \"The sun rises in the east and sets in the west\"\n   - \"Water flows downhill due to gravity\"\n   - \"Mountains are tall and rocky formations\"\n\n2. **Instruction Tuning**: Learns conversational patterns\n   - \"User: How do mountains form? Assistant: Mountains are formed through tectonic forces...\"\n   - Handles greetings, explanations, and follow-up questions\n\n## ğŸš€ Quick Start\n\n```bash\n# Clone and run\ngit clone https://github.com/tekaratzas/RustGPT.git\ncd RustGPT\ncargo run\n\n# The model will:\n# 1. Build vocabulary from training data\n# 2. Pre-train on factual statements (100 epochs)\n# 3. Instruction-tune on conversational data (100 epochs)\n# 4. Enter interactive mode for testing\n```\n\n## ğŸ® Interactive Mode\n\nAfter training, test the model interactively:\n\n```\nEnter prompt: How do mountains form?\nModel output: Mountains are formed through tectonic forces or volcanism over long geological time periods\n\nEnter prompt: What causes rain?\nModel output: Rain is caused by water vapor in clouds condensing into droplets that become too heavy to remain airborne\n```\n\n## ğŸ§® Technical Implementation\n\n### Model Configuration\n- **Vocabulary Size**: Dynamic (built from training data)\n- **Embedding Dimension**: 128 (defined by `EMBEDDING_DIM` in `src/lib.rs`)\n- **Hidden Dimension**: 256 (defined by `HIDDEN_DIM` in `src/lib.rs`)\n- **Max Sequence Length**: 80 tokens (defined by `MAX_SEQ_LEN` in `src/lib.rs`)\n- **Architecture**: 3 Transformer blocks + embeddings + output projection\n\n### Training Details\n- **Optimizer**: Adam with gradient clipping\n- **Pre-training LR**: 0.0005 (100 epochs)\n- **Instruction Tuning LR**: 0.0001 (100 epochs)\n- **Loss Function**: Cross-entropy loss\n- **Gradient Clipping**: L2 norm capped at 5.0\n\n### Key Features\n- **Custom tokenization** with punctuation handling\n- **Greedy decoding** for text generation\n- **Gradient clipping** for training stability\n- **Modular layer system** with clean interfaces\n- **Comprehensive test coverage** for all components\n\n## ğŸ”§ Development\n\n```bash\n# Run all tests\ncargo test\n\n# Test specific components\ncargo test --test llm_test\ncargo test --test transformer_test\ncargo test --test self_attention_test\n\n# Buil",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-27T02:14:01.720863"
  },
  {
    "basic_info": {
      "name": "shimmy",
      "full_name": "Michael-A-Kuykendall/shimmy",
      "owner": "Michael-A-Kuykendall",
      "description": "âš¡ Python-free Rust inference server â€” OpenAI-API compatible. GGUF + SafeTensors, hot model swap, auto-discovery, single binary. FREE now, FREE forever.",
      "url": "https://github.com/Michael-A-Kuykendall/shimmy",
      "clone_url": "https://github.com/Michael-A-Kuykendall/shimmy.git",
      "ssh_url": "git@github.com:Michael-A-Kuykendall/shimmy.git",
      "homepage": "",
      "created_at": "2025-08-28T22:55:46Z",
      "updated_at": "2025-09-27T01:15:35Z",
      "pushed_at": "2025-09-24T02:59:52Z"
    },
    "stats": {
      "stars": 2615,
      "forks": 172,
      "watchers": 2615,
      "open_issues": 2,
      "size": 213430
    },
    "tech_info": {
      "language": "Rust",
      "languages": {
        "Rust": 617150,
        "C": 158306,
        "C++": 77564,
        "Shell": 46795,
        "TOML": 18131,
        "Python": 14810,
        "TypeScript": 10203,
        "JavaScript": 7526,
        "YAML": 6387,
        "Dockerfile": 4809,
        "Batchfile": 4747,
        "Ruby": 931
      },
      "license": "MIT License",
      "topics": [
        "api-server",
        "command-line-tool",
        "developer-tools",
        "gguf",
        "huggingface",
        "huggingface-models",
        "huggingface-transformers",
        "inference-server",
        "llama",
        "llamacpp",
        "llm-inference",
        "local-ai",
        "lora",
        "machine-learning",
        "ollama-api",
        "openai-compatible",
        "rust",
        "rust-crate",
        "transformers"
      ]
    },
    "content": {
      "readme": "<div align=\"center\">\r\n  <img src=\"assets/shimmy-logo.png\" alt=\"Shimmy Logo\" width=\"300\" height=\"auto\" />\r\n  \r\n  # The Privacy-First Alternative to Ollama\r\n  \r\n  ### ğŸ”’ Local AI Without the Lock-in ğŸš€\r\n\r\n  [![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\r\n  [![Security](https://img.shields.io/badge/Security-Audited-green)](https://github.com/Michael-A-Kuykendall/shimmy/security)\r\n  [![Crates.io](https://img.shields.io/crates/v/shimmy.svg)](https://crates.io/crates/shimmy)\r\n  [![Downloads](https://img.shields.io/crates/d/shimmy.svg)](https://crates.io/crates/shimmy)\r\n  [![Rust](https://img.shields.io/badge/rust-stable-brightgreen.svg)](https://rustup.rs/)\r\n  [![GitHub Stars](https://img.shields.io/github/stars/Michael-A-Kuykendall/shimmy?style=social)](https://github.com/Michael-A-Kuykendall/shimmy/stargazers)\r\n  \r\n  [![ğŸ’ Sponsor this project](https://img.shields.io/badge/ğŸ’_Sponsor_this_project-ea4aaa?style=for-the-badge&logo=github&logoColor=white)](https://github.com/sponsors/Michael-A-Kuykendall)\r\n</div>\r\n\r\n**Shimmy will be free forever.** No asterisks. No \"free for now.\" No pivot to paid.\r\n\r\n### ğŸ’ Support Shimmy's Growth\r\n\r\nğŸš€ **If Shimmy helps you, consider [sponsoring](https://github.com/sponsors/Michael-A-Kuykendall) â€” 100% of support goes to keeping it free forever.**\r\n\r\n- **$5/month**: Coffee tier â˜• - Eternal gratitude + sponsor badge  \r\n- **$25/month**: Bug prioritizer ğŸ› - Priority support + name in [SPONSORS.md](SPONSORS.md)\r\n- **$100/month**: Corporate backer ğŸ¢ - Logo placement + monthly office hours  \r\n- **$500/month**: Infrastructure partner ğŸš€ - Direct support + roadmap input\r\n\r\n[**ğŸ¯ Become a Sponsor**](https://github.com/sponsors/Michael-A-Kuykendall) | See our amazing [sponsors](SPONSORS.md) ğŸ™\r\n\r\n---\r\n\r\n## Drop-in OpenAI API Replacement for Local LLMs\r\n\r\nShimmy is a **5.1MB single-binary** that provides **100% OpenAI-compatible endpoints** for GGUF models. Point your existing AI tools to Shimmy and they just work â€” locally, privately, and free.\r\n\r\n## ğŸ¤” What are you building with Shimmy?\r\n\r\n**New developer tools and specifications included!** Whether you're forking Shimmy for your application or integrating it as a service, we now provide:\r\n\r\n- **ğŸ”§ Integration Templates**: Copy-paste guidance for embedding Shimmy in your projects\r\n- **ğŸ“‹ Development Specifications**: GitHub Spec-Kit methodology for planning Shimmy-based features\r\n- **ğŸ›¡ï¸ Architectural Guarantees**: Constitutional principles ensuring Shimmy stays reliable and lightweight\r\n- **ğŸ“– Complete Documentation**: Everything you need to build on Shimmy's foundation\r\n\r\n**Building something cool with Shimmy?** These tools help you do it systematically and reliably.\r\n\r\n### ğŸš€ **GitHub Spec-Kit Integration**\r\nShimmy now includes [GitHub's brand-new Spec-Kit methodology](https://github.com/github/spec-kit) â€“ specification-driven development that just launched in September 2025! Get professional-grade development workflows:\r\n\r\n- **ğŸ—ï¸ Systematic Development**: `/specify` â†’ `/plan` â†’ `/tasks` â†’ implement\r\n- **ğŸ¤– AI-Native Workflow**: Works with Claude Code, GitHub Copilot, and other AI assistants  \r\n- **ğŸ“‹ Professional Templates**: Complete specification and planning frameworks\r\n- **ğŸ›¡ï¸ Constitutional Protection**: Built-in governance and architectural validation\r\n\r\n[**ğŸ“– Complete Developer Guide â†’**](DEVELOPERS.md) â€¢ [**ğŸ› ï¸ Learn GitHub Spec-Kit â†’**](https://github.com/github/spec-kit)\r\n\r\n### Try it in 30 seconds\r\n\r\n```bash\r\n# 1) Install + run\r\ncargo install shimmy --features huggingface\r\nshimmy serve &\r\n\r\n# 2) See models and pick one\r\nshimmy list\r\n\r\n# 3) Smoke test the OpenAI API\r\ncurl -s http://127.0.0.1:11435/v1/chat/completions \\\r\n  -H 'Content-Type: application/json' \\\r\n  -d '{\r\n        \"model\":\"REPLACE_WITH_MODEL_FROM_list\",\r\n        \"messages\":[{\"role\":\"user\",\"content\":\"Say hi in 5 words.\"}],\r\n        \"max_tokens\":32\r\n      }' | jq -r '.choices[0].message.content'\r\n```\r\n\r\n## ğŸš€ Works with Your Existing Tools\r\n\r\n**No code changes needed** - just change the API endpoint:\r\n\r\n- **VSCode Extensions**: Point to `http://localhost:11435`\r\n- **Cursor Editor**: Built-in OpenAI compatibility  \r\n- **Continue.dev**: Drop-in model provider\r\n- **Any OpenAI client**: Python, Node.js, curl, etc.\r\n\r\n### Use with OpenAI SDKs\r\n\r\n- Node.js (openai v4)\r\n\r\n```ts\r\nimport OpenAI from \"openai\";\r\n\r\nconst openai = new OpenAI({\r\n  baseURL: \"http://127.0.0.1:11435/v1\",\r\n  apiKey: \"sk-local\", // placeholder, Shimmy ignores it\r\n});\r\n\r\nconst resp = await openai.chat.completions.create({\r\n  model: \"REPLACE_WITH_MODEL\",\r\n  messages: [{ role: \"user\", content: \"Say hi in 5 words.\" }],\r\n  max_tokens: 32,\r\n});\r\n\r\nconsole.log(resp.choices[0].message?.content);\r\n```\r\n\r\n- Python (openai>=1.0.0)\r\n\r\n```python\r\nfrom openai import OpenAI\r\n\r\nclient = OpenAI(base_url=\"http://127.0.0.1:11435/v1\", api_key=\"sk-local\")\r\n\r\nresp = client.chat.completions.create(\r\n    model=\"REPLACE_WITH_MODEL\",\r\n    messages=[{\"role\": \"user\", \"content\": \"Say hi i",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-27T02:14:02.875510"
  },
  {
    "basic_info": {
      "name": "ZHO-nano-banana-Creation",
      "full_name": "ZHO-ZHO-ZHO/ZHO-nano-banana-Creation",
      "owner": "ZHO-ZHO-ZHO",
      "description": "æˆ‘çš„ nano-banana åˆ›æ„ç©æ³•å¤§åˆé›†ï¼  æŒç»­æ›´æ–°ä¸­ï¼",
      "url": "https://github.com/ZHO-ZHO-ZHO/ZHO-nano-banana-Creation",
      "clone_url": "https://github.com/ZHO-ZHO-ZHO/ZHO-nano-banana-Creation.git",
      "ssh_url": "git@github.com:ZHO-ZHO-ZHO/ZHO-nano-banana-Creation.git",
      "homepage": null,
      "created_at": "2025-08-28T13:09:00Z",
      "updated_at": "2025-09-27T01:51:40Z",
      "pushed_at": "2025-09-18T22:04:39Z"
    },
    "stats": {
      "stars": 2504,
      "forks": 246,
      "watchers": 2504,
      "open_issues": 3,
      "size": 85
    },
    "tech_info": {
      "language": null,
      "languages": {},
      "license": "GNU General Public License v3.0",
      "topics": []
    },
    "content": {
      "readme": "<img width=\"1152\" height=\"1536\" alt=\"Mask groupå°é¢\" src=\"https://github.com/user-attachments/assets/48d727e7-7998-4d2a-aa37-59beb41ac23e\" />\n\n\n<div align=\"center\">\n   \n# Nano-Banana Creation ZHO\n   \næˆ‘çš„ Nano-Banana åŸåˆ›ç©æ³•å¤§åˆé›† | [å®Œæ•´å¸–](https://x.com/ZHO_ZHO_ZHO/status/1961073677681983926) | My Nano-Banana Creation collection\n\nï¼ï¼ï¼æ³¨æ„æ ‡æ˜å‡ºå¤„å“¦ï¼ï¼ï¼\n\n\n\n<img width=\"1032\" height=\"1373\" alt=\"Group 320\" src=\"https://github.com/user-attachments/assets/8f6ec719-1097-43f5-b56f-ef68123e2203\" />\n\n\n\n<div align=\"left\">\n\n\n# ï¼ˆ1ï¼‰ç›®å½•ï¼ˆ46é¡¹ï¼‰\n\n- [1ï¸âƒ£ å‡ºåœˆ/ç«çš„ç”¨æ³•ï¼šå›¾ç‰‡å˜æ‰‹åŠ/æ‰‹åŠè§†é¢‘](#1ï¸âƒ£-å‡ºåœˆç«çš„ç”¨æ³•å›¾ç‰‡å˜æ‰‹åŠæ‰‹åŠè§†é¢‘)\n- [2ï¸âƒ£ åäºº/æŒ‡å®šäººç‰©ï¼ˆä¸Šä¼ å›¾ç‰‡ï¼‰è¶…å†™å®ç…§ç‰‡çº§ç”Ÿæˆ](#2ï¸âƒ£-åäººæŒ‡å®šäººç‰©ä¸Šä¼ å›¾ç‰‡è¶…å†™å®ç…§ç‰‡çº§ç”Ÿæˆ)\n- [3ï¸âƒ£ æŒ‡å®šäººç‰©çŸ­è§†é¢‘ï¼šäººåƒç‰¹å¾ä¿æŒ + åˆ‡æ¢è§†è§’ + veo3 é¦–å°¾å¸§](#3ï¸âƒ£-æŒ‡å®šäººç‰©çŸ­è§†é¢‘äººåƒç‰¹å¾ä¿æŒ--åˆ‡æ¢è§†è§’--veo3-é¦–å°¾å¸§)\n- [4ï¸âƒ£ å»ºç­‘å›¾è½¬æ¨¡å‹/å»ºæ¨¡](#4ï¸âƒ£-å»ºç­‘å›¾è½¬æ¨¡å‹å»ºæ¨¡)\n- [5ï¸âƒ£ è¿ç»­ç¼–è¾‘ + ç‰©ä½“ç»„åˆ + èƒŒæ™¯è®¾è®¡](#5ï¸âƒ£-è¿ç»­ç¼–è¾‘--ç‰©ä½“ç»„åˆ--èƒŒæ™¯è®¾è®¡)\n- [6ï¸âƒ£ é«˜æ¸…ä¿®å¤](#6ï¸âƒ£-é«˜æ¸…ä¿®å¤)\n- [7ï¸âƒ£ ç‰©ä½“ç»„åˆ/ç‰ˆæœ¬å¯¹æ¯”](#7ï¸âƒ£-ç‰©ä½“ç»„åˆç‰ˆæœ¬å¯¹æ¯”)\n- [8ï¸âƒ£ å•†å“å¹¿å‘ŠçŸ­ç‰‡ï¼šæŒ‡å®šäººç‰© + å•†å“](#8ï¸âƒ£-å•†å“å¹¿å‘ŠçŸ­ç‰‡æŒ‡å®šäººç‰©--å•†å“)\n- [9ï¸âƒ£ äººç¾¤ä¸­åˆ†ç¦»æŒ‡å®šæ¨¡ç³Šäººç‰© + é«˜æ¸…ç”Ÿæˆ](#9ï¸âƒ£-äººç¾¤ä¸­åˆ†ç¦»æŒ‡å®šæ¨¡ç³Šäººç‰©--é«˜æ¸…ç”Ÿæˆ)\n- [1ï¸âƒ£0ï¸âƒ£ å›¾è½¬çº¿ç¨¿ + è‰²å¡ä¸Šè‰²](#1ï¸âƒ£0ï¸âƒ£-å›¾è½¬çº¿ç¨¿--è‰²å¡ä¸Šè‰²)\n- [1ï¸âƒ£1ï¸âƒ£ ä¸€å¥è¯ç”Ÿæˆä¸€å¥—è§’è‰²è®¾å®š/æ•…äº‹ä¹¦](#1ï¸âƒ£1ï¸âƒ£-ä¸€å¥è¯ç”Ÿæˆä¸€å¥—è§’è‰²è®¾å®šæ•…äº‹ä¹¦)\n- [1ï¸âƒ£2ï¸âƒ£ è™šå®ç»“åˆ/è·¨æ¬¡å…ƒï¼šæ’ç”»äººç‰©æ¢åº—](#1ï¸âƒ£2ï¸âƒ£-è™šå®ç»“åˆè·¨æ¬¡å…ƒæ’ç”»äººç‰©æ¢åº—)\n- [1ï¸âƒ£3ï¸âƒ£ æŒ‡å®šäººç‰© + è®¾è®¡ å®æ™¯ä½“éªŒ/æ•ˆæœå›¾](#1ï¸âƒ£3ï¸âƒ£-æŒ‡å®šäººç‰©--è®¾è®¡-å®æ™¯ä½“éªŒæ•ˆæœå›¾)\n- [1ï¸âƒ£4ï¸âƒ£ ç²¾å‡†æ›¿æ¢è§†é¢‘äººç‰©](#1ï¸âƒ£4ï¸âƒ£-ç²¾å‡†æ›¿æ¢è§†é¢‘äººç‰©)\n- [1ï¸âƒ£5ï¸âƒ£ åŠ¨æ¼«è½¬çœŸäººï¼ˆæ¥è¿‘ 1:1 è¿˜åŸï¼‰](#1ï¸âƒ£5ï¸âƒ£-åŠ¨æ¼«è½¬çœŸäººæ¥è¿‘-11-è¿˜åŸ)\n- [1ï¸âƒ£6ï¸âƒ£ é«˜è´¨é‡æ‘„å½±ï¼šæŒ‡å®šäººç‰© + é«˜è´¨é‡å§¿åŠ¿å‚è€ƒ](#1ï¸âƒ£6ï¸âƒ£-é«˜è´¨é‡æ‘„å½±æŒ‡å®šäººç‰©--é«˜è´¨é‡å§¿åŠ¿å‚è€ƒ)\n- [1ï¸âƒ£7ï¸âƒ£ å›¾ç‰‡è½¬äººå¶ç©å…·](#1ï¸âƒ£7ï¸âƒ£-å›¾ç‰‡è½¬äººå¶ç©å…·)\n- [1ï¸âƒ£8ï¸âƒ£ å›¾ç‰‡è½¬-funko-pop-å…¬ä»”](#1ï¸âƒ£8ï¸âƒ£-å›¾ç‰‡è½¬-funko-pop-å…¬ä»”)\n- [1ï¸âƒ£9ï¸âƒ£ å›¾ç‰‡è½¬ä¹é«˜](#1ï¸âƒ£9ï¸âƒ£-å›¾ç‰‡è½¬ä¹é«˜)\n- [2ï¸âƒ£0ï¸âƒ£ å›¾ç‰‡è½¬é’ˆç»‡ç©å¶](#2ï¸âƒ£0ï¸âƒ£-å›¾ç‰‡è½¬é’ˆç»‡ç©å¶)\n- [2ï¸âƒ£1ï¸âƒ£ å›¾ç‰‡è½¬èŠ­æ¯”å¨ƒå¨ƒ](#2ï¸âƒ£1ï¸âƒ£-å›¾ç‰‡è½¬èŠ­æ¯”å¨ƒå¨ƒ)\n- [2ï¸âƒ£2ï¸âƒ£ ä¸‡ç‰©å˜é«˜è¾¾](#2ï¸âƒ£2ï¸âƒ£-ä¸‡ç‰©å˜é«˜è¾¾)\n- [2ï¸âƒ£3ï¸âƒ£ èµ›åšç”Ÿå¨ƒï¼Ÿï¼ä¸¤å¼ äººè„¸ç”Ÿæˆå­©å­è„¸](#2ï¸âƒ£3ï¸âƒ£-èµ›åšç”Ÿå¨ƒä¸¤å¼ äººè„¸ç”Ÿæˆå­©å­è„¸)\n- [2ï¸âƒ£4ï¸âƒ£ äº§å“è®¾è®¡å›¾è½¬çœŸå®æ•ˆæœ/æ¸²æŸ“](#2ï¸âƒ£4ï¸âƒ£-äº§å“è®¾è®¡å›¾è½¬çœŸå®æ•ˆæœæ¸²æŸ“)\n- [2ï¸âƒ£5ï¸âƒ£ éšæ‰‹æ‹ç§’å˜ä¸“ä¸šæ‘„å½±å¤§ç‰‡ï¼Ÿï¼nano-banana-æ‹¯æ•‘ä½ çš„åºŸç‰‡](#2ï¸âƒ£5ï¸âƒ£-éšæ‰‹æ‹ç§’å˜ä¸“ä¸šæ‘„å½±å¤§ç‰‡nano-banana-æ‹¯æ•‘ä½ çš„åºŸç‰‡)\n- [2ï¸âƒ£6ï¸âƒ£ å…‰å½±å‚è€ƒ](#2ï¸âƒ£6ï¸âƒ£-å…‰å½±å‚è€ƒ)\n- [2ï¸âƒ£7ï¸âƒ£ ä½¿ç”¨å…‰å½±äººå¶åšæ‰“å…‰å‚è€ƒ](#2ï¸âƒ£7ï¸âƒ£-ä½¿ç”¨å…‰å½±äººå¶åšæ‰“å…‰å‚è€ƒ)\n- [2ï¸âƒ£8ï¸âƒ£ ç”Ÿæˆç»˜ç”»/æ¸²æŸ“è¿‡ç¨‹å››å®«æ ¼](#2ï¸âƒ£8ï¸âƒ£-ç”Ÿæˆç»˜ç”»æ¸²æŸ“è¿‡ç¨‹å››å®«æ ¼)\n- [2ï¸âƒ£9ï¸âƒ£ ä¸€å¥è¯-ç…§ç‰‡å˜æ’ç”»-è¿˜é™„å¸¦ç»˜ç”»è¿‡ç¨‹](#2ï¸âƒ£9ï¸âƒ£-ä¸€å¥è¯-ç…§ç‰‡å˜æ’ç”»-è¿˜é™„å¸¦ç»˜ç”»è¿‡ç¨‹)\n- [3ï¸âƒ£0ï¸âƒ£ è„¸å‹å‚è€ƒ/æ§åˆ¶ï¼Œç§’å˜å¡é€šå½¢è±¡](#3ï¸âƒ£0ï¸âƒ£-è„¸å‹å‚è€ƒæ§åˆ¶ç§’å˜å¡é€šå½¢è±¡)\n- [3ï¸âƒ£1ï¸âƒ£ ä¸€å¥å’’è¯­ä»»ä½•é£æ ¼å˜å†™å®](#3ï¸âƒ£1ï¸âƒ£-ä¸€å¥å’’è¯­ä»»ä½•é£æ ¼å˜å†™å®)\n- [3ï¸âƒ£2ï¸âƒ£ æ›²é¢å±è´´å›¾](#3ï¸âƒ£2ï¸âƒ£-æ›²é¢å±è´´å›¾)\n- [3ï¸âƒ£3ï¸âƒ£ ç›´æ¥ä¸ºå›¾ç‰‡ä¸­çš„æ›²é¢å¤§å±ç”Ÿæˆ-æŒ‡å®šçš„-è£¸çœ¼-3d-å†…å®¹](#3ï¸âƒ£3ï¸âƒ£-ç›´æ¥ä¸ºå›¾ç‰‡ä¸­çš„æ›²é¢å¤§å±ç”Ÿæˆ-æŒ‡å®šçš„-è£¸çœ¼-3d-å†…å®¹)\n- [3ï¸âƒ£4ï¸âƒ£ ä»»æ„å›¾ç‰‡ï¼ˆæ˜æ˜Ÿ/åŠ¨æ¼«ï¼‰å˜æŒ‚ä»¶æŒ‚åœ¨è‡ªå·±/å¥³æœ‹å‹åŒ…åŒ…ä¸Š](#3ï¸âƒ£4ï¸âƒ£-ä»»æ„å›¾ç‰‡æ˜æ˜ŸåŠ¨æ¼«å˜æŒ‚ä»¶æŒ‚åœ¨è‡ªå·±å¥³æœ‹å‹åŒ…åŒ…ä¸Š)\n- [3ï¸âƒ£5ï¸âƒ£ å åŠ æŒ‡å®š-æè´¨è´¨æ„Ÿ/æ•ˆæœ](#3ï¸âƒ£5ï¸âƒ£-å åŠ æŒ‡å®š-æè´¨è´¨æ„Ÿæ•ˆæœ)\n- [3ï¸âƒ£6ï¸âƒ£ ç…§ç‰‡å˜å¨ƒå¨ƒ](#3ï¸âƒ£6ï¸âƒ£-ç…§ç‰‡å˜å¨ƒå¨ƒ)\n- [3ï¸âƒ£7ï¸âƒ£ äº§å“åŒ…è£…è´´åˆ](#3ï¸âƒ£7ï¸âƒ£-äº§å“åŒ…è£…è´´åˆ)\n- [3ï¸âƒ£8ï¸âƒ£ æŠŠæŒ‡å®šå›¾ç‰‡è´´åœ¨å¤§é˜¶æ¢¯ä¸Š](#3ï¸âƒ£8ï¸âƒ£-æŠŠæŒ‡å®šå›¾ç‰‡è´´åœ¨å¤§é˜¶æ¢¯ä¸Š)\n- [3ï¸âƒ£9ï¸âƒ£ è™šæ‹Ÿè¯•å¦†åŒ–æŒ‡å®šå¦†é¢](#3ï¸âƒ£9ï¸âƒ£-è™šæ‹Ÿè¯•å¦†åŒ–æŒ‡å®šå¦†é¢)\n- [4ï¸âƒ£0ï¸âƒ£ å¦†é¢åˆ†æ-+-ä¼˜åŒ–å»ºè®®](#4ï¸âƒ£0ï¸âƒ£-å¦†é¢åˆ†æ--ä¼˜åŒ–å»ºè®®)\n- [4ï¸âƒ£1ï¸âƒ£ å·¥ä¸šè®¾è®¡-æ‰‹ç»˜-ç§’å˜-å®æ™¯æ•ˆæœ](#4ï¸âƒ£1ï¸âƒ£-å·¥ä¸šè®¾è®¡-æ‰‹ç»˜-ç§’å˜-å®æ™¯æ•ˆæœ)\n- [4ï¸âƒ£2ï¸âƒ£ å·¥ä¸šè®¾è®¡å¥—å›¾é©¬å…‹ç¬”ã€æ°´å½©ã€åˆ†æå›¾ã€æ¸²æŸ“å›¾](#4ï¸âƒ£2ï¸âƒ£-å·¥ä¸šè®¾è®¡å¥—å›¾é©¬å…‹ç¬”æ°´å½©åˆ†æå›¾æ¸²æŸ“å›¾)\n- [4ï¸âƒ£3ï¸âƒ£ è¡¨æƒ…å‡†ç¡®å‚è€ƒåŠ¨æ¼«ã€çœŸäººéƒ½æ²¡é—®é¢˜](#4ï¸âƒ£3ï¸âƒ£-è¡¨æƒ…å‡†ç¡®å‚è€ƒåŠ¨æ¼«çœŸäººéƒ½æ²¡é—®é¢˜)\n- [4ï¸âƒ£4ï¸âƒ£ åŠ¨ç‰©æ‹Ÿäººè¡¨æƒ…](#4ï¸âƒ£4ï¸âƒ£-åŠ¨ç‰©æ‹Ÿäººè¡¨æƒ…)\n- [4ï¸âƒ£5ï¸âƒ£ ç»ç¾å¡ç‰‡è®¾è®¡](#4ï¸âƒ£5ï¸âƒ£-ç»ç¾å¡ç‰‡è®¾è®¡)\n- [4ï¸âƒ£6ï¸âƒ£ å¤šäººç‰©æ’ç”»é›†](#4ï¸âƒ£6ï¸âƒ£-å¤šäººç‰©æ’ç”»é›†)\n\n\n# ï¼ˆ2ï¼‰åœ¨çº¿ä½“éªŒ/æœ¬åœ°éƒ¨ç½²ï¼ˆå¼€æºï¼‰ï¼šğŸŒ ä¸‰ä»¶å¥—\n\n\n- çª—å£å¼ï¼š[1ï¸âƒ£ Nano Bananaryï½œé¦™è•‰è¶…å¸‚](#1ï¸âƒ£-nano-bananaryé¦™è•‰è¶…å¸‚)\n\n- ç™½æ¿/ç”»å¸ƒå¼ï¼š[2ï¸âƒ£ BananaPodï½œé¦™è•‰é“ºå­](#2ï¸âƒ£-bananapodé¦™è•‰é“ºå­)\n\n- èŠ‚ç‚¹/å·¥ä½œæµå¼ï¼š[3ï¸âƒ£ BananaFlowï½œé¦™è•‰å·¥å‚](#3ï¸âƒ£-bananaflowé¦™è•‰å·¥å‚)\n\n\n\n## 1ï¸âƒ£ Nano Bananaryï½œé¦™è•‰è¶…å¸‚\n\n<img width=\"1251\" height=\"2051\" alt=\"Group 336\" src=\"https://github.com/user-attachments/assets/51a70ae5-9f94-4dc3-88ca-8ef688d1ee5c\" />\n\n\næ‰€æœ‰ç©æ³•å·²é™†ç»­ä¸Šçº¿æˆ‘è‡ªå·±çš„å¼€æº APPï¼šNano Bananaryï½œé¦™è•‰è¶…å¸‚ï¼Œæ— éœ€æç¤ºè¯ï¼Œä¸æ»‘è¡”æ¥ï¼Œå·²ä¸Šçº¿ è§†é¢‘ç”Ÿæˆ åŠŸèƒ½\n\n\nä¸€é”®ç›´è¾¾ï¼š[Nano Bananaryï½œé¦™è•‰è¶…å¸‚](https://github.com/ZHO-ZHO-ZHO/Nano-Bananary)\n\n\n\n## 2ï¸âƒ£ BananaPodï½œé¦™è•‰é“ºå­\n\n\n<img width=\"2261\" height=\"1410\" alt=\"screenshot-20250909-020322\" src=\"https://github.com/user-attachments/assets/9997b969-c773-4f7b-9c0b-dfeed5cf9e71\" />\n\nå„ç§ç©æ³•å·²å†…ç½®ï¼Œæ”¯æŒæ‰‹ç»˜ç”Ÿå›¾ï¼Œæ”¯æŒå¤šå›¾æ¡†é€‰ï¼Œè½»æ¾æ„å»ºåˆ›æ„ç”»æ¿\n\nä¸€é”®ç›´è¾¾ï¼š[BananaPodï½œé¦™è•‰é“ºå­](https://github.com/ZHO-ZHO-ZHO/BananaPod)\n\n\n\n## 3ï¸âƒ£ BananaFlowï½œé¦™è•‰å·¥å‚\n\n<img width=\"2667\" height=\"3518\" alt=\"Group 357\" src=\"https://github.com/user-attachments/assets/a934c87c-90a6-4628-8866-428c07eb434d\" />\n\nNanoBanana + Veo çš„å¼€æºå·¥ä½œæµåˆ›æ„å¹³å°ï¼Œå·¥ä½œæµ + çª—å£ åŒæ¨¡å¼çºµäº«ä¸æ»‘ï¼Œå„ç§ç©æ³•å·²å†…ç½®ï¼Œè½»æ¾æ„å»ºåˆ›æ„å·¥ä½œæµ\n\nä¸€é”®ç›´è¾¾ï¼š[BananaFlowï½œé¦™è•‰å·¥å‚](https://github.com/ZHO-ZHO-ZHO/BananaFlow-ZHO)\n\n\n\n# ï¼ˆ3ï¼‰åŸåˆ›ç©æ³• + æç¤ºè¯\n\n## 1ï¸âƒ£ å‡ºåœˆ/ç«çš„ç”¨æ³•ï¼šå›¾ç‰‡å˜æ‰‹åŠ/æ‰‹åŠè§†é¢‘\n\nhttps://github.com/user-attachments/assets/6440b1eb-4ecb-4d77-8dba-63c6d61c1ad3\n\n1ï¼‰å˜æ‰‹åŠ Promptï¼š\n\n```\nturn this photo into a character figure. Behind it, place a box with the characterâ€™s image printed on it, and a computer showing the Blender modeling process on its screen. In front of the box, add a round plastic base with the character figure standing on it. set the scene indoors if possible\n```\n\n2ï¼‰å˜è§†é¢‘ Promptï¼ˆVeo3ï¼‰ï¼š\n\n```\nA pair of hands picks up the figurine and examines it closely\n```\n\n<details>\n<summary>æˆ‘çš„åŸå¸–</summary>\n\n  \n[å›¾ç‰‡å˜æ‰‹åŠ](https://x.com/ZHO_ZHO_ZHO/status/1958539464994959715)\n\n[å›¾ç‰‡å˜æ‰‹åŠä¼˜åŒ–ç‰ˆ](https://x.com/ZHO_ZHO_ZHO/status/1959524430520135749)\n\n[æ‰‹åŠå˜è§†é¢‘](https://x.com/ZHO_ZHO_ZHO/status/1958550998815023573)\n\n\n</details>\n\n\n\n## 2ï¸âƒ£ åäºº/æŒ‡å®šäººç‰©ï¼ˆä¸Šä¼ å›¾ç‰‡ï¼‰è¶…å†™å®ç…§ç‰‡çº§ç”Ÿæˆ\n\n\n<img width=\"894\" height=\"900\" alt=\"image\" src=\"https://github.com/user-attachments/assets/0a0fd607-66c4-4993-bd10-fe3f1b734ef0\" />\n\n\nPromptï¼š\n\n```\nç”»é¢é‡‡ç”¨ä¸­æ™¯è¿‘ä¹åŠèº«çš„æ„å›¾ï¼Œé•œå¤´ä¸äººç‰©å‡ ä¹å¹³è§†ï¼Œä½†é€è§†æ„Ÿå¼ºçƒˆï¼Œä½†å› ä¸ºä¸»ä½“å¾®å¾®å‰å€¾ï¼Œè§†è§‰ä¸Šäº§ç”Ÿä¸€ç§ç•¥å¸¦ä¿¯è§†æ„Ÿçš„å‹ç¼©æ•ˆæœï¼Œè®©è§‚è€…ä¸æ¨¡ç‰¹ä¹‹é—´çš„è·ç¦»æ˜¾å¾—äº²å¯†è€Œç›´æ¥ã€‚äººç‰©å¾®å¾®æŠ¬å¤´å†²å‘é•œå¤´ï¼Œæœ‰ç§æ‹½å§çš„æ„Ÿè§‰ã€‚é—ªå…‰ç¯ä»æ­£é¢åå·¦ä¸Šæ–¹æ‰“æ¥ï¼Œåˆ¶é€ å‡ºç¡¬æœ—çš„é«˜å…‰ä¸æ·±é‡é˜´å½±â€”â€”å¢¨é•œé•œç‰‡ä¸Šæœ‰æ˜æ˜¾é«˜å…‰åå°„ï¼Œäººç‰©åæ–¹å¢™é¢å‡ºç°æ·¡æ·¡çš„æŠ•å½±ï¼Œæ•´ä½“å‘ˆç°å…¸å‹çš„â€œç›´é—ªâ€è´¨æ„Ÿï¼šé¢—ç²’æ„Ÿè½»å¾®ï¼Œå¯è§èƒ¶ç‰‡é£æ ¼æˆ–é«˜æ„Ÿå…‰åº¦æ•°ç æ‹æ‘„çš„ç²—ç²çº¹ç†ã€‚äººç‰©é¢éƒ¨ç¨ç¨æœ‰ç‚¹è¿‡åº¦æ›å…‰\n\nè‰²å½©åŸºè°ƒä»¥ä½é¥±å’Œçš„ä¸­æ€§è‰²ä¸ºä¸»ï¼šé»‘è‰²çš„å®½è‚©ä¸ç»’è¥¿è£…å¤–å¥—å æ®ç”»é¢æœ€å¤§é¢ç§¯ï¼Œé¢æ–™ä¼¼ä¹æ˜¯ä¸ç»’ï¼Œå¸¦æœ‰ç»†è…»çš„ç»’é¢çº¹è·¯ï¼›å†…æ­çš„é»‘è‰²ä¸ç»¸åŠå¸¦ï¼ˆæ¬è¿ä¸æ ‡å‡ºå¤„æ­»å¦ˆï¼‰ã€‚æ¨¡ç‰¹ä¸‹è£…æ˜¯é»‘è‰²è¶…çŸ­è£™å’Œè–„é€è´¨æ„Ÿçš„è¿è£¤è¢œã€‚èƒŒæ™¯å·¦ä¾§å †æ”¾çš„å¢¨ç»¿è‰²ä¸å’–è‰²æ£’çƒå¸½ã€å³ä¾§å æ”¾çš„äº®è“è‰²å¤¹å…‹åŠé»‘è‰²å¤´ç›”ç­‰æ‚ç‰©ï¼Œåœ¨æŸ”å’Œé˜´",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-27T02:14:04.027486"
  },
  {
    "basic_info": {
      "name": "Qwen3-Omni",
      "full_name": "QwenLM/Qwen3-Omni",
      "owner": "QwenLM",
      "description": "Qwen3-omni is a natively end-to-end, omni-modal LLM developed by the Qwen team at Alibaba Cloud, capable of understanding text, audio, images, and video, as well as generating speech in real time.",
      "url": "https://github.com/QwenLM/Qwen3-Omni",
      "clone_url": "https://github.com/QwenLM/Qwen3-Omni.git",
      "ssh_url": "git@github.com:QwenLM/Qwen3-Omni.git",
      "homepage": null,
      "created_at": "2025-09-21T09:46:10Z",
      "updated_at": "2025-09-27T02:13:02Z",
      "pushed_at": "2025-09-27T02:03:00Z"
    },
    "stats": {
      "stars": 2155,
      "forks": 87,
      "watchers": 2155,
      "open_issues": 1,
      "size": 26895
    },
    "tech_info": {
      "language": "Jupyter Notebook",
      "languages": {
        "Jupyter Notebook": 38834506,
        "Python": 29676
      },
      "license": "Apache License 2.0",
      "topics": []
    },
    "content": {
      "readme": "# Qwen3-Omni\n\n<br>\n\n<p align=\"center\">\n    <img src=\"https://qianwen-res.oss-cn-beijing.aliyuncs.com//Qwen3-Omni/qwen3_omni_logo.png\" width=\"400\"/>\n<p>\n\n<p align=\"center\">\n        ğŸ’œ <a href=\"https://chat.qwen.ai/\"><b>Qwen Chat</b></a>&nbsp&nbsp | &nbsp&nbspğŸ¤— <a href=\"https://huggingface.co/collections/Qwen/qwen3-omni-68d100a86cd0906843ceccbe\">Hugging Face</a>&nbsp&nbsp | &nbsp&nbspğŸ¤– <a href=\"https://modelscope.cn/collections/Qwen3-Omni-867aef131e7d4f\">ModelScope</a>&nbsp&nbsp | &nbsp&nbspğŸ“‘ <a href=\"https://qwen.ai/blog?id=65f766fc2dcba7905c1cb69cc4cab90e94126bf4&from=research.latest-advancements-list\">Blog</a>&nbsp&nbsp | &nbsp&nbspğŸ“š <a href=\"https://github.com/QwenLM/Qwen3-Omni/tree/main/cookbooks\">Cookbooks</a>&nbsp&nbsp | &nbsp&nbspğŸ“‘ <a href=\"https://arxiv.org/pdf/2509.17765\">Paper</a>&nbsp&nbsp\n<br>\nğŸ–¥ï¸ <a href=\"https://huggingface.co/spaces/Qwen/Qwen3-Omni-Demo\">Hugging Face Demo</a>&nbsp&nbsp | &nbsp&nbsp ğŸ–¥ï¸ <a href=\"https://modelscope.cn/studios/Qwen/Qwen3-Omni-Demo\">ModelScope Demo</a>&nbsp&nbsp | &nbsp&nbspğŸ’¬ <a href=\"https://github.com/QwenLM/Qwen/blob/main/assets/wechat.png\">WeChat (å¾®ä¿¡)</a>&nbsp&nbsp | &nbsp&nbspğŸ«¨ <a href=\"https://discord.gg/CV4E9rpNSD\">Discord</a>&nbsp&nbsp | &nbsp&nbspğŸ“‘ <a href=\"https://help.aliyun.com/zh/model-studio/user-guide/qwen-omni\">API</a>\n\n</p>\n\nWe release **Qwen3-Omni**, the natively end-to-end multilingual omni-modal foundation models. It is designed to process diverse inputs including text, images, audio, and video, while delivering real-time streaming responses in both text and natural speech. Click the video below for more information ğŸ˜ƒ\n\n<details open>\n<summary>English Version</summary>\n<a href=\"https://youtu.be/_zdOrPju4_g\" target=\"_blank\">\n  <img src=\"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/videocover.png\" alt=\"Open English Video\"/>\n</a>\n</details>\n\n<details>\n<summary>Chinese Version</summary>\n<a href=\"https://youtu.be/Wtjsw5deXfQ\" target=\"_blank\">\n  <img src=\"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/videocover.png\" alt=\"æ‰“å¼€ä¸­æ–‡è§†é¢‘\"/>\n</a>\n</details>\n\n\n## News\n* 2025.09.26: â­ï¸â­ï¸â­ï¸ Qwen3-Omni reaches top-1 on Hugging Face Trending! \n* 2025.09.22: ğŸ‰ğŸ‰ğŸ‰ We have released [Qwen3-Omni](https://huggingface.co/collections/Qwen/qwen3-omni-68d100a86cd0906843ceccbe). For more details, please check our [blog](https://qwen.ai/blog?id=65f766fc2dcba7905c1cb69cc4cab90e94126bf4&from=research.latest-advancements-list)!\n\n## Contents <!-- omit in toc -->\n\n- [Overview](#overview)\n  - [Introduction](#introduction)\n  - [Model Architecture](#model-architecture)\n  - [Cookbooks for Usage Cases](#cookbooks-for-usage-cases)\n- [QuickStart](#quickstart)\n  - [Model Description and Download](#model-description-and-download)\n  - [Transformers Usage](#transformers-usage)\n  - [vLLM Usage](#vllm-usage)\n  - [DashScope API Usage](#dashscope-api-usage)\n  - [Usage Tips (Recommended Reading)](#usage-tips-recommended-reading)\n- [Interaction with Qwen3-Omni](#interaction-with-qwen3-omni)\n  - [Online Demo](#online-demo)\n  - [Real-Time Interaction](#real-time-interaction)\n  - [Launch Local Web UI Demo](#launch-local-web-ui-demo)\n- [Docker](#-docker)\n- [Evaluation](#evaluation)\n  - [Performance of Qwen3-Omni](#performance-of-qwen3-omni)\n  - [Setting for Evaluation](#setting-for-evaluation)\n- [Citation](#citation)\n\n## Overview\n### Introduction\n\n<p align=\"center\">\n    <img src=\"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/q3o_introduction.png\" width=\"90%\"/>\n<p>\n\nQwen3-Omni is the natively end-to-end multilingual omni-modal foundation models. It processes text, images, audio, and video, and delivers real-time streaming responses in both text and natural speech. We introduce several architectural upgrades to improve performance and efficiency. Key features:\n\n* **State-of-the-art across modalities**: Early text-first pretraining and mixed multimodal training provide native multimodal support. While achieving strong audio and audio-video results, unimodal text and image performance does not regress. Reaches SOTA on 22 of 36 audio/video benchmarks and open-source SOTA on 32 of 36; ASR, audio understanding, and voice conversation performance is comparable to Gemini 2.5 Pro.\n\n* **Multilingual**: Supports 119 text languages, 19 speech input languages, and 10 speech output languages.\n  - **Speech Input**: English, Chinese, Korean, Japanese, German, Russian, Italian, French, Spanish, Portuguese, Malay, Dutch, Indonesian, Turkish, Vietnamese, Cantonese, Arabic, Urdu.\n  - **Speech Output**: English, Chinese, French, German, Russian, Italian, Spanish, Portuguese, Japanese, Korean.\n\n* **Novel Architecture**: MoE-based Thinkerâ€“Talker design with AuT pretraining for strong general representations, plus a multi-codebook design that drives latency to a minimum.\n\n* **Real-time Audio/Video Interaction**: Low-latency streaming with natural turn-taking and immediate text or speech responses.\n\n* **Flexible Control**: Customize behavior via system prompts for fine-grained control and easy ",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-27T02:14:05.180115"
  },
  {
    "basic_info": {
      "name": "Dayflow",
      "full_name": "JerryZLiu/Dayflow",
      "owner": "JerryZLiu",
      "description": "Generate a timeline of your day, automatically",
      "url": "https://github.com/JerryZLiu/Dayflow",
      "clone_url": "https://github.com/JerryZLiu/Dayflow.git",
      "ssh_url": "git@github.com:JerryZLiu/Dayflow.git",
      "homepage": null,
      "created_at": "2025-09-23T01:58:21Z",
      "updated_at": "2025-09-27T02:11:46Z",
      "pushed_at": "2025-09-26T00:34:07Z"
    },
    "stats": {
      "stars": 1878,
      "forks": 55,
      "watchers": 1878,
      "open_issues": 20,
      "size": 41104
    },
    "tech_info": {
      "language": "Swift",
      "languages": {
        "Swift": 736897,
        "Shell": 25722
      },
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "\n<div align=\"center\">\n  <img src=\"docs/images/dayflow_header.png\" alt=\"Dayflow\" width=\"400\">\n</div>\n\n<div align=\"center\">\n  <em>A timeline of your day, automatically.</em><br>\n  Turns your screen activity into a clean timeline with AI summaries and distraction highlights.\n</div>\n\n<div align=\"center\">\n  <!-- Badges -->\n  <img src=\"https://img.shields.io/badge/macOS-13%2B-000?logo=apple\" alt=\"Platform: macOS 13+\">\n  <img src=\"https://img.shields.io/badge/SwiftUI-âœ“-orange\" alt=\"SwiftUI\">\n  <img src=\"https://img.shields.io/badge/Updates-Sparkle-informational\" alt=\"Updates: Sparkle\">\n  <img src=\"https://img.shields.io/badge/AI-Gemini%20or%20Local-blue\" alt=\"AI: Gemini / Local\">\n  <img src=\"https://img.shields.io/badge/License-MIT-green\" alt=\"License: MIT\">\n</div>\n\n<div align=\"center\">\n  <img src=\"docs/images/hero_animation_1080p.gif\" alt=\"Dayflow Hero Animation\" width=\"800\">\n</div>\n\n<div align=\"center\">\n  <a href=\"https://github.com/JerryZLiu/Dayflow/releases/latest\">\n    <img src=\"https://img.shields.io/badge/Download%20for%20Mac-â¬‡%20%20Dayflow.dmg-blue?style=for-the-badge&logo=apple\" alt=\"Download for Mac\">\n  </a>\n</div>\n\n<p align=\"center\">\n  <a href=\"#quickstart\">Quickstart</a> â€¢\n  <a href=\"#why-i-built-dayflow\">Why I built Dayflow</a> â€¢\n  <a href=\"#features\">Features</a> â€¢\n  <a href=\"#how-it-works\">How it works</a> â€¢\n  <a href=\"#installation\">Installation</a> â€¢\n  <a href=\"#data--privacy\">Data & Privacy</a> â€¢\n  <a href=\"#debug--developer-tools\">Debug & Developer Tools</a> â€¢\n  <a href=\"#auto-updates-sparkle\">Autoâ€‘updates</a> â€¢\n  <a href=\"#contributing\">Contributing</a>\n</p>\n\n---\n\n## What is Dayflow?\n\nDayflow is a **native macOS app** (SwiftUI) that records your screen at **1 FPS**, analyzes it **every 15 minutes** with AI, and generates a **timeline** of your activities with summaries. \nIt's lightweight (25MB app size) and uses ~100MB of RAM and <1% cpu. \n\n> _Privacyâ€‘minded by design_: You choose your AI provider. Use **Gemini** (bring your own API key) or **local models** (Ollama / LM Studio). See **Data & Privacy** for details.\n\n\n## Why I built Dayflow\n\nI built Dayflow after realizing that my calendar wasn't the source of truth for how I actually spent my time. My screen was. I wanted a calm, trustworthy timeline that let me see my workday without turning into yet another dashboard I had to maintain.\n\nDayflow stands for ownership and privacy by default. You control the data, you choose the AI provider, and you can keep everything local if that's what makes you comfortable. It's MIT licensed and fully open source because anything that watches your screen all day should be completely transparent about what it does with that information. The app should feel like a quiet assistant: respectful of your attention, honest about what it captures, and easy to shut off.\n\n\n---\n\n## Features\n\n- **Automatic timeline** of your day with concise summaries.\n- **1 FPS recording** - minimal CPU/storage impact.\n- **15-minute analysis intervals** for timely updates.\n- **Watch timelapses of your day**.\n- **Auto storage cleanup** - removes old recordings after 3 days.\n- **Distraction highlights** to see what pulled you offâ€‘task.\n- **Native UX** built with **SwiftUI**.\n- **Autoâ€‘updates** with **Sparkle** (daily check + background download).\n\n### Coming soon\n\n- **Infinitely customizable dashboard** â€” ask any question about your workday, pipe the answers into tiles you arrange yourself, and track trends over time.\n\n  <div align=\"center\">\n    <img src=\"docs/images/DashboardPreview.png\" alt=\"Dayflow dashboard preview\" width=\"800\">\n  </div>\n\n- **Daily journal** â€” review the highlights Dayflow captured, reflect with guided prompts, and drop screenshots or notes alongside your generated timeline.\n\n  <div align=\"center\">\n    <img src=\"docs/images/JournalPreview.png\" alt=\"Dayflow journal preview\" width=\"800\">\n  </div>\n\n## How it works\n\n1) **Capture** â€” Records screen at 1 FPS in 15-second chunks.\n2) **Analyze** â€” Every 15 minutes, sends recent footage to AI.\n3) **Generate** â€” AI creates timeline cards with activity summaries.\n4) **Display** â€” Shows your day as a visual timeline.\n5) **Cleanup** â€” Auto-deletes recordings older than 3 days.\n\n### AI Processing Pipeline\n\nThe efficiency of your timeline generation depends on your chosen AI provider:\n\n```mermaid\nflowchart LR\n    subgraph Gemini[\"Gemini Flow: 2 LLM Calls\"]\n        direction LR\n        GV[Video] --> GU[Upload + Transcribe<br/>1 LLM call] --> GC[Generate Cards<br/>1 LLM call] --> GD[Done]\n    end\n\n    subgraph Local[\"Local Flow: 33+ LLM Calls\"]\n        direction LR\n        LV[Video] --> LE[Extract 30 frames] --> LD[30 descriptions<br/>30 LLM calls] --> LM[Merge<br/>1 call] --> LT[Title<br/>1 call] --> LC[Merge Check<br/>1 call] --> LMC[Merge Cards<br/>1 call] --> LD2[Done]\n    end\n\n    %% Styling\n    classDef geminiFlow fill:#e8f5e8,stroke:#4caf50,stroke-width:2px\n    classDef localFlow fill:#fff8e1,stroke:#ff9800,stroke-width:2px\n    classDef geminiStep fill:#4caf50,color:#fff\n    cla",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-27T02:14:06.327901"
  },
  {
    "basic_info": {
      "name": "linear-algebra",
      "full_name": "little-book-of/linear-algebra",
      "owner": "little-book-of",
      "description": "A concise, beginner-friendly introduction to the core ideas of linear algebra.",
      "url": "https://github.com/little-book-of/linear-algebra",
      "clone_url": "https://github.com/little-book-of/linear-algebra.git",
      "ssh_url": "git@github.com:little-book-of/linear-algebra.git",
      "homepage": "https://little-book-of.github.io/linear-algebra/",
      "created_at": "2025-09-02T03:28:30Z",
      "updated_at": "2025-09-27T00:30:35Z",
      "pushed_at": "2025-09-26T09:42:31Z"
    },
    "stats": {
      "stars": 1752,
      "forks": 53,
      "watchers": 1752,
      "open_issues": 7,
      "size": 11916
    },
    "tech_info": {
      "language": "Jupyter Notebook",
      "languages": {
        "Jupyter Notebook": 2325068,
        "TeX": 1387617,
        "Makefile": 1007
      },
      "license": "Other",
      "topics": []
    },
    "content": {
      "readme": "# The Little Book of Linear Algebra\n\nA concise, beginner-friendly introduction to the core ideas of Linear Algebra. \n\nA clear, hands-on guide to the geometry and structure behind vectors and matrices.\n\n## Formats\n\n- [Download PDF](releases/book.pdf) - print-ready\n- [Download EPUB](releases/book.epub) - e-reader friendly\n- [View LaTeX](releases/book.tex) - `.tex` source\n- [Read on GitHub Pages](https://little-book-of.github.io/linear-algebra/) - online website\n- [Notebook](https://little-book-of.github.io/linear-algebra/books/en-US/lab.html) - Learning with Python\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](\n  https://colab.research.google.com/github/little-book-of/linear-algebra/blob/main/releases/lab.ipynb\n)\n\nFor old version (v1), here is the link\n\n- [Download PDF (v1)](archived/v1/book.pdf) - print-ready\n- [Download EPUB (v1)](archived/v1/book.epub) - e-reader friendly\n- [View LaTeX (v1)](archived/v1/book.tex) - `.tex` source\n\n## Build it yourself (Quarto)\n\nWe use [Quarto](https://quarto.org/docs/get-started/) to generate all outputs.\n\nPreview locally:\n\n```bash\nquarto preview\n```\n\nRender outputs:\n\n```bash\n# All configured formats\nquarto render\n\n# Individual formats\nquarto render --to html     # site into docs/\nquarto render --to pdf      # docs/book.pdf\nquarto render --to epub     # docs/book.epub\nquarto render --to latex    # docs/book-latex/book.tex\n```\n\n# The Book\n\n## Chapter 1. Vectors, scalars, and geometry \n\n#### Opening\n```\nArrows in the air,\ndirections whisper softlyâ€”\nthe plane comes alive.\n```\n\n### 1. Scalars, Vectors, and Coordinate Systems\n\nWhen we begin learning linear algebra, everything starts with the simplest building blocks: scalars and vectors. A scalar is just a single number, like 3, â€“7, or Ï€. It carries only magnitude and no direction. Scalars are what we use for counting, measuring length, or scaling other objects up and down. A vector, by contrast, is an ordered collection of numbers. You can picture it as an arrow pointing somewhere in space, or simply as a list like (2, 5) in 2D or (1, â€“3, 4) in 3D. Where scalars measure \"how much,\" vectors measure both \"how much\" and \"which way.\"\n\n#### Coordinate Systems\n\nTo talk about vectors, we need a coordinate system. Imagine laying down two perpendicular axes on a sheet of paper: the x-axis (left to right) and the y-axis (up and down). Every point on the sheet can be described with two numbers: how far along the x-axis, and how far along the y-axis. This pair of numbers is a vector in 2D. Add a z-axis pointing up from the page, and you have 3D space. Each coordinate system gives us a way to describe vectors numerically, even though the underlying \"space\" is the same.\n\n#### Visualizing Scalars vs. Vectors\n\n- A scalar is like a single tick mark on a ruler.\n- A vector is like an arrow that starts at the origin (0, 0, â€¦) and ends at the point defined by its components.\n  For example, the vector (3, 4) in 2D points from the origin to the point 3 units along the x-axis and 4 units along the y-axis.\n\n#### Why Start Here?\n\nUnderstanding the difference between scalars and vectors is the foundation for everything else in linear algebra. Every concept-matrices, linear transformations, eigenvalues-eventually reduces to how we manipulate vectors and scale them with scalars. Without this distinction, the rest of the subject would have no anchor.\n\n#### Why It Matters\n\nNearly every field of science and engineering depends on this idea. Physics uses vectors for velocity, acceleration, and force. Computer graphics uses them to represent points, colors, and transformations. Data science treats entire datasets as high-dimensional vectors. By mastering scalars and vectors early, you unlock the language in which modern science and technology are written.\n\n#### Try It Yourself\n\n1. Draw an x- and y-axis on a piece of paper. Plot the vector (2, 3).\n2. Now draw the vector (â€“1, 4). Compare their directions and lengths.\n3. Think: which of these two vectors points \"more upward\"? Which is \"longer\"?\n\nThese simple experiments already give you intuition for the operations you'll perform again and again in linear algebra.\n\n### 2. Vector Notation, Components, and Arrows\n\nLinear algebra gives us powerful ways to describe and manipulate vectors, but before we can do anything with them, we need a precise notation system. Notation is not just cosmetic-it tells us how to read, write, and think about vectors clearly and unambiguously. In this section, we'll explore how vectors are written, how their components are represented, and how we can interpret them visually as arrows.\n\n#### Writing Vectors\n\nVectors are usually denoted by lowercase letters in bold (like $\\mathbf{v}, \\mathbf{w}, \\mathbf{x}$)  \nor with an arrow overhead (like $\\vec{v}$).  \n\nFor instance, the vector $\\mathbf{v} = (2, 5)$ is the same as $\\vec{v} = (2, 5)$.  \n\nThe style depends on context: mathematicians often use bold, physicists often use arrows.  \nIn handwritten notes, people",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-27T02:14:07.525803"
  },
  {
    "basic_info": {
      "name": "map-anything",
      "full_name": "facebookresearch/map-anything",
      "owner": "facebookresearch",
      "description": "MapAnything: Universal Feed-Forward Metric 3D Reconstruction",
      "url": "https://github.com/facebookresearch/map-anything",
      "clone_url": "https://github.com/facebookresearch/map-anything.git",
      "ssh_url": "git@github.com:facebookresearch/map-anything.git",
      "homepage": "",
      "created_at": "2025-09-04T14:37:36Z",
      "updated_at": "2025-09-26T22:01:57Z",
      "pushed_at": "2025-09-25T18:12:18Z"
    },
    "stats": {
      "stars": 1748,
      "forks": 80,
      "watchers": 1748,
      "open_issues": 22,
      "size": 6043
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 1727145,
        "Shell": 157713
      },
      "license": "Apache License 2.0",
      "topics": [
        "3d-reconstruction",
        "ai",
        "calibration",
        "depth-completion",
        "depth-estimation",
        "image-to-3d",
        "multi-view-stereo",
        "robotics",
        "sfm"
      ]
    },
    "content": {
      "readme": "<div align=\"center\">\n<h1>MapAnything: Universal Feed-Forward Metric <br>3D Reconstruction</h1>\n<a href=\"https://map-anything.github.io/assets/MapAnything.pdf\"><img src=\"https://img.shields.io/badge/Paper-blue\" alt=\"Paper\"></a>\n<a href=\"https://arxiv.org/abs/2509.13414\"><img src=\"https://img.shields.io/badge/arXiv-2509.13414-b31b1b\" alt=\"arXiv\"></a>\n<a href=\"https://map-anything.github.io/\"><img src=\"https://img.shields.io/badge/Project_Page-green\" alt=\"Project Page\"></a>\n<a href=\"https://huggingface.co/spaces/facebook/map-anything\"><img src='https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Demo-blue'></a>\n<br>\n<br>\n<strong>\n<a href=\"https://nik-v9.github.io/\">Nikhil Keetha<sup>1,2</sup></a>\n&nbsp;&nbsp;\n<a href=\"https://sirwyver.github.io/\">Norman MÃ¼ller<sup>1</sup></a>\n&nbsp;&nbsp;\n<a href=\"https://demuc.de/\">Johannes SchÃ¶nberger<sup>1</sup></a>\n&nbsp;&nbsp;\n<a href=\"https://www.linkedin.com/in/lorenzoporzi\">Lorenzo Porzi<sup>1</sup></a>\n&nbsp;&nbsp;\n<a href=\"https://infinity1096.github.io/\">Yuchen Zhang<sup>2</sup></a>\n<br>\n<a href=\"https://tobiasfshr.github.io/\">Tobias Fischer<sup>1</sup></a>\n&nbsp;&nbsp;\n<a href=\"https://www.linkedin.com/in/arno-knapitsch\">Arno Knapitsch<sup>1</sup></a>\n&nbsp;&nbsp;\n<a href=\"https://www.linkedin.com/in/duncan-zauss\">Duncan Zauss<sup>1</sup></a>\n&nbsp;&nbsp;\n<a href=\"https://ethanweber.me/\">Ethan Weber<sup>1</sup></a>\n&nbsp;&nbsp;\n<a href=\"https://www.linkedin.com/in/nelsonantunes7\">Nelson Antunes<sup>1</sup></a>\n<br>\n<a href=\"https://x.com/jonathonluiten?lang=en\">Jonathon Luiten<sup>1</sup></a>\n&nbsp;&nbsp;\n<a href=\"https://m.lopezantequera.com/\">Manuel Lopez-Antequera<sup>1</sup></a>\n&nbsp;&nbsp;\n<a href=\"https://scholar.google.com/citations?user=484sccEAAAAJ\">Samuel Rota BulÃ²<sup>1</sup></a>\n&nbsp;&nbsp;\n<a href=\"https://richardt.name/\">Christian Richardt<sup>1</sup></a>\n<br>\n<a href=\"https://www.cs.cmu.edu/~deva/\">Deva Ramanan<sup>2</sup></a>\n&nbsp;&nbsp;\n<a href=\"https://theairlab.org/team/sebastian/\">Sebastian Scherer<sup>2</sup></a>\n&nbsp;&nbsp;\n<a href=\"https://www.linkedin.com/in/peter-kontschieder-2a6410134\">Peter Kontschieder<sup>1</sup></a>\n<br>\n<br>\n<sup>1</sup> Meta &nbsp;&nbsp;\n<sup>2</sup> Carnegie Mellon University\n</strong>\n\n</div>\n\n## Overview\n\nMapAnything is a simple, end-to-end trained transformer model that directly regresses the factored metric 3D geometry of a scene given various types of inputs (images, calibration, poses, or depth). A single feed-forward model supports over 12 different 3D reconstruction tasks including multi-image sfm, multi-view stereo, monocular metric depth estimation, registration, depth completion and more.\n\n![Overview](./assets/teaser.png)\n\n## Table of Contents\n\n- [Quick Start](#quick-start)\n  - [Installation](#installation)\n  - [Image-Only Inference](#image-only-inference)\n  - [Multi-Modal Inference](#multi-modal-inference)\n- [Interactive Demos](#interactive-demos)\n  - [Online Demo](#online-demo)\n  - [Local Gradio Demo](#local-gradio-demo)\n  - [Rerun Demo](#rerun-demo)\n- [COLMAP & GSplat Support](#colmap--gsplat-support)\n  - [Exporting to COLMAP Format](#exporting-to-colmap-format)\n  - [Integration with Gaussian Splatting](#integration-with-gaussian-splatting)\n- [Data Processing for Training & Benchmarking](#data-processing-for-training--benchmarking)\n- [Training](#training)\n- [Benchmarking](#benchmarking)\n- [Code License](#code-license)\n- [Models](#models)\n- [Building Blocks for MapAnything](#building-blocks-for-mapanything)\n- [Acknowledgments](#acknowledgments)\n- [Citation](#citation)\n\n## Quick Start\n\n### Installation\n\n```bash\ngit clone https://github.com/facebookresearch/map-anything.git\ncd map-anything\n\n# Create and activate conda environment\nconda create -n mapanything python=3.12 -y\nconda activate mapanything\n\n# Optional: Install torch, torchvision & torchaudio specific to your system\n# Install MapAnything\npip install -e .\n\n# For all optional dependencies\n# See pyproject.toml for more details\npip install -e \".[all]\"\npre-commit install\n```\n\nNote that we don't pin a specific version of PyTorch or CUDA in our requirements. Please feel free to install PyTorch based on your specific system.\n\n### Image-Only Inference\n\nFor metric 3D reconstruction from images without additional geometric inputs:\n\n```python\n# Optional config for better memory efficiency\nimport os\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n\n# Required imports\nimport torch\nfrom mapanything.models import MapAnything\nfrom mapanything.utils.image import load_images\n\n# Get inference device\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# Init model - This requries internet access or the huggingface hub cache to be pre-downloaded\n# For Apache 2.0 license model, use \"facebook/map-anything-apache\"\nmodel = MapAnything.from_pretrained(\"facebook/map-anything\").to(device)\n\n# Load and preprocess images from a folder or list of paths\nimages = \"path/to/your/images/\"  # or [\"path/to/img1.jpg\", \"path/to/img2.jpg\", ...]\nviews = loa",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-27T02:14:08.661329"
  },
  {
    "basic_info": {
      "name": "Super-Mario-Bros.-Remastered-Public",
      "full_name": "JHDev2006/Super-Mario-Bros.-Remastered-Public",
      "owner": "JHDev2006",
      "description": "A Remake / Celebration of the original 'Super Mario Bros.' games. Features new levels, custom modes, new characters, alongside a full level editor / custom level system!",
      "url": "https://github.com/JHDev2006/Super-Mario-Bros.-Remastered-Public",
      "clone_url": "https://github.com/JHDev2006/Super-Mario-Bros.-Remastered-Public.git",
      "ssh_url": "git@github.com:JHDev2006/Super-Mario-Bros.-Remastered-Public.git",
      "homepage": "",
      "created_at": "2025-09-13T15:29:58Z",
      "updated_at": "2025-09-27T00:49:12Z",
      "pushed_at": "2025-09-26T20:38:06Z"
    },
    "stats": {
      "stars": 1446,
      "forks": 176,
      "watchers": 1446,
      "open_issues": 189,
      "size": 54150
    },
    "tech_info": {
      "language": "GDScript",
      "languages": {
        "GDScript": 960105,
        "GAP": 36590,
        "C#": 10724,
        "GDShader": 2322
      },
      "license": "GNU General Public License v3.0",
      "topics": []
    },
    "content": {
      "readme": "# Super Mario Bros Remastered\nA Remake / Celebration of the original 'Super Mario Bros.' games. Features new levels, custom modes, new characters, alongside a full level editor / custom level system!\n\n<img width=\"3840\" height=\"2160\" alt=\"SMB1R_BANNER_printable\" src=\"https://github.com/user-attachments/assets/ed0e97a8-614a-44e2-b69f-2654fca6196c\" />\n\n### Art by [@krystalphantasm.bsky.social](https://bsky.app/profile/krystalphantasm.bsky.social/post/3lvgmgvjeks2f)\n\n### Download: https://github.com/JHDev2006/Super-Mario-Bros.-Remastered-Public/releases\n\n# Requires an original SMB1 NES ROM to play! None of the original assets are contained in the source code, unless it was originally made by us!\n\n# This does NOT act as a replacement for the original Super Mario Bros. games. Super Mario Bros. & Super Mario Bros.: The Lost Levels, can be played now on Nintendo Switch, through Nintendo Switch Online\n\n## Features\n- Super Mario Bros., Super Mario Bros.: The Lost Levels, Super Mario Bros. Special and All Night Nippon: Super Mario Bros. Fully recreated from the ground up!\n- Improved physics / level design\n- Resource Packs! Fully customize how the game looks and sounds.\n- Custom Characters - Add in your own characters to use in game.\n- Fully Open Source!\n- Level Share Square Partnered\n- Portable mode by creating `portable.txt` in the executable directory\n\n## Downloading\n\n### Windows/Linux\n1. Go to the 'Releases' page\n2. Look for the latest version\n3. Download the .zip for your OS\n4. Extract and run\n5. Enjoy!\n\n### macOS (Unofficial)\n1. Go to the [macOS repo](https://github.com/yuriko-shimizu/Super-Mario-Bros.-Remastered-Public-Mac/releases)\n2. (NOTE: THIS IS AN UNOFFICIAL FORK OF THE GAME)\n3. Look for the latest version\n4. Download the .zip file\n5. Extract, drag into the 'Applictions' folder and run\n6. Enjoy!\n\n## Importing for editing\n1. Download the source\n2. Download Godot 4.5 beta 3\n3. Import the project\n4. Enjoy!\n\n## Contributing\nYou are more than welcome to contribute any fixes / improvements you'd like, simply open a pull request, and I'll review it ASAP!\n\n## System Requirements\n\nPlease refer to the Godot engine requirements for minimum and recommended hardware specifications.\n\n[Minimum Requirements](https://docs.godotengine.org/en/latest/about/system_requirements.html#desktop-or-laptop-pc-minimum)\n\n[Recommended Requirements](https://docs.godotengine.org/en/latest/about/system_requirements.html#id3)\n\n\n## Issues\nWhen opening an issue, please keep it to one report, per post, and try and be as helpful as possible, when telling me what has occured, so that its as easy to fix as possible.\nPlease do not open issues, for feature requests, suggestions, or opinions. BUG REPORTS ONLY\n\n## Known Issues\nThere are a couple known issues, mainly due to being built off of Godot, and these issues existing in the engine itself.\n- Steam deck controls do not work natively, you can circumvent this by setting up controller bindings to emulate keys instead, apologies.\n- Physics are weird, when interacting with corners + the camera barrier\n- Drop shadows jitter when playing with \"Smooth Rendering\"\n- Several entities jitter at times.\n- Blocks + coins, respawn when reloading resource packs\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-27T02:14:09.803495"
  },
  {
    "basic_info": {
      "name": "VoxCPM",
      "full_name": "OpenBMB/VoxCPM",
      "owner": "OpenBMB",
      "description": "VoxCPM: Tokenizer-Free TTS for Context-Aware Speech Generation and True-to-Life Voice Cloning",
      "url": "https://github.com/OpenBMB/VoxCPM",
      "clone_url": "https://github.com/OpenBMB/VoxCPM.git",
      "ssh_url": "git@github.com:OpenBMB/VoxCPM.git",
      "homepage": "",
      "created_at": "2025-09-16T03:41:49Z",
      "updated_at": "2025-09-27T02:04:11Z",
      "pushed_at": "2025-09-23T06:12:14Z"
    },
    "stats": {
      "stars": 1397,
      "forks": 147,
      "watchers": 1397,
      "open_issues": 19,
      "size": 1454
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 112327
      },
      "license": "Apache License 2.0",
      "topics": []
    },
    "content": {
      "readme": "## ğŸ™ï¸ VoxCPM: Tokenizer-Free TTS for Context-Aware Speech Generation and True-to-Life Voice Cloning\n\n\n[![Project Page](https://img.shields.io/badge/Project%20Page-GitHub-blue)](https://github.com/OpenBMB/VoxCPM/) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-OpenBMB-yellow)](https://huggingface.co/openbmb/VoxCPM-0.5B) [![ModelScope](https://img.shields.io/badge/ModelScope-OpenBMB-purple)](https://modelscope.cn/models/OpenBMB/VoxCPM-0.5B)  [![Live Playground](https://img.shields.io/badge/Live%20PlayGround-Demo-orange)](https://huggingface.co/spaces/OpenBMB/VoxCPM-Demo) [![Samples](https://img.shields.io/badge/Page-Samples-red)](https://openbmb.github.io/VoxCPM-demopage)\n\n\n\n<div align=\"center\">\n  <img src=\"assets/voxcpm_logo.png\" alt=\"VoxCPM Logo\" width=\"40%\">\n</div>\n\n<div align=\"center\">\n\nğŸ‘‹ Contact us on [WeChat](assets/wechat.png)\n\n</div>\n\n## News \n* [2025.09.16] ğŸ”¥ ğŸ”¥ ğŸ”¥  We Open Source the VoxCPM-0.5B [weights](https://huggingface.co/openbmb/VoxCPM-0.5B)!\n* [2025.09.16] ğŸ‰ ğŸ‰ ğŸ‰  We Provide the [Gradio PlayGround](https://huggingface.co/spaces/OpenBMB/VoxCPM-Demo) for VoxCPM-0.5B, try it now! \n\n## Overview\n\nVoxCPM is a novel tokenizer-free Text-to-Speech (TTS) system that redefines realism in speech synthesis. By modeling speech in a continuous space, it overcomes the limitations of discrete tokenization and enables two flagship capabilities: context-aware speech generation and true-to-life zero-shot voice cloning.\n\nUnlike mainstream approaches that convert speech to discrete tokens, VoxCPM uses an end-to-end diffusion autoregressive architecture that directly generates continuous speech representations from text. Built on [MiniCPM-4](https://huggingface.co/openbmb/MiniCPM4-0.5B) backbone, it achieves implicit semantic-acoustic decoupling through hierachical language modeling and FSQ constraints, greatly enhancing both expressiveness and generation stability.\n\n<div align=\"center\">\n  <img src=\"assets/voxcpm_model.png\" alt=\"VoxCPM Model Architecture\" width=\"90%\">\n</div>\n\n\n###  ğŸš€ Key Features\n- **Context-Aware, Expressive Speech Generation** - VoxCPM comprehends text to infer and generate appropriate prosody, delivering speech with remarkable expressiveness and natural flow. It spontaneously adapts speaking style based on content, producing highly fitting vocal expression trained on a massive 1.8 million-hour bilingual corpus.\n- **True-to-Life Voice Cloning** - With only a short reference audio clip, VoxCPM performs accurate zero-shot voice cloning, capturing not only the speakerâ€™s timbre but also fine-grained characteristics such as accent, emotional tone, rhythm, and pacing to create a faithful and natural replica.\n- **High-Efficiency Synthesis** - VoxCPM supports streaming synthesis with a Real-Time Factor (RTF) as low as 0.17 on a consumer-grade NVIDIA RTX 4090 GPU, making it possible for real-time applications.\n\n\n\n\n\n##  Quick Start\n\n### ğŸ”§ Install from PyPI\n``` sh\npip install voxcpm\n```\n### 1.  Model Download (Optional)\nBy default, when you first run the script, the model will be downloaded automatically, but you can also download the model in advance.\n- Download VoxCPM-0.5B\n    ```\n    from huggingface_hub import snapshot_download\n    snapshot_download(\"openbmb/VoxCPM-0.5B\")\n    ```\n- Download ZipEnhancer and SenseVoice-Small. We use ZipEnhancer to enhance speech prompts and SenseVoice-Small for speech prompt ASR in the web demo.\n    ```\n    from modelscope import snapshot_download\n    snapshot_download('iic/speech_zipenhancer_ans_multiloss_16k_base')\n    snapshot_download('iic/SenseVoiceSmall')\n    ```\n\n### 2. Basic Usage\n```python\nimport soundfile as sf\nimport numpy as np\nfrom voxcpm import VoxCPM\n\nmodel = VoxCPM.from_pretrained(\"openbmb/VoxCPM-0.5B\")\n\n# Non-streaming\nwav = model.generate(\n    text=\"VoxCPM is an innovative end-to-end TTS model from ModelBest, designed to generate highly expressive speech.\",\n    prompt_wav_path=None,      # optional: path to a prompt speech for voice cloning\n    prompt_text=None,          # optional: reference text\n    cfg_value=2.0,             # LM guidance on LocDiT, higher for better adherence to the prompt, but maybe worse\n    inference_timesteps=10,   # LocDiT inference timesteps, higher for better result, lower for fast speed\n    normalize=True,           # enable external TN tool\n    denoise=True,             # enable external Denoise tool\n    retry_badcase=True,        # enable retrying mode for some bad cases (unstoppable)\n    retry_badcase_max_times=3,  # maximum retrying times\n    retry_badcase_ratio_threshold=6.0, # maximum length restriction for bad case detection (simple but effective), it could be adjusted for slow pace speech\n)\n\nsf.write(\"output.wav\", wav, 16000)\nprint(\"saved: output.wav\")\n\n# Streaming\nchunks = []\nfor chunk in model.generate_streaming(\n    text = \"Streaming text to speech is easy with VoxCPM!\",\n    # supports same args as above\n):\n    chunks.append(chunk)\nwav = np.concatenate(chunks)\n\nsf.write(\"output_streaming.",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-27T02:14:10.945842"
  },
  {
    "basic_info": {
      "name": "cagent",
      "full_name": "docker/cagent",
      "owner": "docker",
      "description": "Agent Builder and Runtime by Docker Engineering",
      "url": "https://github.com/docker/cagent",
      "clone_url": "https://github.com/docker/cagent.git",
      "ssh_url": "git@github.com:docker/cagent.git",
      "homepage": "",
      "created_at": "2025-09-01T12:14:45Z",
      "updated_at": "2025-09-26T21:45:10Z",
      "pushed_at": "2025-09-26T21:45:05Z"
    },
    "stats": {
      "stars": 1204,
      "forks": 115,
      "watchers": 1204,
      "open_issues": 48,
      "size": 12324
    },
    "tech_info": {
      "language": "Go",
      "languages": {
        "Go": 762360,
        "Dockerfile": 2127
      },
      "license": "Apache License 2.0",
      "topics": [
        "agents",
        "ai"
      ]
    },
    "content": {
      "readme": "# ğŸ¤– `cagent` ğŸ¤–\n\n> A powerful, easy to use, customizable multi-agent runtime that orchestrates AI agents with\n> specialized capabilities and tools, and the interactions between agents.\n\n![cagent in action](docs/assets/cagent-run.gif)\n\n## âœ¨ What is `cagent`? âœ¨\n\n`cagent` lets you create and run intelligent AI agents, where each agent has\nspecialized knowledge, tools, and capabilities.\n\nThink of it as allowing you to quickly build, share and run a team of virtual experts that\ncollaborate to solve complex problems for you.\n\nAnd it's dead easy to use!\n\nâš ï¸ Note: `cagent` is in active development, **breaking changes are to be expected** âš ï¸\n\n### Your First Agent\n\nExample [basic_agent.yaml](/examples/basic_agent.yaml):\n\nCreating agents with cagent is very simple. They are described in a short yaml file, like this one:\n\n```yaml\nagents:\n  root:\n    model: openai/gpt-5-mini\n    description: A helpful AI assistant\n    instruction: |\n      You are a knowledgeable assistant that helps users with various tasks.\n      Be helpful, accurate, and concise in your responses.\n```\n\nRun it in a terminal with `cagent run basic_agent.yaml`.\n\nMany more examples can be found [here](/examples/README.md)!\n\n### Improving an agent with MCP tools\n\n`cagent` supports MCP servers, enabling agents to use a wide variety of external tools and services.\n\nIt supports three transport types: `stdio`, `http` and `sse`.\n\nGiving an agent access to tools via MCP is a quick way to greatly improve its capabilities, the quality of its results and its general useful-ness.\n\nGet started quickly with the [Docker MCP Toolkit](https://docs.docker.com/ai/mcp-catalog-and-toolkit/toolkit/) and [catalog](https://docs.docker.com/ai/mcp-catalog-and-toolkit/catalog/)\n\nHere, we're giving the same basic agent from the example above access to a **containerized** `duckduckgo` mcp server and it's tools by using Docker's MCP Gateway:\n\n```yaml\nversion: \"2\"\n\nagents:\n  root:\n    model: openai/gpt-5-mini\n    description: A helpful AI assistant\n    instruction: |\n      You are a knowledgeable assistant that helps users with various tasks.\n      Be helpful, accurate, and concise in your responses.\n    toolsets:\n      - type: mcp\n        ref: docker:duckduckgo # stdio transport\n```\n\nWhen using a containerized server via the Docker MCP gateway, you can configure any required settings/secrets/authentication using the [Docker MCP Toolkit](https://docs.docker.com/ai/mcp-catalog-and-toolkit/toolkit/#example-use-the-github-official-mcp-server) in Docker Desktop.\n\nAside from the containerized MCP severs the Docker MCP Gateway provides, any standard MCP server can be used with cagent!\n\nHere's an example similar to the above but adding `read_file` and `write_file` tools from the `rust-mcp-filesystem` MCP server:\n\n```yaml\nversion: \"2\"\n\nagents:\n  root:\n    model: openai/gpt-5-mini\n    description: A helpful AI assistant\n    instruction: |\n      You are a knowledgeable assistant that helps users with various tasks.\n      Be helpful, accurate, and concise in your responses. Write your search results to disk.\n    toolsets:\n      - type: mcp\n        ref: docker:duckduckgo\n      - type: mcp\n        command: rust-mcp-filesystem # installed with `cargo install rust-mcp-filesystem`\n        args: [\"--allow-write\", \".\"]\n        tools: [\"read_file\", \"write_file\"] # Optional: specific tools only\n        env:\n          - \"RUST_LOG=debug\"\n```\n\nSee [the USAGE docs](./docs/USAGE.md#tool-configuration) for more detailed information and examples\n\n### ğŸ¯ Key Features\n\n- **ğŸ—ï¸ Multi-agent architecture** - Create specialized agents for different domains.\n- **ğŸ”§ Rich tool ecosystem** - Agents can use external tools and APIs via the MCP protocol.\n- **ğŸ”„ Smart delegation** - Agents can automatically route tasks to the most suitable specialist.\n- **ğŸ“ YAML configuration** - Declarative model and agent configuration.\n- **ğŸ’­ Advanced reasoning** - Built-in \"think\", \"todo\" and \"memory\" tools for complex problem-solving.\n- **ğŸŒ Multiple AI providers** - Support for OpenAI, Anthropic, Gemini and [Docker Model Runner](https://docs.docker.com/ai/model-runner/).\n\n## ğŸš€ Quick Start ğŸš€\n\n### Installation\n\n[Prebuilt binaries](https://github.com/docker/cagent/releases) for Windows, macOS and Linux can be found on the releases page of the [project's GitHub repository](https://github.com/docker/cagent/releases)\n\nOnce you've downloaded the appropriate binary for your platform, you may need to give it executable permissions.\nOn macOS and Linux, this is done with the following command:\n\n```sh\n# linux amd64 build example\nchmod +x /path/to/downloads/cagent-linux-amd64\n```\n\nYou can then rename the binary to `cagent` and configure your `PATH` to be able to find it (configuration varies by platform).\n\n### **Set your API keys**\n\nBased on the models you configure your agents to use, you will need to set the corresponding provider API key accordingly,\nall theses keys are optional, you will likely need at least one of these, though:\n\n```bash\n# For OpenAI",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-27T02:14:12.113626"
  },
  {
    "basic_info": {
      "name": "multi-agent-coding-system",
      "full_name": "Danau5tin/multi-agent-coding-system",
      "owner": "Danau5tin",
      "description": "Reached #13 on Stanford's Terminal Bench leaderboard. Orchestrator, explorer & coder agents working together with intelligent context sharing.",
      "url": "https://github.com/Danau5tin/multi-agent-coding-system",
      "clone_url": "https://github.com/Danau5tin/multi-agent-coding-system.git",
      "ssh_url": "git@github.com:Danau5tin/multi-agent-coding-system.git",
      "homepage": "",
      "created_at": "2025-08-30T19:23:45Z",
      "updated_at": "2025-09-26T10:41:45Z",
      "pushed_at": "2025-09-07T16:01:23Z"
    },
    "stats": {
      "stars": 1198,
      "forks": 145,
      "watchers": 1198,
      "open_issues": 2,
      "size": 849
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 139397,
        "Shell": 483
      },
      "license": null,
      "topics": []
    },
    "content": {
      "readme": "# ğŸ¤“ Orchestrator: A multi-agent AI coder. Reached #13 on Stanford's TerminalBench. Open sourced!\n\nTL;DR:\n- Over the weekend, quite unexpectedly, I made a multi-agent AI system that places slightly higher than Claude Code on Stanford's TerminalBench leaderboard (13th place).\n- This AI system consists of an orchestration agent that dispatches multiple explorer and coder agents to do all the work.\n- The orchestrator explicitly defines what knowledge artifacts subagents must return, then reuses and synthesises these artifacts across future tasks - creating compound intelligence where each action builds meaningfully on previous discoveries.\n\n![Orchestrator with claude-sonnet-4 on standford's terminal bench](./readme_imgs/orchestrator-sonnet-4-stanford-terminal-bench-leaderboard.png)\n\n## How the System Works\n\n![System architecture overview](readme_imgs/orch_agent_sys_arch.png)\n\nThe orchestrator acts as the brain of the operation - it receives the user's task but never touches code directly. Instead, it:\n\n1. **Analyses** the task and breaks it into focused subtasks\n2. **Dispatches** explorer agents to understand the system\n3. **Delegates** implementation work to coder agents with precise instructions\n4. **Verifies** all changes through additional explorer agents\n5. **Maintains** the context store with all discovered knowledge\n\nThe orchestrator's lack of direct code access forces proper delegation and verification patterns, leading to more strategic solutions.\n\nFor a full breakdown of this project's code structure, see [here](./PROJECT_STRUCTURE.md)\n\n\n## ğŸ“ˆ Evaluation Results\n\n### Performance on TerminalBench\n\n[Terminal bench](https://www.tbench.ai/) is a brilliant benchmark created by Stanford and [Laude Institute](https://www.laude.org/) to quantify agents' ability to complete complex tasks in the terminal. My Orchestrator system achieved **13th place** on the leaderboard, demonstrating competitive performance against leading AI coding assistants.\n\nI ran the Orchestrator evaluations with both Claude-4-Sonnet and also Qwen3-Coder-480B-A35B:\n\n![Performance comparison chart](readme_imgs/perf_chart.png)\n![Orchestrator with qwen-3-coder on standford's terminal bench](./readme_imgs/orchestrator-qwen-3-coder-stanford-terminal-bench-leaderboard.png)\n\nThis image shows Qwen-3-Coder performance on the benchmark. The screenshot towards the top of this README shows Sonnet-4 performance.\n\n### Cost & Efficiency\n\nOne of the most striking results is the amount of tokens used by Sonnet-4 as opposed to Qwen3-Coder.\n\nThe below table shows the total tokens (input and output included) processed across the TerminalBench evaluation run (5 attempts at 80 tasks = 400 trajectories).\n\n| Model | Success Rate | Total Evaluation Cost | Token Usage |\n|-------|--------------|------------|-------------|\n| **Claude Sonnet-4** | 37.0% | $263.56* | 93.2M tokens |\n| **Qwen-3-Coder** | 19.7% | $217.83 | 14.7M tokens |\n\n*Claude Sonnet-4 costs reflect heavy caching usage, reducing actual API costs\n\n\n## ğŸ¤– The Agents\n\nWhile all agents use the same underlying LLM, each operates with its own context window, specialised system message, and distinct toolset. This creates functionally different agents optimised for their specific roles.\n\n### ğŸ¯ Orchestrator Agent\n[System message](./src/agents/system_msgs/md_files/orchestrator_sys_msg_v0.1.md)\n**Role:** Strategic coordinator and persistent intelligence layer  \n**Capabilities:** Task decomposition, context management, subagent delegation  \n**Tools:** Task creation, subagent launching, context store management  \n**Restrictions:** Cannot read or modify code directly - operates purely at architectural level  \n\nThe orchestrator maintains the complete picture across all tasks, tracking discoveries and progress. It crafts precise task descriptions that explicitly specify what contexts subagents should return, ensuring focused and valuable information gathering.\n\n**Trust Calibration Strategy:**  \nThe orchestrator employs adaptive delegation based on task complexity:\n- **Low Complexity Tasks**: Grants extremely high autonomy to the coder agent for simple modifications and bug fixes\n- **Medium/Large Tasks**: Maintains strong trust but uses iterative decomposition - breaking complex problems into atomic, verifiable steps\n- **Verification Philosophy**: Uses explorer agents liberally to verify progress, especially when tasks involve critical functionality\n\n\n### ğŸ” Explorer Agent \n[System message](./src/agents/system_msgs/md_files/explorer_sys_msg_v0.1.md) \n**Role:** Read-only investigation and verification specialist  \n**Capabilities:** System exploration, code analysis, test execution, verification  \n**Tools:** File reading, search operations (grep/glob), bash commands, temporary script creation  \n**Restrictions:** Cannot modify existing files - strictly read-only operations  \n\nExplorers gather intelligence about the codebase, verify implementations, and discover system behaviors. They create knowledge artifacts that eliminat",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-27T02:14:13.276826"
  },
  {
    "basic_info": {
      "name": "sj.h",
      "full_name": "rxi/sj.h",
      "owner": "rxi",
      "description": "A tiny little JSON parsing library",
      "url": "https://github.com/rxi/sj.h",
      "clone_url": "https://github.com/rxi/sj.h.git",
      "ssh_url": "git@github.com:rxi/sj.h.git",
      "homepage": null,
      "created_at": "2025-09-19T18:20:20Z",
      "updated_at": "2025-09-26T23:35:25Z",
      "pushed_at": "2025-09-21T18:29:44Z"
    },
    "stats": {
      "stars": 1140,
      "forks": 23,
      "watchers": 1140,
      "open_issues": 3,
      "size": 10
    },
    "tech_info": {
      "language": "C",
      "languages": {
        "C": 4081
      },
      "license": "The Unlicense",
      "topics": []
    },
    "content": {
      "readme": "# sj.h\nA tiny little JSON parsing library\n\n- ~150 lines of C99\n- Zero-allocations with minimal state\n- Error messages with `line:column:` location\n- No number parsing: `strtod`, `atoi`? Handle them how you want\n- No string parsing: bring your own unicode surrogate pair handling (or don't)\n\n\n## Usage\nA small program to load a rectangle from a JSON string into a `Rect` struct:\n```c\nchar *json_text = \"{ \\\"x\\\": 10, \\\"y\\\": 20, \\\"w\\\": 30, \\\"h\\\": 40 }\";\n\ntypedef struct { int x, y, w, h; } Rect;\n\nbool eq(sj_Value val, char *s) {\n    size_t len = val.end - val.start;\n    return strlen(s) == len && !memcmp(s, val.start, len);\n}\n\nint main(void) {\n    Rect rect = {0};\n\n    sj_Reader r = sj_reader(json_text, strlen(json_text));\n    sj_Value obj = sj_read(&r);\n\n    sj_Value key, val;\n    while (sj_iter_object(&r, obj, &key, &val)) {\n        if (eq(key, \"x\")) { rect.x = atoi(val.start); }\n        if (eq(key, \"y\")) { rect.y = atoi(val.start); }\n        if (eq(key, \"w\")) { rect.w = atoi(val.start); }\n        if (eq(key, \"h\")) { rect.h = atoi(val.start); }\n    }\n\n    printf(\"rect: { %d, %d, %d, %d }\\n\", rect.x, rect.y, rect.w, rect.h);\n    return 0;\n}\n```\n\nSee the [**demo**](demo/) folder for further usage examples.\n\n\n## License\nThis is free and unencumbered software released into the public domain. See\n[LICENSE](LICENSE) for details.",
      "default_branch": "master"
    },
    "fetched_at": "2025-09-27T02:14:14.425606"
  },
  {
    "basic_info": {
      "name": "LongCat-Flash-Chat",
      "full_name": "meituan-longcat/LongCat-Flash-Chat",
      "owner": "meituan-longcat",
      "description": null,
      "url": "https://github.com/meituan-longcat/LongCat-Flash-Chat",
      "clone_url": "https://github.com/meituan-longcat/LongCat-Flash-Chat.git",
      "ssh_url": "git@github.com:meituan-longcat/LongCat-Flash-Chat.git",
      "homepage": null,
      "created_at": "2025-08-30T16:00:04Z",
      "updated_at": "2025-09-26T18:50:39Z",
      "pushed_at": "2025-09-26T03:15:19Z"
    },
    "stats": {
      "stars": 1130,
      "forks": 52,
      "watchers": 1130,
      "open_issues": 17,
      "size": 20091
    },
    "tech_info": {
      "language": null,
      "languages": {},
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "# LongCat-Flash-Chat\n\n<div align=\"center\">\n  <img src=\"figures/longcat_logo.svg\" width=\"45%\" alt=\"LongCat-Flash\" />\n</div>\n<hr>\n\n<div align=\"center\" style=\"line-height: 1;\">\n  <a href=\"https://longcat.ai/\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Chat\" src=\"https://img.shields.io/badge/ğŸ¤–%20Chat-LongCat--Flash--Chat-ADFF2F?color=29E154&logoColor=white\"  fill-opacity=\"1\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://huggingface.co/meituan-longcat\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Hugging Face\" src=\"https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-LongCat-ffc107?color=ffc107&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n</div>\n\n<div align=\"center\" style=\"line-height: 1;\">\n  <a href=\"https://github.com/meituan-longcat/LongCat-Flash-Chat/blob/main/figures/wechat_official_accounts.png\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Wechat\" src=\"https://img.shields.io/badge/WeChat-LongCat-brightgreen?logo=wechat&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://x.com/Meituan_LongCat\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Twitter Follow\" src=\"https://img.shields.io/badge/Twitter-LongCat-white?logo=x&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n</div>\n\n<div align=\"center\" style=\"line-height: 1;\">\n  <a href=\"LICENSE\" style=\"margin: 2px;\">\n    <img alt=\"License\" src=\"https://img.shields.io/badge/License-MIT-f5de53?&color=f5de53\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n</div>\n\n<p align=\"center\">\n  <a href=\"https://arxiv.org/abs/2509.01322\"><b>Tech Report</b>&nbsp;ğŸ“„</a>\n</p>\n\n## Model Introduction\nWe introduce LongCat-Flash, a powerful and efficient language model with 560 billion total parameters, featuring an innovative Mixture-of-Experts (MoE) architecture. The model incorporates a dynamic computation mechanism that activates 18.6Bâˆ¼31.3B parameters (averagingâˆ¼27B) based on contextual demands, optimizing both computational efficiency and performance. To achieve advanced training and inference efficiency, we employ a shortcut-connected architecture that expands computation-communication overlap window, achieving over 100 tokens per second (TPS) for inference cost-effectively. Our comprehensive training and scaling strategies ensure stable, efficient training, while tailored data strategies enhance model performance.\n\nNow we release LongCat-Flash-Chat, a non-thinking foundation model that delivers highly competitive performance among leading models, with exceptional strengths in agentic tasks.\n\n\n### Key Features\n\n#### ğŸŒŸ Scalable Architectural Design for Computational Efficiency\n\nLongCat-Flash is designed and optimized under two key principles: efficient computation utilization, as well as  efficient training and inference. Specifically, (1) As not all tokens are equal, we introduce the zero-computation experts mechanism in MoE blocks to allocate a dynamic computation budget to important tokens based on their significance, i.e., activating 18.6 to 31.3 billion parameters (out of 560 billion total) based on contextual demands. To ensure consistent computation load, we employ expert bias adjusted by a PID-controller, maintaining an average ofâˆ¼27 billion activated parameters per token. (2) As communication overhead becomes a bottleneck during MoE model scaling, we incorporate the Shortcut-connected MoE (ScMoE) design to expand the computation-communication overlap window. Combined with customized infrastructure optimizations, this design enables training at a massive scale of over tens of thousands accelerators and inference with high throughput and low latency.\n\n\n#### ğŸŒŸ Effective Model Scaling Strategy\n\nEffectively and efficiently scaling model size remains a key challenge in strategy design. To this end, we develop a comprehensive stability-and-scaling framework for robustly training large-scale models: (1) We successfully apply a hyperparameter transfer strategy to such a large model, predicting optimal hyperparameter configurations by leveraging results from smaller proxy models with theoretical guarantees. (2) We initialize the model using a model-growth mechanism based on a refined half-scale checkpoint, achieving improved performance compared to conventional initialization methods. (3) A multi-pronged stability suite incorporates principled router-gradient balancing, a hidden z-loss to suppress massive activations, and fine-tuned optimizer configurations. (4) To enhance the reliability of large-scale cluster training, we introduce deterministic computation. This guarantees the exact reproducibility of experiments and enables the detection of SDC (Silent Data Corruption) during the training process. These interventions ensure that LongCat-Flash â€™s training remains stable, with no irrecoverable loss spikes.\n\n#### ğŸŒŸ Multi-Stage Training Pipeline for Agentic Capability\nThrough a metic",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-27T02:14:15.562176"
  },
  {
    "basic_info": {
      "name": "Nano-Bananary",
      "full_name": "ZHO-ZHO-ZHO/Nano-Bananary",
      "owner": "ZHO-ZHO-ZHO",
      "description": "é¦™è•‰è¶…å¸‚ï½œå„ç§ç©æ³•ä¸€é”®ç”Ÿæˆï¼Œæ— éœ€æç¤ºè¯ï¼Œæ”¯æŒå±€éƒ¨æ¶‚é€‰ã€è¿ç»­ç¼–è¾‘",
      "url": "https://github.com/ZHO-ZHO-ZHO/Nano-Bananary",
      "clone_url": "https://github.com/ZHO-ZHO-ZHO/Nano-Bananary.git",
      "ssh_url": "git@github.com:ZHO-ZHO-ZHO/Nano-Bananary.git",
      "homepage": null,
      "created_at": "2025-09-05T10:18:39Z",
      "updated_at": "2025-09-27T01:14:46Z",
      "pushed_at": "2025-09-16T15:14:05Z"
    },
    "stats": {
      "stars": 1069,
      "forks": 197,
      "watchers": 1069,
      "open_issues": 5,
      "size": 104
    },
    "tech_info": {
      "language": "TypeScript",
      "languages": {
        "TypeScript": 154642,
        "HTML": 2693
      },
      "license": null,
      "topics": []
    },
    "content": {
      "readme": "<div align=\"center\">\n<img width=\"2816\" height=\"1536\" alt=\"Group 333\" src=\"https://github.com/user-attachments/assets/0800046e-71aa-4dee-93a6-731b9f914a35\" />\n\n\n\n# Nano Bananary ï½œ é¦™è•‰è¶…å¸‚ ï½œ ZHO\n\n\n\n<img width=\"1251\" height=\"2051\" alt=\"Group 336\" src=\"https://github.com/user-attachments/assets/6b1fc8a2-e86b-403b-be21-97eb2ff65034\" />\n\n\n\n<img width=\"1556\" height=\"1086\" alt=\"screenshot-20250905-191422\" src=\"https://github.com/user-attachments/assets/23953d15-8ebb-4574-bbc9-74b44b79f341\" />\n\n\n</div>\n\n\n## ğŸ†• æ–°å¢ è§†é¢‘ç”Ÿæˆ + ä¸­æ–‡ç•Œé¢ + æµ…è‰²ä¸»é¢˜ï¼\n\n\n### 1ï¼‰ä¸­æ–‡ç•Œé¢ + æµ…è‰²ä¸»é¢˜ ä¸€é”®åˆ‡æ¢ï¼\n\n<img width=\"1311\" height=\"738\" alt=\"screenshot-20250916-153520\" src=\"https://github.com/user-attachments/assets/476241e6-b7bf-4f66-81c5-fdaaa0695bcd\" />\n\n\n### 2ï¼‰ä¸Šçº¿è§†é¢‘ç”ŸæˆåŠŸèƒ½ï¼æ‰€æœ‰ç©æ³•ä¸€é”®è½¬åŒ–ä¸ºè§†é¢‘ï¼Œçºµäº«ä¸æ»‘ï¼\n\n\nhttps://github.com/user-attachments/assets/4cce75d2-9783-41a4-94cc-2837c365d5a8\n\n\n\n\nhttps://github.com/user-attachments/assets/be93114f-3bc4-4ddf-947c-4694268444e4\n\n\n\n\n## 1ï¼‰æ— éœ€æç¤ºè¯ï¼Œä¸æ»‘è¡”æ¥\n\n\n\nhttps://github.com/user-attachments/assets/39976fe1-fafe-4ecf-94b2-5f6053f92c7f\n\n\n\nâœ…å„ç§ç©æ³•ä¸€é”®ç”Ÿæˆï¼Œæ— éœ€æç¤ºè¯ï¼Œæ”¯æŒå±€éƒ¨æ¶‚é€‰ã€è¿ç»­ç¼–è¾‘ï¼š\n\n1âƒ£é€‰æ‹©ç”¨æ³•\n\n2âƒ£ä¸Šä¼ å›¾ç‰‡\n\n3âƒ£ç‚¹å‡»ç”Ÿæˆ/å±€éƒ¨æ¶‚é€‰\n\n4âƒ£ç›´æ¥å‘é€åˆ°æ–°ç©æ³•ä¸­ç»§ç»­\n\n\n## 2ï¼‰æ ¸å¿ƒåŠŸèƒ½ï¼šæ¯æ¬¡è¾“å‡ºéƒ½èƒ½ç›´æ¥ä½œä¸ºè¾“å…¥è¿›è¡Œä¸‹ä¸€æ¬¡ç¼–è¾‘/ç”Ÿæˆ\n\n\n\nhttps://github.com/user-attachments/assets/ca1cc851-ccca-44c6-b3f4-138b0650c0f9\n\n\n\n<img width=\"1664\" height=\"1248\" alt=\"comparison-image-1757069194058\" src=\"https://github.com/user-attachments/assets/6260a835-8404-4772-a152-303d10ab9551\" />\n\n\n<img width=\"1392\" height=\"1280\" alt=\"screenshot-20250905-220201\" src=\"https://github.com/user-attachments/assets/5cb573dc-78dd-4c5c-8194-9172af94d65d\" />\n\n\n\n<img width=\"2048\" height=\"2507\" alt=\"Group 334\" src=\"https://github.com/user-attachments/assets/a4dd528c-e0a6-4a52-844c-ff0c28d0c99c\" />\n\n\n\n## Online\n\n\nUse in AI Studio: https://ai.studio/apps/drive/1JknFrFFdiOm7FIA8MLOJa_vtJN2g24c1\n\n\n## Run Locally\n\n**Prerequisites:**  Node.js\n\n\n1. Install dependencies:\n   `npm install`\n2. Set the `GEMINI_API_KEY` in [.env.local](.env.local) to your Gemini API key\n3. Run the app:\n   `npm run dev`\n\n\n\n\n## æ›´æ–°æ—¥å¿—\n\n- 20250916\n\n  æ–°å¢ è§†é¢‘ç”Ÿæˆ + ä¸­æ–‡ç•Œé¢ + æµ…è‰²ä¸»é¢˜\n\n\n- 20250906\n\n  åŠŸèƒ½æ›´æ–°ï¼šå¢åŠ å†å²è®°å½•åŠŸèƒ½ï¼Œæ–¹ä¾¿ç›´æ¥ä½¿ç”¨å·²ç”Ÿæˆçš„å›¾åƒä½œä¸ºè¾“å…¥\n\n  ç©æ³•æ›´æ–°ï¼šå·²ç»æŠŠ æˆ‘çš„[ğŸŒæç¤ºè¯åº“](https://github.com/ZHO-ZHO-ZHO/ZHO-nano-banana-Creation)çš„ä¸»è¦ç©æ³•æ›´æ–°ä¸Šå»äº†\n\n\n- 20250905\n \n  æ›´æ–°é…è‰²ï¼šblack & orange\n\n  æ–°å¢åŠŸèƒ½ï¼š\n  \n  1âƒ£ æ”¾å¤§é¢„è§ˆ\n  \n  2âƒ£æ¨ªç½®å¯¹æ¯” + å¯¹æ¯”å›¾ç›´æ¥ä¸‹è½½ï¼ˆæ–¹ä¾¿ç›´æ¥å‘ç¤¾äº¤åª’ä½“ï¼‰\n  \n  3âƒ£æ»‘å—å¯¹æ¯”ï¼šæ–¹ä¾¿åŸå›¾å¯¹ç…§\n  \n  4âƒ£è‡ªå®šä¹‰æç¤ºè¯æ¨¡å—ï¼šæ–¹ä¾¿ä¸ªæ€§åŒ–ç”Ÿæˆ\n\n\n- 20250095\n  \n  åˆ›å»ºé¡¹ç›®\n  \n\n## Stars \n\n[![Star History Chart](https://api.star-history.com/svg?repos=ZHO-ZHO-ZHO/Nano-Bananary&type=Date)](https://star-history.com/#ZHO-ZHO-ZHO/Nano-Bananary&Date)\n\n\n## å…³äºæˆ‘ | About me\n\nğŸ“¬ **è”ç³»æˆ‘**ï¼š\n- é‚®ç®±ï¼šzhozho3965@gmail.com\n  \n\nğŸ”— **ç¤¾äº¤åª’ä½“**ï¼š\n- ä¸ªäººé¡µï¼š[-Zho-](https://jike.city/zho)\n- Bilibiliï¼š[æˆ‘çš„Bç«™ä¸»é¡µ](https://space.bilibili.com/484366804)\n- Xï¼ˆTwitterï¼‰ï¼š[æˆ‘çš„Twitter](https://twitter.com/ZHO_ZHO_ZHO)\n- å°çº¢ä¹¦ï¼š[æˆ‘çš„å°çº¢ä¹¦ä¸»é¡µ](https://www.xiaohongshu.com/user/profile/63f11530000000001001e0c8?xhsshare=CopyLink&appuid=63f11530000000001001e0c8&apptime=1690528872)\n\nğŸ’¡ **æ”¯æŒæˆ‘**ï¼š\n- Bç«™ï¼š[Bç«™å……ç”µ](https://space.bilibili.com/484366804)\n- çˆ±å‘ç”µï¼š[ä¸ºæˆ‘å……ç”µ](https://afdian.com/a/ZHOZHO)\n\n\n## Credits\n\n[Gemini 2.5 Flash Image](https://gemini.google.com/app)\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-27T02:14:16.708544"
  },
  {
    "basic_info": {
      "name": "hallbayes",
      "full_name": "leochlon/hallbayes",
      "owner": "leochlon",
      "description": null,
      "url": "https://github.com/leochlon/hallbayes",
      "clone_url": "https://github.com/leochlon/hallbayes.git",
      "ssh_url": "git@github.com:leochlon/hallbayes.git",
      "homepage": null,
      "created_at": "2025-09-01T12:51:01Z",
      "updated_at": "2025-09-27T02:06:55Z",
      "pushed_at": "2025-09-27T00:07:37Z"
    },
    "stats": {
      "stars": 1067,
      "forks": 100,
      "watchers": 1067,
      "open_issues": 9,
      "size": 193012
    },
    "tech_info": {
      "language": "HTML",
      "languages": {
        "HTML": 268076,
        "TeX": 97584,
        "Python": 97395,
        "JavaScript": 3738,
        "Shell": 1350,
        "Batchfile": 935
      },
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "# Hallucination Risk Calculator & Prompt Re-engineering Toolkit (OpenAI-only)\n\n**Post-hoc calibration without retraining** for large language models. This toolkit turns a raw prompt into:  \n1) a **bounded hallucination risk** using the Expectation-level Decompression Law (EDFL), and  \n2) a **decision** to **ANSWER** or **REFUSE** under a target SLA, with transparent math (nats).\n\nIt supports two deployment modes:\n\n- **Evidence-based:** prompts include *evidence/context*; rolling priors are built by erasing that evidence.\n- **Closed-book:** prompts have *no evidence*; rolling priors are built by semantic masking of entities/numbers/titles.\n\nAll scoring relies **only** on the OpenAI Chat Completions API. No retraining required.\n\n---\n\n## Table of Contents\n- [Install & Setup](#install--setup)\n- [Core Mathematical Framework](#core-mathematical-framework)\n- [Understanding System Behavior](#understanding-system-behavior)\n- [Two Ways to Build Rolling Priors](#two-ways-to-build-rolling-priors)\n- [API Surface](#api-surface)\n- [Calibration & Validation](#calibration--validation)\n- [Practical Considerations](#practical-considerations)\n- [Project Layout](#project-layout)\n- [Deployment Options](#deployment-options)\n\n---\n\n## Install & Setup\n\n```bash\npip install --upgrade openai\nexport OPENAI_API_KEY=sk-...\n```\n\n> The module uses `openai>=1.0.0` and the Chat Completions API (e.g., `gpt-4o`, `gpt-4o-mini`).\n\n---\n\n## Core Mathematical Framework\n\n### The EDFL Principle\n\nLet the binary event $\\mathcal{A}$ be the thing you want to guarantee (e.g., **Answer** in decision mode, or **Correct** for factual accuracy).  \nBuild an ensemble of **content-weakened prompts** (the *rolling priors*) $\\{S_k\\}_{k=1}^m$. For the realized label $y$, estimate:\n\n- **Information budget:**  \n  $$\\bar{\\Delta} = \\tfrac{1}{m}\\sum_k \\mathrm{clip}_+(\\log P(y) - \\log S_k(y), B)$$\n  (one-sided clipping; default $B=12$ nats to prevent outliers while maintaining conservative bounds).\n\n- **Prior masses:** $q_k = S_k(\\mathcal{A})$, with:\n  - $\\bar{q}=\\tfrac{1}{m}\\sum_k q_k$ (average prior for EDFL bound)\n  - $q_{\\text{lo}}=\\min_k q_k$ (worst-case prior for SLA gating)\n\nBy EDFL, the achievable reliability is bounded by:  \n$$\\bar{\\Delta} \\ge \\mathrm{KL}(\\mathrm{Ber}(p) \\| \\mathrm{Ber}(\\bar{q})) \\Rightarrow p\\le p_{\\max}(\\bar{\\Delta},\\bar{q})$$\n\nThus the **hallucination risk** (error) is bounded by $\\overline{\\mathrm{RoH}} \\le 1 - p_{\\max}$.\n\n### Decision Rule (SLA Gating)\n\nFor target hallucination rate $h^*$:\n- **Bits-to-Trust:** $\\mathrm{B2T} = \\mathrm{KL}(\\mathrm{Ber}(1-h^*) \\| \\mathrm{Ber}(q_{\\text{lo}}))$\n- **Information Sufficiency Ratio:** $\\mathrm{ISR} = \\bar{\\Delta}/\\mathrm{B2T}$\n- **ANSWER** iff $\\mathrm{ISR}\\ge 1$ *and* $\\bar{\\Delta} \\ge \\mathrm{B2T} + \\text{margin}$ (default `marginâ‰ˆ0.2` nats)\n\n> **Why two priors?** The gate uses **worst-case** $q_{\\text{lo}}$ for strict SLA compliance. The RoH bound uses **average** $\\bar{q}$ per EDFL theory. This dual approach ensures conservative safety while providing realistic risk bounds.\n\n---\n\n## Understanding System Behavior\n\n### Expected Behavioral Patterns\n\nThe toolkit exhibits different behaviors across query types, which is **mathematically consistent** with the framework:\n\n#### Simple Arithmetic Queries\n**Observation:** May abstain despite apparent simplicity  \n**Explanation:**\n- Models often attempt answers even with masked numbers (pattern recognition)\n- This yields **low information lift** $\\bar{\\Delta} \\approx 0$ between full prompt and skeletons\n- Despite potentially low EDFL risk bound, worst-case prior gate triggers **abstention** (ISR < 1)\n\n#### Named-Entity Factoids\n**Observation:** Generally answered with confidence  \n**Explanation:**\n- Masking entities/dates substantially reduces answer propensity in skeletons\n- Restoring these yields **large** $\\bar{\\Delta}$ that clears B2T threshold\n- System **answers** with tight EDFL risk bound\n\n**This is not a bug but a feature**: The framework prioritizes safety through worst-case guarantees while providing realistic average-case bounds.\n\n### Mitigation Strategies\n\n1. **Switch Event Measurement**\n   - Use **Correct/Incorrect** instead of Answer/Refuse for factual QA\n   - Skeletons without key information rarely yield correct results â†’ large $\\bar{\\Delta}$\n\n2. **Enhance Skeleton Weakening**\n   - Implement mask-aware decision head that refuses on redaction tokens\n   - Ensures skeletons have strictly lower \"Answer\" mass than full prompt\n\n3. **Calibration Adjustments**\n   - Relax $h^*$ slightly (e.g., 0.10 instead of 0.05) for higher answer rates\n   - Reduce margin for less conservative gating\n   - Increase sampling ($n=7-10$) for stability\n\n4. **Provide Evidence**\n   - Adding compact, relevant evidence increases $\\bar{\\Delta}$ while preserving bounds\n\n---\n\n## Two Ways to Build Rolling Priors\n\n### 1) Evidence-based (when you have context)\n\n- **Prompt** contains a field like `Evidence:` (or JSON keys)\n- **Skeletons** erase the evidence content but preserve st",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-27T02:14:17.837252"
  },
  {
    "basic_info": {
      "name": "SRPO",
      "full_name": "Tencent-Hunyuan/SRPO",
      "owner": "Tencent-Hunyuan",
      "description": "Directly Aligning the Full Diffusion Trajectory with Fine-Grained Human Preference",
      "url": "https://github.com/Tencent-Hunyuan/SRPO",
      "clone_url": "https://github.com/Tencent-Hunyuan/SRPO.git",
      "ssh_url": "git@github.com:Tencent-Hunyuan/SRPO.git",
      "homepage": "https://tencent.github.io/srpo-project-page/",
      "created_at": "2025-09-09T07:36:49Z",
      "updated_at": "2025-09-26T23:04:17Z",
      "pushed_at": "2025-09-16T06:21:00Z"
    },
    "stats": {
      "stars": 1014,
      "forks": 32,
      "watchers": 1014,
      "open_issues": 13,
      "size": 12251
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 712645,
        "Shell": 4058
      },
      "license": "Other",
      "topics": []
    },
    "content": {
      "readme": "<div align=â€œcenterâ€ style=â€œfont-family: charter;â€>\n<h1 align=\"center\">Directly Aligning the Full Diffusion Trajectory with Fine-Grained Human Preference </h1>\n<div align=\"center\">\n  <a href='https://arxiv.org/abs/2509.06942'><img src='https://img.shields.io/badge/ArXiv-red?logo=arxiv'></a>  &nbsp;\n  <a href='https://huggingface.co/tencent/SRPO/'><img src='https://img.shields.io/badge/Model-blue?logo=huggingface'></a> &nbsp; \n  <a href='https://tencent.github.io/srpo-project-page/'><img src='https://img.shields.io/badge/%F0%9F%92%BB_Project-SRPO-blue'></a> &nbsp;\n</div>\n<div align=\"center\">\n  Xiangwei Shen<sup>1,2,3*</sup>,\n  <a href=\"https://scholar.google.com/citations?user=Lnr1FQEAAAAJ&hl=zh-CN\" target=\"_blank\"><b>Zhimin Li</b></a><sup>1*</sup>,\n  <a href=\"https://scholar.google.com.hk/citations?user=Fz3X5FwAAAAJ\" target=\"_blank\"><b>Zhantao Yang</b></a><sup>1</sup>, \n  <a href=\"https://shiyi-zh0408.github.io/\" target=\"_blank\"><b>Shiyi Zhang</b></a><sup>3</sup>,\n  Yingfang Zhang<sup>1</sup>,\n  Donghao Li<sup>1</sup>,\n  <br>\n  <a href=\"https://scholar.google.com/citations?user=VXQV5xwAAAAJ&hl=en\" target=\"_blank\"><b>Chunyu Wang</b></a><sup>1âœ</sup>,\n  <a href=\"https://openreview.net/profile?id=%7EQinglin_Lu2\" target=\"_blank\"><b>Qinglin Lu</b></a><sup>1</sup>,\n  <a href=\"https://andytang15.github.io\" target=\"_blank\"><b>Yansong Tang</b></a><sup>3,âœ‰ï¸</sup>\n</div>\n<div align=\"center\">\n  <sup>1</sup>Hunyuan, Tencentâ€ƒ\n  <br>\n  <sup>2</sup>School of Science and Engineering, The Chinese University of Hong Kong, Shenzhenâ€ƒ\n  <br>\n  <sup>3</sup>Shenzhen International Graduate School, Tsinghua Universityâ€ƒ\n  <br>\n  <sup>*</sup>Equal contributionâ€ƒ\n  <sup>âœ</sup>Project leadâ€ƒ\n  <sup>âœ‰ï¸</sup>Corresponding author\n</div>\n\n![head](assets/head.jpg)\n\n## ğŸ‰ Key Features\n1. **Direct Align**: We introduce a new sampling strategy for diffusion fine-tuning that can effectively restore highly noisy images, leading to an optimization process that is more stable and less computationally demanding, especially during the initial timesteps.\n2. **Faster Training**:   By rolling out only a single image and optimizing directly with analytical gradientsâ€”a key distinction from GRPOâ€”our method achieves significant performance improvements for FLUX.1.dev in under 10 minutes of training. To further accelerate the process, our method supports replacing online rollouts entirely with a small dataset of real images; we find that fewer than 1500 images are sufficient to effectively train FLUX.1.dev.\n3. **Free of Reward Hacking**: We have improved the training strategy for method that direct backpropagation on reward signal (such as ReFL and DRaFT). Moreover, we directly regularize the model using negative rewards, without the need for KL divergence or a separate reward system. In our experiments, this approach achieves comparable performance with multiple different rewards, improving the perceptual quality of FLUX.1.dev without suffering from reward hacking issues, such as overfitting to color or oversaturation preferences.\n4. **Potential for Controllable Fine-tuning**: For the first time in online RL, we incorporate dynamically controllable text conditions, enabling on-the-fly adjustment of reward preference towards styles within the scope of the reward model.\n\n## ğŸ”¥ News\n\n- __[2025.9.12]__:  ğŸ‰ We released the complete training code. We also share tips and experiences to help you train your models. Youâ€™re welcome to discuss and ask questions in the issues! ğŸ’¬âœ¨\n- __[2025.9.12]__:  ğŸ‰ We provide a standard workflowâ€”feel free to use it in ComfyUI.\n- __[2025.9.8]__:   ğŸ‰ We released the paper, checkpoint, inference code.\n\n## ğŸ“‘ Open-source Plan\n- [X] The training code is under internal review and will be open-sourced by this weekend at the latest.\n- [ ] Release a quantized version for the FLUX community.\n- [ ] Extend support to other models.\n\n## ğŸ› ï¸ Dependencies and Installation\n\n```bash\nconda create -n SRPO python=3.10.16 -y\nconda activate SRPO\nbash ./env_setup.sh \n```\nğŸ’¡ The environment dependency is basically the same as DanceGRPO\n\n## ğŸ¤— Download Models\n\n1. Model Cards\n\n|       Model       |                           Huggingface Download URL                                      |  \n|:-----------------:|:---------------------------------------------------------------------------------------:|\n|       SRPO        |           [diffusion_pytorch_model](https://huggingface.co/tencent/SRPO/tree/main)      |\n\n2. Download our `diffusion_pytorch_model.safetensors` in [https://huggingface.co/tencent/SRPO]\n```bash\nmkdir ./srpo\nhuggingface-cli login\nhuggingface-cli download --resume-download Tencent/SRPO diffusion_pytorch_model.safetensors --local-dir ./srpo/\n```\n3. Load your FLUX cache or use the `black-forest-labs/FLUX.1-dev`[https://huggingface.co/black-forest-labs/FLUX.1-dev]\n```bash\nmkdir ./data/flux\nhuggingface-cli login\nhuggingface-cli download --resume-download  black-forest-labs/FLUX.1-dev --local-dir ./data/flux\n```\n\n## ğŸ”‘ Inference\n\n### Using ComfyUI\n\nYou can use",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-27T02:14:18.982499"
  }
]