[
  {
    "basic_info": {
      "name": "RustGPT",
      "full_name": "tekaratzas/RustGPT",
      "owner": "tekaratzas",
      "description": "An transformer based LLM. Written completely in Rust",
      "url": "https://github.com/tekaratzas/RustGPT",
      "clone_url": "https://github.com/tekaratzas/RustGPT.git",
      "ssh_url": "git@github.com:tekaratzas/RustGPT.git",
      "homepage": null,
      "created_at": "2025-09-13T22:05:55Z",
      "updated_at": "2025-10-03T01:33:39Z",
      "pushed_at": "2025-09-29T14:58:12Z"
    },
    "stats": {
      "stars": 2739,
      "forks": 221,
      "watchers": 2739,
      "open_issues": 6,
      "size": 174
    },
    "tech_info": {
      "language": "Rust",
      "languages": {
        "Rust": 65199
      },
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "# ü¶Ä Rust LLM from Scratch\n\n[![Rust](https://github.com/tekaratzas/RustGPT/actions/workflows/rust.yml/badge.svg)](https://github.com/tekaratzas/RustGPT/actions/workflows/rust.yml)\n\nhttps://github.com/user-attachments/assets/ec4a4100-b03a-4b3c-a7d6-806ea54ed4ed\n\nA complete **Large Language Model implementation in pure Rust** with no external ML frameworks. Built from the ground up using only `ndarray` for matrix operations.\n\n## üöÄ What This Is\n\nThis project demonstrates how to build a transformer-based language model from scratch in Rust, including:\n- **Pre-training** on factual text completion\n- **Instruction tuning** for conversational AI\n- **Interactive chat mode** for testing\n- **Full backpropagation** with gradient clipping\n- **Modular architecture** with clean separation of concerns\n\n## ‚ùå What This Isn't\n\nThis is not a production grade LLM. It is so far away from the larger models.\n\nThis is just a toy project that demonstrates how these models work under the hood.\n\n## üîç Key Files to Explore\n\nStart with these two core files to understand the implementation:\n\n- **[`src/main.rs`](src/main.rs)** - Training pipeline, data preparation, and interactive mode\n- **[`src/llm.rs`](src/llm.rs)** - Core LLM implementation with forward/backward passes and training logic\n\n## üèóÔ∏è Architecture\n\nThe model uses a **transformer-based architecture** with the following components:\n\n```\nInput Text ‚Üí Tokenization ‚Üí Embeddings ‚Üí Transformer Blocks ‚Üí Output Projection ‚Üí Predictions\n```\n\n### Project Structure\n\n```\nsrc/\n‚îú‚îÄ‚îÄ main.rs              # üéØ Training pipeline and interactive mode\n‚îú‚îÄ‚îÄ llm.rs               # üß† Core LLM implementation and training logic\n‚îú‚îÄ‚îÄ lib.rs               # üìö Library exports and constants\n‚îú‚îÄ‚îÄ transformer.rs       # üîÑ Transformer block (attention + feed-forward)\n‚îú‚îÄ‚îÄ self_attention.rs    # üëÄ Multi-head self-attention mechanism\n‚îú‚îÄ‚îÄ feed_forward.rs      # ‚ö° Position-wise feed-forward networks\n‚îú‚îÄ‚îÄ embeddings.rs        # üìä Token embedding layer\n‚îú‚îÄ‚îÄ output_projection.rs # üé∞ Final linear layer for vocabulary predictions\n‚îú‚îÄ‚îÄ vocab.rs            # üìù Vocabulary management and tokenization\n‚îú‚îÄ‚îÄ layer_norm.rs       # üßÆ Layer normalization\n‚îî‚îÄ‚îÄ adam.rs             # üèÉ Adam optimizer implementation\n\ntests/\n‚îú‚îÄ‚îÄ llm_test.rs         # Tests for core LLM functionality\n‚îú‚îÄ‚îÄ transformer_test.rs # Tests for transformer blocks\n‚îú‚îÄ‚îÄ self_attention_test.rs # Tests for attention mechanisms\n‚îú‚îÄ‚îÄ feed_forward_test.rs # Tests for feed-forward layers\n‚îú‚îÄ‚îÄ embeddings_test.rs  # Tests for embedding layers\n‚îú‚îÄ‚îÄ vocab_test.rs       # Tests for vocabulary handling\n‚îú‚îÄ‚îÄ adam_test.rs        # Tests for optimizer\n‚îî‚îÄ‚îÄ output_projection_test.rs # Tests for output layer\n```\n\n## üß™ What The Model Learns\n\nThe implementation includes two training phases:\n\n1. **Pre-training**: Learns basic world knowledge from factual statements\n   - \"The sun rises in the east and sets in the west\"\n   - \"Water flows downhill due to gravity\"\n   - \"Mountains are tall and rocky formations\"\n\n2. **Instruction Tuning**: Learns conversational patterns\n   - \"User: How do mountains form? Assistant: Mountains are formed through tectonic forces...\"\n   - Handles greetings, explanations, and follow-up questions\n\n## üöÄ Quick Start\n\n```bash\n# Clone and run\ngit clone https://github.com/tekaratzas/RustGPT.git\ncd RustGPT\ncargo run\n\n# The model will:\n# 1. Build vocabulary from training data\n# 2. Pre-train on factual statements (100 epochs)\n# 3. Instruction-tune on conversational data (100 epochs)\n# 4. Enter interactive mode for testing\n```\n\n## üéÆ Interactive Mode\n\nAfter training, test the model interactively:\n\n```\nEnter prompt: How do mountains form?\nModel output: Mountains are formed through tectonic forces or volcanism over long geological time periods\n\nEnter prompt: What causes rain?\nModel output: Rain is caused by water vapor in clouds condensing into droplets that become too heavy to remain airborne\n```\n\n## üßÆ Technical Implementation\n\n### Model Configuration\n- **Vocabulary Size**: Dynamic (built from training data)\n- **Embedding Dimension**: 128 (defined by `EMBEDDING_DIM` in `src/lib.rs`)\n- **Hidden Dimension**: 256 (defined by `HIDDEN_DIM` in `src/lib.rs`)\n- **Max Sequence Length**: 80 tokens (defined by `MAX_SEQ_LEN` in `src/lib.rs`)\n- **Architecture**: 3 Transformer blocks + embeddings + output projection\n\n### Training Details\n- **Optimizer**: Adam with gradient clipping\n- **Pre-training LR**: 0.0005 (100 epochs)\n- **Instruction Tuning LR**: 0.0001 (100 epochs)\n- **Loss Function**: Cross-entropy loss\n- **Gradient Clipping**: L2 norm capped at 5.0\n\n### Key Features\n- **Custom tokenization** with punctuation handling\n- **Greedy decoding** for text generation\n- **Gradient clipping** for training stability\n- **Modular layer system** with clean interfaces\n- **Comprehensive test coverage** for all components\n\n## üîß Development\n\n```bash\n# Run all tests\ncargo test\n\n# Test specific components\ncargo test --test llm_test\ncargo test --test transformer_test\ncargo test --test self_attention_test\n\n# Buil",
      "default_branch": "main"
    },
    "fetched_at": "2025-10-03T02:16:30.953177"
  },
  {
    "basic_info": {
      "name": "desktop-tui",
      "full_name": "Julien-cpsn/desktop-tui",
      "owner": "Julien-cpsn",
      "description": "A desktop environment without graphics",
      "url": "https://github.com/Julien-cpsn/desktop-tui",
      "clone_url": "https://github.com/Julien-cpsn/desktop-tui.git",
      "ssh_url": "git@github.com:Julien-cpsn/desktop-tui.git",
      "homepage": null,
      "created_at": "2025-09-06T00:42:53Z",
      "updated_at": "2025-10-02T13:38:54Z",
      "pushed_at": "2025-09-21T20:51:37Z"
    },
    "stats": {
      "stars": 964,
      "forks": 18,
      "watchers": 964,
      "open_issues": 5,
      "size": 6353
    },
    "tech_info": {
      "language": "Rust",
      "languages": {
        "Rust": 46719,
        "Nix": 637
      },
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "Desktop-TUI üñ•Ô∏è\n===\n\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n![GitHub Release](https://img.shields.io/github/v/release/julien-cpsn/desktop-tui?link=https%3A%2F%2Fgithub.com%2FJulien-cpsn%2Fdesktop-tuiC%2Freleases%2Flatest)\n[![Crates.io](https://repology.org/badge/version-for-repo/crates_io/desktop-tui.svg)](https://crates.io/crates/desktop-tui)\n\nA desktop environment without graphics (tmux-like).\n\nFeatures:\n- [x] Parse shortcut files containing apps\n  - [x] Custom additional commands\n  - [x] Custom window options\n  - [x] Custom terminal options\n- [x] Display any application or command that uses stdout\n  - [x] Move and resize windows\n  - [x] Handle and display application error\n- [x] Change tilling options\n- [x] Can let the user select a file or a folder to use its path as a command argument\n- [x] Clock\n\n![demo](./demo.gif)\n\n## How to use\n\n### Install\n\n```shell\ncargo install desktop-tui\n```\n\n### Compile\n\n```shell\ncargo build\n```\n\n```shell\ncargo build --release\n```\n\n### Run\n\nYou can replace `cargo run --` with `desktop-tui`\n\n```shell\ncargo run -- <shortcut_folder_path>\n```\n\nOr in release :\n\n```shell\ncargo run --release -- <shortcut_folder_path>\n```\n\n## Shortcut file\n\nExample `helix.toml` shortcut file:\n\n```toml\n# Window name\nname = \"Text editor\"\n\n# Command to execute\ncommand = \"hx\"\n# Each command argument\nargs = []\n\n[taskbar]\n# Shortcut position on the action bar\n# Optional\nposition = 3\n\n# Optional\n[[taskbar.additional_commands]]\n# Command name\nname = \"Open folder\"\n# Command to execute\ncommand = \"hx\"\n# <FILE_PATH> or <FOLDER_PATH> will be replaced by a path selected in a dialog\nargs = [\"<FOLDER_PATH>\"]\n\n[[taskbar.additional_commands]]\nname = \"Open file\"\ncommand = \"hx\"\nargs = [\"<FILE_PATH>\"]\n\n[window]\nresizable = true\nclose_button = true\nfixed_position = false\n# Optional\nsize = { width = 10, height = 5 }\n\n[terminal]\n# Pad inner window\npadding = [0, 0]\n# Optional\nbackground_color = { r = 30, g = 30, b = 30 }\n```\n\n## Star history\n\n<a href=\"https://www.star-history.com/#julien-cpsn/desktop-tui&Date\">\n <picture>\n   <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://api.star-history.com/svg?repos=julien-cpsn/desktop-tui&type=Date&theme=dark\" />\n   <source media=\"(prefers-color-scheme: light)\" srcset=\"https://api.star-history.com/svg?repos=julien-cpsn/desktop-tui&type=Date\" />\n   <img alt=\"Star History Chart\" src=\"https://api.star-history.com/svg?repos=julien-cpsn/desktop-tui&type=Date\" />\n </picture>\n</a>\n\n## License\n\nThe MIT license for this project can be seen [here](./LICENSE)\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-10-03T02:16:32.303937"
  },
  {
    "basic_info": {
      "name": "pingoo",
      "full_name": "pingooio/pingoo",
      "owner": "pingooio",
      "description": "The fast and secure Load Balancer / API Gateway / Reverse Proxy with built-in service discovery, GeoIP, WAF, bot protection and much more - https://pingoo.io",
      "url": "https://github.com/pingooio/pingoo",
      "clone_url": "https://github.com/pingooio/pingoo.git",
      "ssh_url": "git@github.com:pingooio/pingoo.git",
      "homepage": "https://pingoo.io",
      "created_at": "2025-09-17T07:18:40Z",
      "updated_at": "2025-10-02T19:04:27Z",
      "pushed_at": "2025-10-02T08:25:38Z"
    },
    "stats": {
      "stars": 723,
      "forks": 25,
      "watchers": 723,
      "open_issues": 9,
      "size": 417
    },
    "tech_info": {
      "language": "Rust",
      "languages": {
        "Rust": 202231,
        "TypeScript": 7020,
        "Dockerfile": 6728,
        "Makefile": 2265,
        "Shell": 1620,
        "HTML": 892,
        "CSS": 620,
        "Vim Script": 19
      },
      "license": "MIT License",
      "topics": [
        "akamai",
        "anti-bot",
        "apache2",
        "api",
        "api-gateway",
        "captcha",
        "cloudflare",
        "fastly",
        "firewall",
        "haproxy",
        "load-balancer",
        "nginx",
        "pingoo",
        "proxy",
        "quic",
        "reverse-proxy",
        "rust",
        "security",
        "service-discovery",
        "waf"
      ]
    },
    "content": {
      "readme": "<p align=\"center\">\n  <a href=\"https://pingoo.io\" target=\"_blank\" rel=\"noopener\"><img alt=\"Pingoo logo\" src=\"https://pingoo.io/icon-256.png\" height=\"128\" /></a>\n  <h1 align=\"center\">Pingoo</h1>\n  <h3 align=\"center\">The fast and secure Load Balancer / API Gateway / Reverse Proxy with built-in service discovery, GeoIP, WAF, bot protection and much more</h3>\n  <h3 align=\"center\">\n    <a href=\"https://pingoo.io\">Documentation</a> | <a href=\"https://kerkour.com/announcing-pingoo\">Read the launch post</a>\n  </h3>\n</p>\n\nOpen Source load balancers and reverse proxies are stuck in the past century with a very slow pace of development and most of the important features reserved for \"Enterprise Editions\" which lead developers to use third-party cloud services, exposing their users' traffic to legal, security and reliability risks.\n\nPingoo is a modern Load Balancer / API Gateway / Reverse Proxy that run on your own servers and already have (or will have soon) all the features you expect from managed services and even more. All of that with a huge boost in performance and security thanks to reduced latency and, of course, Rust ;)\n\n* Service Discovery (Docker, DNS...)\n* Web Application Firewall (WAF)\n* Easy compliance because the data never leaves your servers\n* Bot protection and management\n* TCP proxying\n* Post-Quantum TLS\n* GeoIP (country, ASN)\n* Static sites\n* And much more\n\n> ‚ö†Ô∏è Pingoo is currently in beta, use with caution.\n\n## Quickstart\n\n```bash\n# You have a static site in the www folder\n$ ls www\nindex.html\n$ docker run --rm -ti --network host -v `pwd`/www:/var/wwww ghcr.io/pingooio/pingoo\n# Pingoo is now listenning on http://0.0.0.0:8080\n```\n\n## Documentation\n\nSee https://pingoo.io\n\n\n## Updates\n\n[Click Here](https://kerkour.com/blog) to visit the blog and [subscribe](https://kerkour.com/subscribe) by RSS or email to get weekly / monthly updates. No spam ever, only technical deep dives.\n\n\n## Contributing\n\nPlease open an issue to discuss your idea before submitting a Pull Request.\n\n\n## Support\n\nDo you have custom needs? Do you want your features to be prioritized? Are you under attack and need help? Do you need support for deploying and self-hosting Pingoo?\n\nFeel free to reach our team of experts to see how we can help: https://pingoo.io/contact\n\n\n## Security\n\nWe are committed to make Pingoo the most secure Load Balancer / Reverse Proxy in the universe and beyond. If you've found a security issue in Pingoo, we appreciate your help in disclosing it to us in a responsible manner by contacting us: https://pingoo.io/contact\n\n\n## License\n\nMIT. See `LICENSE.txt`\n\nForever Open Source. No Open Core or \"Enterprise Edition\".\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-10-03T02:16:33.692952"
  },
  {
    "basic_info": {
      "name": "hotpath",
      "full_name": "pawurb/hotpath",
      "owner": "pawurb",
      "description": "A simple Rust profiler that shows exactly where your code spends time and allocates",
      "url": "https://github.com/pawurb/hotpath",
      "clone_url": "https://github.com/pawurb/hotpath.git",
      "ssh_url": "git@github.com:pawurb/hotpath.git",
      "homepage": "",
      "created_at": "2025-09-05T20:59:12Z",
      "updated_at": "2025-10-03T02:09:12Z",
      "pushed_at": "2025-10-02T22:20:08Z"
    },
    "stats": {
      "stars": 612,
      "forks": 9,
      "watchers": 612,
      "open_issues": 0,
      "size": 956
    },
    "tech_info": {
      "language": "Rust",
      "languages": {
        "Rust": 173688
      },
      "license": "MIT License",
      "topics": [
        "allocations",
        "benchmark",
        "performance",
        "rust"
      ]
    },
    "content": {
      "readme": "# hotpath - find and profile bottlenecks in Rust\n[![Latest Version](https://img.shields.io/crates/v/hotpath.svg)](https://crates.io/crates/hotpath) [![GH Actions](https://github.com/pawurb/hotpath/actions/workflows/ci.yml/badge.svg)](https://github.com/pawurb/hotpath/actions)\n\n[![Profiling report for mevlog-rs](hotpath-report3.png)](https://github.com/pawurb/mevlog-rs)\n\nA lightweight, easy-to-configure Rust profiler that shows exactly where your code spends time and allocates memory. Instrument any function or code block to quickly spot bottlenecks, and focus your optimizations where they matter most.\n\n## Features\n\n- **Zero-cost when disabled** ‚Äî fully gated by a feature flag.\n- **Low-overhead** profiling for both sync and async code.\n- **Memory allocation tracking** ‚Äî track bytes allocated or allocation counts per function.\n- **Detailed stats**: avg, total time, call count, % of total runtime, and configurable percentiles (p95, p99, etc.).\n- **Background processing** for minimal profiling impact.\n\n## Quick Start\n\n> **‚ö†Ô∏è Note**  \n> This README reflects the latest development on the `main` branch.\n> For documentation matching the current release, see [crates.io](https://crates.io/crates/hotpath) ‚Äî it stays in sync with the published crate.\n\nAdd to your `Cargo.toml`:\n\n```toml\n[dependencies]\nhotpath = { version = \"0.2\", optional = true }\n\n[features]\nhotpath = [\"dep:hotpath\", \"hotpath/hotpath\"]\nhotpath-alloc-bytes-total = [\"hotpath/hotpath-alloc-bytes-total\"]\nhotpath-alloc-bytes-max = [\"hotpath/hotpath-alloc-bytes-max\"]\nhotpath-alloc-count-total= [\"hotpath/hotpath-alloc-count-total\"]\nhotpath-alloc-count-max= [\"hotpath/hotpath-alloc-count-max\"]\nhotpath-off = [\"hotpath/hotpath-off\"]\n```\n\nThis config ensures that the lib has **zero** overhead unless explicitly enabled via a `hotpath` feature.\n\nProfiling features are mutually exclusive. To ensure compatibility with `--all-features` setting, the crate defines an additional `hotpath-off` flag. This is handled automatically - you should never need to enable it manually.\n\n## Usage\n\n```rust\nuse std::time::Duration;\n\n#[cfg_attr(feature = \"hotpath\", hotpath::measure)]\nfn sync_function(sleep: u64) {\n    std::thread::sleep(Duration::from_nanos(sleep));\n}\n\n#[cfg_attr(feature = \"hotpath\", hotpath::measure)]\nasync fn async_function(sleep: u64) {\n    tokio::time::sleep(Duration::from_nanos(sleep)).await;\n}\n\n// When using with tokio, place the #[tokio::main] first\n#[tokio::main]\n// You can configure any percentile between 0 and 100\n#[cfg_attr(feature = \"hotpath\", hotpath::main(percentiles = [99]))]\nasync fn main() {\n    for i in 0..100 {\n        // Measured functions will automatically send metrics\n        sync_function(i);\n        async_function(i * 2).await;\n\n        // Measure code blocks with static labels\n        #[cfg(feature = \"hotpath\")]\n        hotpath::measure_block!(\"custom_block\", {\n            std::thread::sleep(Duration::from_nanos(i * 3))\n        });\n    }\n}\n```\n\nRun your program with a `hotpath` feature:\n\n```\ncargo run --features=hotpath\n```\n\nOutput:\n\n```\n[hotpath] Performance summary from basic::main (Total time: 122.13ms):\n+-----------------------+-------+---------+---------+----------+---------+\n| Function              | Calls | Avg     | P99     | Total    | % Total |\n+-----------------------+-------+---------+---------+----------+---------+\n| basic::async_function | 100   | 1.16ms  | 1.20ms  | 116.03ms | 95.01%  |\n+-----------------------+-------+---------+---------+----------+---------+\n| custom_block          | 100   | 17.09¬µs | 39.55¬µs | 1.71ms   | 1.40%   |\n+-----------------------+-------+---------+---------+----------+---------+\n| basic::sync_function  | 100   | 16.99¬µs | 35.42¬µs | 1.70ms   | 1.39%   |\n+-----------------------+-------+---------+---------+----------+---------+\n```\n\n## Allocation Tracking\n\nIn addition to time-based profiling, `hotpath` can track memory allocations. This feature uses a custom global allocator from [allocation-counter crate](https://github.com/fornwall/allocation-counter) to intercept all memory allocations and provides detailed statistics about memory usage per function.\n\nAvailable alloc profiling modes:\n\n- `hotpath-alloc-bytes-total` - Tracks total bytes allocated during each function call\n- `hotpath-alloc-bytes-max` - Tracks peak memory usage during each function call\n- `hotpath-alloc-count-total` - Tracks total number of allocations per function call\n- `hotpath-alloc-count-max` - Tracks peak number of live allocations per function call\n\nRun your program with a selected flag to print a similar report:\n\n```\ncargo run --features='hotpath,hotpath-alloc-bytes-max'\n```\n\n![Alloc report](alloc-report.png)\n\n### Profiling memory allocations for async functions\n\nTo profile memory usage of `async` functions you have to use a similar config:\n\n```rust\n#[cfg(any(\n    feature = \"hotpath-alloc-bytes-total\",\n    feature = \"hotpath-alloc-bytes-max\",\n    feature = \"hotpath-alloc-count-total\",\n    feature = \"hotpath-alloc-count-max\",\n))]\n#",
      "default_branch": "main"
    },
    "fetched_at": "2025-10-03T02:16:34.973176"
  },
  {
    "basic_info": {
      "name": "cachey",
      "full_name": "s2-streamstore/cachey",
      "owner": "s2-streamstore",
      "description": "Read-through cache for object storage",
      "url": "https://github.com/s2-streamstore/cachey",
      "clone_url": "https://github.com/s2-streamstore/cachey.git",
      "ssh_url": "git@github.com:s2-streamstore/cachey.git",
      "homepage": "http://cachey.dev",
      "created_at": "2025-09-05T15:05:06Z",
      "updated_at": "2025-10-03T00:33:04Z",
      "pushed_at": "2025-09-23T15:05:35Z"
    },
    "stats": {
      "stars": 455,
      "forks": 11,
      "watchers": 455,
      "open_issues": 2,
      "size": 284
    },
    "tech_info": {
      "language": "Rust",
      "languages": {
        "Rust": 162035,
        "Dockerfile": 1094,
        "Just": 746
      },
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "# Cachey\n\nHigh-performance read-through cache for object storage.\n\n- Simple HTTP API\n- Hybrid memory + disk cache powered by [foyer](https://github.com/foyer-rs/foyer)\n- Designed for caching immutable blobs\n- Works with any S3-compatible backend, but has its own `/fetch` API requiring a precise `Range`\n- Fixed page size (16 MiB) ‚Äì maps requested byte range to page-aligned lookups\n- Coalesces concurrent requests for the same page\n- Makes hedged requests to manage tail latency of object storage\n- Can attempt redundant buckets for a given object\n\n[Motivating context](https://www.reddit.com/r/databasedevelopment/comments/1nh1goo/cachey_a_readthrough_cache_for_s3)\n\n## API\n\n### Fetching data\n\n#### Request\n\n```\nHEAD|GET /fetch/{kind}/{object}\n```\n- `kind` + `object` form the cache key\n- `kind` identifies the bucket set (up to 64 chars)\n- `object` is the S3 object key\n\n| Header | Required | Description |\n|--------|----------|-------------|\n| `Range` | yes | Byte range in format `bytes={first}-{last}` |\n| `C0-Bucket` | no | Bucket(s) containing the object |\n| `C0-Config` | no | Override S3 request config |\n\n`C0-Bucket` behavior:\n- Multiple headers indicate bucket preference order\n- If omitted, `kind` is used as the singular bucket name\n- Client preference may be overridden based on internal latency/error stats\n- At most 2 buckets attempted per page miss\n\n`C0-Config` overrides:\nSpace-separated key-value pairs to override S3 request configuration per page miss.\n- `ct=<ms>` Connect timeout (in case an existing connection could not be reused)\n- `rt=<ms>` Read timeout (time-to-first-byte)\n- `ot=<ms>` Operation timeout (across retries)\n- `oat=<ms>` Operation attempt timeout\n- `ma=<num>` Maximum attempts\n- `ib=<ms>` Initial backoff duration\n- `mb=<ms>` Maximum backoff duration\n\n#### Example Request\n\n```http\nGET /fetch/prod-videos/movie-2024.mp4 HTTP/1.1\nRange: bytes=1048576-18874367\nC0-Bucket: us-west-videos\nC0-Bucket: us-east-videos-backup\nC0-Config: ct=1000 oat=1500 ma=5 ib=10 mb=100\n```\n\n#### Response\n\nThe service maps requests to 16 MiB page-aligned ranges and the response has standard HTTP semantics (`206 Partial Content`, `404 Not Found` etc.)\n\n| Header | Description |\n|--------|-------------|\n| `Content-Range` | Actual byte range served |\n| `Content-Length` | Number of bytes in response |\n| `Last-Modified` | Timestamp from first page |\n| `Content-Type` | Always `application/octet-stream` |\n| `C0-Status` | Status for first page |\n\n`C0-Status` format: `{first}-{last}; {bucket}; {cached_at}`\n- Byte range and which bucket was used\n- `cached_at` is Unix timestamp with 0 implying a cache miss\n- Only first page status is sent as a header; status for subsequent pages follows the body as trailers\n\n#### Example Response\n\n```http\nHTTP/1.1 206 Partial Content\nContent-Range: bytes 1048576-18874367/52428800\nContent-Length: 17825792\nContent-Type: application/octet-stream\nC0-Status: 1048576-16777215; us-west-videos; 1704067200\n\n<data>\n\nC0-Status: 16777216-18874367; us-west-videos; 0\n```\n\n### Monitoring\n\n`GET /stats` returns throughput stats as JSON for load balancing and health checking.\n\n`GET /metrics` returns a more comprehensive set of metrics in Prometheus text format.\n\n## Command line\n\n[Docker images](https://github.com/s2-streamstore/cachey/pkgs/container/cachey) are available.\n\n```\nUsage: server [OPTIONS]\n\nOptions:\n      --memory <MEMORY>\n          Maximum memory to use for cache (e.g., \"512MiB\", \"2GB\", \"1.5GiB\") [default: 4GiB]\n      --disk-path <DISK_PATH>\n          Path to disk cache storage, which may be a directory or block device\n      --disk-kind <DISK_KIND>\n          Kind of disk cache, which may be a file system or block device [default: fs] [possible values: block, fs]\n      --disk-capacity <DISK_CAPACITY>\n          Maximum disk cache capacity (e.g., \"100GiB\") If not specified, up to 80% of the available space will be used\n      --hedge-quantile <HEDGE_QUANTILE>\n          Latency quantile for making hedged requests (0.0-1.0, use 0 to disable hedging) [default: 0.99]\n      --tls-self\n          Use a self-signed certificate for TLS\n      --tls-cert <TLS_CERT>\n          Path to the TLS certificate file (e.g., cert.pem) Must be used together with --tls-key\n      --tls-key <TLS_KEY>\n          Path to the private key file (e.g., key.pem) Must be used together with --tls-cert\n      --port <PORT>\n          Port to listen on [default: 443 if HTTPS configured, otherwise 80 for HTTP]\n  -h, --help\n          Print help\n  -V, --version\n          Print version\n```\n\n## Development\n\n- [justfile](./justfile) contains commands for [just](https://just.systems/man/en/) doing things\n- [AGENTS.md](./AGENTS.md) and symlinks for your favorite coding buddies\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-10-03T02:16:36.271986"
  },
  {
    "basic_info": {
      "name": "bake",
      "full_name": "losfair/bake",
      "owner": "losfair",
      "description": "Bake microVMs into standalone executables",
      "url": "https://github.com/losfair/bake",
      "clone_url": "https://github.com/losfair/bake.git",
      "ssh_url": "git@github.com:losfair/bake.git",
      "homepage": "https://bottlefire.dev",
      "created_at": "2025-09-06T12:23:48Z",
      "updated_at": "2025-09-24T06:28:34Z",
      "pushed_at": "2025-09-14T10:31:03Z"
    },
    "stats": {
      "stars": 407,
      "forks": 9,
      "watchers": 407,
      "open_issues": 0,
      "size": 89
    },
    "tech_info": {
      "language": "Rust",
      "languages": {
        "Rust": 100731,
        "Dockerfile": 5470,
        "Shell": 1306
      },
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "# bake\n\n`bake` is a Linux CLI tool that can embed microVM resources (firecracker binary, kernel, initrd, boot disk) into itself. It also implements bidirectional communication between VM and host - including networking and directory sharing - entirely in userspace, without requiring root privilege.\n\n## Usage\n\nThe Docker image includes pre-packaged `bake`, firecracker, kernel and initrd binaries for amd64 and arm64 platforms.\n\n```bash\n# make sure `./rootfs.squashfs.img` exists\n# create output directory\n$ mkdir -p output\n\n# assuming you are building on an amd64 host for an amd64 target\n$ docker run -it --rm \\\n  -v ./rootfs.squashfs.img:/rootfs.img:ro \\\n  -v ./output:/output \\\n  --entrypoint /opt/bake/bake.amd64 \\\n  ghcr.io/losfair/bake \\\n  --input /opt/bake/bake.amd64 \\\n  --firecracker /opt/bake/firecracker.amd64 \\\n  --kernel /opt/bake/kernel.amd64 \\\n  --initrd /opt/bake/initrd.amd64.img \\\n  --rootfs /rootfs.img \\\n  --output /output/app.elf\n\n# start microVM and print uname\n$ ./output/app.elf -- uname -a\nLinux container 6.1.149-bottlefire #1 SMP Sat Sep  6 13:50:25 UTC 2025 x86_64 GNU/Linux\n\n# show usage\n$ ./output/app.elf --help\nBottlefire microVM Image\n\nUsage: app.elf [OPTIONS] [SUBCOMMAND]\n\nOptions:\n      --cpus <CPUS>              Number of CPU cores\n      --memory <MEMORY>          Amount of memory (in MB) allocated to the microVM [default: 256]\n      --boot-args <BOOT_ARGS>    Kernel command line [default: \"console=ttyS0 reboot=k panic=-1\"]\n      --entrypoint <ENTRYPOINT>  Container entrypoint\n      --                         Separator; everything after goes to the container\n      --env <KEY=VALUE>          Container environment variables\n      --verbose                  Enable verbose output\n      --cwd <CWD>                Container working directory [default: ]\n  -p, --publish <HOST:VM>        Publish host:vm port forward (e.g. -p 8080:8080)\n  -v, --volume <HOST:VM[:ro]>    Directory/volume mappings (e.g. -v ./data:/data)\n  -h, --help                     Print help\n\nSubcommands:\n  ssh        Auto-connect to the running microVM via SSH\n             Options: -p, --pid <PID>\n             Pass-through: arguments after `--` go to ssh(1)\n  systemd    Print a systemd service unit and exit\n```\n\n## How it works\n\nDepending on whether embedded data is detected and whether running as PID 1, `bake` runs in one of the following modes:\n\n- If PID is 1 and env var `BAKE_NOT_INIT` is not `1`: vminit mode. `bake` assumes that it is running as the init task inside the Firecracker VM, and perform the init sequence.\n- If PID is not 1, and embedded data is detected: run mode - accept Firecracker startup parameters (e.g. number of CPUs, memory size, network config), extract kernel and initrd into memfd, start firecracker.\n- If PID is not 1, and embedded data is not detected: build mode - accept `--input`, `--firecracker`, `--kernel`, `--initrd`, `--rootfs`, build a binary from `/proc/self/exe` (or the provided input elf) with everything embedded.\n\n### Init sequence (src/vminit.rs)\n\nWhen running as PID 1 inside the microVM, `bake` executes an init routine that prepares the root filesystem, host-guest connectivity, optional volume mounts, and finally launches the container process with `runc`.\n\n- Bootstrap system mounts and loopback\n  - Mount `proc`, `sysfs`, `devtmpfs`, and unified `cgroup2`.\n  - Bring `lo` up.\n\n- Parse kernel cmdline and banner\n  - Read `/proc/cmdline`, parse `bake.*` parameters and `quiet`.\n  - If not quiet, print a banner and `/proc/version` for diagnostics.\n  - Fetch BootManifest from host vsock port 13 containing container runtime parameters.\n\n- Expose embedded rootfs via device-mapper\n  - Read `bake.rootfs_offset` and `bake.rootfs_size` (sectors) from cmdline.\n  - Create a linear mapping `rootfs` with `dmsetup` over `/dev/vda` at the given offset/size.\n\n- Build overlay root on top of ephemeral disk\n  - Format `/dev/vdb` as ext4 and mount at `/ephemeral`.\n  - Prepare overlay dirs: `/ephemeral/rootfs.overlay/{upper,work}` and `/ephemeral/container-tmp` (mode 1777).\n  - Mount the base rootfs from `/dev/mapper/rootfs` at `/rootfs.base`.\n  - Mount an overlay at `/rootfs` with `lowerdir=/rootfs.base`, `upperdir=/ephemeral/rootfs.overlay/upper`, `workdir=/ephemeral/rootfs.overlay/work`.\n\n- Set up host-guest networking over vsock with SOCKS5 and tun2socks\n  - Inside the VM, start a SOCKS5 server listening on vsock port 10.\n  - Start a small TCP proxy that exposes that vsock service on `127.0.0.10:10` for local clients.\n  - Create a TUN device `hostnet` (L3), assign `198.18.0.1/32`, bring it up, and add a default route via `hostnet`.\n  - Start a UDP bridge that exchanges UDP packets with the host over vsock port 11 (length-prefixed rkyv-encoded frames).\n  - Add nftables and `ip rule` entries to policy-route UDP (fwmark `0x64`) via table 100 (via interface `hostudp` created by the UDP injector).\n  - Launch `tun2socks` to route TCP over the local SOCKS5 proxy (`socks5://127.0.0.10:10`), keeping the VM‚Äôs loopback a",
      "default_branch": "main"
    },
    "fetched_at": "2025-10-03T02:16:37.563264"
  },
  {
    "basic_info": {
      "name": "MFTool",
      "full_name": "Kudaes/MFTool",
      "owner": "Kudaes",
      "description": "Direct access to NTFS volumes",
      "url": "https://github.com/Kudaes/MFTool",
      "clone_url": "https://github.com/Kudaes/MFTool.git",
      "ssh_url": "git@github.com:Kudaes/MFTool.git",
      "homepage": "",
      "created_at": "2025-09-09T08:33:44Z",
      "updated_at": "2025-10-02T16:56:02Z",
      "pushed_at": "2025-09-09T08:59:00Z"
    },
    "stats": {
      "stars": 274,
      "forks": 24,
      "watchers": 274,
      "open_issues": 0,
      "size": 47
    },
    "tech_info": {
      "language": "Rust",
      "languages": {
        "Rust": 164285
      },
      "license": "Apache License 2.0",
      "topics": []
    },
    "content": {
      "readme": "# Description\n\nMFTool is a red team-oriented NTFS parser. Instead of asking Windows for files, it parses the on-disk structures of a mounted NTFS volume directly to build an in-memory copy of the [Master File Table](https://learn.microsoft.com/en-us/windows/win32/fileio/master-file-table). That in-memory MFT is kept encrypted and is then used to:\n\n- Search the entire disk for files and metadata.\n- Retrieve file contents **without opening an OS-level file handle**, enabling access to data that is typically locked by the operating system (e.g., `SAM`, `NTUSER.dat`, `SYSTEM`, `pagefile.sys`, etc.) as well as deleted files (hereafter referred to as \"hidden\").\n\nDirect NTFS parsing is not new and is widely used in forensics, although this tool has been developed taking into account the needs and requirements from a red team perspective. Also, I wasn't able to find a public tool that performs in the way I pictured it, so I decided to create my own NTFS parser.\n\n# Content\n\n- [How it works](#How-it-works)\n- [How to use it](#How-to-use-it)\n- [Commands](#Commands)\n- [Examples](#Examples)\n  - [Retrieving metadata of an entry](#Retrieving-metadata-of-an-entry)\n  - [Accessing deleted and locked files](#Accessing-deleted-and-locked-files)\n  - [Directory listing and regex-based search](#Directory-listing-and-regex-based-search)\n- [Limitations and Known Issues](#Limitations-and-Known-Issues)\n- [Links](#Links)\n\n# How it works\n\nMFTool interacts directly with a mounted NTFS volume by opening a handle to it and parsing the on-disk filesystem structures. Instead of relying on Windows APIs, it walks through the Master File Table to build an internal representation of the filesystem.\n\n1. **Boot sector parsing**  \n   Once a handle to the volume is opened, MFTool parses the boot sector to locate the offset of the first MFT entry. From there, it follows the cluster chains to enumerate the rest of the entries.\n\n2. **MFT entry reconstruction**  \n   Each MFT record is reconstructed by replacing the Update Sequence Number (USN) with the corresponding values from the Update Sequence Array (USA). The reconstructed entries are stored in an encrypted in-memory cache to prevent accidental data leakage. This cache is rebuilt every time a new target volume is selected.\n\n3. **File content retrieval**  \n   To read a file, MFTool does not rely on an OS-level file handle. Instead, it parses the file's MFT entry, extracts the unnamed `$DATA` attribute, and follows its data run list to locate the clusters containing the file's content.  \n   - Data is read directly from disk offsets, ignoring Windows' file access controls (note that administrative privileges are still required to run the tool, so this should not be considered an ACL bypass per se).  \n   - If the file is compressed, the content is split into logical units and decompressed using [`RtlDecompressBuffer`](https://learn.microsoft.com/en-us/windows-hardware/drivers/ddi/ntifs/nf-ntifs-rtldecompressbuffer).  \n   - This allows retrieval of normal, locked, and even deleted files in case the content is still present in the disk.   \n\n4. **Searching and directory listing**  \n   File search and directory enumeration rely on parsing the `$I30` index attributes (`INDEX_ROOT`, `INDEX_ALLOCATION` structures). This allows for efficient lookups with logarithmic complexity `O(log n)`, and supports both exact name matching and regex-based searches (regex-based searches are not logarithmic tho).\n\n5. **Reparse point handling**  \n   The parser currently resolves reparse points of type **symlink** and **mount point**, ensuring correct navigation across linked or mounted paths.  \n\n# How to use it\n\nTo build the tool just compile it in `release` mode:\n\n\tC:\\Path\\To\\MFTool> cargo build --release\n\nOnce executed, the tool will wait for commands out of the list commented in the next section.\n\n# Commands\n\n## set_target\nSets the target volume to be parsed.  \nThis command expects a string pointing to a mounted NTFS volume, either by drive letter or by volume GUID path (e.g., `\\\\.\\C:` or `\\\\?\\Volume{04171d6a-0000-0000-0000-100000000000}`).  \nOnce a valid volume path is provided, MFTool rebuilds its in-memory cache of the MFT. From this point, all further interactions with the volume are performed against that cache.\n\n## rebuild\nRebuilds the in-memory MFT cache for the current target volume.  \n\n## ls\nParses the `$I30` index attributes to list the files contained in a directory.  \nBoth the Win32 name and the DOS (short) name (if any) of each file are displayed.\n\n## show\nGiven a directory path and a filename, retrieves the metadata stored in the file's MFT entry.\n\n## show_by_id\nSame as `show`, but instead of requiring a path and filename, it expects the MFT entry index.\n\n## show_by_regex\nSearches for files across the entire volume using a regular expression (expressed as `/regex/`).  \nThis command performs a sequential search of all MFT entries, so its complexity is linear.  \nIf invoked with the `hidden` flag, it restricts th",
      "default_branch": "main"
    },
    "fetched_at": "2025-10-03T02:16:38.827500"
  },
  {
    "basic_info": {
      "name": "gpu-kill",
      "full_name": "kagehq/gpu-kill",
      "owner": "kagehq",
      "description": "Manage your GPUs across NVIDIA, AMD, Intel, and Apple Silicon systems.",
      "url": "https://github.com/kagehq/gpu-kill",
      "clone_url": "https://github.com/kagehq/gpu-kill.git",
      "ssh_url": "git@github.com:kagehq/gpu-kill.git",
      "homepage": "https://gpukill.com",
      "created_at": "2025-09-18T17:21:38Z",
      "updated_at": "2025-10-03T01:24:39Z",
      "pushed_at": "2025-09-28T02:18:45Z"
    },
    "stats": {
      "stars": 240,
      "forks": 7,
      "watchers": 240,
      "open_issues": 1,
      "size": 485
    },
    "tech_info": {
      "language": "Rust",
      "languages": {
        "Rust": 391186,
        "Vue": 112328,
        "Shell": 21084,
        "JavaScript": 5562,
        "CSS": 3006,
        "PowerShell": 2451,
        "TypeScript": 1417
      },
      "license": "Other",
      "topics": []
    },
    "content": {
      "readme": "# GPU Kill\n\nA CLI tool for managing GPUs across NVIDIA, AMD, Intel, and Apple Silicon systems. Monitor, control, and secure your GPU infrastructure with ease.\n\n## Community & Support\n\nJoin our Discord community for discussions, support, and updates:\n\n[![Discord](https://img.shields.io/badge/Discord-Join%20our%20community-7289DA?style=for-the-badge&logo=discord&logoColor=white)](https://discord.gg/KqdBcqRk5E)\n\n\n## Features\n\n- **Monitor GPUs**: Real-time usage, memory, temperature, and processes\n- **Kill Processes**: Gracefully terminate stuck GPU processes\n- **Security**: Detect crypto miners and suspicious activity\n- **Guard Mode**: Policy enforcement to prevent resource abuse\n- **Dashboard**: Web interface for cluster monitoring\n- **Remote**: Manage GPUs across multiple servers\n- **Multi-Vendor**: Works with NVIDIA, AMD, Intel, and Apple Silicon\n- **AI Integration**: MCP server for AI assistant integration\n\n## Requirements\n\n### Build Performance\n\n**For faster development builds:**\n```bash\n# Fast release build (recommended for development)\ncargo build --profile release-fast\n\n# Standard release build (optimized for production)\ncargo build --release\n\n# Maximum optimization (slowest, best performance)\ncargo build --profile release-max\n```\n\n**Build times on typical hardware:**\n- Debug build: ~3 seconds\n- Release-fast: ~28 seconds  \n- Release: ~28 seconds (improved from 76 seconds)\n- Release-max: ~60+ seconds (maximum optimization)\n\n### System Dependencies\n\n**Linux (Ubuntu/Debian):**\n```bash\nsudo apt install build-essential libssl-dev pkg-config\n```\n\n**Linux (Fedora/RHEL/CentOS):**\n```bash\nsudo dnf install gcc gcc-c++ pkg-config openssl-devel\n# or for older systems:\n# sudo yum install gcc gcc-c++ pkg-config openssl-devel\n```\n\n**macOS:**\n```bash\n# Install Xcode command line tools\nxcode-select --install\n# OpenSSL is included with macOS\n```\n\n**Windows:**\n- Install Visual Studio Build Tools\n- OpenSSL is handled automatically by vcpkg\n\n### GPU Drivers\n\n- **NVIDIA**: NVIDIA drivers installed\n- **AMD**: ROCm drivers installed  \n- **Intel**: intel-gpu-tools package installed\n- **Apple Silicon**: macOS with Apple Silicon (M1/M2/M3/M4)\n\n### Build Requirements\n\n- **OS**: Linux, macOS, or Windows\n- **Rust**: 1.70+ (for building from source)\n\n## Quick Start\n\n### Install & Run\n```bash\n# Build from source (first build may take 2-3 minutes)\ngit clone https://github.com/kagehq/gpu-kill.git\ncd gpu-kill\ncargo build --release\n\n# Or install via Cargo\ncargo install gpukill\n\n# Or one-liner installers (recommended)\n# macOS/Linux\ncurl -fsSL https://raw.githubusercontent.com/kagehq/gpu-kill/refs/heads/main/scripts/install.sh | sh\n# Windows (PowerShell)\nirm https://raw.githubusercontent.com/kagehq/gpu-kill/refs/heads/main/scripts/install.ps1 | iex\n\n# List your GPUs\ngpukill --list\n\n# Watch GPU usage in real-time\ngpukill --list --watch\n```\n\n### Dead-simple cheatsheet\n```bash\n# Live watch (alias)\ngpukill watch            # = gpukill --list --watch\n\n# Kill job by PID (positional alias)\ngpukill 12345            # = gpukill --kill --pid 12345\n\n# Free a specific GPU index (kill all jobs on GPU 0)\ngpukill --kill --gpu 0   # add --batch to actually kill; preview without it\n\n# Force reset a GPU (shorthand)\ngpukill --reset 0        # = gpukill --reset --gpu 0\n\n# Safe mode: dry-run first (no changes)\ngpukill 12345 --safe     # alias: --dry-run\n\n# Bring up the backend API (and optionally open dashboard)\ngpukill up               # = gpukill --server --server-port 8080\ngpukill up --open        # tries to open http://localhost:3000\n```\n\n## Dashboard\n\nStart the web interface for cluster monitoring:\n\n```bash\n# 1. Start the backend API server\ngpukill --server --server-port 8080\n\n# 2. Start the dashboard UI (in a new terminal)\ncd dashboard\nnpm install  # First time only\nnpm run dev\n\n# 3. Access the dashboard\nopen http://localhost:3000\n```\n\n**Note**: You need both the backend server (port 8080) and frontend UI (port 3000) running for the dashboard to work.\n\n![GPU Kill Dashboard](dashboard/public/screenshot.png)\n\nThe dashboard provides:\n- **Real-time monitoring** of all GPUs\n- **Security detection** with threat analysis\n- **Policy management** for resource control\n- **Cluster overview** with Magic Moment insights\n\n## MCP Server\n\nGPU Kill includes a MCP server that enables AI assistants to interact with GPU management functionality:\n\n- **Resources**: Read GPU status, processes, audit data, policies, and security scans\n- **Tools**: Kill processes, reset GPUs, scan for threats, create policies\n\n```bash\n# Start the MCP server\ncargo run --release -p gpukill-mcp\n\n# Server runs on http://localhost:3001/mcp\n```\n\n## Usage\n\nAsk your AI to use the tools.\n\n```text\nWhat GPUs do I have and what's their current usage?\n```\n\n```text\nKill the Python process that's stuck on GPU 0\n```\n\n```text\nKill all training processes that are using too much GPU memory\n```\n\n```text\nShow me GPU usage and kill any stuck processes\n```\n\n```text\nScan for crypto miners and suspicious activity\n```\n\n`",
      "default_branch": "main"
    },
    "fetched_at": "2025-10-03T02:16:40.148881"
  },
  {
    "basic_info": {
      "name": "mdserve",
      "full_name": "jfernandez/mdserve",
      "owner": "jfernandez",
      "description": "Fast markdown preview server with live reload and theme support.",
      "url": "https://github.com/jfernandez/mdserve",
      "clone_url": "https://github.com/jfernandez/mdserve.git",
      "ssh_url": "git@github.com:jfernandez/mdserve.git",
      "homepage": "",
      "created_at": "2025-09-22T04:15:44Z",
      "updated_at": "2025-10-03T01:42:28Z",
      "pushed_at": "2025-10-03T00:18:07Z"
    },
    "stats": {
      "stars": 198,
      "forks": 15,
      "watchers": 198,
      "open_issues": 6,
      "size": 1368
    },
    "tech_info": {
      "language": "Rust",
      "languages": {
        "Rust": 29010,
        "HTML": 18579,
        "Shell": 7179,
        "Nix": 1005
      },
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "# mdserve\n\nFast markdown preview server with **live reload** and **theme support**.\n\nJust run `mdserve file.md` and start writing. One statically-compiled executable that runs anywhere - no installation, no dependencies.\n\n![Terminal output when starting mdserve](mdserve-terminal-output.png)\n\n## Features\n\n- ‚ö° **Instant Live Reload** - Real-time updates via WebSocket when markdown file changes\n- üé® **Multiple Themes** - Built-in theme selector with 5 themes including Catppuccin variants\n- üìù **GitHub Flavored Markdown** - Full GFM support including tables, strikethrough, code blocks, and task lists\n- üìä **Mermaid Diagrams** - Automatic rendering of flowcharts, sequence diagrams, class diagrams, and more\n- üöÄ **Fast** - Built with Rust and Axum for excellent performance and low memory usage\n\n## Installation\n\n### macOS (Homebrew)\n\n```bash\n# Add the tap\nbrew tap jfernandez/mdserve\n\n# Install mdserve\nbrew install mdserve\n```\n\n### Linux\n\n```bash\ncurl -sSfL https://raw.githubusercontent.com/jfernandez/mdserve/main/install.sh | bash\n```\n\nThis will automatically detect your platform and install the latest binary to your system.\n\n### Alternative Methods\n\n#### Using Cargo\n\n‚ö†Ô∏è Name conflict: This project is not the `mdserve` crate on crates.io.\nDo not run cargo install mdserve. I'm coordinating with that crate's owner on naming.\n\n#### Arch Linux (AUR)\n\n```bash\nyay -S mdserve\n```\n\n#### Nix Package Manager\n\n``` bash\nnix profile install github:jfernandez/mdserve\n```\n\n#### From Source\n\n```bash\ngit clone https://github.com/jfernandez/mdserve.git\ncd mdserve\ncargo build --release\ncp target/release/mdserve <folder in your PATH>\n```\n\n#### Manual Download\n\nDownload the appropriate binary for your platform from the [latest release](https://github.com/jfernandez/mdserve/releases/latest).\n\n## Usage\n\n### Basic Usage\n\n```bash\n# Serve a markdown file on default port (3000)\nmdserve README.md\n\n# Serve on custom port\nmdserve README.md --port 8080\nmdserve README.md -p 8080\n```\n\n\n## Endpoints\n\nOnce running, the server provides (default: [http://localhost:3000](http://localhost:3000)):\n\n- **[`/`](http://localhost:3000/)** - Rendered HTML with live reload via WebSocket\n- **[`/raw`](http://localhost:3000/raw)** - Raw markdown content (useful for debugging)\n- **[`/ws`](http://localhost:3000/ws)** - WebSocket endpoint for real-time updates\n\n## Theme System\n\n**Built-in Theme Selector**\n- Click the üé® button in the top-right corner to open theme selector\n- **5 Available Themes**:\n  - **Light**: Clean, bright theme optimized for readability\n  - **Dark**: GitHub-inspired dark theme with comfortable contrast\n  - **Catppuccin Latte**: Warm light theme with soothing pastels\n  - **Catppuccin Macchiato**: Cozy mid-tone theme with rich colors\n  - **Catppuccin Mocha**: Deep dark theme with vibrant accents\n- **Persistent Preference**: Your theme choice is automatically saved in browser localStorage\n\n*Click the theme button (üé®) to access the built-in theme selector*\n\n![Theme picker interface](mdserve-theme-picker.png)\n\n*mdserve running with the Catppuccin Macchiato theme - notice the warm, cozy colors and excellent readability*\n\n![mdserve with Catppuccin Macchiato theme](mdserve-catppuccin-macchiato.png)\n\n## Development\n\n### Prerequisites\n\n- Rust 1.85+ (2024 edition)\n\n### Building\n\n```bash\ncargo build --release\n```\n\n### Running Tests\n\n```bash\n# Run all tests\ncargo test\n\n# Run integration tests only\ncargo test --test integration_test\n```\n\n## License\n\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.\n\n## Acknowledgments\n\n- Built with [Axum](https://github.com/tokio-rs/axum) web framework\n- Markdown parsing by [markdown-rs](https://github.com/wooorm/markdown-rs)\n- [Catppuccin](https://catppuccin.com/) color themes\n- Inspired by various markdown preview tools\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-10-03T02:16:41.426631"
  },
  {
    "basic_info": {
      "name": "Wyrm",
      "full_name": "0xflux/Wyrm",
      "owner": "0xflux",
      "description": " The dragon in the dark. A red team post exploitation framework for testing security controls during red team assessments. ",
      "url": "https://github.com/0xflux/Wyrm",
      "clone_url": "https://github.com/0xflux/Wyrm.git",
      "ssh_url": "git@github.com:0xflux/Wyrm.git",
      "homepage": "",
      "created_at": "2025-09-25T16:41:37Z",
      "updated_at": "2025-10-03T01:39:09Z",
      "pushed_at": "2025-10-02T06:38:10Z"
    },
    "stats": {
      "stars": 174,
      "forks": 19,
      "watchers": 174,
      "open_issues": 1,
      "size": 3856
    },
    "tech_info": {
      "language": "Rust",
      "languages": {
        "Rust": 260642,
        "HTML": 15757,
        "CSS": 5881,
        "Shell": 3318,
        "PowerShell": 729,
        "PLpgSQL": 691,
        "Dockerfile": 646
      },
      "license": "MIT License",
      "topics": [
        "adversary-emulation",
        "adversary-simulation",
        "c2",
        "command-and-control",
        "pentest",
        "pentesting",
        "red-team",
        "red-teaming",
        "rust",
        "security-tools",
        "wyrm"
      ]
    },
    "content": {
      "readme": "# Wyrm - v0.3 Hatchling\n\n&#128679; Pre-release version &#128679;. If you want to support this project, please give it a star! I will be releasing updates and\ndevlogs on my [blog](https://fluxsec.red/) and [YouTube](https://www.youtube.com/@FluxSec) to document progress, so please give me a follow there.\n\nWyrm (pronounced 'worm', an old English word for 'serpent' or 'dragon') is a post exploitation, open source, Red Team security testing framework framework, written in Rust designed to be used by Red Teams, Purple Teams, \nPenetration Testers, and general infosec hobbyists. This project is fully built in Rust, with extra effort going into obfuscating artifacts which\ncould be present in memory. Project created and maintained by [flux](https://github.com/0xflux/), for **legal authorised security testing only**.\n\n![Wyrm Malware Post Exploitation Implant Red Team](resources/wyrm_landscape.png)\n\nWyrm currently supports only HTTP(S) agents using a custom encryption scheme for encrypting traffic below TLS, with a unique packet design so that\nthe packets cannot be realistically decrypted even under firewall level TLS inspection.\n\nThis project is a work in progress, currently released at v0.2 (Hatchling). Updates are planned through versions 1,0, 2.0, 3.0, and 4.0. You can view\nthe planned roadmap in this project (see [Milestones.md](https://github.com/0xflux/Wyrm/blob/master/Milestones.md)). In time, this is designed to be an open source competitor to **Cobalt Strike**, **Mythic**, **Sliver**, etc.\n\nFor any bugs, or feature requests, please use the Issues tab, and for anything else - please use GitHub Discussions. I am active on this project,\nso I will be attentive to anything raised.\n\n### Features\n\n- Implant uses a configurable profile to customise features and configurations\n- IOCs encrypted in the payload to assist in anti-analysis and anti-yara hardening\n- Implant transmits data encrypted below TLS, defeating perimeter inspection security tools out the box\n- Dynamic payload generation\n- Easy mechanism to stage files (such as built implants, PDF, zip, etc) on the C2 for download to support phishing campaigns and initial attack vectors\n- Supports native Windows API commands, more planned in future updates\n- Easy to use terminal client for the operator to task & inspect agents, and to manage staged resources\n- Implant uses the most common User-Agent for comms to help it blend in covertly with traffic by default, this is also configurable to suit your engagement\n- Easy, automated C2 infrastructure deployment with `install_server.sh`\n- Anti-sandbox techniques which are highly configurable by the operator through profiles\n- Backed by a database, fully timestamped to make reporting easier\n\nThis project is not currently accepting contributions, please **raise issues** or use **GitHub Discussions** and I will look into them, and help\nanswer any questions.\n\n**Before deploying the C2**, you should read the C2 readme file, found in the `/c2` directory. Proper docs are coming soon\nin time for v1.0 release, at https://wyrm-c2.com.\n\nA mental model for the C2 is as follows:\n\n![Wyrm C2](resources/c2_model.png)\n\nThe below image demonstrates the **Below TLS Encryption** feature and how it is implemented:\n\n![Wyrm Below TLS Encryption](resources/wyrm_post_diag.png)\n\n### Updates\n\n**WARNING:** Before pulling an update; please check the [release notes](https://github.com/0xflux/Wyrm/blob/master/RELEASE_NOTES.md) to see whether there are any breaking changes - for example if the\n**configurable C2 profile** changes in a breaking way from a previous profile you have, you will want to make sure you backup and migrate\nyour profile. I will be excluding `/c2/profiles/*` from git once the project is published in pre-release to prevent accidentally overwriting\nyour previous profile when running `git pull` to update your software.\n\nAs per the roadmap, this project will see significant development over the next 12 months. To pull updates, whether they are new features\nor bug fixes, you simply just do a git pull, re-build the c2 in release mode via:\n\n- `sudo systemctl stop wyrm`\n- `cd c2`, \n- `cargo build --release`\n- `sudo systemctl start wyrm`\n\n### Setup\n\nThe project contains an install shell script, and is designed to be run on `Debian` based Linux flavours.\nThe install script will install all required dependencies to the project, as well as making a new user, `wyrm_user`\nthat will run the C2 service.\n\nThe user account is created as `sudo useradd --system --no-create-home --shell /usr/sbin/nologin wyrm_user`.\n\n**Server Setup**\n\n1) Install your favourite reverse proxy (NGINX / Apache etc). The web app will default to serve on `0.0.0.0` at `:8080`. You can edit this in `/c2/.env` (at step 2), so configure your reverse proxy to use whatever you define in the `.env`.\n2) Clone the repo to your server & mark the install script executable.\n3) **SECURITY**: \n   1) In `c2/.env` edit:\n      1) `POSTGRES_PASSWORD`\n      2) `ADMIN_TOKEN` - **DO NOT USE THE ",
      "default_branch": "master"
    },
    "fetched_at": "2025-10-03T02:16:42.772190"
  },
  {
    "basic_info": {
      "name": "db-back-tool",
      "full_name": "iKeepLearn/db-back-tool",
      "owner": "iKeepLearn",
      "description": "postgresql„ÄÅmysqlÊï∞ÊçÆÂ∫ìÂ§á‰ªΩÂπ∂‰∏ä‰º†Âà∞ËÖæËÆØ‰∫ëÊàñËÄÖÈòøÈáå‰∫ëÊàñÂÖºÂÆπS3ÂçèËÆÆÁöÑÂÖ∂‰ªñ‰∫ëÂ≠òÂÇ®ÔºåÂêåÊó∂ÂèØÂàóÂá∫„ÄÅÂà†Èô§‰∫ë‰∏äÂ≠òÂÇ®ÁöÑÂ§á‰ªΩÊñá‰ª∂„ÄÇ",
      "url": "https://github.com/iKeepLearn/db-back-tool",
      "clone_url": "https://github.com/iKeepLearn/db-back-tool.git",
      "ssh_url": "git@github.com:iKeepLearn/db-back-tool.git",
      "homepage": "",
      "created_at": "2025-09-22T08:49:42Z",
      "updated_at": "2025-10-02T02:06:19Z",
      "pushed_at": "2025-10-01T01:45:04Z"
    },
    "stats": {
      "stars": 155,
      "forks": 13,
      "watchers": 155,
      "open_issues": 0,
      "size": 289
    },
    "tech_info": {
      "language": "Rust",
      "languages": {
        "Rust": 44511
      },
      "license": null,
      "topics": []
    },
    "content": {
      "readme": "# ‰ΩøÁî®ËØ¥Êòé\n\n‰∏ÄÊ¨æÂü∫‰∫é Rust ÂºÄÂèëÁöÑÊï∞ÊçÆÂ∫ìÂ§á‰ªΩÂ∑•ÂÖ∑ÔºåÊîØÊåÅÂçïÂÆû‰æã PostgreSQL/MySQL Êï∞ÊçÆÂ∫ìÁöÑËá™Âä®Â§á‰ªΩ„ÄÅÂä†ÂØÜ„ÄÅÂéãÁº©ÔºåÂπ∂ÂèØÂ∞ÜÂ§á‰ªΩÊñá‰ª∂‰∏ä‰º†Ëá≥ËÖæËÆØ‰∫ë COS ÊàñÈòøÈáå‰∫ë OSSÊàñÂÖºÂÆπ S3 ÂçèËÆÆÁöÑÂÖ∂‰ªñ‰∫ëÂ≠òÂÇ®„ÄÇ\n\nÂºÄÂèëÂä®Êú∫ÊòØÊú¨‰∫∫Áª¥Êä§ÁùÄÂæàÂ§öÂçï‰ΩìÊúçÂä°ÂàÜÂ∏ÉÂú®ÂêÑ‰∏™‰∫ëÊúçÂä°Âô®‰∏äÔºåÊØè‰∏™Âçï‰ΩìÊúçÂä°ÈÉΩ‰ΩøÁî®ÂêÑËá™ÁöÑÊï∞ÊçÆÂ∫ìÂÆû‰æãÔºåÂõ†‰∏∫Áî≤ÊñπÈ¢ÑÁÆóÂéüÂõ†Ê≤°ÊúâÈÖçÁΩÆÊï∞ÊçÆÂ∫ì‰∏ª‰ªéÂ§á‰ªΩ„ÄÇ\n‰ΩÜÂèàÊúâÂ§á‰ªΩÁöÑÈúÄÊ±ÇÔºåÊâÄ‰ª•Âè™Â•ΩÂÜô‰∏™Â∑•ÂÖ∑‰ΩøÁî®Êï∞ÊçÆÂ∫ìËá™Â∏¶ÁöÑ dump Â∑•ÂÖ∑Â§á‰ªΩÔºåÂÜçÂä†ÂØÜÂéãÁº©‰∏ä‰º†Âà∞‰∫ëÂ≠òÂÇ®„ÄÇ\n\n## ÂäüËÉΩÁâπÊÄß\n\n- ÊîØÊåÅ PostgreSQL\\MySql Êï∞ÊçÆÂ∫ìËá™Âä®Â§á‰ªΩ\n- Â§á‰ªΩÊñá‰ª∂Ëá™Âä®Âä†ÂØÜ„ÄÅÂéãÁº©\n- ‰∏ÄÈîÆ‰∏ä‰º†Â§á‰ªΩÂà∞ËÖæËÆØ‰∫ë COS\\ÈòøÈáå‰∫ë OSS\\ÂÖºÂÆπS3ÂçèËÆÆÁöÑÂÖ∂‰ªñ‰∫ëÂ≠òÂÇ®\n- ÊîØÊåÅÂ§á‰ªΩÊñá‰ª∂ÁöÑÊâπÈáè‰∏ä‰º†„ÄÅÊâπÈáèÂà†Èô§„ÄÅÂàóË°®Êü•Áúã\n- ÊîØÊåÅËá™ÂÆö‰πâÈÖçÁΩÆÊñá‰ª∂\n\n## ÂâçÁΩÆÊù°‰ª∂\n\nËØ∑Á°Æ‰øùÊúçÂä°Âô®Â∑≤ÂÆâË£Ö `7z`„ÄÇ  \nÂÆâË£ÖÂëΩ‰ª§ÔºàDebian/UbuntuÔºâÔºö\n\n```bash\nsudo apt install p7zip-full\n```\n\n---\n\n## Âø´ÈÄüÂºÄÂßã\n\n1. ‰ªé [release È°µÈù¢](https://github.com/iKeepLearn/db-back-tool/releases) ‰∏ãËΩΩÂèØÊâßË°åÊñá‰ª∂ÁöÑ zip ÂåÖ„ÄÇ\n2. Ëß£ÂéãÂêéÔºå‰øÆÊîπÂÖ∂‰∏≠ÁöÑ `config.yaml` ÈÖçÁΩÆÊñá‰ª∂‰∏∫Ê≠£Á°ÆÁöÑÈÖçÁΩÆ„ÄÇ\n\n---\n\n## Â∏∏Áî®ÂëΩ‰ª§Á§∫‰æã\n\n- **Â§á‰ªΩÊåáÂÆöÊï∞ÊçÆÂ∫ì**\n\n  ```bash\n  ./backupdbtool --config config.yaml backup <database_name>\n  ```\n\n- **‰∏ä‰º†ÊâÄÊúâÂæÖ‰∏ä‰º†Â§á‰ªΩÊñá‰ª∂**\n\n  ```bash\n  ./backupdbtool --config config.yaml upload --all\n  ```\n\n- **‰∏ä‰º†Âçï‰∏™Â§á‰ªΩÊñá‰ª∂**\n\n  ```bash\n  ./backupdbtool --config config.yaml upload --file /path/to/filename.ext\n  ```\n\n- **Âà†Èô§ÊâÄÊúâ‰∏§Â§©ÂâçÁöÑÂ§á‰ªΩ‰ª•ÂáèÂ∞ë‰∫ëÂ≠òÂÇ®ÊàêÊú¨**\n\n  ```bash\n  ./backupdbtool --config config.yaml delete --all\n  ```\n\n- **Âà†Èô§Âçï‰∏™‰∫ëÂ≠òÂÇ®Êñá‰ª∂**\n\n  ```bash\n  ./backupdbtool --config config.yaml delete --key key\n  ```\n  > key ‰∏∫‰∫ëÂ≠òÂÇ®‰∏≠ÁöÑÂÆåÊï¥Ë∑ØÂæÑÔºåÊØîÂ¶ÇÊÉ≥Âà†Èô§‰∏ãÊñπ list ‰∏≠ÁöÑ config.yaml Âàô key ‰∏∫ db/config.yaml„ÄÇ\n\n  > ÂÆåÊï¥Á§∫‰æã: ./backupdbtool --config config.yaml delete --key db/config.yaml„ÄÇ\n\n- **ÂàóÂá∫ÊâÄÊúâÂ§á‰ªΩÊñá‰ª∂**\n  ```bash\n  ./backupdbtool --config config.yaml list\n  ```\n  ![list](images/list.png)\n\n## ÂÆöÊó∂‰ªªÂä°ÔºàCronÔºâÊé®ËçêÈÖçÁΩÆ\n\n- **ÊØèÊó•ÂáåÊô® 2 ÁÇπËá™Âä®Â§á‰ªΩÊï∞ÊçÆÂ∫ì**\n\n  ```bash\n  0 2 * * * /path/to/backupdbtool --config /path/to/config.yaml backup <database_name>\n  ```\n\n- **ÊØèÊó•ÂáåÊô® 2:30 ‰∏ä‰º†ÊâÄÊúâÂæÖ‰∏ä‰º†Â§á‰ªΩ**\n\n  ```bash\n  30 2 * * * /path/to/backupdbtool --config /path/to/config.yaml upload --all\n  ```\n\n- **ÊØèÂë®Êó•ÂáåÊô® 3 ÁÇπÂà†Èô§ÊâÄÊúâ‰∏§Â§©ÂâçÁöÑÂ§á‰ªΩ‰ª•ÂáèÂ∞ë‰∫ëÂ≠òÂÇ®ÊàêÊú¨**\n  ```bash\n  0 3 * * 0 /path/to/backupdbtool --config /path/to/config.yaml delete --all\n  ```\n\n> ËØ∑Â∞Ü `/path/to/backupdbtool` Âíå `/path/to/config.yaml` ÊõøÊç¢‰∏∫ÂÆûÈôÖË∑ØÂæÑÔºå`<database_name>` ÊõøÊç¢‰∏∫ÁõÆÊ†áÊï∞ÊçÆÂ∫ìÂêçÁß∞„ÄÇ\n\n## ËÅîÁ≥ªÊñπÂºè\n\nÂ¶ÇÊúâÁñëÈóÆÔºåËØ∑ËÅîÁ≥ªÂºÄÂèëËÄÖ„ÄÇ\n\n![ËÅîÁ≥ª‰ΩúËÄÖ](images/ccwechat.jpg)\n",
      "default_branch": "master"
    },
    "fetched_at": "2025-10-03T02:16:44.033774"
  },
  {
    "basic_info": {
      "name": "crispy-palm-tree",
      "full_name": "bingcicle/crispy-palm-tree",
      "owner": "bingcicle",
      "description": "dirhash",
      "url": "https://github.com/bingcicle/crispy-palm-tree",
      "clone_url": "https://github.com/bingcicle/crispy-palm-tree.git",
      "ssh_url": "git@github.com:bingcicle/crispy-palm-tree.git",
      "homepage": null,
      "created_at": "2025-09-29T15:13:51Z",
      "updated_at": "2025-10-02T14:02:06Z",
      "pushed_at": "2025-09-29T15:15:30Z"
    },
    "stats": {
      "stars": 151,
      "forks": 0,
      "watchers": 151,
      "open_issues": 0,
      "size": 6
    },
    "tech_info": {
      "language": "Rust",
      "languages": {
        "Rust": 2599
      },
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "# dirhash\nCompute SHA-256 for files in a directory and list duplicates.\n\n## Build\n```bash\ncargo build --release\n./target/release/dirhash ./data --exts jpg,png --dupes > dupes.csv\n```\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-10-03T02:16:45.376148"
  },
  {
    "basic_info": {
      "name": "tsink",
      "full_name": "h2337/tsink",
      "owner": "h2337",
      "description": "Embedded time-series database for Rust",
      "url": "https://github.com/h2337/tsink",
      "clone_url": "https://github.com/h2337/tsink.git",
      "ssh_url": "git@github.com:h2337/tsink.git",
      "homepage": "",
      "created_at": "2025-09-12T21:29:24Z",
      "updated_at": "2025-09-28T21:39:14Z",
      "pushed_at": "2025-09-21T01:11:14Z"
    },
    "stats": {
      "stars": 149,
      "forks": 7,
      "watchers": 149,
      "open_issues": 2,
      "size": 245
    },
    "tech_info": {
      "language": "Rust",
      "languages": {
        "Rust": 186920
      },
      "license": "MIT License",
      "topics": [
        "database",
        "embedded-database",
        "rust",
        "time-series",
        "timeseries",
        "timeseries-database",
        "tsdb"
      ]
    },
    "content": {
      "readme": "# tsink\n\n<div align=\"center\">\n\n<p align=\"right\">\n  <img src=\"https://raw.githubusercontent.com/h2337/tsink/refs/heads/master/logo.svg\" width=\"250\" height=\"250\">\n</p>\n\n**A high-performance embedded time-series database for Rust**\n\n</div>\n\n## Overview\n\ntsink is a lightweight, high-performance time-series database engine written in Rust. It provides efficient storage and retrieval of time-series data with automatic compression, time-based partitioning, and thread-safe operations.\n\n### Key Features\n\n- **üöÄ High Performance**: Gorilla compression achieves ~1.37 bytes per data point\n- **üîí Thread-Safe**: Lock-free reads and concurrent writes with configurable worker pools\n- **üíæ Flexible Storage**: Choose between in-memory or persistent disk storage\n- **üìä Time Partitioning**: Automatic data organization by configurable time ranges\n- **üè∑Ô∏è Label Support**: Multi-dimensional metrics with key-value labels\n- **üìù WAL Support**: Write-ahead logging for durability and crash recovery\n- **üóëÔ∏è Auto-Retention**: Configurable automatic data expiration\n- **üê≥ Container-Aware**: cgroup support for optimal resource usage in containers\n- **‚ö° Zero-Copy Reads**: Memory-mapped files for efficient disk operations\n\n## Installation\n\nAdd tsink to your `Cargo.toml`:\n\n```toml\n[dependencies]\ntsink = \"0.3.1\"\n```\n\n## Quick Start\n\n### Basic Usage\n\n```rust\nuse tsink::{DataPoint, Row, StorageBuilder, Storage, TimestampPrecision};\n\nfn main() -> Result<(), Box<dyn std::error::Error>> {\n    // Create storage with default settings\n    let storage = StorageBuilder::new()\n        .with_timestamp_precision(TimestampPrecision::Seconds)\n        .build()?;\n\n    // Insert data points\n    let rows = vec![\n        Row::new(\"cpu_usage\", DataPoint::new(1600000000, 45.5)),\n        Row::new(\"cpu_usage\", DataPoint::new(1600000060, 47.2)),\n        Row::new(\"cpu_usage\", DataPoint::new(1600000120, 46.8)),\n    ];\n    storage.insert_rows(&rows)?;\n\n    // Note: Using timestamp 0 will automatically use the current timestamp\n    // let row = Row::new(\"cpu_usage\", DataPoint::new(0, 50.0));  // timestamp = current time\n\n    // Query data points\n    let points = storage.select(\"cpu_usage\", &[], 1600000000, 1600000121)?;\n    for point in points {\n        println!(\"Timestamp: {}, Value: {}\", point.timestamp, point.value);\n    }\n\n    storage.close()?;\n    Ok(())\n}\n```\n\n### Persistent Storage\n\n```rust\nuse tsink::{StorageBuilder, Storage};\nuse std::time::Duration;\n\nlet storage = StorageBuilder::new()\n    .with_data_path(\"./tsink-data\")              // Enable disk persistence\n    .with_partition_duration(Duration::from_secs(3600))  // 1-hour partitions\n    .with_retention(Duration::from_secs(7 * 24 * 3600))  // 7-day retention\n    .with_wal_buffer_size(8192)                  // 8KB WAL buffer\n    .build()?;\n```\n\n### Multi-Dimensional Metrics with Labels\n\n```rust\nuse tsink::{DataPoint, Label, Row};\n\n// Create metrics with labels for detailed categorization\nlet rows = vec![\n    Row::with_labels(\n        \"http_requests\",\n        vec![\n            Label::new(\"method\", \"GET\"),\n            Label::new(\"status\", \"200\"),\n            Label::new(\"endpoint\", \"/api/users\"),\n        ],\n        DataPoint::new(1600000000, 150.0),\n    ),\n    Row::with_labels(\n        \"http_requests\",\n        vec![\n            Label::new(\"method\", \"POST\"),\n            Label::new(\"status\", \"201\"),\n            Label::new(\"endpoint\", \"/api/users\"),\n        ],\n        DataPoint::new(1600000000, 25.0),\n    ),\n];\n\nstorage.insert_rows(&rows)?;\n\n// Query specific label combinations\nlet points = storage.select(\n    \"http_requests\",\n    &[\n        Label::new(\"method\", \"GET\"),\n        Label::new(\"status\", \"200\"),\n    ],\n    1600000000,\n    1600000100,\n)?;\n\n// Query all label combinations for a metric\nlet all_results = storage.select_all(\"http_requests\", 1600000000, 1600000100)?;\nfor (labels, points) in all_results {\n    println!(\"Labels: {:?}, Points: {}\", labels, points.len());\n}\n```\n\n## Architecture\n\ntsink uses a linear-order partition model that divides time-series data into time-bounded chunks:\n\n```\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ             tsink Storage               ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ                                         ‚îÇ\n‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  Active Partition    ‚îÇ\n‚îÇ  ‚îÇ Memory Part.  ‚îÇ‚óÑ‚îÄ (Writable)         ‚îÇ\n‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                      ‚îÇ\n‚îÇ                                         ‚îÇ\n‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  Buffer Partition    ‚îÇ\n‚îÇ  ‚îÇ Memory Part.  ‚îÇ‚óÑ‚îÄ (Out-of-order)     ‚îÇ\n‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                      ‚îÇ\n‚îÇ                                         ‚îÇ\n‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                      ‚îÇ\n‚îÇ  ‚îÇ Disk Part. 1  ‚îÇ‚óÑ‚îÄ Read-only          ‚îÇ\n‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   (Memory-mapped)    ‚îÇ\n‚îÇ                                         ‚îÇ\n‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                      ‚îÇ\n‚îÇ  ‚îÇ Disk Part. 2  ‚îÇ‚óÑ‚îÄ Read-only          ‚îÇ\n‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                      ‚îÇ\n‚îÇ         ...                             ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n```\n\n### Partition ",
      "default_branch": "master"
    },
    "fetched_at": "2025-10-03T02:16:46.669086"
  },
  {
    "basic_info": {
      "name": "wakezilla",
      "full_name": "guibeira/wakezilla",
      "owner": "guibeira",
      "description": "A simple Wake-on-LAN & reverse proxy toolkit ‚Äî wake, route, and control your machines from anywhere. ü¶ñ   ",
      "url": "https://github.com/guibeira/wakezilla",
      "clone_url": "https://github.com/guibeira/wakezilla.git",
      "ssh_url": "git@github.com:guibeira/wakezilla.git",
      "homepage": null,
      "created_at": "2025-09-06T16:13:34Z",
      "updated_at": "2025-09-29T05:09:09Z",
      "pushed_at": "2025-10-03T01:43:14Z"
    },
    "stats": {
      "stars": 132,
      "forks": 8,
      "watchers": 132,
      "open_issues": 0,
      "size": 1049
    },
    "tech_info": {
      "language": "Rust",
      "languages": {
        "Rust": 124222,
        "HTML": 38904
      },
      "license": null,
      "topics": []
    },
    "content": {
      "readme": "# Wakezilla ü¶ñ\n<img width=\"200\" height=\"159\" src=\"https://github.com/user-attachments/assets/e88f084b-47b8-467b-a5c6-d64327805792\" align=\"left\" alt=\"wakezilla\"/>\n\n‚ö° Wake-on-LAN made simple ‚Üí power on your machines remotely whenever needed.\n\nüåê Reverse proxy ‚Üí intercepts traffic and wakes the server automatically if it‚Äôs offline.\n\nüîå Automatic shutdown ‚Üí saves energy by powering down idle machines after configurable thresholds.\n\n\n## Web interface\n<img width=\"2698\" height=\"2012\" alt=\"image\" src=\"https://github.com/user-attachments/assets/667eedeb-431c-4aa2-bf7a-3eadd4221452\" />\n\n## Features\n\n- **Wake-on-LAN**: Send magic packets to wake sleeping machines\n- **TCP Proxy**: Forward ports to remote machines with automatic WOL\n- **Web Interface**: Manage machines, ports, and monitor activity through a web dashboard\n- **Automatic Shutdown**: Automatically turn off machines after inactivity periods\n- **Network Scanner**: Discover machines on your local network\n\n## Installation\n\n### Server Installation\n\n1. **Install Rust**:\n   ```bash\n   curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh\n   source $HOME/.cargo/env\n   ```\n\n2. **Clone and Build**:\n   ```bash\n   git clone <repository-url>\n   cd wakezilla\n   cargo build --release\n   ```\n\n3. **Configure the Server**:\n   Create a `machines.json` file (optional, will be created automatically):\n   ```json\n   []\n   ```\n\n4. **Run the Server**:\n   ```bash\n   ./target/release/wakezilla --server\n   ```\n   \n   By default, the web interface runs on port 3000.\n\n### Client Installation\n   make sure the machine was configured with wake on lan.\n1. **Install Rust** (if not already installed):\n   ```bash\n   curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh\n   source $HOME/.cargo/env\n   ```\n\n2. **Clone and Build**:\n   ```bash\n   git clone <repository-url>\n   cd wakezilla\n   cargo build --release\n   ```\n## Usage\n\n### Starting the Proxy Server\n```bash\n# Basic server start\n./target/release/wakezilla proxy-server\n\n# With custom port\n./target/release/wakezilla proxy-server --port 8080\n\n```\n\n### Starting the Client\n```bash\n# Connect to server\n./target/release/wakezilla client-server \n\n# With custom port\n./target/release/wakezilla client-server --port 8080\n\n```\n\n### Web Interface\nAccess the web interface at `http://<server-ip>:3000` to:\n- Add and manage machines\n- Configure port forwards\n- View network scan results\n- Send WOL packets manually\n- Configure automatic shutdown settings\n\n### Adding Machines\n1. Navigate to the web interface\n2. Click \"Add Machine\" or use the network scanner\n3. Fill in MAC address, IP, and name\n4. Configure:\n   - Turn-off port (if remote shutdown is needed)\n   - Request rate limiting (requests per hour and period minutes)\n   - Port forwards as needed\n\n### Configuring Automatic Shutdown\n1. When adding or editing a machine, enable \"Can be turned off remotely\"\n2. Set the \"Turn Off Port\" (typically 3001 for the client server)\n3. Configure rate limiting:\n   - Requests per Hour: Number of requests allowed\n   - Period Minutes: Time window for rate limiting\n4. The machine will automatically shut down after the configured inactivity period\n\n### Port Forwarding\n1. Add a machine to the system\n2. Configure port forwards for that machine:\n   - Local Port: Port on the server to listen on\n   - Target Port: Port on the remote machine to forward to\n3. When traffic hits the local port, the machine will be woken up if needed and traffic forwarded\n\n\n### Machine Configuration\nEach machine can be configured with:\n- MAC Address\n- IP Address\n- Name and Description\n- Turn-off Port (for remote shutdown)\n- Request Rate Limiting:\n  - Requests per Hour: Maximum requests allowed\n  - Period Minutes: Time window for rate limiting\n- Port Forwards:\n  - Local Port: Port on the server\n  - Target Port: Port on the remote machine\n\n## How It Works\n\n1. **Server Mode**: Runs the web interface and proxy services\n2. **Client Mode**: Runs on target machines to enable remote shutdown\n3. **WOL Process**: \n   - When traffic hits a configured port, the server sends a WOL packet\n   - Waits for the machine to become reachable\n   - Forwards traffic once the machine is up\n4. **Automatic Shutdown**: \n   - Monitors request activity for each machine\n   - After configured inactivity periods, sends shutdown signal\n   - Uses HTTP requests to the client for shutdown\n\n## Security Considerations\n\n- The server should be run on a trusted network\n- Access to the web interface should be restricted if exposed to the internet\n- The turn-off endpoint on clients should only be accessible from the server\n\n## Troubleshooting\n\n### Common Issues\n\n1. **Machine not waking up**:\n   - Verify the MAC address is correct\n   - Ensure WOL is enabled in the machine's BIOS/UEFI\n   - Check firewall settings on the target machine\n   - Verify the target machine supports WOL\n\n2. **Proxy not working**:\n   - Check that the target port is correct\n   - Verify the machine is reachable after WOL\n   - Ensure no firewall is blocking the conne",
      "default_branch": "main"
    },
    "fetched_at": "2025-10-03T02:16:47.954743"
  },
  {
    "basic_info": {
      "name": "DriftDB",
      "full_name": "DavidLiedle/DriftDB",
      "owner": "DavidLiedle",
      "description": "DriftDB - An experimental append-only database with built-in time travel. Query any point in history, guaranteed data integrity, and immutable audit trails. Written in Rust.",
      "url": "https://github.com/DavidLiedle/DriftDB",
      "clone_url": "https://github.com/DavidLiedle/DriftDB.git",
      "ssh_url": "git@github.com:DavidLiedle/DriftDB.git",
      "homepage": null,
      "created_at": "2025-09-14T16:27:28Z",
      "updated_at": "2025-10-01T03:13:16Z",
      "pushed_at": "2025-09-25T00:18:28Z"
    },
    "stats": {
      "stars": 129,
      "forks": 5,
      "watchers": 129,
      "open_issues": 2,
      "size": 2576
    },
    "tech_info": {
      "language": "Rust",
      "languages": {
        "Rust": 2453325,
        "Python": 338364,
        "Shell": 4308,
        "PLpgSQL": 2708,
        "Makefile": 2693,
        "Dockerfile": 1122
      },
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "# DriftDB\n\n**Experimental PostgreSQL-Compatible Time-Travel Database (v0.7.3-alpha)** - An ambitious temporal database project with advanced architectural designs for enterprise features. Query your data at any point in history using standard SQL.\n\n‚ö†Ô∏è **ALPHA SOFTWARE - NOT FOR PRODUCTION USE**: This version contains experimental implementations of enterprise features. The codebase now compiles cleanly with minimal warnings (reduced from 335 to 17). Many advanced features remain as architectural designs requiring implementation.\n\n## üöÄ Quick Start\n\n```bash\n# Start the PostgreSQL-compatible server\n./target/release/driftdb-server --data-path ./data\n\n# Connect with any PostgreSQL client\npsql -h localhost -p 5433 -d driftdb\n\n# Use standard SQL with time-travel\nCREATE TABLE events (id INT PRIMARY KEY, data VARCHAR);\nINSERT INTO events (id, data) VALUES (1, 'original');\nUPDATE events SET data = 'modified' WHERE id = 1;\n\n-- Query historical state!\nSELECT * FROM events AS OF @seq:1;  -- Shows 'original'\nSELECT * FROM events;                -- Shows 'modified'\n```\n\n## ‚úÖ Working Features\n\n### Full SQL Support\n- **All 5 standard JOIN types**: INNER, LEFT, RIGHT, FULL OUTER, CROSS (including self-joins)\n- **Subqueries**: IN/NOT IN, EXISTS/NOT EXISTS (including correlated!), scalar subqueries\n- **Common Table Expressions (CTEs)**: WITH clause including RECURSIVE CTEs\n- **Transactions**: BEGIN, COMMIT, ROLLBACK with ACID guarantees\n- **Views**: CREATE/DROP VIEW with persistence across restarts\n- **DDL operations**: CREATE TABLE, ALTER TABLE ADD COLUMN, CREATE INDEX, TRUNCATE\n- **Aggregation functions**: COUNT(*), COUNT(DISTINCT), SUM, AVG, MIN, MAX\n- **GROUP BY and HAVING**: Full support for grouping with aggregate filtering\n- **CASE WHEN expressions**: Conditional logic in queries\n- **Set operations**: UNION, INTERSECT, EXCEPT\n- **Multi-row INSERT**: INSERT INTO ... VALUES (row1), (row2), ...\n- **Foreign key constraints**: Referential integrity enforcement\n- **Time-travel queries**: `AS OF` for querying historical states\n\n### Core Database Engine\n- **Event sourcing**: Every change is an immutable event with full history\n- **Time-travel queries**: Query any historical state by sequence number\n- **ACID compliance**: Full transaction support with isolation levels\n- **CRC32 verification**: Data integrity on every frame\n- **Append-only storage**: Never lose data, perfect audit trail\n- **JSON documents**: Flexible schema with structured data\n\n### Tested & Verified\n- ‚úÖ Python psycopg2 driver\n- ‚úÖ Node.js pg driver\n- ‚úÖ JDBC PostgreSQL driver\n- ‚úÖ SQLAlchemy ORM\n- ‚úÖ Any PostgreSQL client\n\n## üéØ Perfect For\n\n- **Debugging Production Issues**: \"What was the state when the bug occurred?\"\n- **Compliance & Auditing**: Complete audit trail built-in, no extra work\n- **Data Recovery**: Accidentally deleted data? It's still there!\n- **Analytics**: Track how metrics changed over time\n- **Testing**: Reset to any point, perfect for test scenarios\n- **Development**: Branch your database like Git\n\n## ‚ú® Core Features\n\n### SQL:2011 Temporal Queries (Native Support)\n- **`FOR SYSTEM_TIME AS OF`**: Query data at any point in time\n- **`FOR SYSTEM_TIME BETWEEN`**: Get all versions in a time range\n- **`FOR SYSTEM_TIME FROM...TO`**: Exclusive range queries\n- **`FOR SYSTEM_TIME ALL`**: Complete history of changes\n- **System-versioned tables**: Automatic history tracking\n\n### Data Model & Storage\n- **Append-only storage**: Immutable events preserve complete history\n- **Time travel queries**: Standard SQL:2011 temporal syntax\n- **ACID transactions**: Full transaction support with isolation levels\n- **Secondary indexes**: B-tree indexes for fast lookups\n- **Snapshots & compaction**: Optimized performance with compression\n\n### Planned Enterprise Features (Not Yet Functional)\nThe following features have been architecturally designed but are not yet operational:\n- **Authentication & Authorization**: Planned RBAC with user management (code incomplete)\n- **Encryption at Rest**: Designed AES-256-GCM encryption (not functional)\n- **Distributed Consensus**: Raft protocol structure (requires debugging)\n- **Advanced Transactions**: MVCC design for isolation levels (partial implementation)\n- **Enterprise Backup**: Backup system architecture (compilation errors)\n- **Security Monitoring**: Monitoring framework (not integrated)\n\n### Working Infrastructure\n- **Connection pooling**: Thread-safe connection pool with RAII guards\n- **Health checks**: Basic metrics endpoint\n- **Rate limiting**: Token bucket algorithm for connection limits\n\n### Query Features (Partially Working)\n- **B-tree indexes**: Secondary indexes for fast lookups (functional)\n- **Basic query planner**: Simple execution plans (working)\n- **Prepared statements**: Statement caching (functional)\n\n### Planned Query Optimization (Design Phase)\n- **Advanced Query Optimizer**: Cost-based optimization design (not implemented)\n- **Join Strategies**: Theoretical star schema optimization (code incomplete)\n- **Subquery O",
      "default_branch": "main"
    },
    "fetched_at": "2025-10-03T02:16:49.233535"
  },
  {
    "basic_info": {
      "name": "kotofetch",
      "full_name": "hxpe-dev/kotofetch",
      "owner": "hxpe-dev",
      "description": "kotofetch is a small, configurable CLI that displays Japanese quotes in the terminal.",
      "url": "https://github.com/hxpe-dev/kotofetch",
      "clone_url": "https://github.com/hxpe-dev/kotofetch.git",
      "ssh_url": "git@github.com:hxpe-dev/kotofetch.git",
      "homepage": "",
      "created_at": "2025-09-10T15:26:52Z",
      "updated_at": "2025-10-02T21:17:40Z",
      "pushed_at": "2025-10-02T17:17:56Z"
    },
    "stats": {
      "stars": 112,
      "forks": 5,
      "watchers": 112,
      "open_issues": 1,
      "size": 2134
    },
    "tech_info": {
      "language": "Rust",
      "languages": {
        "Rust": 22332,
        "Nix": 706
      },
      "license": "MIT License",
      "topics": [
        "archlinux",
        "cli",
        "command-line",
        "fetch",
        "hyprland",
        "i3",
        "i3wm",
        "japanese",
        "linux",
        "quotes",
        "sway",
        "swaywm",
        "terminal",
        "unixporn"
      ]
    },
    "content": {
      "readme": "# kotofetch\n\nkotofetch is a small, configurable CLI tool that displays Japanese quotes in the terminal. It comes with built-in quotes and allows users to customize display options such as padding, width, translation display, and text styles.\n\n![image](./images/demo-01.png)\n\n## Installation\n\n### Arch Linux / AUR\nYou can install the stable release from the AUR:\n\n```bash\nyay -S kotofetch\n```\n\n> When prompted, choose All to clean-build the package from the downloaded PKGBUILD.\n\nOr by cloning the AUR from [here](https://aur.archlinux.org/packages/kotofetch):\n```bash\ngit clone https://aur.archlinux.org/kotofetch.git\ncd kotofetch\nmakepkg -si\n```\n\n### Nix / NixOS\nIf you use Nix, you can install `kotofetch` using those commands:\n```bash\ngit clone https://github.com/hxpe-dev/kotofetch.git\ncd kotofetch\nnix-build\n```\n\n### Prebuilt Binaries for Linux\nYou can download prebuilt binaries from the [Releases](https://github.com/hxpe-dev/kotofetch/releases). Available formats:\n- **tar.gz** (generic Linux)\n- **.deb** (Debian / Ubuntu)\n- **.rpm** (Fedora / CentOS / openSUSE)\n\n### From Source\nRequires **Rust** and **Cargo**:\n\n```bash\ngit clone https://github.com/hxpe-dev/kotofetch.git\ncd kotofetch\ncargo install --path .\n```\n\nAfter installation, you can run `kotofetch` from anywhere in your terminal.\n\n## Configuration\n\n### Config File\n\nUser configuration lives in:\n\n```\n~/.config/kotofetch/config.toml\n```\n\nHere you can customize:\n- `horizontal_padding` / `vertical_padding` - spacing around quotes\n- `width` - max width for text wrapping (`0` for automatic width)\n- `show_translation` - translation mode (`\"none\"`, `\"english\"`, `\"romaji\"`)\n- `quote_color` - named ANSI colors (`\"red\"`, `\"yellow\"`, `\"dim\"`, etc.) or hex (`\"#ffcc00\"`)\n- `translation_color` - named ANSI colors (`\"red\"`, `\"yellow\"`, `\"dim\"`, etc.) or hex (`\"#ffcc00\"`)\n- `border_color` - named ANSI colors (`\"red\"`, `\"yellow\"`, `\"dim\"`, etc.) or hex (`\"#ffcc00\"`)\n- `font_size` - small, medium, or large (adds spacing between characters)\n- `bold` - bold Japanese text (true/false)\n- `border` - show a box border (true/false)\n- `rounded_border` - show rounded border (need `border` to be enabled) (true/false)\n- `source` - show the quote source (true/false)\n- `modes` - list of quote files to use (any `.toml` file in `~/.config/kotofetch/quotes/` or built-in)\n- `seed` - RNG seed for random quotes (`0` for random seed)\n- `centered` - center text (true/false)\n\nExample `config.toml`:\n```toml\n[display]\nhorizontal_padding = 3\nvertical_padding = 1\nwidth = 50\nshow_translation = \"romaji\"\nquote_color = \"#a3be8c\"\ntranslation_color = \"dim\"\nborder_color = \"#be8ca3\"\nfont_size = \"medium\"\nbold = true\nborder = true\nrounded_border = true\nsource = true\nmodes = [\"proverb\", \"anime\"]\nseed = 0\ncentered = true\n```\n\n### Custom quotes\nBuilt-in quotes are embedded in the binary. To add your own quotes, create:\n```\n~/.config/kotofetch/quotes/\n```\n- Place any `.toml` file there.\n- The filenames can be arbitrary, the program automatically reads all `.toml` files in this folder.\n- Each `.toml` must follow this structure:\n\n```toml\n[[quote]]\njapanese = \"ÈÄÉ„Åí„Å°„ÇÉ„ÉÄ„É°„Å†\"\ntranslation = \"You mustn't run away.\"\nromaji = \"Nigeccha dame da\"\nsource = \"Neon Genesis Evangelion\"\n\n[[quote]]\njapanese = \"‰∫∫„ÅØÂøÉ„ÅßÁîü„Åç„Çã„Çì„Å†\"\ntranslation = \"People live by their hearts.\"\nromaji = \"Hito wa kokoro de ikiru nda\"\nsource = \"Your Name\"\n```\n- These custom quotes automatically merge with the built-in ones.\n> **Tip:** You can rename the built-in examples files (`anime.toml`, `proverb.toml`, `haiku.toml`) or create new ones, the program detects them automatically.\n\n## Usage\n```bash\nkotofetch                               # display a quote following the config\nkotofetch --horizontal-padding 3        # override specific config parameter temporarily\nkotofetch --modes anime,mycustomquotes  # display quotes from specific files\n```\n\n## Contributing\nContributions are welcome! Here's how you can help:\n1. **Fork** the repository.\n2. **Clone** your fork locally:\n```bash\ngit clone https://github.com/YOUR_USERNAME/kotofetch.git\ncd kotofetch\n```\n3. **Create a branch** for your changes:\n```bash\ngit checkout -b feature/my-feature\n```\n\n4. **Make changes** and **commit**:\n```bash\ngit add .\ngit commit -m \"Add my feature\"\n```\n\n5. **Push** your branch:\n```bash\ngit push origin feature/my-feature\n```\n\n6. **Open a Pull Request** on GitHub!\n\n---\n\nMade with ‚ù§Ô∏è by [hxpe](https://github.com/hxpe-dev)  \nIf you enjoy **kotofetch**, consider starring the [GitHub repository](https://github.com/hxpe-dev/kotofetch)!\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-10-03T02:16:50.511380"
  },
  {
    "basic_info": {
      "name": "mv",
      "full_name": "humblepenguinn/mv",
      "owner": "humblepenguinn",
      "description": "Real time C++ memory visualization tool built for beginners and students",
      "url": "https://github.com/humblepenguinn/mv",
      "clone_url": "https://github.com/humblepenguinn/mv.git",
      "ssh_url": "git@github.com:humblepenguinn/mv.git",
      "homepage": "https://humblepenguinn.github.io/mv/",
      "created_at": "2025-09-09T14:57:19Z",
      "updated_at": "2025-09-24T14:50:02Z",
      "pushed_at": "2025-09-09T15:40:47Z"
    },
    "stats": {
      "stars": 96,
      "forks": 5,
      "watchers": 96,
      "open_issues": 1,
      "size": 2459
    },
    "tech_info": {
      "language": "Rust",
      "languages": {
        "Rust": 158115,
        "TypeScript": 106498,
        "CSS": 4595,
        "JavaScript": 1807,
        "Makefile": 800,
        "Shell": 449,
        "HTML": 356
      },
      "license": "Other",
      "topics": []
    },
    "content": {
      "readme": "<p align=\"center\">\n<h1 align=\"center\">MV</h1>\n\n<h5 align=\"center\">Real time memory visualization tool for C++</h5>\n\n<p align=\"center\">\n    <a href=\"https://github.com/humblepenguinn/mv/actions/workflows/release.yaml\" target=\"_blank\"><img alt=\"Build Status\" src=\"https://img.shields.io/github/actions/workflow/status/humblepenguinn/mv/release.yaml?style=flat&logo=github&labelColor=%2324292e\" /></a>\n    <a href=\"https://github.com/humblepenguinn/mv/releases/latest\" target=\"_blank\"><img alt=\"Release\" src=\"https://img.shields.io/github/v/release/humblepenguinn/mv?sort=semver&style=flat&color=1976d2&labelColor=1e1e1e&logo=github&logoColor=ffffff\"></a>\n</p>\n\nhttps://github.com/user-attachments/assets/86ba198c-ff18-4822-bcad-fcc6bb5b000d\n\n> [!WARNING]\n> MV is in beta and will have bugs and issues\n\n## About\n\nMV is a real-time memory visualization tool designed to help you understand C++ memory management concepts. As you write code, it displays the stack and heap in real-time, making it perfect for learning:\n\n- **Pointers and references** - See how they work in memory\n- **Stack vs Heap** - Visualize the difference between stack and heap allocation\n- **Memory leaks** - Identify and understand memory leaks\n- **Dangling pointers** - See what happens when pointers become invalid\n- **Heap management** - Learn about allocation, deallocation, and memory management\n\n> **Note:** MV currently supports only a small subset of the C++ language, but it's powerful enough to learn many fundamental C++ memory concepts\n\n## Installation\n\nDownload the latest release from [GitHub Releases](https://github.com/humblepenguinn/mv/releases/latest)\n\n- **Linux**: `.deb`, `.rpm`, `.AppImage`\n- **macOS**: `.dmg`, `.tar.gz` (Apple Silicon & Intel)\n- **Windows**: `.exe`, `.msi`\n\nThere is also a web build available [here](https://humblepenguinn.github.io/mv/)\n\n## Contributing\n\nTake a look at the [CONTRIBUTING.md](./CONTRIBUTING.md) file first, then check out the [project roadmap](./docs/roadmap.md)\n\n## License\n\nSee the [LICENSE](./LICENSE) file for license details\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-10-03T02:16:51.884302"
  },
  {
    "basic_info": {
      "name": "pingora_web",
      "full_name": "pingora-web/pingora_web",
      "owner": "pingora-web",
      "description": null,
      "url": "https://github.com/pingora-web/pingora_web",
      "clone_url": "https://github.com/pingora-web/pingora_web.git",
      "ssh_url": "git@github.com:pingora-web/pingora_web.git",
      "homepage": null,
      "created_at": "2025-09-13T18:03:17Z",
      "updated_at": "2025-09-26T01:22:53Z",
      "pushed_at": "2025-09-14T12:25:51Z"
    },
    "stats": {
      "stars": 85,
      "forks": 9,
      "watchers": 85,
      "open_issues": 0,
      "size": 140
    },
    "tech_info": {
      "language": "Rust",
      "languages": {
        "Rust": 82881
      },
      "license": "Apache License 2.0",
      "topics": []
    },
    "content": {
      "readme": "# üöÄ pingora_web\n\n[![CI](https://github.com/pingora-web/pingora_web/actions/workflows/ci.yml/badge.svg)](https://github.com/pingora-web/pingora_web/actions/workflows/ci.yml)\n[![Crates.io](https://img.shields.io/crates/v/pingora_web.svg)](https://crates.io/crates/pingora_web)\n[![Documentation](https://docs.rs/pingora_web/badge.svg)](https://docs.rs/pingora_web)\n[![License](https://img.shields.io/badge/license-MIT%2FApache--2.0-blue.svg)](LICENSE)\n[![Downloads](https://img.shields.io/crates/d/pingora_web.svg)](https://crates.io/crates/pingora_web)\n[![Stars](https://img.shields.io/github/stars/pingora-web/pingora_web.svg)](https://github.com/pingora-web/pingora_web)\n\n**üî• Fast setup | Built on Pingora | Beginner friendly** ü¶Ä\n\n[English](README.md) | [‰∏≠Êñá](README_zh.md)\n\nA web framework built on Cloudflare's Pingora proxy infrastructure, designed to be fast, reliable, and easy to use.\n\n## ‚ú® Features\n\n### Core Features\n- üõ£Ô∏è **Path routing** with parameters (`/users/{id}`)\n- üßÖ **Middleware system** with onion model (like Express.js)\n- üè∑Ô∏è **Request ID tracking** (automatic `x-request-id` header)\n- üìù **Structured logging** with tracing integration\n- üì¶ **JSON support** with automatic serialization\n- üìÅ **Static file serving** with MIME type detection\n- üåä **Streaming responses** for large data transfers\n\n### Built on Pingora\n- ‚ö° **High performance** - leverages Cloudflare's production-tested proxy\n- üóúÔ∏è **HTTP compression** - built-in gzip support\n- üõ°Ô∏è **Request limits** - timeout, body size, and header constraints\n- üö® **Panic recovery** - automatic error handling\n- üîó **HTTP/1.1 & HTTP/2** support via Pingora\n\n## üöÄ Quick Start\n\n### 1. Create a new project\n```bash\ncargo new my_api && cd my_api\n```\n\n### 2. Add dependencies to `Cargo.toml`\n\n**Minimal setup (Hello World):**\n```toml\n[dependencies]\npingora_web = \"0.1\"\n```\n\n**Full setup (with JSON, logging, etc.):**\n```toml\n[dependencies]\npingora_web = \"0.1\"\nserde = { version = \"1.0\", features = [\"derive\"] }\nserde_json = \"1.0\"\ntracing-subscriber = { version = \"0.3\", features = [\"env-filter\"] }\n```\n\n### 3. Hello World (5 lines - like Express/Gin)\n\n```rust\nuse pingora_web::{App, StatusCode, PingoraWebHttpResponse, WebError, PingoraHttpRequest};\n\nfn main() {\n    let mut app = App::default();\n    app.get_fn(\"/\", |_req: PingoraHttpRequest| -> Result<PingoraWebHttpResponse, WebError> {\n        Ok(PingoraWebHttpResponse::text(StatusCode::OK, \"Hello World!\"))\n    });\n    app.listen(\"0.0.0.0:8080\").unwrap();\n}\n```\n\n### 4. With Parameters (beginner-friendly)\n\n```rust\nuse pingora_web::{App, StatusCode, PingoraWebHttpResponse, WebError, PingoraHttpRequest};\n\nfn main() {\n    let mut app = App::default();\n    app.get_fn(\"/\", |_req: PingoraHttpRequest| -> Result<PingoraWebHttpResponse, WebError> {\n        Ok(PingoraWebHttpResponse::text(StatusCode::OK, \"Hello World!\"))\n    });\n    app.get_fn(\"/hi/{name}\", |req: PingoraHttpRequest| -> Result<PingoraWebHttpResponse, WebError> {\n        let name = req.param(\"name\").unwrap_or(\"world\");\n        Ok(PingoraWebHttpResponse::text(StatusCode::OK, format!(\"Hello {}\", name)))\n    });\n    app.listen(\"0.0.0.0:8080\").unwrap();\n}\n```\n\n### 5. Full-featured example\n\n```rust\nuse async_trait::async_trait;\nuse pingora_web::{App, Handler, StatusCode, TracingMiddleware, ResponseCompressionBuilder, WebError, PingoraHttpRequest, PingoraWebHttpResponse};\nuse std::sync::Arc;\n\nstruct Hello;\n#[async_trait]\nimpl Handler for Hello {\n    async fn handle(&self, req: PingoraHttpRequest) -> Result<PingoraWebHttpResponse, WebError> {\n        let name = req.param(\"name\").unwrap_or(\"world\");\n        Ok(PingoraWebHttpResponse::text(StatusCode::OK, format!(\"Hello {}\", name)))\n    }\n}\n\nfn main() {\n    tracing_subscriber::fmt()\n        .with_env_filter(\"info\")\n        .init();\n\n    let mut app = App::default();\n    app.get(\"/hi/{name}\", Arc::new(Hello));\n    app.use_middleware(TracingMiddleware::new());\n    app.add_http_module(ResponseCompressionBuilder::enable(6));\n\n    app.listen(\"0.0.0.0:8080\").unwrap();\n}\n```\n\n### 6. Run the server\n```bash\ncargo run\n```\n\nVisit `http://localhost:8080/` or `http://localhost:8080/hi/world` to see it working!\n\n### Advanced usage (for complex setups)\n\nIf you need more control over the server configuration:\n\n```rust\nuse async_trait::async_trait;\nuse pingora_web::{App, Handler, StatusCode, WebError, PingoraHttpRequest, PingoraWebHttpResponse};\nuse pingora::server::Server;\nuse std::sync::Arc;\n\nstruct Hello;\n#[async_trait]\nimpl Handler for Hello {\n    async fn handle(&self, req: PingoraHttpRequest) -> Result<PingoraWebHttpResponse, WebError> {\n        let name = req.param(\"name\").unwrap_or(\"world\");\n        Ok(PingoraWebHttpResponse::text(StatusCode::OK, format!(\"Hello {}\", name)))\n    }\n}\n\nfn main() {\n    let mut app = App::default();\n    app.get(\"/hi/{name}\", Arc::new(Hello));\n    let app = app;\n\n    // Advanced: Convert to service for more control\n    let mut service = app.to_service(\"my-web-app\");\n    service.add_tcp(\"0.0.0.0:8080\");\n    service.add_tcp",
      "default_branch": "main"
    },
    "fetched_at": "2025-10-03T02:16:53.256132"
  },
  {
    "basic_info": {
      "name": "ghostscan",
      "full_name": "h2337/ghostscan",
      "owner": "h2337",
      "description": "A modern, Rust-powered Linux scanner that unmasks hidden rootkits, stealthy eBPF tricks, and ghost processes in one fast sweep (45+ scanners)",
      "url": "https://github.com/h2337/ghostscan",
      "clone_url": "https://github.com/h2337/ghostscan.git",
      "ssh_url": "git@github.com:h2337/ghostscan.git",
      "homepage": "",
      "created_at": "2025-09-28T05:45:03Z",
      "updated_at": "2025-10-03T02:10:06Z",
      "pushed_at": "2025-09-29T10:51:35Z"
    },
    "stats": {
      "stars": 81,
      "forks": 2,
      "watchers": 81,
      "open_issues": 0,
      "size": 82
    },
    "tech_info": {
      "language": "Rust",
      "languages": {
        "Rust": 189766,
        "C": 3549
      },
      "license": "MIT License",
      "topics": [
        "antivirus",
        "linux-security",
        "malware-detection",
        "rootkit-detection",
        "scanner",
        "security",
        "security-scanner",
        "security-tools"
      ]
    },
    "content": {
      "readme": "# ghostscan\n\nFast one-shot sweep for Linux incident response. Drop the binary on a host, run it once, and collect actionable leads from the kernel, procfs, bpffs, systemd, cron, sockets, and more.\n\n## Quick start\n\n1. Install a current Rust toolchain.\n2. Build with `cargo build --release`.\n3. Copy `target/release/ghostscan` to the target host.\n4. Run as root (or with equivalent capabilities): `sudo ./ghostscan`.\n5. Optional helpers (`bpftool`, `nft`, `ss`, `journalctl`, `auditctl`) expand coverage; when missing, the output explains what was skipped.\n\n## Reading results\n\n- Each scanner prints a bracketed name followed by either findings, `OK`, or an error string.\n- The process always exits with code `0`; treat the log itself as the verdict.\n- Findings are heuristics designed for triage; validate before acting.\n\n## Available scanners\n\n- **Hidden LKM**: compares procfs/sysfs clusters against `kallsyms` to surface hidden modules.\n- **Kernel taint**: highlights taint flags that lack a visible explanation.\n- **Ftrace redirection**: spots risky `ftrace` hooks on critical kernel paths.\n- **Unknown kprobes**: looks for kprobes attached to sensitive symbols that ghostscan cannot explain.\n- **Syscall table integrity**: verifies syscall table pointers for tampering.\n- **Netfilter hook drift**: finds orphaned or invalid netfilter hook jumps.\n- **Module linkage tamper**: checks module list pointers for manipulation.\n- **Ownerless BPF objects**: reports BPF maps/programs without a backing task.\n- **BPF kprobe attachments**: flags kprobes pointed at high-value kernel routines.\n- **BPF LSM**: notes when BPF LSM programs are active.\n- **Detached XDP/TC programs**: detects XDP or TC programs that no longer have an interface.\n- **Sockmap/Sockhash verdicts**: surfaces sockmap/sockhash programs lacking owners.\n- **Sensitive kfunc usage**: tracks invocations of dangerous `kfunc` targets.\n- **Non-bpffs pins**: finds BPF pins created outside bpffs mounts.\n- **Netlink vs proc**: compares netlink inventories with `/proc/net` to expose hidden sockets.\n- **Task list mismatch**: contrasts BPF snapshots with `/proc` task lists to expose hidden PIDs.\n- **Hidden PIDs**: uses BPF-only views to reveal task IDs invisible to `/proc`.\n- **Kernel thread masquerade**: detects kernel threads spoofing user process metadata.\n- **Suspicious ptrace edges**: reports unusual ptrace parent/child relationships.\n- **Deleted or memfd binaries**: lists processes executing from deleted files or memfd mounts.\n- **Hidden listeners**: identifies listeners seen via netlink vs `/proc` vs BPF.\n- **Ownerless sockets**: reports sockets without an owning task.\n- **Netfilter cloaking**: spots tampering patterns that hide netfilter rules.\n- **Local port backdoors**: highlights sockets bound to deleted or temporary paths.\n- **`ld.so.preload` tamper**: inspects `ld.so.preload` for unexpected entries.\n- **Cron ghosts**: checks cron/anacron/at directories for orphaned or cloaked jobs.\n- **Systemd ghosts**: finds unit files pointing to deleted or temporary executables.\n- **SSH footholds**: surfaces dangerous `authorized_keys` options and forced commands.\n- **OverlayFS whiteouts**: reports suspicious opaque or whiteout entries in OverlayFS.\n- **Hidden bind mounts**: lists bind or immutable mounts likely used for concealment.\n- **PAM/NSS modules**: flags PAM or NSS modules loaded from non-system paths.\n- **Live `LD_PRELOAD`**: notes processes still using deleted or writable preload libraries.\n- **Library search hijack**: checks SUID/privileged binaries for unsafe search paths.\n- **`LD_AUDIT` daemons**: finds daemons configured with `LD_AUDIT` despite lacking TTYs.\n- **Large RX regions**: surfaces non-JIT daemons with large anonymous RX memory.\n- **Kernel text RO**: verifies that kernel text sections remain read-only.\n- **`/etc/scripts.d` provenance**: warns on executable scripts from tmp or non-root owners.\n- **Sudoers**: examines sudoers entries for insecure privilege escalation paths.\n- **Kernel cmdline**: alerts on boot parameters that disable audit, lockdown, or IMA.\n- **Sensitive host mounts**: identifies sensitive host paths exposed inside containers.\n- **Host PID namespace**: reports containers sharing the host PID namespace.\n- **Overlay lowerdir**: catches OverlayFS lowerdirs that escape the storage root.\n- **Audit disabled**: detects when auditd is off or dropping records.\n- **Journal gaps**: looks for missing spans in the current boot's journal.\n- **Kernel message suppression**: notices unusual suppression of kernel logs.\n\n## Development pointers\n\n- Format and lint locally with `cargo fmt && cargo check`.\n- New scanners live in `src/scanners/` and expose `pub fn run() -> ScanOutcome` before being registered in `SCANNERS` inside `src/main.rs`.\n\n## Operational notes\n\n- Most modules require elevated privileges to read privileged interfaces, and they report missing access instead of silently failing.\n\n## License\n\nMIT\n",
      "default_branch": "master"
    },
    "fetched_at": "2025-10-03T02:16:54.542820"
  },
  {
    "basic_info": {
      "name": "derive-aliases",
      "full_name": "nik-rev/derive-aliases",
      "owner": "nik-rev",
      "description": "#[derive] aliases",
      "url": "https://github.com/nik-rev/derive-aliases",
      "clone_url": "https://github.com/nik-rev/derive-aliases.git",
      "ssh_url": "git@github.com:nik-rev/derive-aliases.git",
      "homepage": "",
      "created_at": "2025-09-14T22:24:46Z",
      "updated_at": "2025-10-02T19:45:13Z",
      "pushed_at": "2025-10-02T19:45:31Z"
    },
    "stats": {
      "stars": 75,
      "forks": 0,
      "watchers": 75,
      "open_issues": 1,
      "size": 285
    },
    "tech_info": {
      "language": "Rust",
      "languages": {
        "Rust": 99739
      },
      "license": "Apache License 2.0",
      "topics": []
    },
    "content": {
      "readme": "# `derive_aliases`\n\n<!-- cargo-rdme start -->\n\n[![crates.io](https://img.shields.io/crates/v/derive_aliases?style=flat-square&logo=rust)](https://crates.io/crates/derive_aliases)\n[![docs.rs](https://img.shields.io/badge/docs.rs-derive_aliases-blue?style=flat-square&logo=docs.rs)](https://docs.rs/derive_aliases)\n![license](https://img.shields.io/badge/license-Apache--2.0_OR_MIT-blue?style=flat-square)\n![msrv](https://img.shields.io/badge/msrv-1.60-blue?style=flat-square&logo=rust)\n[![github](https://img.shields.io/github/stars/nik-rev/derive-aliases)](https://github.com/nik-rev/derive-aliases)\n\nThis crate improves Rust's `derive` macro by supporting user-defined Derive aliases.\n\n```toml\n[dependencies]\nderive_aliases = \"0.3\"\n```\n\n## Usage\n\nDefine aliases using [`define!`](define), and use them with [`#[derive]`](derive):\n\n```rust\nmod derive_alias {\n    derive_aliases::define! {\n        Eq = ::core::cmp::PartialEq, ::core::cmp::Eq;\n        Ord = ..Eq, ::core::cmp::PartialOrd, ::core::cmp::Ord;\n        Copy = ::core::marker::Copy, ::core::clone::Clone;\n    }\n}\n\nuse derive_aliases::derive;\n\n// Use the aliases:\n#[derive(Debug, ..Ord, ..Copy)]\nstruct User;\n```\n\nThe above expands to this:\n\n```rust\n#[derive(Debug, PartialEq, Eq, PartialOrd, Ord, Copy, Clone)]\nstruct User;\n```\n\n- `#[derive(..Eq)]` expands to `#[derive(::core::cmp::PartialEq, ::core::cmp::Eq)]`\n- `#[derive(..Ord)]` expands to `#[derive(..Eq, ::core::cmp::PartialOrd, ::core::cmp::Ord)]`, which expands to `#[derive(::core::cmp::PartialEq, ::core::cmp::Eq, ::core::cmp::PartialOrd, ::core::cmp::Ord)]`\n\n## IDE Support\n\nHovering over an alias `#[derive(..Alias)]` shows *exactly* what it expands into, and even Goto Definition directly brings you where the alias is defined.\n\n![IDE Support](https://raw.githubusercontent.com/nik-rev/derive-aliases/main/ide_support.png)\n\n## Tip\n\nTo globally override `#[std::derive]` with [`#[derive_aliases::derive]`](derive), add the following:\n\n```rust\n#[macro_use]\nextern crate derive_aliases;\n```\n\nThe above lets you [`define!`](macro@define) aliases and then use them anywhere in your crate!\n\nI have put a **ton** of effort into optimizing `derive_aliases` to be as zero-cost as possible in terms of compile-time over the standard library's `derive`,\nso don't worry about any overhead of `#[derive_aliases::derive]` even when no aliases are used! `derive_aliases` has 0 dependencies (not even `quote` or `syn`!)\n\n## Derives are de-duplicated\n\nEach derive alias expands into a bunch of derives, then de-duplicated. If there are 2 or more of the same derive, only 1 is kept.\nThis is useful when there are some \"pre-requisite\" derives needed, but if they already exist then don't add them (instead of compile error'ing).\n\n```rust\nextern crate zerocopy;\n\nmod derive_alias {\n    derive_aliases::define! {\n        FastHash = ::zerocopy::ByteHash, ::zerocopy::Immutable, ::zerocopy::IntoBytes;\n        FastEq = ::zerocopy::ByteEq, ::zerocopy::Immutable, ::zerocopy::IntoBytes;\n    }\n}\n\n#[derive(..FastHash)]\nstruct Example;\n\n// expands to:\n#[derive(::zerocopy::ByteHash, ::zerocopy::Immutable, ::zerocopy::IntoBytes)]\nstruct Example;\n\n\n\n#[derive(..FastEq)]\nstruct Example;\n\n// expands to:\n#[derive(::zerocopy::ByteEq, ::zerocopy::Immutable, ::zerocopy::IntoBytes)]\nstruct Example;\n\n\n\n#[derive(..FastEq, ..FastHash)]\nstruct Example;\n\n// expands to:\n#[derive(::zerocopy::ByteEq, ::zerocopy::ByteHash, ::zerocopy::Immutable, ::zerocopy::IntoBytes)]\nstruct Example;\n\n// note that the 2 `Immutable` and 2 `IntoBytes` derives were de-duplicated\n```\n\n## Splitting up derive aliases\n\nAll derive aliases must exist at your `crate::derive_alias`, so invoke the `derive_aliases::define!` macro there.\n\nYou can break `define!` apart into multiple definitions:\n\n```rust\nmod derive_alias {\n    mod foo {\n        derive_aliases::define! {\n            Eq = ::core::cmp::Eq, ::core::cmp::PartialEq;\n            Ord = ::core::cmp::PartialOrd, ::core::cmp::Ord, ..Eq;\n        }\n    }\n    mod bar {\n        derive_aliases::define! {\n            Copy = ::core::marker::Copy, ::core::clone::Clone;\n            StdTraits = ..Eq, ..Ord, ..Copy, ::core::fmt::Debug, ::core::hash::Hash;\n        }\n    }\n\n    pub use foo::{Eq, Ord};\n    pub use bar::{Copy, StdTraits};\n}\n\n#[derive(..StdTraits)]\nstruct User;\n```\n\nThe above Just Works. Most importantly, derive aliases need to available at `crate::derive_alias`. This also allows you to share derive aliases across crates\n\n<!-- cargo-rdme end -->\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-10-03T02:16:55.844884"
  }
]