[
  {
    "basic_info": {
      "name": "OmniWorld",
      "full_name": "yangzhou24/OmniWorld",
      "owner": "yangzhou24",
      "description": "OmniWorld: A Multi-Domain and Multi-Modal Dataset for 4D World Modeling",
      "url": "https://github.com/yangzhou24/OmniWorld",
      "clone_url": "https://github.com/yangzhou24/OmniWorld.git",
      "ssh_url": "git@github.com:yangzhou24/OmniWorld.git",
      "homepage": "https://yangzhou24.github.io/OmniWorld/",
      "created_at": "2025-09-15T13:11:18Z",
      "updated_at": "2025-09-16T17:57:33Z",
      "pushed_at": "2025-09-16T05:02:35Z"
    },
    "stats": {
      "stars": 162,
      "forks": 1,
      "watchers": 162,
      "open_issues": 1,
      "size": 11541
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 12580,
        "Shell": 1242
      },
      "license": null,
      "topics": []
    },
    "content": {
      "readme": "<h1 align='center'>OmniWorld: A Multi-Domain and Multi-Modal Dataset for 4D World Modeling</h1>\n<div align='center'>\n    <a href='https://github.com/yangzhou24' target='_blank'>Yang Zhou</a><sup>1</sup>‚ÄÉ\n    <a href='https://github.com/yyfz' target='_blank'>Yifan Wang</a><sup>1</sup>‚ÄÉ\n    <a href='https://zhoutimemachine.github.io' target='_blank'>Jianjun Zhou</a><sup>1,2</sup>‚ÄÉ\n    <a href='https://github.com/AmberHeart' target='_blank'>Wenzheng Chang</a><sup>1</sup>‚ÄÉ\n    <a href='https://github.com/ghy0324' target='_blank'>Haoyu Guo</a><sup>1</sup>‚ÄÉ\n    <a href='https://github.com/LiZizun' target='_blank'>Zizun Li</a><sup>1</sup>‚ÄÉ\n    <a href='https://kaijing.space/' target='_blank'>Kaijing Ma</a><sup>1</sup>‚ÄÉ\n    \n</div>\n<div align='center'>\n<a href='https://scholar.google.com/citations?user=VuTRUg8AAAAJ' target='_blank'>Xinyue Li</a><sup>1</sup>‚ÄÉ\n    <a href='https://scholar.google.com/citations?user=5SuBWh0AAAAJ' target='_blank'>Yating Wang</a><sup>1</sup>‚ÄÉ\n    <a href='https://www.haoyizhu.site/' target='_blank'>Haoyi Zhu</a><sup>1</sup>‚ÄÉ\n    <a href='https://mingyulau.github.io/' target='_blank'>Mingyu Liu</a><sup>1,2</sup>‚ÄÉ\n    <a href='https://scholar.google.com/citations?user=FbSpETgAAAAJ' target='_blank'>Dingning Liu</a><sup>1</sup>‚ÄÉ\n    <a href='https://yangjiangeyjg.github.io/' target='_blank'>Jiange Yang</a><sup>1</sup>\n    <a href='https://github.com/Kr1sJFU' target='_blank'>Zhoujie Fu</a><sup>1</sup>‚ÄÉ‚ÄÉ\n    \n</div>\n<div align='center'>\n    <a href='https://sotamak1r.github.io/' target='_blank'>Junyi Chen</a><sup>1</sup>‚ÄÉ\n    <a href='https://cshen.github.io' target='_blank'>Chunhua Shen</a><sup>2</sup>‚ÄÉ\n    <a href='https://oceanpang.github.io' target='_blank'>Jiangmiao Pang</a><sup>1</sup>‚ÄÉ\n    <a href='https://kpzhang93.github.io/' target='_blank'>Kaipeng Zhang</a><sup>1</sup>\n    <a href='https://tonghe90.github.io/' target='_blank'>Tong He</a><sup>1‚Ä†</sup>\n</div>\n<div align='center'>\n    <sup>1</sup>Shanghai AI Lab‚ÄÉ <sup>2</sup>ZJU‚ÄÉ\n</div>\n<br>\n<div align=\"center\">\n  <a href=\"https://yangzhou24.github.io/OmniWorld/\"><img src=\"https://img.shields.io/badge/Project Page-5745BB?logo=google-chrome&logoColor=white\"></a> ‚ÄÇ\n  <a href=\"https://arxiv.org/abs/2509.12201\"><img src=\"https://img.shields.io/static/v1?label=Paper&message=Arxiv&color=red&logo=arxiv\"></a> ‚ÄÇ\n  <a href=\"https://github.com/yangzhou24/OmniWorld\"><img src=\"https://img.shields.io/static/v1?label=Code&message=Github&color=blue&logo=github\"></a> ‚ÄÇ\n  <a href=\"https://huggingface.co/datasets/InternRobotics/OmniWorld\"><img src=\"https://img.shields.io/static/v1?label=Dataset&message=HuggingFace&color=yellow&logo=huggingface\"></a> ‚ÄÇ\n</div>\n\n<img src=\"assets/teaser.png\" width=\"1000px\">\n\n## üéâ NEWS\n- [2025.9.16] üî• The first 1.2k splits release of **OmniWorld-Game** is now live on Hugging Face! **More data is coming soon, stay tuned!**\n\n## ‚ú® Overview\n\nOmniWorld is a large-scale, multi-domain, and multi-modal dataset specifically designed for üåç**4D world modeling**, e.g. 4D geometric reconstruction, future prediction & camera-controlled video generation.\n\n### üîë Key Features\n\n- üìä **Massive Scale**: 4000+ hours, 600K+ sequences, 300M+ frames\n- ü§ñ **Diverse Domains**: sourced from simulartor, robot, human & the Internet\n- üé® **Rich Multi-Modality**: depth maps, camera poses, text captions, optical flow & foreground mask\n\n### üéÆ Introducing _OmniWorld-Game_\n\n_OmniWorld-Game_ is a newly collected high-quality synthetic subset of the main _OmniWorld_ dataset. It features:\n\n- üìä **Scale**: 214 hours, 96K video clips, 18M+ frames\n- üß© **Resolution & Diversity**: 720P RGB image capatured from a wide range of dynamic game environments\n- üé® **Comprehensive Annotations**: cover all annotation types of the _OmniWorld_ dataset\n\n### üèÜ _OmniWorld-Game_ Benchmark\n\n_OmniWorld-Game_ Benchmark offers 4D world modeling evaluation for 3D Geometric Prediction &\nCamera Control Video Generation. Found: \n\n- üö´ Current state-of-the-art approaches **still show great limitations** in modeling complex 4D environments, based on both quantitative metrics and qualitative results.\n- üìà **Fine-tuning** existing SOTA methods on _OmniWorld_ leads to **significant performance gains** across 4D reconstruction and video generation tasks, highlighting the value of our dataset.\n\n\n## üí° Dataset Download\nYou can download the entire OmniWorld dataset using the following command:\n```bash\n# 1. Install (if you haven't yet)\npip install --upgrade \"huggingface_hub[cli]\"\n\n# 2. Full download\nhf download InternRobotics/OmniWorld \\\n           --repo-type dataset \\\n           --local-dir /path/to/DATA_PATH\n```\nFor downloading specific files (instead of the full dataset), please refer to the [`dowanload_specific.py`](scripts/dowanload_specific.py).\n\n\n\n## üöÄ Visualize as Point Cloud\n\nThis script allows you to convert a scene from the dataset into a 3D point cloud for inspection.\n\n### 1\\. Prerequisites\n\nPlease follow the instructions in the \"Dataset Download\" section to acquire the dataset.\n\n### 2\\. Data Str",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-16T18:45:19.765609"
  },
  {
    "basic_info": {
      "name": "provenance-action",
      "full_name": "danielroe/provenance-action",
      "owner": "danielroe",
      "description": "GitHub Action that detects dependency provenance downgrades from lockfile changes (npm/pnpm/yarn).",
      "url": "https://github.com/danielroe/provenance-action",
      "clone_url": "https://github.com/danielroe/provenance-action.git",
      "ssh_url": "git@github.com:danielroe/provenance-action.git",
      "homepage": "",
      "created_at": "2025-09-16T11:08:14Z",
      "updated_at": "2025-09-16T18:29:36Z",
      "pushed_at": "2025-09-16T17:33:08Z"
    },
    "stats": {
      "stars": 122,
      "forks": 3,
      "watchers": 122,
      "open_issues": 2,
      "size": 79
    },
    "tech_info": {
      "language": "TypeScript",
      "languages": {
        "TypeScript": 40786
      },
      "license": null,
      "topics": []
    },
    "content": {
      "readme": "# `danielroe/provenance-action`\n\nDetect and fail CI when dependencies in your lockfile lose npm provenance or trusted publisher status.\n\n> [!WARNING]\n> This action is under active development and is only one tool to assist in securing your dependencies.\n\n## ‚ú® Features\n- `pnpm-lock.yaml`, `package-lock.json`, `yarn.lock` (v1 and v2+)\n- Handles transitives by comparing resolved versions\n- Inline GitHub annotations at the lockfile line\n- JSON output and optional hard‚Äëfail (default: on)\n- Pure TypeScript, Node 24+\n\nüëâ See it in action: [danielroe/provenance-action-test](https://github.com/danielroe/provenance-action-test)\n\n## üöÄ Quick start\n```yaml\nname: ci\non:\n  pull_request:\n    branches:\n      - main\npermissions:\n  contents: read\njobs:\n  check-provenance:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n        with:\n          fetch-depth: 0\n      - name: Check provenance downgrades\n        uses: danielroe/provenance-action@main\n        id: check\n        with:\n          fail-on-provenance-change: true # optional, default: false\n        #   lockfile: pnpm-lock.yaml      # optional\n        #   base-ref: origin/main         # optional, default: origin/main\n        #   fail-on-downgrade: true       # optional, default: true\n      - name: Print result\n        run: \"echo 'Downgraded: ${{ steps.check.outputs.downgraded }}'\"\n```\n\n## üîß Inputs\n- `lockfile` (optional): Path to the lockfile. Auto-detected if omitted.\n- `workspace-path` (optional): Path to workspace root. Default: `.`\n- `base-ref` (optional): Git ref to compare against. Default: `origin/main`.\n- `fail-on-downgrade` (optional): Controls failure behavior. Accepts `true`, `false`, `any`, or `only-provenance-loss`. Default: `true` (which is the same as `any`).\n- `fail-on-provenance-change` (optional): When `true`, fail on provenance repository/branch changes. Default: `false`.\n\n## üì§ Outputs\n- `downgraded`: JSON array of `{ name, from, to, downgradeType }` for detected downgrades. `downgradeType` is `provenance` or `trusted_publisher`.\n- `changed`: JSON array of provenance change events `{ name, from, to, type, previousRepository?, newRepository?, previousBranch?, newBranch? }`.\n\n## üß† How it works\n1. Diffs your lockfile against the base ref and collects changed resolved versions (including transitives).\n2. Checks npm provenance via the attestations API for each `name@version`.\n3. Falls back to version metadata for `dist.attestations`.\n4. Emits file+line annotations in the lockfile.\n5. If provenance exists for both the previous and new version, extracts GitHub `owner/repo` and branch from attestations and warns when they differ (repo changed or branch changed).\n\n## üîí Why this matters\nTrusted publishing links a package back to its source repo and build workflow, providing strong provenance guarantees. It helps ensure the package you install corresponds to audited source and CI.\n\nHowever, maintainers can still be phished or coerced into publishing without trusted publishing enabled, or switching to a non‚Äëtrusted path. In those cases, packages may still carry attestations, but the chain back to the trusted publisher can be weakened.\n\nThis action:\n- Detects when a dependency update loses npm provenance (no attestations) or loses trusted publisher (attestations but no trusted publisher marker), and\n- Fails CI by default (configurable), before that change lands in your main branch.\n\nThis is a stopgap until package managers enforce stronger policies natively. Until then, it offers a lightweight guardrail in CI.\n\n## ‚ö†Ô∏è Notes\n- Runs on Node 24+ and executes the TypeScript entrypoint directly.\n- Bun (`bun.lockb`) is not yet supported.\n - Repository and branch change detection is best‚Äëeffort; attestation shapes vary and some packages omit repo/ref details.\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-16T18:45:21.069848"
  },
  {
    "basic_info": {
      "name": "dsts",
      "full_name": "current-ai-llc/dsts",
      "owner": "current-ai-llc",
      "description": null,
      "url": "https://github.com/current-ai-llc/dsts",
      "clone_url": "https://github.com/current-ai-llc/dsts.git",
      "ssh_url": "git@github.com:current-ai-llc/dsts.git",
      "homepage": null,
      "created_at": "2025-09-15T00:03:28Z",
      "updated_at": "2025-09-16T17:13:41Z",
      "pushed_at": "2025-09-15T16:15:49Z"
    },
    "stats": {
      "stars": 54,
      "forks": 1,
      "watchers": 54,
      "open_issues": 1,
      "size": 52
    },
    "tech_info": {
      "language": "TypeScript",
      "languages": {
        "TypeScript": 51252
      },
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "# DSTS ‚Äî Dynamic Self‚Äëimproving TypeScript\n\nDSTS is a minimal, AI SDK‚Äìaligned prompt optimizer for TypeScript. It optimizes prompts for both generateObject (with Zod schemas) and generateText with the latest GEPA optimizer (soft G like ‚Äúgiraffe‚Äù). GEPA evolves a prompt along a Pareto frontier across multiple objectives that matter in practice: task performance, latency, and cost.\n\n- AI Gateway‚Äìaligned: pass model ids as strings; generateObject with a Zod schema for a structured object or generateText with a string.\n- Minimal abstractions: one default adapter to call `generateObject`/`generateText` and one optimizer.\n- Multi‚Äëobjective first: correctness + latency (and cost) tracked per iteration; Pareto front and hyper‚Äëvolume (2D) reported.\n- Persistence & budgets: checkpoint/resume, per‚Äëcall cost estimation (via tokenlens) and budget caps, seeded minibatching.\n\n## Install\n\n```bash\nnpm i @currentai/dsts zod\n```\n\n## Quick start\n\n```ts\nimport { z } from \"zod\";\nimport { optimize, DefaultAdapterTask } from \"@currentai/dsts\";\n\n// Define schema (generateObject)\nconst Item = z.object({ title: z.string(), url: z.string().url() });\n\n// Training data: use schema ‚áí generateObject; provide expectedOutput and/or a scorer\nconst trainset: DefaultAdapterTask<z.infer<typeof Item>>[] = [\n  {\n    input: \"link to TS docs\",\n    expectedOutput: {\n      title: \"TypeScript\",\n      url: \"https://www.typescriptlang.org\",\n    },\n    schema: Item,\n  },\n];\n\nconst result = await optimize({\n  seedCandidate: { system: \"Extract a title and a valid URL from the text.\" },\n  trainset,\n  // Optional valset (defaults to trainset)\n  taskLM: \"openai/gpt-5-nano\",\n  reflectionLM: \"openai/o3\",\n  maxIterations: 5,\n  maxMetricCalls: 200,\n  maxBudgetUSD: 50,\n  reflectionMinibatchSize: 3,\n  candidateSelectionStrategy: \"pareto\",\n  componentSelector: \"round_robin\",\n  logger: {\n    log: (lvl, msg, data) => {\n      if (lvl === \"info\") console.log(`[${lvl}] ${msg}`, data || \"\");\n    },\n  },\n  persistence: {\n    dir: \"runs/quickstart\",\n    checkpointEveryIterations: 1,\n    resume: true,\n  },\n});\n\nconsole.log(\"Best system prompt:\", result.bestCandidate.system);\n```\n\n## Examples\n\n- Email extraction to a rich object schema: [`examples/email-extraction.ts`](file:///home/swiecki/coding/dsts/examples/email-extraction.ts)\n- Message spam classification: [`examples/message-spam.ts`](file:///home/swiecki/coding/dsts/examples/message-spam.ts)\n\nEach example:\n\n- Loads `.env` locally (AI Gateway by default),\n- Prints total iterations, metric calls, cost (USD), and duration (ms),\n- Enables persistence to `runs/...`.\n\nRun:\n\n```bash\nnpm run example              # email extraction\nnpm run example:message-spam # spam classification\n```\n\n## How it works\n\n- Default adapter decides generateObject vs generateText based on `schema` presence in each task; collects per‚Äëinstance scores, latency_ms, and cost_usd (via tokenlens when usage is available).\n- GEPA optimizer maintains a candidate archive, runs minibatch reflection, and accepts improving children. It computes per‚Äëcandidate metrics:\n  - correctness = average(score[])\n  - latency = ‚àíavg(latency_ms) (stored negative so higher is better)\n  - cost is tracked cumulatively and enforced via `maxBudgetUSD`.\n- Pareto front and 2D hyper‚Äëvolume (when exactly two objectives) are logged per iteration and at the end.\n\nKey files:\n\n- Optimizer: [`src/gepa.ts`](file:///home/swiecki/coding/dsts/src/gepa.ts#L1-L495)\n- Adapter: [`src/adapters/default-adapter.ts`](file:///home/swiecki/coding/dsts/src/adapters/default-adapter.ts#L1-L267) (default maxConcurrency = 10)\n- Pareto utilities: [`src/pareto-utils.ts`](file:///home/swiecki/coding/dsts/src/pareto-utils.ts#L1-L241)\n- Types: [`src/types.ts`](file:///home/swiecki/coding/dsts/src/types.ts#L1-L158)\n- Persistence: [`src/persistence.ts`](file:///home/swiecki/coding/dsts/src/persistence.ts#L1-L63)\n\n## Design choices\n\n- No custom LLM classes: pass model ids as strings (Gateway format, e.g., `openai/gpt-5-nano`). The adapter uses AI SDK directly.\n- Minimal knobs: set budgets (`maxMetricCalls`, `maxBudgetUSD`), minibatch size, and selectors. Concurrency defaults to 10.\n- Multi‚Äëobjective by default: we optimise for correctness and latency together; add cost as an explicit objective later if desired.\n\n## Environment\n\n- AI Gateway by default. Set `AI_GATEWAY_API_KEY` in `.env` or export it in your shell.\n- If you prefer provider‚Äëdirect, swap to `@ai-sdk/openai` models and pass model objects; the adapter will forward them.\n\n## Roadmap\n\n- Centralized eval helper for exact metric‚Äëcall counting and pre‚Äëcall budget gates.\n- Parent minibatch result reuse to avoid duplicate evaluations.\n- Extend hyper‚Äëvolume and objectives (e.g., cost as a third dimension) with explicit reference points.\n- Reflection concurrency (optional) and parent/child evaluation parallelism.\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-16T18:45:22.363141"
  },
  {
    "basic_info": {
      "name": "AI-Outfit-Change",
      "full_name": "AI-Banana/AI-Outfit-Change",
      "owner": "AI-Banana",
      "description": "Ever wish you could hit 'remove layer' in Real Life? Same. At least your Pics can do it Now.  https://tinyurl.com/removerapp Go Test It Out!",
      "url": "https://github.com/AI-Banana/AI-Outfit-Change",
      "clone_url": "https://github.com/AI-Banana/AI-Outfit-Change.git",
      "ssh_url": "git@github.com:AI-Banana/AI-Outfit-Change.git",
      "homepage": "",
      "created_at": "2025-09-15T13:30:02Z",
      "updated_at": "2025-09-16T18:42:38Z",
      "pushed_at": "2025-09-15T13:31:47Z"
    },
    "stats": {
      "stars": 49,
      "forks": 0,
      "watchers": 49,
      "open_issues": 0,
      "size": 0
    },
    "tech_info": {
      "language": null,
      "languages": {},
      "license": null,
      "topics": [
        "ai-nude",
        "ai-nude-generator",
        "ai-nudes"
      ]
    },
    "content": {
      "readme": "# [Test It Free!](https://tinyurl.com/removerapp)\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-16T18:45:23.680246"
  },
  {
    "basic_info": {
      "name": "MediLogic",
      "full_name": "praneethavaranasi/MediLogic",
      "owner": "praneethavaranasi",
      "description": "Doctor Consultation Platform",
      "url": "https://github.com/praneethavaranasi/MediLogic",
      "clone_url": "https://github.com/praneethavaranasi/MediLogic.git",
      "ssh_url": "git@github.com:praneethavaranasi/MediLogic.git",
      "homepage": null,
      "created_at": "2025-09-15T13:40:05Z",
      "updated_at": "2025-09-16T03:36:38Z",
      "pushed_at": "2025-09-15T14:34:13Z"
    },
    "stats": {
      "stars": 48,
      "forks": 23,
      "watchers": 48,
      "open_issues": 0,
      "size": 14322
    },
    "tech_info": {
      "language": "JavaScript",
      "languages": {
        "JavaScript": 96909,
        "CSS": 5804
      },
      "license": null,
      "topics": []
    },
    "content": {
      "readme": "***\n\n# Healthcare Website - At Home Guidance and Support (MEDILOGIC)\n\n## Project Overview\n\nMillions face difficulty accessing timely healthcare guidance for non-emergency symptoms, ongoing wellness, or medication management. This project introduces a **virtual healthcare assistant** designed to deliver support and guidance conveniently at home, aiming to enhance health outcomes and promote self-care.[^1]\n\n## Problem Statement\n\n- Long waiting times for medical consultations cause delays and inconvenience.[^1]\n- Healthcare is often inaccessible in remote or under-served areas, risking poor health outcomes for vulnerable populations.[^2][^4]\n- There is a growing need for easy-to-use, preventive tools for early symptom checking, medication reminders, and general health advice.\n\n\n## Objectives\n\n- Provide preliminary medical guidance and symptom assessment from home.\n- Reduce avoidable hospital visits for routine inquiries.\n- Offer medication reminders, wellness tips, and health monitoring.\n- Enable remote consultations and easy access to health information using video conferencing and seamless communication.\n\n\n## Key Features\n\n- Symptom checker and actionable health advice.\n- Medication reminders and tracking.\n- Personalized health tips, diet, and wellness recommendations.\n- Integration with wearable devices for automated health monitoring.\n- Emergency alerts and contact options for nearby medical facilities.\n- Secure messaging with healthcare professionals for remote consultation.\n\n\n## Technology Stack\n\n- Backend:Node.js,Next.js,Zod,React Hook form,pdf js and under development\n- Frontend: React,ShadCn UI, Acternity UI\n- Database: NeonDB,Prisma\n- Deployment:netlify.com\n- Video & Chat SDK: Still in development\n\n## Use Cases\n\n- A patient enters symptoms to receive preliminary advice and next steps.\n- Users set up daily medication reminders and wellness notifications.\n- Remote appointment booking or instant emergency alerts.\n- Integration with fitness/wearable devices for continuous monitoring.\n\n\n## Future Scope\n\n- Advanced AI-based diagnosis support.\n- Multilingual and region-specific health guidance.\n- Secure video/audio consultation with specialists.\n- Enhanced data security, privacy, and compliance with health data regulations.\n\n\n## Contributors\n\n- [KARTHIKEYA CHUNDURU]\n- [PRANEETHA VARANASI]\n  \nTEAM MEMBERS\n- [SUNAND CHOUDARY V]\n- [SAI SRIHITHA PICHIKA]\n- [GAURANG P KHASNE]\n- [VARANASI ADITYA]\n\n\n\n\n\nThis is a [Next.js](https://nextjs.org) project bootstrapped with [`create-next-app`](https://github.com/vercel/next.js/tree/canary/packages/create-next-app).\n\n## Getting Started\n\nFirst, run the development server:\n\n```bash\nnpm run dev\n# or\nyarn dev\n# or\npnpm dev\n# or\nbun dev\n```\n\nOpen [http://localhost:3000](http://localhost:3000) with your browser to see the result.\n\nYou can start editing the page by modifying `app/page.js`. The page auto-updates as you edit the file.\n\nThis project uses [`next/font`](https://nextjs.org/docs/app/building-your-application/optimizing/fonts) to automatically optimize and load [Geist](https://vercel.com/font), a new font family for Vercel.\n\n## Learn More\n\nTo learn more about Next.js, take a look at the following resources:\n\n- [Next.js Documentation](https://nextjs.org/docs) - learn about Next.js features and API.\n- [Learn Next.js](https://nextjs.org/learn) - an interactive Next.js tutorial.\n\nYou can check out [the Next.js GitHub repository](https://github.com/vercel/next.js) - your feedback and contributions are welcome!\n\n## Deploy on Vercel\n\nThe easiest way to deploy your Next.js app is to use the [Vercel Platform](https://vercel.com/new?utm_medium=default-template&filter=next.js&utm_source=create-next-app&utm_campaign=create-next-app-readme) from the creators of Next.js.\n\nCheck out our [Next.js deployment documentation](https://nextjs.org/docs/app/building-your-application/deploying) for more details.\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-16T18:45:24.989758"
  },
  {
    "basic_info": {
      "name": "VibeVoice-finetuning",
      "full_name": "voicepowered-ai/VibeVoice-finetuning",
      "owner": "voicepowered-ai",
      "description": "Unofficial WIP LoRa Finetuning repository for VibeVoice",
      "url": "https://github.com/voicepowered-ai/VibeVoice-finetuning",
      "clone_url": "https://github.com/voicepowered-ai/VibeVoice-finetuning.git",
      "ssh_url": "git@github.com:voicepowered-ai/VibeVoice-finetuning.git",
      "homepage": null,
      "created_at": "2025-09-16T10:57:57Z",
      "updated_at": "2025-09-16T18:35:10Z",
      "pushed_at": "2025-09-16T16:31:47Z"
    },
    "stats": {
      "stars": 47,
      "forks": 9,
      "watchers": 47,
      "open_issues": 1,
      "size": 189
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 312954
      },
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "\r\n  \r\n\r\n# Unofficial WIP Finetuning repo for VibeVoice\r\n\r\n  \r\n\r\n# Hardware requirements\r\n\r\n  \r\n\r\nTo train a VibeVoice 1.5B LoRa, a machine with at least 16gb VRAM is recommended.\r\n\r\nTo train a VibeVoice 7B LoRa, a machine with at least 48gb VRAM is recommended.\r\n\r\nKeep in mind longer audios increase VRAM requirements\r\n\r\n  \r\n\r\n# Installation\r\n\r\nIt is recommended to install this in a fresh environment. Specifically, the Dockerized environment `runpod/pytorch:2.8.0-py3.11-cuda12.8.1-cudnn-devel-ubuntu22.04` has been tested to work.\r\n\r\n  \r\n\r\nTransformers version 4.51.3 is known to work, while other versions have errors related to Qwen2 architecture.\r\n\r\n  \r\n```\r\ngit clone https://github.com/voicepowered-ai/VibeVoice-finetuning\r\n\r\npip install -e .\r\n\r\npip uninstall -y transformers && pip install transformers==4.51.3\r\n\r\n(OPTIONAL) wandb login\r\n\r\n(OPTIONAL) export HF_HOME=/workspace/hf_models\r\n```\r\n\r\n  \r\n\r\n# Usage\r\n\r\n  \r\n\r\n## VibeVoice 1.5B / 7B (LoRA) fine-tuning\r\n\r\n  \r\n\r\n  \r\n\r\nWe put some code together for training VibeVoice (7B) with LoRA. This uses the vendored VibeVoice model/processor and trains with a dual loss: masked CE on text tokens plus diffusion MSE on acoustic latents.\r\n\r\n  \r\n\r\n  \r\n\r\nRequirements:\r\n\r\n  \r\n\r\n- Download a compatible VibeVoice 7B or 1.5b checkpoint (config + weights) and its processor files (preprocessor_config.json) or run straight from HF model.\r\n\r\n- A 24khz audio dataset with audio files (target audio), text prompts (transcriptions) and optionally voice prompts (reference audio)\r\n\r\n  \r\n\r\n  \r\n  \r\n\r\n### Training with Hugging Face Dataset\r\n\r\n  \r\n```\r\npython -m src.finetune_vibevoice_lora \\\r\n\r\n--model_name_or_path aoi-ot/VibeVoice-Large \\\r\n\r\n--processor_name_or_path src/vibevoice/processor \\\r\n\r\n--dataset_name your/dataset \\\r\n\r\n--text_column_name text \\\r\n\r\n--audio_column_name audio \\\r\n\r\n--voice_prompts_column_name voice_prompts \\\r\n\r\n--output_dir outputTrain3 \\\r\n\r\n--per_device_train_batch_size 8 \\\r\n\r\n--gradient_accumulation_steps 16 \\\r\n\r\n--learning_rate 2.5e-5 \\\r\n\r\n--num_train_epochs 5 \\\r\n\r\n--logging_steps 10 \\\r\n\r\n--save_steps 100 \\\r\n\r\n--eval_steps 100 \\\r\n\r\n--report_to wandb \\\r\n\r\n--remove_unused_columns False \\\r\n\r\n--bf16 True \\\r\n\r\n--do_train \\\r\n\r\n--gradient_clipping \\\r\n\r\n--gradient_checkpointing False \\\r\n\r\n--ddpm_batch_mul 4 \\\r\n\r\n--diffusion_loss_weight 1.4 \\\r\n\r\n--train_diffusion_head True \\\r\n\r\n--ce_loss_weight 0.04 \\\r\n\r\n--voice_prompt_drop_rate 0.2 \\\r\n\r\n--lora_target_modules q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj \\\r\n\r\n--lr_scheduler_type cosine \\\r\n\r\n--warmup_ratio 0.03 \\\r\n\r\n--max_grad_norm 0.8\r\n```\r\n  \r\n\r\n----------\r\n\r\n  \r\n\r\n### Training with Local JSONL Dataset\r\n\r\n  \r\n```\r\npython -m src.finetune_vibevoice_lora \\\r\n\r\n--model_name_or_path aoi-ot/VibeVoice-Large \\\r\n\r\n--processor_name_or_path src/vibevoice/processor \\\r\n\r\n--train_jsonl prompts.jsonl \\\r\n\r\n--text_column_name text \\\r\n\r\n--audio_column_name audio \\\r\n\r\n--output_dir outputTrain3 \\\r\n\r\n--per_device_train_batch_size 8 \\\r\n\r\n--gradient_accumulation_steps 16 \\\r\n\r\n--learning_rate 2.5e-5 \\\r\n\r\n--num_train_epochs 5 \\\r\n\r\n--logging_steps 10 \\\r\n\r\n--save_steps 100 \\\r\n\r\n--report_to wandb \\\r\n\r\n--remove_unused_columns False \\\r\n\r\n--bf16 True \\\r\n\r\n--do_train \\\r\n\r\n--gradient_clipping \\\r\n\r\n--gradient_checkpointing False \\\r\n\r\n--ddpm_batch_mul 4 \\\r\n\r\n--diffusion_loss_weight 1.4 \\\r\n\r\n--train_diffusion_head True \\\r\n\r\n--ce_loss_weight 0.04 \\\r\n\r\n--voice_prompt_drop_rate 0.2 \\\r\n\r\n--lora_target_modules q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj \\\r\n\r\n--lr_scheduler_type cosine \\\r\n\r\n--warmup_ratio 0.03 \\\r\n\r\n--max_grad_norm 0.8\r\n```\r\n\r\n\r\n### JSONL format:\r\n\r\n  \r\n\r\nYou can provide an optional `voice_prompts` key. If it is omitted, a voice prompt will be automatically generated from the target audio.\r\n\r\n  \r\n\r\n**Example without a pre-defined voice prompt (will be auto-generated):**\r\n\r\n`{\"text\": \"Speaker 0: Speaker0 transcription.\", \"audio\": \"/workspace/wavs/segment_000000.wav\"}`\r\n\r\n  \r\n\r\n**Example with a pre-defined voice prompt:**\r\n\r\n`{\"text\": \"Speaker 0: Speaker0 transcription.\", \"audio\": \"/workspace/wavs/segment_000000.wav\", \"voice_prompts\": \"/path/to/a/different/prompt.wav\"}`\r\n\r\n  \r\n\r\n**Example with multiple speakers and voice prompts:**\r\n\r\n`{\"text\": \"Speaker 0: How is the project coming along?\\nSpeaker 1: It's going well, we should be finished by Friday.\", \"audio\": \"/data/conversations/convo_01.wav\", \"voice_prompts\": [\"/data/prompts/alice_voice_prompt.wav\", \"/data/prompts/bob_voice_prompt.wav\"]}`\r\n\r\n  \r\n  \r\n  \r\n\r\n# Notes:\r\n\r\n  \r\n\r\n- Audio is assumed to be 24 kHz; input audio will be loaded/resampled to 24 kHz.\r\n\r\n  \r\n\r\n- If you pass raw NumPy arrays or torch Tensors as audio (without sampling rate metadata), the collator assumes they are already 24 kHz. To trigger resampling, provide dicts like {\"array\": <np.ndarray>, \"sampling_rate\": <int>} or file paths.\r\n\r\n  \r\n\r\n- Tokenizers (acoustic/semantic) are frozen by default. LoRA is applied to the LLM (Qwen) and optionally to the diffusion head.\r\n\r\n  \r\n\r\n- The collator builds interleaved seq",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-16T18:45:26.246270"
  },
  {
    "basic_info": {
      "name": "simkit",
      "full_name": "Fallomai/simkit",
      "owner": "Fallomai",
      "description": null,
      "url": "https://github.com/Fallomai/simkit",
      "clone_url": "https://github.com/Fallomai/simkit.git",
      "ssh_url": "git@github.com:Fallomai/simkit.git",
      "homepage": null,
      "created_at": "2025-09-15T19:50:12Z",
      "updated_at": "2025-09-16T18:24:35Z",
      "pushed_at": "2025-09-15T21:07:25Z"
    },
    "stats": {
      "stars": 42,
      "forks": 1,
      "watchers": 42,
      "open_issues": 0,
      "size": 45
    },
    "tech_info": {
      "language": "TypeScript",
      "languages": {
        "TypeScript": 12375
      },
      "license": null,
      "topics": []
    },
    "content": {
      "readme": "# SimKit\n\n[![npm version](https://badge.fury.io/js/%40fallom%2Fsimkit.svg)](https://www.npmjs.com/package/@fallom/simkit)\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n[![TypeScript](https://img.shields.io/badge/TypeScript-007ACC?style=flat&logo=typescript&logoColor=white)](https://www.typescriptlang.org/)\n[![Downloads](https://img.shields.io/npm/dm/@fallom/simkit.svg)](https://www.npmjs.com/package/@fallom/simkit)\n\n> ü§ñ **A TypeScript simulation framework for testing and running AI agents**\n\n## What is SimKit?\n\nSimKit lets you build, test, and run AI agents in your own custom simulated environments. It gives you a simple game loop for running agents step-by-step, supports multiple agents, and includes built-in tools (OTEL) for tracking what happens during your simulations.\n\n### Agent Agnostic & No Vendor Lock-in\n\nSimKit works with any AI agent or LLM, no lock-in. Use your own models and run everything locally. OTEL logs can be saved to a local file or sent to a remote server.\n\n### Why Use Simulations?\n\nSimulations let you see how your AI agents perform on real world tasks, step by step, in a safe and controlled way.\n\nTraditional evals are great for simple tasks, but they don't give you the full picture. You can't see how your agents handle:\n\n- üéØ Multi-step tasks that need planning and memory\n- üõ†Ô∏è Lots of different tools and actions\n- üåç Realistic data and changing situations\n- ‚ö° Decisions that matter over time\n- üîÑ Long-term planning and decision-making\n- üìö Processing and reasoning over large amounts of context and information\n\nSurprisingly, most AI agents begin to fail when they are asked to do anything more than a few simple tasks.\n\n## üîÑ Core: The Simulation Loop\n\nSimKit's heart is a simple but powerful tick-based loop:\n\n```typescript\nimport { createSimulation, type LoopState } from \"@fallom/simkit/simulation\";\n\ninterface SupportTestState extends LoopState {\n  totalIssues: number;\n  resolvedIssues: number;\n  averageResponseTime: number;\n  satisfactionScores: number[];\n}\n\nconst customerIssues = [\n  \"My account is locked and I can't access my files\",\n  \"Billing error - charged twice for same month\", \n  \"App crashes every time I try to upload\",\n  \"Can't find my downloaded files anywhere\"\n];\n\nconst simulation = createSimulation<SupportTestState>({\n  maxTicks: 10,\n  initialState: { totalIssues: 0, resolvedIssues: 0, averageResponseTime: 0, satisfactionScores: [] },\n  \n  onTick: async (state) => {\n    // Get today's customer issues\n    const dailyIssues = getRandomIssues(customerIssues, 2);\n    \n    for (const issue of dailyIssues) {\n      const startTime = Date.now();\n      \n      // Test your AI support agent\n      const agentResponse = await supportAgent.handle(issue);\n      \n      const responseTime = Date.now() - startTime;\n      const satisfaction = scoreResponse(agentResponse, issue);\n      \n      state.totalIssues++;\n      if (satisfaction > 7) state.resolvedIssues++;\n      state.satisfactionScores.push(satisfaction);\n      \n      // Update running averages\n      const avgSatisfaction = state.satisfactionScores.reduce((a,b) => a+b, 0) / state.satisfactionScores.length;\n      const resolutionRate = (state.resolvedIssues / state.totalIssues) * 100;\n      \n      console.log(`Resolution Rate: ${resolutionRate.toFixed(1)}% | Avg Satisfaction: ${avgSatisfaction.toFixed(1)}/10`);\n    }\n    \n    return state.tick < 9; // Test for 10 days\n  },\n  \n  onEnd: (state) => {\n    const finalSatisfaction = state.satisfactionScores.reduce((a,b) => a+b, 0) / state.satisfactionScores.length;\n    console.log(`üéØ Final Results: ${((state.resolvedIssues/state.totalIssues)*100).toFixed(1)}% resolution rate, ${finalSatisfaction.toFixed(1)}/10 satisfaction`);\n  }\n});\n\nawait simulation.run();\n```\n\n**What's happening here?** Each tick simulates a day of customer support. SimKit feeds random issues to your AI agent, measures response quality and speed, then tracks KPIs over time. Perfect for A/B testing different models, regression testing after prompt changes, or measuring performance before production deployment.\n\n## ü§ñ Built for AI Agents\n\n### Global State Access\nAI agents need access to simulation state from anywhere:\n\n```typescript\nimport { setSimState, getSimState } from \"@fallom/simkit/state\";\n\n// In your simulation loop\nsetSimState(state);\n\n// In your AI tools\nconst currentState = getSimState<MyState>();\n```\n\n### Deterministic Testing\nReproduce exact scenarios with seeded randomness - perfect for fair model comparisons:\n\n```typescript\nimport { initializeRandom, choice, shuffle } from \"@fallom/simkit/random\";\n\n// Test Model A\ninitializeRandom(12345); // Same seed = same test scenarios\nconst modelA_results = await testSupportAgent(modelA);\n\n// Test Model B with identical scenarios\ninitializeRandom(12345); // Reset to same seed\nconst modelB_results = await testSupportAgent(modelB);\n\n// Now you can fairly compare: both models faced the exact same issues\nconsole.log(`Model",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-16T18:45:27.564699"
  },
  {
    "basic_info": {
      "name": "hometube",
      "full_name": "EgalitarianMonkey/hometube",
      "owner": "EgalitarianMonkey",
      "description": "HomeTube is a featured friendly video downloader managing video URL into organized content for home media library.",
      "url": "https://github.com/EgalitarianMonkey/hometube",
      "clone_url": "https://github.com/EgalitarianMonkey/hometube.git",
      "ssh_url": "git@github.com:EgalitarianMonkey/hometube.git",
      "homepage": "",
      "created_at": "2025-09-15T09:25:00Z",
      "updated_at": "2025-09-16T18:36:05Z",
      "pushed_at": "2025-09-16T13:09:40Z"
    },
    "stats": {
      "stars": 42,
      "forks": 0,
      "watchers": 42,
      "open_issues": 2,
      "size": 79466
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 156976,
        "Makefile": 7425,
        "Dockerfile": 1560,
        "Shell": 776
      },
      "license": "GNU Affero General Public License v3.0",
      "topics": [
        "docker",
        "homelab",
        "jellyfin",
        "open-source",
        "plex",
        "selfhosted",
        "sponsorblock",
        "youtube-downloader",
        "yt-dlp"
      ]
    },
    "content": {
      "readme": "<div align=\"center\">\n\n<br/>\n\n# üé¨ HomeTube\n\n<br/>\n\n[![Python](https://img.shields.io/badge/Python-3.10+-blue.svg)](https://python.org)\n[![Streamlit](https://img.shields.io/badge/Streamlit-1.49+-red.svg)](https://streamlit.io)\n[![Latest Release](https://img.shields.io/github/v/release/EgalitarianMonkey/hometube)](https://github.com/EgalitarianMonkey/hometube/releases)\n[![Docker Image](https://ghcr-badge.egpl.dev/egalitarianmonkey/hometube/latest_tag?trim=major&label=Docker)](https://github.com/EgalitarianMonkey/hometube/pkgs/container/hometube)\n[![License](https://img.shields.io/badge/License-AGPL--3.0-green.svg)](LICENSE)\n\n<br/>\n\n**üåê Universal Video Downloader for your HomeLab**\n\n*Download, process and organize videos at Home*\n\n<br/>\n\n</div>\n\n<br/>\n<br/>\n\n<!-- --- -->\n\n<!-- ## üéØ What is HomeTube? -->\n\n\nüé¨ HomeTube is a simple web UI for downloading single videos from the internet with the highest quality available and moving them to specific local locations automatically managed and integrated by media server such as Plex or Jellyfin.\n\nA simple friendly solution for easily integrating preferred videos from Youtube and others platforms to local media server.\n\n### üè† **HomeLab Integration**\n- **üé¨ Media server Ready**: Download best quality videos with explicit name and location directly in your HomeLab media server structure and get automatic watch experience on Plex, Jellyfin, Emby or even on your PC\n- **üì± Network Access**: Web interface videos download accessible from any device on your network\n\n### ‚ö° **Features**\n- **üéØ One-Click Downloads**: Paste URL ‚Üí Get perfectly organized video\n- **üö´ Ad-Free Content**: Block videos' sponsors and ads\n- **üé¨ Advanced Processing**: Cut clips, embed subtitles, convert formats\n- **üîê Unlock restricted videos**: Cookies support for member-only videos, restricted age, etc.\n- **üìä Quality Control**: Auto-select best quality or manual override\n- **üé• Video Sources**: **YouTube**, Reddit, Vimeo, Dailymotion, TikTok, Twitch, Facebook, Instagra, etc. [See complete list (1800+)](docs/supported-platforms.md)\n\n<!-- ## ‚ö° Technical Highlights\n\n<div align=\"center\">\n\n| üéØ **Easy to Use** | üîß **Powerful** | üè† **HomeLab Ready** |\n|:---:|:---:|:---:|\n| Web interface | 1800+ platforms | Docker deployment |\n| One-click downloads | Advanced processing | Network accessible |\n| Auto-organization | Cookie authentication | Plex/Jellyfin ready |\n\n</div> -->\n\n<!-- --- -->\n\n<br/>\n<br/>\n\n![Application Demo](./docs/images/simple_ui_demo.gif)\n\n<br/>\n<br/>\n\n<!-- --- -->\n\n## üõ†Ô∏è HomeTube Options\n\n### üö´ SponsorBlock Integration\n\n**Automatically skip sponsors, ads, and promotional content** with built-in SponsorBlock support. Just download your video and sponsors segments are automatically detected and marked.\n\n- ‚úÖ **Auto-detection**: Sponsors segments automatically identified\n- ‚úÖ **Manage sponsors to block**: Sponsors segments to block or mark can be managed in the UI\n- ‚úÖ **Community-driven**: Powered by SponsorBlock's crowd-sourced database\n- ‚úÖ **Zero configuration**: Works out of the box for YouTube videos\n\n[Learn more about SponsorBlock features](docs/usage.md#-sponsorblock-integration).\n\n### üè† HomeLab Integration\n\n**Perfect integration with self-hosted setup**:\n\n- **üê≥ Docker Ready**: One-command deployment with Docker Compose\n- **üé¨ Media Server Integration**: Direct integration with media server thanks to well named video files automatically moved to chosen locations watched by media server such as Plex, Jellyfin, or Emby.\n- **üì± Network Access**: Web interface accessible from any device on your network\n- **üîê Secure**: No cloud dependencies, everything runs locally\n- **‚öôÔ∏è Configurable**: Extensive customization through environment variables\n\n[Setup your HomeLab integration](docs/deployment.md).\n\n### üç™ Unlock restricted videos (Cookies)\n\nPrivate content, age-restricted, or member-only videos are restricted without authentication on platforms like YouTube. We can unlock restricted content thanks to **cookies** authentication.\n\nWe can use **Browser cookies** if on a machine sharing a browser, otherwise **Cookies File** in HomeLab setup.\n\n[More details about Cookies authentication setup](docs/usage.md#-authentication--private-content).\n\n### ‚úÇÔ∏è Advanced Video Processing\n\nTransform your downloads with **powerful built-in video processing tools**:\n\n- **üé¨ Clip Extraction**: Cut specific segments from videos with precision timing\n- **üìù Subtitle Embedding**: Automatically embed subtitles in multiple languages\n- **üîÑ Format Conversion**: Convert between video formats (MP4, MKV, WebM, etc.)\n- **üéµ Audio Extraction**: Extract audio-only versions in high quality\n- **üì± Mobile Optimization**: Optimize videos for mobile devices\n\n[Explore all processing options](docs/usage.md#-video-processing).\n\n### üéØ Smart Download Management\n\n**Intelligent download system** that adapts to your needs:\n\n- **üèÜ Quality Selection**: Auto-select best quality or manual override\n- **üìÅ Auto-Organization**: Videos organized by channel/creator automatical",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-16T18:45:28.872948"
  },
  {
    "basic_info": {
      "name": "CRT_Python_AI_A",
      "full_name": "gilshan-s/CRT_Python_AI_A",
      "owner": "gilshan-s",
      "description": "Coding",
      "url": "https://github.com/gilshan-s/CRT_Python_AI_A",
      "clone_url": "https://github.com/gilshan-s/CRT_Python_AI_A.git",
      "ssh_url": "git@github.com:gilshan-s/CRT_Python_AI_A.git",
      "homepage": null,
      "created_at": "2025-09-16T04:28:17Z",
      "updated_at": "2025-09-16T09:35:22Z",
      "pushed_at": "2025-09-16T05:24:03Z"
    },
    "stats": {
      "stars": 38,
      "forks": 34,
      "watchers": 38,
      "open_issues": 0,
      "size": 2
    },
    "tech_info": {
      "language": null,
      "languages": {},
      "license": null,
      "topics": []
    },
    "content": {
      "readme": "# CRT_Python_AI_A\nCoding\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-16T18:45:30.119786"
  },
  {
    "basic_info": {
      "name": "LLaVA-OneVision-1.5",
      "full_name": "EvolvingLMMs-Lab/LLaVA-OneVision-1.5",
      "owner": "EvolvingLMMs-Lab",
      "description": null,
      "url": "https://github.com/EvolvingLMMs-Lab/LLaVA-OneVision-1.5",
      "clone_url": "https://github.com/EvolvingLMMs-Lab/LLaVA-OneVision-1.5.git",
      "ssh_url": "git@github.com:EvolvingLMMs-Lab/LLaVA-OneVision-1.5.git",
      "homepage": null,
      "created_at": "2025-09-16T14:05:47Z",
      "updated_at": "2025-09-16T18:42:50Z",
      "pushed_at": "2025-09-16T14:10:29Z"
    },
    "stats": {
      "stars": 37,
      "forks": 1,
      "watchers": 37,
      "open_issues": 1,
      "size": 2636
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 7572960,
        "Jupyter Notebook": 521664,
        "Shell": 178138,
        "C++": 38717,
        "Cuda": 16941,
        "C": 2951,
        "Dockerfile": 2631,
        "HTML": 2625,
        "Makefile": 313
      },
      "license": null,
      "topics": []
    },
    "content": {
      "readme": "# LLaVA-OneVision-1.5: Fully Open Framework for Democratized Multimodal Training\n\n\n[ü§ó Mid-Training-Data (Uploading!)](https://huggingface.co/datasets/lmms-lab/LLaVA-One-Vision-1.5-Mid-Training-85M) | \n[ü§ó Insturct-Data (Uploading!)](https://huggingface.co/datasets/lmms-lab/LLaVA-One-Vision-1.5-Insturct-26M) \n\n**LLaVA-OneVision1.5** introduces a novel family of **fully open-source** Large Multimodal Models (LMMs) that achieves **state-of-the-art performance**  with substantially **lower cost** through training on **native resolution** images.\n\n1. **Superior Performance**\nA family of fully open-source large multimodal models demonstrating **superior performance** across multiple multimodal benchmarks, **outperforming Qwen2.5-VL** in most evaluation tasks.\n\n2. **High-Quality Data at Scale**\nMeticulously curated **pre-training and SFT data** with rigorous filtering and quality control, achieving **superior data efficiency** with only **64B tokens**.\n- Concept-balanced, highly diverse, high-quality caption data\n- Comprehensive instruction fine-tuning data covering a wide range of tasks\n\n3. **Ultra-Efficient Training Framework**\nComplete end-to-end training framework designed for maximum efficiency:\n- **$16K total budget** for full model training\n- **45% HFU efficiency** on A100 GPUs ($0.6 per GPU/Hour)\n- Built on **MegatronLM** with support for **MoE**, **FP8**, and **long sequence parallelization**\n- Optimized codebase for cost-effective scaling\n\n4. **Fully Open Framework** for community access and reproducibility:\n- ‚úÖ High-quality pre-training & SFT data\n- ‚úÖ Complete training framework & code\n- ‚úÖ Training recipes & configurations\n- ‚úÖ Base & instruct model checkpoints\n- ‚úÖ Comprehensive training logs & metrics\n\n\n## Model\n\n| Model                  | #Vision Param | #Language Param | #Total Param | HF Link                                                                      |\n|------------------------|---------------|-----------------|--------------|------------------------------------------------------------------------------|\n| LLaVA-OV-1.5-4B-Instruct      | 0.3B          | 4.4B            | 4.7B         | [ü§ó link]()                |\n| LLaVA-OV-1.5-8B-Instruct      | 0.3B          | 8.2B            | 8.5B         | [ü§ó link](https://huggingface.co/lmms-lab/LLaVA-One-Vision-1.5-8B-Instruct)                |\n\n\n## Dataset\n\n![Dataset Visualization](asset/dataset.jpg)\n\n\n| Description | Link |\n|-------------|------|\n| Mid-training data for LLaVA-OneVision-1.5 | [ü§ó Download (Uploading!)](https://huggingface.co/datasets/lmms-lab/LLaVA-One-Vision-1.5-Mid-Training-85M) |\n| SFT data for LLaVA-OneVision-1.5 | [ü§ó Download (Uploading!)](https://huggingface.co/datasets/lmms-lab/LLaVA-One-Vision-1.5-Insturct-26M) |\n\n\n## Evaluation Results\n\n\nAll evaluations were conducted using lmms_eval.\n\n|                                  | **LLaVA-OV-1.5-8B** | **Qwen2.5 VL 7B** | **LLaVA-OV-1.5-4B** | **Qwen2.5 VL 3B** |\n|:----------------------------------|:---------------:|:-------------:|:---------------:|:-------------:|\n| MMMU (Validation)                 |    **55.44**    |     51.33     |    **51.44**    |     46.44     |\n| MMMU-Pro (Standard)               |    **37.40**    |     36.30     |    **33.24**    |     31.10     |\n| MMMU-Pro (Vision)                 |      25.15      |   **32.83**   |    **23.53**    |     21.27     |\n| MMBench (English; Test)           |    **84.14**    |     83.40     |    **82.29**    |     77.97     |\n| MMBench (Chinese; Test)           |      81.00      |   **81.61**   |    **76.73**    |     74.55     |\n| MME-RealWorld (English)           |    **62.31**    |     57.33     |    **57.16**    |     51.60     |\n| MME-RealWorld (Chinese)           |    **56.11**    |     51.50     |      21.38      |   **45.38**   |\n| AI2D (With Mask)                  |    **84.16**    |     82.58     |    **84.62**    |     78.56     |\n| AI2D (Without Mask)               |    **94.11**    |     93.36     |    **92.84**    |     90.74     |\n| CV-Bench                          |    **80.82**    |     79.95     |    **74.00**    |     71.53     |\n| VL-RewardBench                    |      45.90      |   **49.65**   |    **45.90**    |     42.06     |\n| V*                                |    **78.01**    |     76.96     |      66.49      |   **69.63**   |\n| PixmoCount                        |      62.19      |   **63.33**   |    **59.17**    |     50.85     |\n| CountBench                        |    **88.19**    |     86.35     |    **77.80**    |     72.51     |\n| ChartQA                           |    **86.48**    |     84.08     |    **85.11**    |     83.36     |\n| CharXiv (Direct Questions)        |    **74.10**    |     69.80     |    **70.70**    |     58.20     |\n| DocVQA (Test)                     |    **95.00**    |     94.93     |    **93.48**    |     92.67     |\n| InfoVQA (Test)                    |      78.42      |   **81.67**   |    **75.27**    |     75.63     |\n| WeMath                            |    **33.6",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-16T18:45:31.427488"
  },
  {
    "basic_info": {
      "name": "anthropic-claude-max-proxy",
      "full_name": "Pimzino/anthropic-claude-max-proxy",
      "owner": "Pimzino",
      "description": "OpenAI-compatible proxy for Claude Pro/Max subscriptions using OAuth.",
      "url": "https://github.com/Pimzino/anthropic-claude-max-proxy",
      "clone_url": "https://github.com/Pimzino/anthropic-claude-max-proxy.git",
      "ssh_url": "git@github.com:Pimzino/anthropic-claude-max-proxy.git",
      "homepage": "",
      "created_at": "2025-09-15T00:52:57Z",
      "updated_at": "2025-09-16T18:05:01Z",
      "pushed_at": "2025-09-16T12:02:00Z"
    },
    "stats": {
      "stars": 33,
      "forks": 8,
      "watchers": 33,
      "open_issues": 0,
      "size": 69
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 52223
      },
      "license": "Other",
      "topics": []
    },
    "content": {
      "readme": "# Anthropic Claude Max Proxy\n\nPure Anthropic proxy for Claude Pro/Max subscriptions using OAuth.\n\n## SUPPORT MY WORK\n<a href=\"https://buymeacoffee.com/Pimzino\" target=\"_blank\"><img src=\"https://cdn.buymeacoffee.com/buttons/v2/default-yellow.png\" alt=\"Buy Me A Coffee\" style=\"height: 60px !important;width: 217px !important;\" ></a>\n\n## DISCLAIMER\n\n**FOR EDUCATIONAL PURPOSES ONLY**\n\nThis tool:\n- Is NOT affiliated with or endorsed by Anthropic\n- Uses undocumented OAuth flows from Claude Code\n- May violate Anthropic's Terms of Service\n- Could stop working at any time without notice\n- Comes with NO WARRANTY or support\n\n**USE AT YOUR OWN RISK. The authors assume no liability for any consequences.**\n\nFor official access, use Claude Code or Anthropic's API with console API keys.\n\n## Prerequisites\n\n- Active Claude Pro or Claude Max subscription\n- Python 3.8+\n- pip\n\n## Quick Start\n\n1. **Virtual Environment Setup (Recommended)**\n```bash\npython -m venv venv\n```\n\n2. **Install:**\n```bash\nvenv/Scripts/Activate.ps1\npip install -r requirements.txt\n```\n\n3. **Configure (optional):**\n```bash\ncp config.example.json config.json\n```\n\n4. **Run:**\n```bash\npython cli.py\n```\n\n5. **Authenticate:**\n- Select option 2 (Login)\n- Browser opens automatically\n- Complete login at claude.ai\n- Copy the authorization code\n- Paste in terminal\n\n6. **Start proxy:**\n- Select option 1 (Start Proxy Server)\n- Server runs at `http://127.0.0.1:8081`\n\n## Client Configuration\n\nConfigure your Anthropic API client:\n\n- **Base URL:** `http://127.0.0.1:8081`\n- **API Key:** Any non-empty string (e.g., \"dummy\")\n- **Model:** `claude-sonnet-4-20250514` (or any available Claude model)\n- **Endpoint:** `/v1/messages`\n\n## Available Models\n\n- `claude-sonnet-4-20250514` - Claude 4 Sonnet (latest) **[RECOMMENDED]**\n- `claude-3-5-sonnet-20241022` - Claude 3.5 Sonnet (latest)\n- `claude-3-5-haiku-20241022` - Claude 3.5 Haiku (latest)\n- `claude-3-opus-20240229` - Claude 3 Opus\n- See proxy startup output for complete model list\n\n## Configuration Priority\n\n1. Environment variables (highest)\n2. config.json file\n3. Built-in defaults (lowest)\n\n## License\n\nMIT License - see [LICENSE](LICENSE) file\n\nThis software is provided for educational purposes only. Users assume all risks.\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-16T18:45:32.704545"
  },
  {
    "basic_info": {
      "name": "SOLANA_SNIPER_BOT",
      "full_name": "0xKronus/SOLANA_SNIPER_BOT",
      "owner": "0xKronus",
      "description": "A Solana sniper in the booming SOL ecosystem ‚Äì a sophisticated automated tool built to instantly detect and acquire newly listed tokens.",
      "url": "https://github.com/0xKronus/SOLANA_SNIPER_BOT",
      "clone_url": "https://github.com/0xKronus/SOLANA_SNIPER_BOT.git",
      "ssh_url": "git@github.com:0xKronus/SOLANA_SNIPER_BOT.git",
      "homepage": "",
      "created_at": "2025-09-16T03:46:30Z",
      "updated_at": "2025-09-16T05:44:08Z",
      "pushed_at": "2025-09-16T04:52:11Z"
    },
    "stats": {
      "stars": 32,
      "forks": 209,
      "watchers": 32,
      "open_issues": 0,
      "size": 1263
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 446988,
        "PHP": 3934
      },
      "license": null,
      "topics": [
        "memecoin-sniping-bot",
        "pumpfun",
        "pumpfun-sniper-go",
        "raydium",
        "solana-memecoin-sniper-bot",
        "solana-rug",
        "solana-sniper",
        "solana-sniping",
        "solana-trading",
        "solanamemecoines",
        "solanamevbot"
      ]
    },
    "content": {
      "readme": "<h1 align=\"center\"> SOLANA BOT </h1> <br>\n<p align=\"center\">\n  <a href=\"\"> \n    \n  </a>  \n</p>\n \n<p align=\"center\">\n  A Bot in your pocket based on take profit or buy/sell on Raydium.\n</p>\n\n\n<!-- START doctoc generated TOC please keep comment here to allow auto update -->\n<!-- DON'T EDIT THIS SECTION, INSTEAD RE-RUN doctoc TO UPDATE -->\n## Table of Contents\n\n- [Introduction](#introduction)\n- [Features](#features)\n- [Usage](#Usage)\n- [Setting](#Setting-)\n- [Disclaimer ](#Disclaimer)\n- [Contact ](#Contact)\n\n<!-- END doctoc generated TOC please keep comment here to allow auto update -->\n\n## Introduction\n\nThe Solana Sniper/Trading Bot is a groundbreaking tool in the booming Solana ecosystem, designed to tackle a common issue faced by traders: missing out on profit opportunities after purchasing tokens on the Solana network, leading to token rug-pulls or dumps. This software not only integrates sniping functionality, allowing users to instantly acquire tokens upon their launch but also adds trading tools to optimize one‚Äôs position.\n\n**Available for both iOS and Android and PC .**\n\n\n![image](https://github.com/user-attachments/assets/8b825c7d-1f6e-4178-a68c-af6c4dc4877d)\n\n\n\n<p align=\"center\">\n  <a href =\"https://t.me/z3Zrsolana\">\n  \n  </a>\n</p>\n\n## Features\n\nA few of the things you can do with Bot:\n\n- Sniping: Execute buy transactions instantly when liquidity is added to an SPL token, ensuring you're among the first to buy in promising new tokens.\n- Take Profit: Automatically sell tokens at a predefined profit percentage, securing gains.\n- Buy/Sell x Times: Execute repeated buy orders to average down or scale into positions.\n- Sell Limit Order: Set your tokens to sell automatically at a predetermined price, locking in profits.\n- User friendly interface - hands-on interface\n- **Making the first to trade in new tokens.**\n\n<img width=\"1010\" height=\"269\" alt=\"Screenshot 2025-09-16 at 11 30 40\" src=\"https://github.com/user-attachments/assets/e99f66c4-26f6-412a-b5b5-b6b3995a8873\" />\n\n<img width=\"1009\" height=\"272\" alt=\"Screenshot 2025-09-16 at 11 31 13\" src=\"https://github.com/user-attachments/assets/982a188b-93a1-4626-b00d-324f0010659f\" />\n\n\n<img width=\"910\" alt=\"2\" src=\"https://github.com/user-attachments/assets/a85bd1f2-c152-42a3-8b27-c3bb31cb59e2\">\n\n\n## Installation\n\n- Downloads Python ( Recommend the latest version )  [Python 3.13.7](https://www.python.org/downloads/)\n-  ***VERY IMPORTANT***: When installing Python also install **\"Add python.exe to path\"** and ***\"Use admin privileges when installing py.exe:*** => Tick\n\n## Usage\n<video src=\"https://github.com/user-attachments/assets/fab474c3-3c18-40be-9fe8-9b108dad738e\" width=\"320\" height=\"240\" controls></video>\n\n\n\n\n- Update `pip` Run the following command to update pip to the latest version\n\n```python\npython -m pip install --upgrade pip\n```\n- Clone or download the project\n\n```git \ngit clone https://github.com/0xKronus/SOLANA_SNIPER_BOT.git\n```\n\nOption 2: Download the project directly\n\nGo to the project's GitHub page, click the \"Code\" button and select \"Download ZIP\". Unzip the downloaded ZIP file to get the project folder.\n\n- Navigate to the project folder\n\nOpen a terminal and navigate to the project folder\n\n```python\ncd SOLANA_SNIPER_BOT\n```\n\n- Install libraries\n\nRun the following command to install the required libraries for the project:\n\n```python\npip install -r requirements.txt\n```\n\n- Run the project\n\nRun the following command to start the project:\n\n\n\n```python\npython main.py\n```\n\n\n\n## Setting\n- **BALANCE** : Show Balance & Profit\n- **BUY DELAY** : In seconds after launch. Set to 0, Token will buy immediately after token launch\n- **TAKE PROFIT** : Take-Profit Order (TP) . Token places a sell order and confirms immediately after reaching the target\n- **SELL DELAY** : to the number of seconds you want to wait before selling the token. Set to 0, token will be sold immediately after it is bought.\n- **CHECK RUG** : Set to true to check the risk score and protect against rug pulls.\n\n\n\nExample: \n\n<img width=\"1176\" alt=\"s\" src=\"https://github.com/user-attachments/assets/97d97112-703d-48f8-8075-a2de60d85cb1\">\n\n\n![image](https://github.com/user-attachments/assets/8b825c7d-1f6e-4178-a68c-af6c4dc4877d)\n\n\n\n\n\n  \n\n\n\n## Disclaimer\n\n- This extension is not affiliated with Solana Foundation or Solana Labs. It is a non-profit community project.\n- Solana Snipe is in active development, so all the snippets are subject to change.\n- The snippets are unaudited. Use at your own risk.\n\n![Happy_GIF](https://media.giphy.com/media/erePhJFWkfYMwTpNT8/giphy.gif) \n",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-16T18:45:33.971712"
  },
  {
    "basic_info": {
      "name": "kipm_cs_javascript",
      "full_name": "debasishsahoo/kipm_cs_javascript",
      "owner": "debasishsahoo",
      "description": null,
      "url": "https://github.com/debasishsahoo/kipm_cs_javascript",
      "clone_url": "https://github.com/debasishsahoo/kipm_cs_javascript.git",
      "ssh_url": "git@github.com:debasishsahoo/kipm_cs_javascript.git",
      "homepage": null,
      "created_at": "2025-09-15T04:33:59Z",
      "updated_at": "2025-09-16T15:42:35Z",
      "pushed_at": "2025-09-16T10:48:45Z"
    },
    "stats": {
      "stars": 31,
      "forks": 11,
      "watchers": 31,
      "open_issues": 0,
      "size": 31
    },
    "tech_info": {
      "language": "JavaScript",
      "languages": {
        "JavaScript": 29593,
        "HTML": 7156,
        "CSS": 742
      },
      "license": null,
      "topics": []
    },
    "content": {
      "readme": "",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-16T18:45:35.251515"
  },
  {
    "basic_info": {
      "name": "mint-mono",
      "full_name": "yuru7/mint-mono",
      "owner": "yuru7",
      "description": "ÈñãÁô∫ËÄÖÂêë„Åë„Éï„Ç©„É≥„Éà„ÅÆ Intel One Mono „Å®Êó•Êú¨Ë™û„Éï„Ç©„É≥„Éà„ÅÆ Circle M+ Á≠â„ÇíÂêàÊàê„Åó„Åü„Éó„É≠„Ç∞„É©„Éü„É≥„Ç∞Âêë„Åë„Éï„Ç©„É≥„Éà „ÄåMint Mono„Äç",
      "url": "https://github.com/yuru7/mint-mono",
      "clone_url": "https://github.com/yuru7/mint-mono.git",
      "ssh_url": "git@github.com:yuru7/mint-mono.git",
      "homepage": "",
      "created_at": "2025-09-15T07:12:35Z",
      "updated_at": "2025-09-16T12:48:18Z",
      "pushed_at": "2025-09-15T07:23:57Z"
    },
    "stats": {
      "stars": 31,
      "forks": 0,
      "watchers": 31,
      "open_issues": 0,
      "size": 8910
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 48975,
        "PowerShell": 2629
      },
      "license": "SIL Open Font License 1.1",
      "topics": []
    },
    "content": {
      "readme": "# Mint Mono\n\nMint Mono „ÅØ„ÄÅÈñãÁô∫ËÄÖÂêë„Åë„Éï„Ç©„É≥„Éà„ÅÆ [Intel One Mono](https://github.com/intel/intel-one-mono) „Å®Êó•Êú¨Ë™û„Éï„Ç©„É≥„Éà„ÅÆ [Circle M+](https://itouhiro.github.io/mixfont-mplus-ipa/mplus/) Á≠â„ÇíÂêàÊàê„Åó„Åü„Éó„É≠„Ç∞„É©„Éü„É≥„Ç∞Âêë„Åë„Éï„Ç©„É≥„Éà„Åß„Åô„ÄÇ\n\nË¶ñË™çÊÄß„ÅåÈ´ò„ÅèÊ©üËÉΩÁöÑ„Å™ Intel One Mono „Å®„ÄÅÂêå„Åò„ÅèË¶ñË™çÊÄß„ÉªÊ©üËÉΩÊÄß„Å´ÂÑ™„Çå„ÅüÊó•Êú¨Ë™û„Éï„Ç©„É≥„Éà Circle M+ (M+ FONTS „ÅÆÊ¥æÁîüÁâà) „ÇíÁµÑ„ÅøÂêà„Çè„Åõ„Åæ„Åó„Åü„ÄÇ\n\n[üëâ „ÉÄ„Ç¶„É≥„É≠„Éº„Éâ](https://github.com/yuru7/mint-mono/releases/latest)  \n‚Äª„ÄåAssets„ÄçÂÜÖ„ÅÆ zip „Éï„Ç°„Ç§„É´„Çí„ÉÄ„Ç¶„É≥„É≠„Éº„Éâ„Åó„Å¶„ÅîÂà©Áî®„Åè„Å†„Åï„ÅÑ„ÄÇ\n\n## ÁâπÂæ¥\n\n‰ª•‰∏ã„ÅÆÁâπÂæ¥„ÇíÂÇô„Åà„Å¶„ÅÑ„Åæ„Åô„ÄÇ\n\n- Intel One Mono Áî±Êù•„ÅÆË¶ñË™çÊÄß„ÅåÈ´ò„ÅèË™≠„Åø„ÇÑ„Åô„ÅÑ„É©„ÉÜ„É≥ÊñáÂ≠ó\n- Circle M+ Áî±Êù•„ÅÆ„É¢„ÉÄ„É≥„ÅßË™≠„ÅøÊòì„ÅÑÊó•Êú¨Ë™ûÊñáÂ≠ó\n    - Circle M+ „Åß„ÅØÂÖÉ„ÄÖË™≠„Åø„ÇÑ„Åô„ÅÑ M+ FONTS „Çí„Åï„Çâ„Å´Áô∫Â±ï„Åï„Åõ„ÄÅ‰ª•‰∏ã„ÅÆ„Çà„ÅÜ„Å™ÁâπÂæ¥„ÇíÊåÅ„Å£„Å¶„ÅÑ„Åæ„Åô\n        - ÂçäÊøÅÁÇπ„ÅåÂ§ß„Åç„ÅÑ\n        - „Ç´Âäõ „Ç®Â∑• „É≠Âè£ „Éº‰∏Ä „Éã‰∫å „Å∏„Éò „ÅÆÂå∫Âà•\n        - „ÄúÔΩûÔºàÊ≥¢„ÉÄ„ÉÉ„Ç∑„É•„ÉªÂÖ®Ëßí„ÉÅ„É´„ÉÄÔºâ„ÅÆÂå∫Âà•\n- ÂÖ®Ëßí„Çπ„Éö„Éº„Çπ„ÅÆÂèØË¶ñÂåñ\n- [IBM Plex Sans JP](https://github.com/IBM/plex) „ÇíËøΩÂä†ÂêàÊàê„Åô„Çã„Åì„Å®„Åß„ÄåAdobe-Japan1-7„ÄçÊñáÂ≠ó„Ç≥„É¨„ÇØ„Ç∑„Éß„É≥„Å´ÂØæÂøú\n\n### „Éê„É™„Ç®„Éº„Ç∑„Éß„É≥\n\n| Á®ÆÈ°û | Ë™¨Êòé | ÂëΩÂêç„Éë„Çø„Éº„É≥ |\n| --- | --- | --- |\n| ÊñáÂ≠óÂπÖÊØîÁéá ÂçäËßí1:ÂÖ®Ëßí2 | Intel One Mono „ÇíÁ∏ÆÂ∞è„Åô„Çã„Åì„Å®„Åß„ÄÅÂçäËßí1:ÂÖ®Ëßí2„ÅÆÊñáÂ≠óÂπÖÊØîÁéá„Å®„Å™„Çã„Çà„ÅÜ„Å´ÂêàÊàê„Åó„Åü„Éê„É™„Ç®„Éº„Ç∑„Éß„É≥„ÄÇ | `MintMono-*.ttf`<br>‚Äª„Éï„Ç°„Ç§„É´Âêç„Å´ `35` „ÅåÂê´„Åæ„Çå„Å¶ **„ÅÑ„Å™„ÅÑ** „ÇÇ„ÅÆ |\n| ÊñáÂ≠óÂπÖÊØîÁéá ÂçäËßí3:ÂÖ®Ëßí5 | Intel One Mono „ÇíÁ∏ÆÂ∞è„Åõ„Åö„Å´ÂêàÊàê„Åó„ÄÅÂçäËßí3:ÂÖ®Ëßí5„ÅÆÊñáÂ≠óÂπÖÊØîÁéá„Å®„Åó„Åü„Éê„É™„Ç®„Éº„Ç∑„Éß„É≥„ÄÇÂçäËßí1:ÂÖ®Ëßí2„Å®ÊØî„Åπ„ÄÅËã±Êï∞Â≠ó„Å™„Å©„ÅÆÂçäËßíÊñáÂ≠ó„Åå„ÇÜ„Å®„Çä„ÅÆ„ÅÇ„ÇãÂπÖ„ÅßË°®Á§∫„Åï„Çå„Çã„ÄÇ| `MintMono35-*.ttf`<br>‚Äª„Éï„Ç°„Ç§„É´Âêç„Å´ `35` „ÅåÂê´„Åæ„Çå„Å¶ **„ÅÑ„Çã** „ÇÇ„ÅÆ |\n\n## Ë°®Á§∫„Çµ„É≥„Éó„É´\n\n| ÈÄöÂ∏∏Áâà (ÂπÖÊØîÁéá ÂçäËßí1:ÂÖ®Ëßí2) | 35Áâà (ÂπÖÊØîÁéá ÂçäËßí3:ÂÖ®Ëßí5) |\n| :---: | :---: |\n| <img width=\"839\" height=\"474\" alt=\"image\" src=\"https://github.com/user-attachments/assets/d74c6526-d9f2-458d-b9f3-d4136410d5c3\" /> | <img width=\"812\" height=\"479\" alt=\"image\" src=\"https://github.com/user-attachments/assets/32a5b04f-0c5a-41e9-bf7b-ca83e7c5d94c\" /> |\n\n## „Éì„É´„Éâ\n\n### Áí∞Â¢É\n\n- [FontForge](https://fontforge.org/en-US/)\n- Python 3.x\n\n### ‰æùÂ≠ò„Éë„ÉÉ„Ç±„Éº„Ç∏„ÅÆ„Ç§„É≥„Çπ„Éà„Éº„É´\n\n```shell\npip install fonttools\n```\n\n### „Éì„É´„Éâ„ÅÆÂÆüË°å (PowerShell)\n\n```powershell\n./make.ps1\n```\n\n„Çπ„ÇØ„É™„Éó„Éà„ÅåÊ≠£Â∏∏„Å´ÂÆå‰∫Ü„Åô„Çã„Å®„ÄÅ`build` „Éá„Ç£„É¨„ÇØ„Éà„É™„Å´„Éï„Ç©„É≥„Éà„Éï„Ç°„Ç§„É´„ÅåÁîüÊàê„Åï„Çå„Åæ„Åô„ÄÇ\n\n### „Éì„É´„Éâ„Ç™„Éó„Ç∑„Éß„É≥\n\n`make.ps1` „ÇíÁ∑®ÈõÜ„Åô„Çã„Åì„Å®„Åß„ÄÅ`fontforge_script.py` „Å´‰ª•‰∏ã„ÅÆ„Ç™„Éó„Ç∑„Éß„É≥„ÇíÊ∏°„Åó„Å¶„ÄÅÁîüÊàê„Åô„Çã„Éï„Ç©„É≥„Éà„Çí„Ç´„Çπ„Çø„Éû„Ç§„Ç∫„Åß„Åç„Åæ„Åô„ÄÇ\n\n- `--35`: ÂçäËßí3:ÂÖ®Ëßí5 „ÅÆÂπÖ„Å´„Åô„Çã\n- `--hidden-zenkaku-space`: ÂÖ®Ëßí„Çπ„Éö„Éº„ÇπÂèØË¶ñÂåñ„Çí„Åó„Å™„ÅÑ\n\n## „É©„Ç§„Çª„É≥„Çπ\n\nSIL OPEN FONT LICENSE Version 1.1 „ÅåÈÅ©Áî®„Åï„Çå„ÄÅÂïÜÁî®„ÉªÈùûÂïÜÁî®Âïè„Çè„ÅöÂà©Áî®ÂèØËÉΩ„Åß„Åô„ÄÇ\n\nÂêÑÂêàÊàêÂÖÉ„Éï„Ç©„É≥„Éà„ÅÆ„É©„Ç§„Çª„É≥„Çπ„ÅØ [source](./source/) „Éá„Ç£„É¨„ÇØ„Éà„É™„Å´Ê†ºÁ¥ç„Åï„Çå„Å¶„ÅÑ„Åæ„Åô„ÄÇ\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-16T18:45:36.515210"
  },
  {
    "basic_info": {
      "name": "free-sqlite",
      "full_name": "fjb040911/free-sqlite",
      "owner": "fjb040911",
      "description": "Free SQLite for VSCode.Support writing SQL statements",
      "url": "https://github.com/fjb040911/free-sqlite",
      "clone_url": "https://github.com/fjb040911/free-sqlite.git",
      "ssh_url": "git@github.com:fjb040911/free-sqlite.git",
      "homepage": null,
      "created_at": "2025-09-16T06:20:19Z",
      "updated_at": "2025-09-16T18:23:34Z",
      "pushed_at": "2025-09-16T10:16:42Z"
    },
    "stats": {
      "stars": 30,
      "forks": 0,
      "watchers": 30,
      "open_issues": 0,
      "size": 9881
    },
    "tech_info": {
      "language": "TypeScript",
      "languages": {
        "TypeScript": 34158,
        "JavaScript": 16831
      },
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "# Free sqlite\n\nVSCode extension to explore and query SQLite databases.\nopen-source and free.\n\n## Features\n - Open any SQLite file - Simply click on .sqlite or .db files to open them instantly\n - Table Explorer - Browse all tables in your database from an integrated sidebar\n - Data Visualization - View table data in a clean, modern interface that adapts to your VS Code theme\n - SQL statement editor - SQL statement editor, Automatically complete SQL keywords, table names, and fields\n - Query result export - The query results can be exported as Excel or CSV\n - Favorites - Collect some of your most commonly used SQL statements\n\n## How to use\n\n### Install\nVSCode extension install!\n[Install free sqlite](https://marketplace.visualstudio.com/items?itemName=free-sqlite.free-sqlite)\n\n### Open database\nNow! Browse all tables in your database in the right panel\n![open](https://github.com/fjb040911/free-sqlite/blob/main/doc/open.gif?raw=true)\n\n### Multiple files\n![multiple](https://github.com/fjb040911/free-sqlite/blob/main/doc/multi.gif?raw=true)\n\n### SQL Editor\nAutomatically complete SQL keywords, table names, and fields\n![editor](https://github.com/fjb040911/free-sqlite/blob/main/doc/select.gif?raw=true)\n\n### Favorite\nQuickly execute or view your frequently used SQL\n![favorite](https://github.com/fjb040911/free-sqlite/blob/main/doc/favoites.gif?raw=true)\n\n### Export\n![export](https://github.com/fjb040911/free-sqlite/blob/main/doc/expot.gif?raw=true)\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-16T18:45:37.821422"
  },
  {
    "basic_info": {
      "name": "C3_CRT_Python",
      "full_name": "gilshan-s/C3_CRT_Python",
      "owner": "gilshan-s",
      "description": "Coding",
      "url": "https://github.com/gilshan-s/C3_CRT_Python",
      "clone_url": "https://github.com/gilshan-s/C3_CRT_Python.git",
      "ssh_url": "git@github.com:gilshan-s/C3_CRT_Python.git",
      "homepage": null,
      "created_at": "2025-09-16T09:17:49Z",
      "updated_at": "2025-09-16T09:46:04Z",
      "pushed_at": "2025-09-16T09:41:18Z"
    },
    "stats": {
      "stars": 30,
      "forks": 28,
      "watchers": 30,
      "open_issues": 0,
      "size": 2
    },
    "tech_info": {
      "language": null,
      "languages": {},
      "license": null,
      "topics": []
    },
    "content": {
      "readme": "# C3_CRT_Python\nCoding\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-16T18:45:39.079885"
  },
  {
    "basic_info": {
      "name": "grok2api",
      "full_name": "VeroFess/grok2api",
      "owner": "VeroFess",
      "description": "rewrite grok2api",
      "url": "https://github.com/VeroFess/grok2api",
      "clone_url": "https://github.com/VeroFess/grok2api.git",
      "ssh_url": "git@github.com:VeroFess/grok2api.git",
      "homepage": null,
      "created_at": "2025-09-16T12:48:33Z",
      "updated_at": "2025-09-16T18:33:37Z",
      "pushed_at": "2025-09-16T14:36:37Z"
    },
    "stats": {
      "stars": 28,
      "forks": 14,
      "watchers": 28,
      "open_issues": 0,
      "size": 36
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 137940,
        "HTML": 33516,
        "Dockerfile": 306
      },
      "license": null,
      "topics": []
    },
    "content": {
      "readme": "# Grok API Gateway\n\n## ‰∏éÂéüÁâàÂ∑ÆÂºÇ\n\nÊú¨ fork ÁâàÊú¨Áõ∏ËæÉ‰∫éÂéüÁâàÂ¢ûÂä†‰∫Ü‰ª•‰∏ãÂäüËÉΩÔºö\n\n0. **Âü∫Êú¨ÂÖ®ÈÉ®ÈáçÂÜô‰∫Ü...**\n1. **Ëá™Âä®Ëé∑Âèñ x-statsig-id** - ‰ΩøÁî® Playwright Ëá™Âä®Ëé∑ÂèñÂπ∂ÁÆ°ÁêÜËÆ§ËØÅÂ§¥\n2. **ÊµÅÊ®°ÂºèÊ†áÁ≠æËøáÊª§** - Ëá™Âä®ÁßªÈô§ÂìçÂ∫î‰∏≠ÁöÑ `<xaiArtifact` Á≠âÊ†áÁ≠æ\n3. **Â¢ûÂº∫ÁªüËÆ°ÂäüËÉΩ** - ÊîπËøõÁöÑ‰ª§Áâå‰ΩøÁî®ÁªüËÆ°ÂíåÁõëÊéß\n4. **Grok4ÊîØÊåÅ** - ÂèçÊ≠£ÊàëËÉΩÁî®.jpg\n\n## ÁéØÂ¢ÉÂèòÈáèÈÖçÁΩÆ\n\n### ÂøÖÈúÄÈÖçÁΩÆ\n\n| ÁéØÂ¢ÉÂèòÈáè | ÊèèËø∞ | ÈªòËÆ§ÂÄº | Á§∫‰æã |\n|---------|------|--------|------|\n| `API_KEY` | API ËÆøÈóÆÂØÜÈí• | `sk-123456` | `sk-your-api-key` |\n| `SSO` | Grok SSO ‰ª§ÁâåÔºàÊôÆÈÄöÔºâ | - | `token1,token2,token3` |\n| `SSO_SUPER` | Grok SSO ‰ª§ÁâåÔºàË∂ÖÁ∫ßÔºâ | - | `super_token1,super_token2` |\n\n### ÂèØÈÄâÈÖçÁΩÆ\n\n| ÁéØÂ¢ÉÂèòÈáè | ÊèèËø∞ | ÈªòËÆ§ÂÄº | ÊúâÊïàÂÄº | Á§∫‰æã |\n|---------|------|--------|--------|------|\n| `IS_CUSTOM_SSO` | ÂÖÅËÆ∏Âä®ÊÄÅ SSO ‰ª§Áâå | `false` | `true/false` | `true` |\n| `IS_TEMP_CONVERSATION` | ‰∏¥Êó∂ÂØπËØùÊ®°Âºè | `true` | `true/false` | `false` |\n| `SHOW_THINKING` | ÊòæÁ§∫Êé®ÁêÜËøáÁ®ã | `false` | `true/false` | `true` |\n| `SHOW_SEARCH_RESULTS` | ÊòæÁ§∫ÊêúÁ¥¢ÁªìÊûú | `true` | `true/false` | `false` |\n| `IS_SUPER_GROK` | ÂêØÁî®Ë∂ÖÁ∫ß Grok ÂäüËÉΩ | `false` | `true/false` | `true` |\n| `MANAGER_SWITCH` | ÂêØÁî® Web ÁÆ°ÁêÜÁïåÈù¢ | - | `true/false` | `true` |\n| `ADMINPASSWORD` | ÁÆ°ÁêÜÁïåÈù¢ÂØÜÁ†Å | - | ‰ªªÊÑèÂ≠óÁ¨¶‰∏≤ | `admin123` |\n| `PORT` | ÊúçÂä°Á´ØÂè£ | `5200` | Êï∞Â≠ó | `8080` |\n| `PROXY` | ‰ª£ÁêÜÊúçÂä°Âô® | - | HTTP/SOCKS5 URL | `http://127.0.0.1:1080` |\n| `CF_CLEARANCE` | Cloudflare ‰ª§Áâå | - | CF ‰ª§ÁâåÂ≠óÁ¨¶‰∏≤ | `cf_clearance_token` |\n| `PICGO_KEY` | PICGO ÂõæÂ∫ä API ÂØÜÈí• | - | Â≠óÁ¨¶‰∏≤ | `picgo_api_key` |\n| `TUMY_KEY` | Tumy ÂõæÂ∫ä API ÂØÜÈí• | - | Â≠óÁ¨¶‰∏≤ | `tumy_api_key` |\n| `FILTERED_TAGS` | ËøáÊª§Ê†áÁ≠æÂàóË°® | `xaiArtifact` | ÈÄóÂè∑ÂàÜÈöî | `tag1,tag2,tag3` |\n\n## Âø´ÈÄüÂºÄÂßã\n\n### ‰ΩøÁî® Docker Hub ÈïúÂÉè\n\nÁé∞Âú®ÂèØ‰ª•Áõ¥Êé•‰ªé Docker Hub ÊãâÂèñÈ¢ÑÊûÑÂª∫ÁöÑÈïúÂÉèÔºö\n\n```bash\n# ÊãâÂèñÈïúÂÉè\ndocker pull verofess/grok2api\n\n# ËøêË°åÂÆπÂô®\ndocker run -d \\\n  --name grok2api \\\n  -p 5200:5200 \\\n  -e API_KEY=sk-your-api-key \\\n  -e SSO=your-sso-token \\\n  verofess/grok2api\n\n# ÊàñËÄÖ‰ΩøÁî® docker-compose\ndocker-compose up -d\n```\n\n### Docker Compose Á§∫‰æã\n\n```yaml\nservices:\n  grok2api:\n    image: verofess/grok2api\n    container_name: grok2api\n    ports:\n      - \"5200:5200\"\n    environment:\n      - API_KEY=sk-your-api-key\n      - SSO=your-sso-token\n      - IS_TEMP_CONVERSATION=true\n      - SHOW_THINKING=false\n    restart: unless-stopped\n```",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-16T18:45:40.344265"
  },
  {
    "basic_info": {
      "name": "VLAC",
      "full_name": "InternRobotics/VLAC",
      "owner": "InternRobotics",
      "description": "VLAC: A Vision-Language-Action-Critic Model for Robotic Real-World Reinforcement Learning",
      "url": "https://github.com/InternRobotics/VLAC",
      "clone_url": "https://github.com/InternRobotics/VLAC.git",
      "ssh_url": "git@github.com:InternRobotics/VLAC.git",
      "homepage": null,
      "created_at": "2025-09-15T13:53:14Z",
      "updated_at": "2025-09-16T18:16:08Z",
      "pushed_at": "2025-09-16T12:02:26Z"
    },
    "stats": {
      "stars": 28,
      "forks": 0,
      "watchers": 28,
      "open_issues": 0,
      "size": 126176
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 156404
      },
      "license": null,
      "topics": []
    },
    "content": {
      "readme": "# VLAC: A Vision-Language-Action-Critic Model for Robotic Real-World Reinforcement Learning\n<div align=\"center\">\n\n[[paper]](data/VLAC_EAI.pdf)\n[[code]](https://github.com/InternRobotics/VLAC)\n[[model]](https://huggingface.co/InternRobotics/VLAC)\n\n</div>\n\n## üöÄ Interactive Demo & Homepage\n\n<div align=\"center\">\n\n### [üéÆ **Try Interactive & Homepage**](https://vlac.intern-ai.org.cn/)\n> **Online Demo is available now in Homepage, Try as you like!!!**\n\n</div>\n\n<div align=\"center\">\n  <img src=\"data/title_banner-2.gif\" alt=\"VLAC banner\" width=\"800\"></img>\n</div>\n\n## VLAC\n\nVLAC is a general-purpose pair-wise critic and manipulation model which designed for real world robot reinforcement learning and data refinement. \n\nIt provides robust evaluation capabilities for task progress prediction and task completion verification base one images and task description.\n\nVLAC trained on 3000h+ human egocentric data, 1200h+ comprehensive public robotic manipulation data, and 15h+ self-collected manipulation data.\n\n## ‚ú® Key Features\n\n‚Ä¢ **Pair-wise comparison mechanism** for improved progressing dense critic accuracy, better recognition of state changes, and each step can be the start of the trajectory.\n\n‚Ä¢ **Multi-modal capabilities** - Supports process tracking, task completion judgment, task description estimation, visual question answering, and even embodied action output, equipped with VLA capabilities.\n\n‚Ä¢ **Flexible zero-shot and one-shot** - in-context capabilities, maintaining excellent performance across entities, scenarios, and tasks.\n\n‚Ä¢ **Human-task synesthesia** - Based on the ego4D human dataset, model understands common tasks and build synesthesia for real-world human tasks and embodied tasks.\n\n‚Ä¢ **Trajectory quality screening** - VLAC can evaluate the collected trajectories and filters out low score trajectories based on the VOC value and mask the action with negative pair-wise score, that is, data with low fluency and quality, improving the effect and efficiency of imitation learning.\n\n## Framework\n\n<div align=\"center\">\n  <img src=\"data/framework.png\" alt=\"VLAC Framework\" width=\"800\"/>\n</div>\n\n*The VLAC model is trained on a combination of comprehensive public robotic manipulation datasets, human demonstration data, self-collected manipulation data, and various image understanding datasets. Video data is processed into pair-wise samples to learn the different task progress between any two frames, supplemented with task descriptions and task completion evaluation to enable task progress understanding and action generation, as illustrated in the bottom-left corner. As shown in the diagram on the right, the model demonstrates strong generalization capabilities to new robots, scenarios, and tasks not covered in the training dataset. It can predict task progress and distinguish failure action or trajectory, providing dense reward feedback for real-world reinforcement learning and offering guidance for data refinement. Additionally, the model can directly perform manipulation tasks, exhibiting zero-shot capabilities to handle different scenarios.*\n\n## Performance\n\nDetails about the model's performance and evaluation metrics can be found in the [Homepage](https://vlac.intern-ai.org.cn/).\n\n## üõ†Ô∏è Installation\n\nTo install from source:\n```shell\ngit clone https://github.com/InternRobotics/VLAC.git\ncd VLAC\npip install -e .\n```\nRunning Environment:\n\n|              | Range        | Recommended | Notes                                     |\n| ------------ |--------------| ----------- | ----------------------------------------- |\n| python       | >=3.9        | 3.10        |                                           |\n| cuda         |              | cuda12      | No need to install if using CPU, NPU, MPS |\n| torch        | >=2.0        |             |                                           |\n| transformers | >=4.51       | 4.51.3      |                                           |\n| peft | >=0.15.2       |      |                                           |\n| ms-swift |        | 3.3      |                                           |\n\n\n## üöÄ Quick Start\n\n```python\nfrom evo_vlac import GAC_model\nfrom evo_vlac.utils.video_tool import compress_video\nimport os\n#Consistent with the web interface, the value and citic rewards of video input can be evaluated.\n\n\n#assign local model path\nmodel_path=\"set to your local model path\"\n#download model form https://huggingface.co/InternRobotics/VLAC\n\n#assign video path and task description\ntest_video='evo_vlac/examples/videos/pick-bowl-test.mp4'\nref_video='evo_vlac/examples/videos/pick-bowl-ref.mov'\ntask_description='Put up the bowl and place it back in the white storage box.'\n\n#init model\nCritic=GAC_model(tag='critic')\nCritic.init_model(model_path=model_path,model_type='internvl2',device_map=f'cuda:0')\nCritic.temperature=0.5\nCritic.top_k=1\nCritic.set_config()\nCritic.set_system_prompt()\n\n# transform video\ntest_video_compressed = os.path.join(os.path.dirname(test_video),\"test.mp4\")\n_,output_fps=compress_",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-16T18:45:41.603620"
  },
  {
    "basic_info": {
      "name": "botguard-reverse",
      "full_name": "dsekz/botguard-reverse",
      "owner": "dsekz",
      "description": "Botguard full reverse",
      "url": "https://github.com/dsekz/botguard-reverse",
      "clone_url": "https://github.com/dsekz/botguard-reverse.git",
      "ssh_url": "git@github.com:dsekz/botguard-reverse.git",
      "homepage": null,
      "created_at": "2025-09-15T01:12:51Z",
      "updated_at": "2025-09-16T17:17:23Z",
      "pushed_at": "2025-09-15T09:39:33Z"
    },
    "stats": {
      "stars": 26,
      "forks": 0,
      "watchers": 26,
      "open_issues": 0,
      "size": 210
    },
    "tech_info": {
      "language": null,
      "languages": {},
      "license": null,
      "topics": []
    },
    "content": {
      "readme": "Visit this link for the full reverse of Botguard: [cdn.csolver.net/botguard](https://cdn.csolver.net/botguard)\r\n\r\nThis project belong to Cypa.\r\n\r\n\r\n# ReCaptcha BotGuard\r\n\r\nBefore we begin the process of reversing BotGuard, let's start with some information.\r\n\r\nBotGuard is one of the most sophisticated antibots; it is known as the most difficult antibot to reverse.\r\nIt is used on many Google products, but each script serves a different purpose.\r\nThe one I present in this repository is ReCaptcha's BotGuard; they use it on their primary payload (`/reload`), and in ReCaptcha v2, they use it on the submit endpoint as well (`/userverify`)\r\n\r\nNow that you know a little about where it is used, let's understand what exactly it is.\r\nBotGuard is a script that generates a token, which is used to verify the validity of requests to Google's servers. In ReCaptcha, it is not used as a fingerprint, which many believe it is; instead, it is used to verify that the request is from a browser, as it can only be generated if executed by a browser.\r\nWhile this may seem like a \"weak\" approach, it is actually very sophisticated; there are many checks against falsey environments, so patching and sandboxing are no longer viable options. If you do not wish to reverse it but still want to generate it, you will need a browser emulator that can bypass its detection. Tools such as Puppeteer or Playwright can be helpful for this purpose. They emulate a real browser so that the environment checks won‚Äôt be effective against them. Due to this, you won‚Äôt need to do much setup to create a valid token generator as long as it‚Äôs a real browser.\r\n\r\n# What Is a VM?\r\n\r\nBefore we delve into the fun stuff, we need to understand what a VM is and how we can reverse what we don't know. Let's talk about what it is and how it operates. A VM is designed to emulate a CPU running a binary program, but it's custom, written in JavaScript, and the opcodes are often not standard; they usually include some custom ones that aren't available on a CPU. Now, in the case of VMs, it uses this custom \"CPU\" to execute a \"binary\" that is the given bytecode. It then decodes or decrypts it, depending on the VM, and begins to implement it. This is one of the most powerful obfuscation techniques at this time; it beats any standard CFF (control flow flattening) by a lot, since you need to write a whole disassembler and decompiler to understand what it does, then, after that, you still have to find and reverse the algorithm that was compiled in the first place.\r\n\r\nWith VMs, there are multiple types; we have register-based, which emulate that of a modern CPU. The browser you're using? That binary is being run through your register-based CPU. Stack-based is based on older CPUs and JVMs (Java VMs) use stack-based; WASM (WebAssembly) uses stack-based. That should help you understand what a VM is and how it works. Now that we've got that out of the way, we can continue reversing BotGuard.\r\n\r\n# What Makes It Difficult?\r\n\r\nMany people have tried and failed to reverse this antibot, and because of this, it has a reputation for being the \"hardest\" antibot, but what exactly causes such trouble?\r\nThe script isn't like any other script, where you can debug a little, understand the algorithm, and translate it to a production language, and you're done.\r\nThis script utilizes a VM (Virtual Machine), which completely virtualizes the code, making it impossible to locate the algorithm or debug into it, unlike many other scripts.\r\nYou need to write a **disassembler** and **decompiler** for this custom bytecode interpreter, which makes it even harder. Even then, if you have experience with VMs, this shouldn't be so hard. **WRONG!** This VM has much more to it than standard ones you see in other VMs like **Kasada** or **TikTok**. They utilize numerous special features, including register encryptions and flow-changing opcodes.\r\n\r\nSo what? It has more features, but we can still debug to figure it out! Well, they've got some **anti-debug** measures which make it very hard to debug without the script throwing you onto a random path with incorrect flow.\r\nThe most obvious check they have is timing; they use `performance.now()` along with `+Date.now()` to check the time. This allows the script to determine if a breakpoint has been placed; if the time is too high, the user was likely broken previously. In the screenshot below, you can see how it changes the **seed**, which is used to determine the next byte, so it diverts the program when debugging is detected. \r\n\r\n# Anti-Debugging Methods\r\n\r\nNow, the `K` variable is the VM context; it holds critical globals the VM uses, including the **seed**, which in this case is `K.U`. Notice how it has some quick math operations to determine an amount to xor the seed by? This should remain at `0`, and if it isn't, the value changes; this also causes the following bytes in the program to change, making it a very effective anti-debug method.\r\n\r\n<img src=\"images/photo_1.jpg\" ",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-16T18:45:42.862821"
  },
  {
    "basic_info": {
      "name": "whaileys",
      "full_name": "canove/whaileys",
      "owner": "canove",
      "description": "Send and receive messages as WhatsApp web on Nodejs",
      "url": "https://github.com/canove/whaileys",
      "clone_url": "https://github.com/canove/whaileys.git",
      "ssh_url": "git@github.com:canove/whaileys.git",
      "homepage": null,
      "created_at": "2025-09-15T01:24:06Z",
      "updated_at": "2025-09-16T16:06:32Z",
      "pushed_at": "2025-09-16T12:26:46Z"
    },
    "stats": {
      "stars": 25,
      "forks": 17,
      "watchers": 25,
      "open_issues": 3,
      "size": 11235
    },
    "tech_info": {
      "language": "JavaScript",
      "languages": {
        "JavaScript": 8654722,
        "TypeScript": 414763,
        "Shell": 401
      },
      "license": null,
      "topics": []
    },
    "content": {
      "readme": "# Baileys - Typescript/Javascript WhatsApp Web API\n\nVersion based on [baileys 4.x](https://github.com/canove/whaileys/commit/6663357a37c6e295338cd82ecff5cabceeee1736). It should be compatible with v6.x of [whiskeysockets/baileys](https://github.com/WhiskeySockets/Baileys)\n\nThanks to all [whiskeysockets/baileys](https://github.com/WhiskeySockets/Baileys) contributors for their work along the way.\n\n## Why another fork of baileys?\n\n- Its not a new version. It has been used in production for over 3 years, with over 5,000 active connections.\n- Has been maintained mainly by [whaticket](https://github.com/canove/whaticket-community) creator, [canove](https://github.com/canove).\n- This version aims to maintain **simplicity and stability** over functionality.\n- Every change will be thoroughly tested before release. The main branch will always be stable.\n\n## Known limitations of this version\n\nThe focus of this version has always been to maintain the core functionality (one-on-one chat messaging). Therefore, some group features may require optimization.\n\nSupport for other features (Meta AI, catalog, tags, adding and removing contacts, etc.) may not be available and will be released in the future only if it doesn't add excessive maintenance complexity to the code.\n\n## Old description (to be updated)\n\nBaileys does not require Selenium or any other browser to be interface with WhatsApp Web, it does so directly using a **WebSocket**. Not running Selenium or Chromimum saves you like **half a gig** of ram :/\n\nBaileys supports interacting with the multi-device & web versions of WhatsApp.\n\nThank you to [@pokearaujo](https://github.com/pokearaujo/multidevice) for writing his observations on the workings of WhatsApp Multi-Device. Also, thank you to [@Sigalor](https://github.com/sigalor/whatsapp-web-reveng) for writing his observations on the workings of WhatsApp Web and thanks to [@Rhymen](https://github.com/Rhymen/go-whatsapp/) for the **go** implementation.\n\nBaileys is type-safe, extensible and simple to use. If you require more functionality than provided, it's super easy to write an extension. More on this [here](#WritingCustomFunctionality).\n\n## Example\n\nDo check out & run [example.ts](https://github.com/canove/whaileys/blob/master/Example/example.ts) to see an example usage of the library.\nThe script covers most common use cases.\nTo run the example script, download or clone the repo and then type the following in a terminal:\n\n1. `cd path/to/Baileys`\n2. `yarn`\n3. `yarn example`\n\n## Install\n\nUse the stable version:\n\n```\nnpm i whaileys\n```\n\nUse the edge version (no guarantee of stability, but latest fixes + features)\n\n```\nnpm i github:canove/whaileys\n```\n\nThen import your code using:\n\n```ts\nimport makeWASocket from \"whaileys\";\n```\n\n## Unit Tests\n\nTODO\n\n## Connecting\n\n```ts\nimport makeWASocket, { DisconnectReason } from \"whaileys\";\nimport { Boom } from \"@hapi/boom\";\n\nasync function connectToWhatsApp() {\n  const sock = makeWASocket({\n    // can provide additional config here\n    printQRInTerminal: true\n  });\n  sock.ev.on(\"connection.update\", update => {\n    const { connection, lastDisconnect } = update;\n    if (connection === \"close\") {\n      const shouldReconnect =\n        (lastDisconnect.error as Boom)?.output?.statusCode !==\n        DisconnectReason.loggedOut;\n      console.log(\n        \"connection closed due to \",\n        lastDisconnect.error,\n        \", reconnecting \",\n        shouldReconnect\n      );\n      // reconnect if not logged out\n      if (shouldReconnect) {\n        connectToWhatsApp();\n      }\n    } else if (connection === \"open\") {\n      console.log(\"opened connection\");\n    }\n  });\n  sock.ev.on(\"messages.upsert\", m => {\n    console.log(JSON.stringify(m, undefined, 2));\n\n    console.log(\"replying to\", m.messages[0].key.remoteJid);\n    await sock.sendMessage(m.messages[0].key.remoteJid!, {\n      text: \"Hello there!\"\n    });\n  });\n}\n// run in main file\nconnectToWhatsApp();\n```\n\nIf the connection is successful, you will see a QR code printed on your terminal screen, scan it with WhatsApp on your phone and you'll be logged in!\n\n**Note:** install `qrcode-terminal` using `yarn add qrcode-terminal` to auto-print the QR to the terminal.\n\n**Note:** the code to support the legacy version of WA Web (pre multi-device) has been removed in v5. Only the standard multi-device connection is now supported. This is done as WA seems to have completely dropped support for the legacy version.\n\n## Configuring the Connection\n\nYou can configure the connection by passing a `SocketConfig` object.\n\nThe entire `SocketConfig` structure is mentioned here with default values:\n\n```ts\ntype SocketConfig = {\n  /** the WS url to connect to WA */\n  waWebSocketUrl: string | URL;\n  /** Fails the connection if the socket times out in this interval */\n  connectTimeoutMs: number;\n  /** Default timeout for queries, undefined for no timeout */\n  defaultQueryTimeoutMs: number | undefined;\n  /** ping-pong interval for WS connection */\n  keepAliveIntervalMs: ",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-16T18:45:44.150439"
  }
]