[
  {
    "basic_info": {
      "name": "MiMo-Audio",
      "full_name": "XiaomiMiMo/MiMo-Audio",
      "owner": "XiaomiMiMo",
      "description": null,
      "url": "https://github.com/XiaomiMiMo/MiMo-Audio",
      "clone_url": "https://github.com/XiaomiMiMo/MiMo-Audio.git",
      "ssh_url": "git@github.com:XiaomiMiMo/MiMo-Audio.git",
      "homepage": null,
      "created_at": "2025-09-19T00:46:49Z",
      "updated_at": "2025-09-19T03:45:56Z",
      "pushed_at": "2025-09-19T01:08:55Z"
    },
    "stats": {
      "stars": 80,
      "forks": 5,
      "watchers": 80,
      "open_issues": 0,
      "size": 6024
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 223404
      },
      "license": "Apache License 2.0",
      "topics": []
    },
    "content": {
      "readme": "<div align=\"center\">\n  <picture>\n    <source srcset=\"https://github.com/XiaomiMiMo/MiMo-VL/raw/main/figures/Xiaomi_MiMo_darkmode.png?raw=true\" media=\"(prefers-color-scheme: dark)\">\n    <img src=\"https://github.com/XiaomiMiMo/MiMo-VL/raw/main/figures/Xiaomi_MiMo.png?raw=true\" width=\"60%\" alt=\"Xiaomi-MiMo\" />\n  </picture>\n</div>\n\n<h3 align=\"center\">\n  <b>\n    <span>━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n    <br/>\n    MiMo Audio: Audio Language Models are Few-Shot Learners\n    <br/>\n    <span>━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n    <br/>\n  </b>\n</h3>\n\n<br/>\n\n<div align=\"center\" style=\"line-height: 1;\">\n  |\n  <a href=\"https://huggingface.co/collections/XiaomiMiMo/mimo-audio-68cc7202692c27dae881cce0\" target=\"_blank\">🤗 HuggingFace</a>\n  &nbsp;|\n  <a href=\"https://github.com/XiaomiMiMo/MiMo-Audio/blob/main/MiMo-Audio-Technical-Report.pdf\" target=\"_blank\">📄 Paper</a>\n  &nbsp;|\n  <a href=\"https://xiaomimimo.github.io/MiMo-Audio-Demo\" target=\"_blank\">📰 Blog</a>\n  &nbsp;|\n  <a href=\"https://huggingface.co/spaces/XiaomiMiMo/mimo_audio_chat\" target=\"_blank\">🔥 Online Demo</a>\n  &nbsp;|\n  <a href=\"https://github.com/XiaomiMiMo/MiMo-Audio-Eval\" target=\"_blank\">📊 MiMo-Audio-Eval</a>\n  &nbsp;|\n\n  <br/>\n</div>\n\n<br/>\n\n## Introduction\n\nExisting audio language models typically rely on task-specific fine-tuning to accomplish particular audio tasks. In contrast, humans are able to generalize to new audio tasks with only a few examples or simple instructions. GPT-3 has shown that scaling next-token prediction pretraining enables strong generalization capabilities in text, and we believe this paradigm is equally applicable to the audio domain. By scaling MiMo-Audio's pretraining data to over one hundred million of hours, we observe the emergence of few-shot learning capabilities across a diverse set of audio tasks. We develop a systematic evaluation of these capabilities and find that MiMo-Audio-7B-Base achieves SOTA performance on both speech intelligence and audio understanding benchmarks among open-source models. Beyond standard metrics, MiMo-Audio-7B-Base generalizes to tasks absent from its training data, such as voice conversion, style transfer, and speech editing. MiMo-Audio-7B-Base also demonstrates powerful speech continuation capabilities, capable of generating highly realistic talk shows, recitations, livestreaming and debates. At the post-training stage, we curate a diverse instruction-tuning corpus and introduce thinking mechanisms into both audio understanding and generation. MiMo-Audio-7B-Instruct achieves open-source SOTA on audio understanding benchmarks, spoken dialogue benchmarks and instruct-TTS evaluations, approaching or surpassing closed-source models.\n\n\n![Results](assets/Results.png)\n\n\n\n## Architecture\n### MiMo-Audio-Tokenizer\nMiMo-Audio-Tokenizer is a 1.2B-parameter Transformer operating at 25 Hz. It employs an eight-layer RVQ stack to generate 200 tokens per second. By jointly optimizing semantic and reconstruction objectives, we train MiMo-Audio-Tokenizer from scratch on a 10-million-hour corpus, achieving superior reconstruction quality and facilitating downstream language modeling.\n\n![Tokenizer](assets/tokenizer.png)\n\nMiMo-Audio couples a patch encoder, an LLM, and a patch decoder to improve modeling efficiency for high-rate sequences and bridge the length mismatch between speech and text. The patch encoder aggregates four consecutive time steps of RVQ tokens into a single patch, downsampling the sequence to a 6.25 Hz representation for the LLM. The patch decoder autoregressively generates the full 25 Hz RVQ token sequence via a delayed-generation scheme.\n### MiMo-Audio\n![Arch](assets/architecture.png)\n\n##  Explore MiMo-Audio Now! 🚀🚀🚀\n- 🎧 **Try the Hugging Face demo:** [MiMo-Audio Demo](https://huggingface.co/spaces/XiaomiMiMo/mimo_audio_chat)\n- 📰 **Read the Official Blog:** [MiMo-Audio Blog](https://xiaomimimo.github.io/MiMo-Audio-Demo)\n- 📄 **Dive into the Technical Report:** [MiMo-Audio Technical Report](https://github.com/XiaomiMiMo/MiMo-Audio/blob/main/MiMo-Audio-Technical-Report.pdf)\n\n\n## Model Download\n| Models   | 🤗 Hugging Face |\n|-------|-------|\n| MiMo-Audio-Tokenizer | [XiaomiMiMo/MiMo-Audio-Tokenizer](https://huggingface.co/XiaomiMiMo/MiMo-Audio-Tokenizer) |\n| MiMo-Audio-7B-Base | [XiaomiMiMo/MiMo-Audio-7B-Base](https://huggingface.co/XiaomiMiMo/MiMo-Audio-7B-Base) |\n| MiMo-Audio-7B-Instruct | [XiaomiMiMo/MiMo-Audio-7B-Instruct](https://huggingface.co/XiaomiMiMo/MiMo-Audio-7B-Instruct) |\n\n\n\n## Getting Started\n\nSpin up the MiMo-Audio demo in minutes with the built-in Gradio app.\n\n### Installation\n``` sh\ngit clone https://github.com/XiaomiMiMo/MiMo-Audio.git\ncd MiMo-Audio\npip install -e .\n```\n### Run the demo\n``` sh\npython run_mimo_audio.py\n```\n\nThis launches a local Gradio interface where you can try MiMo-Audio interactively.\n\n![Demo UI](assets/demo_ui.jpg)\n\nEnter the local paths for `MiMo-Audio-Tokenizer` and `MiMo-Audio-7B-Instruct`, then enjoy the full functionality ",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-19T03:46:00.451436"
  },
  {
    "basic_info": {
      "name": "ai-humanizer-api",
      "full_name": "HuzefaUsama25/ai-humanizer-api",
      "owner": "HuzefaUsama25",
      "description": "AI Humanizer API converts AI text into high quality undetectable human-like writing. Bypasses Turnitin, GPTZero, Originality.ai, CopyLeaks. Fast, natural, high quality output for students, marketers, and content creators",
      "url": "https://github.com/HuzefaUsama25/ai-humanizer-api",
      "clone_url": "https://github.com/HuzefaUsama25/ai-humanizer-api.git",
      "ssh_url": "git@github.com:HuzefaUsama25/ai-humanizer-api.git",
      "homepage": null,
      "created_at": "2025-09-18T08:21:03Z",
      "updated_at": "2025-09-18T20:11:30Z",
      "pushed_at": "2025-09-18T08:37:02Z"
    },
    "stats": {
      "stars": 40,
      "forks": 0,
      "watchers": 40,
      "open_issues": 0,
      "size": 4
    },
    "tech_info": {
      "language": null,
      "languages": {},
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "# AI Humanizer API\n\nThe **AI Humanizer API** is the **best and only truly effective AI humanizer API**. It transforms AI-generated text into **natural, human-like writing** that passes every major AI detector without losing meaning, tone, or quality. Unlike other tools that produce awkward or robotic rewrites, the AI Humanizer API generates fluent, authentic, and **indistinguishable human text**.\n\nThis repository provides examples, quick integration steps, and developer resources for the [WriteHybrid AI Humanizer API](https://writehybrid.com/api-docs).\n\n---\n\n## Table of Contents\n\n- [Why Use the AI Humanizer API](#why-use-the-ai-humanizer-api)  \n- [Key Features](#key-features)  \n- [Quick Start](#quick-start)  \n  - [Authentication](#authentication)  \n  - [Base URL](#base-url)  \n  - [Example Request (cURL)](#example-request-curl)  \n  - [Example Response](#example-response)  \n- [Parameters](#parameters)  \n- [Pricing](#pricing)  \n- [Who Is It For](#who-is-it-for)  \n- [Why Trust This API](#why-trust-this-api)  \n- [Get Started](#get-started-with-ai-humanizer-api)  \n\n---\n\n## Why Use the AI Humanizer API\n\nAI text often gets flagged by detectors such as **Turnitin, GPTZero, Originality.ai, Copyleaks, Sapling, and more**. Getting flagged can result in:\n\n- SEO penalties and lower Google rankings  \n- Academic or publishing rejection  \n- Damaged trust with clients, readers, or customers  \n\nThe **AI Humanizer API is the only solution that reliably bypasses every major detector** while preserving the original context and style. It is fast, scalable, and trusted by real users.\n\n---\n\n## Key Features\n\n- **Only Useful Humanizer API**: Every other tool falls short; this one works.  \n- **Detector-Proof**: Passes Turnitin, GPTZero, Originality.ai, Copyleaks, and more.  \n- **Transparent Billing**: 1 credit = 1 word.  \n- **High Quality**: Meaning and nuance are preserved.  \n- **Fast**: Humanize thousands of words in seconds.  \n- **Natural Output**: Fluent, human-like text every time.  \n- **Developer Friendly**: RESTful API with simple JSON responses.  \n- **Scalable**: From indie projects to enterprise-grade platforms.  \n\n---\n\n## Quick Start\n\n### Authentication\n\nAll requests require an API key. Add it to the `Authorization` header as a Bearer token.\n\n### Base URL\n\n[https://whbserver.com/api/v1](https://whbserver.com/api/v1)\n\n\n### Example Request (cURL)\n\n```bash\ncurl -X POST https://whbserver.com/api/v1/humanizer/ \\\n  -H \"Authorization: Bearer YOUR_API_KEY\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n        \"text\": \"AI-generated text goes here...\",\n        \"complexity\": \"medium\",\n        \"purpose\": \"general\"\n      }'\n````\n\n### Example Response\n\n```json\n{\n  \"success\": true,\n  \"humanized_text\": \"In today’s fast-paced world, technology continues to evolve at incredible speed...\",\n  \"original_length\": 245,\n  \"humanized_length\": 263,\n  \"processing_time\": 1.1,\n  \"detection_score\": 0.01,\n  \"credits_used\": 245\n}\n```\n\n---\n\n## Parameters\n\n| Field        | Type   | Description                                                      |\n| ------------ | ------ | ---------------------------------------------------------------- |\n| `text`       | string | The AI-generated text you want to humanize                       |\n| `complexity` | string | Output style: `simple`, `medium`, `complex`                      |\n| `purpose`    | string | Rewrite context: `general`, `seo`, `academic`, `marketing`, etc. |\n\n---\n\n## Pricing\n\n* **Starter** – 20,000 credits/month (20,000 words), email support\n* **Pro** – 60,000 credits/month (60,000 words), advanced tones, faster processing\n* **Agency** – 150,000 credits/month (150,000 words), team access, priority support\n* **Enterprise** – Custom high-volume solutions\n\n**Note:** 1 credit = 1 word.\n\nFull details: [writehybrid.com/api-docs](https://writehybrid.com/api-docs)\n\n---\n\n## Who Is It For?\n\n* **SEO Teams** – Safely scale content that ranks without AI penalties.\n* **Students & Academics** – Submit detection-proof work that reads as authentic.\n* **Content Agencies** – Deliver client-ready copy at scale with no manual rewrites.\n* **SaaS Builders** – Add humanization as a core feature inside your product.\n* **Freelancers** – Ensure client work is never penalized for AI generation.\n\n---\n\n## Why Trust This API\n\n* **Proven**: Successfully bypasses every major AI detector.\n* **Transparent**: 1 credit = 1 word, no hidden rules.\n* **Reliable**: Built on robust, scalable infrastructure.\n* **Trusted**: Used daily by agencies, startups, and independent creators worldwide.\n\n---\n\n## Get Started with AI Humanizer API\n\nStop wasting time on tools that don’t work. The **AI Humanizer API is the only real solution** for creating undetectable, human-like text at scale.\n\n👉 [**Get your API key today**](https://writehybrid.com/api-docs) and start humanizing AI text instantly.\n\n\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-19T03:46:01.589027"
  },
  {
    "basic_info": {
      "name": "RamTorch",
      "full_name": "lodestone-rock/RamTorch",
      "owner": "lodestone-rock",
      "description": "RAM is all you need",
      "url": "https://github.com/lodestone-rock/RamTorch",
      "clone_url": "https://github.com/lodestone-rock/RamTorch.git",
      "ssh_url": "git@github.com:lodestone-rock/RamTorch.git",
      "homepage": null,
      "created_at": "2025-09-18T12:16:49Z",
      "updated_at": "2025-09-19T03:09:41Z",
      "pushed_at": "2025-09-18T14:44:33Z"
    },
    "stats": {
      "stars": 37,
      "forks": 1,
      "watchers": 37,
      "open_issues": 0,
      "size": 14
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 9145
      },
      "license": "Apache License 2.0",
      "topics": []
    },
    "content": {
      "readme": "# RamTorch\n\n**RAM is All You Need** - A PyTorch library for memory-efficient deep learning that enables training and inference of large models that don't fit in GPU memory.\n\n## Overview\n\nRamTorch provides CPU-GPU hybrid implementations of neural network components that keep parameters in CPU memory and transfer them to GPU on-demand. This approach dramatically reduces GPU memory usage while maintaining computational efficiency through asynchronous CUDA streams and intelligent batching.\n\n## Key Features\n\n- **Memory-Efficient Linear Layers**: CPU-stored parameters with on-demand GPU transfer\n- **Asynchronous CUDA Streams**: Overlap computation with data transfer for minimal latency\n- **ZeRO-1 Optimizer Support**: Distributed optimizer state sharding across multiple GPUs\n- **Drop-in Replacement**: Compatible with existing PyTorch code\n- **Configurable Transfer Throttling**: Controllable memory usage\n\n## Installation\n\n```bash\npip install ramtorch\n```\n\nOr install from source:\n\n```bash\ngit clone https://github.com/lodestone-rock/RamTorch.git\ncd RamTorch\npip install -e .\n```\n\n## Quick Start\n\n### Basic Usage\n\nReplace `torch.nn.Linear` with `ramtorch.modules.Linear` for automatic memory optimization:\n\n```python\nimport torch\nimport ramtorch.modules as ram_modules\n\n# Standard PyTorch approach (high GPU memory usage)\n# linear = torch.nn.Linear(1000, 1000)\n\n# RamTorch approach (low GPU memory usage)\nlinear = ram_modules.Linear(1000, 1000, device=\"cuda\")\n\n# Use exactly like a normal PyTorch layer\nx = torch.randn(32, 1000, device=\"cuda\")\noutput = linear(x)  # Parameters automatically transferred from CPU to GPU\n```\n\n### Building Models\n\n```python\nimport torch.nn as nn\nimport ramtorch.modules as ram_modules\n\nclass MemoryEfficientModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layers = nn.Sequential(\n            ram_modules.Linear(1000, 2000),\n            nn.ReLU(),\n            ram_modules.Linear(2000, 2000),\n            nn.ReLU(),\n            ram_modules.Linear(2000, 100)\n        )\n    \n    def forward(self, x):\n        return self.layers(x)\n\nmodel = MemoryEfficientModel()\n```\n\n### ZeRO-1 Optimizer Sharding\n\nFor distributed training with optimizer state sharding:\n\n```python\nimport torch.distributed as dist\nfrom ramtorch.zero1 import create_zero_param_groups, broadcast_zero_params\n\n# Initialize distributed training\ndist.init_process_group(backend='nccl')\nmodel = YourModel()\nall_params = list(model.parameters())\nrank = dist.get_rank()\nworld_size = dist.get_world_size()\n\n# Create ZeRO-1 sharded optimizer\nparam_groups = [{'params': all_params, 'lr': 1e-3, 'weight_decay': 0.01}]\nsharded_groups, owner_ranks = create_zero_param_groups(param_groups, rank, world_size)\noptimizer = torch.optim.AdamW(sharded_groups)\n\n# Scheduler works normally with sharded optimizer\nscheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n\n# Training loop\nfor epoch in range(num_epochs):\n    for batch in dataloader:\n        # Forward/backward with gradient accumulation\n        for micro_batch in split_batch(batch):\n            loss = model(micro_batch)\n            loss.backward()\n\n        # All-reduce gradients across ranks (you need to implement this)\n        all_reduce_gradients(all_params)\n        \n        # Each rank updates only its owned parameters\n        optimizer.step()\n        \n        # Broadcast updated parameters from owners to all ranks\n        broadcast_zero_params(all_params, owner_ranks)\n        \n        model.zero_grad()\n        scheduler.step()\n```\n## Configuration\n\n### Environment Variables\n\n- `MAX_INFLIGHT`: Maximum number of concurrent CPU-to-GPU transfers (default: 2)\n  ```bash\n  export MAX_INFLIGHT=4  # Allow more concurrent transfers\n  ```\n\n### Transfer Stream Management\n\nRamTorch automatically manages CUDA streams for optimal performance. The library uses a dedicated transfer stream to overlap data movement with computation.\n\n## Performance Considerations\n\n### When to Use RamTorch\n\n**Best suited for:**\n- Large models that don't fit in GPU memory\n- Inference scenarios with memory constraints\n- Training with limited GPU memory but abundant CPU memory\n- Distributed training with many parameters\n\n**Less suitable for:**\n- Small models that fit comfortably in GPU memory\n- Scenarios where CPU-GPU bandwidth is the bottleneck\n- Real-time applications requiring minimal latency\n\n### Optimization Tips\n\n1. **Use Larger Batch Sizes**: Helps amortize transfer costs\n2. **Configure MAX_INFLIGHT**: Tune based on your GPU memory availability\n3. **Mixed Precision**: Combine with `torch.cuda.amp` for additional memory savings\n4. **Strategic Placement**: Use RamTorch layers for the largest components only\n\n## Architecture\n\n### CPU Bouncing Linear Layer\n\n\n1. Stores parameters on CPU memory (with `share_memory_()` for multiprocessing)\n2. Asynchronously transfers weights to GPU during forward pass\n3. Uses CUDA events for proper stream synchronization\n4. Automatically throttles transfers to preve",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-19T03:46:02.691512"
  },
  {
    "basic_info": {
      "name": "awesome-a2a-hub",
      "full_name": "questflowai/awesome-a2a-hub",
      "owner": "questflowai",
      "description": "A curated list of awesome Agent2Agent (A2A) protocol agents, tools, and resources, with a focus on the https://a2a.build. This repository is supported by https://questflow.ai.",
      "url": "https://github.com/questflowai/awesome-a2a-hub",
      "clone_url": "https://github.com/questflowai/awesome-a2a-hub.git",
      "ssh_url": "git@github.com:questflowai/awesome-a2a-hub.git",
      "homepage": "",
      "created_at": "2025-09-18T03:54:09Z",
      "updated_at": "2025-09-19T03:37:45Z",
      "pushed_at": "2025-09-18T06:08:39Z"
    },
    "stats": {
      "stars": 36,
      "forks": 1,
      "watchers": 36,
      "open_issues": 0,
      "size": 13
    },
    "tech_info": {
      "language": null,
      "languages": {},
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "# Awesome A2A Hub [![Awesome](https://awesome.re/badge.svg)](https://awesome.re)\n\nA curated list of awesome Agent2Agent (A2A) protocol agents, tools, and resources, with a focus on the [A2A Hub](https://a2a.build). This repository is supported by [Questflow](https://questflow.ai).\n\n## What is A2A?\n\n[A2A](https://github.com/google/A2A) (Agent2Agent) is an open protocol created by Google that enables different AI agents to communicate and collaborate with each other using a standardized interface. The protocol allows agents to discover capabilities, submit tasks for execution, monitor task progress, and receive task results in a unified way.\n\n## What is A2A Hub?\n\nThe [A2A Hub](https://a2a.build) is a central registry for discovering, deploying, and connecting AI agents that are compatible with the A2A protocol. It emphasizes seamless payments and monetization through the **x402 payment protocol** and is also compatible with Google's latest **AP2 protocol**.\n\n## Featured Agents on A2A Hub\n\nHere is a list of agents available on the A2A Hub. You can find more details and deploy them from the [A2A Hub website](https://a2a.build/agents).\n\n| Agent Name | Description | Version |\n| :--- | :--- | :--- |\n| [coingecko(Pro)](https://a2a.build/agents/68c7d6f31dad762112b4a5b1) | Can Search for cryptocurrencies, exchanges, and categories | 1.0.2 |\n| [Spec Token Agent](https://a2a.build/agents/68c770531dad762112b47394) | Spec Token Agent | 1.0.4 |\n| [GoPlus](https://a2a.build/agents/68c76fb71dad762112b47315) | GoPlus | 1.0.0 |\n| [Sim Dune](https://a2a.build/agents/68c76f071dad762112b471f7) | Sim Dune | 1.0.1 |\n| [Alpha Vantage Stock Market](https://a2a.build/agents/68c26db13367242ab91229d5) | Alpha Vantage Inc. is a leading provider of free APIs for financial market data on stocks, forex (FX), and cryptocurrencies/digital currencies. | 1.0.0 |\n| [Amap Maps](https://a2a.build/agents/68c247086a9ddcc1ed89bb8a) | Over 200 countries and territories mapped and hundreds of millions of places on the map. | 1.0.1 |\n| [Perplexity AI-powered Answer Engine](https://a2a.build/agents/68c23e8b3367242ab9121139) | Perplexity is a free AI-powered answer engine that provides accurate, trusted, and real-time answers to any question. | 1.0.0 |\n| [ElevenLabs AI Voice Generator](https://a2a.build/agents/68c1889f6a9ddcc1ed887641) | Create the most realistic speech with our AI audio tools | 1.0.0 |\n| [exa search](https://a2a.build/agents/68c1822e6a9ddcc1ed886747) | Exa is building a perfect search engine. | 1.0.0 |\n| [Nano Banana Agent](https://a2a.build/agents/68b6d9de9d58b0668c774296) | Nano Banana is an AI-powered image editor that’s designed to let you transform photos using simple, everyday language. | 1.0.1 |\n| [BWEnews](https://a2a.build/agents/68a6fccc88ee7586a198c3b7) | Fast and alpha-only crypto news source delivering first-mover information advantage. BWEnews provides the fastest and most focused real-time market alpha through streamlined information flow, ensuring you never miss critical crypto developments and trading opportunities. | 1.0.0 |\n| [Gloria](https://a2a.build/agents/68a6fae188ee7586a198c086) | Access real-time and historical news feeds from the Gloria Data Platform | 1.0.1 |\n| [parallel](https://a2a.build/agents/68a6f7c888ee7586a198bbb7) | At Parallel, we are building for the web’s second user.\\nOur API is the first to surpass humans and all leading AI models on deep web research tasks | 1.0.1 |\n| [aixbt](https://a2a.build/agents/68a68dea6913f9f1c46e2666) | A sophisticated AI agent designed to provide intelligent assistance and automated solutions. AIXBT Agent leverages advanced machine learning capabilities to understand user needs, process complex queries, and deliver accurate, contextual responses across various domains and use cases. | 1.0.1 |\n| [Financial Datasets](https://a2a.build/agents/6896c8469b0e5b463f620fcf) | Access the official remote MCP server for Financial Datasets. | 1.0.0 |\n| [Financial Dataset](https://a2a.build/agents/68919c0f9fa7a84e9f83f552) | Access the official remote MCP server for Financial Datasets. | 1.0.0 |\n| [ACP Client Agent](https://a2a.build/agents/688057886817cae0ea3b7674) | ACP Client Agent | 1.0.1 |\n| [Virtuals Agent](https://a2a.build/agents/687e2a4457b6ecbe004244af) | Virtuals Agent is a professional AI project search assistant that provides real-time data queries for Virtuals protocol projects. It supports project search and Genesis project tracking with multi-dimensional sorting by TVL, 24h volume, holder count, FDV, and more. Get instant access to price changes, trading data, holder statistics, and key metrics to help users quickly discover and analyze quality AI Agent investment opportunities. | 1.0.2 |\n| [Twitter Agent](https://a2a.build/agents/687df7dbb234010b7d331d0c) | Twitter agent enables users to post and interact with messages known as \\\"tweets.\\\" Can write a tweet, listen to twitter user post new tweet, query or search user latest tweets, get tweet detail, listen to a speci",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-19T03:46:03.828417"
  },
  {
    "basic_info": {
      "name": "obex",
      "full_name": "dis0rder0x00/obex",
      "owner": "dis0rder0x00",
      "description": "Obex – Blocking unwanted DLLs in user mode",
      "url": "https://github.com/dis0rder0x00/obex",
      "clone_url": "https://github.com/dis0rder0x00/obex.git",
      "ssh_url": "git@github.com:dis0rder0x00/obex.git",
      "homepage": null,
      "created_at": "2025-09-18T15:43:10Z",
      "updated_at": "2025-09-19T03:45:10Z",
      "pushed_at": "2025-09-18T15:47:17Z"
    },
    "stats": {
      "stars": 34,
      "forks": 4,
      "watchers": 34,
      "open_issues": 0,
      "size": 234
    },
    "tech_info": {
      "language": "C",
      "languages": {
        "C": 14552
      },
      "license": "BSD 3-Clause \"New\" or \"Revised\" License",
      "topics": []
    },
    "content": {
      "readme": "# Obex - DLL Blocking\n\n**Obex** is a PoC tool/technique that can be used to prevent unwanted modules (e.g., EDR or monitoring libraries) from being loaded into a newly started process during process initialization or at runtime.\n\n## Features\n- Spawns any process with arguments under debug control.\n- Blocks a configurable list of DLLs by name.\n- Works both for startup DLLs and dynamically loaded DLLs (`LoadLibrary*`).\n- Written in plain C with no external dependencies.\n\n## Usage\n```\nobex.exe \"<command with args>\" [dll1.dll,dll2.dll,...]\n```\n- If no DLL list is provided, a default blocklist is used (at the time of writing just `amsi.dll`).\n- DLL names are case-insensitive.\n\n## How Does It Work?\nBesides parsing cli arguments the PoC does the following (in a rough overview):\n![](./images/flow.png)\n\nFor deeper understanding check code (obviously) or contact me on discord or [twitter](https://x.com/dis0rder_0x00).\n## Screenshot\nThe screenshot shows `obex` spawning `powershell.exe` with the default blocklist (only `amsi.dll`).\nAdditionally you can see the spawned process’s module list to verify that `amsi.dll` was not loaded.\n\n![](./images/1.png)\n\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-19T03:46:04.925174"
  },
  {
    "basic_info": {
      "name": "Telegram-Adding-Users",
      "full_name": "duongtsa/Telegram-Adding-Users",
      "owner": "duongtsa",
      "description": "An automation for the process of adding communitymembers to any target of your choice. Simple to use and up to date!",
      "url": "https://github.com/duongtsa/Telegram-Adding-Users",
      "clone_url": "https://github.com/duongtsa/Telegram-Adding-Users.git",
      "ssh_url": "git@github.com:duongtsa/Telegram-Adding-Users.git",
      "homepage": "",
      "created_at": "2025-09-18T11:54:18Z",
      "updated_at": "2025-09-18T16:13:09Z",
      "pushed_at": "2025-09-18T11:56:56Z"
    },
    "stats": {
      "stars": 23,
      "forks": 0,
      "watchers": 23,
      "open_issues": 0,
      "size": 19779
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 56
      },
      "license": null,
      "topics": [
        "adders",
        "memberss-scri",
        "scraper",
        "telegram",
        "telegram-add",
        "telegram-add-user",
        "telegram-add-users",
        "telegram-adding-members",
        "telegram-channel-clone",
        "telegram-copy-channel",
        "telegram-copy-members",
        "telegram-copy-users",
        "telegram-group-clone",
        "telegram-member-adder-2025",
        "telegram-member-adders-2025",
        "telegram-memberadder",
        "telegram-members-add",
        "telegram-members-adder-2025",
        "telegram-membersadder",
        "telegram-user-adders-2025"
      ]
    },
    "content": {
      "readme": "# Telegram-Adding-Users\nAn automation for the process of adding communitymembers to any target of your choice. Simple to use and up to date!\n\n# 📁 GET IT HERE: https://shorturl.at/MdvDy\n# CONTACT FOR QUESTIONS: https://shorturl.at/qRqTq\n\n<img src='UI1.png' width='450'>\n\n- EXTRACT MEMBERS, MESSAGES, MEDIA, CHANNELS AND MORE! (EVEN HIDDEN MEMBERS)\n![](scrap.gif)\n- ADD MEMBERS TO YOUR GROUPS/CHANNELS AUTOMATICALLY!\n- FILTERING ONLY PREMIUM MEMBERS POSSIBLE! (OPTIONAL)\n![](add.gif)\n- AUTOMATICALLY FORWARD ANY POST/MESSAGE/MEDIA TO ANY TARGET!\n- MASSDM ANYONE ON TELEGRAM!\n![](mass.gif)\n- CLONE AND COPY ANY CHANNELS/GROUPS!\n![](copy.gif)\n- JOIN TO TARGETS WITH ALL OF YOUR ACCOUNTS AUTOMATICALLY!\n![](join.gif)\n- GET RID OF YOUR COMPETITION EASILY!\n- GROW YOUR AUDIENCE EASILY!\n- GROW YOUR VIEWS AUTOMATICALLY!\n![](view_post.gif)\n- VOTE ON ANY POLLS AUTOMATICALLY!\n- UNSPAM AND UNFREEZE YOUR ACCOUNTS EASILY!\n- REACT TO ANY POST AUTOMATICALLY WITH EMOJI'S!\n- MAKE BACKUPS!\n- NO CODING SKILLS REQUIRED!\n- PROXY SUPPORTED (OPTIONAL)\n- THE ONLY TG TOOL WHICH IS UPDATED TO 2025!\n- SUPPORT AND UPDATES FOR LIFETIME!\n- AND MUCH MORE!\n\nNEW FEATURES WILL BE IMPLEMENTED AT WISH!\nIf you have any questions, make sure to contact us.\n\n# 📁 GET IT HERE: https://shorturl.at/MdvDy\n# CONTACT FOR QUESTIONS: https://shorturl.at/qRqTq",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-19T03:46:06.046049"
  },
  {
    "basic_info": {
      "name": "model-memory-calculator",
      "full_name": "KolosalAI/model-memory-calculator",
      "owner": "KolosalAI",
      "description": "Simple model memory requirements calculator for GGUF",
      "url": "https://github.com/KolosalAI/model-memory-calculator",
      "clone_url": "https://github.com/KolosalAI/model-memory-calculator.git",
      "ssh_url": "git@github.com:KolosalAI/model-memory-calculator.git",
      "homepage": null,
      "created_at": "2025-09-18T06:51:30Z",
      "updated_at": "2025-09-19T00:02:32Z",
      "pushed_at": "2025-09-18T23:59:26Z"
    },
    "stats": {
      "stars": 23,
      "forks": 2,
      "watchers": 23,
      "open_issues": 0,
      "size": 54
    },
    "tech_info": {
      "language": "HTML",
      "languages": {
        "HTML": 28776
      },
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "# GGUF Metadata Reader (Browser)\n\nA single-file, static web app to read GGUF model metadata directly in the browser and estimate memory usage (RAM/VRAM) for a chosen context window and KV cache quantization.\n\n- Works with remote URLs that support HTTP Range requests (e.g., many Hugging Face files)\n- Works with local `.gguf` files (drag-and-drop via file picker)\n- Detects sharded models (e.g., `-00001-of-00013`) and sums total size\n- No server required; everything runs client-side\n\n## Quick Start\n\nOption A: Open the file directly\n1. Open `index.html` in a modern browser (Chrome, Edge, Safari).\n2. Paste a GGUF URL or choose a local `.gguf` file and click the corresponding button.\n\nOption B: Serve locally (helps with some CORS setups)\n```bash\ncd path/to/model-memory-calculator\npython -m http.server 8000\n# Then open http://localhost:8000 in your browser\n```\n\n## Usage\n\n- GGUF URL: Paste a direct link to a `.gguf` (e.g., a Hugging Face “resolve/main” URL). Many hosts allow partial download via HTTP Range.\n- Or choose a local GGUF file: Uses the browser’s File API; no upload leaves your machine.\n- Context size (tokens): Select a preset (e.g., 4K, 16K, 128K) or choose \"Custom…\" and enter any positive integer token length.\n- KV cache quantization: Choose how keys/values are stored in memory. Options show approximate bytes per value.\n- Verbose: Prints debug logs of what’s read and how size is determined.\n\nClick “Read URL” or “Read File”. If successful, you’ll see:\n- Extracted params: `attention_heads`, `kv_heads`, `hidden_layers`, `hidden_size`, `split_count` (if present)\n- Memory estimate: model size + KV cache size at your chosen context/quantization\n\n## How It Works\n\n- GGUF parsing: Reads just enough of the GGUF header to extract:\n  - `.attention.head_count`\n  - `.attention.head_count_kv`\n  - `.block_count`\n  - `.embedding_length`\n  - `split.count` (if present)\n- Remote file size:\n  - Tries `HEAD` to get `Content-Length`.\n  - Falls back to a `Range: bytes=0-0` request and reads `Content-Range`.\n- Sharded models:\n  - Detects `-00001-of-000NN` style patterns in URLs or uses `split.count` metadata.\n  - Sums sizes across parts (remote) or estimates total from a single shard (local) when possible.\n- KV cache estimate:\n  - Uses a simplified formula: `bytes_per_value × hidden_size × hidden_layers × context_tokens`.\n  - Shows total as: `Model + KV` (MB/GB). Actual usage can vary by backend/implementation.\n\n## Notes & Limitations\n\n- GGUF versions: Supports GGUF v1–v3 headers for the fields listed above.\n- CORS & Range: Remote hosts must allow cross-origin requests and HTTP Range. If not, size detection may fail; download the file and use the local option instead.\n- Range ignored: Some servers respond `200` without honoring `Range`. The app avoids downloading the full body for size only; estimates can fail in this case.\n- Sanity limits: Very long strings/arrays in metadata are bounded to avoid huge reads.\n- Estimates only: KV cache math is intentionally simplified. Different runtimes store KV differently (e.g., layout, precision, per-head factors).\n\n## Troubleshooting\n\n- “Failed to read params.”\n  - The file may not be GGUF or uses unsupported/unexpected metadata. Try another file or update the URL.\n- “Could not determine file size or compute usage (CORS/Range?).”\n  - The remote host may block CORS or not report size via `HEAD`/`Range`. Try serving the page locally, a different host, or the local file picker.\n- Split detection issues\n  - Ensure URLs use a stable pattern (e.g., `-00001-of-000xx`) or that `split.count` is present in metadata.\n\n## Privacy\n\n- The local file option never uploads your file; parsing happens entirely in your browser.\n- For remote URLs, the app performs small range requests to read the header and determine file size. It aborts early once required metadata is read.\n\n## Development\n\n- No build step required. The app is a single page:\n  - `index.html` — All logic and UI\n- Open in a browser or serve with any static server.\n\n## License\n\nThis project is licensed under the terms in `LICENSE`.\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-19T03:46:07.146147"
  },
  {
    "basic_info": {
      "name": "InternVLA-A1",
      "full_name": "InternRobotics/InternVLA-A1",
      "owner": "InternRobotics",
      "description": "InternVLA-A1: Unifying Understanding, Generation, and Action for Robotic Manipulation​",
      "url": "https://github.com/InternRobotics/InternVLA-A1",
      "clone_url": "https://github.com/InternRobotics/InternVLA-A1.git",
      "ssh_url": "git@github.com:InternRobotics/InternVLA-A1.git",
      "homepage": null,
      "created_at": "2025-09-18T01:00:55Z",
      "updated_at": "2025-09-19T03:44:19Z",
      "pushed_at": "2025-09-18T10:49:23Z"
    },
    "stats": {
      "stars": 22,
      "forks": 0,
      "watchers": 22,
      "open_issues": 0,
      "size": 437
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 517220
      },
      "license": null,
      "topics": []
    },
    "content": {
      "readme": "<div align=\"center\">\n\n# InternVLA-A1: Unifying Understanding, Generation, and Action for Robotic Manipulation​\n\n</div>\n\n---\n\nInternVLA-A1 is an end-to-end vision–language–action (VLA) framework unifing understanding, generation ,and action for robotic manipulation. It leverages predictive imagination of task evolution to guide execution, enabling enhanced manipulation in highly dynamic environments. \n\n## :fire: Highlights <a name=\"high\"></a>\n<img width=\"1000\" alt=\"seer\" src=\"assets/internvla_a1_framework.jpg\">\n\n- **Novel Model Archituecture**: A Mixture-of-Transformers architecture for unified understanding, generation, and action.\n- **Hybrid Synthetic-Real Data Corpus**: A hybrid synthetic-real manipulation dataset [InternData-A1](https://huggingface.co/datasets/InternRobotics/InternData-A1), integrating 5 heterogeneous robots, 15 skills, and 200+ scenes, emphasizing multi-robot collaboration under dynamic scenarios.\n- **Impressive Real-World performance**: InternVLA-A1 demonstrates strong effectiveness and generalization in highly dynamic scenarios involving dynamic grasping of conveyor belts and multi-robot collaboration.\n\n### 🏆 **Unified Understanding-Generation-Action Family**\n\n- **F1-VLA** (F1 is a prequel version of InternVLA-A1): [Paper](https://arxiv.org/abs/2509.06951) | [Code](https://github.com/InternRobotics/F1-VLA) | [Model](https://huggingface.co/InternRobotics/F1-VLA)\n- **InternVLA-A1**: [Code](https://github.com/InternRobotics/InternVLA-A1) | [Paper/Model (Scheduled for late September release)]()\n\n## 🤖 Real-World Robot Demonstrations\n\n### **Package grabbing and flipping in conveyor belt**\n<div align=\"center\">\n    <video src=\"https://github.com/user-attachments/assets/c7d8989c-be14-428e-b498-d02dc1fc1475\"\n         controls autoplay muted playsinline loop width=\"720\"></video>\n  <p><em>The model handles dynamically shaped packages on conveyor belts, tracking and predicting their trajectories in real-time to achieve high-speed stable grasping, while adaptively flipping packages and identifying express information from delivery notes.</em></p>\n</div>\n\n\n### **Multi-robot collaboration on long-horizon tasks in dynamic environments**\n<div align=\"center\">\n      <video src=\"https://github.com/user-attachments/assets/c438ff8a-4536-45b3-9117-e210c36ba8a0\"\n         controls autoplay muted playsinline loop width=\"720\"></video>\n  <p><em>The model swiftly identifies, locates, and grips high-speed ingredients based on task demands, showcasing its adaptability in complex environments.</em></p>\n</div>\n\n\n## 🚀 Quick Start\n\n### **Prerequisites**\n- Python ≥ 3.10\n- torch ≥ 2.6.0\n- CUDA ≥ 12.4\n\n### **Installation**\n```bash\n# Clone repository\ngit clone https://github.com/InternRobotics/InternVLA-A1.git\n\n# Create environment\nconda create -f internvla_a1 python==3.10\nconda activate internvla_a1\n\n# Install dependencies\npip install torch==2.6.0 torchvision==0.21.0 torchaudio==2.6.0 torchcodec==0.2.1 --index-url https://download.pytorch.org/whl/cu124\n\n# install other requirements\npip install -r requirements.txt\n\npip install numpy==1.26.4\n```\n\n## 📄 License\n\nThis project is licensed under the MIT License.\n\n## 🙏 Acknowledgments\n\n- [Lerobot](https://github.com/huggingface/lerobot)\n- [InternVL](https://github.com/OpenGVLab/InternVL)\n- [COSMOS](https://github.com/nvidia-cosmos)\n- [Any4lerobot](https://github.com/Tavish9/any4lerobot/)\n- [VAR](https://github.com/FoundationVision/VAR)\n",
      "default_branch": "master"
    },
    "fetched_at": "2025-09-19T03:46:08.257647"
  },
  {
    "basic_info": {
      "name": "qb-phone-pro",
      "full_name": "QBCoreStore/qb-phone-pro",
      "owner": "QBCoreStore",
      "description": "QBCore Advanced Phone for FiveM 📱 Inspired by iPhoe. Sleek UI, customizable themes, GPS, banking, calling, social apps & QBCore integration, free for the community! ",
      "url": "https://github.com/QBCoreStore/qb-phone-pro",
      "clone_url": "https://github.com/QBCoreStore/qb-phone-pro.git",
      "ssh_url": "git@github.com:QBCoreStore/qb-phone-pro.git",
      "homepage": "https://fivem-qbcore.com/",
      "created_at": "2025-09-18T09:23:28Z",
      "updated_at": "2025-09-19T03:15:27Z",
      "pushed_at": "2025-09-18T12:03:21Z"
    },
    "stats": {
      "stars": 22,
      "forks": 0,
      "watchers": 22,
      "open_issues": 0,
      "size": 33649
    },
    "tech_info": {
      "language": null,
      "languages": {},
      "license": null,
      "topics": [
        "fivem",
        "iphone",
        "nopixel",
        "qb-phone",
        "qb-scripts",
        "qbcore",
        "qbcore-framework",
        "qbcore-iphone",
        "qbcore-script",
        "qbcore-scripts",
        "qbcore-ui",
        "qbcorestore"
      ]
    },
    "content": {
      "readme": "## ⚡ About Us  \nWe are the official **QBCore Store**, active since 2020.  \nWhile we release free scripts for the community, our main focus is **Premium Pre-Made All-in-One Servers**.  \nIf you want a **ready-to-use, optimized, and complete QBCore roleplay server** with 400+ premium resources, maps, UIs, and monthly updates – check us out below:  \n\n🌍 Website: [fivem-qbcore.com](https://fivem-qbcore.com)  \n💬 Discord: [discord.gg/qbcoreframework](https://discord.gg/qbcoreframework)  \n\n# 📱 qb-phone pro version\n**QBCore Advanced Phone for FiveM – Inspired by iPhone**  \n\nWelcome to the **QBCore Phone Pro Version** – a modern, feature-packed smartphone system built for **FiveM roleplay servers**.  \nThis phone is designed with **sleek UI, customizable themes, GPS, banking, calling, social apps, and seamless QBCore integration** – all free for the community!  \n\n---\n\n## ✨ Why Use This Phone?  \n- Full optimized design, works perfectly without bugs.  \n- Inspired by real modern smartphones for the **best RP experience**.  \n- Community-focused: we release **free scripts** regularly.  \n- **Open-source & customizable** for developers and server owners.  \n- Do not resell it. You Will Get a Copyright Strike.\n\n---\n\n## ⭐ Goal: 300+ Stars!  \nIf you enjoy this release, please support us by giving a ⭐ star on GitHub – it motivates us to release more high-quality scripts for the community!  \n\n---\n---\n\n## 📸 Preview  \n\n![Preview](https://files.fivemerr.com/images/08b49fe2-55ca-4553-b22a-9df31da5f714.png)  \n\n---\n\n🚀 Enhance your FiveM server today with **qb-phone pro** – Free for all, Premium for those who want the next level!  \n",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-19T03:46:09.360993"
  },
  {
    "basic_info": {
      "name": "MiMo-Audio-Tokenizer",
      "full_name": "XiaomiMiMo/MiMo-Audio-Tokenizer",
      "owner": "XiaomiMiMo",
      "description": "A unified tokenizer that is capable of both extracting semantic information and enabling high-fidelity audio reconstruction.",
      "url": "https://github.com/XiaomiMiMo/MiMo-Audio-Tokenizer",
      "clone_url": "https://github.com/XiaomiMiMo/MiMo-Audio-Tokenizer.git",
      "ssh_url": "git@github.com:XiaomiMiMo/MiMo-Audio-Tokenizer.git",
      "homepage": "",
      "created_at": "2025-09-18T16:02:05Z",
      "updated_at": "2025-09-19T03:35:56Z",
      "pushed_at": "2025-09-18T16:42:57Z"
    },
    "stats": {
      "stars": 21,
      "forks": 3,
      "watchers": 21,
      "open_issues": 0,
      "size": 2086
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 88623
      },
      "license": "Apache License 2.0",
      "topics": []
    },
    "content": {
      "readme": "<div align=\"center\">\n\n<img src=\"https://raw.githubusercontent.com/XiaomiMiMo/MiMo-VL/main/figures/Xiaomi_MiMo.png\" alt=\"Description\" width=\"25%\" />\n\n# MiMo-Audio-Tokenizer\n\n<img src=\"mimo_audio_tokenizer/assets/Tokenizer_01.png\" alt=\"Description\" width=\"90%\" />\n\n<p><em>A unified tokenizer that is capable of both extracting semantic information and enabling high-fidelity audio reconstruction.</em></p>\n\n</div>\n\n## Key Features\n\n- Scaled parameters and training data bootstrap the frontier of audio tokenization\n  - 1.2B pure transformer-based architecture to keep both efficiency and effectiveness\n  - trained from scratch over 11 million hours covering both audio reconstruction task and the audio-to-text (A2T) task\n\n- Unified representation enhance both cross-modal alignment and speech reconstruction quality\n  - jointly capture both semantic and acoustic information while further alleviates the semantic-acoustic representation conflict\n\n## Installation\n\n```sh\ngit clone https://github.com/XiaomiMiMo/MiMo-Audio-Tokenizer\ncd mimo-audio-tokenizer\n# Install base dependencies\npip install -e .\n# Install flash-attn\npip install -e \".[flash]\"\n```\n\n## Model Download\n\n```sh\n# you might need `sudo apt-get install git-lfs` before download this model\ngit clone https://huggingface.co/XiaomiMiMo/MiMo-Audio-Tokenizer\n```\n\n## Example Usage\n\n### 0. Quick start\n\n```py\nimport mimo_audio_tokenizer\n\n# one-line model init\ntokenizer = mimo_audio_tokenizer.load_model(\"path to your model\").bfloat16().cuda()  # FlashAttention only support fp16 and bf16 data type\n\n# preprocess\nmels = []\nwav_paths = [\"mimo_audio_tokenizer/assets/BAC009S0764W0121.wav\", \"mimo_audio_tokenizer/assets/BAC009S0764W0122.wav\", \"mimo_audio_tokenizer/assets/猪八戒_gt.wav\"]\nfor wav_path in wav_paths:\n    wav = mimo_audio_tokenizer.load_audio(wav_path, tokenizer.config.sampling_rate)\n    mels.append(mimo_audio_tokenizer.mel_spectrogram(wav, tokenizer.config))\nmels, mels_lens = mimo_audio_tokenizer.padding(mels)  # (batch_size, n_mels, seq_len), (batch_size,)\n\n# one-line encode\ncodes, codes_lens, _ = tokenizer.encode(mels.cuda(), mels_lens.cuda())  # (batch_size, max_len, num_quantizers), (batch_size,)\n\n# one-line decode\nwavs, wavs_lens, _ = tokenizer.decode(codes, codes_lens)  # (batch_size, 1, wav_len)\n\n# inspect results\nfor i in range(len(wav_paths)):\n    print(codes[i, :codes_lens[i].item()])\n\nfor i in range(len(wav_paths)):\n    torchaudio.save(f\"{i}.wav\", wavs[i, :, :wavs_lens[i].item()].float().cpu().detach(),\n                    tokenizer.config.sampling_rate, format='wav', encoding='PCM_S')\n\n```\n\n### 1. Distributed offline batch inference via command-line tools\n\n`mimo_audio_tokenizer` is built for distributed offline batch inference.\n\n```sh\n# 1 node 8 gpu, try to decrease `batch_size` if OOM\n# task choices:\n#   \"wav2token\": need `key` / `wav` / `quantized_tokens` available in data.jsonl\n#   \"token2wav\": need `key` / `quantized_tokens` / `reconstructed_wav` available in data.jsonl\n#   \"wav2token2wav\": need `key` / `wav` / `quantized_tokens` / `reconstructed_wav` available in data.jsonl\ntorchrun --nproc_per_node=8 --nnodes=1 \\\n     --rdzv_id=2025 --rdzv_backend=\"c10d\" --rdzv_endpoint=\"localhost:0\" \\\n    `which mimo_audio_tokenizer` \\\n        --model_path \"path to your model\" \\\n        --data_list \"path to your data.jsonl\" \\\n        --batch_size 64 \\\n        --num_workers 8 \\\n        --prefetch 16 \\\n        --num_quantizers 20 \\\n        --task \"wav2token2wav\"\n```\n\n### Example Data Format\n\nHere is example `data.jsonl`:\n\n```json\n{\"key\": \"uttid_1\", \"wav\": \"/mnt/data/audio/uttid_1.wav\", \"quantized_tokens\": \"/mnt/data/audio_reconstructed/uttid_1.json\", \"reconstructed_wav\": \"/mnt/data/audio_reconstructed/uttid_1.wav\"}\n...\n{\"key\": \"uttid_2\", \"wav\": \"/mnt/data/audio/uttid_2.wav\", \"quantized_tokens\": \"/mnt/data/audio_reconstructed/uttid_2.json\", \"reconstructed_wav\": \"/mnt/data/audio_reconstructed/uttid_2.wav\"}\n...\n```\n\n- `key` is the key of this sample.\n- `wav` is the original audio.\n- `quantized_tokens` is the json path to save quantized tokens (we highly recommend to pre-define the save path before running the script).\n- `reconstructed_wav` is the wav path to save reconstructed result (we highly recommend to pre-define the save path before running the script).\n\n### 2. Online speech code extraction\n\n`mimo_audio_tokenizer` can also be used in online code extraction to power the training of AudioLLM.\n\n<table>\n<tr>\n<th>Before (extract code offline)</th>\n<th>After (extract code online)</th>\n</tr>\n<tr>\n<td>\n<sub>\n\n```py\n\nclass AudioLLM(nn.Module):\n    ...\n    def __init__(self, ...):\n        ...\n\n    def forward(self, speech_codes: Tensor, text_ids: Tensor, ...):\n        ...\n```\n\n</sub>\n<td>\n<sub>\n\n```py\nimport mimo_audio_tokenizer\n\nclass AudioLLM(nn.Module):\n    ...\n    def __init__(self, ...):\n        ...\n        self.audio_tokenizer = mimo_audio_tokenizer.load_model(\"path to your model\")\n        self.audio_tokenizer.freeze()  # no need for gradient calculation\n        ...\n\n    de",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-19T03:46:10.489616"
  },
  {
    "basic_info": {
      "name": "contracts",
      "full_name": "forum-online-protocol/contracts",
      "owner": "forum-online-protocol",
      "description": null,
      "url": "https://github.com/forum-online-protocol/contracts",
      "clone_url": "https://github.com/forum-online-protocol/contracts.git",
      "ssh_url": "git@github.com:forum-online-protocol/contracts.git",
      "homepage": null,
      "created_at": "2025-09-18T06:42:33Z",
      "updated_at": "2025-09-18T17:53:56Z",
      "pushed_at": "2025-09-18T15:26:15Z"
    },
    "stats": {
      "stars": 18,
      "forks": 0,
      "watchers": 18,
      "open_issues": 0,
      "size": 42
    },
    "tech_info": {
      "language": "Solidity",
      "languages": {
        "Solidity": 77818,
        "JavaScript": 46947,
        "Circom": 772
      },
      "license": null,
      "topics": []
    },
    "content": {
      "readme": "# Votta E-Voting System\n\n![Votta](https://via.placeholder.com/150?text=Votta)\n\nA secure, transparent, and decentralized electronic voting system built on blockchain technology with zero-knowledge proofs.\n\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n\n## Table of Contents\n\n- [Overview](#overview)\n- [System Architecture](#system-architecture)\n- [Smart Contracts](#smart-contracts)\n- [Off-Chain Services](#off-chain-services)\n- [Installation](#installation)\n- [Usage Guide](#usage-guide)\n  - [Running the System](#running-the-system)\n  - [Configuration](#configuration)\n  - [Monitoring](#monitoring)\n- [Development](#development)\n  - [Testing](#testing)\n  - [Deployment](#deployment)\n- [Security](#security)\n- [FAQ](#faq)\n- [Contributing](#contributing)\n- [License](#license)\n\n## Overview\n\nVotta is a next-generation e-voting system designed to provide unparalleled security, transparency, and user privacy. Built on blockchain technology with zero-knowledge proofs, it enables secure and auditable voting while maintaining voter anonymity.\n\n### Key Features\n\n- **Multiple Voting Polls**: Support for running multiple elections simultaneously\n- **Credential-Based Voting**: Secure voter registration with cryptographic credentials\n- **Batch Processing**: Efficient aggregation of votes into verifiable batches\n- **Zero-Knowledge Proofs**: Privacy-preserving vote verification\n- **Challenge Mechanism**: Security monitoring and fraud detection\n- **Gas-Optimized**: Subsidized transaction costs for voters\n- **Decentralized Governance**: Protocol-level management of system components\n\n## System Architecture\n\nThe Votta e-voting system consists of several interconnected components:\n\n\n1. **Smart Contracts**: Core blockchain components handling voting logic, credential management, and batch processing\n2. **Aggregator Service**: Off-chain service that collects voting receipts and submits them as batches\n3. **Watch-Tower Service**: Security monitoring service that challenges fraudulent batches\n4. **ZK-Prover**: Zero-knowledge proof generation and verification service\n5. **User Interface**: Web and mobile interfaces for voter interaction\n\n## Smart Contracts\n\nThe system includes the following smart contracts:\n\n- **VottaProtocol**: Central management contract that coordinates all other components\n- **CredentialRegistry**: Manages voter credentials and verifies eligibility\n- **VotingBatch**: Processes batched votes and handles challenges\n- **PenaltyVault**: Holds bonds and manages slashing for malicious actors\n- **VotingFactory**: Creates and manages voting polls\n- **VotingPaymaster**: Covers gas costs for voting operations\n- **PlonkVerifier**: Verifies zero-knowledge proofs for batched votes\n- **AAValidate**: Validates user operations using account abstraction\n\n## Off-Chain Services\n\n### Aggregator Service\n\nThe Aggregator Service, written in Rust, is responsible for:\n\n- Collecting voting receipts from users\n- Building Merkle trees of receipt hashes\n- Generating zero-knowledge proofs\n- Submitting batches to the VotingBatch contract\n\n### Watch-Tower Service\n\nThe Watch-Tower Service, written in Go, performs security monitoring:\n\n- Monitors chain events for new batch submissions\n- Verifies batch validity and detects fraud\n- Challenges fraudulent batches with evidence\n- Acts as a security backstop for the system\n\n### ZK-Prover Integration\n\nThe ZK-Prover component:\n\n- Generates zero-knowledge proofs for vote batches\n- Ensures vote counts match the claimed receipts\n- Preserves privacy while enabling verification\n\n## Installation\n\n### Prerequisites\n\n- Node.js v16+ and npm\n- Go v1.18+\n- Rust v1.65+\n- Solidity v0.8.24+\n- Hardhat\n\n### Installing Dependencies\n\n#### Smart Contracts\n\n```bash\n# Install JavaScript dependencies\nnpm install\n\n# Install Solidity dependencies\nnpm install @openzeppelin/contracts @openzeppelin/contracts-upgradeable\n```\n\n#### Watch-Tower Service\n\n```bash\ncd services/watchtower\ngo mod download\n```\n\n#### Aggregator Service\n\n```bash\ncd services/aggregator\ncargo build --release\n```\n\n## Usage Guide\n\n### Running the System\n\n#### Option 1: Native Execution\n\n1. **Start Local Blockchain**\n\n```bash\nnpx hardhat node\n```\n\n2. **Deploy Smart Contracts**\n\n```bash\nnpx hardhat run scripts/deploy.js --network localhost\n```\n\n3. **Start Aggregator Service**\n\n```bash\ncd services/aggregator\ncargo run --release -- --config config.toml\n```\n\n4. **Start Watch-Tower Service**\n\n```bash\ncd services/watchtower\ngo run main.go\n```\n\n#### Option 2: Docker Containers\n\nWe provide Docker containers for both the Aggregator and Watch-Tower services to simplify deployment and ensure consistent environments.\n\n1. **Build and Start Services with Docker Compose**\n\n```bash\n# From the project root directory\ndocker-compose up -d\n```\n\nThis will start both services as defined in the `docker-compose.yml` file.\n\n2. **View Service Logs**\n\n```bash\n# View logs for all services\ndocker-compose logs -f\n\n# View logs for a specifi",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-19T03:46:11.612918"
  },
  {
    "basic_info": {
      "name": "Kali_Linux_MCP",
      "full_name": "i3T4AN/Kali_Linux_MCP",
      "owner": "i3T4AN",
      "description": "Two-component system bridging Kali Linux penetration testing tools with AI agents via MCP. Flask API server executes 10+ security tools (Nmap, SQLMap, Metasploit, etc.) while MCP client provides seamless AI integration for automated security workflows.",
      "url": "https://github.com/i3T4AN/Kali_Linux_MCP",
      "clone_url": "https://github.com/i3T4AN/Kali_Linux_MCP.git",
      "ssh_url": "git@github.com:i3T4AN/Kali_Linux_MCP.git",
      "homepage": "",
      "created_at": "2025-09-18T13:49:53Z",
      "updated_at": "2025-09-18T22:48:49Z",
      "pushed_at": "2025-09-18T17:31:10Z"
    },
    "stats": {
      "stars": 17,
      "forks": 0,
      "watchers": 17,
      "open_issues": 0,
      "size": 13
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 23338
      },
      "license": "MIT License",
      "topics": [
        "kali-linux",
        "mcp",
        "mcp-server",
        "pentesting",
        "security"
      ]
    },
    "content": {
      "readme": "# Kali_Linux_MCP\n\n## Overview\nKali_Linux_MCP exposes Kali tools through:\n- **Kali_Linux_Server.py**: Flask API wrapping tools like `nmap`, `gobuster`, `nikto`, `sqlmap`, `metasploit`, `hydra`, `john`, `wpscan`, `enum4linux`.  \n- **MCP_Server.py**: MCP bridge using FastMCP, forwarding requests from MCP clients to the API.\n\nUse it for **authorized labs, CTFs, HTB/THM machines**, or AI-assisted testing via MCP clients (Claude Desktop, 5ire, etc.).\n\n---\n\n## Requirements\n- Kali Linux (or Linux with tools installed in PATH).  \n- Python 3 with `flask`, `requests`, `mcp`.  \n- Install:  \n  pip install flask requests mcp\n\n---\n\n## Run\n\n1. Start API:  \n   python3 Kali_Linux_Server.py --port 5000  \n\n2. Health check:  \n   curl http://localhost:5000/health  \n\n3. Start MCP bridge:  \n   python3 MCP_Server.py --server http://localhost:5000 --timeout 300  \n\n---\n\n## API Endpoints\n- GET `/health` — tool status.  \n- POST `/api/command` — run any command.  \n- POST `/api/tools/<tool>` — wrappers for nmap, gobuster, dirb, nikto, sqlmap, metasploit, hydra, john, wpscan, enum4linux.  \n\nEach requires JSON body with tool-specific args (`target`, `url`, etc.).\n\n---\n\n## MCP Tools\nBridge registers MCP tools: `nmap_scan`, `gobuster_scan`, `dirb_scan`, `nikto_scan`, `sqlmap_scan`, `metasploit_run`, `hydra_attack`, `john_crack`, `wpscan_analyze`, `enum4linux_scan`, plus `execute_command` and `check_health`.\n\n---\n\n## Example Usage\n- Nmap:  \n  curl -X POST http://localhost:5000/api/tools/nmap -H \"Content-Type: application/json\" -d '{\"target\":\"scanme.nmap.org\",\"additional_args\":\"-sV\"}'  \n\n- WPScan:  \n  curl -X POST http://localhost:5000/api/tools/wpscan -H \"Content-Type: application/json\" -d '{\"url\":\"https://example.com\",\"additional_args\":\"--enumerate u\"}'  \n\n- MCP Client:  \n  Add MCP config pointing `python3 /path/to/MCP_Server.py --server http://LINUX_IP:5000`.\n\n---\n\n## Scenarios\n- Recon: `nmap_scan` to map services.  \n- Web enum: `gobuster_scan` or `dirb_scan`.  \n- Vuln triage: `nikto_scan`, `sqlmap_scan`.  \n- WordPress checks: `wpscan_analyze`.  \n- SMB recon: `enum4linux_scan`.  \n- Credential tests (lab only): `hydra_attack`.\n- Exploit check: `metasploit_run`.  \n\n---\n\n## Notes\n- Supports AI-assisted workflows: models suggest and run commands.  \n- Works with Claude Desktop, 5ire MCP clients.  \n- Extendable: other forensic tools (Volatility, SleuthKit) possible.  \n\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-19T03:46:12.724449"
  },
  {
    "basic_info": {
      "name": "LiquidGlass-JetpackCompose",
      "full_name": "ardakazanci/LiquidGlass-JetpackCompose",
      "owner": "ardakazanci",
      "description": null,
      "url": "https://github.com/ardakazanci/LiquidGlass-JetpackCompose",
      "clone_url": "https://github.com/ardakazanci/LiquidGlass-JetpackCompose.git",
      "ssh_url": "git@github.com:ardakazanci/LiquidGlass-JetpackCompose.git",
      "homepage": null,
      "created_at": "2025-09-18T15:22:51Z",
      "updated_at": "2025-09-19T03:09:33Z",
      "pushed_at": "2025-09-18T15:28:38Z"
    },
    "stats": {
      "stars": 15,
      "forks": 0,
      "watchers": 15,
      "open_issues": 0,
      "size": 585
    },
    "tech_info": {
      "language": "Kotlin",
      "languages": {
        "Kotlin": 12328
      },
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "https://www.linkedin.com/posts/ardakazanci_jetpackcompose-androiddevelopment-androidprogramming-activity-7374464292516945920-jHLU?utm_source=share&utm_medium=member_desktop&rcm=ACoAAB2vx2QBVowkGS1YKnq1EhEAH-eJwQf_4T8\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-19T03:46:13.844023"
  },
  {
    "basic_info": {
      "name": "Book-video-generate",
      "full_name": "SheenHalo/Book-video-generate",
      "owner": "SheenHalo",
      "description": "一个自动化书籍推广视频生成工具，可以根据书名自动生成带有配音和字幕的短视频。",
      "url": "https://github.com/SheenHalo/Book-video-generate",
      "clone_url": "https://github.com/SheenHalo/Book-video-generate.git",
      "ssh_url": "git@github.com:SheenHalo/Book-video-generate.git",
      "homepage": null,
      "created_at": "2025-09-18T07:37:45Z",
      "updated_at": "2025-09-19T02:37:46Z",
      "pushed_at": "2025-09-18T08:16:20Z"
    },
    "stats": {
      "stars": 15,
      "forks": 6,
      "watchers": 15,
      "open_issues": 0,
      "size": 50414
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 43289
      },
      "license": null,
      "topics": []
    },
    "content": {
      "readme": "# 📚 Book Video Generator\n\n[![Python](https://img.shields.io/badge/Python-3.7%2B-blue.svg)](https://www.python.org/)\n[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)\n[![FFmpeg](https://img.shields.io/badge/FFmpeg-required-red.svg)](https://ffmpeg.org/)\n\n一个自动化书籍推广视频生成工具，可以根据书名自动生成带有配音和字幕的短视频。\n\n## 🖼️ 效果预览\n\n[示例视频](https://github.com/user-attachments/assets/385a804c-904a-4aae-a595-58f9240a66b9)\n\n\n### 视频特性\n生成的视频包含：\n- 🎬 **动态封面展示效果** - 书籍封面滑动动画，4秒片头效果，书籍封面在`resource/covers/`中随机获取\n- 🖼️ **背景图片自动切换** - 每10秒切换背景，营造氛围，背景图片随机从`resource/backgrounds/`中获取\n- 📝 **同步字幕显示** - 根据音频时长精准同步，底部居中显示\n- 🎵 **多音轨混合** - 配音 + 背景音乐 + 音效，音量自动平衡，背景音乐随机从`resource/bgm/`中获取\n\n### 使用建议\n1. **首次使用**: 建议先下载示例视频查看效果\n2. **测试运行**: 使用简单的书籍名称进行测试\n3. **参数调整**: 根据需要调整视频参数和语音选择\n\n## 📋 系统要求\n\n- **Python**: 3.7+\n- **FFmpeg**: 必须安装并添加到系统PATH\n- **操作系统**: Windows / macOS / Linux\n\n## 🚀 快速开始\n\n### 1. 克隆项目\n\n```bash\ngit clone https://github.com/SheenHalo/Book-video-generate.git\ncd Book-video-generate\n```\n\n### 2. 创建虚拟环境\n\n**Windows:**\n```bash\npython -m venv venv\nvenv\\Scripts\\activate\n```\n\n**macOS/Linux:**\n```bash\npython3 -m venv venv\nsource venv/bin/activate\n```\n\n### 3. 安装依赖\n\n```bash\npip install -r requirements.txt\n```\n\n### 4. 检查FFmpeg\n\n项目依赖FFmpeg进行视频合成，请确保已正确安装：\n\n```bash\npython video_processor.py\n```\n\n如果显示\"ffmpeg 可用\"，则安装成功。如果显示\"ffmpeg 不可用\"，请按以下步骤安装：\n\n#### Windows FFmpeg安装\n1. 下载FFmpeg: https://ffmpeg.org/download.html\n2. 解压到 `C:\\ffmpeg`\n3. 添加 `C:\\ffmpeg\\bin` 到系统PATH环境变量\n4. 重启命令行并验证：`ffmpeg -version`\n\n#### macOS FFmpeg安装\n```bash\nbrew install ffmpeg\n```\n\n#### Linux FFmpeg安装\n```bash\n# Ubuntu/Debian\nsudo apt update && sudo apt install ffmpeg\n\n# CentOS/RHEL/Fedora\nsudo yum install ffmpeg\n```\n\n### 5. 配置LLM API\n提供了一个免费的LLM API接口。如果失效了，请自行配置。\n编辑 `llm.py` 文件，配置你的LLM API信息：\n\n```python\n# 在LLMClient类中修改\nself.api_url = \"你的API地址\"\nself.api_key = \"你的API密钥\"\n```\n\n### 6. 准备资源文件\n\n确保以下目录包含必要的文件：\n\n```\nresource/\n├── backgrounds/    # 背景图片 (jpg/png)\n├── bgm/           # 背景音乐 (mp3)\n├── covers/        # 书籍封面存储位置\n├── effects/       # 音效文件 (mp3)\n└── fonts/         # 字体文件 (包含msyh.ttc)\n```\n\n### 7. 运行程序\n\n```bash\npython main.py\n```\n\n按照提示输入书名，程序将自动生成视频：\n\n```\n请输入书名: 巴别塔\n正在获取书籍信息...\n正在生成文案...\n正在生成语音...\n正在生成视频...\n开始合成音视频...\n最终视频已保存到: appdata/巴别塔/final_video.mp4\n```\n\n## 🛠️ 高级配置\n\n### 修改语音类型\n\n在 `main.py` 中修改语音选择：\n\n```python\n# 查看所有可用语音\nprint(voice_dict.keys())\n\n# 选择特定语音\nvoice = voice_dict.get(\"晓秋-女\")\n```\n\n### 自定义视频参数\n\n在 `app.py` 的 `make_movie` 函数中可以调整：\n- 屏幕尺寸\n- 动画时长\n- 音量大小\n- 背景切换时间\n\n### 支持的语音列表\n\n项目支持43种中文语音变体：\n\n| 语音名称 | 类型 | 特点 |\n|---------|------|------|\n| 晓晓（标准）-女 | 标准 | 温暖，全面，生动 |\n| 晓辰（标准）-女 | 标准 | 友好，休闲，乐观 |\n| 云峰-男 | 标准 | 自信，生动，情感 |\n| 晓晓（多语言）-女 | 多语言 | 温暖，生动，明亮 |\n| 晓通（吴语）-女 | 方言 | 温暖，友好，舒缓 |\n| 晓敏（粤语）-女 | 方言 | 明亮，清晰，自信 |\n| ...更多语音详见代码 | | |\n\n## 📁 项目结构\n\n```\nBook-video-generate/\n├── main.py              # 主入口文件\n├── app.py               # 视频生成核心\n├── spider.py            # 豆瓣爬虫\n├── llm.py               # LLM客户端\n├── tts_generator.py     # TTS生成器\n├── video_processor.py   # 视频处理工具\n├── requirements.txt     # 依赖列表\n├── appdata/            # 生成的文件\n└── resource/           # 资源文件\n```\n\n## 📄 许可证\n\n本项目采用MIT许可证 - 详见 [LICENSE](LICENSE) 文件\n\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-19T03:46:14.944583"
  },
  {
    "basic_info": {
      "name": "Student-Success-Analysis",
      "full_name": "ankitsharma-tech/Student-Success-Analysis",
      "owner": "ankitsharma-tech",
      "description": "Student success analysis – statisitical data analyisis AI project",
      "url": "https://github.com/ankitsharma-tech/Student-Success-Analysis",
      "clone_url": "https://github.com/ankitsharma-tech/Student-Success-Analysis.git",
      "ssh_url": "git@github.com:ankitsharma-tech/Student-Success-Analysis.git",
      "homepage": "",
      "created_at": "2025-09-18T17:49:39Z",
      "updated_at": "2025-09-18T18:16:26Z",
      "pushed_at": "2025-09-18T17:53:18Z"
    },
    "stats": {
      "stars": 15,
      "forks": 0,
      "watchers": 15,
      "open_issues": 0,
      "size": 12094
    },
    "tech_info": {
      "language": "HTML",
      "languages": {
        "HTML": 641646
      },
      "license": "MIT License",
      "topics": [
        "ai",
        "artificial-intelligence",
        "r",
        "statistical-analysis",
        "statistical-data-analysis",
        "statistics"
      ]
    },
    "content": {
      "readme": "# 🎓 Student Success Analysis\n\n## Final report: [report.pdf](./report.pdf)\n\n## 📖 Project Overview\n\nThe main goal of this project was to create a comprehensive report explaining concepts of **statistical data analysis** applied to an existing dataset.\n \n- The choice of statistical methods was flexible, as long as they were relevant and covered in the course curriculum.\n- The report included test cases, either from the recommended list provided by faculty or created by team members.\n- **R language** was used for data analysis and report generation.\n\nProject evaluation was based on:\n\n1. **Report quality**\n2. **Oral examination** testing knowledge of theoretical concepts (e.g., when to use a test, test assumptions, and method details).\n\n---\n\n## 📊 Dataset\n\nThe dataset consists of survey responses and student grades in **mathematics** and **Portuguese** from two high schools.\n\nCollecting this type of data is essential for analyzing and improving the quality of the education system.  \nMore details: [pdfs/dataset_documentation.pdf](./pdfs/dataset_documentation.pdf)\n\n---\n\n## 📂 Directory Structure\n\n| Directory                     | Description                                        |\n| ----------------------------- | -------------------------------------------------- |\n| [auditorne](./auditorne/)     | Reference to existing problems and their solutions |\n| [cheatsheets](./cheatsheets/) | Tidyverse cheat sheets in PDF format               |\n| [pdfs](./pdfs/)               | Dataset and project descriptions                   |\n| [src](./src/)                 | R Markdown source files and dataset                |\n\n---\n\n## ⚙️ Installation\n\n### Windows\n\n- Install [RStudio](https://www.rstudio.com/products/rstudio/download/#download)\n- Install [R](https://cran.r-project.org/bin/windows/base/)\n\n### Linux\n\n- Install [RStudio](https://www.rstudio.com/products/rstudio/download/#download)\n- Install R and tidyverse dependencies:\n  ```bash\n  sudo apt-get install r-cran-curl r-cran-openssl r-cran-xml2 libxml2-dev\n  ```\n\n## 📦 R Packages Setup\n\n1. Open **RStudio** → Open Project → `student-success-analysis.Rproj`\n2. The file `student-success-analysis.Rmd` should open automatically\n   - If not, navigate to it in the **Files** panel and double-click\n3. Run the first code chunk (`Ctrl + Shift + Enter`) containing the `library` functions\n4. A popup will prompt to install required packages → click **Yes**\n5. Installation may take ~20 minutes\n\n---\n\n## 📝 Notes\n\n**KS Test:**\n\n- If _p = 1_ → data surely come from the same distribution\n- If _p = 0_ → data come from different distributions\n\n## 📋 To-Do\n\n- [ ] Spellcheck the report\n- [x] Write introduction to the problem\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-19T03:46:16.074795"
  },
  {
    "basic_info": {
      "name": "Time-Management-Tool",
      "full_name": "ankitsharma-tech/Time-Management-Tool",
      "owner": "ankitsharma-tech",
      "description": "An AI-powered time management tool built with Python to help users plan, prioritize, and optimize their daily tasks efficiently.",
      "url": "https://github.com/ankitsharma-tech/Time-Management-Tool",
      "clone_url": "https://github.com/ankitsharma-tech/Time-Management-Tool.git",
      "ssh_url": "git@github.com:ankitsharma-tech/Time-Management-Tool.git",
      "homepage": "",
      "created_at": "2025-09-18T17:44:29Z",
      "updated_at": "2025-09-18T18:16:28Z",
      "pushed_at": "2025-09-18T17:47:26Z"
    },
    "stats": {
      "stars": 15,
      "forks": 0,
      "watchers": 15,
      "open_issues": 0,
      "size": 498
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 7968,
        "Batchfile": 54
      },
      "license": "MIT License",
      "topics": [
        "ai-project",
        "ml",
        "ml-project",
        "project",
        "python",
        "python-project",
        "python3",
        "timemanagement"
      ]
    },
    "content": {
      "readme": "# Time Management Assistant Tool\n\nWelcome to the ultimate tool designed to streamline your tasks and manage your time efficiently. With its sleek, futuristic interface and cutting-edge functionality, TMA is here to revolutionize how you stay organized.\n\n## 🚀 Setup\n\n**Step 1: Clone the Repository**\n\n```bash\ngit clone https://github.com/ankitsharma-tech/Time-Management-Tool.git\n```\n\n**Step 2: Navigate to the Directory**\n\n```bash\ncd Time-Management-Tool\n```\n\n**Step 3: Navigate to the Directory**\n\n```bash\npip install -r requirements.txt\n```\n\n**Step 4: Open the Configuration File**\n\n- Open the `.bat` file in Notepad or your preferred text editor.\n\n**Step 5: Insert Your Main Script Path**\n\n- Copy the path to your `main.py` file.\n\n**Step 6: Update the BAT File**\n\n- Paste the copied path into the second line of the `.bat` file.\n\n## 🌟 Usage\n\nTo start managing your time like a pro:\n\n- Simply double-click the `.bat` file to launch the tool.\n- If you wan't to sheel the list use command\n\n```\nShow\n```\n\ninside the TMA - Tool\n\n- for Example Command type any thing wrong\n\n```\n                        ______ Available commands _____\n                      1. show - to show the Schedule file\n                      2. tell me - try like \"tell me to sleep at 09:00pm\"\n```\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-19T03:46:17.182467"
  },
  {
    "basic_info": {
      "name": "Student-Performance-Prediction",
      "full_name": "ankitsharma-tech/Student-Performance-Prediction",
      "owner": "ankitsharma-tech",
      "description": "This is a simple machine learning project using classifiers for predicting factors which affect student grades, using data from CSV file.",
      "url": "https://github.com/ankitsharma-tech/Student-Performance-Prediction",
      "clone_url": "https://github.com/ankitsharma-tech/Student-Performance-Prediction.git",
      "ssh_url": "git@github.com:ankitsharma-tech/Student-Performance-Prediction.git",
      "homepage": "",
      "created_at": "2025-09-18T17:48:17Z",
      "updated_at": "2025-09-18T18:16:27Z",
      "pushed_at": "2025-09-18T17:48:55Z"
    },
    "stats": {
      "stars": 15,
      "forks": 0,
      "watchers": 15,
      "open_issues": 0,
      "size": 10
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 10521
      },
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "# 🎓 Student Performance Prediction\nThis project demonstrates a simple machine learning approach to predict factors that influence student grades using a dataset stored in a CSV file.\n\n## 📊 Project Overview\n\nThe dataset contains information about students from different nationalities and grade levels, along with key determining factors such as:\n\n- Number of hands raised\n- Number of attendances\n- Hours studied\n- And more\n\nThe goal of this project is to analyze these factors and predict their impact on student performance.\n\n## 🧠 Machine Learning Models\n\nSeveral classifiers and machine learning models have been implemented and compared to achieve the most accurate predictions of the factors affecting student marks.\n\n## 📈 Visualizations\n\nTo better understand the results, visual aids have been generated, including:\n\n- Graphs for data insights\n- Confusion matrices for model evaluation\n\n---\n\nThis project provides a hands-on demonstration of applying machine learning techniques to an educational dataset, highlighting the relationship between study habits, engagement, and student success.\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-19T03:46:18.291675"
  },
  {
    "basic_info": {
      "name": "AI-Object-Detection",
      "full_name": "ankitsharma-tech/AI-Object-Detection",
      "owner": "ankitsharma-tech",
      "description": "A web AI object detection",
      "url": "https://github.com/ankitsharma-tech/AI-Object-Detection",
      "clone_url": "https://github.com/ankitsharma-tech/AI-Object-Detection.git",
      "ssh_url": "git@github.com:ankitsharma-tech/AI-Object-Detection.git",
      "homepage": "https://woody.pizza/tensorflow/object-detection/",
      "created_at": "2025-09-18T17:38:04Z",
      "updated_at": "2025-09-18T18:16:29Z",
      "pushed_at": "2025-09-18T17:42:09Z"
    },
    "stats": {
      "stars": 15,
      "forks": 0,
      "watchers": 15,
      "open_issues": 0,
      "size": 4
    },
    "tech_info": {
      "language": "JavaScript",
      "languages": {
        "JavaScript": 3145,
        "HTML": 1575,
        "CSS": 175
      },
      "license": "MIT License",
      "topics": [
        "ai",
        "artificial-intelligence",
        "artificial-intelligence-projects",
        "camera",
        "camera-detect",
        "css",
        "detection",
        "html",
        "javascript",
        "javascript-ai",
        "javascript-artificial-intelligence",
        "javascript-tensorflow",
        "ml5js",
        "object-detection",
        "object-detector",
        "tensorflow",
        "web"
      ]
    },
    "content": {
      "readme": "# 🤖 AI Object Detection\n\n## 👋 About this Project\nThis is a web-based AI object detection application that runs directly in your browser. It uses your device’s camera to detect objects in real time, making it easy and accessible without requiring any installation.\n\n---\n\n## ⚙️ Features\n- ✅ Toggle switch to enable or disable AI detection  \n- ✅ Range slider to control frame rate  \n- ✅ Real-time object detection through your camera  \n\n---\n\n## 🖼️ Preview\n<a href=\"https://ibb.co/JCNgfJr\"><img src=\"https://i.ibb.co/3kwQDZS/preview-combined.jpg\" alt=\"preview-combined\" width=\"100%\"></a>\n\n---\n\n## 💪 Try It\nNot convinced yet? Try it out yourself here:  \n👉 [Live Demo](https://woody.pizza/tensorflow/object-detection/)\n\n---\n\n## 🌐 Browser Support\nThe app works on most modern browsers. Below are the tested ones:\n\n### Desktop\n| Browser           | Supported |\n|-------------------|:---------:|\n| Firefox           | ✅ |\n| Chrome            | ✅ |\n| Edge              | ✅ |\n| Internet Explorer | ❌ |\n\n### Mobile\n| Browser  | Supported |\n|----------|:---------:|\n| Firefox  | ✅ |\n| Chrome   | ✅ |\n\n---\n\n## ✌️ Credits\n- [Materialize](https://materializecss.com/) – for UI components  \n- [ml5.js](https://ml5js.org/) – for machine learning in the browser  \n",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-19T03:46:19.412404"
  },
  {
    "basic_info": {
      "name": "Magic-Door-Godot-4.5-release-page",
      "full_name": "lukky-nl/Magic-Door-Godot-4.5-release-page",
      "owner": "lukky-nl",
      "description": "Magic door seen on the Godot 4.5 release page",
      "url": "https://github.com/lukky-nl/Magic-Door-Godot-4.5-release-page",
      "clone_url": "https://github.com/lukky-nl/Magic-Door-Godot-4.5-release-page.git",
      "ssh_url": "git@github.com:lukky-nl/Magic-Door-Godot-4.5-release-page.git",
      "homepage": null,
      "created_at": "2025-09-18T15:07:18Z",
      "updated_at": "2025-09-19T02:37:49Z",
      "pushed_at": "2025-09-18T15:07:20Z"
    },
    "stats": {
      "stars": 14,
      "forks": 1,
      "watchers": 14,
      "open_issues": 0,
      "size": 24
    },
    "tech_info": {
      "language": null,
      "languages": {},
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-19T03:46:20.492425"
  },
  {
    "basic_info": {
      "name": "vggt-mps",
      "full_name": "jmanhype/vggt-mps",
      "owner": "jmanhype",
      "description": "VGGT 3D Vision Agent optimized for Apple Silicon with Metal Performance Shaders",
      "url": "https://github.com/jmanhype/vggt-mps",
      "clone_url": "https://github.com/jmanhype/vggt-mps.git",
      "ssh_url": "git@github.com:jmanhype/vggt-mps.git",
      "homepage": "https://github.com/facebookresearch/vggt",
      "created_at": "2025-09-18T15:23:05Z",
      "updated_at": "2025-09-19T03:45:47Z",
      "pushed_at": "2025-09-18T17:15:40Z"
    },
    "stats": {
      "stars": 14,
      "forks": 3,
      "watchers": 14,
      "open_issues": 0,
      "size": 35801
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 649111,
        "Shell": 595
      },
      "license": "MIT License",
      "topics": [
        "3d-reconstruction",
        "apple-silicon",
        "claude-desktop",
        "computer-vision",
        "depth-estimation",
        "m1",
        "m2",
        "m3",
        "macos",
        "mcp",
        "metal-performance-shaders",
        "mps",
        "pytorch",
        "vggt"
      ]
    },
    "content": {
      "readme": "# VGGT-MPS: 3D Vision Agent for Apple Silicon\n\n🍎 **VGGT (Visual Geometry Grounded Transformer) optimized for Apple Silicon with Metal Performance Shaders (MPS)**\n\nTransform single or multi-view images into rich 3D reconstructions using Facebook Research's VGGT model, now accelerated on M1/M2/M3 Macs.\n\n## ✨ Features\n\n- **🚀 MPS Acceleration**: Full GPU acceleration on Apple Silicon using Metal Performance Shaders\n- **⚡ Sparse Attention**: O(n) memory scaling for city-scale reconstruction (100x savings!)\n- **🎥 Multi-View 3D Reconstruction**: Generate depth maps, point clouds, and camera poses from images\n- **🔧 MCP Integration**: Model Context Protocol server for Claude Desktop integration\n- **📦 5GB Model**: Efficient 1B parameter model that runs smoothly on Apple Silicon\n- **🛠️ Multiple Tools**: Video processing, 3D scene generation, COLMAP integration\n\n## 🎯 What VGGT Does\n\nVGGT reconstructs 3D scenes from images by predicting:\n- **Depth Maps**: Per-pixel depth estimation\n- **Camera Poses**: 6DOF camera parameters\n- **3D Point Clouds**: Dense 3D reconstruction\n- **Confidence Maps**: Reliability scores for predictions\n\n## 📋 Requirements\n\n- Apple Silicon Mac (M1/M2/M3)\n- Python 3.10+\n- 8GB+ RAM\n- 6GB disk space for model\n\n## 🚀 Quick Start\n\n### 1. Clone and Setup\n\n```bash\ngit clone https://github.com/jmanhype/vggt-mps.git\ncd vggt-mps\n\n# Create virtual environment\npython -m venv vggt-env\nsource vggt-env/bin/activate\n\n# Install dependencies\npip install -r requirements.txt\n```\n\n### 2. Download Model Weights\n\n```bash\n# Download the 5GB VGGT model\npython scripts/download_model.py\n```\n\nOr manually download from [Hugging Face](https://huggingface.co/facebook/VGGT-1B/resolve/main/model.pt)\n\n### 3. Test MPS Support\n\n```bash\npython tests/test_vggt_mps.py\n```\n\nExpected output:\n```\n✅ MPS (Metal Performance Shaders) available!\n   Running on Apple Silicon GPU\n✅ Model weights loaded to mps\n✅ MPS operations working correctly!\n```\n\n### 4. Run Demo\n\n```bash\n# Create test images\npython examples/create_test_images.py\n\n# Run 3D reconstruction demo\npython examples/demo_vggt_mps.py\n\n# Test sparse attention (O(n) scaling)\npython tests/sparse_attention/test_sparse_vggt_final.py\n```\n\n## 🔧 MCP Server Integration\n\n### Add to Claude Desktop\n\n1. Edit `~/Library/Application Support/Claude/claude_desktop_config.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"vggt-agent\": {\n      \"command\": \"uv\",\n      \"args\": [\n        \"run\",\n        \"--python\",\n        \"/path/to/vggt-mps/vggt-env/bin/python\",\n        \"--with\",\n        \"fastmcp\",\n        \"fastmcp\",\n        \"run\",\n        \"/path/to/vggt-mps/src/vggt_mps_mcp.py\"\n      ]\n    }\n  }\n}\n```\n\n2. Restart Claude Desktop\n\n### Available MCP Tools\n\n- `vggt_quick_start_inference` - Quick 3D reconstruction from images\n- `vggt_extract_video_frames` - Extract frames from video\n- `vggt_process_images` - Full VGGT pipeline\n- `vggt_create_3d_scene` - Generate GLB 3D files\n- `vggt_reconstruct_3d_scene` - Multi-view reconstruction\n- `vggt_visualize_reconstruction` - Create visualizations\n\n## 📁 Project Structure\n\n```\nvggt-mps/\n├── src/                         # Source code\n│   ├── vggt_mps_mcp.py         # MCP server\n│   └── tools/                   # VGGT tools (11 total)\n│       ├── readme.py           # Quick inference\n│       ├── demo_gradio.py      # Video & 3D tools\n│       ├── demo_viser.py       # Visualization\n│       └── demo_colmap.py      # COLMAP integration\n├── examples/                    # Example scripts\n│   ├── demo_vggt_mps.py       # Main demo\n│   ├── create_test_images.py  # Generate test data\n│   └── vggt_mps_inference.py  # Direct inference\n├── tests/                       # Test scripts\n│   ├── test_vggt_mps.py       # MPS test\n│   └── test_hub_load.py       # Hub loading test\n├── scripts/                     # Utility scripts\n│   └── download_model.py      # Model downloader\n├── repo/vggt/                   # VGGT source\n│   ├── hubconf.py              # Torch hub config\n│   └── vggt_model.pt          # Model (5GB)\n├── tmp/                         # Working directory\n│   ├── inputs/                 # Input images\n│   └── outputs/                # Results\n├── requirements.txt             # Dependencies\n├── README.md                    # Documentation\n└── LICENSE                      # MIT License\n```\n\n## 🖼️ Usage Examples\n\n### Process Images\n\n```python\nfrom src.tools.readme import vggt_quick_start_inference\n\nresult = vggt_quick_start_inference(\n    image_directory=\"./tmp/inputs\",\n    device=\"mps\",  # Use Apple Silicon GPU\n    max_images=4,\n    save_outputs=True\n)\n```\n\n### Extract Video Frames\n\n```python\nfrom src.tools.demo_gradio import vggt_extract_video_frames\n\nresult = vggt_extract_video_frames(\n    video_path=\"input_video.mp4\",\n    frame_interval_seconds=1.0\n)\n```\n\n### Create 3D Scene\n\n```python\nfrom src.tools.demo_viser import vggt_reconstruct_3d_scene\n\nresult = vggt_reconstruct_3d_scene(\n    images_dir=\"./tmp/inputs\",\n    device_type=\"mps\",\n    confidence_threshold=0.5\n)\n```\n\n## ⚡ Sparse Attent",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-19T03:46:21.591411"
  }
]