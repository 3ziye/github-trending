[
  {
    "basic_info": {
      "name": "Open-AutoGLM",
      "full_name": "zai-org/Open-AutoGLM",
      "owner": "zai-org",
      "description": "An Open Phone Agent Model & Framework. Unlocking the AI Phone for Everyone",
      "url": "https://github.com/zai-org/Open-AutoGLM",
      "clone_url": "https://github.com/zai-org/Open-AutoGLM.git",
      "ssh_url": "git@github.com:zai-org/Open-AutoGLM.git",
      "homepage": "https://autoglm.z.ai/blog",
      "created_at": "2025-12-08T09:23:44Z",
      "updated_at": "2026-01-04T03:04:30Z",
      "pushed_at": "2025-12-31T10:33:17Z"
    },
    "stats": {
      "stars": 20571,
      "forks": 3294,
      "watchers": 20571,
      "open_issues": 187,
      "size": 5249
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 265241
      },
      "license": "Apache License 2.0",
      "topics": [
        "agent",
        "phone-use-agent"
      ]
    },
    "content": {
      "readme": "# Open-AutoGLM\n\n[Readme in English](README_en.md)\n\n<div align=\"center\">\n<img src=resources/logo.svg width=\"20%\"/>\n</div>\n<p align=\"center\">\n    ğŸ‘‹ åŠ å…¥æˆ‘ä»¬çš„ <a href=\"resources/WECHAT.md\" target=\"_blank\">å¾®ä¿¡</a> ç¤¾åŒº\n</p>\n<p align=\"center\">\n    ğŸ¤ è¿›ä¸€æ­¥åœ¨æˆ‘ä»¬çš„äº§å“ <a href=\"https://autoglm.zhipuai.cn/autotyper/\" target=\"_blank\">æ™ºè°± AI è¾“å…¥æ³•</a> ä½“éªŒâ€œç”¨å˜´å‘æŒ‡ä»¤â€\n</p\n><p align=\"center\">\n    <a href=\"https://mp.weixin.qq.com/s/wRp22dmRVF23ySEiATiWIQ\" target=\"_blank\">AutoGLM å®æˆ˜æ´¾</a> å¼€å‘è€…æ¿€åŠ±æ´»åŠ¨ç«çƒ­è¿›è¡Œä¸­ï¼Œè·‘é€šã€äºŒåˆ›å³å¯ç“œåˆ†æ•°ä¸‡å…ƒç°é‡‘å¥–æ± ï¼æˆæœæäº¤ ğŸ‘‰ <a href=\"https://zhipu-ai.feishu.cn/share/base/form/shrcnE3ZuPD5tlOyVJ7d5Wtir8c?from=navigation\" target=\"_blank\">å…¥å£</a>\n</p>\n\n## æ‡’äººç‰ˆå¿«é€Ÿå®‰è£…\n\nä½ å¯ä»¥ä½¿ç”¨Claude Codeï¼Œé…ç½® [GLM Coding Plan](https://bigmodel.cn/glm-coding) åï¼Œè¾“å…¥ä»¥ä¸‹æç¤ºè¯ï¼Œå¿«é€Ÿéƒ¨ç½²æœ¬é¡¹ç›®ã€‚\n\n```\nè®¿é—®æ–‡æ¡£ï¼Œä¸ºæˆ‘å®‰è£… AutoGLM\nhttps://raw.githubusercontent.com/zai-org/Open-AutoGLM/refs/heads/main/README.md\n```\n\n## é¡¹ç›®ä»‹ç»\n\nPhone Agent æ˜¯ä¸€ä¸ªåŸºäº AutoGLM æ„å»ºçš„æ‰‹æœºç«¯æ™ºèƒ½åŠ©ç†æ¡†æ¶ï¼Œå®ƒèƒ½å¤Ÿä»¥å¤šæ¨¡æ€æ–¹å¼ç†è§£æ‰‹æœºå±å¹•å†…å®¹ï¼Œå¹¶é€šè¿‡è‡ªåŠ¨åŒ–æ“ä½œå¸®åŠ©ç”¨æˆ·å®Œæˆä»»åŠ¡ã€‚ç³»ç»Ÿé€šè¿‡\nADB(Android Debug Bridge)æ¥æ§åˆ¶è®¾å¤‡ï¼Œä»¥è§†è§‰è¯­è¨€æ¨¡å‹è¿›è¡Œå±å¹•æ„ŸçŸ¥ï¼Œå†ç»“åˆæ™ºèƒ½è§„åˆ’èƒ½åŠ›ç”Ÿæˆå¹¶æ‰§è¡Œæ“ä½œæµç¨‹ã€‚ç”¨æˆ·åªéœ€ç”¨è‡ªç„¶è¯­è¨€æè¿°éœ€æ±‚ï¼Œå¦‚â€œæ‰“å¼€å°çº¢ä¹¦æœç´¢ç¾é£Ÿâ€ï¼ŒPhone\nAgent å³å¯è‡ªåŠ¨è§£ææ„å›¾ã€ç†è§£å½“å‰ç•Œé¢ã€è§„åˆ’ä¸‹ä¸€æ­¥åŠ¨ä½œå¹¶å®Œæˆæ•´ä¸ªæµç¨‹ã€‚ç³»ç»Ÿè¿˜å†…ç½®æ•æ„Ÿæ“ä½œç¡®è®¤æœºåˆ¶ï¼Œå¹¶æ”¯æŒåœ¨ç™»å½•æˆ–éªŒè¯ç åœºæ™¯ä¸‹è¿›è¡Œäººå·¥æ¥ç®¡ã€‚åŒæ—¶ï¼Œå®ƒæä¾›è¿œç¨‹\nADB è°ƒè¯•èƒ½åŠ›ï¼Œå¯é€šè¿‡ WiFi æˆ–ç½‘ç»œè¿æ¥è®¾å¤‡ï¼Œå®ç°çµæ´»çš„è¿œç¨‹æ§åˆ¶ä¸å¼€å‘ã€‚\n\n> âš ï¸\n> æœ¬é¡¹ç›®ä»…ä¾›ç ”ç©¶å’Œå­¦ä¹ ä½¿ç”¨ã€‚ä¸¥ç¦ç”¨äºéæ³•è·å–ä¿¡æ¯ã€å¹²æ‰°ç³»ç»Ÿæˆ–ä»»ä½•è¿æ³•æ´»åŠ¨ã€‚è¯·ä»”ç»†å®¡é˜… [ä½¿ç”¨æ¡æ¬¾](resources/privacy_policy.txt)ã€‚\n\n## æ¨¡å‹ä¸‹è½½åœ°å€\n\n| Model                         | Download Links                                                                                                                                                         |\n|-------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| AutoGLM-Phone-9B              | [ğŸ¤— Hugging Face](https://huggingface.co/zai-org/AutoGLM-Phone-9B)<br>[ğŸ¤– ModelScope](https://modelscope.cn/models/ZhipuAI/AutoGLM-Phone-9B)                           |\n| AutoGLM-Phone-9B-Multilingual | [ğŸ¤— Hugging Face](https://huggingface.co/zai-org/AutoGLM-Phone-9B-Multilingual)<br>[ğŸ¤– ModelScope](https://modelscope.cn/models/ZhipuAI/AutoGLM-Phone-9B-Multilingual) |\n\nå…¶ä¸­ï¼Œ`AutoGLM-Phone-9B` æ˜¯é’ˆå¯¹ä¸­æ–‡æ‰‹æœºåº”ç”¨ä¼˜åŒ–çš„æ¨¡å‹ï¼Œè€Œ `AutoGLM-Phone-9B-Multilingual` æ”¯æŒè‹±è¯­åœºæ™¯ï¼Œé€‚ç”¨äºåŒ…å«è‹±æ–‡ç­‰å…¶ä»–è¯­è¨€å†…å®¹çš„åº”ç”¨ã€‚\n\n## Android ç¯å¢ƒå‡†å¤‡\n\n### 1. Python ç¯å¢ƒ\n\nå»ºè®®ä½¿ç”¨ Python 3.10 åŠä»¥ä¸Šç‰ˆæœ¬ã€‚\n\n### 2. æ‰‹æœºè°ƒè¯•å‘½ä»¤è¡Œå·¥å…·\n\næ ¹æ®ä½ çš„è®¾å¤‡ç±»å‹é€‰æ‹©ç›¸åº”çš„å·¥å…·ï¼š\n\n#### å¯¹äº Android è®¾å¤‡ - ä½¿ç”¨ ADB\n\n1. ä¸‹è½½å®˜æ–¹ ADB [å®‰è£…åŒ…](https://developer.android.com/tools/releases/platform-tools?hl=zh-cn)ï¼Œå¹¶è§£å‹åˆ°è‡ªå®šä¹‰è·¯å¾„\n2. é…ç½®ç¯å¢ƒå˜é‡\n\n- MacOS é…ç½®æ–¹æ³•ï¼šåœ¨ `Terminal` æˆ–è€…ä»»ä½•å‘½ä»¤è¡Œå·¥å…·é‡Œ\n\n  ```bash\n  # å‡è®¾è§£å‹åçš„ç›®å½•ä¸º ~/Downloads/platform-toolsã€‚å¦‚æœä¸æ˜¯è¯·è‡ªè¡Œè°ƒæ•´å‘½ä»¤ã€‚\n  export PATH=${PATH}:~/Downloads/platform-tools\n  ```\n\n- Windows é…ç½®æ–¹æ³•ï¼šå¯å‚è€ƒ [ç¬¬ä¸‰æ–¹æ•™ç¨‹](https://blog.csdn.net/x2584179909/article/details/108319973) è¿›è¡Œé…ç½®ã€‚\n\n#### å¯¹äºé¸¿è’™è®¾å¤‡ (HarmonyOS NEXTç‰ˆæœ¬ä»¥ä¸Š) - ä½¿ç”¨ HDC\n\n1. ä¸‹è½½ HDC å·¥å…·ï¼š\n   - ä» [HarmonyOS SDK](https://developer.huawei.com/consumer/cn/download/) ä¸‹è½½\n2. é…ç½®ç¯å¢ƒå˜é‡\n\n- MacOS/Linux é…ç½®æ–¹æ³•ï¼š\n\n  ```bash\n  # å‡è®¾è§£å‹åçš„ç›®å½•ä¸º ~/Downloads/harmonyos-sdk/toolchainsã€‚è¯·æ ¹æ®å®é™…è·¯å¾„è°ƒæ•´ã€‚\n  export PATH=${PATH}:~/Downloads/harmonyos-sdk/toolchains\n  ```\n\n- Windows é…ç½®æ–¹æ³•ï¼šå°† HDC å·¥å…·æ‰€åœ¨ç›®å½•æ·»åŠ åˆ°ç³»ç»Ÿ PATH ç¯å¢ƒå˜é‡\n\n### 3. Android 7.0+ æˆ– HarmonyOS è®¾å¤‡ï¼Œå¹¶å¯ç”¨ `å¼€å‘è€…æ¨¡å¼` å’Œ `USB è°ƒè¯•`\n\n1. å¼€å‘è€…æ¨¡å¼å¯ç”¨ï¼šé€šå¸¸å¯ç”¨æ–¹æ³•æ˜¯ï¼Œæ‰¾åˆ° `è®¾ç½®-å…³äºæ‰‹æœº-ç‰ˆæœ¬å·` ç„¶åè¿ç»­å¿«é€Ÿç‚¹å‡» 10\n   æ¬¡å·¦å³ï¼Œç›´åˆ°å¼¹å‡ºå¼¹çª—æ˜¾ç¤ºâ€œå¼€å‘è€…æ¨¡å¼å·²å¯ç”¨â€ã€‚ä¸åŒæ‰‹æœºä¼šæœ‰äº›è®¸å·®åˆ«ï¼Œå¦‚æœæ‰¾ä¸åˆ°ï¼Œå¯ä»¥ä¸Šç½‘æœç´¢ä¸€ä¸‹æ•™ç¨‹ã€‚\n2. USB è°ƒè¯•å¯ç”¨ï¼šå¯ç”¨å¼€å‘è€…æ¨¡å¼ä¹‹åï¼Œä¼šå‡ºç° `è®¾ç½®-å¼€å‘è€…é€‰é¡¹-USB è°ƒè¯•`ï¼Œå‹¾é€‰å¯ç”¨\n3. éƒ¨åˆ†æœºå‹åœ¨è®¾ç½®å¼€å‘è€…é€‰é¡¹ä»¥å, å¯èƒ½éœ€è¦é‡å¯è®¾å¤‡æ‰èƒ½ç”Ÿæ•ˆ. å¯ä»¥æµ‹è¯•ä¸€ä¸‹: å°†æ‰‹æœºç”¨USBæ•°æ®çº¿è¿æ¥åˆ°ç”µè„‘å, `adb devices`\n   æŸ¥çœ‹æ˜¯å¦æœ‰è®¾å¤‡ä¿¡æ¯, å¦‚æœæ²¡æœ‰è¯´æ˜è¿æ¥å¤±è´¥.\n\n**è¯·åŠ¡å¿…ä»”ç»†æ£€æŸ¥ç›¸å…³æƒé™**\n\n![æƒé™](resources/screenshot-20251209-181423.png)\n\n### 4. å®‰è£… ADB Keyboard(ä»… Android è®¾å¤‡éœ€è¦ï¼Œç”¨äºæ–‡æœ¬è¾“å…¥)\n\n**æ³¨æ„ï¼šé¸¿è’™è®¾å¤‡ä½¿ç”¨åŸç”Ÿè¾“å…¥æ–¹æ³•ï¼Œæ— éœ€å®‰è£… ADB Keyboardã€‚**\n\nå¦‚æœä½ ä½¿ç”¨çš„æ˜¯ Android è®¾å¤‡ï¼š\n\nä¸‹è½½ [å®‰è£…åŒ…](https://github.com/senzhk/ADBKeyBoard/blob/master/ADBKeyboard.apk) å¹¶åœ¨å¯¹åº”çš„å®‰å“è®¾å¤‡ä¸­è¿›è¡Œå®‰è£…ã€‚\næ³¨æ„ï¼Œå®‰è£…å®Œæˆåè¿˜éœ€è¦åˆ° `è®¾ç½®-è¾“å…¥æ³•` æˆ–è€… `è®¾ç½®-é”®ç›˜åˆ—è¡¨` ä¸­å¯ç”¨ `ADB Keyboard` æ‰èƒ½ç”Ÿæ•ˆ(æˆ–ä½¿ç”¨å‘½ä»¤`adb shell ime enable com.android.adbkeyboard/.AdbIME`[How-to-use](https://github.com/senzhk/ADBKeyBoard/blob/master/README.md#how-to-use))\n\n## iPhone ç¯å¢ƒå‡†å¤‡\n\nå¦‚æœä½ ä½¿ç”¨çš„æ˜¯ iPhone è®¾å¤‡ï¼Œè¯·å‚è€ƒä¸“é—¨çš„ iOS é…ç½®æ–‡æ¡£ï¼š\n\nğŸ“± [iOS ç¯å¢ƒé…ç½®æŒ‡å—](docs/ios_setup/ios_setup.md)\n\nè¯¥æ–‡æ¡£è¯¦ç»†ä»‹ç»äº†å¦‚ä½•é…ç½® WebDriverAgent å’Œ iPhone è®¾å¤‡ï¼Œä»¥ä¾¿åœ¨ iOS ä¸Šä½¿ç”¨ AutoGLMã€‚\n\n## éƒ¨ç½²å‡†å¤‡å·¥ä½œ\n\n### 1. å®‰è£…ä¾èµ–\n\n```bash\npip install -r requirements.txt \npip install -e .\n```\n\n### 2. é…ç½® ADB æˆ– HDC\n\n#### å¯¹äº Android è®¾å¤‡\n\nç¡®è®¤ **USBæ•°æ®çº¿å…·æœ‰æ•°æ®ä¼ è¾“åŠŸèƒ½**, è€Œä¸æ˜¯ä»…æœ‰å……ç”µåŠŸèƒ½\n\nç¡®ä¿å·²å®‰è£… ADB å¹¶ä½¿ç”¨ **USBæ•°æ®çº¿** è¿æ¥è®¾å¤‡ï¼š\n\n```bash\n# æ£€æŸ¥å·²è¿æ¥çš„è®¾å¤‡\nadb devices\n\n# è¾“å‡ºç»“æœåº”æ˜¾ç¤ºä½ çš„è®¾å¤‡ï¼Œå¦‚ï¼š\n# List of devices attached\n# emulator-5554   device\n```\n\n#### å¯¹äºé¸¿è’™è®¾å¤‡\n\nç¡®è®¤ **USBæ•°æ®çº¿å…·æœ‰æ•°æ®ä¼ è¾“åŠŸèƒ½**, è€Œä¸æ˜¯ä»…æœ‰å……ç”µåŠŸèƒ½\n\nç¡®ä¿å·²å®‰è£… HDC å¹¶ä½¿ç”¨ **USBæ•°æ®çº¿** è¿æ¥è®¾å¤‡ï¼š\n\n```bash\n# æ£€æŸ¥å·²è¿æ¥çš„è®¾å¤‡\nhdc list targets\n\n# è¾“å‡ºç»“æœåº”æ˜¾ç¤ºä½ çš„è®¾å¤‡ï¼Œå¦‚ï¼š\n# 7001005458323933328a01bce01c2500\n```\n\n### 3. å¯åŠ¨æ¨¡å‹æœåŠ¡\n\nä½ å¯ä»¥é€‰æ‹©è‡ªè¡Œéƒ¨ç½²æ¨¡å‹æœåŠ¡ï¼Œæˆ–ä½¿ç”¨ç¬¬ä¸‰æ–¹æ¨¡å‹æœåŠ¡å•†ã€‚\n\n#### é€‰é¡¹ A: ä½¿ç”¨ç¬¬ä¸‰æ–¹æ¨¡å‹æœåŠ¡\n\nå¦‚æœä½ ä¸æƒ³è‡ªè¡Œéƒ¨ç½²æ¨¡å‹ï¼Œå¯ä»¥ä½¿ç”¨ä»¥ä¸‹å·²éƒ¨ç½²æˆ‘ä»¬æ¨¡å‹çš„ç¬¬ä¸‰æ–¹æœåŠ¡ï¼š\n\n**1. æ™ºè°± BigModel**\n\n- æ–‡æ¡£: https://docs.bigmodel.cn/cn/api/introduction\n- `--base-url`: `https://open.bigmodel.cn/api/paas/v4`\n- `--model`: `autoglm-phone`\n- `--apikey`: åœ¨æ™ºè°±å¹³å°ç”³è¯·ä½ çš„ API Key\n\n**2. ModelScope(é­”æ­ç¤¾åŒº)**\n\n- æ–‡æ¡£: https://modelscope.cn/models/ZhipuAI/AutoGLM-Phone-9B\n- `--base-url`: `https://api-inference.modelscope.cn/v1`\n- `--model`: `ZhipuAI/AutoGLM-Phone-9B`\n- `--apikey`: åœ¨ ModelScope å¹³å°ç”³è¯·ä½ çš„ API Key\n\nä½¿ç”¨ç¬¬ä¸‰æ–¹æœåŠ¡çš„ç¤ºä¾‹ï¼š\n\n```bash\n# ä½¿ç”¨æ™ºè°± BigModel\npython main.py --base-url https://open.bigmodel.cn/api/paas/",
      "default_branch": "main"
    },
    "fetched_at": "2026-01-04T03:04:55.177375"
  },
  {
    "basic_info": {
      "name": "ml-sharp",
      "full_name": "apple/ml-sharp",
      "owner": "apple",
      "description": "Sharp Monocular View Synthesis in Less Than a Second",
      "url": "https://github.com/apple/ml-sharp",
      "clone_url": "https://github.com/apple/ml-sharp.git",
      "ssh_url": "git@github.com:apple/ml-sharp.git",
      "homepage": "https://apple.github.io/ml-sharp/",
      "created_at": "2025-12-12T03:46:09Z",
      "updated_at": "2026-01-04T02:25:32Z",
      "pushed_at": "2025-12-19T05:14:10Z"
    },
    "stats": {
      "stars": 6410,
      "forks": 410,
      "watchers": 6410,
      "open_issues": 50,
      "size": 189483
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 186219
      },
      "license": "Other",
      "topics": []
    },
    "content": {
      "readme": "# Sharp Monocular View Synthesis in Less Than a Second\n\n[![Project Page](https://img.shields.io/badge/Project-Page-green)](https://apple.github.io/ml-sharp/)\n[![arXiv](https://img.shields.io/badge/arXiv-2512.10685-b31b1b.svg)](https://arxiv.org/abs/2512.10685)\n\nThis software project accompanies the research paper: _Sharp Monocular View Synthesis in Less Than a Second_\nby _Lars Mescheder, Wei Dong, Shiwei Li, Xuyang Bai, Marcel Santos, Peiyun Hu, Bruno Lecouat, Mingmin Zhen, AmaÃ«l Delaunoy,\nTian Fang, Yanghai Tsin, Stephan Richter and Vladlen Koltun_.\n\n![](data/teaser.jpg)\n\nWe present SHARP, an approach to photorealistic view synthesis from a single image. Given a single photograph, SHARP regresses the parameters of a 3D Gaussian representation of the depicted scene. This is done in less than a second on a standard GPU via a single feedforward pass through a neural network. The 3D Gaussian representation produced by SHARP can then be rendered in real time, yielding high-resolution photorealistic images for nearby views. The representation is metric, with absolute scale, supporting metric camera movements. Experimental results demonstrate that SHARP delivers robust zero-shot generalization across datasets. It sets a new state of the art on multiple datasets, reducing LPIPS by 25â€“34% and DISTS by 21â€“43% versus the best prior model, while lowering the synthesis time by three orders of magnitude.\n\n## Getting started\n\nWe recommend to first create a python environment:\n\n```\nconda create -n sharp python=3.13\n```\n\nAfterwards, you can install the project using\n\n```\npip install -r requirements.txt\n```\n\nTo test the installation, run\n\n```\nsharp --help\n```\n\n## Using the CLI\n\nTo run prediction:\n\n```\nsharp predict -i /path/to/input/images -o /path/to/output/gaussians\n```\n\nThe model checkpoint will be downloaded automatically on first run and cached locally at `~/.cache/torch/hub/checkpoints/`.\n\nAlternatively, you can download the model directly:\n\n```\nwget https://ml-site.cdn-apple.com/models/sharp/sharp_2572gikvuh.pt\n```\n\nTo use a manually downloaded checkpoint, specify it with the `-c` flag:\n\n```\nsharp predict -i /path/to/input/images -o /path/to/output/gaussians -c sharp_2572gikvuh.pt\n```\n\nThe results will be 3D gaussian splats (3DGS) in the output folder. The 3DGS `.ply` files are compatible to various public 3DGS renderers. We follow the OpenCV coordinate convention (x right, y down, z forward). The 3DGS scene center is roughly at (0, 0, +z). When dealing with 3rdparty renderers, please scale and rotate to re-center the scene accordingly.\n\n### Rendering trajectories (CUDA GPU only)\n\nAdditionally you can render videos with a camera trajectory. While the gaussians prediction works for all CPU, CUDA, and MPS, rendering videos via the `--render` option currently requires a CUDA GPU. The gsplat renderer takes a while to initialize at the first launch.\n\n```\nsharp predict -i /path/to/input/images -o /path/to/output/gaussians --render\n\n# Or from the intermediate gaussians:\nsharp render -i /path/to/output/gaussians -o /path/to/output/renderings\n```\n\n## Evaluation\n\nPlease refer to the paper for both quantitative and qualitative evaluations.\nAdditionally, please check out this [qualitative examples page](https://apple.github.io/ml-sharp/) containing several video comparisons against related work.\n\n## Citation\n\nIf you find our work useful, please cite the following paper:\n\n```bibtex\n@inproceedings{Sharp2025:arxiv,\n  title      = {Sharp Monocular View Synthesis in Less Than a Second},\n  author     = {Lars Mescheder and Wei Dong and Shiwei Li and Xuyang Bai and Marcel Santos and Peiyun Hu and Bruno Lecouat and Mingmin Zhen and Ama\\\"{e}l Delaunoy and Tian Fang and Yanghai Tsin and Stephan R. Richter and Vladlen Koltun},\n  journal    = {arXiv preprint arXiv:2512.10685},\n  year       = {2025},\n  url        = {https://arxiv.org/abs/2512.10685},\n}\n```\n\n## Acknowledgements\n\nOur codebase is built using multiple opensource contributions, please see [ACKNOWLEDGEMENTS](ACKNOWLEDGEMENTS) for more details.\n\n## License\n\nPlease check out the repository [LICENSE](LICENSE) before using the provided code and\n[LICENSE_MODEL](LICENSE_MODEL) for the released models.\n",
      "default_branch": "main"
    },
    "fetched_at": "2026-01-04T03:04:56.349833"
  },
  {
    "basic_info": {
      "name": "Agent-Skills-for-Context-Engineering",
      "full_name": "muratcankoylan/Agent-Skills-for-Context-Engineering",
      "owner": "muratcankoylan",
      "description": "A comprehensive collection of Agent Skills for context engineering, multi-agent architectures, and production agent systems. Use when building, optimizing, or debugging agent systems that require effective context management.",
      "url": "https://github.com/muratcankoylan/Agent-Skills-for-Context-Engineering",
      "clone_url": "https://github.com/muratcankoylan/Agent-Skills-for-Context-Engineering.git",
      "ssh_url": "git@github.com:muratcankoylan/Agent-Skills-for-Context-Engineering.git",
      "homepage": null,
      "created_at": "2025-12-21T02:43:42Z",
      "updated_at": "2026-01-04T03:04:35Z",
      "pushed_at": "2026-01-02T18:16:01Z"
    },
    "stats": {
      "stars": 5398,
      "forks": 424,
      "watchers": 5398,
      "open_issues": 3,
      "size": 3395
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 226646,
        "PLpgSQL": 6239
      },
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "# Agent Skills for Context Engineering\n\nA comprehensive, open collection of Agent Skills focused on context engineering principles for building production-grade AI agent systems. These skills teach the art and science of curating context to maximize agent effectiveness across any agent platform.\n\n## What is Context Engineering?\n\nContext engineering is the discipline of managing the language model's context window. Unlike prompt engineering, which focuses on crafting effective instructions, context engineering addresses the holistic curation of all information that enters the model's limited attention budget: system prompts, tool definitions, retrieved documents, message history, and tool outputs.\n\nThe fundamental challenge is that context windows are constrained not by raw token capacity but by attention mechanics. As context length increases, models exhibit predictable degradation patterns: the \"lost-in-the-middle\" phenomenon, U-shaped attention curves, and attention scarcity. Effective context engineering means finding the smallest possible set of high-signal tokens that maximize the likelihood of desired outcomes.\n\n## Skills Overview\n\n### Foundational Skills\n\nThese skills establish the foundational understanding required for all subsequent context engineering work.\n\n| Skill | Description |\n|-------|-------------|\n| [context-fundamentals](skills/context-fundamentals/) | Understand what context is, why it matters, and the anatomy of context in agent systems |\n| [context-degradation](skills/context-degradation/) | Recognize patterns of context failure: lost-in-middle, poisoning, distraction, and clash |\n| [context-compression](skills/context-compression/) | Design and evaluate compression strategies for long-running sessions |\n\n### Architectural Skills\n\nThese skills cover the patterns and structures for building effective agent systems.\n\n| Skill | Description |\n|-------|-------------|\n| [multi-agent-patterns](skills/multi-agent-patterns/) | Master orchestrator, peer-to-peer, and hierarchical multi-agent architectures |\n| [memory-systems](skills/memory-systems/) | Design short-term, long-term, and graph-based memory architectures |\n| [tool-design](skills/tool-design/) | Build tools that agents can use effectively |\n\n### Operational Skills\n\nThese skills address the ongoing operation and optimization of agent systems.\n\n| Skill | Description |\n|-------|-------------|\n| [context-optimization](skills/context-optimization/) | Apply compaction, masking, and caching strategies |\n| [evaluation](skills/evaluation/) | Build evaluation frameworks for agent systems |\n| [advanced-evaluation](skills/advanced-evaluation/) | Master LLM-as-a-Judge techniques: direct scoring, pairwise comparison, rubric generation, and bias mitigation |\n\n### Development Methodology\n\nThese skills cover the meta-level practices for building LLM-powered projects.\n\n| Skill | Description |\n|-------|-------------|\n| [project-development](skills/project-development/) | **NEW** Design and build LLM projects from ideation through deployment, including task-model fit analysis, pipeline architecture, and structured output design |\n\n## Design Philosophy\n\n### Progressive Disclosure\n\nEach skill is structured for efficient context use. At startup, agents load only skill names and descriptions. Full content loads only when a skill is activated for relevant tasks.\n\n### Platform Agnosticism\n\nThese skills focus on transferable principles rather than vendor-specific implementations. The patterns work across Claude Code, Cursor, and any agent platform that supports skills or allows custom instructions.\n\n### Conceptual Foundation with Practical Examples\n\nScripts and examples demonstrate concepts using Python pseudocode that works across environments without requiring specific dependency installations.\n\n## Usage\n\n### Usage with Claude Code\n\nThis repository is a **Claude Code Plugin Marketplace** containing context engineering skills that Claude automatically discovers and activates based on your task context.\n\n### Installation\n\n**Step 1: Add the Marketplace**\n\nRun this command in Claude Code to register this repository as a plugin source:\n\n```\n/plugin marketplace add muratcankoylan/Agent-Skills-for-Context-Engineering\n```\n\n**Step 2: Browse and Install**\n\nOption A - Browse available plugins:\n1. Select `Browse and install plugins`\n2. Select `context-engineering-marketplace`\n3. Choose a plugin (e.g., `context-engineering-fundamentals`, `agent-architecture`)\n4. Select `Install now`\n\nOption B - Direct install via command:\n\n```\n/plugin install context-engineering-fundamentals@context-engineering-marketplace\n/plugin install agent-architecture@context-engineering-marketplace\n/plugin install agent-evaluation@context-engineering-marketplace\n/plugin install agent-development@context-engineering-marketplace\n```\n\n### Available Plugins\n\n| Plugin | Skills Included |\n|--------|-----------------|\n| `context-engineering-fundamentals` | context-fundamentals, context-degradation, conte",
      "default_branch": "main"
    },
    "fetched_at": "2026-01-04T03:04:57.527897"
  },
  {
    "basic_info": {
      "name": "vibe-coding-cn",
      "full_name": "2025Emma/vibe-coding-cn",
      "owner": "2025Emma",
      "description": null,
      "url": "https://github.com/2025Emma/vibe-coding-cn",
      "clone_url": "https://github.com/2025Emma/vibe-coding-cn.git",
      "ssh_url": "git@github.com:2025Emma/vibe-coding-cn.git",
      "homepage": null,
      "created_at": "2025-12-17T00:10:23Z",
      "updated_at": "2026-01-04T03:02:20Z",
      "pushed_at": "2025-12-17T00:11:18Z"
    },
    "stats": {
      "stars": 4965,
      "forks": 586,
      "watchers": 4965,
      "open_issues": 0,
      "size": 28890
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 34941,
        "Shell": 28256,
        "Makefile": 842
      },
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "<!--\n-------------------------------------------------------------------------------\n  é¡¹ç›®å¤´éƒ¨åŒºåŸŸ (HEADER)\n-------------------------------------------------------------------------------\n-->\n<p align=\"center\">\n  <!-- å»ºè®®å°ºå¯¸: 1280x640pxã€‚å¯ä»¥ä½¿ç”¨ Canva, Figma æˆ– https://banners.beyondco.de/ ç­‰å·¥å…·åˆ¶ä½œ -->\n  <img src=\"https://github.com/tukuaiai.png\" alt=\"Vibe Coding æŒ‡å—\" width=\"80px\">\n</p>\n\n<div align=\"center\">\n\n# Vibe Coding æŒ‡å—\n\n**ä¸€ä¸ªé€šè¿‡ä¸ AI ç»“å¯¹ç¼–ç¨‹ï¼Œå°†æƒ³æ³•å˜ä¸ºç°å®çš„ç»ˆæå·¥ä½œç«™**\n\n---\n\n<!--\n  å¾½ç« åŒºåŸŸ (BADGES)\n-->\n<!-- é¡¹ç›®çŠ¶æ€å¾½ç«  -->\n<p>\n  <a href=\"https://github.com/tukuaiai/vibe-coding-cn/actions\"><img src=\"https://img.shields.io/github/actions/workflow/status/tukuaiai/vibe-coding-cn/main.yml?label=%E6%9E%84%E5%BB%BA%E7%8A%B6%E6%80%81&style=for-the-badge\" alt=\"æ„å»ºçŠ¶æ€\"></a>\n  <a href=\"https://github.com/tukuaiai/vibe-coding-cn/releases\"><img src=\"https://img.shields.io/github/v/release/tukuaiai/vibe-coding-cn?label=%E6%9C%80%E6%96%B0%E7%89%88%E6%9C%AC&style=for-the-badge\" alt=\"æœ€æ–°ç‰ˆæœ¬\"></a>\n  <a href=\"LICENSE\"><img src=\"https://img.shields.io/github/license/tukuaiai/vibe-coding-cn?label=%E8%AE%B8%E5%8F%AF%E8%AF%81&style=for-the-badge\" alt=\"è®¸å¯è¯\"></a>\n  <a href=\"https://github.com/tukuaiai/vibe-coding-cn\"><img src=\"https://img.shields.io/github/languages/top/tukuaiai/vibe-coding-cn?label=%E4%B8%BB%E8%A6%81%E8%AF%AD%E8%A8%80&style=for-the-badge\" alt=\"ä¸»è¦è¯­è¨€\"></a>\n  <a href=\"https://github.com/tukuaiai/vibe-coding-cn\"><img src=\"https://img.shields.io/github/languages/code-size/tukuaiai/vibe-coding-cn?label=%E4%BB%A3%E7%A0%81%E9%87%8F&style=for-the-badge\" alt=\"ä»£ç é‡\"></a>\n  <a href=\"https://github.com/tukuaiai/vibe-coding-cn/graphs/contributors\"><img src=\"https://img.shields.io/github/contributors/tukuaiai/vibe-coding-cn?label=%E8%B4%A1%E7%8C%AE%E8%80%85&style=for-the-badge\" alt=\"è´¡çŒ®è€…\"></a>\n  <a href=\"https://t.me/glue_coding\"><img src=\"https://img.shields.io/badge/èŠå¤©-Telegram-blue?style=for-the-badge&logo=telegram\" alt=\"äº¤æµç¾¤\"></a>\n</p>\n\n<!-- å¤šè¯­è¨€å…¥å£ -->\n<p>\n  <a href=\"./i18n/zh/README.md\"><img src=\"https://img.shields.io/badge/è¯­è¨€-ä¸­æ–‡-red?style=for-the-badge\" alt=\"ç®€ä½“ä¸­æ–‡\"></a>\n  <a href=\"./i18n/en/README.md\"><img src=\"https://img.shields.io/badge/è¯­è¨€-English-lightgrey?style=for-the-badge\" alt=\"English\"></a>\n  <a href=\"./i18n/he/\"><img src=\"https://img.shields.io/badge/è¯­è¨€-×¢×‘×¨×™×ª-navy?style=for-the-badge\" alt=\"Hebrew\"></a>\n  <a href=\"./i18n/ar/\"><img src=\"https://img.shields.io/badge/è¯­è¨€-Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©-brown?style=for-the-badge\" alt=\"Arabic\"></a>\n  <a href=\"./i18n/bn/\"><img src=\"https://img.shields.io/badge/è¯­è¨€-à¦¬à¦¾à¦‚à¦²à¦¾-orange?style=for-the-badge\" alt=\"Bengali\"></a>\n  <a href=\"./i18n/de/\"><img src=\"https://img.shields.io/badge/è¯­è¨€-Deutsch-black?style=for-the-badge\" alt=\"Deutsch\"></a>\n  <a href=\"./i18n/es/\"><img src=\"https://img.shields.io/badge/è¯­è¨€-EspaÃ±ol-yellow?style=for-the-badge\" alt=\"EspaÃ±ol\"></a>\n  <a href=\"./i18n/fa/\"><img src=\"https://img.shields.io/badge/è¯­è¨€-ÙØ§Ø±Ø³ÛŒ-purple?style=for-the-badge\" alt=\"Farsi\"></a>\n  <a href=\"./i18n/fr/\"><img src=\"https://img.shields.io/badge/è¯­è¨€-FranÃ§ais-blue?style=for-the-badge\" alt=\"FranÃ§ais\"></a>\n  <a href=\"./i18n/ha/\"><img src=\"https://img.shields.io/badge/è¯­è¨€-Hausa-darkgreen?style=for-the-badge\" alt=\"Hausa\"></a>\n  <a href=\"./i18n/hi/\"><img src=\"https://img.shields.io/badge/è¯­è¨€-à¤¹à¤¿à¤¨à¥à¤¦à¥€-darkorange?style=for-the-badge\" alt=\"Hindi\"></a>\n  <a href=\"./i18n/id/\"><img src=\"https://img.shields.io/badge/è¯­è¨€-Bahasa%20Indonesia-teal?style=for-the-badge\" alt=\"Bahasa Indonesia\"></a>\n  <a href=\"./i18n/it/\"><img src=\"https://img.shields.io/badge/è¯­è¨€-Italiano-green?style=for-the-badge\" alt=\"Italiano\"></a>\n  <a href=\"./i18n/ja/\"><img src=\"https://img.shields.io/badge/è¯­è¨€-æ—¥æœ¬èª-indigo?style=for-the-badge\" alt=\"æ—¥æœ¬èª\"></a>\n  <a href=\"./i18n/ko/\"><img src=\"https://img.shields.io/badge/è¯­è¨€-í•œêµ­ì–´-slateblue?style=for-the-badge\" alt=\"í•œêµ­ì–´\"></a>\n  <a href=\"./i18n/ms/\"><img src=\"https://img.shields.io/badge/è¯­è¨€-Bahasa%20Melayu-seagreen?style=for-the-badge\" alt=\"Bahasa Melayu\"></a>\n  <a href=\"./i18n/nl/\"><img src=\"https://img.shields.io/badge/è¯­è¨€-Nederlands-darkred?style=for-the-badge\" alt=\"Nederlands\"></a>\n  <a href=\"./i18n/pl/\"><img src=\"https://img.shields.io/badge/è¯­è¨€-Polski-crimson?style=for-the-badge\" alt=\"Polski\"></a>\n  <a href=\"./i18n/pt/\"><img src=\"https://img.shields.io/badge/è¯­è¨€-PortuguÃªs-darkslategray?style=for-the-badge\" alt=\"PortuguÃªs\"></a>\n  <a href=\"./i18n/ru/\"><img src=\"https://img.shields.io/badge/è¯­è¨€-Ğ ÑƒÑÑĞºĞ¸Ğ¹-steelblue?style=for-the-badge\" alt=\"Ğ ÑƒÑÑĞºĞ¸Ğ¹\"></a>\n  <a href=\"./i18n/sw/\"><img src=\"https://img.shields.io/badge/è¯­è¨€-Kiswahili-forestgreen?style=for-the-badge\" alt=\"Swahili\"></a>\n  <a href=\"./i18n/ta/\"><img src=\"https://img.shields.io/badge/è¯­è¨€-à®¤à®®à®¿à®´à¯-darkmagenta?style=for-the-badge\" alt=\"Tamil\"></a>\n  <a href=\"./i18n/th/\"><img src=\"https://img.shields.io/badge/è¯­è¨€-à¹„à¸—à¸¢-royalblue?style=for-the-badge\" alt=\"à¸ à¸²à¸©à¸²à¹„à¸—à¸¢\"></a>\n  <a href=\"./i18n/tr/\"><img src=\"https://img.shields.io/badge/è¯­è¨€-TÃ¼rkÃ§e-firebrick?style=for-the-badge\" alt=\"TÃ¼rkÃ§e\"></a>\n  <a href=\"./i18n/uk/\"><img src=\"https://img.shields.io/badge/è¯­è¨€-Ğ£ĞºÑ€Ğ°Ñ—Ğ½ÑÑŒĞºĞ°-cornflowerblue?style=for-the-badge\" alt=\"Ğ£ĞºÑ€Ğ°Ñ—Ğ½ÑÑŒĞºĞ°\"></a>\n  <a href=",
      "default_branch": "main"
    },
    "fetched_at": "2026-01-04T03:04:58.677585"
  },
  {
    "basic_info": {
      "name": "agentskills",
      "full_name": "agentskills/agentskills",
      "owner": "agentskills",
      "description": "Specification and documentation for Agent Skills",
      "url": "https://github.com/agentskills/agentskills",
      "clone_url": "https://github.com/agentskills/agentskills.git",
      "ssh_url": "git@github.com:agentskills/agentskills.git",
      "homepage": "https://agentskills.io",
      "created_at": "2025-12-16T15:47:19Z",
      "updated_at": "2026-01-04T03:03:04Z",
      "pushed_at": "2025-12-20T02:41:18Z"
    },
    "stats": {
      "stars": 4274,
      "forks": 210,
      "watchers": 4274,
      "open_issues": 39,
      "size": 92
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 29204,
        "Shell": 268
      },
      "license": null,
      "topics": []
    },
    "content": {
      "readme": "# Agent Skills\n\n[Agent Skills](https://agentskills.io) are a simple, open format for giving agents new capabilities and expertise.\n\nSkills are folders of instructions, scripts, and resources that agents can discover and use to perform better at specific tasks. Write once, use everywhere.\n\n## Getting Started\n\n- [Documentation](https://agentskills.io) - Guides and tutorials\n- [Specification](https://agentskills.io/specification) - Format details\n- [Example Skills](https://github.com/anthropics/skills) - See what's possible\n\nThis repo contains the specification, documentation, and reference SDK. Also see a list of example skills [here](https://github.com/anthropics/skills).\n\n## About\n\nAgent Skills is an open format maintained by [Anthropic](https://anthropic.com) and open to contributions from the community.",
      "default_branch": "main"
    },
    "fetched_at": "2026-01-04T03:04:59.842324"
  },
  {
    "basic_info": {
      "name": "TurboDiffusion",
      "full_name": "thu-ml/TurboDiffusion",
      "owner": "thu-ml",
      "description": "TurboDiffusion: 100â€“200Ã— Acceleration for Video Diffusion Models",
      "url": "https://github.com/thu-ml/TurboDiffusion",
      "clone_url": "https://github.com/thu-ml/TurboDiffusion.git",
      "ssh_url": "git@github.com:thu-ml/TurboDiffusion.git",
      "homepage": "https://arxiv.org/pdf/2512.16093",
      "created_at": "2025-12-06T00:25:35Z",
      "updated_at": "2026-01-04T02:33:42Z",
      "pushed_at": "2026-01-01T17:12:30Z"
    },
    "stats": {
      "stars": 2990,
      "forks": 195,
      "watchers": 2990,
      "open_issues": 51,
      "size": 208814
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 1052864,
        "C++": 46267,
        "Shell": 8086,
        "Cuda": 7222
      },
      "license": "Apache License 2.0",
      "topics": [
        "ai-infra",
        "consistency-model",
        "diffusion-models",
        "distillation",
        "inference-acceleration",
        "mlsystem",
        "rcm",
        "sageattention",
        "sparse-linear-attention",
        "video-generation"
      ]
    },
    "content": {
      "readme": "# TurboDiffusion\n\n<div align=\"center\">\n<img src=assets/TurboDiffusion_Logo.png width=\"30%\"/>\n</div>\n\nThis repository provides the official implementation of **TurboDiffusion**, a video generation acceleration framework that can speed up end-to-end diffusion generation by $100 \\sim 200\\times$ on a single RTX 5090, while maintaining video quality.   \nTurboDiffusion primarily uses [SageAttention](https://github.com/thu-ml/SageAttention), [SLA (Sparse-Linear Attention)](https://github.com/thu-ml/SLA) for attention acceleration, and [rCM](https://github.com/NVlabs/rcm) for timestep distillation.\n\nPaper: [TurboDiffusion: Accelerating Video Diffusion Models by 100-200 Times](https://arxiv.org/pdf/2512.16093)\n\n**Note**: the checkpoints and paper are not finalized, and will be updated later to improve quality.\n\n<div align=\"center\">\n<img src=\"assets/TurboDiffusion_speedup.png\" width=\"99%\"/>\n</div>\n\n<div align=\"center\">\n<img src=\"assets/acceleration_decomposition.png\" width=\"93%\"/>\n</div>\n\n<div align=\"center\">\n<table>\n<tr>\n<td align=\"center\" style=\"border: 2px solid #000; padding: 10px;\">\n<div style=\"font-size: 1.1em;\">Original, E2E Time: 184s</div>\n<div><img src=\"assets/videos/original/1.3B/11.gif\" width=\"387\"/></div>\n</td>\n<td align=\"center\" style=\"border: 2px solid #000; padding: 10px;\">\n<div style=\"font-size: 1.1em;\">TurboDiffusion, E2E Time: <b>1.9s</b></div>\n<div><img src=\"assets/videos/turbodiffusion/1.3B/11.gif\" width=\"387\"/></div>\n</td>\n</tr>\n</table>\nAn example of a <b>5-second video</b> generated by Wan-2.1-T2V-1.3B-480P on a single <b>RTX 5090</b>.\n</div>\n\n## Available Models\n\n|              Model Name               |                       Checkpoint Link                        | Best Resolution |\n| :-----------------------------------: | :----------------------------------------------------------: | :-------------: |\n| `TurboWan2.2-I2V-A14B-720P` | [Huggingface Model](https://huggingface.co/TurboDiffusion/TurboWan2.2-I2V-A14B-720P) | 720p            |\n| `TurboWan2.1-T2V-1.3B-480P` | [Huggingface Model](https://huggingface.co/TurboDiffusion/TurboWan2.1-T2V-1.3B-480P) | 480p            |\n| `TurboWan2.1-T2V-14B-480P`  | [Huggingface Model](https://huggingface.co/TurboDiffusion/TurboWan2.1-T2V-14B-480P) | 480p            |\n| `TurboWan2.1-T2V-14B-720P`  | [Huggingface Model](https://huggingface.co/TurboDiffusion/TurboWan2.1-T2V-14B-720P) | 720p            |\n\n\n\nNote: All checkpoints support generating videos at 480p or 720p. The \"Best Resolution\" column indicates the resolution at which the model provides the best video quality.\n\n\n## Installation\n\n**Base environment**: `python>=3.9`, `torch>=2.7.0`. `torch==2.8.0` is recommended, as higher versions may cause OOM.\n\nInstall TurboDiffusion by pip:\n\n```bash\nconda create -n turbodiffusion python=3.12\nconda activate turbodiffusion\n\npip install turbodiffusion --no-build-isolation\n```\n\nOr compile from source:\n\n```bash\ngit clone https://github.com/thu-ml/TurboDiffusion.git\ncd TurboDiffusion\ngit submodule update --init --recursive\npip install -e . --no-build-isolation\n```\n\nTo enable SageSLA, a fast SLA forward pass based on SageAttention, install [SpargeAttn](https://github.com/thu-ml/SpargeAttn) first:\n\n```bash\npip install git+https://github.com/thu-ml/SpargeAttn.git --no-build-isolation\n```\n\n\n## Inference\nFor GPUs with more than 40GB of GPU memory, **e.g., H100, please use the unquantized checkpoints (without `-quant`) and remove `--quant_linear` from the command. For RTX 5090, RTX 4090, or similar GPUs, please use the quantized checkpoints (with `-quant`) and add `--quant_linear` in the command.)**\n\n1.  Download the VAE (**applicable for both Wan2.1 and Wan2.2**) and umT5 text encoder checkpoints:\n\n    ```bash\n    mkdir checkpoints\n    cd checkpoints\n    wget https://huggingface.co/Wan-AI/Wan2.1-T2V-1.3B/resolve/main/Wan2.1_VAE.pth\n    wget https://huggingface.co/Wan-AI/Wan2.1-T2V-1.3B/resolve/main/models_t5_umt5-xxl-enc-bf16.pth\n    ```\n\n2. Download our quantized model checkpoints (For RTX 5090 or similar GPUs):\n\n    ```bash\n    # For Wan2.1-T2V-1.3B\n    wget https://huggingface.co/TurboDiffusion/TurboWan2.1-T2V-1.3B-480P/resolve/main/TurboWan2.1-T2V-1.3B-480P-quant.pth\n\n    # For Wan2.2-I2V-14B\n    wget https://huggingface.co/TurboDiffusion/TurboWan2.2-I2V-A14B-720P/resolve/main/TurboWan2.2-I2V-A14B-high-720P-quant.pth\n    wget https://huggingface.co/TurboDiffusion/TurboWan2.2-I2V-A14B-720P/resolve/main/TurboWan2.2-I2V-A14B-low-720P-quant.pth\n    ```\n\n    **Or** download our unquantized model checkpoints (For H100 or similar GPUs):\n    ```bash\n    # For Wan2.1-T2V-1.3B\n    wget https://huggingface.co/TurboDiffusion/TurboWan2.1-T2V-1.3B-480P/resolve/main/TurboWan2.1-T2V-1.3B-480P.pth\n\n    # For Wan2.2-I2V-14B\n    wget https://huggingface.co/TurboDiffusion/TurboWan2.2-I2V-A14B-720P/resolve/main/TurboWan2.2-I2V-A14B-high-720P.pth\n    wget https://huggingface.co/TurboDiffusion/TurboWan2.2-I2V-A14B-720P/resolve/main/TurboWan2.2-I2V-A14B-low-720P.pth\n    ```\n    \n\n3.  Use",
      "default_branch": "main"
    },
    "fetched_at": "2026-01-04T03:05:01.039469"
  },
  {
    "basic_info": {
      "name": "Paper2Slides",
      "full_name": "HKUDS/Paper2Slides",
      "owner": "HKUDS",
      "description": "\"Paper2Slides: From Paper to Presentation in One Click\"",
      "url": "https://github.com/HKUDS/Paper2Slides",
      "clone_url": "https://github.com/HKUDS/Paper2Slides.git",
      "ssh_url": "git@github.com:HKUDS/Paper2Slides.git",
      "homepage": "",
      "created_at": "2025-12-07T06:15:43Z",
      "updated_at": "2026-01-04T01:51:52Z",
      "pushed_at": "2025-12-31T17:12:59Z"
    },
    "stats": {
      "stars": 2635,
      "forks": 353,
      "watchers": 2635,
      "open_issues": 12,
      "size": 27564
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 557392,
        "JavaScript": 157643,
        "Shell": 12922,
        "CSS": 442,
        "HTML": 404
      },
      "license": "MIT License",
      "topics": [
        "agentic-ai",
        "llm-agents",
        "paper2poster",
        "paper2slides"
      ]
    },
    "content": {
      "readme": "<div align=\"center\">\n\n<img src=\"assets/paper2slides_logo.png\" alt=\"Paper2Slides Logo\" width=\"200\"/><br>\n\n# Paper2Slides: From Paper to Presentation in One Click\n\n[![Python](https://img.shields.io/badge/Python-3.12+-FCE7D6.svg)](https://www.python.org/)\n[![License](https://img.shields.io/badge/License-MIT-C1E5F5.svg)](https://opensource.org/licenses/MIT/)\n[![Feishu](https://img.shields.io/badge/Feishu-Group-E9DBFC?style=flat&logo=wechat&logoColor=white)](./COMMUNICATION.md) \n[![WeChat](https://img.shields.io/badge/WeChat-Group-C5EAB4?style=flat&logo=wechat&logoColor=white)](./COMMUNICATION.md)\n\nâœ¨ **Never Build Slides from Scratch Again** âœ¨\n\n| ğŸ“„ **Universal File Support** &nbsp;|&nbsp; ğŸ¯ **RAG-Powered Precision** &nbsp;|&nbsp; ğŸ¨ **Custom Styling** &nbsp;|&nbsp; âš¡ **Lightning Speed** |\n\n</div>\n\n---\n\n## ğŸ¯ What is Paper2Slides?\n\nTurns your **research papers**, **reports**, and **documents** into **professional slides & posters** in **minutes**.\n\n### âœ¨ Key Features\n- ğŸ“„ **Universal Document Support**<br>\n  Seamlessly process PDF, Word, Excel, PowerPoint, Markdown, and multiple file formats simultaneously.\n  \n- ğŸ¯ **Comprehensive Content Extraction**<br>\n  RAG-powered mechanism ensures every critical insight, figure, and data point is captured with precision.\n  \n- ğŸ”— **Source-Linked Accuracy**<br>\n  Maintains direct traceability between generated content and original sources, eliminating information drift.\n  \n- ğŸ¨ **Custom Styling Freedom**<br>\n  Choose from professional built-in themes or describe your vision in natural language for custom styling.\n  \n- âš¡ **Lightning-Fast Generation**<br>\n  Instant preview mode enables rapid experimentation and real-time refinements.\n  \n- ğŸ’¾ **Seamless Session Management**<br>\n  Advanced checkpoint system preserves all progressâ€”pause, resume, or switch themes instantly without loss.\n  \n- âœ¨ **Professional-Grade Visuals**<br>\n  Deliver polished, presentation-ready slides and posters with publication-quality design standards.\n\n### âš¡ Easy as One Command\n```bash\n# One command to generate slides from a paper\npython -m paper2slides --input paper.pdf --output slides --style doraemon --length medium --fast --parallel 2\n```\n\n---\n\n## ğŸ”¥ News\n\n- **[2025.12.09]** Added parallel slide generation (`--parallel`) for faster processing\n- **[2025.12.08]** Paper2Slides is now open source!\n\n---\n\n## ğŸ¨ Custom Styling Showcase\n\n<div align=\"center\">\n\n<table>\n<tr>\n<td align=\"center\" width=\"290\"><img src=\"assets/doraemon_poster.png?v=2\" width=\"280\"/><br/><code>doraemon</code></td>\n<td align=\"center\" width=\"290\"><img src=\"assets/academic_poster.png?v=2\" width=\"280\"/><br/><code>academic</code></td>\n<td align=\"center\" width=\"290\"><img src=\"assets/totoro_poster.png?v=2\" width=\"280\"/><br/><code>custom</code></td>\n</tr>\n</table>\n\n<table>\n<tr>\n<td align=\"center\" width=\"290\"><a href=\"assets/doraemon_slides.pdf\"><img src=\"assets/doraemon_slides_preview.png?v=2\" width=\"280\"/></a><br/><code>doraemon</code></td>\n<td align=\"center\" width=\"290\"><a href=\"assets/academic_slides.pdf\"><img src=\"assets/academic_slides_preview.png?v=2\" width=\"280\"/></a><br/><code>academic</code></td>\n<td align=\"center\" width=\"290\"><a href=\"assets/totoro_slides.pdf\"><img src=\"assets/totoro_slides_preview.png?v=2\" width=\"280\"/></a><br/><code>custom</code></td>\n</tr>\n</table>\n\n<sub>âœ¨ Multiple styles available â€” simply modify the <code>--style</code> parameter<br/>\nExamples from <a href=\"https://arxiv.org/abs/2512.02556\">DeepSeek-V3.2: Pushing the Frontier of Open Large Language Models</a></sub>\n\n</div>\n\n<details>\n<summary><b>ğŸ’¡ Custom Style Example: Totoro Theme</b></summary>\n\n```\n--style \"Studio Ghibli anime style with warm whimsical aesthetic. Use soft watercolor Morandi tones with light cream background, muted sage green and dusty pink accents. Totoro character can appear as a friendly guide relating to the content, with nature elements like soft clouds or leaves.\"\n```\n\n</details>\n\n---\n\n### ğŸŒ Paper2Slides Web Interface\n\n<div align=\"center\">\n<table>\n<tr>\n<td><img src=\"assets/ui_1.png\" width=\"420\"/></td>\n<td><img src=\"assets/ui_2.png\" width=\"420\"/></td>\n</tr>\n</table>\n</div>\n\n---\n\n## ğŸ“‹ Table of Contents\n\n- [ğŸ¯ Quick Start](#-quick-start)\n- [ğŸ—ï¸ Paper2Slides Framework](#%EF%B8%8F-paper2slides-framework)\n- [ğŸ”§ Configuration](#%EF%B8%8F-configuration)\n- [ğŸ“ Code Structure](#-code-structure)\n\n---\n\n## ğŸƒ Quick Start\n\n### 1. Environment Setup\n\n```bash\n# Clone repository\ngit clone https://github.com/HKUDS/Paper2Slides.git\ncd Paper2Slides\n\n# Create and activate conda environment\nconda create -n paper2slides python=3.12 -y\nconda activate paper2slides\n\n# Install dependencies\npip install -r requirements.txt\n```\n\n> [!NOTE]\n> Create a `.env` file in `paper2slides/` directory with your API keys. Refer to `paper2slides/.env.example` for the required variables.\n\n### 2. Command Line Usage\n\n```bash\n# Basic usage - generate slides from a paper\npython -m paper2slides --input paper.pdf --output slides --length medium\n\n# Generate poster with custom style\npython -m paper2slides ",
      "default_branch": "main"
    },
    "fetched_at": "2026-01-04T03:05:02.200126"
  },
  {
    "basic_info": {
      "name": "sqlit",
      "full_name": "Maxteabag/sqlit",
      "owner": "Maxteabag",
      "description": "A user friendly TUI for SQL databases. Written in python. Supports SQL server, Mysql, PostreSQL and SQLite, Turso and more.",
      "url": "https://github.com/Maxteabag/sqlit",
      "clone_url": "https://github.com/Maxteabag/sqlit.git",
      "ssh_url": "git@github.com:Maxteabag/sqlit.git",
      "homepage": "",
      "created_at": "2025-12-13T00:10:36Z",
      "updated_at": "2026-01-04T01:49:54Z",
      "pushed_at": "2026-01-03T02:22:24Z"
    },
    "stats": {
      "stars": 2597,
      "forks": 57,
      "watchers": 2597,
      "open_issues": 10,
      "size": 25332
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 2422929,
        "Shell": 6869,
        "HCL": 6254,
        "CSS": 4292,
        "Dockerfile": 278
      },
      "license": "MIT License",
      "topics": [
        "cockroachdb",
        "command-line-tool",
        "duckdb",
        "mariadb",
        "mysql",
        "oracle",
        "postgresql",
        "python",
        "sql",
        "sqlite",
        "ssh",
        "tui",
        "turso"
      ]
    },
    "content": {
      "readme": "<p align=\"center\">\n  <img src=\"assets/favorites/logo_sqlit.png\" alt=\"sqlit logo\" width=\"180\">\n</p>\n\n<h3 align=\"center\">The lazygit of SQL databases</h3>\n\n<p align=\"center\">\n  <em>Connect and query your database from your terminal in seconds.</em>\n</p>\n\n<p align=\"center\">\n  <a href=\"https://github.com/Maxteabag/sqlit/stargazers\"><img src=\"https://img.shields.io/github/stars/Maxteabag/sqlit?style=flat&color=yellow\" alt=\"GitHub Stars\"></a>\n  <img src=\"https://img.shields.io/badge/python-3.10+-blue.svg\" alt=\"Python\">\n  <img src=\"https://img.shields.io/badge/license-MIT-green.svg\" alt=\"License\">\n</p>\n\n<p align=\"center\">\n  <code>pipx install sqlit-tui</code>\n</p>\n\n<p align=\"center\">\n  <a href=\"https://www.buymeacoffee.com/PeterAdams\"><img src=\"https://img.shields.io/badge/Buy%20Me%20a%20Coffee-ffdd00?style=flat&logo=buy-me-a-coffee&logoColor=black\" alt=\"Buy Me a Coffee\"></a>\n</p>\n\n---\n\n### Connect\nSupports all major databases: SQL Server, PostgreSQL, MySQL, SQLite, MariaDB, FirebirdSQL, Oracle, DuckDB, CockroachDB, ClickHouse, Snowflake, Supabase, CloudFlare D1, Turso, Athena, BigQuery, RedShift, IBM Db2, SAP HANA, Teradata, Trino, Presto and Apache Flight SQL.\n\n![Database Providers](docs/demos/demo-providers.gif)\n\n### Query\nSyntax highlighting. History. Vim-style keybindings.\n\n![Query History](docs/demos/demo-history.gif)\n\n### Results\nLoad millions of rows. Inspect data, filter by content, fuzzy search.\n\n![Filter results](docs/demos/demo-filter/demo-filter.gif)\n\n### Docker Discovery\nAutomatically finds running database containers. Press 'Enter' to connect, sqlit figures out the details for you.\n\n![Docker Discovery](docs/demos/demo-docker-picker.gif)\n\n---\n\n## Features\n\n**Connection manager:** Save and switch connections without CLI args\n\n**Just run `sqlit`:** No CLI config needed, pick a connection and go\n\n**Multi-database support:** PostgreSQL, MySQL, SQLite, SQL Server, and 10+ more\n\n**Docker integration:** Auto-detect running database containers\n\n**Cloud CLI integration:** Easily browse and connect to your external databases through Azure, AWS and GCP CLI's\n\n**SSH tunnels:** Connect to remote databases securely with password or key auth\n\n**Secure credentials:** Passwords stored in your OS keyring\n\n**Vim-style editing:** Modal editing for terminal purists\n\n**Query history:** Searchable, per-connection history\n\n**Filter results:** Fuzzy search through millions of rows\n\n**Context-aware help:** Keybindings shown on screen\n\n**Browse databases:** Tables, views, procedures, indexes, triggers, sequences\n\n**Autocomplete:** Sophisticated SQL completion engine for tables, columns, and procedures\n\n**CLI mode:** Execute SQL from the command line\n\n**Themes:** Rose Pine, Tokyo Night, Nord, Gruvbox\n\n**Dependency wizard:** Auto-install missing drivers\n\n---\n\n## Motivation\n\nThroughout my career, the undesputed truth was that heavy GUI's like SSMS was the only respectable way to access a database. It didn't matter that I wasn't a DBA, or that I didn't need complex performance graphs. I was expected to install a gigabyte-heavy behemoth that took ages to launch all for the mere purpose of running a few queries to update and view a couple of rows.\n\nWhen I switched to Linux, I was suddenly unable to return to the devil I know, and I asked myself: _how do I access my data now?_\n\nThe popular answer was VS Code's SQL extension. But why should we developers launch a heavy Electron app designed for coding just to execute SQL?\n\nI had recently grown fond of Terminal UI's for their speed and keybinding focus. I looked for SQL TUIs, but the options were sparse. The ones I found lacked the user-friendliness and immediate \"pick-up-and-go\" nature of tools I loved, like lazygit, and I shortly returning to vscode sql extension.\n\nSomething wasn't right. I asked myself, why is it that running SQL queries can't be enjoyable? So I created sqlit.\n\nsqlit is for the developer who just wants to query their database with a user friendly UI without their RAM being eaten alive. It is a lightweight, beautiful, and keyboard-driven TUI designed to make accessing your data enjoyable, fast and easy like it should be-- all from inside your favorite terminal.\n\n---\n\n## Installation\n\n| Method | Command |\n| :----- | :------ |\n| pipx *(recommended)* | `pipx install sqlit-tui` |\n| uv | `uv tool install sqlit-tui` |\n| pip | `pip install sqlit-tui` |\n\n## Usage\n\n```bash\nsqlit\n```\n\nThe keybindings are shown at the bottom of the screen.\n\n### Try it without a database\n\nWant to explore the UI without connecting to a real database? Run with mock data:\n\n```bash\nsqlit --mock=sqlite-demo\n```\n\n### CLI\n\n```bash\n# Run a query\nsqlit query -c \"MyConnection\" -q \"SELECT * FROM Users\"\n\n# Output as CSV or JSON\nsqlit query -c \"MyConnection\" -q \"SELECT * FROM Users\" --format csv\nsqlit query -c \"MyConnection\" -f \"script.sql\" --format json\n\n# Create connections for different databases\nsqlit connections add mssql --name \"MySqlServer\" --server \"localhost\" --auth-type sql\nsqlit connections add po",
      "default_branch": "main"
    },
    "fetched_at": "2026-01-04T03:05:03.368532"
  },
  {
    "basic_info": {
      "name": "mistral-vibe",
      "full_name": "mistralai/mistral-vibe",
      "owner": "mistralai",
      "description": "Minimal CLI coding agent by Mistral",
      "url": "https://github.com/mistralai/mistral-vibe",
      "clone_url": "https://github.com/mistralai/mistral-vibe.git",
      "ssh_url": "git@github.com:mistralai/mistral-vibe.git",
      "homepage": "",
      "created_at": "2025-12-08T18:56:59Z",
      "updated_at": "2026-01-03T21:13:02Z",
      "pushed_at": "2025-12-26T16:09:46Z"
    },
    "stats": {
      "stars": 2465,
      "forks": 209,
      "watchers": 2465,
      "open_issues": 129,
      "size": 224
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 1031645,
        "Nix": 4392,
        "Shell": 3363
      },
      "license": "Apache License 2.0",
      "topics": []
    },
    "content": {
      "readme": "# Mistral Vibe\n\n[![PyPI Version](https://img.shields.io/pypi/v/mistral-vibe)](https://pypi.org/project/mistral-vibe)\n[![Python Version](https://img.shields.io/badge/python-3.12%2B-blue)](https://www.python.org/downloads/release/python-3120/)\n[![CI Status](https://github.com/mistralai/mistral-vibe/actions/workflows/ci.yml/badge.svg)](https://github.com/mistralai/mistral-vibe/actions/workflows/ci.yml)\n[![License](https://img.shields.io/github/license/mistralai/mistral-vibe)](https://github.com/mistralai/mistral-vibe/blob/main/LICENSE)\n\n```\nâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘\nâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘\nâ–ˆâ–ˆâ–ˆâ–ˆ  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  â–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘\nâ–ˆâ–ˆâ–ˆâ–ˆ    â–ˆâ–ˆ    â–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘\nâ–ˆâ–ˆâ–ˆâ–ˆ          â–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘\nâ–ˆâ–ˆâ–ˆâ–ˆ  â–ˆâ–ˆ  â–ˆâ–ˆ  â–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘\nâ–ˆâ–ˆ      â–ˆâ–ˆ      â–ˆâ–ˆâ–‘â–‘\nâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘\nâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘\n```\n\n**Mistral's open-source CLI coding assistant.**\n\nMistral Vibe is a command-line coding assistant powered by Mistral's models. It provides a conversational interface to your codebase, allowing you to use natural language to explore, modify, and interact with your projects through a powerful set of tools.\n\n> [!WARNING]\n> Mistral Vibe works on Windows, but we officially support and target UNIX environments.\n\n### One-line install (recommended)\n\n**Linux and macOS**\n\n```bash\ncurl -LsSf https://mistral.ai/vibe/install.sh | bash\n```\n\n**Windows**\n\nFirst, install uv\n```bash\npowershell -ExecutionPolicy ByPass -c \"irm https://astral.sh/uv/install.ps1 | iex\"\n```\n\nThen, use uv command below.\n\n### Using uv\n\n```bash\nuv tool install mistral-vibe\n```\n\n### Using pip\n\n```bash\npip install mistral-vibe\n```\n\n## Features\n\n- **Interactive Chat**: A conversational AI agent that understands your requests and breaks down complex tasks.\n- **Powerful Toolset**: A suite of tools for file manipulation, code searching, version control, and command execution, right from the chat prompt.\n  - Read, write, and patch files (`read_file`, `write_file`, `search_replace`).\n  - Execute shell commands in a stateful terminal (`bash`).\n  - Recursively search code with `grep` (with `ripgrep` support).\n  - Manage a `todo` list to track the agent's work.\n- **Project-Aware Context**: Vibe automatically scans your project's file structure and Git status to provide relevant context to the agent, improving its understanding of your codebase.\n- **Advanced CLI Experience**: Built with modern libraries for a smooth and efficient workflow.\n  - Autocompletion for slash commands (`/`) and file paths (`@`).\n  - Persistent command history.\n  - Beautiful Themes.\n- **Highly Configurable**: Customize models, providers, tool permissions, and UI preferences through a simple `config.toml` file.\n- **Safety First**: Features tool execution approval.\n\n## Quick Start\n\n1. Navigate to your project's root directory:\n\n   ```bash\n   cd /path/to/your/project\n   ```\n\n2. Run Vibe:\n\n   ```bash\n   vibe\n   ```\n\n3. If this is your first time running Vibe, it will:\n\n   - Create a default configuration file at `~/.vibe/config.toml`\n   - Prompt you to enter your API key if it's not already configured\n   - Save your API key to `~/.vibe/.env` for future use\n\n4. Start interacting with the agent!\n\n   ```\n   > Can you find all instances of the word \"TODO\" in the project?\n\n   ğŸ¤– The user wants to find all instances of \"TODO\". The `grep` tool is perfect for this. I will use it to search the current directory.\n\n   > grep(pattern=\"TODO\", path=\".\")\n\n   ... (grep tool output) ...\n\n   ğŸ¤– I found the following \"TODO\" comments in your project.\n   ```\n\n## Usage\n\n### Interactive Mode\n\nSimply run `vibe` to enter the interactive chat loop.\n\n- **Multi-line Input**: Press `Ctrl+J` or `Shift+Enter` for select terminals to insert a newline.\n- **File Paths**: Reference files in your prompt using the `@` symbol for smart autocompletion (e.g., `> Read the file @src/agent.py`).\n- **Shell Commands**: Prefix any command with `!` to execute it directly in your shell, bypassing the agent (e.g., `> !ls -l`).\n\nYou can start Vibe with a prompt with the following command:\n\n```bash\nvibe \"Refactor the main function in cli/main.py to be more modular.\"\n```\n\n**Note**: The `--auto-approve` flag automatically approves all tool executions without prompting. In interactive mode, you can also toggle auto-approve on/off using `Shift+Tab`.\n\n### Programmatic Mode\n\nYou can run Vibe non-interactively by piping input or using the `--prompt` flag. This is useful for scripting.\n\n```bash\nvibe --prompt \"Refactor the main function in cli/main.py to be more modular.\"\n```\n\nby default it will use `auto-approve` mode.\n\n### Slash Commands\n\nUse slash commands for meta-actions and configuration changes during a session.\n\n## Configuration\n\nVibe is configured via a `config.toml` file. It looks for this file first in `./.vibe/config.toml` and then falls back to `~/.vibe/config.toml`.\n\n### API Key Configuration\n\nVibe supports multiple ways to configure your API keys:\n\n1. **Interactive Setup (Recommended for first-time users)**: When you run Vibe for the first time or if your API key is missing, Vibe will prompt you to enter it. The ke",
      "default_branch": "main"
    },
    "fetched_at": "2026-01-04T03:05:04.547318"
  },
  {
    "basic_info": {
      "name": "tgbot-verify",
      "full_name": "PastKing/tgbot-verify",
      "owner": "PastKing",
      "description": "ä¸€ä¸ªåŸºäº Python Telegram Bot çš„è‡ªåŠ¨åŒ–è®¤è¯å·¥å…·ï¼Œèƒ½å¤Ÿè‡ªåŠ¨å®Œæˆ SheerID å¹³å°çš„å­¦ç”Ÿ/æ•™å¸ˆèº«ä»½éªŒè¯æµç¨‹ã€‚",
      "url": "https://github.com/PastKing/tgbot-verify",
      "clone_url": "https://github.com/PastKing/tgbot-verify.git",
      "ssh_url": "git@github.com:PastKing/tgbot-verify.git",
      "homepage": null,
      "created_at": "2025-12-07T06:27:57Z",
      "updated_at": "2026-01-04T02:53:48Z",
      "pushed_at": "2025-12-24T05:27:41Z"
    },
    "stats": {
      "stars": 1750,
      "forks": 628,
      "watchers": 1750,
      "open_issues": 3,
      "size": 88
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 220565,
        "HTML": 9190,
        "Dockerfile": 1609
      },
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "# SheerID è‡ªåŠ¨è®¤è¯ Telegram æœºå™¨äºº\n\n![Stars](https://img.shields.io/github/stars/PastKing/tgbot-verify?style=social)\n![Forks](https://img.shields.io/github/forks/PastKing/tgbot-verify?style=social)\n![Issues](https://img.shields.io/github/issues/PastKing/tgbot-verify)\n![License](https://img.shields.io/github/license/PastKing/tgbot-verify)\n\n> ğŸ¤– è‡ªåŠ¨å®Œæˆ SheerID å­¦ç”Ÿ/æ•™å¸ˆè®¤è¯çš„ Telegram æœºå™¨äºº\n> \n> åŸºäº [@auto_sheerid_bot](https://t.me/auto_sheerid_bot) GGBond çš„æ—§ç‰ˆä»£ç æ”¹è¿›\n\n---\n\n## ğŸ“‹ é¡¹ç›®ç®€ä»‹\n\nè¿™æ˜¯ä¸€ä¸ªåŸºäº Python çš„ Telegram æœºå™¨äººï¼Œå¯ä»¥è‡ªåŠ¨å®Œæˆå¤šä¸ªå¹³å°çš„ SheerID å­¦ç”Ÿ/æ•™å¸ˆèº«ä»½è®¤è¯ã€‚æœºå™¨äººè‡ªåŠ¨ç”Ÿæˆèº«ä»½ä¿¡æ¯ã€åˆ›å»ºè®¤è¯æ–‡æ¡£å¹¶æäº¤åˆ° SheerID å¹³å°ï¼Œå¤§å¤§ç®€åŒ–äº†è®¤è¯æµç¨‹ã€‚\n\n> **âš ï¸ é‡è¦æç¤º**ï¼š\n> \n> - **Gemini One Pro**ã€**ChatGPT Teacher K12**ã€**Spotify Student**ã€**YouTube Premium Student** ç­‰æœåŠ¡åœ¨ä½¿ç”¨å‰éœ€è¦æ›´æ–°å„æ¨¡å—é…ç½®æ–‡ä»¶ä¸­çš„ `programId` ç­‰éªŒè¯èµ„æ–™ï¼Œå…·ä½“è¯·å‚è€ƒä¸‹æ–¹\"ä½¿ç”¨å‰å¿…è¯»\"ç« èŠ‚ã€‚\n> - æœ¬é¡¹ç›®è¿˜æä¾›äº† **ChatGPT å†›äººè®¤è¯**çš„å®ç°æ€è·¯å’Œæ¥å£æ–‡æ¡£ï¼Œè¯¦ç»†å†…å®¹è¯·æŸ¥çœ‹ [`military/README.md`](military/README.md)ï¼Œç”¨æˆ·å¯æ ¹æ®æ–‡æ¡£è‡ªè¡Œé›†æˆã€‚\n\n### ğŸ¯ æ”¯æŒçš„è®¤è¯æœåŠ¡\n\n| å‘½ä»¤ | æœåŠ¡ | ç±»å‹ | çŠ¶æ€ | è¯´æ˜ |\n|------|------|------|------|------|\n| `/verify` | Gemini One Pro | æ•™å¸ˆè®¤è¯ | âœ… å®Œæ•´ | Google AI Studio æ•™è‚²ä¼˜æƒ  |\n| `/verify2` | ChatGPT Teacher K12 | æ•™å¸ˆè®¤è¯ | âœ… å®Œæ•´ | OpenAI ChatGPT æ•™è‚²ä¼˜æƒ  |\n| `/verify3` | Spotify Student | å­¦ç”Ÿè®¤è¯ | âœ… å®Œæ•´ | Spotify å­¦ç”Ÿè®¢é˜…ä¼˜æƒ  |\n| `/verify4` | Bolt.new Teacher | æ•™å¸ˆè®¤è¯ | âœ… å®Œæ•´ | Bolt.new æ•™è‚²ä¼˜æƒ ï¼ˆè‡ªåŠ¨è·å– codeï¼‰|\n| `/verify5` | YouTube Premium Student | å­¦ç”Ÿè®¤è¯ | âš ï¸ åŠæˆå“ | YouTube Premium å­¦ç”Ÿä¼˜æƒ ï¼ˆè§ä¸‹æ–¹è¯´æ˜ï¼‰|\n\n> **âš ï¸ YouTube è®¤è¯ç‰¹åˆ«è¯´æ˜**ï¼š\n> \n> YouTube è®¤è¯åŠŸèƒ½ç›®å‰ä¸ºåŠæˆå“çŠ¶æ€ï¼Œä½¿ç”¨å‰è¯·ä»”ç»†é˜…è¯» [`youtube/HELP.MD`](youtube/HELP.MD) æ–‡æ¡£ã€‚\n> \n> **ä¸»è¦åŒºåˆ«**ï¼š\n> - YouTube çš„åŸå§‹é“¾æ¥æ ¼å¼ä¸å…¶ä»–æœåŠ¡ä¸åŒ\n> - éœ€è¦æ‰‹åŠ¨ä»æµè§ˆå™¨ç½‘ç»œæ—¥å¿—ä¸­æå– `programId` å’Œ `verificationId`\n> - ç„¶åæ‰‹åŠ¨ç»„æˆæ ‡å‡†çš„ SheerID é“¾æ¥æ ¼å¼\n> \n> **ä½¿ç”¨æ­¥éª¤**ï¼š\n> 1. è®¿é—® YouTube Premium å­¦ç”Ÿè®¤è¯é¡µé¢\n> 2. æ‰“å¼€æµè§ˆå™¨å¼€å‘è€…å·¥å…·ï¼ˆF12ï¼‰â†’ ç½‘ç»œï¼ˆNetworkï¼‰æ ‡ç­¾\n> 3. å¼€å§‹è®¤è¯æµç¨‹ï¼Œæœç´¢ `https://services.sheerid.com/rest/v2/verification/`\n> 4. ä»è¯·æ±‚è½½è·ä¸­è·å– `programId`ï¼Œä»å“åº”ä¸­è·å– `verificationId`\n> 5. æ‰‹åŠ¨ç»„æˆé“¾æ¥ï¼š`https://services.sheerid.com/verify/{programId}/?verificationId={verificationId}`\n> 6. ä½¿ç”¨ `/verify5` å‘½ä»¤æäº¤è¯¥é“¾æ¥\n\n> **ğŸ’¡ ChatGPT å†›äººè®¤è¯æ€è·¯**ï¼š\n> \n> æœ¬é¡¹ç›®æä¾›äº† ChatGPT å†›äºº SheerID è®¤è¯çš„å®ç°æ€è·¯å’Œæ¥å£æ–‡æ¡£ã€‚å†›äººè®¤è¯æµç¨‹ä¸æ™®é€šå­¦ç”Ÿ/æ•™å¸ˆè®¤è¯ä¸åŒï¼Œéœ€è¦å…ˆæ‰§è¡Œ `collectMilitaryStatus` æ¥å£è®¾ç½®å†›äººçŠ¶æ€ï¼Œç„¶åå†æäº¤ä¸ªäººä¿¡æ¯è¡¨å•ã€‚è¯¦ç»†å®ç°æ€è·¯å’Œæ¥å£è¯´æ˜è¯·æŸ¥çœ‹ [`military/README.md`](military/README.md) æ–‡æ¡£ã€‚ç”¨æˆ·å¯æ ¹æ®è¯¥æ–‡æ¡£è‡ªè¡Œé›†æˆåˆ°æœºå™¨äººä¸­ã€‚\n\n### âœ¨ æ ¸å¿ƒåŠŸèƒ½\n\n- ğŸš€ **è‡ªåŠ¨åŒ–æµç¨‹**ï¼šä¸€é”®å®Œæˆä¿¡æ¯ç”Ÿæˆã€æ–‡æ¡£åˆ›å»ºã€è®¤è¯æäº¤\n- ğŸ¨ **æ™ºèƒ½ç”Ÿæˆ**ï¼šè‡ªåŠ¨ç”Ÿæˆå­¦ç”Ÿè¯/æ•™å¸ˆè¯ PNG å›¾ç‰‡\n- ğŸ’° **ç§¯åˆ†ç³»ç»Ÿ**ï¼šç­¾åˆ°ã€é‚€è¯·ã€å¡å¯†å…‘æ¢ç­‰å¤šç§è·å–æ–¹å¼\n- ğŸ” **å®‰å…¨å¯é **ï¼šä½¿ç”¨ MySQL æ•°æ®åº“ï¼Œæ”¯æŒç¯å¢ƒå˜é‡é…ç½®\n- âš¡ **å¹¶å‘æ§åˆ¶**ï¼šæ™ºèƒ½ç®¡ç†å¹¶å‘è¯·æ±‚ï¼Œç¡®ä¿ç¨³å®šæ€§\n- ğŸ‘¥ **ç®¡ç†åŠŸèƒ½**ï¼šå®Œå–„çš„ç”¨æˆ·ç®¡ç†å’Œç§¯åˆ†ç®¡ç†ç³»ç»Ÿ\n\n---\n\n## ğŸ› ï¸ æŠ€æœ¯æ ˆ\n\n- **è¯­è¨€**ï¼šPython 3.11+\n- **Botæ¡†æ¶**ï¼špython-telegram-bot 20.0+\n- **æ•°æ®åº“**ï¼šMySQL 5.7+\n- **æµè§ˆå™¨è‡ªåŠ¨åŒ–**ï¼šPlaywright\n- **HTTPå®¢æˆ·ç«¯**ï¼šhttpx\n- **å›¾åƒå¤„ç†**ï¼šPillow, reportlab, xhtml2pdf\n- **ç¯å¢ƒç®¡ç†**ï¼špython-dotenv\n\n---\n\n## ğŸš€ å¿«é€Ÿå¼€å§‹\n\n### 1. å…‹éš†é¡¹ç›®\n\n```bash\ngit clone https://github.com/PastKing/tgbot-verify.git\ncd tgbot-verify\n```\n\n### 2. å®‰è£…ä¾èµ–\n\n```bash\npip install -r requirements.txt\nplaywright install chromium\n```\n\n### 3. é…ç½®ç¯å¢ƒå˜é‡\n\nå¤åˆ¶ `env.example` ä¸º `.env` å¹¶å¡«å†™é…ç½®ï¼š\n\n```env\n# Telegram Bot é…ç½®\nBOT_TOKEN=your_bot_token_here\nCHANNEL_USERNAME=your_channel\nCHANNEL_URL=https://t.me/your_channel\nADMIN_USER_ID=your_admin_id\n\n# MySQL æ•°æ®åº“é…ç½®\nMYSQL_HOST=localhost\nMYSQL_PORT=3306\nMYSQL_USER=root\nMYSQL_PASSWORD=your_password\nMYSQL_DATABASE=tgbot_verify\n```\n\n### 4. å¯åŠ¨æœºå™¨äºº\n\n```bash\npython bot.py\n```\n\n---\n\n## ğŸ³ Docker éƒ¨ç½²\n\n### ä½¿ç”¨ Docker Composeï¼ˆæ¨èï¼‰\n\n```bash\n# 1. ä¿®æ”¹ .env æ–‡ä»¶é…ç½®\ncp env.example .env\nnano .env\n\n# 2. å¯åŠ¨æœåŠ¡\ndocker-compose up -d\n\n# 3. æŸ¥çœ‹æ—¥å¿—\ndocker-compose logs -f\n```\n\n### æ‰‹åŠ¨ Docker éƒ¨ç½²\n\n```bash\n# æ„å»ºé•œåƒ\ndocker build -t tgbot-verify .\n\n# è¿è¡Œå®¹å™¨\ndocker run -d \\\n  --name tgbot-verify \\\n  --env-file .env \\\n  -v $(pwd)/logs:/app/logs \\\n  tgbot-verify\n```\n\n---\n\n## ğŸ“– ä½¿ç”¨è¯´æ˜\n\n### ç”¨æˆ·å‘½ä»¤\n\n```bash\n/start              # å¼€å§‹ä½¿ç”¨ï¼ˆæ³¨å†Œï¼‰\n/about              # äº†è§£æœºå™¨äººåŠŸèƒ½\n/balance            # æŸ¥çœ‹ç§¯åˆ†ä½™é¢\n/qd                 # æ¯æ—¥ç­¾åˆ°ï¼ˆ+1ç§¯åˆ†ï¼‰\n/invite             # ç”Ÿæˆé‚€è¯·é“¾æ¥ï¼ˆ+2ç§¯åˆ†/äººï¼‰\n/use <å¡å¯†>         # ä½¿ç”¨å¡å¯†å…‘æ¢ç§¯åˆ†\n/verify <é“¾æ¥>      # Gemini One Pro è®¤è¯\n/verify2 <é“¾æ¥>     # ChatGPT Teacher K12 è®¤è¯\n/verify3 <é“¾æ¥>     # Spotify Student è®¤è¯\n/verify4 <é“¾æ¥>     # Bolt.new Teacher è®¤è¯\n/verify5 <é“¾æ¥>     # YouTube Premium Student è®¤è¯\n/getV4Code <id>     # è·å– Bolt.new è®¤è¯ç \n/help               # æŸ¥çœ‹å¸®åŠ©ä¿¡æ¯\n```\n\n### ç®¡ç†å‘˜å‘½ä»¤\n\n```bash\n/addbalance <ç”¨æˆ·ID> <ç§¯åˆ†>     # å¢åŠ ç”¨æˆ·ç§¯åˆ†\n/block <ç”¨æˆ·ID>                 # æ‹‰é»‘ç”¨æˆ·\n/white <ç”¨æˆ·ID>                 # å–æ¶ˆæ‹‰é»‘\n/blacklist                      # æŸ¥çœ‹é»‘åå•\n/genkey <å¡å¯†> <ç§¯åˆ†> [æ¬¡æ•°] [å¤©æ•°]  # ç”Ÿæˆå¡å¯†\n/listkeys                       # æŸ¥çœ‹å¡å¯†åˆ—è¡¨\n/broadcast <æ–‡æœ¬>               # ç¾¤å‘é€šçŸ¥\n```\n\n### ä½¿ç”¨æµç¨‹\n\n1. **è·å–è®¤è¯é“¾æ¥**\n   - è®¿é—®å¯¹åº”æœåŠ¡çš„è®¤è¯é¡µé¢\n   - å¼€å§‹è®¤è¯æµç¨‹\n   - å¤åˆ¶æµè§ˆå™¨åœ°å€æ ä¸­çš„å®Œæ•´ URLï¼ˆåŒ…å« `verificationId`ï¼‰\n\n2. **æäº¤è®¤è¯è¯·æ±‚**\n   ```\n   /verify3 https://services.sheerid.com/verify/xxx/?verificationId=yyy\n   ```\n\n3. **ç­‰å¾…å¤„ç†**\n   - æœºå™¨äººè‡ªåŠ¨ç”Ÿæˆèº«ä»½ä¿¡æ¯\n   - åˆ›å»ºå­¦ç”Ÿè¯/æ•™å¸ˆè¯å›¾ç‰‡\n   - æäº¤åˆ° SheerID å¹³å°\n\n4. **è·å–ç»“æœ**\n   - å®¡æ ¸é€šå¸¸åœ¨å‡ åˆ†é’Ÿå†…å®Œæˆ\n   - æˆåŠŸåä¼šè¿”å›è·³è½¬é“¾æ¥\n\n---\n\n## ğŸ“ é¡¹ç›®ç»“æ„\n\n```\ntgbot-verify/\nâ”œâ”€â”€ bot.py                  # æœºå™¨äººä¸»ç¨‹åº\nâ”œâ”€â”€ config.py               # å…¨å±€é…ç½®\nâ”œâ”€â”€ database_mysql.py       # MySQL æ•°æ®åº“ç®¡ç†\nâ”œâ”€â”€ .env                    # ç¯å¢ƒå˜é‡é…ç½®ï¼ˆéœ€è‡ªè¡Œåˆ›å»ºï¼‰\nâ”œâ”€â”€ env.example             # ç¯å¢ƒå˜é‡æ¨¡æ¿\nâ”œâ”€â”€ requirements.txt        # Python ä¾èµ–\nâ”œâ”€â”€ Dockerfile              # Docker é•œåƒæ„å»º\nâ”œâ”€â”€ docker-compose.yml      # Docker Compose é…ç½®\nâ”œâ”€â”€ handlers/               # å‘½ä»¤å¤„ç†å™¨\nâ”‚   â”œâ”€â”€ user_commands.py    # ç”¨æˆ·å‘½ä»¤\nâ”‚   â”œâ”€â”€ admin_commands.py   # ç®¡ç†å‘˜å‘½ä»¤\nâ”‚   â””â”€â”€ verify_commands.py  # è®¤è¯å‘½ä»¤\nâ”œâ”€â”€ one/    ",
      "default_branch": "main"
    },
    "fetched_at": "2026-01-04T03:05:05.707561"
  },
  {
    "basic_info": {
      "name": "ech-wk",
      "full_name": "byJoey/ech-wk",
      "owner": "byJoey",
      "description": null,
      "url": "https://github.com/byJoey/ech-wk",
      "clone_url": "https://github.com/byJoey/ech-wk.git",
      "ssh_url": "git@github.com:byJoey/ech-wk.git",
      "homepage": null,
      "created_at": "2025-12-06T07:20:27Z",
      "updated_at": "2026-01-04T00:32:45Z",
      "pushed_at": "2025-12-19T09:00:15Z"
    },
    "stats": {
      "stars": 1622,
      "forks": 851,
      "watchers": 1622,
      "open_issues": 52,
      "size": 199
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 75305,
        "Go": 38017,
        "Shell": 13407,
        "JavaScript": 5189
      },
      "license": null,
      "topics": []
    },
    "content": {
      "readme": "# ECH Workers å®¢æˆ·ç«¯\n\n[![GitHub release](https://img.shields.io/github/release/byJoey/ech-wk.svg)](https://github.com/byJoey/ech-wk/releases)\n[![License](https://img.shields.io/badge/license-MIT-blue.svg)](LICENSE)\n\nè·¨å¹³å°çš„ ECH Workers ä»£ç†å®¢æˆ·ç«¯ï¼Œæ”¯æŒ Windowsã€macOS å’Œ Linuxï¼ˆARM/x86ï¼‰ï¼Œæä¾›å›¾å½¢ç•Œé¢å’Œå‘½ä»¤è¡Œä¸¤ç§ä½¿ç”¨æ–¹å¼ã€‚\n\n## ğŸ“‹ ç›®å½•\n\n- [åŠŸèƒ½ç‰¹æ€§](#åŠŸèƒ½ç‰¹æ€§)\n- [ç‰ˆæœ¬æ›´æ–°](#ç‰ˆæœ¬æ›´æ–°)\n- [å¿«é€Ÿå¼€å§‹](#å¿«é€Ÿå¼€å§‹)\n- [å‘½ä»¤è¡Œä½¿ç”¨](#å‘½ä»¤è¡Œä½¿ç”¨)\n- [å›¾å½¢ç•Œé¢ä½¿ç”¨](#å›¾å½¢ç•Œé¢ä½¿ç”¨)\n- [è½¯è·¯ç”±éƒ¨ç½²](#è½¯è·¯ç”±éƒ¨ç½²)\n- [ç³»ç»Ÿè¦æ±‚](#ç³»ç»Ÿè¦æ±‚)\n- [æ•…éšœæ’é™¤](#æ•…éšœæ’é™¤)\n- [æŠ€æœ¯æ–‡æ¡£](#æŠ€æœ¯æ–‡æ¡£)\n\n## âœ¨ åŠŸèƒ½ç‰¹æ€§\n\n### æ ¸å¿ƒåŠŸèƒ½\n- âœ… **ECH åŠ å¯†** - åŸºäº TLS 1.3 ECH (Encrypted Client Hello) æŠ€æœ¯ï¼ŒåŠ å¯† SNI ä¿¡æ¯\n- âœ… **å¤šåè®®æ”¯æŒ** - åŒæ—¶æ”¯æŒ SOCKS5 å’Œ HTTP CONNECT ä»£ç†åè®®\n- âœ… **æ™ºèƒ½åˆ†æµ** - ä¸‰ç§åˆ†æµæ¨¡å¼ï¼šå…¨å±€ä»£ç†ã€è·³è¿‡ä¸­å›½å¤§é™†ã€ç›´è¿æ¨¡å¼\n- âœ… **IPv4/IPv6 åŒæ ˆ** - å®Œæ•´æ”¯æŒ IPv4 å’Œ IPv6 åœ°å€çš„åˆ†æµåˆ¤æ–­\n\n### å›¾å½¢ç•Œé¢åŠŸèƒ½\n- âœ… **å¤šæœåŠ¡å™¨ç®¡ç†** - æ”¯æŒå¤šä¸ªæœåŠ¡å™¨é…ç½®ï¼Œå¿«é€Ÿåˆ‡æ¢\n- âœ… **ä¸€é”®ç³»ç»Ÿä»£ç†** - è‡ªåŠ¨è®¾ç½®ç³»ç»Ÿä»£ç†ï¼Œæ”¯æŒåˆ†æµæ¨¡å¼\n- âœ… **ç³»ç»Ÿæ‰˜ç›˜** - æœ€å°åŒ–åˆ°ç³»ç»Ÿæ‰˜ç›˜ï¼Œä¸å ç”¨ä»»åŠ¡æ \n- âœ… **å¼€æœºè‡ªå¯** - æ”¯æŒ Windows å’Œ macOS å¼€æœºè‡ªåŠ¨å¯åŠ¨\n- âœ… **é«˜ DPI æ”¯æŒ** - å®Œç¾æ”¯æŒé«˜åˆ†è¾¨ç‡æ˜¾ç¤ºå™¨\n- âœ… **å®æ—¶æ—¥å¿—** - æŸ¥çœ‹ä»£ç†è¿è¡ŒçŠ¶æ€å’Œæ—¥å¿—\n- âœ… **é…ç½®æŒä¹…åŒ–** - è‡ªåŠ¨ä¿å­˜é…ç½®ï¼Œä¸‹æ¬¡å¯åŠ¨è‡ªåŠ¨åŠ è½½\n\n### é«˜çº§åŠŸèƒ½\n- âœ… **è‡ªåŠ¨ IP åˆ—è¡¨æ›´æ–°** - è‡ªåŠ¨ä¸‹è½½å¹¶åº”ç”¨å®Œæ•´çš„ä¸­å›½ IP åˆ—è¡¨ï¼ˆIPv4/IPv6ï¼‰\n- âœ… **DNS ä¼˜é€‰** - æ”¯æŒè‡ªå®šä¹‰ DoH æœåŠ¡å™¨è¿›è¡Œ ECH æŸ¥è¯¢\n- âœ… **IP ç›´è¿** - æ”¯æŒæŒ‡å®šæœåŠ¡ç«¯ IPï¼Œç»•è¿‡ DNS è§£æ\n- âœ… **è·¨å¹³å°æ”¯æŒ** - Windowsã€macOSã€Linuxï¼ˆx86_64/ARM64ï¼‰\n\n## ğŸ†• ç‰ˆæœ¬æ›´æ–°\n\n### v1.3 æœ€æ–°ä¼˜åŒ–\n\n#### æ ¸å¿ƒåŠŸèƒ½å¢å¼º\n- **IPv6 å®Œæ•´æ”¯æŒ**\n  - æ–°å¢ IPv6 åœ°å€åˆ†æµåˆ¤æ–­åŠŸèƒ½\n  - è‡ªåŠ¨ä¸‹è½½å¹¶åŠ è½½ä¸­å›½ IPv6 IP åˆ—è¡¨ï¼ˆ`chn_ip_v6.txt`ï¼‰\n  - æ”¯æŒ IPv4/IPv6 åŒæ ˆç¯å¢ƒä¸‹çš„æ™ºèƒ½åˆ†æµ\n\n- **æ™ºèƒ½ IP åˆ—è¡¨ç®¡ç†**\n  - è‡ªåŠ¨æ£€æµ‹ IP åˆ—è¡¨æ–‡ä»¶æ˜¯å¦å­˜åœ¨æˆ–ä¸ºç©º\n  - æ–‡ä»¶ç¼ºå¤±æ—¶è‡ªåŠ¨ä» GitHub ä¸‹è½½æœ€æ–°åˆ—è¡¨\n  - æ”¯æŒ IPv4 å’Œ IPv6 åˆ—è¡¨çš„ç‹¬ç«‹ç®¡ç†\n  - åˆ—è¡¨æ¥æºï¼š[mayaxcn/china-ip-list](https://github.com/mayaxcn/china-ip-list)\n\n- **åˆ†æµé€»è¾‘ä¼˜åŒ–**\n  - åˆ†æµåˆ¤æ–­é€»è¾‘ç§»è‡³ Go æ ¸å¿ƒç¨‹åºï¼Œæ€§èƒ½æ›´ä¼˜\n  - æ”¯æŒåŸŸåè§£æåçš„å¤š IP åœ°å€åˆ¤æ–­\n  - æ”¹è¿›çš„äºŒåˆ†æŸ¥æ‰¾ç®—æ³•ï¼Œæå‡æŸ¥è¯¢æ•ˆç‡\n\n#### å‘½ä»¤è¡Œä½“éªŒæ”¹è¿›\n- **é»˜è®¤è¡Œä¸ºä¼˜åŒ–**\n  - å‘½ä»¤è¡Œæ¨¡å¼ä¸‹ï¼Œ`-routing` å‚æ•°é»˜è®¤å€¼æ”¹ä¸º `global`ï¼ˆå…¨å±€ä»£ç†ï¼‰\n  - æ›´ç¬¦åˆå‘½ä»¤è¡Œç”¨æˆ·çš„ä½¿ç”¨ä¹ æƒ¯\n  - GUI æ¨¡å¼ä¸å—å½±å“ï¼Œä»ä½¿ç”¨é…ç½®çš„é»˜è®¤å€¼\n\n- **å‚æ•°è¯´æ˜å®Œå–„**\n  - æ›´æ–°å¸®åŠ©ä¿¡æ¯ï¼Œæ˜ç¡®å„å‚æ•°çš„ä½œç”¨å’Œé»˜è®¤å€¼\n  - æ·»åŠ åˆ†æµæ¨¡å¼çš„è¯¦ç»†è¯´æ˜\n\n#### å…¼å®¹æ€§æå‡\n- **å‘åå…¼å®¹**\n  - ä¿æŒä¸æ—§ç‰ˆæœ¬é…ç½®æ–‡ä»¶çš„å…¼å®¹æ€§\n  - è‡ªåŠ¨è¿ç§»å’Œå‡çº§é…ç½®æ ¼å¼\n  - å¹³æ»‘å‡çº§ä½“éªŒ\n\n### å†å²ç‰ˆæœ¬\n\n#### v1.0\n- åˆå§‹ç‰ˆæœ¬å‘å¸ƒ\n- åŸºç¡€ä»£ç†åŠŸèƒ½\n- å›¾å½¢ç•Œé¢æ”¯æŒ\n- ç³»ç»Ÿä»£ç†è®¾ç½®\n\n## ğŸš€ å¿«é€Ÿå¼€å§‹\n\n### æ–¹æ³• 1: ä½¿ç”¨é¢„ç¼–è¯‘ç‰ˆæœ¬ï¼ˆæ¨èï¼‰\n\nä» [GitHub Releases](https://github.com/byJoey/ech-wk/releases) ä¸‹è½½å¯¹åº”å¹³å°çš„å‹ç¼©åŒ…ï¼š\n\n#### æ¡Œé¢ç‰ˆæœ¬ï¼ˆåŒ…å« GUIï¼‰\n- **Windows x64**: `ECHWorkers-windows-amd64.zip`\n- **macOS Intel**: `ECHWorkers-darwin-amd64.zip`\n- **macOS Apple Silicon**: `ECHWorkers-darwin-arm64.zip`\n- **Linux x86_64**: `ECHWorkers-linux-amd64.tar.gz`\n- **Linux ARM64**: `ECHWorkers-linux-arm64.tar.gz`\n\n#### è½¯è·¯ç”±ç‰ˆæœ¬ï¼ˆä»…å‘½ä»¤è¡Œï¼‰\n- **Linux x86_64**: `ECHWorkers-linux-amd64-softrouter.tar.gz`\n- **Linux ARM64**: `ECHWorkers-linux-arm64-softrouter.tar.gz`\n#### Dockerç‰ˆæœ¬ï¼ˆä»…æµ‹è¯•x86ï¼‰\n- **DockerHubä»“åº“**ï¼šhttps://hub.docker.com/r/cirnosalt/ech-workers-docker\n#### å®‰è£…æ­¥éª¤\n\n1. **è§£å‹æ–‡ä»¶**\n   ```bash\n   # Windows: è§£å‹åˆ°ä»»æ„ç›®å½•\n   # macOS/Linux: è§£å‹åˆ° /usr/local/bin æˆ–è‡ªå®šä¹‰ç›®å½•\n   tar -xzf ECHWorkers-linux-amd64.tar.gz\n   ```\n\n2. **è®¾ç½®æ‰§è¡Œæƒé™**ï¼ˆLinux/macOSï¼‰\n   ```bash\n   chmod +x ech-workers\n   chmod +x ECHWorkersGUI  # å¦‚æœä½¿ç”¨ GUI\n   ```\n\n3. **è¿è¡Œç¨‹åº**\n   - **Windows**: åŒå‡» `ECHWorkersGUI.exe` å¯åŠ¨ GUIï¼Œæˆ–è¿è¡Œ `ech-workers.exe` ä½¿ç”¨å‘½ä»¤è¡Œ\n   - **macOS/Linux**: è¿è¡Œ `./ECHWorkersGUI` å¯åŠ¨ GUIï¼Œæˆ–è¿è¡Œ `./ech-workers` ä½¿ç”¨å‘½ä»¤è¡Œ\n\n> **æ³¨æ„**: é¢„ç¼–è¯‘ç‰ˆæœ¬å·²åŒ…å«æ‰€æœ‰ä¾èµ–ï¼Œæ— éœ€å®‰è£… Python æˆ–ä»»ä½•å…¶ä»–è½¯ä»¶ã€‚  \n> é¦–æ¬¡è¿è¡Œ\"è·³è¿‡ä¸­å›½å¤§é™†\"æ¨¡å¼æ—¶ï¼Œç¨‹åºä¼šè‡ªåŠ¨ä¸‹è½½ IP åˆ—è¡¨æ–‡ä»¶ã€‚\n\n## ğŸ’» å‘½ä»¤è¡Œä½¿ç”¨\n\n`ech-workers` æ”¯æŒçº¯å‘½ä»¤è¡Œè¿è¡Œï¼Œé€‚åˆæœåŠ¡å™¨ç¯å¢ƒã€è½¯è·¯ç”±æˆ–æ— å›¾å½¢ç•Œé¢åœºæ™¯ã€‚\n\n### å‘½ä»¤è¯­æ³•\n\n```bash\nech-workers [é€‰é¡¹]\n```\n\n### å‚æ•°è¯´æ˜\n\n#### å¿…éœ€å‚æ•°\n\n| å‚æ•° | è¯´æ˜ | ç¤ºä¾‹ |\n|------|------|------|\n| `-f` | æœåŠ¡ç«¯åœ°å€ï¼ˆå¿…éœ€ï¼‰ | `-f your-worker.workers.dev:443` |\n\n#### å¯é€‰å‚æ•°\n\n| å‚æ•° | é»˜è®¤å€¼ | è¯´æ˜ | ç¤ºä¾‹ |\n|------|--------|------|------|\n| `-l` | `127.0.0.1:30000` | æœ¬åœ°ç›‘å¬åœ°å€ | `-l 0.0.0.0:30001` |\n| `-token` | ç©º | èº«ä»½éªŒè¯ä»¤ç‰Œ | `-token your-token-here` |\n| `-ip` | ç©º | æŒ‡å®šæœåŠ¡ç«¯ IPï¼ˆç»•è¿‡ DNSï¼‰ | `-ip 1.2.3.4` |\n| `-dns` | `dns.alidns.com/dns-query` | ECH æŸ¥è¯¢ DoH æœåŠ¡å™¨ | `-dns dns.alidns.com/dns-query` |\n| `-ech` | `cloudflare-ech.com` | ECH æŸ¥è¯¢åŸŸå | `-ech cloudflare-ech.com` |\n| `-routing` | `global` | åˆ†æµæ¨¡å¼ | `-routing bypass_cn` |\n\n#### åˆ†æµæ¨¡å¼è¯´æ˜\n\n| æ¨¡å¼ | å€¼ | è¯´æ˜ |\n|------|-----|------|\n| **å…¨å±€ä»£ç†** | `global` | æ‰€æœ‰æµé‡éƒ½èµ°ä»£ç†ï¼ˆé»˜è®¤æ¨¡å¼ï¼‰ |\n| **è·³è¿‡ä¸­å›½å¤§é™†** | `bypass_cn` | ä¸­å›½ IP ç›´è¿ï¼Œå…¶ä»–èµ°ä»£ç† |\n| **ç›´è¿æ¨¡å¼** | `none` | æ‰€æœ‰æµé‡ç›´è¿ï¼Œä¸è®¾ç½®ä»£ç† |\n\n> **æ³¨æ„**: \n> - ä½¿ç”¨ `bypass_cn` æ¨¡å¼æ—¶ï¼Œç¨‹åºä¼šè‡ªåŠ¨ä¸‹è½½ä¸­å›½ IP åˆ—è¡¨ï¼ˆIPv4/IPv6ï¼‰\n> - å¦‚æœ IP åˆ—è¡¨æ–‡ä»¶ä¸å­˜åœ¨æˆ–ä¸ºç©ºï¼Œç¨‹åºä¼šè‡ªåŠ¨ä» GitHub ä¸‹è½½\n> - IP åˆ—è¡¨æ–‡ä»¶ä¿å­˜åœ¨ç¨‹åºç›®å½•ï¼š`chn_ip.txt`ï¼ˆIPv4ï¼‰å’Œ `chn_ip_v6.txt`ï¼ˆIPv6ï¼‰\n\n### ä½¿ç”¨ç¤ºä¾‹\n\n#### åŸºæœ¬ç”¨æ³•\n\n```bash\n# Windows\nech-workers.exe -f your-worker.workers.dev:443\n\n# macOS / Linux\n./ech-workers -f your-worker.workers.dev:443\n```\n\n#### æŒ‡å®šç›‘å¬åœ°å€\n\n```bash\n# ç›‘å¬æ‰€æœ‰ç½‘ç»œæ¥å£ï¼ˆé€‚åˆè½¯è·¯ç”±ï¼‰\n./ech-workers -f your-worker.workers.dev:443 -l 0.0.0.0:30001\n\n# ä»…ç›‘å¬æœ¬åœ°ï¼ˆé»˜è®¤ï¼‰\n./ech-workers -f your-worker.workers.dev:443 -l 127.0.0.1:30001\n```\n\n#### ä½¿ç”¨åˆ†æµæ¨¡å¼\n\n```bash\n# å…¨å±€ä»£ç†æ¨¡å¼ï¼ˆé»˜è®¤ï¼‰\n./ech-workers -f your-worker.workers.dev:443 -routing global\n\n# è·³è¿‡ä¸­å›½å¤§é™†æ¨¡å¼ï¼ˆè‡ªåŠ¨ä¸‹è½½ IP åˆ—è¡¨ï¼‰\n./ech-workers -f your-worker.workers.dev:443 -routing bypass_cn\n\n# ç›´è¿æ¨¡å¼\n./ech-workers -f your-worker.workers.dev:443 -routing none\n```\n\n#### å®Œæ•´å‚æ•°ç¤ºä¾‹\n\n```bash\n./ech-workers \\\n  -f your-worker.workers.dev:443 \\\n  -l 0.0.0.0:30001 \\\n  -token your-token \\\n  -ip saas.sin.fan \\\n  -dns dns.alidns.com/dns-query \\\n  -ech cloudflare-ech.com \\\n  -routing bypass_cn\n```\n\n#### æŸ¥çœ‹å¸®åŠ©\n\n```bash\n./ech-workers -h\n# æˆ–\n./ech-workers --help\n```\n\n### åå°è¿è¡Œ\n\n#### Linux/macOS\n\n**ä½¿ç”¨ nohup:**\n```bash\nnohup ./ech-workers -f your-worker.workers.dev:443 -l 127.0.0.1:30001 > ech-workers.log 2>&1 &\n```\n\n**ä½¿ç”¨ screen:**\n```bash\nscreen -S ech-work",
      "default_branch": "main"
    },
    "fetched_at": "2026-01-04T03:05:06.873454"
  },
  {
    "basic_info": {
      "name": "DeepTutor",
      "full_name": "HKUDS/DeepTutor",
      "owner": "HKUDS",
      "description": "\"DeepTutor: AI-Powered Personalized Learning Assistant\"",
      "url": "https://github.com/HKUDS/DeepTutor",
      "clone_url": "https://github.com/HKUDS/DeepTutor.git",
      "ssh_url": "git@github.com:HKUDS/DeepTutor.git",
      "homepage": "https://hkuds.github.io/DeepTutor",
      "created_at": "2025-12-28T15:35:54Z",
      "updated_at": "2026-01-04T02:57:26Z",
      "pushed_at": "2026-01-03T17:48:03Z"
    },
    "stats": {
      "stars": 1464,
      "forks": 209,
      "watchers": 1464,
      "open_issues": 1,
      "size": 59634
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 1302503,
        "TypeScript": 785025,
        "Shell": 13838,
        "Dockerfile": 10237,
        "CSS": 8674,
        "JavaScript": 2636
      },
      "license": "GNU Affero General Public License v3.0",
      "topics": [
        "ai-agents",
        "ai-tutor",
        "deepresearch",
        "idea-generation",
        "interactive-learning",
        "knowledge-graph",
        "large-language-models",
        "multi-agent-systems",
        "rag"
      ]
    },
    "content": {
      "readme": "<div align=\"center\">\n\n<img src=\"assets/logo-ver2.png\" alt=\"DeepTutor Logo\" width=\"150\" style=\"border-radius: 15px;\">\n\n# DeepTutor: AI-Powered Personalized Learning Assistant\n\n[![Python](https://img.shields.io/badge/Python-3.10%2B-3776AB?style=flat-square&logo=python&logoColor=white)](https://www.python.org/downloads/)\n[![Next.js](https://img.shields.io/badge/Next.js-16-000000?style=flat-square&logo=next.js&logoColor=white)](https://nextjs.org/)\n[![FastAPI](https://img.shields.io/badge/FastAPI-0.100+-009688?style=flat-square&logo=fastapi&logoColor=white)](https://fastapi.tiangolo.com/)\n[![License](https://img.shields.io/badge/License-AGPL--3.0-blue?style=flat-square)](LICENSE)\n[![Discord](https://img.shields.io/badge/Discord-Join-7289DA?style=flat&logo=discord&logoColor=white)](https://discord.gg/aka9p9EW)\n[![Feishu](https://img.shields.io/badge/Feishu-Group-blue?style=flat)](./Communication.md)\n[![WeChat](https://img.shields.io/badge/WeChat-Group-green?style=flat&logo=wechat)](./Communication.md)\n\n\n\n[**Quick Start**](#quick-start) Â· [**Core Modules**](#core-modules) Â· [**FAQ**](#faq)\n\n[ğŸ‡¨ğŸ‡³ ä¸­æ–‡](assets/README/README_CN.md) Â· [ğŸ‡¯ğŸ‡µ æ—¥æœ¬èª](assets/README/README_JA.md) Â· [ğŸ‡ªğŸ‡¸ EspaÃ±ol](assets/README/README_ES.md) Â· [ğŸ‡«ğŸ‡· FranÃ§ais](assets/README/README_FR.md) Â· [ğŸ‡¸ğŸ‡¦ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©](assets/README/README_AR.md) Â· [ğŸ‡·ğŸ‡º Ğ ÑƒÑÑĞºĞ¸Ğ¹](assets/README/README_RU.md) Â· [ğŸ‡®ğŸ‡³ à¤¹à¤¿à¤¨à¥à¤¦à¥€](assets/README/README_HI.md) Â· [ğŸ‡µğŸ‡¹ PortuguÃªs](assets/README/README_PT.md)\n\n</div>\n\n<div align=\"center\">\n\nğŸ“š **Massive Document Knowledge Q&A** &nbsp;â€¢&nbsp; ğŸ¨ **Interactive Learning Visualization**<br>\nğŸ¯ **Knowledge Reinforcement** &nbsp;â€¢&nbsp; ğŸ” **Deep Research & Idea Generation**\n\n</div>\n\n---\n> **[2026.1.3]** Released DeepTutor [v0.2.0](https://github.com/HKUDS/DeepTutor/releases/tag/v0.2.0) - thanks to all the contributors! â¤ï¸\n\n> **[2026.1.1]** Happy New Year! Join our [GitHub Discussions](https://github.com/HKUDS/DeepTutor/discussions) - shape the future of DeepTutor! ğŸ’¬\n\n> **[2025.12.30]** Visit our [Official Website](https://hkuds.github.io/DeepTutor/) for more details!\n\n> **[2025.12.29]** DeepTutor v0.1 is now live! âœ¨\n---\n\n## Key Features of DeepTutor\n\n### ğŸ“š Massive Document Knowledge Q&A\nâ€¢ **Smart Knowledge Base**: Upload textbooks, research papers, technical manuals, and domain-specific documents. Build a comprehensive AI-powered knowledge repository for instant access.<br>\nâ€¢ **Multi-Agent Problem Solving**: Dual-loop reasoning architecture with RAG, web search, and code execution -- delivering step-by-step solutions with precise citations.\n\n### ğŸ¨ Interactive Learning Visualization\nâ€¢ **Knowledge Simplification & Explanations**: Transform complex concepts, knowledge, and algorithms into easy-to-understand visual aids, detailed step-by-step breakdowns, and engaging interactive demonstrations.<br>\nâ€¢ **Personalized Q&A**: Context-aware conversations that adapt to your learning progress, with interactive pages and session-based knowledge tracking.\n\n### ğŸ¯ Knowledge Reinforcement with Practice Exercise Generator\nâ€¢ **Intelligent Exercise Creation**: Generate targeted quizzes, practice problems, and customized assessments tailored to your current knowledge level and specific learning objectives.<br>\nâ€¢ **Authentic Exam Simulation**: Upload reference exams to generate practice questions that perfectly match the original style, format, and difficultyâ€”giving you realistic preparation for the actual test.\n\n### ğŸ” Deep Research & Idea Generation\nâ€¢ **Comprehensive Research & Literature Review**: Conduct in-depth topic exploration with systematic analysis. Identify patterns, connect related concepts across disciplines, and synthesize existing research findings.<br>\nâ€¢ **Novel Insight Discovery**: Generate structured learning materials and uncover knowledge gaps. Identify promising new research directions through intelligent cross-domain knowledge synthesis.\n\n---\n\n<div align=\"center\">\n  <img src=\"assets/figs/title_gradient.svg\" alt=\"All-in-One Tutoring System\" width=\"70%\">\n</div>\n\n<!-- â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” Core Learning Experience â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” -->\n\n<table>\n<tr>\n<td width=\"50%\" align=\"center\" valign=\"top\">\n\n<h3>ğŸ“š Massive Document Knowledge Q&A</h3>\n<a href=\"#problem-solving-agent\">\n<img src=\"assets/gifs/solve.gif\" width=\"100%\">\n</a>\n<br>\n<sub>Multi-agent Problem Solving with Exact Citations</sub>\n\n</td>\n<td width=\"50%\" align=\"center\" valign=\"top\">\n\n<h3>ğŸ¨ Interactive Learning Visualization</h3>\n<a href=\"#guided-learning\">\n<img src=\"assets/gifs/guided-learning.gif\" width=\"100%\">\n</a>\n<br>\n<sub>Step-by-step Visual Explanations with Personal QAs.</sub>\n\n</td>\n</tr>\n</table>\n\n<!-- â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” Practice & Reinforcement â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” -->\n\n<h3 align=\"center\">ğŸ¯ Knowledge Reinforcement</h3>\n\n<table>\n<tr>\n<td width=\"50%\" valign=\"top\" align=\"center\">\n\n<a href=\"#question-generator\">\n<img src=\"assets/gifs/question-1.gif\" width=\"100%\">\n</a>\n\n**Custom Questions**  \n<sub>Auto-Validated Practice Questions Generation</sub>\n\n</td>\n<td width=\"50%\" valign=\"top\" align=\"center\">\n\n<a href=\"#question-g",
      "default_branch": "main"
    },
    "fetched_at": "2026-01-04T03:05:08.071366"
  },
  {
    "basic_info": {
      "name": "Continuous-Claude-v2",
      "full_name": "parcadei/Continuous-Claude-v2",
      "owner": "parcadei",
      "description": "Context management for Claude Code. Hooks maintain state via ledgers and handoffs. MCP execution without context pollution. Agent orchestration with isolated context windows.",
      "url": "https://github.com/parcadei/Continuous-Claude-v2",
      "clone_url": "https://github.com/parcadei/Continuous-Claude-v2.git",
      "ssh_url": "git@github.com:parcadei/Continuous-Claude-v2.git",
      "homepage": "",
      "created_at": "2025-12-23T00:12:49Z",
      "updated_at": "2026-01-04T02:58:26Z",
      "pushed_at": "2026-01-01T00:14:31Z"
    },
    "stats": {
      "stars": 1432,
      "forks": 86,
      "watchers": 1432,
      "open_issues": 3,
      "size": 541
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 527926,
        "Shell": 77597,
        "TypeScript": 53805,
        "JavaScript": 8324
      },
      "license": "Other",
      "topics": [
        "agents",
        "claude-code",
        "claude-code-cli",
        "claude-code-hooks",
        "claude-code-mcp",
        "claude-code-skills",
        "claude-code-subagents",
        "claude-skills",
        "mcp"
      ]
    },
    "content": {
      "readme": "# Continuous Claude\n\nSession continuity, token-efficient MCP execution, and agentic workflows for Claude Code.\n\n---\n\n## Table of Contents\n\n- [Architecture Overview](#architecture-overview)\n- [The Problem](#the-problem) / [The Solution](#the-solution)\n- [Quick Start](#quick-start) (project or global install)\n- [How to Talk to Claude](#how-to-talk-to-claude)\n- [Skills vs Agents](#skills-vs-agents)\n- [MCP Code Execution](#mcp-code-execution)\n- [Continuity System](#continuity-system)\n- [Hooks System](#hooks-system)\n- [Reasoning History](#reasoning-history)\n- [Braintrust Session Tracing](#braintrust-session-tracing-optional) + [Compound Learnings](#compound-learnings)\n- [Artifact Index](#artifact-index) (handoff search, outcome tracking)\n- [TDD Workflow](#tdd-workflow)\n- [Code Quality (qlty)](#code-quality-qlty)\n- [Directory Structure](#directory-structure)\n- [Environment Variables](#environment-variables)\n- [Glossary](#glossary)\n- [Troubleshooting](#troubleshooting)\n- [Acknowledgments](#acknowledgments)\n\n---\n\n## Architecture Overview\n\n```\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                            CLAUDE CODE SESSION                              â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚                                                                             â”‚\nâ”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚\nâ”‚   â”‚ SessionStartâ”‚â”€â”€â”€â–¶â”‚   Working   â”‚â”€â”€â”€â–¶â”‚  PreCompact â”‚â”€â”€â”€â–¶â”‚ SessionEnd â”‚  â”‚\nâ”‚   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â”‚\nâ”‚          â”‚                  â”‚                  â”‚                  â”‚         â”‚\nâ”‚          â–¼                  â–¼                  â–¼                  â–¼         â”‚\nâ”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚\nâ”‚   â”‚Load Ledger   â”‚   â”‚PreToolUse    â”‚   â”‚Auto-Handoff  â”‚   â”‚Mark       â”‚   â”‚\nâ”‚   â”‚Load Handoff  â”‚   â”‚ TS Preflight â”‚   â”‚Block Manual  â”‚   â”‚Outcome    â”‚   â”‚\nâ”‚   â”‚Surface       â”‚   â”‚PostToolUse   â”‚   â”‚              â”‚   â”‚Cleanup    â”‚   â”‚\nâ”‚   â”‚Learnings     â”‚   â”‚UserPrompt    â”‚   â”‚              â”‚   â”‚Learn      â”‚   â”‚\nâ”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚\nâ”‚                                                                             â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                â”‚                â”‚                â”‚                â”‚\n                â–¼                â–¼                â–¼                â–¼\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                              DATA LAYER                                     â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚                                                                             â”‚\nâ”‚  thoughts/                    .claude/cache/                                â”‚\nâ”‚  â”œâ”€â”€ ledgers/                 â”œâ”€â”€ artifact-index/                           â”‚\nâ”‚  â”‚   â””â”€â”€ CONTINUITY_*.md          â””â”€â”€ context.db (SQLite+FTS5)             â”‚\nâ”‚  â””â”€â”€ shared/                  â”œâ”€â”€ learnings/                                â”‚\nâ”‚      â”œâ”€â”€ handoffs/                â””â”€â”€ <date>_<session>.md                   â”‚\nâ”‚      â”‚   â””â”€â”€ <session>/       â””â”€â”€ braintrust_sessions/                      â”‚\nâ”‚      â”‚       â””â”€â”€ *.md             â””â”€â”€ <session>.json                        â”‚\nâ”‚      â””â”€â”€ plans/                                                             â”‚\nâ”‚          â””â”€â”€ *.md             .git/claude/                                  â”‚\nâ”‚                               â””â”€â”€ commits/                                  â”‚\nâ”‚                                   â””â”€â”€ <hash>/reasoning.md                   â”‚\nâ”‚                                                                             â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                â”‚                                        â”‚\n                â–¼                                        â–¼\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚          SKILLS                   â”‚  â”‚           AGENTS                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚                                   â”‚  â”‚                                   â”‚\nâ”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚\nâ”‚  â”‚ continuity_ledgerâ”‚ Save state  â”‚  â”‚  â”‚ plan-agent       â”‚ Create plan â”‚\nâ”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤             â”‚  â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤             â”‚\nâ”‚  â”‚ create_handoff   â”‚ End session â”‚  â”‚  â”‚ validate-agent   â”‚ Check tech  â”‚\nâ”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤             â”‚  â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤             â”‚\nâ”‚  â”‚ resume_handoff   â”‚ Resume work â”‚  â”‚  â”‚ implement_plan   â”‚ Execute     â”‚\nâ”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤             â”‚  â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤             â”‚\nâ”‚  â”‚ commit           â”‚ Git commit  â”‚  â”‚  â”‚ research-agent   â”‚ Research    â”‚\nâ”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤             â”‚  â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€",
      "default_branch": "main"
    },
    "fetched_at": "2026-01-04T03:05:09.234042"
  },
  {
    "basic_info": {
      "name": "HY-Motion-1.0",
      "full_name": "Tencent-Hunyuan/HY-Motion-1.0",
      "owner": "Tencent-Hunyuan",
      "description": "HY-Motion model for 3D character animation generation. ",
      "url": "https://github.com/Tencent-Hunyuan/HY-Motion-1.0",
      "clone_url": "https://github.com/Tencent-Hunyuan/HY-Motion-1.0.git",
      "ssh_url": "git@github.com:Tencent-Hunyuan/HY-Motion-1.0.git",
      "homepage": "https://hunyuan.tencent.com/motion",
      "created_at": "2025-12-29T11:09:18Z",
      "updated_at": "2026-01-04T02:59:17Z",
      "pushed_at": "2026-01-04T02:42:04Z"
    },
    "stats": {
      "stars": 1332,
      "forks": 80,
      "watchers": 1332,
      "open_issues": 7,
      "size": 20526
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 285533,
        "HTML": 40286
      },
      "license": "Other",
      "topics": []
    },
    "content": {
      "readme": "[ä¸­æ–‡é˜…è¯»](README_zh_cn.md)\n\n\n<p align=\"center\">\n  <img src=\"./assets/banner.png\" alt=\"Banner\" width=\"100%\">\n</p>\n\n<div align=\"center\">\n  <a href=\"https://hunyuan.tencent.com/motion\" target=\"_blank\">\n    <img src=\"https://img.shields.io/badge/Official%20Site-333399.svg?logo=homepage\" height=\"22px\" alt=\"Official Site\">\n  </a>\n  <a href=\"https://github.com/Tencent-Hunyuan/HY-Motion-1.0\" target=\"_blank\">\n    <img src=\"https://img.shields.io/badge/GitHub-Repo-181717?logo=github&logoColor=white\" height=\"22px\" alt=\"Github Repo\">\n  </a>\n  <a href=\"https://huggingface.co/spaces/tencent/HY-Motion-1.0\" target=\"_blank\">\n    <img src=\"https://img.shields.io/badge/%F0%9F%A4%97%20Demo-276cb4.svg\" height=\"22px\" alt=\"HuggingFace Space\">\n  </a>\n  <a href=\"https://huggingface.co/tencent/HY-Motion-1.0\" target=\"_blank\">\n    <img src=\"https://img.shields.io/badge/%F0%9F%A4%97%20Models-d96902.svg\" height=\"22px\" alt=\"HuggingFace Models\">\n  </a>\n  <a href=\"https://arxiv.org/pdf/2512.23464\" target=\"_blank\">\n    <img src=\"https://img.shields.io/badge/Report-b5212f.svg?logo=arxiv\" height=\"22px\" alt=\"ArXiv Report\">\n  </a>\n  <a href=\"https://x.com/TencentHunyuan\" target=\"_blank\">\n    <img src=\"https://img.shields.io/badge/Hunyuan-black.svg?logo=x\" height=\"22px\" alt=\"X (Twitter)\">\n  </a>\n</div>\n\n\n# HY-Motion 1.0: Scaling Flow Matching Models for 3D Motion Generation\n\n\n<p align=\"center\">\n  <img src=\"./assets/teaser.jpg\" alt=\"Teaser\" width=\"100%\">\n</p>\n\n\n## ğŸ”¥ News\n- **Dec 30, 2025**: ğŸ¤— We released the inference code and pretrained models of [HY-Motion 1.0](https://huggingface.co/tencent/HY-Motion-1.0). Please give it a try via our [HuggingFace Space](https://huggingface.co/spaces/tencent/HY-Motion-1.0) and our [Official Site](https://hunyuan.tencent.com/motion)!\n\n\n## **Introduction**\n\n**HY-Motion 1.0** is a series of text-to-3D human motion generation models based on Diffusion Transformer (DiT) and Flow Matching. It allows developers to generate skeleton-based 3D character animations from simple text prompts, which can be directly integrated into various 3D animation pipelines. This model series is the first to scale DiT-based text-to-motion models to the billion-parameter level, achieving significant improvements in instruction-following capabilities and motion quality over existing open-source models.\n\n### Key Features\n- **State-of-the-Art Performance**: Achieves state-of-the-art performance in both instruction-following capability and generated motion quality.\n\n- **Billion-Scale Models**: We are the first to successfully scale DiT-based models to the billion-parameter level for text-to-motion generation. This results in superior instruction understanding and following capabilities, outperforming comparable open-source models.\n\n- **Advanced Three-Stage Training**: Our models are trained using a comprehensive three-stage process:\n\n    - *Large-Scale Pre-training*: Trained on over 3,000 hours of diverse motion data to learn a broad motion prior.\n\n    - *High-Quality Fine-tuning*: Fine-tuned on 400 hours of curated, high-quality 3D motion data to enhance motion detail and smoothness.\n\n    - *Reinforcement Learning*: Utilizes Reinforcement Learning from human feedback and reward models to further refine instruction-following and motion naturalness.\n\n\n\n<p align=\"center\">\n  <img src=\"./assets/pipeline.png\" alt=\"System Overview\" width=\"100%\">\n</p>\n\n<p align=\"center\">\n  <img src=\"./assets/arch.png\" alt=\"Architecture\" width=\"100%\">\n</p>\n\n<p align=\"center\">\n  <img src=\"./assets/sotacomp.jpg\" alt=\"ComparisonSoTA\" width=\"100%\">\n</p>\n\n\n\n\n## ğŸ Model Zoo\n\n**HY-Motion 1.0 Series**\n\n| Model | Description | Date | Size | Huggingface | VRAM (min) |\n|:-------|:-------------|:------:|:------:|:-------------:|:-------------:|\n| **HY-Motion-1.0** | Standard Text2Motion Model | 2025-12-30 | 1.0B | [Download](https://huggingface.co/tencent/HY-Motion-1.0/tree/main/HY-Motion-1.0) | 26GB |\n| **HY-Motion-1.0-Lite** | Lightweight Text2Motion Model | 2025-12-30 | 0.46B | [Download](https://huggingface.co/tencent/HY-Motion-1.0/tree/main/HY-Motion-1.0-Lite) | 24GB |\n\n*Note*: To reduce GPU VRAM requirements, please use the following settings: `--num_seeds=1`, text prompt with less than 30 words, and motion length less than 5 seconds.  \n*Note*: This table does not includes GPU VRAM requirements for LLM-based prompt engineering feature. If you have sufficient VRAM to run HY-Motion-1.0 model but gradio fails with a VRAM-related error, Run the Gradio application with prompt engineering disabled by setting the environment variable like this: `DISABLE_PROMPT_ENGINEERING=True python3 gradio_app.py`\n\n## ğŸ¤— Get Started with HY-Motion 1.0\n\nHY-Motion 1.0 supports macOS, Windows, and Linux.\n\n\n- [Code Usage (CLI)](#code-usage-cli)\n- [Gradio App](#gradio-app)\n\n\n#### 1. Installation\n\nFirst, install PyTorch via the [official site](https://pytorch.org/). Then install the dependencies:\n\n```bash\ngit clone https://github.com/Tencent-Hunyuan/HY-Motion-1.0.git\ncd HY-Motion-1.0/\n# Make sure git-lf",
      "default_branch": "master"
    },
    "fetched_at": "2026-01-04T03:05:10.405473"
  },
  {
    "basic_info": {
      "name": "NitroGen",
      "full_name": "MineDojo/NitroGen",
      "owner": "MineDojo",
      "description": "A Foundation Model for Generalist Gaming Agents",
      "url": "https://github.com/MineDojo/NitroGen",
      "clone_url": "https://github.com/MineDojo/NitroGen.git",
      "ssh_url": "git@github.com:MineDojo/NitroGen.git",
      "homepage": "",
      "created_at": "2025-12-17T17:03:47Z",
      "updated_at": "2026-01-04T02:25:36Z",
      "pushed_at": "2025-12-26T16:30:16Z"
    },
    "stats": {
      "stars": 1301,
      "forks": 153,
      "watchers": 1301,
      "open_issues": 17,
      "size": 27890
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 117031
      },
      "license": "Other",
      "topics": []
    },
    "content": {
      "readme": "<img src=\"assets/github_banner.gif\" width=\"100%\" />\n\n<div align=\"center\">\n  <p style=\"font-size: 1.2em;\">\n    <a href=\"https://nitrogen.minedojo.org/\"><strong>Website</strong></a> | \n    <a href=\"https://huggingface.co/nvidia/NitroGen\"><strong>Model</strong></a> |\n    <a href=\"https://huggingface.co/datasets/nvidia/NitroGen\"><strong>Dataset</strong></a> |\n    <a href=\"https://nitrogen.minedojo.org/assets/documents/nitrogen.pdf\"><strong>Paper</strong></a>\n  </p>\n</div>\n\n\n# NitroGen\n\nNitroGen is an open foundation model for generalist gaming agents. This multi-game model takes pixel input and predicts gamepad actions.\n\nNitroGen is trained through behavior cloning on the largest video-action gameplay dataset, assembled exclusively from internet videos. It can be adapted via post-training to unseen games.\n\n# Installation\n\n## Prerequisites\n\nWe **do not distribute game environments**, you must use your own copies of the games. This repository only supports running the agent on **Windows games**. You can serve the model from a Linux machine for inference, but the game ultimately has to run on Windows. We have tested on Windows 11 with Python â‰¥ 3.12.\n\n## Setup\n\nInstall this repo:\n```bash\ngit clone https://github.com/MineDojo/NitroGen.git\ncd NitroGen\npip install -e .\n```\n\nDownload NitroGen checkpoint from [HuggingFace](https://huggingface.co/nvidia/NitroGen):\n```bash\nhf download nvidia/NitroGen ng.pt\n```\n\n# Getting Started\n\nFirst, start an inference server for the model:\n```bash\npython scripts/serve.py <path_to_ng.pt>  \n```\n\nThen, run the agent on the game of your choice:\n```bash\npython scripts/play.py --process '<game_executable_name>.exe'\n```\n\nThe `--process` parameter must be the exact executable name of the game you want to play. You can find it by right-clicking on the game process in Windows Task Manager (Ctrl+Shift+Esc), and selecting `Properties`. The process name should be in the `General` tab and end with `.exe`.\n\n# Paper and Citation\n\nIf you find our work useful, please consider citing us!\n\n```bibtex\n@misc{Magne2025NitroGen,\n  title        = {NitroGen: An Open Foundation Model for Generalist Gaming Agents},\n  author       = {Magne, Lo{\\\"\\i}c and Awadalla, Anas and Wang, Guanzhi and Xu, Yinzhen and Belofsky, Joshua and Hu, Fengyuan and Kim, Joohwan and Schmidt, Ludwig and Gkioxari, Georgia and Kautz, Jan and Yue, Yisong and Choi, Yejin and Zhu, Yuke and Fan, Linxi},\n  year         = {2025},\n  howpublished = {\\url{https://nitrogen.minedojo.org/}},\n}\n```\n\n**Disclaimer**: This project is strictly for research purposes and is not an official NVIDIA product.\n",
      "default_branch": "main"
    },
    "fetched_at": "2026-01-04T03:05:11.593802"
  },
  {
    "basic_info": {
      "name": "android-action-kernel",
      "full_name": "Action-State-Labs/android-action-kernel",
      "owner": "Action-State-Labs",
      "description": null,
      "url": "https://github.com/Action-State-Labs/android-action-kernel",
      "clone_url": "https://github.com/Action-State-Labs/android-action-kernel.git",
      "ssh_url": "git@github.com:Action-State-Labs/android-action-kernel.git",
      "homepage": null,
      "created_at": "2025-12-11T11:07:23Z",
      "updated_at": "2026-01-04T02:51:20Z",
      "pushed_at": "2026-01-03T00:56:45Z"
    },
    "stats": {
      "stars": 1278,
      "forks": 166,
      "watchers": 1278,
      "open_issues": 6,
      "size": 56
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 6679
      },
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "# Android Use\n\n<div align=\"center\">\n\n# [Join Discord Community](https://discord.gg/HWaZByt5SA)\n\n<h1>AI Agents for Android Devices</h1>\n\n<h3>Open-source library for AI agents to control native Android apps</h3>\n\n**Built for field workers, logistics, gig economy, and mobile-first industries**\n\n<br>\n\n[![Twitter](https://img.shields.io/badge/5.3M+-views-1DA1F2?style=for-the-badge&logo=x&logoColor=white)](https://x.com/ethanjlim/status/1999152070428148108?s=20)\n[![Stars](https://img.shields.io/github/stars/actionstatelabs/android-action-kernel?style=for-the-badge)](https://github.com/actionstatelabs/android-action-kernel/stargazers)\n[![License](https://img.shields.io/badge/license-MIT-green?style=for-the-badge)](LICENSE)\n[![Python](https://img.shields.io/badge/python-3.10+-blue?style=for-the-badge&logo=python&logoColor=white)](https://www.python.org/)\n\n<br>\n\n### Demo\n\n**[Watch it automate a logistics workflow in 60 seconds](https://x.com/ethanjlim/status/1999152070428148108?s=20)**\n\n<sub>Driver texts a photo â†’ Agent handles WhatsApp â†’ Scanner app â†’ Banking app â†’ Invoice submitted</sub>\n\n<br>\n\n**[â­ Star this repo (1100+ â†’ 1,500 goal!)](https://github.com/actionstatelabs/android-action-kernel) â€¢ [Quick Start](#quick-start) â€¢ [Book Partnership Meeting](https://build.fillout.com/editor/ctqhgaBkaKus/share)**\n\n**5.3M+ views. 1100+ stars in days. Help us reach 1,500!** â€¢ **Priority partnerships:** Mobile QA testing â€¢ Consumer Productivity â€¢ [Request meeting â†’](https://build.fillout.com/editor/ctqhgaBkaKus/share)\n\n</div>\n\n---\n\n## The Problem\n\nBrowser agents only work on websites. Computer Use requires a desktop.\n\nBut the real economy runs on mobile devices, in places where laptops don't fit:\n\n- **Truck drivers** submit invoices from the cab using factoring apps (RTS Pro, OTR Capital)\n- **Delivery drivers** scan packages on handheld devicesâ€”200+ per route\n- **Gig workers** accept orders on phones between ridesâ€”losing 20% earnings to slow manual switching\n- **Field technicians** log work orders on tablets at job sites\n- **Mobile banking** happens on native apps with 2FA, not web browsers\n\n**3 billion Android devices. $40 trillion in GDP from mobile-first workflows. Zero AI agent solutions that actually work on these devices.**\n\n---\n\n## Real Example: Logistics Automation\n\n**Priority partnership area.** Android Use automating an entire logistics workflow:\n\n### Before (Manual - 10+ minutes)\n```\n1. Driver takes photo of Bill of Lading\n2. Opens WhatsApp, sends to back office\n3. Back office downloads image\n4. Opens banking app, fills invoice form\n5. Uploads documents\n6. Submits for payment\n```\n\n### After (Automated - 30 seconds)\n```python\n# Driver just texts the photo. Agent does the rest.\nrun_agent(\"\"\"\n1. Get latest image from WhatsApp\n2. Open native scanner app and process it\n3. Switch to RTS Pro factoring app\n4. Fill invoice form with extracted data\n5. Upload PDF and submit for payment\n\"\"\")\n```\n\n**Result:** Driver gets paid same day instead of waiting weeks. Back-office work eliminated. No laptop needed.\n\n---\n\n## Why This Works\n\n<table>\n<tr>\n<td width=\"50%\">\n\n### Computer Use (Anthropic)\n- Requires desktop/laptop\n- Takes screenshots â†’ OCR\n- Sends images to vision model\n- **$0.15 per action**\n- 3-5 second latency\n- Doesn't work on phones\n\n</td>\n<td width=\"50%\">\n\n### Android Use (This Library)\n- Works on handheld devices\n- Reads accessibility tree (XML)\n- Structured data â†’ LLM\n- **$0.01 per action (95% cheaper)**\n- <1 second latency\n- Native mobile app control\n\n</td>\n</tr>\n</table>\n\n**The breakthrough:** Android's accessibility API provides structured UI data (buttons, text, coordinates) without expensive vision models.\n\n**Real impact:** 95% cost savings + 5x faster + works where laptops can't.\n\n---\n\n## Traction\n\nLaunched with the logistics demo:\n\n- **5.3M+ views** on X/Twitter ([watch demo](https://x.com/ethanjlim/status/1999152070428148108?s=20))\n- **1100+ GitHub stars** (from 12 stars at launch - help us reach 1,500!)\n- **150+ inbound messages** from logistics companies, gig platforms, field service providers  \n- **5 active pilot programs** with trucking companies and delivery fleets\n- **3 factoring companies** exploring partnership integrations\n- Validated product-market fit within first 24 hours\n\n**Star growth shows real demand.** Help us reach 1,500 stars â†’ **[Star this repo now](https://github.com/actionstatelabs/android-action-kernel/stargazers)**\n\n**Current priority partnerships:**\n- **Trucking/logistics companies** - Factoring app automation, invoice processing, driver workflows\n- **QA testing teams** - Automated mobile app testing at scale\n\nDue to overwhelming demand, we created a meeting scheduler. **[Request a partnership meeting â†’](https://build.fillout.com/editor/ctqhgaBkaKus/share)**\n\n---\n\n## The Market: Mobile-First Industries\n\n| Industry | Why They Need This | Market Size | Current State |\n|----------|-------------------|-------------|---------------|\n| **Logistics** | Drivers use factoring apps (RTS Pro, OTR",
      "default_branch": "main"
    },
    "fetched_at": "2026-01-04T03:05:12.816462"
  },
  {
    "basic_info": {
      "name": "Qwen-Image-Layered",
      "full_name": "QwenLM/Qwen-Image-Layered",
      "owner": "QwenLM",
      "description": "Qwen-Image-Layered: Layered Decomposition for Inherent Editablity",
      "url": "https://github.com/QwenLM/Qwen-Image-Layered",
      "clone_url": "https://github.com/QwenLM/Qwen-Image-Layered.git",
      "ssh_url": "git@github.com:QwenLM/Qwen-Image-Layered.git",
      "homepage": null,
      "created_at": "2025-12-18T08:54:50Z",
      "updated_at": "2026-01-04T02:35:44Z",
      "pushed_at": "2025-12-31T11:40:35Z"
    },
    "stats": {
      "stars": 1207,
      "forks": 96,
      "watchers": 1207,
      "open_issues": 14,
      "size": 23145
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 14946
      },
      "license": "Apache License 2.0",
      "topics": []
    },
    "content": {
      "readme": "<p align=\"center\">\n    <img src=\"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/layered/qwen-image-layered-logo.png\" width=\"800\"/>\n<p> \n<p align=\"center\">&nbsp&nbspğŸ¤— <a href=\"https://huggingface.co/Qwen/Qwen-Image-Layered\">HuggingFace</a>&nbsp&nbsp | &nbsp&nbspğŸ¤– <a href=\"https://modelscope.cn/models/Qwen/Qwen-Image-Layered\">ModelScope</a>&nbsp&nbsp | &nbsp&nbsp ğŸ“‘ <a href=\"https://arxiv.org/abs/2512.15603\">Research Paper</a> &nbsp&nbsp | &nbsp&nbsp ğŸ“‘ <a href=\"https://qwen.ai/blog?id=qwen-image-layered\">Blog</a> &nbsp&nbsp | &nbsp&nbsp ğŸ¤— <a href=\"https://huggingface.co/spaces/Qwen/Qwen-Image-Layered\">Demo</a> &nbsp&nbsp \n</p>\n\n<p align=\"center\">\n    <img src=\"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/layered/layered.JPG\" width=\"1024\"/>\n<p>\n\n## Introduction\nWe are excited to introduce **Qwen-Image-Layered**, a model capable of decomposing an image into multiple RGBA layers. This layered representation unlocks **inherent editability**: each layer can be independently manipulated without affecting other content. Meanwhile, such a layered representation naturally supports **high-fidelity elementary operations**-such as resizing, reposition, and recoloring. By physically isolating semantic or structural components into distinct layers, our approach enables high-fidelity and consistent editing.\n\n\n[![Qwen Image Layered](https://img.youtube.com/vi/OVhmiBrsziQ/0.jpg)](https://www.youtube.com/watch?v=OVhmiBrsziQ)\n\n\n## News\n- 2025.12.22: You can try Qwen-Image-Layered on [Huggingface Spaces](https://huggingface.co/spaces/Qwen/Qwen-Image-Layered) and [Modelscope Studio](https://modelscope.cn/studios/Qwen/Qwen-Image-Layered).\n- 2025.12.19: We released Qwen-Image-Layered weights! Check at [Huggingface](https://huggingface.co/Qwen/Qwen-Image-Layered) and [ModelScope](https://modelscope.cn/models/Qwen/Qwen-Image-Layered)!\n- 2025.12.19: We released Qwen-Image-Layered! Check our [Blog](https://qwen.ai/blog?id=qwen-image-layered) for more details!\n- 2025.12.18: We released our [Research Paper](https://arxiv.org/abs/2512.15603) on Arxiv!\n\n> [!NOTE]\n> - The text prompt is intended to describe the overall content of the input imageâ€”including elements that may be partially occluded (e.g., you may specify the text hidden behind a foreground object). It is not designed to control the semantic content of individual layers explicitly.\n> - The released weights are specifically fine-tuned for the image-to-multi-RGBA decomposition task. As a result, while the model supports text-conditioned inference, its performance on text-to-multi-RGBA generation is limited.\n\n## Quick Start\n\n1. Make sure your transformers>=4.51.3 (Supporting Qwen2.5-VL)\n\n2. Install the latest version of diffusers\n```\npip install git+https://github.com/huggingface/diffusers\npip install python-pptx\npip install psd-tools\n```\n\n\n```python\nfrom diffusers import QwenImageLayeredPipeline\nimport torch\nfrom PIL import Image\n\npipeline = QwenImageLayeredPipeline.from_pretrained(\"Qwen/Qwen-Image-Layered\")\npipeline = pipeline.to(\"cuda\", torch.bfloat16)\npipeline.set_progress_bar_config(disable=None)\n\nimage = Image.open(\"asserts/test_images/1.png\").convert(\"RGBA\")\ninputs = {\n    \"image\": image,\n    \"generator\": torch.Generator(device='cuda').manual_seed(777),\n    \"true_cfg_scale\": 4.0,\n    \"negative_prompt\": \" \",\n    \"num_inference_steps\": 50,\n    \"num_images_per_prompt\": 1,\n    \"layers\": 4,\n    \"resolution\": 640,      # Using different bucket (640, 1024) to determine the resolution. For this version, 640 is recommended\n    \"cfg_normalize\": True,  # Whether enable cfg normalization.\n    \"use_en_prompt\": True,  # Automatic caption language if user does not provide caption\n}\n\nwith torch.inference_mode():\n    output = pipeline(**inputs)\n    output_image = output.images[0]\n\nfor i, image in enumerate(output_image):\n    image.save(f\"{i}.png\")\n```\n\n## Deploy Qwen-Image-Layered\nThe following scripts will start a Gradio-based web interface where you can decompose an image and export the layers into pptx, zip, and psd files, where you can edit and move these layers flexibly.\n```bash\npython src/app.py\n```\n\nAfter decomposition, you may want to edit specific layers. The following scripts will launch a Gradio-based web interface where you can edit images with transparency using Qwen-Image-Edit.\n```bash\npython src/tool/edit_rgba_image.py\n```\n\nAfter editing the individual decomposed layers, you can use the following script to combine them into a new image. Remember to upload the layers in orderâ€”from the bottom layer to the top.\n```bash\npython src/tool/combine_layers.py\n```\n\n### vLLM-Omni\n[vLLM-Omni](https://github.com/vllm-project/vllm-omni) now supports Qwen-Image-Layered. See the [recipes](https://docs.vllm.ai/projects/recipes/en/latest/Qwen/Qwen-Image.html) for up-to-date details.\n\n## Showcase\n### Layered Decomposition in Application\nGiven an image, Qwen-Image-Layered can decompose it into several RGBA layers:\n![Example Image](https://qianwen-res.oss-cn-beijing.aliyunc",
      "default_branch": "main"
    },
    "fetched_at": "2026-01-04T03:05:14.012336"
  },
  {
    "basic_info": {
      "name": "IQuest-Coder-V1",
      "full_name": "IQuestLab/IQuest-Coder-V1",
      "owner": "IQuestLab",
      "description": null,
      "url": "https://github.com/IQuestLab/IQuest-Coder-V1",
      "clone_url": "https://github.com/IQuestLab/IQuest-Coder-V1.git",
      "ssh_url": "git@github.com:IQuestLab/IQuest-Coder-V1.git",
      "homepage": null,
      "created_at": "2025-12-31T06:42:57Z",
      "updated_at": "2026-01-04T02:53:40Z",
      "pushed_at": "2026-01-03T17:47:18Z"
    },
    "stats": {
      "stars": 900,
      "forks": 50,
      "watchers": 900,
      "open_issues": 12,
      "size": 28395
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 706343,
        "Shell": 47957
      },
      "license": "Other",
      "topics": []
    },
    "content": {
      "readme": "![Evaluation Results](./papers/iquest-coder-v1-logo.png)\n\n<p align=\"center\">\n  ğŸ“˜ <a href=\"https://iquestlab.github.io\">Blog</a >\n  &nbsp;â€¢&nbsp;\n  ğŸ“„ <a href=\"https://github.com/IQuestLab/IQuest-Coder-V1/blob/main/papers/IQuest_Coder_Technical_Report.pdf\">Technical Report</a >\n</p >\n\n# IQuest-Coder-V1 Model Family\n\n| Model | Link |\n|-------|------|\n| IQuest-Coder-V1-40B-Base-Stage1 | [ğŸ¤— Hugging Face](https://huggingface.co/IQuestLab/IQuest-Coder-V1-40B-Base-Stage1) |\n| IQuest-Coder-V1-40B-Base | [ğŸ¤— Hugging Face](https://huggingface.co/IQuestLab/IQuest-Coder-V1-40B-Base) |\n| IQuest-Coder-V1-40B-Instruct | [ğŸ¤— Hugging Face](https://huggingface.co/IQuestLab/IQuest-Coder-V1-40B-Instruct) |\n| IQuest-Coder-V1-40B-Loop-Instruct | [ğŸ¤— Hugging Face](https://huggingface.co/IQuestLab/IQuest-Coder-V1-40B-Loop-Instruct) |\n\n## Sampling Parameters:\nFor the IQuest-Coder-V1-Instruct: We suggest using Temperature=0.6, TopP=0.85, TopK=20.\n\n## IQuest-Coder-V1 Highlights\n\nIQuest-Coder-V1 is a new family of code large language models (LLMs) designed to advance autonomous software engineering and code intelligence. Built on the innovative code-flow multi-stage training paradigm, IQuest-Coder-V1 captures the dynamic evolution of software logic, delivering state-of-the-art performance across critical dimensions:\n\n- **State-of-the-Art Performance**: Achieves leading results on SWE-Bench Verified (76.2%), BigCodeBench (49.9%), LiveCodeBench v6 (81.1%), and other major coding benchmarks, surpassing competitive models across agentic software engineering, competitive programming, and complex tool use.\n- **Code-Flow Training Paradigm**: Moving beyond static code representations, our models learn from repository evolution patterns, commit transitions, and dynamic code transformations to understand real-world software development processes.\n- **Dual Specialization Paths**: Bifurcated post-training delivers two specialized variantsâ€”Thinking models (utilizing reasoning-driven RL for complex problem-solving) and Instruct models (optimized for general coding assistance and instruction-following).\n- **Efficient Architecture**: The IQuest-Coder-V1-Loop variant introduces a recurrent mechanism that optimizes the trade-off between model capacity and deployment footprint.\n- **Native Long Context**: All models natively support up to 128K tokens without requiring additional scaling techniques.\n\n## Model Overview\n\nThe IQuest-Coder-V1 series includes models ranging from 7B to 40B parameters, with both standard and Loop variants:\n\n| Model | Parameters | Layers | Hidden Size | Attention Heads (Q/KV) | Context Length |\n|-------|------------|--------|-------------|------------------------|----------------|\n| IQuest-Coder-V1-7B-Instruct | 7B | 14 | 5120 | 40/8 | 128K |\n| IQuest-Coder-V1-7B-Thinking | 7B | 14 | 5120 | 40/8 | 128K |\n| IQuest-Coder-V1-14B-Instruct | 14B | 28 | 5120 | 40/8 | 128K |\n| IQuest-Coder-V1-14B-Thinking | 14B | 28 | 5120 | 40/8 | 128K |\n| IQuest-Coder-V1-40B-Instruct | 40B | 80 | 5120 | 40/8 | 128K |\n| IQuest-Coder-V1-40B-Thinking | 40B | 80 | 5120 | 40/8 | 128K |\n| IQuest-Coder-V1-40B-Loop-Instruct | 40B | 80 (2 iterations) | 5120 | 40/8 | 128K |\n| IQuest-Coder-V1-40B-Loop-Thinking | 40B | 80 (2 iterations) | 5120 | 40/8 | 128K |\n\n**Architecture Features:**\n\n- Grouped Query Attention (GQA) for efficient inference\n- Native 128K context length support\n- Vocabulary size: 76,800 tokens\n- Loop variants use recurrent transformer design with shared parameters across two iterations\n\nFor more details, please refer to our Technical Report, GitHub.\n\n## Quickstart\n\nIQuest-Coder-V1 uses custom modeling code via Hugging Face's auto_map feature. We recommend using transformers>=4.52.4.\n\n### Basic Usage with Transformers\n\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_name = \"IQuest/IQuest-Coder-V1-40B-Instruct\"\n\n# Load the tokenizer and model\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype=\"auto\",\n    device_map=\"auto\"\n)\n\n# Prepare the input\nprompt = \"Write a Python function to calculate the Fibonacci sequence using dynamic programming.\"\nmessages = [\n    {\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n\n# Generate response\ngenerated_ids = model.generate(\n    **model_inputs,\n    max_new_tokens=8192\n)\ngenerated_ids = generated_ids[0][len(model_inputs.input_ids[0]):]\nresponse = tokenizer.decode(generated_ids, skip_special_tokens=True)\n\nprint(response)\n```\n\n### Using Thinking Models\n\nFor complex reasoning tasks, use the Thinking variant:\n\n```python\nmodel_name = \"IQuestLab/IQuest-Coder-V1-40B-Thinking\"\n\n# The Thinking model includes explicit reasoning traces\n# Use similar code as above, but expect longer, more detailed responses\n# with step-by-step problem decomposition\n```\n\n### Dep",
      "default_branch": "main"
    },
    "fetched_at": "2026-01-04T03:05:15.198852"
  },
  {
    "basic_info": {
      "name": "claude-workflow-v2",
      "full_name": "CloudAI-X/claude-workflow-v2",
      "owner": "CloudAI-X",
      "description": "Universal Claude Code workflow plugin with agents, skills, hooks, and commands",
      "url": "https://github.com/CloudAI-X/claude-workflow-v2",
      "clone_url": "https://github.com/CloudAI-X/claude-workflow-v2.git",
      "ssh_url": "git@github.com:CloudAI-X/claude-workflow-v2.git",
      "homepage": null,
      "created_at": "2026-01-01T02:20:41Z",
      "updated_at": "2026-01-04T02:49:56Z",
      "pushed_at": "2026-01-03T10:37:20Z"
    },
    "stats": {
      "stars": 872,
      "forks": 134,
      "watchers": 872,
      "open_issues": 1,
      "size": 92
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 11267,
        "Shell": 3568
      },
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "# project-starter\n\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n[![Claude Code](https://img.shields.io/badge/Claude%20Code-v1.0.33+-blue.svg)](https://code.claude.com)\n[![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg)](https://github.com/CloudAI-X/claude-workflow/pulls)\n\nA universal Claude Code workflow plugin with specialized agents, skills, hooks, and output styles for any software project.\n\n---\n\n## Quick Start\n\n### Option 1: CLI (Per-Session)\n\n```bash\n# Clone the plugin\ngit clone https://github.com/CloudAI-X/claude-workflow.git\n\n# Run Claude Code with the plugin\nclaude --plugin-dir ./claude-workflow\n```\n\n### Option 2: Agent SDK\n\n```typescript\nimport { query } from \"@anthropic-ai/claude-agent-sdk\";\n\nfor await (const message of query({\n  prompt: \"Hello\",\n  options: {\n    plugins: [{ type: \"local\", path: \"./claude-workflow\" }]\n  }\n})) {\n  // Plugin commands, agents, and skills are now available\n}\n```\n\n### Option 3: Install Permanently\n\n```bash\n# Install from marketplace (when available)\nclaude plugin install project-starter\n\n# Or install from local directory\nclaude plugin install ./claude-workflow\n```\n\n### Verify Installation\n\nAfter loading the plugin, verify it's working:\n\n```\n> /plugin\n```\n\nTab to **Installed** - you should see `project-starter` listed.\nTab to **Errors** - should be empty (no errors).\n\nThese commands become available:\n\n```\n/project-starter:architect    # Architecture-first mode\n/project-starter:rapid        # Ship fast mode\n/project-starter:commit       # Auto-generate commit message\n/project-starter:verify-changes  # Multi-agent verification\n```\n\n---\n\n## What's Included\n\n| Component | Count | Description |\n|-----------|-------|-------------|\n| **Agents** | 7 | Specialized subagents for code review, debugging, security, etc. |\n| **Commands** | 17 | Slash commands for workflows and output styles |\n| **Skills** | 6 | Knowledge domains Claude uses autonomously |\n| **Hooks** | 8 | Automation scripts for formatting, security, notifications |\n\n---\n\n## Usage Examples\n\n### Commands in Action\n\n**Auto-commit your changes:**\n```\n> /project-starter:commit\n\nLooking at staged changes...\nâœ“ Created commit: feat(auth): add JWT refresh token endpoint\n```\n\n**Full git workflow:**\n```\n> /project-starter:commit-push-pr\n\nâœ“ Committed: feat: add user dashboard\nâœ“ Pushed to origin/feature/dashboard\nâœ“ Created PR #42: https://github.com/you/repo/pull/42\n```\n\n**Verify before shipping:**\n```\n> /project-starter:verify-changes\n\nSpawning verification agents...\nâ”œâ”€ build-validator: âœ“ Build passes\nâ”œâ”€ test-runner: âœ“ 42 tests pass\nâ”œâ”€ lint-checker: âš  2 warnings (non-blocking)\nâ””â”€ security-scanner: âœ“ No vulnerabilities\n\nReady to ship!\n```\n\n### Agents in Action\n\nAgents spawn automatically based on your request:\n\n**You say:** \"The login is broken, users get 401 errors\"\n```\n[debugger agent activated]\nâ†’ Checking auth middleware... found issue\nâ†’ Token validation uses wrong secret in production\nâ†’ Fix: Update AUTH_SECRET in .env.production\n```\n\n**You say:** \"Review my changes\"\n```\n[code-reviewer agent activated]\nâ†’ Analyzing 3 files changed...\nâœ“ Logic is correct\nâš  Missing null check on line 42\nâš  Consider adding rate limiting to this endpoint\n```\n\n**You say:** \"Add authentication to the API\"\n```\n[orchestrator agent activated]\nâ†’ Breaking down into subtasks:\n  1. Design auth schema (spawning architect)\n  2. Implement JWT middleware\n  3. Add login/register endpoints\n  4. Write tests (spawning test-architect)\n  5. Update API docs (spawning docs-writer)\n```\n\n### Skills in Action\n\nSkills provide domain knowledge automatically:\n\n**You ask:** \"How should I structure the payment service?\"\n```\n[designing-architecture skill applied]\nâ†’ Recommending hexagonal architecture\nâ†’ Payment providers as adapters\nâ†’ Core domain isolated from infrastructure\n```\n\n**You ask:** \"Make this endpoint faster\"\n```\n[optimizing-performance skill applied]\nâ†’ Adding database indexes\nâ†’ Implementing response caching\nâ†’ Using pagination for large results\n```\n\n### Hooks in Action\n\nHooks run automatically on events:\n\n**Security block (pre-edit):**\n```\nâ›” BLOCKED: Potential secret detected\n   File: src/config.ts, Line 5\n   Pattern: API key (sk-...)\n\n   Remove the secret and use environment variables.\n```\n\n**Auto-format (post-edit):**\n```\nâœ“ Formatted with prettier: src/components/Button.tsx\nâœ“ Formatted with black: scripts/deploy.py\n```\n\n**Desktop notifications:**\n```\nğŸ”” \"Claude needs input\" - when waiting for your response\nğŸ”” \"Task complete\" - when finished\n```\n\n---\n\n## Commands Reference\n\nAll commands use the format `/project-starter:<command>`.\n\n### Output Styles\n\n| Command | Mode |\n|---------|------|\n| `/project-starter:architect` | System design mode - architecture before code |\n| `/project-starter:rapid` | Fast development - ship quickly, iterate |\n| `/project-starter:mentor` | Teaching mode - explain the \"why\" |\n| `/project-starter:review` | Code review mode - strict quality |\n\n### Git Workflow ",
      "default_branch": "main"
    },
    "fetched_at": "2026-01-04T03:05:16.370295"
  },
  {
    "basic_info": {
      "name": "ppt-master",
      "full_name": "hugohe3/ppt-master",
      "owner": "hugohe3",
      "description": "AI é©±åŠ¨çš„ SVG æ¼”ç¤ºæ–‡ç¨¿ç”Ÿæˆç³»ç»Ÿï¼Œæ”¯æŒ PPTã€å°çº¢ä¹¦ã€æœ‹å‹åœˆç­‰å¤šæ ¼å¼ | 15 ä¸ªç¤ºä¾‹ | 229 é¡µ | ç”Ÿæˆå¯ç¼–è¾‘çš„ ppt æ ¼å¼",
      "url": "https://github.com/hugohe3/ppt-master",
      "clone_url": "https://github.com/hugohe3/ppt-master.git",
      "ssh_url": "git@github.com:hugohe3/ppt-master.git",
      "homepage": "https://hugohe3.github.io/ppt-master/",
      "created_at": "2025-12-10T06:54:33Z",
      "updated_at": "2026-01-04T01:50:32Z",
      "pushed_at": "2025-12-31T03:06:51Z"
    },
    "stats": {
      "stars": 863,
      "forks": 133,
      "watchers": 863,
      "open_issues": 0,
      "size": 73328
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 248045,
        "HTML": 83967
      },
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "# PPT Master - AI é©±åŠ¨çš„å¤šæ ¼å¼ SVG å†…å®¹ç”Ÿæˆç³»ç»Ÿ\n\n[![Version](https://img.shields.io/badge/version-v1.0.0-blue.svg)](./VERSION)\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n[![GitHub stars](https://img.shields.io/github/stars/hugohe3/ppt-master.svg)](https://github.com/hugohe3/ppt-master/stargazers)\n\n[English](./README_EN.md) | ä¸­æ–‡\n\nä¸€ä¸ªåŸºäº AI çš„æ™ºèƒ½è§†è§‰å†…å®¹ç”Ÿæˆç³»ç»Ÿï¼Œé€šè¿‡å¤šè§’è‰²åä½œï¼Œå°†æºæ–‡æ¡£è½¬åŒ–ä¸ºé«˜è´¨é‡çš„ SVG å†…å®¹ï¼Œ**æ”¯æŒæ¼”ç¤ºæ–‡ç¨¿ã€ç¤¾äº¤åª’ä½“ã€è¥é”€æµ·æŠ¥ç­‰å¤šç§æ ¼å¼**ã€‚\n\n> ğŸ´ **åœ¨çº¿ç¤ºä¾‹**ï¼š[GitHub Pages åœ¨çº¿é¢„è§ˆ](https://hugohe3.github.io/ppt-master/) - æŸ¥çœ‹å®é™…ç”Ÿæˆæ•ˆæœ\n\n> ğŸ¬ **å¿«é€Ÿç¤ºä¾‹**ï¼š[YouTube](https://www.youtube.com/watch?v=jM2fHmvMwx0) | [Bilibili](https://www.bilibili.com/video/BV1iUmQBtEGH/) - è§‚çœ‹è§†é¢‘æ¼”ç¤º\n\n---\n\n## ğŸš€ å¿«é€Ÿä½¿ç”¨æŒ‡å—\n\n### æ¨èå·¥å…·\n\n| å·¥å…· | æ¨èåº¦ | è¯´æ˜ |\n|------|:------:|------|\n| **[Antigravity](https://antigravity.dev/)** | â­â­â­ | **å¼ºçƒˆæ¨è**ï¼å…è´¹ä½¿ç”¨ Opus 4.5ï¼Œé›†æˆ Banana ç”Ÿå›¾åŠŸèƒ½ï¼Œå¯ç›´æ¥åœ¨ä»“åº“é‡Œç”Ÿæˆé…å›¾ |\n| [Cursor](https://cursor.sh/) | â­â­ | ä¸»æµ AI ç¼–è¾‘å™¨ï¼Œæ”¯æŒå¤šç§æ¨¡å‹ |\n| [VS Code + Copilot](https://code.visualstudio.com/) | â­â­ | å¾®è½¯å®˜æ–¹æ–¹æ¡ˆ |\n| [Claude Code](https://claude.ai/) | â­â­ | Anthropic å®˜æ–¹ CLI å·¥å…· |\n\n> ğŸ’¡ **AI ç”Ÿæˆå›¾ç‰‡å»ºè®®**ï¼šå¦‚éœ€ AI ç”Ÿæˆé…å›¾ï¼Œå»ºè®®åœ¨ [Gemini](https://gemini.google.com/) ä¸­ç”Ÿæˆåé€‰æ‹© **Download full size** ä¸‹è½½ï¼Œåˆ†è¾¨ç‡æ¯” Antigravity ç›´æ¥ç”Ÿæˆçš„æ›´é«˜ã€‚Gemini ç”Ÿæˆçš„å›¾ç‰‡å³ä¸‹è§’ä¼šæœ‰æ˜Ÿæ˜Ÿæ°´å°ï¼Œå¯ä½¿ç”¨ [gemini-watermark-remover](https://github.com/journey-ad/gemini-watermark-remover) æˆ–æœ¬é¡¹ç›®çš„ `tools/gemini_watermark_remover.py` å»é™¤ã€‚\n\n### ä¸‰æ­¥å¼€å§‹\n\n```\n1ï¸âƒ£ å…‹éš†ä»“åº“\n   æ‰“å¼€ç¼–è¾‘å™¨ â†’ Clone Repository â†’ è¾“å…¥æœ¬ä»“åº“åœ°å€\n   git clone https://github.com/hugohe3/ppt-master.git\n\n2ï¸âƒ£ æ‰“å¼€èŠå¤©çª—å£\n   åœ¨ç¼–è¾‘å™¨ä¸­æ‰“å¼€ AI èŠå¤©é¢æ¿ï¼ˆAntigravity/Cursor/Copilot Chatï¼‰\n\n3ï¸âƒ£ å¼€å§‹å¯¹è¯\n   ç›´æ¥å‘Šè¯‰ AI ä½ æƒ³åˆ›å»ºä»€ä¹ˆå†…å®¹ï¼ŒAI ä¼šè‡ªåŠ¨æŒ‰ç…§ä»“åº“ä¸­çš„è§’è‰²å®šä¹‰å·¥ä½œ\n```\n\n### ä½¿ç”¨ç¤ºä¾‹\n\n```\nç”¨æˆ·ï¼šæˆ‘æœ‰ä¸€ä»½å…³äº Q3 å­£åº¦ä¸šç»©çš„æŠ¥å‘Šï¼Œéœ€è¦åˆ¶ä½œæˆ PPT\n\nAIï¼ˆStrategist è§’è‰²ï¼‰ï¼šå¥½çš„ï¼Œåœ¨å¼€å§‹ä¹‹å‰æˆ‘éœ€è¦å®Œæˆå…«é¡¹ç¡®è®¤...\n   1. ç”»å¸ƒæ ¼å¼ï¼š[å»ºè®®] PPT 16:9\n   2. é¡µæ•°èŒƒå›´ï¼š[å»ºè®®] 8-10 é¡µ\n   ...\n```\n\nğŸ’¡ **æ¨¡å‹æ¨è**ï¼šOpus 4.5 æ•ˆæœæœ€ä½³ï¼ŒAntigravity ç›®å‰å¯å…è´¹ä½¿ç”¨\n\n---\n\n## ğŸ´ ç²¾é€‰ç¤ºä¾‹\n\n> ğŸ“ **ç¤ºä¾‹åº“**: [`examples/`](./examples/) Â· **15 ä¸ªé¡¹ç›®** Â· **229 é¡µ SVG**\n\n### ç¤ºä¾‹é¡¹ç›®æ€»è§ˆ\n\n| ç±»åˆ« | é¡¹ç›® | é¡µæ•° | ç‰¹è‰² |\n|------|------|:----:|------|\n| ğŸ¢ **å’¨è¯¢é£æ ¼** | [å¿ƒç†æ²»ç–—ä¸­çš„ä¾æ‹](./examples/ppt169_é¡¶çº§å’¨è¯¢é£_å¿ƒç†æ²»ç–—ä¸­çš„ä¾æ‹/) | 32 | é¡¶çº§å’¨è¯¢é£æ ¼ï¼Œæœ€å¤§è§„æ¨¡ç¤ºä¾‹ |\n| | [æ„å»ºæœ‰æ•ˆAIä»£ç†](./examples/ppt169_é¡¶çº§å’¨è¯¢é£_æ„å»ºæœ‰æ•ˆAIä»£ç†_Anthropic/) | 15 | Anthropic å·¥ç¨‹åšå®¢ï¼ŒAI Agent æ¶æ„ |\n| | [é‡åº†å¸‚åŒºåŸŸæŠ¥å‘Š](./examples/ppt169_é¡¶çº§å’¨è¯¢é£_é‡åº†å¸‚åŒºåŸŸæŠ¥å‘Š_ppt169_20251213/) | 20 | åŒºåŸŸè´¢æ”¿åˆ†æï¼Œä¼ä¸šé¢„è­¦é€šæ•°æ® ğŸ†• |\n| | [ç”˜å­œå·ç»æµè´¢æ”¿åˆ†æ](./examples/ppt169_é¡¶çº§å’¨è¯¢é£_ç”˜å­œå·ç»æµè´¢æ”¿åˆ†æ/) | 17 | æ”¿åŠ¡è´¢æ”¿åˆ†æï¼Œè—åŒºæ–‡åŒ–å…ƒç´  |\n| | [å—æ¬§æ±Ÿæ°´ç”µç«™æˆ˜ç•¥è¯„ä¼°](./examples/ppt169_é«˜ç«¯å’¨è¯¢é£_å—æ¬§æ±Ÿæ°´ç”µç«™æˆ˜ç•¥è¯„ä¼°/) | 20 | \"æµåŸŸå±æœº\"è®¾è®¡è¯­è¨€ |\n| | [æ±½è½¦è®¤è¯äº”å¹´æˆ˜ç•¥è§„åˆ’](./examples/ppt169_é«˜ç«¯å’¨è¯¢é£_æ±½è½¦è®¤è¯äº”å¹´æˆ˜ç•¥è§„åˆ’/) | 20 | McKinsey/BCG é£æ ¼ |\n| | [éº¦è‚¯é”¡é£å®¢æˆ·å¿ è¯šåº¦](./examples/ppt169_éº¦è‚¯é”¡é£_kimsoong_customer_loyalty/) | 8 | éº¦è‚¯é”¡ç»å…¸ MECE åŸåˆ™ |\n| | [Google å¹´åº¦å·¥ä½œæ±‡æŠ¥](./examples/ppt169_è°·æ­Œé£_google_annual_report/) | 10 | Google å“ç‰Œè®¾è®¡è¯­è¨€ |\n| ğŸ¨ **é€šç”¨çµæ´»** | [Debug å…­æ­¥æ³•](./examples/ppt169_é€šç”¨çµæ´»+ä»£ç _debugå…­æ­¥æ³•/) | 10 | æ·±è‰²ç§‘æŠ€é£æ ¼ |\n| | [é‡åº†å¤§å­¦è®ºæ–‡æ ¼å¼](./examples/ppt169_é€šç”¨çµæ´»+å­¦æœ¯_é‡åº†å¤§å­¦è®ºæ–‡æ ¼å¼æ ‡å‡†/) | 11 | å­¦æœ¯è§„èŒƒæŒ‡å— |\n| | [AI ç¼–ç¨‹å·¥å…·å¯¹æ¯”](./examples/ppt169_é€šè¿‡çµæ´»+ä»£ç _ä¸‰å¤§AIç¼–ç¨‹ç¥å™¨æ¨ªå‘å¯¹æ¯”/) | 11 | æŠ€æœ¯è¯„æµ‹é£æ ¼ |\n| âœ¨ **åˆ›æ„é£æ ¼** | [åœ°å±±è°¦å¦æ·±åº¦ç ”ç©¶](./examples/ppt169_æ˜“ç†é£_åœ°å±±è°¦å¦æ·±åº¦ç ”ç©¶/) | 20 | æ˜“ç»æœ¬ä½“ç¾å­¦ï¼Œé˜´é˜³çˆ»å˜è®¾è®¡ |\n| | [é‡‘åˆšç»ç¬¬ä¸€å“ç ”ç©¶](./examples/ppt169_ç¦…æ„é£_é‡‘åˆšç»ç¬¬ä¸€å“ç ”ç©¶/) | 15 | ç¦…æ„å­¦æœ¯ï¼Œæ°´å¢¨ç•™ç™½ |\n| | [Git å…¥é—¨æŒ‡å—](./examples/ppt169_åƒç´ é£_git_introduction/) | 10 | åƒç´ å¤å¤æ¸¸æˆé£ |\n| | [PPT Master ä»‹ç»](./examples/demo_project_intro_ppt169_20251211/) | 10 | æ¸…æ–°ç§‘æŠ€é£æ ¼ |\n\nğŸ“– [æŸ¥çœ‹å®Œæ•´ç¤ºä¾‹æ–‡æ¡£](./examples/README.md)\n\n### ä»£è¡¨ä½œå“å±•ç¤º\n\n#### é¡¶çº§å’¨è¯¢é£æ ¼ Â· å¿ƒç†æ²»ç–—ä¸­çš„ä¾æ‹ï¼ˆ32 é¡µï¼‰\n\n> æœ€å¤§è§„æ¨¡ç¤ºä¾‹é¡¹ç›®ï¼Œ\"å®‰å…¨åŸºåœ°\"è§†è§‰éšå–»\n\nğŸ“ [æŸ¥çœ‹é¡¹ç›®](./examples/ppt169_é¡¶çº§å’¨è¯¢é£_å¿ƒç†æ²»ç–—ä¸­çš„ä¾æ‹/) Â· ğŸ“„ [è®¾è®¡è§„èŒƒ](./examples/ppt169_é¡¶çº§å’¨è¯¢é£_å¿ƒç†æ²»ç–—ä¸­çš„ä¾æ‹/è®¾è®¡è§„èŒƒä¸å†…å®¹å¤§çº².md)\n\n#### æ˜“ç†ç„å­¦é£æ ¼ Â· åœ°å±±è°¦å¦æ·±åº¦ç ”ç©¶ï¼ˆ20 é¡µï¼‰\n\n> é˜´é˜³çˆ»å˜è®¾è®¡è¯­è¨€ï¼Œå…­çˆ»å±‚è¿›ç»“æ„\n\nğŸ“ [æŸ¥çœ‹é¡¹ç›®](./examples/ppt169_æ˜“ç†é£_åœ°å±±è°¦å¦æ·±åº¦ç ”ç©¶/) Â· ğŸ“„ [è®¾è®¡è§„èŒƒ](./examples/ppt169_æ˜“ç†é£_åœ°å±±è°¦å¦æ·±åº¦ç ”ç©¶/è®¾è®¡è§„èŒƒä¸å†…å®¹å¤§çº².md)\n\n#### åƒç´ å¤å¤é£æ ¼ Â· Git å…¥é—¨æŒ‡å—ï¼ˆ10 é¡µï¼‰\n\n> éœ“è™¹è‰²ç³»ï¼Œ\"å­˜æ¡£ç‚¹\"éšå–»ç‰ˆæœ¬æ§åˆ¶\n\nğŸ“ [æŸ¥çœ‹é¡¹ç›®](./examples/ppt169_åƒç´ é£_git_introduction/) Â· ğŸ“„ [è®¾è®¡è§„èŒƒ](./examples/ppt169_åƒç´ é£_git_introduction/è®¾è®¡è§„èŒƒä¸å†…å®¹å¤§çº².md)\n\n---\n\n<details>\n<summary><b>ğŸ“‹ ç›®å½•ï¼ˆç‚¹å‡»å±•å¼€ï¼‰</b></summary>\n\n| ç« èŠ‚ | é“¾æ¥ |\n|------|------|\n| ğŸš€ å¿«é€Ÿä½¿ç”¨æŒ‡å— | [è·³è½¬](#-å¿«é€Ÿä½¿ç”¨æŒ‡å—) |\n| ğŸ´ ç²¾é€‰ç¤ºä¾‹ | [è·³è½¬](#-ç²¾é€‰ç¤ºä¾‹) |\n| é¡¹ç›®ç®€ä»‹ | [è·³è½¬](#é¡¹ç›®ç®€ä»‹) |\n| æ ¸å¿ƒç‰¹æ€§ | [è·³è½¬](#æ ¸å¿ƒç‰¹æ€§) |\n| ç³»ç»Ÿæ¶æ„ | [è·³è½¬](#ç³»ç»Ÿæ¶æ„) |\n| æ ¸å¿ƒè§’è‰² | [è·³è½¬](#æ ¸å¿ƒè§’è‰²) |\n| å¿«é€Ÿå¼€å§‹ | [è·³è½¬](#å¿«é€Ÿå¼€å§‹) |\n| æ›´å¤šç¤ºä¾‹ | [è·³è½¬](#æ›´å¤šç¤ºä¾‹) |\n| è®¾è®¡é£æ ¼ | [è·³è½¬](#è®¾è®¡é£æ ¼) |\n| æŠ€æœ¯è§„èŒƒ | [è·³è½¬](#æŠ€æœ¯è§„èŒƒ) |\n| é¡¹ç›®ç»“æ„ | [è·³è½¬](#é¡¹ç›®ç»“æ„) |\n| æœ€ä½³å®è·µ | [è·³è½¬](#æœ€ä½³å®è·µ) |\n| å¸¸è§é—®é¢˜ | [è·³è½¬](#å¸¸è§é—®é¢˜) |\n| è´¡çŒ®æŒ‡å— | [è·³è½¬](#è´¡çŒ®æŒ‡å—) |\n| è·¯çº¿å›¾ | [è·³è½¬](#è·¯çº¿å›¾) |\n| ğŸ› ï¸ å·¥å…·é›† | [è·³è½¬](#ï¸-å·¥å…·é›†) |\n| ğŸ“„ å¼€æºåè®® | [è·³è½¬](#-å¼€æºåè®®) |\n| ğŸ™ è‡´è°¢ | [è·³è½¬](#-è‡´è°¢) |\n| ğŸ“® è”ç³»æ–¹å¼ | [è·³è½¬](#-è”ç³»æ–¹å¼) |\n\n</details>\n\n## ğŸ“š æ–‡æ¡£å¯¼èˆª\n\n- ğŸš€ **æ–°æ‰‹å…¥é—¨**: é˜…è¯»æœ¬ README\n- ğŸ“– **è¯¦ç»†æ•™ç¨‹**: [å·¥ä½œæµæ•™ç¨‹](./docs/workflow_tutorial.md)\n- ğŸ¨ **è®¾è®¡æŒ‡å—**: [è®¾è®¡è§„èŒƒè¯¦è§£](./docs/design_guidelines.md)\n- ğŸ“ **ç”»å¸ƒæ ¼å¼**: [æ”¯æŒçš„æ‰€æœ‰æ ¼å¼](./docs/canvas_formats.md)\n- ğŸ–¼ï¸ **å›¾ç‰‡åµŒå…¥**: [SVG å›¾ç‰‡åµŒå…¥æŒ‡å—](./docs/svg_image_embedding.md)\n- ğŸ“Š **å›¾è¡¨æ¨¡æ¿**: [æ ‡å‡†åŒ–å›¾è¡¨æ¨¡æ¿åº“](./templates/charts/) - 13ç§å¸¸ç”¨å›¾è¡¨ Â· [åœ¨çº¿é¢„è§ˆ](./templates/charts/preview.html)\n- âš¡ **å¿«é€ŸæŸ¥é˜…**: [å¿«é€Ÿå‚è€ƒæŒ‡å—](./docs/quick_reference.md)\n- ğŸ”§ **è§’è‰²å®šä¹‰**: [æŸ¥çœ‹æ‰€æœ‰è§’è‰²](./roles/README.md)\n- ğŸ› ï¸ **å·¥å…·é›†**: [å·¥å…·ä½¿ç”¨æŒ‡å—](./tools/README.md)\n- ğŸ’¼ **ç¤ºä¾‹ç´¢å¼•**: [æŸ¥çœ‹æ‰€æœ‰ç¤ºä¾‹](./examples/README.md)\n\n## é¡¹ç›®ç®€ä»‹\n\nPPT Master æ˜¯ä¸€ä¸ªåˆ›æ–°çš„ AI è¾…åŠ©è§†è§‰å†…å®¹åˆ›ä½œç³»ç»Ÿï¼Œé€šè¿‡å¤šè§’è‰²åä½œï¼ˆç­–ç•¥å¸ˆã€å›¾ç‰‡ç”Ÿæˆå¸ˆã€æ‰§è¡Œå¸ˆã€ä¼˜åŒ–å¸ˆï¼‰ï¼Œå®ç°ä»å†…å®¹ç­–åˆ’åˆ°è§†è§‰ä¼˜åŒ–çš„å®Œæ•´å·¥ä½œæµã€‚ç³»ç»Ÿä¸ä»…æ”¯æŒç”Ÿæˆç¬¦åˆé¡¶å°–å’¨è¯¢å…¬å¸ï¼ˆå¦‚éº¦è‚¯é”¡ã€æ³¢å£«é¡¿å’¨è¯¢ï¼‰æ ‡å‡†çš„å•†ä¸šæ¼”ç¤ºæ–‡ç¨¿ï¼Œè¿˜æ”¯æŒå°çº¢ä¹¦å¸–å­ã€æœ‹å‹åœˆæµ·æŠ¥ã€Instagram ç­‰å¤šç§ç¤¾äº¤åª’ä½“å’Œè¥é”€ç‰©æ–™æ ¼å¼ã€‚\n\n## æ ¸å¿ƒç‰¹æ€§\n\nâœ¨ **æ™ºèƒ½å†…å®¹è§£æ„** - è‡ªåŠ¨åˆ†ææºæ–‡æ¡£å¹¶é‡ç»„ä¸ºæ¸…æ™°çš„é¡µé¢åºåˆ—\nğŸ¨ **ä¸‰é‡è®¾è®¡é£æ ¼** - æ”¯æŒ\"é€šç”¨çµæ´»\"ã€\"ä¸€èˆ¬å’¨è¯¢\"å’Œ\"é¡¶çº§å’¨è¯¢ï¼ˆMBB çº§ï¼‰\"\nğŸ“ **å¤šæ ¼å¼æ”¯æŒ** - æ¼”ç¤ºæ–‡ç¨¿ (16:9/4:3)ã€å°çº¢ä¹¦ (3:4)ã€æœ‹",
      "default_branch": "main"
    },
    "fetched_at": "2026-01-04T03:05:17.607417"
  }
]