## ğŸ“ çƒ­ç‚¹é¡¹ç›®-20251012-python

<div style="background-color: #f5f5f5; padding: 15px; border-radius: 8px; margin: 20px 0;">
- ğŸ¤– æœ¬æŠ¥å‘ŠåŸºäº GitHub API è‡ªåŠ¨ç”Ÿæˆ
- ğŸ“… æ•°æ®æ›´æ–°æ—¶é—´: 2025-10-12 10:22:45 (ä¸Šæµ·æ—¶åŒº)
- ğŸŒŸ æ˜Ÿæ ‡æ•°ç­‰ç»Ÿè®¡ä¿¡æ¯ä¸ºç”Ÿæˆæ—¶çš„å®æ—¶æ•°æ®
- ğŸ“š é¡¹ç›®ä¿¡æ¯æ¥æºäºå„é¡¹ç›®çš„ README æ–‡æ¡£
- ğŸ’¡ çƒ­åº¦æŒ‡æ•°è®¡ç®—æ–¹å¼: æ˜Ÿæ ‡æ•° + Forkæ•° Ã— 0.5
- æœ¬æŠ¥å‘Šç”± ä¸‰å­å¶å¼€æº github-trendingé¡¹ç›®åˆ†æå·¥å…·è‡ªåŠ¨ç”Ÿæˆ
</div>

## ğŸ”— ç›¸å…³é“¾æ¥

- [GitHub API æ–‡æ¡£](https://docs.github.com/en/rest)
- [é¡¹ç›®æ•°æ®è·å–å™¨æºç ](https://github.com/3ziye/github-trending)

---

# çƒ­ç‚¹é¡¹ç›®-20251012-python

<div align="center">
ğŸ“Š <strong>ç”Ÿæˆæ—¶é—´</strong>: 2025å¹´10æœˆ12æ—¥ 10:22  â€¢  
ğŸ¯ <strong>é¡¹ç›®æ•°é‡</strong>: 20 ä¸ª  â€¢  
â±ï¸ <strong>çƒ­åº¦æ—¶é—´</strong>: æœˆæ¦œ  â€¢  
ğŸ”¥ <strong>æ•°æ®æ¥æº</strong>: GitHub API
</div>

---

## ğŸš€ çƒ­é—¨é¡¹ç›®è¯¦æƒ…

### 1. TinyRecursiveModels

**ğŸ”¥ çƒ­åº¦æŒ‡æ•°**: 3,758

**é¡¹ç›®åç§°**: [SamsungSAILMontreal/TinyRecursiveModels](https://github.com/SamsungSAILMontreal/TinyRecursiveModels)
**é¡¹ç›®æè¿°**: æš‚æ— æè¿°
**ä¸»è¦è¯­è¨€**: Python
**è®¸å¯è¯**: MIT License
**â­ æ˜Ÿæ ‡æ•°**: 3,554
**ğŸ´ Fork æ•°**: 409
**ğŸ‘€ å…³æ³¨æ•°**: 3,554
**ğŸ› å¼€æ”¾é—®é¢˜**: 10
**æœ€åæ›´æ–°**: 2025-10-12 (21åˆ†é’Ÿå‰)
**ğŸ“– é¡¹ç›®æ–‡æ¡£**: [README](3ziye-20251012-python/1-TinyRecursiveModels-Readme.md)

**ğŸ  é¡¹ç›®ä¸»é¡µ**: æ— 

**ğŸ“‚ å…‹éš†åœ°å€**: `https://github.com/SamsungSAILMontreal/TinyRecursiveModels.git`

**ğŸ’» æŠ€æœ¯æ ˆ**: **ä¸»è¦è¯­è¨€**: Python

#### ğŸ¯ æ ¸å¿ƒç‰¹æ€§
- Python 3.10 (or similar)
- Cuda 12.6.0 (or similar)

#### ğŸ¨ é€‚ç”¨åœºæ™¯
- æ•°æ®ç§‘å­¦ä¸åˆ†æ
- æœºå™¨å­¦ä¹ é¡¹ç›®
- Web åº”ç”¨å¼€å‘

---

### 2. bdh

**ğŸ”¥ çƒ­åº¦æŒ‡æ•°**: 3,065

**é¡¹ç›®åç§°**: [pathwaycom/bdh](https://github.com/pathwaycom/bdh)
**é¡¹ç›®æè¿°**: Baby Dragon Hatchling (BDH) â€“ Architecture and Code
**ä¸»è¦è¯­è¨€**: Python
**è®¸å¯è¯**: MIT License
**â­ æ˜Ÿæ ‡æ•°**: 3,017
**ğŸ´ Fork æ•°**: 97
**ğŸ‘€ å…³æ³¨æ•°**: 3,017
**ğŸ› å¼€æ”¾é—®é¢˜**: 2
**æœ€åæ›´æ–°**: 2025-10-12 (2å°æ—¶å‰)
**ğŸ“– é¡¹ç›®æ–‡æ¡£**: [README](3ziye-20251012-python/2-bdh-Readme.md)

**ğŸ  é¡¹ç›®ä¸»é¡µ**: æ— 

**ğŸ“‚ å…‹éš†åœ°å€**: `https://github.com/pathwaycom/bdh.git`

**ğŸ’» æŠ€æœ¯æ ˆ**: **ä¸»è¦è¯­è¨€**: Python

#### ğŸ¨ é€‚ç”¨åœºæ™¯
- æ•°æ®ç§‘å­¦ä¸åˆ†æ
- æœºå™¨å­¦ä¹ é¡¹ç›®
- Web åº”ç”¨å¼€å‘

---

### 3. neutts-air

**ğŸ”¥ çƒ­åº¦æŒ‡æ•°**: 2,991

**é¡¹ç›®åç§°**: [neuphonic/neutts-air](https://github.com/neuphonic/neutts-air)
**é¡¹ç›®æè¿°**: On-device TTS model by Neuphonic
**ä¸»è¦è¯­è¨€**: Python
**è®¸å¯è¯**: Apache License 2.0
**â­ æ˜Ÿæ ‡æ•°**: 2,873
**ğŸ´ Fork æ•°**: 236
**ğŸ‘€ å…³æ³¨æ•°**: 2,873
**ğŸ› å¼€æ”¾é—®é¢˜**: 22
**æœ€åæ›´æ–°**: 2025-10-12 (5åˆ†é’Ÿå‰)
**ğŸ“– é¡¹ç›®æ–‡æ¡£**: [README](3ziye-20251012-python/3-neutts-air-Readme.md)

**ğŸ  é¡¹ç›®ä¸»é¡µ**: æ— 

**ğŸ“‚ å…‹éš†åœ°å€**: `https://github.com/neuphonic/neutts-air.git`

**ğŸ’» æŠ€æœ¯æ ˆ**: **ä¸»è¦è¯­è¨€**: Python

#### ğŸ¯ æ ¸å¿ƒç‰¹æ€§
- ğŸ—£Best-in-class realism for its size - produces natural, ultra-realistic voices that sound human
- ğŸ“±Optimised for on-device deployment - provided in GGML format, ready to run on phones, laptops, or even Raspberry Pis
- ğŸ‘«Instant voice cloning - create your own speaker with as little as 3 seconds of audio
- ğŸš„Simple LM + codec architecture built off a 0.5B backbone - the sweet spot between speed, size, and quality for real-world applications
**Supported Languages**: English
**Audio Codec**: [NeuCodec](https://huggingface.co/neuphonic/neucodec) - our 50hz neural audio codec that achieves exceptional audio quality at low bitrates using a single codebook
**Context Window**: 2048 tokens, enough for processing ~30 seconds of audio (including prompt duration)
**Format**: Available in GGML format for efficient on-device inference

#### ğŸ¨ é€‚ç”¨åœºæ™¯
- æ•°æ®ç§‘å­¦ä¸åˆ†æ
- æœºå™¨å­¦ä¹ é¡¹ç›®
- Web åº”ç”¨å¼€å‘

---

### 4. HunyuanImage-3.0

**ğŸ”¥ çƒ­åº¦æŒ‡æ•°**: 2,139

**é¡¹ç›®åç§°**: [Tencent-Hunyuan/HunyuanImage-3.0](https://github.com/Tencent-Hunyuan/HunyuanImage-3.0)
**é¡¹ç›®æè¿°**: HunyuanImage-3.0: A Powerful Native Multimodal Model for Image Generation
**ä¸»è¦è¯­è¨€**: Python
**è®¸å¯è¯**: Other
**â­ æ˜Ÿæ ‡æ•°**: 2,100
**ğŸ´ Fork æ•°**: 79
**ğŸ‘€ å…³æ³¨æ•°**: 2,100
**ğŸ› å¼€æ”¾é—®é¢˜**: 23
**æœ€åæ›´æ–°**: 2025-10-12 (21åˆ†é’Ÿå‰)
**ğŸ“– é¡¹ç›®æ–‡æ¡£**: [README](3ziye-20251012-python/4-HunyuanImage-3.0-Readme.md)

**ğŸ  é¡¹ç›®ä¸»é¡µ**: https://hunyuan.tencent.com/image

**ğŸ“‚ å…‹éš†åœ°å€**: `https://github.com/Tencent-Hunyuan/HunyuanImage-3.0.git`

**ğŸ’» æŠ€æœ¯æ ˆ**: **ä¸»è¦è¯­è¨€**: Python

#### ğŸ·ï¸ é¡¹ç›®æ ‡ç­¾
- `image-generation`
- `native-multimodal-model`

#### ğŸ¯ æ ¸å¿ƒç‰¹æ€§
**September 28, 2025**: ğŸ“– **HunyuanImage-3.0 Technical Report Released** - Comprehensive technical documentation now available
**September 28, 2025**: ğŸš€ **HunyuanImage-3.0 Open Source Release** - Inference code and model weights publicly available
- HunyuanImage-3.0 (Image Generation Model)
- [ğŸ”¥ğŸ”¥ğŸ”¥ News](#-news)
- [ğŸ§© Community Contributions](#-community-contributions)
- [ğŸ“‘ Open-source Plan](#-open-source-plan)
- [ğŸ“– Introduction](#-introduction)
- [âœ¨ Key Features](#-key-features)

#### ğŸ¨ é€‚ç”¨åœºæ™¯
- æ•°æ®ç§‘å­¦ä¸åˆ†æ
- æœºå™¨å­¦ä¹ é¡¹ç›®
- Web åº”ç”¨å¼€å‘

---

### 5. VoxCPM

**ğŸ”¥ çƒ­åº¦æŒ‡æ•°**: 1,785

**é¡¹ç›®åç§°**: [OpenBMB/VoxCPM](https://github.com/OpenBMB/VoxCPM)
**é¡¹ç›®æè¿°**: VoxCPM: Tokenizer-Free TTS for Context-Aware Speech Generation and True-to-Life Voice Cloning
**ä¸»è¦è¯­è¨€**: Python
**è®¸å¯è¯**: Apache License 2.0
**â­ æ˜Ÿæ ‡æ•°**: 1,698
**ğŸ´ Fork æ•°**: 175
**ğŸ‘€ å…³æ³¨æ•°**: 1,698
**ğŸ› å¼€æ”¾é—®é¢˜**: 23
**æœ€åæ›´æ–°**: 2025-10-12 (12åˆ†é’Ÿå‰)
**ğŸ“– é¡¹ç›®æ–‡æ¡£**: [README](3ziye-20251012-python/5-VoxCPM-Readme.md)

**ğŸ  é¡¹ç›®ä¸»é¡µ**: æ— 

**ğŸ“‚ å…‹éš†åœ°å€**: `https://github.com/OpenBMB/VoxCPM.git`

**ğŸ’» æŠ€æœ¯æ ˆ**: **ä¸»è¦è¯­è¨€**: Python

#### ğŸ¯ æ ¸å¿ƒç‰¹æ€§
- [2025.09.30] ğŸ”¥ ğŸ”¥ ğŸ”¥  We Release VoxCPM [Technical Report](https://arxiv.org/abs/2509.24650)!
- [2025.09.16] ğŸ”¥ ğŸ”¥ ğŸ”¥  We Open Source the VoxCPM-0.5B [weights](https://huggingface.co/openbmb/VoxCPM-0.5B)!
- [2025.09.16] ğŸ‰ ğŸ‰ ğŸ‰  We Provide the [Gradio PlayGround](https://huggingface.co/spaces/OpenBMB/VoxCPM-Demo) for VoxCPM-0.5B, try it now!
**High-Efficiency Synthesis** - VoxCPM supports streaming synthesis with a Real-Time Factor

#### ğŸ¨ é€‚ç”¨åœºæ™¯
- æ•°æ®ç§‘å­¦ä¸åˆ†æ
- æœºå™¨å­¦ä¹ é¡¹ç›®
- Web åº”ç”¨å¼€å‘

---

### 6. VLA-Adapter

**ğŸ”¥ çƒ­åº¦æŒ‡æ•°**: 995

**é¡¹ç›®åç§°**: [OpenHelix-Team/VLA-Adapter](https://github.com/OpenHelix-Team/VLA-Adapter)
**é¡¹ç›®æè¿°**: VLA-Adapter: An Effective Paradigm for Tiny-Scale Vision-Language-Action Model
**ä¸»è¦è¯­è¨€**: Python
**è®¸å¯è¯**: MIT License
**â­ æ˜Ÿæ ‡æ•°**: 954
**ğŸ´ Fork æ•°**: 83
**ğŸ‘€ å…³æ³¨æ•°**: 954
**ğŸ› å¼€æ”¾é—®é¢˜**: 5
**æœ€åæ›´æ–°**: 2025-10-12 (26åˆ†é’Ÿå‰)
**ğŸ“– é¡¹ç›®æ–‡æ¡£**: [README](3ziye-20251012-python/6-VLA-Adapter-Readme.md)

**ğŸ  é¡¹ç›®ä¸»é¡µ**: https://vla-adapter.github.io/

**ğŸ“‚ å…‹éš†åœ°å€**: `https://github.com/OpenHelix-Team/VLA-Adapter.git`

**ğŸ’» æŠ€æœ¯æ ˆ**: **ä¸»è¦è¯­è¨€**: Python

#### ğŸ·ï¸ é¡¹ç›®æ ‡ç­¾
- `embodied-ai`
- `robotics`
- `vision-language-action-model`

#### ğŸ¯ æ ¸å¿ƒç‰¹æ€§
**[2025/09/22]** We released our codes! An enhanced **Pro** version is also released (this version conforms to the pipeline in the original paper, but is optimized in implementation). Everyone is welcome to use it!ğŸ‰
**[2025/09/12]** We released the original version of the VLA-Adapter for four LIBERO models on [HuggingFace](https://huggingface.co/VLA-Adapter).
**[2025/09/11]** We released our paper on [ArXiv](https://arxiv.org/abs/2509.09372).
- [x]  Release **checkpoints** for reproduction.
- [x]  Release [VLA-Adapter v2 paper](https://arxiv.org/abs/2509.09372).
- [ ]  A more **powerful version**, **VLA-Adapter++**, and a detailed **technical report** ğŸ“ will be released soon.<br/>
- [ ]  Continue to update the code to adapt to various **real-world systems** deployments, including the configuration of our paper, Franka, UR-5, and AGILE Piper.<br/>

#### ğŸ¨ é€‚ç”¨åœºæ™¯
- æ•°æ®ç§‘å­¦ä¸åˆ†æ
- æœºå™¨å­¦ä¹ é¡¹ç›®
- Web åº”ç”¨å¼€å‘

---

### 7. DeepSeek-V3.2-Exp

**ğŸ”¥ çƒ­åº¦æŒ‡æ•°**: 895

**é¡¹ç›®åç§°**: [deepseek-ai/DeepSeek-V3.2-Exp](https://github.com/deepseek-ai/DeepSeek-V3.2-Exp)
**é¡¹ç›®æè¿°**: æš‚æ— æè¿°
**ä¸»è¦è¯­è¨€**: Python
**è®¸å¯è¯**: MIT License
**â­ æ˜Ÿæ ‡æ•°**: 871
**ğŸ´ Fork æ•°**: 49
**ğŸ‘€ å…³æ³¨æ•°**: 871
**ğŸ› å¼€æ”¾é—®é¢˜**: 10
**æœ€åæ›´æ–°**: 2025-10-12 (46åˆ†é’Ÿå‰)
**ğŸ“– é¡¹ç›®æ–‡æ¡£**: [README](3ziye-20251012-python/7-DeepSeek-V3.2-Exp-Readme.md)

**ğŸ  é¡¹ç›®ä¸»é¡µ**: æ— 

**ğŸ“‚ å…‹éš†åœ°å€**: `https://github.com/deepseek-ai/DeepSeek-V3.2-Exp.git`

**ğŸ’» æŠ€æœ¯æ ˆ**: **ä¸»è¦è¯­è¨€**: Python

#### ğŸ¯ æ ¸å¿ƒç‰¹æ€§
- DeepSeek Sparse Attention (

#### ğŸ¨ é€‚ç”¨åœºæ™¯
- æ•°æ®ç§‘å­¦ä¸åˆ†æ
- æœºå™¨å­¦ä¹ é¡¹ç›®
- Web åº”ç”¨å¼€å‘

---

### 8. ml-simplefold

**ğŸ”¥ çƒ­åº¦æŒ‡æ•°**: 878

**é¡¹ç›®åç§°**: [apple/ml-simplefold](https://github.com/apple/ml-simplefold)
**é¡¹ç›®æè¿°**: æš‚æ— æè¿°
**ä¸»è¦è¯­è¨€**: Python
**è®¸å¯è¯**: MIT License
**â­ æ˜Ÿæ ‡æ•°**: 851
**ğŸ´ Fork æ•°**: 54
**ğŸ‘€ å…³æ³¨æ•°**: 851
**ğŸ› å¼€æ”¾é—®é¢˜**: 17
**æœ€åæ›´æ–°**: 2025-10-12 (6å°æ—¶å‰)
**ğŸ“– é¡¹ç›®æ–‡æ¡£**: [README](3ziye-20251012-python/8-ml-simplefold-Readme.md)

**ğŸ  é¡¹ç›®ä¸»é¡µ**: æ— 

**ğŸ“‚ å…‹éš†åœ°å€**: `https://github.com/apple/ml-simplefold.git`

**ğŸ’» æŠ€æœ¯æ ˆ**: **ä¸»è¦è¯­è¨€**: Python

#### ğŸ¨ é€‚ç”¨åœºæ™¯
- æ•°æ®ç§‘å­¦ä¸åˆ†æ
- æœºå™¨å­¦ä¹ é¡¹ç›®
- Web åº”ç”¨å¼€å‘

---

### 9. MiMo-Audio

**ğŸ”¥ çƒ­åº¦æŒ‡æ•°**: 800

**é¡¹ç›®åç§°**: [XiaomiMiMo/MiMo-Audio](https://github.com/XiaomiMiMo/MiMo-Audio)
**é¡¹ç›®æè¿°**: MiMo-Audio: Audio Language Models are Few-Shot Learners
**ä¸»è¦è¯­è¨€**: Python
**è®¸å¯è¯**: Apache License 2.0
**â­ æ˜Ÿæ ‡æ•°**: 764
**ğŸ´ Fork æ•°**: 72
**ğŸ‘€ å…³æ³¨æ•°**: 764
**ğŸ› å¼€æ”¾é—®é¢˜**: 32
**æœ€åæ›´æ–°**: 2025-10-12 (9å°æ—¶å‰)
**ğŸ“– é¡¹ç›®æ–‡æ¡£**: [README](3ziye-20251012-python/9-MiMo-Audio-Readme.md)

**ğŸ  é¡¹ç›®ä¸»é¡µ**: https://xiaomimimo.github.io/MiMo-Audio-Demo/

**ğŸ“‚ å…‹éš†åœ°å€**: `https://github.com/XiaomiMiMo/MiMo-Audio.git`

**ğŸ’» æŠ€æœ¯æ ˆ**: **ä¸»è¦è¯­è¨€**: Python

#### ğŸ¨ é€‚ç”¨åœºæ™¯
- æ•°æ®ç§‘å­¦ä¸åˆ†æ
- æœºå™¨å­¦ä¹ é¡¹ç›®
- Web åº”ç”¨å¼€å‘

---

### 10. LuoGen-agent

**ğŸ”¥ çƒ­åº¦æŒ‡æ•°**: 710

**é¡¹ç›®åç§°**: [LuoGen-AI/LuoGen-agent](https://github.com/LuoGen-AI/LuoGen-agent)
**é¡¹ç›®æè¿°**: ä¸€é”®äº§å‡ºçˆ†æ¬¾è§†é¢‘ï¼š1.è‡ªåŠ¨æå–å¯¹æ ‡æ–‡æ¡ˆ 2.è‡ªåŠ¨è¿›è¡Œæ–‡æ¡ˆä»¿å†™ 3.è‡ªåŠ¨æ ¹æ®æ–‡æ¡ˆå£°éŸ³å…‹éš† 4.è‡ªåŠ¨ç”Ÿæˆæ•°å­—äººå£æ’­ 5.è‡ªåŠ¨æ·»åŠ å­—å¹• 6.è‡ªåŠ¨æ·»åŠ èƒŒæ™¯éŸ³ä¹ 7.è‡ªåŠ¨æ·»åŠ è§†é¢‘æ ‡é¢˜ 8.è‡ªåŠ¨ç”Ÿæˆè§†é¢‘å°é¢ 9.è‡ªåŠ¨å°†è§†é¢‘å‘å¸ƒåˆ°å„å¹³å°
**ä¸»è¦è¯­è¨€**: Python
**è®¸å¯è¯**: GNU General Public License v3.0
**â­ æ˜Ÿæ ‡æ•°**: 678
**ğŸ´ Fork æ•°**: 64
**ğŸ‘€ å…³æ³¨æ•°**: 678
**ğŸ› å¼€æ”¾é—®é¢˜**: 3
**æœ€åæ›´æ–°**: 2025-10-12 (5åˆ†é’Ÿå‰)
**ğŸ“– é¡¹ç›®æ–‡æ¡£**: [README](3ziye-20251012-python/10-LuoGen-agent-Readme.md)

**ğŸ  é¡¹ç›®ä¸»é¡µ**: æ— 

**ğŸ“‚ å…‹éš†åœ°å€**: `https://github.com/LuoGen-AI/LuoGen-agent.git`

**ğŸ’» æŠ€æœ¯æ ˆ**: **ä¸»è¦è¯­è¨€**: Python

#### ğŸ¯ æ ¸å¿ƒç‰¹æ€§
- ç”±äºä»£ç ä½“ç§¯åŠæ¨¡å‹æ–‡ä»¶è¿‡å¤§ï¼Œè¯·è¯¸ä½ç§»æ­¥ [ä»£ç åœ°å€](ä»£ç åœ°å€.txt) è¿›è¡Œä¸‹è½½ã€‚
- ç”±äºè¯¥åº”ç”¨ä¸ºæœ¬åœ°è¿è¡Œçš„å®¢æˆ·ç«¯åº”ç”¨ï¼Œä¸ºäº†è¯¸ä½çš„ä½¿ç”¨ä½“éªŒï¼Œå…ˆè¿›è¡Œ [ä½¿ç”¨å‰å¿…è£…](ä½¿ç”¨å‰å¿…è£….txt) è¿›è¡Œä¸‹è½½å®‰è£…ã€‚
- ğŸ“ **æ™ºèƒ½æ–‡æ¡ˆå¤„ç†**ï¼šè‡ªåŠ¨æå–å¯¹æ ‡æ–‡æ¡ˆ + æ™ºèƒ½ä»¿å†™ä¼˜åŒ–
- ğŸ¤ **å£°éŸ³å…‹éš†**ï¼šåŸºäº Whisper å’Œ CosyVoice å®ç°é«˜ä¿çœŸè¯­éŸ³åˆæˆ
- ğŸ‘¥ **æ•°å­—äººç”Ÿæˆ**ï¼šé›†æˆ HeyGem å®ç°è‡ªç„¶å£æ’­æ•ˆæœ
- ğŸ¬ **å…¨æµç¨‹è§†é¢‘åˆ¶ä½œ**ï¼šå­—å¹•/BGM/æ ‡é¢˜/å°é¢è‡ªåŠ¨ç”Ÿæˆ + å¤šå¹³å°å‘å¸ƒ
- [social-auto-upload](https://github.com/...) - å¤šå¹³å°å‘å¸ƒæ¡†æ¶
- [CosyVoice](https://github.com/tencent-ailab/cosyvoice) - é«˜è´¨é‡è¯­éŸ³åˆæˆ

#### ğŸ¨ é€‚ç”¨åœºæ™¯
- æ•°æ®ç§‘å­¦ä¸åˆ†æ
- æœºå™¨å­¦ä¹ é¡¹ç›®
- Web åº”ç”¨å¼€å‘

---

### 11. LongLive

**ğŸ”¥ çƒ­åº¦æŒ‡æ•°**: 673

**é¡¹ç›®åç§°**: [NVlabs/LongLive](https://github.com/NVlabs/LongLive)
**é¡¹ç›®æè¿°**: LongLive: Real-time Interactive Long Video Generation
**ä¸»è¦è¯­è¨€**: Python
**è®¸å¯è¯**: Other
**â­ æ˜Ÿæ ‡æ•°**: 656
**ğŸ´ Fork æ•°**: 35
**ğŸ‘€ å…³æ³¨æ•°**: 656
**ğŸ› å¼€æ”¾é—®é¢˜**: 6
**æœ€åæ›´æ–°**: 2025-10-12 (1å°æ—¶å‰)
**ğŸ“– é¡¹ç›®æ–‡æ¡£**: [README](3ziye-20251012-python/11-LongLive-Readme.md)

**ğŸ  é¡¹ç›®ä¸»é¡µ**: https://nvlabs.github.io/LongLive

**ğŸ“‚ å…‹éš†åœ°å€**: `https://github.com/NVlabs/LongLive.git`

**ğŸ’» æŠ€æœ¯æ ˆ**: **ä¸»è¦è¯­è¨€**: Python

#### ğŸ·ï¸ é¡¹ç›®æ ‡ç­¾
- `efficient-tuning`
- `interactive`
- `long-context`
- `real-time`
- `sparse-attention`
- `video-genenratio`

#### ğŸ¯ æ ¸å¿ƒç‰¹æ€§
1. **Long Video Gen**: LongLive supports up to 240s video generation, with visual consistency.
- 2. **Real-time Inference**: LongLive supports 20.7 FPS generation speed on a single H100 GPU, and 24.8 FPS with FP8 quantization with marginal quality loss.
- 3. **Efficient Fine-tuning**: LongLive extends a short-clip model to minute-long generation in 32 H100 GPU-days.

#### ğŸ¨ é€‚ç”¨åœºæ™¯
- æ•°æ®ç§‘å­¦ä¸åˆ†æ
- æœºå™¨å­¦ä¹ é¡¹ç›®
- Web åº”ç”¨å¼€å‘

---

### 12. DEIMv2

**ğŸ”¥ çƒ­åº¦æŒ‡æ•°**: 669

**é¡¹ç›®åç§°**: [Intellindust-AI-Lab/DEIMv2](https://github.com/Intellindust-AI-Lab/DEIMv2)
**é¡¹ç›®æè¿°**: [DEIMv2] Real Time Object Detection Meets DINOv3 
**ä¸»è¦è¯­è¨€**: Python
**è®¸å¯è¯**: Other
**â­ æ˜Ÿæ ‡æ•°**: 639
**ğŸ´ Fork æ•°**: 61
**ğŸ‘€ å…³æ³¨æ•°**: 639
**ğŸ› å¼€æ”¾é—®é¢˜**: 22
**æœ€åæ›´æ–°**: 2025-10-12 (1å°æ—¶å‰)
**ğŸ“– é¡¹ç›®æ–‡æ¡£**: [README](3ziye-20251012-python/12-DEIMv2-Readme.md)

**ğŸ  é¡¹ç›®ä¸»é¡µ**: https://intellindust-ai-lab.github.io/projects/DEIMv2/

**ğŸ“‚ å…‹éš†åœ°å€**: `https://github.com/Intellindust-AI-Lab/DEIMv2.git`

**ğŸ’» æŠ€æœ¯æ ˆ**: **ä¸»è¦è¯­è¨€**: Python

#### ğŸ·ï¸ é¡¹ç›®æ ‡ç­¾
- `detection-transformer`
- `dinov3`
- `object-detection`
- `real-time`
- `real-time-detection`

#### ğŸ¯ æ ¸å¿ƒç‰¹æ€§
- Equal Contribution &nbsp;&nbsp; â€  Corresponding Author
- [x] **\[2025.10.2\]** [DEIMv2 has been integrated into X-AnyLabeling!](https://github.com/Intellindust-AI-Lab/DEIMv2/issues/25#issue-3473960491) Many thanks to the X-AnyLabeling maintainers for making this possible.
- [x] **\[2025.9.26\]** Release DEIMv2 series.
- [1. ğŸ¤– Model Zoo](#1-model-zoo)
- [2. âš¡ Quick Start](#2-quick-start)
- [3. ğŸ› ï¸ Usage](#3-usage)
- [4. ğŸ§° Tools](#4-tools)
- [5. ğŸ“œ Citation](#5-citation)

#### ğŸ¨ é€‚ç”¨åœºæ™¯
- æ•°æ®ç§‘å­¦ä¸åˆ†æ
- æœºå™¨å­¦ä¹ é¡¹ç›®
- Web åº”ç”¨å¼€å‘

---

### 13. Code2Video

**ğŸ”¥ çƒ­åº¦æŒ‡æ•°**: 661

**é¡¹ç›®åç§°**: [showlab/Code2Video](https://github.com/showlab/Code2Video)
**é¡¹ç›®æè¿°**: Video generation via code
**ä¸»è¦è¯­è¨€**: Python
**è®¸å¯è¯**: MIT License
**â­ æ˜Ÿæ ‡æ•°**: 621
**ğŸ´ Fork æ•°**: 80
**ğŸ‘€ å…³æ³¨æ•°**: 621
**ğŸ› å¼€æ”¾é—®é¢˜**: 0
**æœ€åæ›´æ–°**: 2025-10-12 (22åˆ†é’Ÿå‰)
**ğŸ“– é¡¹ç›®æ–‡æ¡£**: [README](3ziye-20251012-python/13-Code2Video-Readme.md)

**ğŸ  é¡¹ç›®ä¸»é¡µ**: https://showlab.github.io/Code2Video/

**ğŸ“‚ å…‹éš†åœ°å€**: `https://github.com/showlab/Code2Video.git`

**ğŸ’» æŠ€æœ¯æ ˆ**: **ä¸»è¦è¯­è¨€**: Python

#### ğŸ·ï¸ é¡¹ç›®æ ‡ç­¾
- `coding`
- `education`
- `multi-agent`
- `video-generation`

#### ğŸ¨ é€‚ç”¨åœºæ™¯
- æ•°æ®ç§‘å­¦ä¸åˆ†æ
- æœºå™¨å­¦ä¹ é¡¹ç›®
- Web åº”ç”¨å¼€å‘

---

### 14. Qwen3-ASR-Toolkit

**ğŸ”¥ çƒ­åº¦æŒ‡æ•°**: 644

**é¡¹ç›®åç§°**: [QwenLM/Qwen3-ASR-Toolkit](https://github.com/QwenLM/Qwen3-ASR-Toolkit)
**é¡¹ç›®æè¿°**: Official Python toolkit for the Qwen3-ASR API. Parallel highâ€‘throughput calls, robust longâ€‘audio transcription, multiâ€‘sa...
**ä¸»è¦è¯­è¨€**: Python
**è®¸å¯è¯**: MIT License
**â­ æ˜Ÿæ ‡æ•°**: 618
**ğŸ´ Fork æ•°**: 52
**ğŸ‘€ å…³æ³¨æ•°**: 618
**ğŸ› å¼€æ”¾é—®é¢˜**: 8
**æœ€åæ›´æ–°**: 2025-10-12 (10å°æ—¶å‰)
**ğŸ“– é¡¹ç›®æ–‡æ¡£**: [README](3ziye-20251012-python/14-Qwen3-ASR-Toolkit-Readme.md)

**ğŸ  é¡¹ç›®ä¸»é¡µ**: æ— 

**ğŸ“‚ å…‹éš†åœ°å€**: `https://github.com/QwenLM/Qwen3-ASR-Toolkit.git`

**ğŸ’» æŠ€æœ¯æ ˆ**: **ä¸»è¦è¯­è¨€**: Python

#### ğŸ¯ æ ¸å¿ƒç‰¹æ€§
**Break the 3-Minute Limit**: Seamlessly transcribe audio and video files of any length by bypassing the official API's duration constraint.
**Smart Audio Splitting**: Utilizes **Voice Activity Detection (VAD)** to split audio into meaningful chunks at natural silent pauses. This ensures that words and sentences are not awkwardly cut off.
**High-Speed Parallel Processing**: Leverages multi-threading to send audio chunks to the Qwen-ASR API concurrently, dramatically reducing the total transcription time for long files.
**Intelligent Post-Processing**: Automatically detects and removes common ASR **hallucinations and repetitive artifacts** for cleaner, more accurate transcripts.
**SRT Subtitle Generation**: Automatically create timestamped **`.srt` subtitle files** based on VAD segments, perfect for adding captions to video content.
**Automatic Audio Resampling**: Automatically converts audio from any sample rate and channel count to the 16kHz mono format required by the Qwen-ASR API. You can use any audio file without worrying about pre-processing.
**Universal Media Support**: Supports virtually any audio and video format (e.g., `.mp4`, `.mov`, `.mkv`, `.mp3`, `.wav`, `.m4a`) thanks to its reliance on FFmpeg.
**Simple & Easy to Use**: A straightforward command-line interface allows you to get started with just a single command.

#### ğŸ¨ é€‚ç”¨åœºæ™¯
- æ•°æ®ç§‘å­¦ä¸åˆ†æ
- æœºå™¨å­¦ä¹ é¡¹ç›®
- Web åº”ç”¨å¼€å‘
- APIæœåŠ¡å¼€å‘

---

### 15. unifolm-world-model-action

**ğŸ”¥ çƒ­åº¦æŒ‡æ•°**: 623

**é¡¹ç›®åç§°**: [unitreerobotics/unifolm-world-model-action](https://github.com/unitreerobotics/unifolm-world-model-action)
**é¡¹ç›®æè¿°**: æš‚æ— æè¿°
**ä¸»è¦è¯­è¨€**: Python
**è®¸å¯è¯**: Other
**â­ æ˜Ÿæ ‡æ•°**: 599
**ğŸ´ Fork æ•°**: 48
**ğŸ‘€ å…³æ³¨æ•°**: 599
**ğŸ› å¼€æ”¾é—®é¢˜**: 6
**æœ€åæ›´æ–°**: 2025-10-12 (1å°æ—¶å‰)
**ğŸ“– é¡¹ç›®æ–‡æ¡£**: [README](3ziye-20251012-python/15-unifolm-world-model-action-Readme.md)

**ğŸ  é¡¹ç›®ä¸»é¡µ**: æ— 

**ğŸ“‚ å…‹éš†åœ°å€**: `https://github.com/unitreerobotics/unifolm-world-model-action.git`

**ğŸ’» æŠ€æœ¯æ ˆ**: **ä¸»è¦è¯­è¨€**: Python

#### ğŸ¯ æ ¸å¿ƒç‰¹æ€§
- Sep 22, 2025: ğŸš€ We released the deployment code for assisting experiments with [Unitree](https://www.unitree.com/) robots.
- Sep 15, 2025: ğŸš€ We released the training and inference code along with the model weights of [**UnifoLM-WMA-0**](https://huggingface.co/collections/unitreerobotics/unifolm-wma-0-68ca23027310c0ca0f34959c).
- [x] Training
- [x] Inference
- [x] Checkpoints
- [x] Deployment

#### ğŸ¨ é€‚ç”¨åœºæ™¯
- æ•°æ®ç§‘å­¦ä¸åˆ†æ
- æœºå™¨å­¦ä¹ é¡¹ç›®
- Web åº”ç”¨å¼€å‘

---

### 16. crypto-tax-calculator

**ğŸ”¥ çƒ­åº¦æŒ‡æ•°**: 680

**é¡¹ç›®åç§°**: [Uak0/crypto-tax-calculator](https://github.com/Uak0/crypto-tax-calculator)
**é¡¹ç›®æè¿°**: An advanced cryptocurrency & personal income tax calculator. 
**ä¸»è¦è¯­è¨€**: Python
**è®¸å¯è¯**: GNU Affero General Public License v3.0
**â­ æ˜Ÿæ ‡æ•°**: 593
**ğŸ´ Fork æ•°**: 174
**ğŸ‘€ å…³æ³¨æ•°**: 593
**ğŸ› å¼€æ”¾é—®é¢˜**: 0
**æœ€åæ›´æ–°**: 2025-10-12 (9å°æ—¶å‰)
**ğŸ“– é¡¹ç›®æ–‡æ¡£**: [README](3ziye-20251012-python/16-crypto-tax-calculator-Readme.md)

**ğŸ  é¡¹ç›®ä¸»é¡µ**: æ— 

**ğŸ“‚ å…‹éš†åœ°å€**: `https://github.com/Uak0/crypto-tax-calculator.git`

**ğŸ’» æŠ€æœ¯æ ˆ**: **ä¸»è¦è¯­è¨€**: Python

#### ğŸ·ï¸ é¡¹ç›®æ ‡ç­¾
- `bitcoin`
- `crypto`
- `crypto-tax-reports`
- `cryptocurrency`
- `cryptotax`
- `tax`
- `tax-calculation`
- `tax-calculator`

#### ğŸ¨ é€‚ç”¨åœºæ™¯
- æ•°æ®ç§‘å­¦ä¸åˆ†æ
- æœºå™¨å­¦ä¹ é¡¹ç›®
- Web åº”ç”¨å¼€å‘

---

### 17. craftgpt

**ğŸ”¥ çƒ­åº¦æŒ‡æ•°**: 605

**é¡¹ç›®åç§°**: [sammyuri/craftgpt](https://github.com/sammyuri/craftgpt)
**é¡¹ç›®æè¿°**: Small language model built in Minecraft.
**ä¸»è¦è¯­è¨€**: Python
**è®¸å¯è¯**: MIT License
**â­ æ˜Ÿæ ‡æ•°**: 589
**ğŸ´ Fork æ•°**: 33
**ğŸ‘€ å…³æ³¨æ•°**: 589
**ğŸ› å¼€æ”¾é—®é¢˜**: 10
**æœ€åæ›´æ–°**: 2025-10-12 (1å°æ—¶å‰)
**ğŸ“– é¡¹ç›®æ–‡æ¡£**: [README](3ziye-20251012-python/17-craftgpt-Readme.md)

**ğŸ  é¡¹ç›®ä¸»é¡µ**: æ— 

**ğŸ“‚ å…‹éš†åœ°å€**: `https://github.com/sammyuri/craftgpt.git`

**ğŸ’» æŠ€æœ¯æ ˆ**: **ä¸»è¦è¯­è¨€**: Python

#### ğŸ¯ æ ¸å¿ƒç‰¹æ€§
- Download MCHPRS, set the plot scale to 7 (this can be found at `./crates/core/src/plot/mod.rs`) and compile it.
- Download the MCHPRS world, unpack it at `./target/release`, and rename it to `world`.
- Type `/rtps unlimited` and `/wsr 1`.
- The default RNG seed is `1`. If you want to enter a different one, enter it in binary at `230, 150, 1000` and push the button to confirm.

#### ğŸ¨ é€‚ç”¨åœºæ™¯
- æ•°æ®ç§‘å­¦ä¸åˆ†æ
- æœºå™¨å­¦ä¹ é¡¹ç›®
- Web åº”ç”¨å¼€å‘

---

### 18. OpenTrack

**ğŸ”¥ çƒ­åº¦æŒ‡æ•°**: 596

**é¡¹ç›®åç§°**: [GalaxyGeneralRobotics/OpenTrack](https://github.com/GalaxyGeneralRobotics/OpenTrack)
**é¡¹ç›®æè¿°**: Official implementation of OpenTrack.
**ä¸»è¦è¯­è¨€**: Python
**è®¸å¯è¯**: None
**â­ æ˜Ÿæ ‡æ•°**: 582
**ğŸ´ Fork æ•°**: 29
**ğŸ‘€ å…³æ³¨æ•°**: 582
**ğŸ› å¼€æ”¾é—®é¢˜**: 8
**æœ€åæ›´æ–°**: 2025-10-11 (10å°æ—¶å‰)
**ğŸ“– é¡¹ç›®æ–‡æ¡£**: [README](3ziye-20251012-python/18-OpenTrack-Readme.md)

**ğŸ  é¡¹ç›®ä¸»é¡µ**: æ— 

**ğŸ“‚ å…‹éš†åœ°å€**: `https://github.com/GalaxyGeneralRobotics/OpenTrack.git`

**ğŸ’» æŠ€æœ¯æ ˆ**: **ä¸»è¦è¯­è¨€**: Python

#### ğŸ¯ æ ¸å¿ƒç‰¹æ€§
- [x] Release AnyTracker
- [x] Release dynamics disturbances
- [ ] Release AnyAdapter
- [ ] Release real deployment code

#### ğŸ¨ é€‚ç”¨åœºæ™¯
- æ•°æ®ç§‘å­¦ä¸åˆ†æ
- æœºå™¨å­¦ä¹ é¡¹ç›®
- Web åº”ç”¨å¼€å‘

---

### 19. Lucy-Edit-ComfyUI

**ğŸ”¥ çƒ­åº¦æŒ‡æ•°**: 611

**é¡¹ç›®åç§°**: [DecartAI/Lucy-Edit-ComfyUI](https://github.com/DecartAI/Lucy-Edit-ComfyUI)
**é¡¹ç›®æè¿°**: æš‚æ— æè¿°
**ä¸»è¦è¯­è¨€**: Python
**è®¸å¯è¯**: None
**â­ æ˜Ÿæ ‡æ•°**: 579
**ğŸ´ Fork æ•°**: 64
**ğŸ‘€ å…³æ³¨æ•°**: 579
**ğŸ› å¼€æ”¾é—®é¢˜**: 10
**æœ€åæ›´æ–°**: 2025-10-11 (19å°æ—¶å‰)
**ğŸ“– é¡¹ç›®æ–‡æ¡£**: [README](3ziye-20251012-python/19-Lucy-Edit-ComfyUI-Readme.md)

**ğŸ  é¡¹ç›®ä¸»é¡µ**: æ— 

**ğŸ“‚ å…‹éš†åœ°å€**: `https://github.com/DecartAI/Lucy-Edit-ComfyUI.git`

**ğŸ’» æŠ€æœ¯æ ˆ**: **ä¸»è¦è¯­è¨€**: Python

#### ğŸ¯ æ ¸å¿ƒç‰¹æ€§
- ğŸƒâ€â™‚ï¸ **Motion Preservation** - preserves the motion and composition of videos perfectly, allowing precise edits.
- ğŸ¯ **Edit reliability** â€” edits are more robust when compared to common inference time methods.
- ğŸ§¢ **Wardrobe & accessories** â€” change outfits, add glasses/earrings/hats/etc.
- ğŸ§Œ **Character Changes** â€” replace characters with monsters, animals and known characters. (e.g., "Replace the person with a polar bear")
- ğŸ—ºï¸ **Scenery swap** â€” move the scene (e.g., "transform the scene into a 2D cartoon,")
- ğŸ“ **Pure text instructions** â€” no finetuning, no masks required for common edits

#### ğŸ¨ é€‚ç”¨åœºæ™¯
- æ•°æ®ç§‘å­¦ä¸åˆ†æ
- æœºå™¨å­¦ä¹ é¡¹ç›®
- Web åº”ç”¨å¼€å‘

---

### 20. MimicKit

**ğŸ”¥ çƒ­åº¦æŒ‡æ•°**: 590

**é¡¹ç›®åç§°**: [xbpeng/MimicKit](https://github.com/xbpeng/MimicKit)
**é¡¹ç›®æè¿°**: Suite of motion imitation methods for training motion controllers.
**ä¸»è¦è¯­è¨€**: Python
**è®¸å¯è¯**: BSD 3-Clause "New" or "Revised" License
**â­ æ˜Ÿæ ‡æ•°**: 570
**ğŸ´ Fork æ•°**: 40
**ğŸ‘€ å…³æ³¨æ•°**: 570
**ğŸ› å¼€æ”¾é—®é¢˜**: 4
**æœ€åæ›´æ–°**: 2025-10-12 (5å°æ—¶å‰)
**ğŸ“– é¡¹ç›®æ–‡æ¡£**: [README](3ziye-20251012-python/20-MimicKit-Readme.md)

**ğŸ  é¡¹ç›®ä¸»é¡µ**: æ— 

**ğŸ“‚ å…‹éš†åœ°å€**: `https://github.com/xbpeng/MimicKit.git`

**ğŸ’» æŠ€æœ¯æ ˆ**: **ä¸»è¦è¯­è¨€**: Python

#### ğŸ¯ æ ¸å¿ƒç‰¹æ€§
- [DeepMimic](https://xbpeng.github.io/projects/DeepMimic/index.html)
- [AMP](https://xbpeng.github.io/projects/AMP/index.html)
- [ASE](https://xbpeng.github.io/projects/ASE/index.html)
- [ADD](https://xbpeng.github.io/projects/ADD/index.html)
- [PPO](https://arxiv.org/abs/1707.06347)
- [AWR](https://xbpeng.github.io/projects/AWR/index.html)
- `--mode` selects either `train` or `test` mode.
- `--num_envs` specifies the number of parallel environments used for simulation.

#### ğŸ¨ é€‚ç”¨åœºæ™¯
- æ•°æ®ç§‘å­¦ä¸åˆ†æ
- æœºå™¨å­¦ä¹ é¡¹ç›®
- Web åº”ç”¨å¼€å‘

---