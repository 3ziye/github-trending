[
  {
    "basic_info": {
      "name": "nanobot",
      "full_name": "HKUDS/nanobot",
      "owner": "HKUDS",
      "description": "\"üêà nanobot: The Ultra-Lightweight OpenClaw\"",
      "url": "https://github.com/HKUDS/nanobot",
      "clone_url": "https://github.com/HKUDS/nanobot.git",
      "ssh_url": "git@github.com:HKUDS/nanobot.git",
      "homepage": "",
      "created_at": "2026-02-01T07:16:15Z",
      "updated_at": "2026-02-22T03:31:59Z",
      "pushed_at": "2026-02-21T17:07:08Z"
    },
    "stats": {
      "stars": 22797,
      "forks": 3538,
      "watchers": 22797,
      "open_issues": 563,
      "size": 33384
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 404420,
        "TypeScript": 8773,
        "Shell": 7009,
        "JavaScript": 1316,
        "Dockerfile": 1294
      },
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "<div align=\"center\">\n  <img src=\"nanobot_logo.png\" alt=\"nanobot\" width=\"500\">\n  <h1>nanobot: Ultra-Lightweight Personal AI Assistant</h1>\n  <p>\n    <a href=\"https://pypi.org/project/nanobot-ai/\"><img src=\"https://img.shields.io/pypi/v/nanobot-ai\" alt=\"PyPI\"></a>\n    <a href=\"https://pepy.tech/project/nanobot-ai\"><img src=\"https://static.pepy.tech/badge/nanobot-ai\" alt=\"Downloads\"></a>\n    <img src=\"https://img.shields.io/badge/python-‚â•3.11-blue\" alt=\"Python\">\n    <img src=\"https://img.shields.io/badge/license-MIT-green\" alt=\"License\">\n    <a href=\"./COMMUNICATION.md\"><img src=\"https://img.shields.io/badge/Feishu-Group-E9DBFC?style=flat&logo=feishu&logoColor=white\" alt=\"Feishu\"></a>\n    <a href=\"./COMMUNICATION.md\"><img src=\"https://img.shields.io/badge/WeChat-Group-C5EAB4?style=flat&logo=wechat&logoColor=white\" alt=\"WeChat\"></a>\n    <a href=\"https://discord.gg/MnCvHqpUGB\"><img src=\"https://img.shields.io/badge/Discord-Community-5865F2?style=flat&logo=discord&logoColor=white\" alt=\"Discord\"></a>\n  </p>\n</div>\n\nüêà **nanobot** is an **ultra-lightweight** personal AI assistant inspired by [OpenClaw](https://github.com/openclaw/openclaw) \n\n‚ö°Ô∏è Delivers core agent functionality in just **~4,000** lines of code ‚Äî **99% smaller** than Clawdbot's 430k+ lines.\n\nüìè Real-time line count: **3,806 lines** (run `bash core_agent_lines.sh` to verify anytime)\n\n## üì¢ News\n\n- **2026-02-21** üéâ Released **v0.1.4.post1** ‚Äî new providers, media support across channels, and major stability improvements. See [release notes](https://github.com/HKUDS/nanobot/releases/tag/v0.1.4.post1) for details.\n- **2026-02-20** üê¶ Feishu now receives multimodal files from users. More reliable memory under the hood.\n- **2026-02-19** ‚ú® Slack now sends files, Discord splits long messages, and subagents work in CLI mode.\n- **2026-02-18** ‚ö°Ô∏è nanobot now supports VolcEngine, MCP custom auth headers, and Anthropic prompt caching.\n- **2026-02-17** üéâ Released **v0.1.4** ‚Äî MCP support, progress streaming, new providers, and multiple channel improvements. Please see [release notes](https://github.com/HKUDS/nanobot/releases/tag/v0.1.4) for details.\n- **2026-02-16** ü¶û nanobot now integrates a [ClawHub](https://clawhub.ai) skill ‚Äî search and install public agent skills.\n- **2026-02-15** üîë nanobot now supports OpenAI Codex provider with OAuth login support.\n- **2026-02-14** üîå nanobot now supports MCP! See [MCP section](#mcp-model-context-protocol) for details.\n- **2026-02-13** üéâ Released **v0.1.3.post7** ‚Äî includes security hardening and multiple improvements. **Please upgrade to the latest version to address security issues**. See [release notes](https://github.com/HKUDS/nanobot/releases/tag/v0.1.3.post7) for more details.\n- **2026-02-12** üß† Redesigned memory system ‚Äî Less code, more reliable. Join the [discussion](https://github.com/HKUDS/nanobot/discussions/566) about it!\n- **2026-02-11** ‚ú® Enhanced CLI experience and added MiniMax support!\n\n<details>\n<summary>Earlier news</summary>\n\n- **2026-02-10** üéâ Released **v0.1.3.post6** with improvements! Check the updates [notes](https://github.com/HKUDS/nanobot/releases/tag/v0.1.3.post6) and our [roadmap](https://github.com/HKUDS/nanobot/discussions/431).\n- **2026-02-09** üí¨ Added Slack, Email, and QQ support ‚Äî nanobot now supports multiple chat platforms!\n- **2026-02-08** üîß Refactored Providers‚Äîadding a new LLM provider now takes just 2 simple steps! Check [here](#providers).\n- **2026-02-07** üöÄ Released **v0.1.3.post5** with Qwen support & several key improvements! Check [here](https://github.com/HKUDS/nanobot/releases/tag/v0.1.3.post5) for details.\n- **2026-02-06** ‚ú® Added Moonshot/Kimi provider, Discord integration, and enhanced security hardening!\n- **2026-02-05** ‚ú® Added Feishu channel, DeepSeek provider, and enhanced scheduled tasks support!\n- **2026-02-04** üöÄ Released **v0.1.3.post4** with multi-provider & Docker support! Check [here](https://github.com/HKUDS/nanobot/releases/tag/v0.1.3.post4) for details.\n- **2026-02-03** ‚ö° Integrated vLLM for local LLM support and improved natural language task scheduling!\n- **2026-02-02** üéâ nanobot officially launched! Welcome to try üêà nanobot!\n\n</details>\n\n## Key Features of nanobot:\n\nü™∂ **Ultra-Lightweight**: Just ~4,000 lines of core agent code ‚Äî 99% smaller than Clawdbot.\n\nüî¨ **Research-Ready**: Clean, readable code that's easy to understand, modify, and extend for research.\n\n‚ö°Ô∏è **Lightning Fast**: Minimal footprint means faster startup, lower resource usage, and quicker iterations.\n\nüíé **Easy-to-Use**: One-click to deploy and you're ready to go.\n\n## üèóÔ∏è Architecture\n\n<p align=\"center\">\n  <img src=\"nanobot_arch.png\" alt=\"nanobot architecture\" width=\"800\">\n</p>\n\n## ‚ú® Features\n\n<table align=\"center\">\n  <tr align=\"center\">\n    <th><p align=\"center\">üìà 24/7 Real-Time Market Analysis</p></th>\n    <th><p align=\"center\">üöÄ Full-Stack Software Engineer</p></th>\n    <th><p align=\"center\">üìÖ Smart Daily Routine Manager</p></th>\n    <th><p align=\"center\">üìö Personal Knowledge Assistant</p></th>\n",
      "default_branch": "main"
    },
    "fetched_at": "2026-02-22T03:32:18.512549"
  },
  {
    "basic_info": {
      "name": "knowledge-work-plugins",
      "full_name": "anthropics/knowledge-work-plugins",
      "owner": "anthropics",
      "description": "Open source repository of plugins primarily intended for knowledge workers to use in Claude Cowork",
      "url": "https://github.com/anthropics/knowledge-work-plugins",
      "clone_url": "https://github.com/anthropics/knowledge-work-plugins.git",
      "ssh_url": "git@github.com:anthropics/knowledge-work-plugins.git",
      "homepage": null,
      "created_at": "2026-01-23T20:11:54Z",
      "updated_at": "2026-02-22T03:08:26Z",
      "pushed_at": "2026-02-18T19:52:26Z"
    },
    "stats": {
      "stars": 7671,
      "forks": 775,
      "watchers": 7671,
      "open_issues": 45,
      "size": 635
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 310600,
        "HTML": 97651
      },
      "license": "Apache License 2.0",
      "topics": []
    },
    "content": {
      "readme": "# Knowledge Work Plugins\n\nPlugins that turn Claude into a specialist for your role, team, and company. Built for [Claude Cowork](https://claude.com/product/cowork), also compatible with [Claude Code](https://claude.com/product/claude-code).\n\n## Why Plugins\n\nCowork lets you set the goal and Claude delivers finished, professional work. Plugins let you go further: tell Claude how you like work done, which tools and data to pull from, how to handle critical workflows, and what slash commands to expose ‚Äî so your team gets better and more consistent outcomes.\n\nEach plugin bundles the skills, connectors, slash commands, and sub-agents for a specific job function. Out of the box, they give Claude a strong starting point for helping anyone in that role. The real power comes when you customize them for your company ‚Äî your tools, your terminology, your processes ‚Äî so Claude works like it was built for your team.\n\n## Plugin Marketplace\n\nWe're open-sourcing 11 plugins built and inspired by our own work:\n\n| Plugin | How it helps | Connectors |\n|--------|-------------|------------|\n| **[productivity](./productivity)** | Manage tasks, calendars, daily workflows, and personal context so you spend less time repeating yourself. | Slack, Notion, Asana, Linear, Jira, Monday, ClickUp, Microsoft 365 |\n| **[sales](./sales)** | Research prospects, prep for calls, review your pipeline, draft outreach, and build competitive battlecards. | Slack, HubSpot, Close, Clay, ZoomInfo, Notion, Jira, Fireflies, Microsoft 365 |\n| **[customer-support](./customer-support)** | Triage tickets, draft responses, package escalations, research customer context, and turn resolved issues into knowledge base articles. | Slack, Intercom, HubSpot, Guru, Jira, Notion, Microsoft 365 |\n| **[product-management](./product-management)** | Write specs, plan roadmaps, synthesize user research, keep stakeholders updated, and track the competitive landscape. | Slack, Linear, Asana, Monday, ClickUp, Jira, Notion, Figma, Amplitude, Pendo, Intercom, Fireflies |\n| **[marketing](./marketing)** | Draft content, plan campaigns, enforce brand voice, brief on competitors, and report on performance across channels. | Slack, Canva, Figma, HubSpot, Amplitude, Notion, Ahrefs, SimilarWeb, Klaviyo |\n| **[legal](./legal)** | Review contracts, triage NDAs, navigate compliance, assess risk, prep for meetings, and draft templated responses. | Slack, Box, Egnyte, Jira, Microsoft 365 |\n| **[finance](./finance)** | Prep journal entries, reconcile accounts, generate financial statements, analyze variances, manage close, and support audits. | Snowflake, Databricks, BigQuery, Slack, Microsoft 365 |\n| **[data](./data)** | Query, visualize, and interpret datasets ‚Äî write SQL, run statistical analysis, build dashboards, and validate your work before sharing. | Snowflake, Databricks, BigQuery, Hex, Amplitude, Jira |\n| **[enterprise-search](./enterprise-search)** | Find anything across email, chat, docs, and wikis ‚Äî one query across all your company's tools. | Slack, Notion, Guru, Jira, Asana, Microsoft 365 |\n| **[bio-research](./bio-research)** | Connect to preclinical research tools and databases (literature search, genomics analysis, target prioritization) to accelerate early-stage life sciences R&D. | PubMed, BioRender, bioRxiv, ClinicalTrials.gov, ChEMBL, Synapse, Wiley, Owkin, Open Targets, Benchling |\n| **[cowork-plugin-management](./cowork-plugin-management)** | Create new plugins or customize existing ones for your organization's specific tools and workflows. | ‚Äî |\n\nInstall these directly from Cowork, browse the full collection here on GitHub, or build your own.\n\n## Getting Started\n\n### Cowork\n\nInstall plugins from [claude.com/plugins](https://claude.com/plugins/).\n\n### Claude Code\n\n```bash\n# Add the marketplace first\nclaude plugin marketplace add anthropics/knowledge-work-plugins\n\n# Then install a specific plugin\nclaude plugin install sales@knowledge-work-plugins\n```\n\nOnce installed, plugins activate automatically. Skills fire when relevant, and slash commands are available in your session (e.g., `/sales:call-prep`, `/data:write-query`).\n\n## How Plugins Work\n\nEvery plugin follows the same structure:\n\n```\nplugin-name/\n‚îú‚îÄ‚îÄ .claude-plugin/plugin.json   # Manifest\n‚îú‚îÄ‚îÄ .mcp.json                    # Tool connections\n‚îú‚îÄ‚îÄ commands/                    # Slash commands you invoke explicitly\n‚îî‚îÄ‚îÄ skills/                      # Domain knowledge Claude draws on automatically\n```\n\n- **Skills** encode the domain expertise, best practices, and step-by-step workflows Claude needs to give you useful help. Claude draws on them automatically when relevant.\n- **Commands** are explicit actions you trigger (e.g., `/finance:reconciliation`, `/product-management:write-spec`).\n- **Connectors** wire Claude to the external tools your role depends on ‚Äî CRMs, project trackers, data warehouses, design tools, and more ‚Äî via [MCP servers](https://modelcontextprotocol.io/).\n\nEvery component is file-based ‚Äî markdown and ",
      "default_branch": "main"
    },
    "fetched_at": "2026-02-22T03:32:19.655779"
  },
  {
    "basic_info": {
      "name": "ClawWork",
      "full_name": "HKUDS/ClawWork",
      "owner": "HKUDS",
      "description": "\"ClawWork: OpenClaw as Your AI Coworker - üí∞ $10K earned in 7 Hours\"",
      "url": "https://github.com/HKUDS/ClawWork",
      "clone_url": "https://github.com/HKUDS/ClawWork.git",
      "ssh_url": "git@github.com:HKUDS/ClawWork.git",
      "homepage": "",
      "created_at": "2026-02-15T16:41:38Z",
      "updated_at": "2026-02-22T03:30:43Z",
      "pushed_at": "2026-02-21T15:54:54Z"
    },
    "stats": {
      "stars": 4879,
      "forks": 595,
      "watchers": 4879,
      "open_issues": 7,
      "size": 287164
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 603308,
        "Jupyter Notebook": 225541,
        "JavaScript": 160148,
        "Shell": 12288,
        "TypeScript": 12229,
        "CSS": 2198,
        "HTML": 389
      },
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "<img alt=\"image\" src=\"assets/live_banner.png\" /><div align=\"center\">\n  <h1>ClawWork: OpenClaw as Your AI Coworker</h1>\n    <p>\n    <img src=\"https://img.shields.io/badge/python-‚â•3.10-blue\" alt=\"Python\">\n    <img src=\"https://img.shields.io/badge/license-MIT-green\" alt=\"License\">\n    <img src=\"https://img.shields.io/badge/dataset-GDPVal%20220%20tasks-orange\" alt=\"GDPVal\">\n    <img src=\"https://img.shields.io/badge/benchmark-economic%20survival-red\" alt=\"Benchmark\">\n    <a href=\"https://github.com/HKUDS/nanobot\"><img src=\"https://img.shields.io/badge/nanobot-integration-C5EAB4?style=flat&logo=github&logoColor=white\" alt=\"nanobot\"></a>\n    <a href=\"https://github.com/HKUDS/.github/blob/main/profile/README.md\"><img src=\"https://img.shields.io/badge/Feishu-Group-E9DBFC?style=flat&logo=feishu&logoColor=white\" alt=\"Feishu\"></a>\n    <a href=\"https://github.com/HKUDS/.github/blob/main/profile/README.md\"><img src=\"https://img.shields.io/badge/WeChat-Group-C5EAB4?style=flat&logo=wechat&logoColor=white\" alt=\"WeChat\"></a>\n  </p>\n  <h3>üí∞ $10K in 7 Hours ‚Äî AI Coworker for 44+ Professions</h3>\n  <h4>| Technology & Engineering | Business & Finance | Healthcare & Social Services | Legal, Media & Operations | </h3>\n  <h3><a href=\"https://hkuds.github.io/ClawWork/\">üî¥ Watch AI Coworkers Earn Money from Real-Life Tasks</a></h3>\n  <p><sub>Agent data on the site is periodically synced to this repo. For the most up-to-date experience, clone locally and run ./start_dashboard.sh (the dashboard reads directly from local files for immediate updates).</sub></p>\n\n</div>\n  \n---\n\n<div align=\"center\">\n<img src=\"assets/clawwork_banner.png\" alt=\"ClawWork\" width=\"800\">\n</div>\n\n### üöÄ AI Assistant ‚Üí AI Coworker Evolution\nTransforms AI assistants into true AI coworkers that complete real work tasks and create genuine economic value.\n\n### üí∞ Real-World Economic Benchmark\nReal-world economic testing system where AI agents must earn income by completing professional tasks from the [GDPVal](https://openai.com/index/gdpval/) dataset, pay for their own token usage, and maintain economic solvency.\n\n### üìä Production AI Validation\nMeasures what truly matters in production environments: **work quality**, **cost efficiency**, and **long-term survival** - not just technical benchmarks.\n\n### ü§ñ Multi-Model Competition Arena\nSupports different AI models (GLM, Kimi, Qwen, etc.) competing head-to-head to determine the ultimate \"AI worker champion\" through actual work performance\n\n---\n\n## üì¢ News\n\n- **2026-02-21 üîÑ ClawMode + Frontend + Agents Update** ‚Äî Updated ClawMode to support ClawWork-specific tools; improved frontend dashboard (untapped potential visualization); added more agents: Claude Sonnet 4.6, Gemini 3.1 Pro and Qwen-3.5-Plus.\n- **2026-02-20 üí∞ Improved Cost Tracking** ‚Äî Token costs are now read directly from various API responses (including thinking tokens) instead of estimation. OpenRouter's reported cost is used verbatim when available.\n- **2026-02-19 üìä Agent Results Updated** ‚Äî Added Qwen3-Max, Kimi-K2.5, GLM-4.7 through Feb 19. Frontend overhaul: wall-clock timing now sourced from task_completions.jsonl.\n- **2026-02-17 üîß Enhanced Nanobot Integration** ‚Äî New /clawwork command for on-demand paid tasks. Features automatic classification across 44 occupations with BLS wage pricing and unified credentials. Try locally: python -m clawmode_integration.cli agent.\n- **2026-02-16 üéâ ClawWork Launch** ‚Äî ClawWork is now officially available! Welcome to explore ClawWork.\n\n---\n\n## ‚ú® ClawWork's Key Features\n\n- **üíº Real Professional Tasks**: 220 GDP validation tasks spanning 44 economic sectors (Manufacturing, Finance, Healthcare, and more) from the GDPVal dataset ‚Äî testing real-world work capability\n\n- **üí∏ Extreme Economic Pressure**: Agents start with just $10 and pay for every token generated. One bad task or careless search can wipe the balance. Income only comes from completing quality work.\n\n- **üß† Strategic Work + Learn Choices**: Agents face daily decisions: work for immediate income or invest in learning to improve future performance ‚Äî mimicking real career trade-offs.\n\n- **üìä React Dashboard**: Visualization of balance changes, task completions, learning progress, and survival metrics from real-life tasks ‚Äî watch the economic drama unfold.\n\n- **ü™∂ Ultra-Lightweight Architecture**: Built on Nanobot ‚Äî your strong AI coworker with minimal infrastructure. Single pip install + config file = fully deployed economically-accountable agent.\n\n- **üèÜ End-to-End Professional Benchmark**: i) Complete workflow: Task Assignment ‚Üí Execution ‚Üí Artifact Creation ‚Üí LLM Evaluation ‚Üí Payment; ii) The strongest models achieve $1,500+/hr equivalent salary ‚Äî surpassing typical human white-collar productivity.\n\n- **üîó Drop-in OpenClaw/Nanobot Integration**: ClawMode wrapper transforms any live Nanobot gateway into a money-earning coworker with economic tracking.\n\n- **‚öñÔ∏è Rigorous LLM Evaluation**: Quality scoring via GPT-5.2 with category-specific rubrics for each of the 44 GDPVal sector",
      "default_branch": "main"
    },
    "fetched_at": "2026-02-22T03:32:20.797929"
  },
  {
    "basic_info": {
      "name": "lingbot-world",
      "full_name": "Robbyant/lingbot-world",
      "owner": "Robbyant",
      "description": "Advancing Open-source World Models",
      "url": "https://github.com/Robbyant/lingbot-world",
      "clone_url": "https://github.com/Robbyant/lingbot-world.git",
      "ssh_url": "git@github.com:Robbyant/lingbot-world.git",
      "homepage": "https://technology.robbyant.com/lingbot-world",
      "created_at": "2026-01-28T04:52:50Z",
      "updated_at": "2026-02-22T03:31:35Z",
      "pushed_at": "2026-02-02T11:46:45Z"
    },
    "stats": {
      "stars": 2917,
      "forks": 237,
      "watchers": 2917,
      "open_issues": 20,
      "size": 47043
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 594764
      },
      "license": "Apache License 2.0",
      "topics": [
        "aigc",
        "image-to-video",
        "lingbot-world",
        "video-generation",
        "world-models"
      ]
    },
    "content": {
      "readme": "<div align=\"center\">\n  <img src=\"assets/teaser.png\">\n\n<h1>LingBot-World: Advancing Open-source World Models</h1>\n\nRobbyant Team\n\n</div>\n\n\n<div align=\"center\">\n\n[![Page](https://img.shields.io/badge/%F0%9F%8C%90%20Project%20Page-Demo-00bfff)](https://technology.robbyant.com/lingbot-world)\n[![Tech Report](https://img.shields.io/static/v1?label=Paper&message=PDF&color=red&logo=arxiv)](https://arxiv.org/abs/2601.20540)\n[![Model](https://img.shields.io/static/v1?label=%F0%9F%A4%97%20Model&message=HuggingFace&color=yellow)](https://huggingface.co/robbyant/lingbot-world-base-cam)\n[![Model](https://img.shields.io/static/v1?label=%F0%9F%A4%96%20Model&message=ModelScope&color=purple)](https://www.modelscope.cn/models/Robbyant/lingbot-world-base-cam)\n[![License](https://img.shields.io/badge/License-Apache--2.0-green)](LICENSE.txt)\n\n\n</div>\n\n-----\n\nWe are excited to introduce **LingBot-World**, an open-sourced world simulator stemming from video generation. Positioned\nas a top-tier world model, LingBot-World offers the following features. \n- **High-Fidelity & Diverse Environments**: It maintains high fidelity and robust dynamics in a broad spectrum of environments, including realism, scientific contexts, cartoon styles, and beyond. \n- **Long-Term Memory & Consistency**: It enables a minute-level horizon while preserving contextual consistency over time, which is also known as long-term memory. \n- **Real-Time Interactivity & Open Access**: It supports real-time interactivity, achieving a latency of under 1 second when producing 16 frames per second. We provide public access to the code and model in an effort to narrow the divide between open-source and closed-source technologies. We believe our release will empower the community with practical applications across areas like content creation, gaming, and robot learning.\n\n## üé¨ Video Demo\n<div align=\"center\">\n  <video src=\"https://github.com/user-attachments/assets/ea4a7a8d-5d9e-4ccf-96e7-02f93797116e\" width=\"100%\" poster=\"\"> </video>\n</div>\n\n## üî• News\n- Jan 29, 2026: üéâ We release the technical report, code, and models for LingBot-World.\n\n<!-- ## üîñ Introduction of LingBot-World\nWe present **LingBot-World**, an **open-sourced** world simulator stemming from video generation. Positioned\nas a top-tier world model, LingBot-World offers the following features. \n- It maintains high fidelity and robust dynamics in a broad spectrum of environments, including realism, scientific contexts, cartoon styles, and beyond. \n- It enables a minute-level horizon while preserving contextual consistency over time, which is also known as **long-term memory**. \n- It supports real-time interactivity, achieving a latency of under 1 second when producing 16 frames per second. We provide public access to the code and model in an effort to narrow the divide between open-source and closed-source technologies. We believe our release will empower the community with practical applications across areas like content creation, gaming, and robot learning. -->\n\n## ‚öôÔ∏è Quick Start\nThis codebase is built upon [Wan2.2](https://github.com/Wan-Video/Wan2.2). Please refer to their documentation for installation instructions.\n### Installation\nClone the repo:\n```sh\ngit clone https://github.com/robbyant/lingbot-world.git\ncd lingbot-world\n```\nInstall dependencies:\n```sh\n# Ensure torch >= 2.4.0\npip install -r requirements.txt\n```\nInstall [`flash_attn`](https://github.com/Dao-AILab/flash-attention):\n```sh\npip install flash-attn --no-build-isolation\n```\n### Model Download\n\n| Model | Control Signals | Resolution | Download Links |\n| :---  | :--- | :--- | :--- |\n| **LingBot-World-Base (Cam)** | Camera Poses | 480P & 720P | ü§ó [HuggingFace](https://huggingface.co/robbyant/lingbot-world-base-cam) ü§ñ [ModelScope](https://www.modelscope.cn/models/Robbyant/lingbot-world-base-cam) |\n| **LingBot-World-Base (Act)** | Actions | - | *To be released* |\n| **LingBot-World-Fast**       |    -    | - | *To be released* |\n\nDownload models using huggingface-cli:\n```sh\npip install \"huggingface_hub[cli]\"\nhuggingface-cli download robbyant/lingbot-world-base-cam --local-dir ./lingbot-world-base-cam\n```\nDownload models using modelscope-cli:\n ```sh\npip install modelscope\nmodelscope download robbyant/lingbot-world-base-cam --local_dir ./lingbot-world-base-cam\n```\n### Inference\nBefore running inference, you need to prepare:\n- Input image\n- Text prompt\n- Control signals (optional, can be generated from a video using [ViPE](https://github.com/nv-tlabs/vipe))\n  - `intrinsics.npy`: Shape `[num_frames, 4]`, where the 4 values represent `[fx, fy, cx, cy]`\n  - `poses.npy`: Shape `[num_frames, 4, 4]`, where each `[4, 4]` represents a transformation matrix in OpenCV coordinates\n\n- 480P:\n``` sh\ntorchrun --nproc_per_node=8 generate.py --task i2v-A14B --size 480*832 --ckpt_dir lingbot-world-base-cam --image examples/00/image.jpg --action_path examples/00 --dit_fsdp --t5_fsdp --ulysses_size 8 --frame_num 161 --prompt \"The video presents a soaring journey through a",
      "default_branch": "main"
    },
    "fetched_at": "2026-02-22T03:32:21.944346"
  },
  {
    "basic_info": {
      "name": "last30days-skill",
      "full_name": "mvanhorn/last30days-skill",
      "owner": "mvanhorn",
      "description": "Claude Code skill that researches any topic across Reddit + X from the last 30 days, then writes copy-paste-ready prompts",
      "url": "https://github.com/mvanhorn/last30days-skill",
      "clone_url": "https://github.com/mvanhorn/last30days-skill.git",
      "ssh_url": "git@github.com:mvanhorn/last30days-skill.git",
      "homepage": null,
      "created_at": "2026-01-23T20:37:37Z",
      "updated_at": "2026-02-22T03:00:50Z",
      "pushed_at": "2026-02-21T07:03:01Z"
    },
    "stats": {
      "stars": 2889,
      "forks": 337,
      "watchers": 2889,
      "open_issues": 18,
      "size": 7662
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 294216,
        "Shell": 1255
      },
      "license": null,
      "topics": [
        "ai-prompts",
        "claude",
        "claude-code",
        "reddit",
        "twitter"
      ]
    },
    "content": {
      "readme": "# /last30days v2.1\n\n**The AI world reinvents itself every month. This skill keeps you current.** /last30days researches your topic across Reddit, X, YouTube, and the web from the last 30 days, finds what the community is actually upvoting, sharing, and saying on camera, and writes you a prompt that works today, not six months ago. Whether it's Seedance 2.0 access, Suno music prompts, or the latest Nano Banana Pro techniques, you'll prompt like someone who's been paying attention.\n\n**New in V2.1  - three headline features:**\n\n1. **Open-class skill with watchlists.** Add any topic to a watchlist  - your competitors, specific people, emerging technologies  - and /last30days re-researches it on demand or via cron. Designed for always-on environments like [Open Claw](https://github.com/openclaw/openclaw) where a bot can run research on a schedule and accumulate findings over time.\n2. **YouTube transcripts as a 4th source.** When yt-dlp is installed, /last30days automatically searches YouTube, grabs view counts, and extracts auto-generated transcripts from the top videos. A 20-minute review contains 10x the signal of a single post - now the skill reads it. Inspired by [@steipete](https://x.com/steipete)'s yt-dlp + [summarize](https://github.com/steipete/summarize) toolchain.\n3. **Works in OpenAI Codex CLI.** Same skill, same engine. Install to `~/.agents/skills/last30days` and invoke with `$last30days`. Claude Code and Codex users get the same research.\n\n**New in V2:** Dramatically better search results. Smarter query construction finds posts that V1 missed entirely, and a new two-phase search automatically discovers key @handles and subreddits from initial results, then drills deeper. Free X search (no xAI key needed), `--days=N` for flexible lookback, and automatic model fallback. [Full changelog below.](#whats-new-in-v2)\n\n**The tradeoff:** V2 finds way more content but takes longer - typically 2-8 minutes depending on how niche your topic is. The old V1 was faster but regularly missed results (like returning 0 X posts on trending topics). We think the depth is worth the wait, but if you'd use a faster \"quick mode\" that trades some depth for speed, let us know: [@mvanhorn](https://x.com/mvanhorn) / [@slashlast30days](https://x.com/slashlast30days).\n\n**Best for prompt research**: discover what prompting techniques actually work for any tool (ChatGPT, Midjourney, Claude, Figma AI, etc.) by learning from real community discussions and best practices.\n\n**But also great for anything trending**: music, culture, news, product recommendations, viral trends, or any question where \"what are people saying right now?\" matters.\n\n## Installation\n\n```bash\n# Clone the repo\ngit clone https://github.com/mvanhorn/last30days-skill.git ~/.claude/skills/last30days\n\n# Add your API keys\nmkdir -p ~/.config/last30days\ncat > ~/.config/last30days/.env << 'EOF'\nOPENAI_API_KEY=sk-...\nXAI_API_KEY=xai-...       # optional  - cookie auth is default for X search\nEOF\nchmod 600 ~/.config/last30days/.env\n```\n\n### X Search Authentication\n\nX search reads your existing browser cookies  - no API keys or login commands needed.\n\n**Safari (recommended on Mac):** Just be logged into x.com. No setup needed.\n\n**Chrome:** Works, but macOS will prompt you to allow Keychain access the first time. Click \"Allow\" (or \"Always Allow\" to stop future prompts).\n\n**Firefox:** Just be logged into x.com. No setup needed.\n\n**Manual fallback:** If cookie auto-detection doesn't work, set these env vars (grab them from your browser's dev tools ‚Üí Application ‚Üí Cookies ‚Üí x.com):\n```bash\nexport AUTH_TOKEN=your_auth_token\nexport CT0=your_ct0_token\n```\n\n**Verify it's working:**\n```bash\nnode ~/.claude/skills/last30days/scripts/lib/vendor/bird-search/bird-search.mjs --whoami\n```\n\n**Requirements:** Node.js 22+ (for the vendored Twitter GraphQL client).\n\n### Codex CLI\n\nThis skill also works in OpenAI Codex CLI. Install to the Codex skills directory instead:\n\n```bash\ngit clone https://github.com/mvanhorn/last30days-skill.git ~/.agents/skills/last30days\n```\n\nSame SKILL.md, same Python engine, same scripts. The `agents/openai.yaml` provides Codex-specific discovery metadata. Invoke with `$last30days` or through the `/skills` menu.\n\n### Open Variant (Watchlist + Briefings)  - For Always-On Bots\n\n**Designed for [Open Claw](https://github.com/openclaw/openclaw) and similar always-on AI environments.** Add your competitors, specific people, or any topic to a watchlist. When paired with a cron job or always-on bot, /last30days re-researches them on a schedule and accumulates findings in a local SQLite database. Ask for a briefing anytime.\n\n**Important:** The watchlist stores schedules as metadata, but nothing triggers runs automatically. You need an external scheduler (cron, launchd, or an always-on bot like Open Claw) to call `watchlist.py run-all` on a timer. In plain Claude Code, you can run `watch run-one` and `watch run-all` manually, but there's no background scheduling.\n\n```bash\n# Ena",
      "default_branch": "main"
    },
    "fetched_at": "2026-02-22T03:32:23.088119"
  },
  {
    "basic_info": {
      "name": "Trellis",
      "full_name": "mindfold-ai/Trellis",
      "owner": "mindfold-ai",
      "description": "All-in-one AI framework & toolkit",
      "url": "https://github.com/mindfold-ai/Trellis",
      "clone_url": "https://github.com/mindfold-ai/Trellis.git",
      "ssh_url": "git@github.com:mindfold-ai/Trellis.git",
      "homepage": "https://trytrellis.app",
      "created_at": "2026-01-26T11:49:10Z",
      "updated_at": "2026-02-22T03:10:46Z",
      "pushed_at": "2026-02-21T03:53:07Z"
    },
    "stats": {
      "stars": 2378,
      "forks": 118,
      "watchers": 2378,
      "open_issues": 1,
      "size": 6514
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 546174,
        "Shell": 317862,
        "TypeScript": 287528,
        "JavaScript": 74070
      },
      "license": "GNU Affero General Public License v3.0",
      "topics": [
        "ai-agent",
        "ai-coding",
        "claude-code",
        "cli",
        "codex",
        "cursor",
        "developer-tools",
        "typescript",
        "workflow"
      ]
    },
    "content": {
      "readme": "<!--<p align=\"center\">\n<img src=\"assets/meme.png\" alt=\"AI Coding Problems Meme\" />\n</p>-->\n\n<p align=\"center\">\n<picture>\n<source srcset=\"assets/trellis.png\" media=\"(prefers-color-scheme: dark)\">\n<source srcset=\"assets/trellis.png\" media=\"(prefers-color-scheme: light)\">\n<img src=\"assets/trellis.png\" alt=\"Trellis Logo\" width=\"500\" style=\"image-rendering: -webkit-optimize-contrast; image-rendering: crisp-edges;\">\n</picture>\n</p>\n\n<p align=\"center\">\n<strong>All-in-one AI framework & toolkit for Claude Code, Cursor, iFlow & Codex</strong><br/>\n<sub>Wild AI ships nothing.</sub>\n</p>\n\n<p align=\"center\">\n<a href=\"https://www.npmjs.com/package/@mindfoldhq/trellis\"><img src=\"https://img.shields.io/npm/v/@mindfoldhq/trellis.svg?style=flat-square&color=blue\" alt=\"npm version\" /></a>\n<a href=\"https://github.com/mindfold-ai/Trellis/blob/main/LICENSE\"><img src=\"https://img.shields.io/badge/license-FSL-green.svg?style=flat-square\" alt=\"license\" /></a>\n<a href=\"https://github.com/mindfold-ai/Trellis/stargazers\"><img src=\"https://img.shields.io/github/stars/mindfold-ai/Trellis?style=flat-square&color=yellow\" alt=\"stars\" /></a>\n<a href=\"https://discord.com/invite/tWcCZ3aRHc\"><img src=\"https://img.shields.io/badge/Discord-Join-7289DA?style=flat-square&logo=discord&logoColor=white\" alt=\"Discord\" /></a>\n</p>\n\n<p align=\"center\">\n<a href=\"#quick-start\">Quick Start</a> ‚Ä¢\n<a href=\"#why-trellis\">Why Trellis</a> ‚Ä¢\n<a href=\"#use-cases\">Use Cases</a> ‚Ä¢\n<a href=\"#how-it-works\">How It Works</a> ‚Ä¢\n<a href=\"#faq\">FAQ</a>\n</p>\n\n## Why Trellis?\n\n| Feature | Problem Solved |\n| --- | --- |\n| **Auto-Injection** | Required specs and workflows auto-inject into every conversation. Write once, apply forever |\n| **Auto-updated Spec Library** | Best practices live in auto-updated spec files. The more you use it, the better it gets |\n| **Parallel Sessions** | Run multiple agents in tandem - each in its own worktree |\n| **Team Sync** | Share specs across your team. One person's best practice benefits everyone |\n| **Session Persistence** | Work traces persist in your repo. AI remembers project context across sessions |\n\n## Quick Start\n\n```bash\n# 1. Install globally\nnpm install -g @mindfoldhq/trellis@latest\n\n# 2. Initialize in your project directory\ntrellis init -u your-name\n\n# Or include iFlow CLI support\ntrellis init --iflow -u your-name\n\n# Or include Codex skills support\ntrellis init --codex -u your-name\n\n# 3. Start Claude Code and begin working\n```\n\n> `your-name` becomes your identifier and creates a personal workspace at `.trellis/workspace/your-name/`\n\n<p align=\"center\">\n<img src=\"assets/info.png\" alt=\"Trellis Initialization Example\" />\n</p>\n\n## Use Cases\n\n### Educating Your AI\n\nWrite your specs in Markdown. Trellis injects them into every AI session ‚Äî no more repeating yourself.\n\n<p align=\"center\">\n<img src=\"assets/usecase1.png\" alt=\"Teaching AI - Teach Once, Apply Forever\" />\n</p>\n\nDefine your component guidelines, file structure rules, and patterns once. AI automatically applies them when creating new code ‚Äî using TypeScript with Props interface, following PascalCase naming, building functional components with hooks.\n\n### Ship in Parallel\n\nSpawn multiple Claude sessions in isolated worktrees with `/trellis:parallel`. Work on several features at once, merge when ready.\n\n<p align=\"center\">\n<img src=\"assets/usecase2.png\" alt=\"Parallel Work - Multiple features developing simultaneously\" />\n</p>\n\nWhile coding, each worker runs in its own worktree (physically isolated directory), no blocking, no interference. Review and merge completed features while others are still in progress.\n\n### Custom Workflows\n\nDefine custom skills & commands that prepare Claude for specific tasks and contexts.\n\n<p align=\"center\">\n<img src=\"assets/usecase3.png\" alt=\"Workflows - Custom commands for instant context loading\" />\n</p>\n\nCreate commands like `/trellis:before-frontend-dev` that load component guidelines, check recent changes, pull in test patterns, and review shared hooks‚Äîall with a single slash.\n\n## How It Works\n\n### Project Structure\n\n```\n.trellis/\n‚îú‚îÄ‚îÄ workflow.md              # Workflow guide (auto-injected on start)\n‚îú‚îÄ‚îÄ worktree.yaml            # Multi-agent config (for /trellis:parallel)\n‚îú‚îÄ‚îÄ spec/                    # Spec library\n‚îÇ   ‚îú‚îÄ‚îÄ frontend/            #   Frontend specs\n‚îÇ   ‚îú‚îÄ‚îÄ backend/             #   Backend specs\n‚îÇ   ‚îî‚îÄ‚îÄ guides/              #   Decision & analysis frameworks\n‚îú‚îÄ‚îÄ workspace/{name}/        # Personal journal\n‚îú‚îÄ‚îÄ tasks/                   # Task management (progress tracking & more)\n‚îî‚îÄ‚îÄ scripts/                 # Utilities\n\n.claude/\n‚îú‚îÄ‚îÄ settings.json            # Hook configuration\n‚îú‚îÄ‚îÄ agents/                  # Agent definitions\n‚îÇ   ‚îú‚îÄ‚îÄ dispatch.md          #   Dispatch Agent (pure routing, doesn't read specs)\n‚îÇ   ‚îú‚îÄ‚îÄ implement.md         #   Implement Agent\n‚îÇ   ‚îú‚îÄ‚îÄ check.md             #   Check Agent\n‚îÇ   ‚îî‚îÄ‚îÄ research.md          #   Research Agent\n‚îú‚îÄ‚îÄ commands/                # Slash commands\n‚îî‚îÄ‚îÄ hooks/                   # Hook scripts\n ",
      "default_branch": "main"
    },
    "fetched_at": "2026-02-22T03:32:24.215632"
  },
  {
    "basic_info": {
      "name": "DeepSeek-OCR-2",
      "full_name": "deepseek-ai/DeepSeek-OCR-2",
      "owner": "deepseek-ai",
      "description": "Visual Causal Flow",
      "url": "https://github.com/deepseek-ai/DeepSeek-OCR-2",
      "clone_url": "https://github.com/deepseek-ai/DeepSeek-OCR-2.git",
      "ssh_url": "git@github.com:deepseek-ai/DeepSeek-OCR-2.git",
      "homepage": "",
      "created_at": "2026-01-27T03:05:42Z",
      "updated_at": "2026-02-22T03:26:50Z",
      "pushed_at": "2026-02-03T00:34:18Z"
    },
    "stats": {
      "stars": 2319,
      "forks": 179,
      "watchers": 2319,
      "open_issues": 46,
      "size": 1075
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 107443
      },
      "license": "Apache License 2.0",
      "topics": []
    },
    "content": {
      "readme": "<!-- markdownlint-disable first-line-h1 -->\n<!-- markdownlint-disable html -->\n<!-- markdownlint-disable no-duplicate-header -->\n\n\n<div align=\"center\">\n  <img src=\"assets/logo.svg\" width=\"60%\" alt=\"DeepSeek AI\" />\n</div>\n\n\n<hr>\n<div align=\"center\">\n  <a href=\"https://www.deepseek.com/\" target=\"_blank\">\n    <img alt=\"Homepage\" src=\"assets/badge.svg\" />\n  </a>\n  <a href=\"https://huggingface.co/deepseek-ai/DeepSeek-OCR-2\" target=\"_blank\">\n    <img alt=\"Hugging Face\" src=\"https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-DeepSeek%20AI-ffc107?color=ffc107&logoColor=white\" />\n  </a>\n\n</div>\n\n<div align=\"center\">\n\n  <a href=\"https://discord.gg/Tc7c45Zzu5\" target=\"_blank\">\n    <img alt=\"Discord\" src=\"https://img.shields.io/badge/Discord-DeepSeek%20AI-7289da?logo=discord&logoColor=white&color=7289da\" />\n  </a>\n  <a href=\"https://twitter.com/deepseek_ai\" target=\"_blank\">\n    <img alt=\"Twitter Follow\" src=\"https://img.shields.io/badge/Twitter-deepseek_ai-white?logo=x&logoColor=white\" />\n  </a>\n\n</div>\n\n\n\n<p align=\"center\">\n  <a href=\"https://huggingface.co/deepseek-ai/DeepSeek-OCR-2\"><b>üì• Model Download</b></a> |\n  <a href=\"https://github.com/deepseek-ai/DeepSeek-OCR-2/blob/main/DeepSeek_OCR2_paper.pdf\"><b>üìÑ Paper Link</b></a> |\n  <a href=\"https://arxiv.org/abs/2601.20552\"><b>üìÑ Arxiv Paper Link</b></a> |\n</p>\n\n<h2>\n<p align=\"center\">\n  <a href=\"\">DeepSeek-OCR 2: Visual Causal Flow</a>\n</p>\n</h2>\n\n<p align=\"center\">\n<img src=\"assets/fig1.png\" style=\"width: 600px\" align=center>\n</p>\n<p align=\"center\">\n<a href=\"\">Explore more human-like visual encoding.</a>       \n</p>\n\n\n## Contents\n- [Install](#install)\n- [vLLM Inference](#vllm-inference)\n- [Transformers Inference](#transformers-inference)\n  \n\n\n\n\n## Install\n>Our environment is cuda11.8+torch2.6.0.\n1. Clone this repository and navigate to the DeepSeek-OCR-2 folder\n```bash\ngit clone https://github.com/deepseek-ai/DeepSeek-OCR-2.git\n```\n2. Conda\n```Shell\nconda create -n deepseek-ocr2 python=3.12.9 -y\nconda activate deepseek-ocr2\n```\n3. Packages\n\n- download the vllm-0.8.5 [whl](https://github.com/vllm-project/vllm/releases/tag/v0.8.5) \n```Shell\npip install torch==2.6.0 torchvision==0.21.0 torchaudio==2.6.0 --index-url https://download.pytorch.org/whl/cu118\npip install vllm-0.8.5+cu118-cp38-abi3-manylinux1_x86_64.whl\npip install -r requirements.txt\npip install flash-attn==2.7.3 --no-build-isolation\n```\n**Note:** if you want vLLM and transformers codes to run in the same environment, you don't need to worry about this installation error like: vllm 0.8.5+cu118 requires transformers>=4.51.1\n\n## vLLM-Inference\n- VLLM:\n>**Note:** change the INPUT_PATH/OUTPUT_PATH and other settings in the DeepSeek-OCR2-master/DeepSeek-OCR2-vllm/config.py\n```Shell\ncd DeepSeek-OCR2-master/DeepSeek-OCR2-vllm\n```\n1. image: streaming output\n```Shell\npython run_dpsk_ocr2_image.py\n```\n2. pdf: concurrency (on-par speed with DeepSeek-OCR)\n```Shell\npython run_dpsk_ocr2_pdf.py\n```\n3. batch eval for benchmarks (i.e., OmniDocBench v1.5)\n```Shell\npython run_dpsk_ocr2_eval_batch.py\n```\n\n## Transformers-Inference\n- Transformers\n```python\nfrom transformers import AutoModel, AutoTokenizer\nimport torch\nimport os\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\nmodel_name = 'deepseek-ai/DeepSeek-OCR-2'\n\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\nmodel = AutoModel.from_pretrained(model_name, _attn_implementation='flash_attention_2', trust_remote_code=True, use_safetensors=True)\nmodel = model.eval().cuda().to(torch.bfloat16)\n\n# prompt = \"<image>\\nFree OCR. \"\nprompt = \"<image>\\n<|grounding|>Convert the document to markdown. \"\nimage_file = 'your_image.jpg'\noutput_path = 'your/output/dir'\n\nres = model.infer(tokenizer, prompt=prompt, image_file=image_file, output_path = output_path, base_size = 1024, image_size = 768, crop_mode=True, save_results = True)\n```\nor you can\n```Shell\ncd DeepSeek-OCR2-master/DeepSeek-OCR2-hf\npython run_dpsk_ocr2.py\n```\n## Support-Modes\n- Dynamic resolution\n  - Default: (0-6)√ó768√ó768 + 1√ó1024√ó1024 ‚Äî (0-6)√ó144 + 256 visual tokens ‚úÖ\n\n## Main Prompts\n```python\n# document: <image>\\n<|grounding|>Convert the document to markdown.\n# without layouts: <image>\\nFree OCR.\n```\n\n\n\n\n## Acknowledgement\n\nWe would like to thank [DeepSeek-OCR](https://github.com/deepseek-ai/DeepSeek-OCR/), [Vary](https://github.com/Ucas-HaoranWei/Vary/), [GOT-OCR2.0](https://github.com/Ucas-HaoranWei/GOT-OCR2.0/), [MinerU](https://github.com/opendatalab/MinerU), [PaddleOCR](https://github.com/PaddlePaddle/PaddleOCR) for their valuable models.\n\nWe also appreciate the benchmark [OmniDocBench](https://github.com/opendatalab/OmniDocBench).\n\n## Citation\n\n```bibtex\n@article{wei2025deepseek,\n  title={DeepSeek-OCR: Contexts Optical Compression},\n  author={Wei, Haoran and Sun, Yaofeng and Li, Yukun},\n  journal={arXiv preprint arXiv:2510.18234},\n  year={2025}\n}\n@article{wei2026deepseek,\n  title={DeepSeek-OCR 2: Visual Causal Flow},\n  author={Wei, Haoran and Sun, Yaofeng and Li, Yukun},\n  journal={arXiv pr",
      "default_branch": "main"
    },
    "fetched_at": "2026-02-22T03:32:25.351107"
  },
  {
    "basic_info": {
      "name": "GLM-OCR",
      "full_name": "zai-org/GLM-OCR",
      "owner": "zai-org",
      "description": "GLM-OCR: Accurate √ó  Fast √ó Comprehensive",
      "url": "https://github.com/zai-org/GLM-OCR",
      "clone_url": "https://github.com/zai-org/GLM-OCR.git",
      "ssh_url": "git@github.com:zai-org/GLM-OCR.git",
      "homepage": "",
      "created_at": "2026-02-02T12:59:43Z",
      "updated_at": "2026-02-22T02:16:56Z",
      "pushed_at": "2026-02-12T13:12:21Z"
    },
    "stats": {
      "stars": 1710,
      "forks": 120,
      "watchers": 1710,
      "open_issues": 35,
      "size": 100609
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 455938,
        "TypeScript": 306509,
        "CSS": 10590,
        "Shell": 6033,
        "Dockerfile": 3009,
        "HTML": 555
      },
      "license": "Apache License 2.0",
      "topics": [
        "glm",
        "image2text",
        "ocr"
      ]
    },
    "content": {
      "readme": "## GLM-OCR\n\n<div align=\"center\">\n<img src=resources/logo.svg width=\"40%\"/>\n</div>\n<p align=\"center\">\n    üëã Join our <a href=\"resources/WECHAT.md\" target=\"_blank\">WeChat</a> and <a href=\"https://discord.gg/QR7SARHRxK\" target=\"_blank\">Discord</a> community\n    <br>\n    üìç Use GLM-OCR's <a href=\"https://docs.z.ai/guides/vlm/glm-ocr\" target=\"_blank\">API</a>\n</p>\n\n<div align=\"center\">\n  <a href=\"README_zh.md\">ÁÆÄ‰Ωì‰∏≠Êñá</a> | English\n</div>\n\n### Model Introduction\n\nGLM-OCR is a multimodal OCR model for complex document understanding, built on the GLM-V encoder‚Äìdecoder architecture. It introduces Multi-Token Prediction (MTP) loss and stable full-task reinforcement learning to improve training efficiency, recognition accuracy, and generalization. The model integrates the CogViT visual encoder pre-trained on large-scale image‚Äìtext data, a lightweight cross-modal connector with efficient token downsampling, and a GLM-0.5B language decoder. Combined with a two-stage pipeline of layout analysis and parallel recognition based on PP-DocLayout-V3, GLM-OCR delivers robust and high-quality OCR performance across diverse document layouts.\n\n**Key Features**\n\n- **State-of-the-Art Performance**: Achieves a score of 94.62 on OmniDocBench V1.5, ranking #1 overall, and delivers state-of-the-art results across major document understanding benchmarks, including formula recognition, table recognition, and information extraction.\n\n- **Optimized for Real-World Scenarios**: Designed and optimized for practical business use cases, maintaining robust performance on complex tables, code-heavy documents, seals, and other challenging real-world layouts.\n\n- **Efficient Inference**: With only 0.9B parameters, GLM-OCR supports deployment via vLLM, SGLang, and Ollama, significantly reducing inference latency and compute cost, making it ideal for high-concurrency services and edge deployments.\n\n- **Easy to Use**: Fully open-sourced and equipped with a comprehensive [SDK](https://github.com/zai-org/GLM-OCR) and inference toolchain, offering simple installation, one-line invocation, and smooth integration into existing production pipelines.\n\n### News & Updates\n\n- **[Coming Soon]** GLM-OCR Technical Report\n- **[2026.2.12]** Fine-tuning tutorial based on LLaMA-Factory is now available. See: [GLM-OCR Fine-tuning Guide](examples/finetune/README.md)\n\n### Download Model\n\n| Model   | Download Links                                                                                                              | Precision |\n| ------- | --------------------------------------------------------------------------------------------------------------------------- | --------- |\n| GLM-OCR | [ü§ó Hugging Face](https://huggingface.co/zai-org/GLM-OCR)<br> [ü§ñ ModelScope](https://modelscope.cn/models/ZhipuAI/GLM-OCR) | BF16      |\n\n## GLM-OCR SDK\n\nWe provide an SDK for using GLM-OCR more efficiently and conveniently.\n\n### Install SDK\n\n> [UV Installation](https://docs.astral.sh/uv/getting-started/installation/)\n\n```bash\n# Install from source\ngit clone https://github.com/zai-org/glm-ocr.git\ncd glm-ocr\nuv venv --python 3.12 --seed && source .venv/bin/activate\nuv pip install -e .\n```\n\n### Model Deployment\n\nTwo ways to use GLM-OCR:\n\n#### Option 1: Zhipu MaaS API (Recommended for Quick Start)\n\nUse the hosted cloud API ‚Äì no GPU needed. The cloud service runs the complete GLM-OCR pipeline internally, so the SDK simply forwards your request and returns the result.\n\n1. Get an API key from https://open.bigmodel.cn\n2. Configure `config.yaml`:\n\n```yaml\npipeline:\n  maas:\n    enabled: true # Enable MaaS mode\n    api_key: your-api-key # Required\n```\n\nThat's it! When `maas.enabled=true`, the SDK acts as a thin wrapper that:\n\n- Forwards your documents to the Zhipu cloud API\n- Returns the results directly (Markdown + JSON layout details)\n- No local processing, no GPU required\n\nInput note (MaaS): the upstream API accepts `file` as a URL or a `data:<mime>;base64,...` data URI.\nIf you have raw base64 without the `data:` prefix, wrap it as a data URI (recommended). The SDK will\nauto-wrap local file paths / bytes / raw base64 into a data URI when calling MaaS.\n\nAPI documentation: https://docs.bigmodel.cn/cn/guide/models/vlm/glm-ocr\n\n#### Option 2: Self-host with vLLM / SGLang\n\nDeploy the GLM-OCR model locally for full control. The SDK provides the complete pipeline: layout detection, parallel region OCR, and result formatting.\n\n##### Using vLLM\n\nInstall vLLM:\n\n```bash\nuv pip install -U vllm --torch-backend=auto --extra-index-url https://wheels.vllm.ai/nightly\n# Or use Docker\ndocker pull vllm/vllm-openai:nightly\n```\n\nLaunch the service:\n\n```bash\n# In docker container, uv may not be need for transformers install\nuv pip install git+https://github.com/huggingface/transformers.git\n\n# Run with MTP for better performance\nvllm serve zai-org/GLM-OCR --allowed-local-media-path / --port 8080 --speculative-config '{\"method\": \"mtp\", \"num_speculative_tokens\": 1}' --served-model-name glm-ocr\n```\n\n##### Using SGLang\n\nIn",
      "default_branch": "main"
    },
    "fetched_at": "2026-02-22T03:32:26.490616"
  },
  {
    "basic_info": {
      "name": "dash",
      "full_name": "agno-agi/dash",
      "owner": "agno-agi",
      "description": "Self-learning data agent that grounds its answers in 6 layers of context. Inspired by OpenAI's in-house implementation.",
      "url": "https://github.com/agno-agi/dash",
      "clone_url": "https://github.com/agno-agi/dash.git",
      "ssh_url": "git@github.com:agno-agi/dash.git",
      "homepage": "",
      "created_at": "2026-01-30T13:54:17Z",
      "updated_at": "2026-02-22T00:58:12Z",
      "pushed_at": "2026-02-16T23:56:00Z"
    },
    "stats": {
      "stars": 1697,
      "forks": 186,
      "watchers": 1697,
      "open_issues": 7,
      "size": 174
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 52313,
        "Shell": 10886,
        "Dockerfile": 662
      },
      "license": "Apache License 2.0",
      "topics": []
    },
    "content": {
      "readme": "# Dash\n\nDash is a **self-learning data agent** that grounds its answers in **6 layers of context** and improves with every run.\n\nInspired by [OpenAI's in-house data agent](https://openai.com/index/inside-our-in-house-data-agent/).\n\n## Get Started\n\n```sh\n# Clone the repo\ngit clone https://github.com/agno-agi/dash.git && cd dash\n\n# Add OPENAI_API_KEY\ncp example.env .env\n# Edit .env and add your key\n\n# Start the application\ndocker compose up -d --build\n\n# Load sample data and knowledge\ndocker exec -it dash-api python -m dash.scripts.load_data\ndocker exec -it dash-api python -m dash.scripts.load_knowledge\n```\n\nConfirm dash is running at [http://localhost:8000/docs](http://localhost:8000/docs).\n\n## Connect to the Web UI\n\n1. Open [os.agno.com](https://os.agno.com) and login\n2. Add OS ‚Üí Local ‚Üí `http://localhost:8000`\n3. Click \"Connect\"\n\n**Try it** (sample F1 dataset):\n\n- Who won the most F1 World Championships?\n- How many races has Lewis Hamilton won?\n- Compare Ferrari vs Mercedes points 2015-2020\n\n## Why Text-to-SQL Breaks in Practice\n\nOur goal is simple: ask a question in english, get a correct, meaningful answer. But raw LLMs writing SQL hit a wall fast:\n\n- **Schemas lack meaning.**\n- **Types are misleading.**\n- **Tribal knowledge is missing.**\n- **No way to learn from mistakes.**\n- **Results generally lack interpretation.**\n\nThe root cause is missing context and missing memory.\n\nDash solves this with **6 layers of grounded context**, a **self-learning loop** that improves with every query, and a focus on **understanding your question** to deliver insights you can act on.\n\n## The Six Layers of Context\n\n| Layer | Purpose | Source |\n|------|--------|--------|\n| **Table Usage** | Schema, columns, relationships | `knowledge/tables/*.json` |\n| **Human Annotations** | Metrics, definitions, and business rules | `knowledge/business/*.json` |\n| **Query Patterns** | SQL that is known to work | `knowledge/queries/*.sql` |\n| **Institutional Knowledge** | Docs, wikis, external references | MCP (optional) |\n| **Learnings** | Error patterns and discovered fixes | Agno `Learning Machine` |\n| **Runtime Context** | Live schema changes | `introspect_schema` tool |\n\nThe agent retrieves relevant context at query time via hybrid search, then generates SQL grounded in patterns that already work.\n\n## The Self-Learning Loop\n\nDash improves without retraining or fine-tuning. We call this gpu-poor continuous learning.\n\nIt learns through two complementary systems:\n\n| System | Stores | How It Evolves |\n|------|--------|----------------|\n| **Knowledge** | Validated queries and business context | Curated by you + dash |\n| **Learnings** | Error patterns and fixes | Managed by `Learning Machine` automatically |\n\n```\nUser Question\n     ‚Üì\nRetrieve Knowledge + Learnings\n     ‚Üì\nReason about intent\n     ‚Üì\nGenerate grounded SQL\n     ‚Üì\nExecute and interpret\n     ‚Üì\n ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n ‚Üì         ‚Üì\nSuccess    Error\n ‚Üì         ‚Üì\n ‚Üì         Diagnose ‚Üí Fix ‚Üí Save Learning\n ‚Üì                           (never repeated)\n ‚Üì\nReturn insight\n ‚Üì\nOptionally save as Knowledge\n```\n\n**Knowledge** is curated‚Äîvalidated queries and business context you want the agent to build on.\n\n**Learnings** is discovered‚Äîpatterns the agent finds through trial and error. When a query fails because `position` is TEXT not INTEGER, the agent saves that gotcha. Next time, it knows.\n\n## Insights, Not Just Rows\n\nDash reasons about what makes an answer useful, not just technically correct.\n\n**Question:**\nWho won the most races in 2019?\n\n| Typical SQL Agent | Dash |\n|------------------|------|\n| `Hamilton: 11` | Lewis Hamilton dominated 2019 with **11 wins out of 21 races**, more than double Bottas‚Äôs 4 wins. This performance secured his sixth world championship. |\n\n## Deploy to Railway\n\n```sh\nrailway login\n\n./scripts/railway_up.sh\n```\n\n### Production Operations\n\n**Load data and knowledge:**\n```sh\nrailway run python -m dash.scripts.load_data\nrailway run python -m dash.scripts.load_knowledge\n```\n\n**View logs:**\n\n```sh\nrailway logs --service dash\n```\n\n**Run commands in production:**\n\n```sh\nrailway run python -m dash  # CLI mode\n```\n\n**Redeploy after changes:**\n\n```sh\nrailway up --service dash -d\n```\n\n**Open dashboard:**\n```sh\nrailway open\n```\n\n## Adding Knowledge\n\nDash works best when it understands how your organization talks about data.\n\n```\nknowledge/\n‚îú‚îÄ‚îÄ tables/      # Table meaning and caveats\n‚îú‚îÄ‚îÄ queries/     # Proven SQL patterns\n‚îî‚îÄ‚îÄ business/    # Metrics and language\n```\n\n### Table Metadata\n\n```\n{\n  \"table_name\": \"orders\",\n  \"table_description\": \"Customer orders with denormalized line items\",\n  \"use_cases\": [\"Revenue reporting\", \"Customer analytics\"],\n  \"data_quality_notes\": [\n    \"created_at is UTC\",\n    \"status values: pending, completed, refunded\",\n    \"amount stored in cents\"\n  ]\n}\n```\n\n### Query Patterns\n\n```\n-- <query name>monthly_revenue</query name>\n-- <query description>\n-- Monthly revenue calculation.\n-- Converts cents to dollars.\n-- Excludes refunded orders.\n-- </query description>",
      "default_branch": "main"
    },
    "fetched_at": "2026-02-22T03:32:27.639495"
  },
  {
    "basic_info": {
      "name": "Qwen3-ASR",
      "full_name": "QwenLM/Qwen3-ASR",
      "owner": "QwenLM",
      "description": "Qwen3-ASR is an open-source series of ASR models developed by the Qwen team at Alibaba Cloud, supporting stable multilingual speech/music/song recognition, language detection and timestamp prediction.",
      "url": "https://github.com/QwenLM/Qwen3-ASR",
      "clone_url": "https://github.com/QwenLM/Qwen3-ASR.git",
      "ssh_url": "git@github.com:QwenLM/Qwen3-ASR.git",
      "homepage": null,
      "created_at": "2026-01-28T05:44:59Z",
      "updated_at": "2026-02-22T02:34:50Z",
      "pushed_at": "2026-01-30T03:24:24Z"
    },
    "stats": {
      "stars": 1600,
      "forks": 136,
      "watchers": 1600,
      "open_issues": 33,
      "size": 2336
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 231743
      },
      "license": "Apache License 2.0",
      "topics": []
    },
    "content": {
      "readme": "# Qwen3-ASR\n\n<br>\n\n<p align=\"center\">\n    <img src=\"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-ASR-Repo/logo.png\" width=\"400\"/>\n<p>\n\n<p align=\"center\">\n&nbsp&nbspü§ó <a href=\"https://huggingface.co/collections/Qwen/qwen3-asr\">Hugging Face</a>&nbsp&nbsp | &nbsp&nbspü§ñ <a href=\"https://modelscope.cn/collections/Qwen/Qwen3-ASR\">ModelScope</a>&nbsp&nbsp | &nbsp&nbspüìë <a href=\"https://qwen.ai/blog?id=qwen3asr\">Blog</a>&nbsp&nbsp | &nbsp&nbspüìë <a href=\"https://arxiv.org/abs/2601.21337\">Paper</a>&nbsp&nbsp\n<br>\nüñ•Ô∏è <a href=\"https://huggingface.co/spaces/Qwen/Qwen3-ASR\">Hugging Face Demo</a>&nbsp&nbsp | &nbsp&nbsp üñ•Ô∏è <a href=\"https://modelscope.cn/studios/Qwen/Qwen3-ASR\">ModelScope Demo</a>&nbsp&nbsp | &nbsp&nbspüí¨ <a href=\"https://github.com/QwenLM/Qwen/blob/main/assets/wechat.png\">WeChat (ÂæÆ‰ø°)</a>&nbsp&nbsp | &nbsp&nbspü´® <a href=\"https://discord.gg/CV4E9rpNSD\">Discord</a>&nbsp&nbsp | &nbsp&nbspüìë <a href=\"https://help.aliyun.com/zh/model-studio/qwen-speech-recognition\">API</a>\n\n</p>\n\nWe release **Qwen3-ASR**, a family that includes two powerful all-in-one speech recognition models that support language identification and ASR for 52 languages and dialects, as well as a novel non-autoregressive speech forced-alignment model that can align text‚Äìspeech pairs in 11 languages.\n\n\n## News\n* 2026.1.29: üéâüéâüéâ We have released the [Qwen3-ASR](https://huggingface.co/collections/Qwen/qwen3-asr) series (0.6B/1.7B) and the Qwen3-ForcedAligner-0.6B model. Please check out our [blog](https://qwen.ai/blog?id=qwen3asr)!\n\n\n## Contents <!-- omit in toc -->\n\n- [Overview](#overview)\n  - [Introduction](#introduction)\n  - [Model Architecture](#model-architecture)\n  - [Released Models Description and Download](#released-models-description-and-download)\n- [Quickstart](#quickstart)\n  - [Environment Setup](#environment-setup)\n  - [Python Package Usage](#python-package-usage)\n    - [Quick Inference](#quick-inference)\n    - [vLLM Backend](#vllm-backend)\n    - [Streaming Inference](#streaming-inference)\n    - [ForcedAligner Usage](#forcedaligner-usage)\n  - [DashScope API Usage](#dashscope-api-usage)\n- [Launch Local Web UI Demo](#launch-local-web-ui-demo)\n  - [Gradio Demo](#gradio-demo)\n  - [Streaming Demo](#streaming-demo)\n- [Deployment with vLLM](#deployment-with-vllm)\n- [Fine Tuning](#fine-tuning)\n- [Docker](#docker)\n- [Evaluation](#evaluation)\n- [Citation](#citation)\n\n\n## Overview\n\n### Introduction\n\n<p align=\"center\">\n    <img src=\"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-ASR-Repo/qwen3_asr_introduction.png\" width=\"90%\"/>\n<p>\n\nThe Qwen3-ASR family includes Qwen3-ASR-1.7B and Qwen3-ASR-0.6B, which support language identification and ASR for 52 languages and dialects. Both leverage large-scale speech training data and the strong audio understanding capability of their foundation model, Qwen3-Omni. Experiments show that the 1.7B version achieves state-of-the-art performance among open-source ASR models and is competitive with the strongest proprietary commercial APIs. Here are the main features:\n\n* **All-in-one**: Qwen3-ASR-1.7B and Qwen3-ASR-0.6B support language identification and speech recognition for 30 languages and 22 Chinese dialects, so as to English accents from multiple countries and regions.\n\n* **Excellent and Fast**: The Qwen3-ASR family ASR models maintains high-quality and robust recognition under complex acoustic environments and challenging text patterns. Qwen3-ASR-1.7B achieves strong performance on both open-sourced and internal benchmarks. While the 0.6B version achieves accuracy-efficient trade-off, it reaches 2000 times throughput at a concurrency of 128. They both achieve streaming / offline unified inference with single model and support transcribe long audio.\n\n* **Novel and strong forced alignment Solution**: We introduce Qwen3-ForcedAligner-0.6B, which supports timestamp prediction for arbitrary units within up to 5 minutes of speech in 11 languages. Evaluations show its timestamp accuracy surpasses E2E based forced-alignment models.\n\n* **Comprehensive inference toolkit**: In addition to open-sourcing the architectures and weights of the Qwen3-ASR series, we also release a powerful, full-featured inference framework that supports vLLM-based batch inference, asynchronous serving, streaming inference, timestamp prediction, and more.\n\n### Model Architecture\n\n<p align=\"center\">\n    <img src=\"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-ASR-Repo/overview.jpg\" width=\"100%\"/>\n<p>\n\n\n### Released Models Description and Download\n\nBelow is an introduction and download information for the Qwen3-ASR models. Please select and download the model that fits your needs.\n\n| Model | Supported Languages | Supported Dialects | Inference Mode | Audio Types |\n|---|---|---|---|---|\n| Qwen3-ASR-1.7B & Qwen3-ASR-0.6B | Chinese (zh), English (en), Cantonese (yue), Arabic (ar), German (de), French (fr), Spanish (es), Portuguese (pt), Indonesian (id), Italian (it), Korean (ko), Russian (ru), Thai (th), Vietnamese (vi), Japan",
      "default_branch": "main"
    },
    "fetched_at": "2026-02-22T03:32:28.829433"
  },
  {
    "basic_info": {
      "name": "claude-seo",
      "full_name": "AgriciDaniel/claude-seo",
      "owner": "AgriciDaniel",
      "description": " Universal SEO skill for Claude Code. Comprehensive SEO analysis for any website or business type.",
      "url": "https://github.com/AgriciDaniel/claude-seo",
      "clone_url": "https://github.com/AgriciDaniel/claude-seo.git",
      "ssh_url": "git@github.com:AgriciDaniel/claude-seo.git",
      "homepage": "https://www.skool.com/ai-marketing-hub-pro",
      "created_at": "2026-02-07T08:17:38Z",
      "updated_at": "2026-02-22T02:06:04Z",
      "pushed_at": "2026-02-19T19:29:57Z"
    },
    "stats": {
      "stars": 1024,
      "forks": 152,
      "watchers": 1024,
      "open_issues": 4,
      "size": 1912
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 25940,
        "Shell": 8812,
        "PowerShell": 7230
      },
      "license": "MIT License",
      "topics": [
        "ai-tools",
        "claude",
        "claude-code",
        "core-web-vitals",
        "schema-markup",
        "seo",
        "seo-audit",
        "seo-tools"
      ]
    },
    "content": {
      "readme": "<!-- Updated: 2026-02-08 -->\n\n![Claude SEO](screenshots/cover-image.jpeg)\n\n# Claude SEO\n\nComprehensive SEO analysis skill for Claude Code. Covers technical SEO, on-page analysis, content quality (E-E-A-T), schema markup, image optimization, sitemap architecture, AI search optimization (GEO), and strategic planning.\n\n![SEO Command Demo](screenshots/seo-command-demo.gif)\n\n[![Claude Code Skill](https://img.shields.io/badge/Claude%20Code-Skill-blue)](https://claude.ai/claude-code)\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)\n\n## Installation\n\n### One-Command Install (Unix/macOS/Linux)\n\n```bash\ncurl -fsSL https://raw.githubusercontent.com/AgriciDaniel/claude-seo/main/install.sh | bash\n```\n\n### Manual Install\n\n```bash\ngit clone https://github.com/AgriciDaniel/claude-seo.git\ncd claude-seo\n./install.sh\n```\n\n### Windows\n\n```powershell\nirm https://raw.githubusercontent.com/AgriciDaniel/claude-seo/main/install.ps1 | iex\n```\n\n## Quick Start\n\n```bash\n# Start Claude Code\nclaude\n\n# Run a full site audit\n/seo audit https://example.com\n\n# Analyze a single page\n/seo page https://example.com/about\n\n# Check schema markup\n/seo schema https://example.com\n\n# Generate a sitemap\n/seo sitemap generate\n\n# Optimize for AI search\n/seo geo https://example.com\n```\n### Demo:\n[Watch the full demo on YouTube](https://www.youtube.com/watch?v=COMnNlUakQk)\n\n**`/seo audit` ‚Äî full site audit with parallel subagents:**\n\n![SEO Audit Demo](screenshots/seo-audit-demo.gif)\n\n## Commands\n\n| Command | Description |\n|---------|-------------|\n| `/seo audit <url>` | Full website audit with parallel subagent delegation |\n| `/seo page <url>` | Deep single-page analysis |\n| `/seo sitemap <url>` | Analyze existing XML sitemap |\n| `/seo sitemap generate` | Generate new sitemap with industry templates |\n| `/seo schema <url>` | Detect, validate, and generate Schema.org markup |\n| `/seo images <url>` | Image optimization analysis |\n| `/seo technical <url>` | Technical SEO audit (8 categories) |\n| `/seo content <url>` | E-E-A-T and content quality analysis |\n| `/seo geo <url>` | AI Overviews / Generative Engine Optimization |\n| `/seo plan <type>` | Strategic SEO planning (saas, local, ecommerce, publisher, agency) |\n| `/seo programmatic <url>` | Programmatic SEO analysis and planning |\n| `/seo competitor-pages <url>` | Competitor comparison page generation |\n| `/seo hreflang <url>` | Hreflang/i18n SEO audit and generation |\n\n### `/seo programmatic [url|plan]`\n**Programmatic SEO Analysis & Planning**\n\nBuild SEO pages at scale from data sources with quality safeguards.\n\n**Capabilities:**\n- Analyze existing programmatic pages for thin content and cannibalization\n- Plan URL patterns and template structures for data-driven pages\n- Internal linking automation between generated pages\n- Canonical strategy and index bloat prevention\n- Quality gates: ‚ö†Ô∏è WARNING at 100+ pages, üõë HARD STOP at 500+ without audit\n\n### `/seo competitor-pages [url|generate]`\n**Competitor Comparison Page Generator**\n\nCreate high-converting \"X vs Y\" and \"alternatives to X\" pages.\n\n**Capabilities:**\n- Structured comparison tables with feature matrices\n- Product schema markup with AggregateRating\n- Conversion-optimized layouts with CTA placement\n- Keyword targeting for comparison intent queries\n- Fairness guidelines for accurate competitor representation\n\n### `/seo hreflang [url]`\n**Hreflang / i18n SEO Audit & Generation**\n\nValidate and generate hreflang tags for multi-language sites.\n\n**Capabilities:**\n- Generate hreflang tags (HTML, HTTP headers, or XML sitemap)\n- Validate self-referencing tags, return tags, x-default\n- Detect common mistakes (missing returns, invalid codes, HTTP/HTTPS mismatch)\n- Cross-domain hreflang support\n- Language/region code validation (ISO 639-1 + ISO 3166-1)\n\n## Features\n\n### Core Web Vitals (Current Metrics)\n- **LCP** (Largest Contentful Paint): Target < 2.5s\n- **INP** (Interaction to Next Paint): Target < 200ms\n- **CLS** (Cumulative Layout Shift): Target < 0.1\n\n> Note: INP replaced FID on March 12, 2024. FID was fully removed from all Chrome tools on September 9, 2024.\n\n### E-E-A-T Analysis\nUpdated to September 2025 Quality Rater Guidelines:\n- **Experience**: First-hand knowledge signals\n- **Expertise**: Author credentials and depth\n- **Authoritativeness**: Industry recognition\n- **Trustworthiness**: Contact info, security, transparency\n\n### Schema Markup\n- Detection: JSON-LD (preferred), Microdata, RDFa\n- Validation against Google's supported types\n- Generation with templates\n- Deprecation awareness:\n  - HowTo: Deprecated (Sept 2023)\n  - FAQ: Restricted to gov/health sites (Aug 2023)\n  - SpecialAnnouncement: Deprecated (July 2025)\n\n### AI Search Optimization (GEO)\nNew for 2026 - optimize for:\n- Google AI Overviews\n- ChatGPT web search\n- Perplexity\n- Other AI-powered search\n\n### Quality Gates\n- Warning at 30+ location pages\n- Hard stop at 50+ location pages\n- Thin content detection per page type\n- Doorway page prevention\n\n## Architectur",
      "default_branch": "main"
    },
    "fetched_at": "2026-02-22T03:32:29.954945"
  },
  {
    "basic_info": {
      "name": "claw-compactor",
      "full_name": "aeromomo/claw-compactor",
      "owner": "aeromomo",
      "description": "ü¶û Claw Compactor ‚Äî The 98% Crusher. Cut your AI agent token spend in half with 5 layered compression techniques.",
      "url": "https://github.com/aeromomo/claw-compactor",
      "clone_url": "https://github.com/aeromomo/claw-compactor.git",
      "ssh_url": "git@github.com:aeromomo/claw-compactor.git",
      "homepage": null,
      "created_at": "2026-02-10T00:27:08Z",
      "updated_at": "2026-02-22T02:17:46Z",
      "pushed_at": "2026-02-12T03:08:57Z"
    },
    "stats": {
      "stars": 967,
      "forks": 81,
      "watchers": 967,
      "open_issues": 1,
      "size": 1006
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 329981
      },
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "# Claw Compactor\n![Claw Compactor Banner](assets/banner.png)\n\n[![Build](https://img.shields.io/badge/build-passing-brightgreen)](https://github.com/aeromomo/claw-compactor) [![Release](https://img.shields.io/github/v/release/aeromomo/claw-compactor?color=blue)](https://github.com/aeromomo/claw-compactor/releases) [![Tests](https://img.shields.io/badge/tests-800%20passed-brightgreen)](https://github.com/aeromomo/claw-compactor) [![Python](https://img.shields.io/badge/python-3.9%2B-blue)](https://python.org) [![License](https://img.shields.io/badge/license-MIT-purple)](LICENSE) [![OpenClaw](https://img.shields.io/badge/OpenClaw-skill-orange)](https://openclaw.ai)\n\n*\"Cut your tokens. Keep your facts.\"*\n\n**Cut your AI agent's token spend in half.** One command compresses your entire workspace ‚Äî memory files, session transcripts, sub-agent context ‚Äî using 5 layered compression techniques. Deterministic. Mostly lossless. No LLM required.\n\n## Features\n- **5 compression layers** working in sequence for maximum savings\n- **Zero LLM cost** ‚Äî all compression is rule-based and deterministic\n- **Lossless roundtrip** for dictionary, RLE, and rule-based compression\n- **~97% savings** on session transcripts via observation extraction\n- **Tiered summaries** (L0/L1/L2) for progressive context loading\n- **CJK-aware** ‚Äî full Chinese/Japanese/Korean support\n- **One command** (`full`) runs everything in optimal order\n\n## 5 Compression Layers\n| 1 | Rule engine | Dedup lines, strip markdown filler, merge sections | 4-8% | |\n| 2 | Dictionary encoding | Auto-learned codebook, `$XX` substitution | 4-5% | |\n| 3 | Observation compression | Session JSONL ‚Üí structured summaries | ~97% | * |\n| 4 | RLE patterns | Path shorthand (`$WS`), IP prefix, enum compaction | 1-2% | |\n| 5 | Compressed Context Protocol | ultra/medium/light abbreviation | 20-60% | * |\n\n\\*Lossy techniques preserve all facts and decisions; only verbose formatting is removed.\n\n## Quick Start\n```bash\ngit clone https://github.com/aeromomo/claw-compactor.git\ncd claw-compactor\n\n# See how much you'd save (non-destructive)\npython3 scripts/mem_compress.py /path/to/workspace benchmark\n\n# Compress everything\npython3 scripts/mem_compress.py /path/to/workspace full\n```\n\n**Requirements:** Python 3.9+. Optional: `pip install tiktoken` for exact token counts (falls back to heuristic).\n\n## Architecture\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ mem_compress.py ‚îÇ\n‚îÇ (unified entry point) ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ\n ‚ñº ‚ñº ‚ñº ‚ñº ‚ñº ‚ñº ‚ñº ‚ñº\n estimate compress dict dedup observe tiers audit optimize\n ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n ‚ñº\n ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n ‚îÇ lib/ ‚îÇ\n ‚îÇ tokens.py ‚îÇ ‚Üê tiktoken or heuristic\n ‚îÇ markdown.py ‚îÇ ‚Üê section parsing\n ‚îÇ dedup.py ‚îÇ ‚Üê shingle hashing\n ‚îÇ dictionary.py ‚îÇ ‚Üê codebook compression\n ‚îÇ rle.py ‚îÇ ‚Üê path/IP/enum encoding\n ‚îÇ tokenizer_ ‚îÇ\n ‚îÇ optimizer.py ‚îÇ ‚Üê format optimization\n ‚îÇ config.py ‚îÇ ‚Üê JSON config\n ‚îÇ exceptions.py ‚îÇ ‚Üê error types\n ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n\n## Commands\nAll commands: `python3 scripts/mem_compress.py <workspace> <command> [options]`\n\n`full`, Description=Complete pipeline (all steps in order), Typical Savings=50%+ combined\n`benchmark`, Description=Dry-run performance report, Typical Savings=‚Äî\n`compress`, Description=Rule-based compression, Typical Savings=4-8%\n`dict`, Description=Dictionary encoding with auto-codebook, Typical Savings=4-5%\n`observe`, Description=Session transcript ‚Üí observations, Typical Savings=~97%\n`tiers`, Description=Generate L0/L1/L2 summaries, Typical Savings=88-95% on sub-agent loads\n`dedup`, Description=Cross-file duplicate detection, Typical Savings=varies\n`estimate`, Description=Token count report, Typical Savings=‚Äî\n`audit`, Description=Workspace health check, Typical Savings=‚Äî\n`optimize`, Description=Tokenizer-level format fixes, Typical Savings=1-3%\n\n### Global Options\n- `--json` ‚Äî Machine-readable JSON output\n- `--dry-run` ‚Äî Preview changes without writing\n- `--since YYYY-MM-DD` ‚Äî Filter sessions by date\n- `--auto-merge` ‚Äî Auto-merge duplicates (dedup)\n\n## Real-World Savings\nSession transcripts (observe), Typical Savings=**~97%**, Notes=Megabytes of JSONL ‚Üí concise observation MD\nVerbose/new workspace, Typical Savings=**50-70%**, Notes=First run on unoptimized workspace\nRegular maintenance, Typical Savings=**10-20%**, Notes=Weekly runs on active workspace\nAlready-optimized, Typical Savings=**3-12%**, Notes=Diminishing returns ‚Äî workspace is clean\n\n## cacheRetention ‚Äî Complementary Optimization\nBefore compression runs, enable **prompt caching** for a 90% discount on cached tokens:\n\n```json\n{\n \"agents\": {\n \"defaults\": {\n \"models\": {\n  \"anthropic/claude-opus-4-6\": {\n   \"params\": {\n    \"cacheRetention\": \"long\"\n   }\n  }\n }\n }\n }\n\nCompression reduces token count, caching reduces cost-per-token. Together: 50% compression + 90% cache discount = **95% effective cost reduction**.\n\n## Heartbeat Automation\nRun weekly or on heartbeat:\n\n```markdown\n\n#",
      "default_branch": "main"
    },
    "fetched_at": "2026-02-22T03:32:31.107783"
  },
  {
    "basic_info": {
      "name": "skill-scanner",
      "full_name": "cisco-ai-defense/skill-scanner",
      "owner": "cisco-ai-defense",
      "description": "Security Scanner for Agent Skills",
      "url": "https://github.com/cisco-ai-defense/skill-scanner",
      "clone_url": "https://github.com/cisco-ai-defense/skill-scanner.git",
      "ssh_url": "git@github.com:cisco-ai-defense/skill-scanner.git",
      "homepage": "https://blogs.cisco.com/ai/personal-ai-agents-like-moltbot-are-a-security-nightmare",
      "created_at": "2026-01-29T01:31:32Z",
      "updated_at": "2026-02-22T03:22:30Z",
      "pushed_at": "2026-02-21T03:47:28Z"
    },
    "stats": {
      "stars": 918,
      "forks": 102,
      "watchers": 918,
      "open_issues": 8,
      "size": 875
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 1911874,
        "YARA": 56120,
        "Ruby": 23517,
        "Shell": 4043,
        "Makefile": 2546
      },
      "license": "Other",
      "topics": [
        "agent",
        "agent-skills",
        "security"
      ]
    },
    "content": {
      "readme": "# Skill Scanner\n\n[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)\n[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)\n[![PyPI version](https://img.shields.io/pypi/v/cisco-ai-skill-scanner.svg)](https://pypi.org/project/cisco-ai-skill-scanner/)\n[![CI](https://github.com/cisco-ai-defense/skill-scanner/actions/workflows/python-tests.yml/badge.svg)](https://github.com/cisco-ai-defense/skill-scanner/actions/workflows/python-tests.yml)\n[![Discord](https://img.shields.io/badge/Discord-Join%20Us-7289da?logo=discord&logoColor=white)](https://discord.com/invite/nKWtDcXxtx)\n[![Cisco AI Defense](https://img.shields.io/badge/Cisco-AI%20Defense-049fd9?logo=cisco&logoColor=white)](https://www.cisco.com/site/us/en/products/security/ai-defense/index.html)\n[![AI Security Framework](https://img.shields.io/badge/AI%20Security-Framework-orange)](https://learn-cloudsecurity.cisco.com/ai-security-framework)\n[![Ask DeepWiki](https://deepwiki.com/badge.svg)](https://deepwiki.com/cisco-ai-defense/skill-scanner)\n\nA best-effort security scanner for AI Agent Skills that detects prompt injection, data exfiltration, and malicious code patterns. Combines **pattern-based detection** (YAML + YARA), **LLM-as-a-judge**, and **behavioral dataflow analysis** to maximize detection coverage of probable threats while minimizing false positives.\n\n> **Important:** This scanner provides best-effort detection, not comprehensive or complete coverage. A scan that returns no findings does not guarantee that a skill is free of all threats. See [Scope and Limitations](#scope-and-limitations) below.\n\nSupports [OpenAI Codex Skills](https://openai.github.io/codex/) and [Cursor Agent Skills](https://docs.cursor.com/context/rules) formats following the [Agent Skills specification](https://agentskills.io).\n\n---\n\n## Highlights\n\n- **Multi-Engine Detection** - Static analysis, behavioral dataflow, LLM semantic analysis, and cloud-based scanning for layered, best-effort coverage\n- **False Positive Filtering** - Meta-analyzer significantly reduces noise while preserving detection capability\n- **CI/CD Ready** - SARIF output for GitHub Code Scanning, exit codes for build failures\n- **Extensible** - Plugin architecture for custom analyzers\n\n**[Join the Cisco AI Discord](https://discord.com/invite/nKWtDcXxtx)** to discuss, share feedback, or connect with the team.\n\n---\n\n## Scope and Limitations\n\nSkill Scanner is a detection tool. It identifies known and probable risk patterns, but it does not certify security.\n\n**Key limitations:**\n\n- **No findings ‚â† no risk.** A scan that returns \"No findings\" indicates that no known threat patterns were detected. It does not guarantee that a skill is secure, benign, or free of vulnerabilities.\n- **Coverage is inherently incomplete.** The scanner combines signature-based detection, LLM-based semantic analysis, behavioral dataflow analysis, optional cloud services, and configurable rule packs. While this approach improve coverage, no automated tool can detect every technique, especially novel or zero-day attacks.\n- **False positives and false negatives can occur.** Consensus modes and meta-analysis reduce noise, but no configuration eliminates all incorrect classifications. Tune the [scan policy](docs/scan-policy.md) to your risk tolerance.\n- **Human review remains essential.** Automated scanning is one component of a defense-in-depth strategy. High-risk or production deployments should pair scanner results with manual code review and/or  threat modeling.\n\n---\n\n## Documentation\n\n| Guide | Description |\n|-------|-------------|\n| [Quick Start](docs/quickstart.md) | Get started in 5 minutes |\n| [Architecture](docs/architecture.md) | System design and components |\n| [Threat Taxonomy](docs/threat-taxonomy.md) | Complete AITech threat taxonomy with examples |\n| [LLM Analyzer](docs/llm-analyzer.md) | LLM configuration and usage |\n| [Meta-Analyzer](docs/meta-analyzer.md) | False positive filtering and prioritization |\n| [Behavioral Analyzer](docs/behavioral-analyzer.md) | Dataflow analysis details |\n| [Scan Policy](docs/scan-policy.md) | Custom policies, presets, and tuning guide |\n| [Policy Quick Reference](docs/POLICY.md) | Compact reference for policy sections and knobs |\n| [Rule Authoring](docs/AUTHORING.md) | How to add signature, YARA, and Python rules |\n| [API Reference](docs/api-server.md) | REST API documentation |\n| [Development Guide](docs/developing.md) | Contributing and development setup |\n\n---\n\n## Installation\n\n**Prerequisites:** Python 3.10+ and [uv](https://docs.astral.sh/uv/) (recommended) or pip\n\n```bash\n# Using uv (recommended)\nuv pip install cisco-ai-skill-scanner\n\n# Using pip\npip install cisco-ai-skill-scanner\n```\n\n<details>\n<summary><strong>Cloud Provider Extras</strong></summary>\n\n```bash\n# AWS Bedrock support\npip install cisco-ai-skill-scanner[bedrock]\n\n# Google Vertex AI support\npip install cisco-ai-s",
      "default_branch": "main"
    },
    "fetched_at": "2026-02-22T03:32:32.246170"
  },
  {
    "basic_info": {
      "name": "skill-compose",
      "full_name": "MooseGoose0701/skill-compose",
      "owner": "MooseGoose0701",
      "description": "Skill Compose is an open-source agent builder and runtime platform for skill-powered agents. No workflow graphs. No CLI.",
      "url": "https://github.com/MooseGoose0701/skill-compose",
      "clone_url": "https://github.com/MooseGoose0701/skill-compose.git",
      "ssh_url": "git@github.com:MooseGoose0701/skill-compose.git",
      "homepage": "",
      "created_at": "2026-02-12T14:11:39Z",
      "updated_at": "2026-02-22T03:31:50Z",
      "pushed_at": "2026-02-22T03:28:28Z"
    },
    "stats": {
      "stars": 903,
      "forks": 76,
      "watchers": 903,
      "open_issues": 2,
      "size": 22919
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 1621558,
        "TypeScript": 825404,
        "Shell": 38858,
        "Makefile": 7562,
        "Dockerfile": 7136,
        "CSS": 4067,
        "OCaml": 2323,
        "JavaScript": 776
      },
      "license": "Apache License 2.0",
      "topics": []
    },
    "content": {
      "readme": "<p align=\"center\">\n  <img src=\"scheader.png\" alt=\"Skill Compose\" width=\"50%\" />\n</p>\n\n<p align=\"center\">\n  <a href=\"./README.md\"><img alt=\"English\" src=\"https://img.shields.io/badge/English-d9d9d9\"></a>\n  <a href=\"./README_es.md\"><img alt=\"Espa√±ol\" src=\"https://img.shields.io/badge/Espa√±ol-d9d9d9\"></a>\n  <a href=\"./README_pt-BR.md\"><img alt=\"Portugu√™s (BR)\" src=\"https://img.shields.io/badge/Portugu√™s (BR)-d9d9d9\"></a>\n  <a href=\"./README_zh-CN.md\"><img alt=\"ÁÆÄ‰Ωì‰∏≠Êñá\" src=\"https://img.shields.io/badge/ÁÆÄ‰Ωì‰∏≠Êñá-d9d9d9\"></a>\n  <a href=\"./README_ja.md\"><img alt=\"Êó•Êú¨Ë™û\" src=\"https://img.shields.io/badge/Êó•Êú¨Ë™û-d9d9d9\"></a>\n</p>\n\n<p align=\"center\">\nSkill Compose is an open-source agent builder and runtime platform for skill-powered agents.<br>\nNo workflow graphs. No CLI.\n</p>\n\n<p align=\"center\">\n  <a href=\"LICENSE\"><img src=\"https://img.shields.io/badge/License-Apache%202.0-blue.svg\" alt=\"License\" /></a>\n  <a href=\"https://www.python.org/\"><img src=\"https://img.shields.io/badge/Python-3.11+-green.svg\" alt=\"Python\" /></a>\n  <a href=\"https://nextjs.org/\"><img src=\"https://img.shields.io/badge/Next.js-14-black.svg\" alt=\"Next.js\" /></a>\n  <a href=\"https://discord.gg/8QK5suCV9m\"><img src=\"https://img.shields.io/badge/Discord-%235865F2.svg?style=flat&logo=discord&logoColor=white\" alt=\"discord\" /></a>\n  <a href=\"https://x.com/SkillComposeAI/\"><img src=\"https://img.shields.io/twitter/follow/SkillComposeAI\" alt=\"twitter\" /></a>\n</p>\n\n<p align=\"center\">\n  <img src=\"docs/images/screenshot.png\" alt=\"Skill Compose Screenshot\" width=\"800\" />\n</p>\n\n## Key Capabilities\n\n- üß© **Skills as first-class artifacts** ‚Äî versioned, reviewable skill packages (contracts, references, rubrics, helpers), not brittle graphs.\n- üß† **\"Skill-Compose My Agent\" workflow** ‚Äî describe what you want; Skill Compose finds/reuses skills, drafts missing ones, and composes an agent.\n- üîå **Tool + MCP wiring** ‚Äî connect tools and MCP servers without hand-writing glue code.\n- üöÄ **Instant publishing** ‚Äî one click to ship as **Web Chat** (shareable link) and/or **API** (integrations-ready endpoint).\n- üõ°Ô∏è **Container-first isolation** ‚Äî run agents in containers (or K8s pods) to keep hosts clean and execution reproducible.\n- üß± **Executors for heavy environments** ‚Äî assign custom Docker images/K8s runtimes per agent (GPU/ML/HPC stacks, custom builds).\n- üì¶ **Skill lifecycle management** ‚Äî GitHub import + one-click updates, multi-format import/export, version history, diff/rollback, and local sync.\n- üîÑ **Skill evolution from reality** ‚Äî improve skills using feedback + execution traces, with proposed rewrites you can review.\n- üóÇÔ∏è **Skill library organization** ‚Äî categories, pinning, and lightweight discovery to stay sane at 100+ skills.\n\n## Examples\n\n<table>\n<tr>\n<td align=\"center\">\n<b>Skill-Compose Your Agent</b><br>\n<sub>Describe what you want and let Skill Compose build the agent for you ‚Äî finding existing skills, drafting missing ones, and wiring everything together.</sub><br><br>\n<img src=\"docs/examples/skill-compose-your-agent.gif\" alt=\"Skill-Compose Your Agent\" width=\"100%\" />\n</td>\n</tr>\n<tr>\n<td align=\"center\">\n<b>Evolve Your Agent</b><br>\n<sub>Improve skills automatically from execution traces and user feedback, review proposed changes, accept the rewrite, and watch your agents and skills get smarter.</sub><br><br>\n<img src=\"docs/examples/evolve-your-agent.gif\" alt=\"Evolve Your Agent\" width=\"100%\" />\n</td>\n</tr>\n<tr>\n<td align=\"center\">\n<b>Demo Agent: Article to Slides</b><br>\n<sub>Turn any article or paper into a polished slide deck. The agent reads the content, extracts key points, draft storyboards, and generates presentation-ready slides.</sub><br><br>\n<img src=\"docs/examples/article-to-slides-agent.gif\" alt=\"Article to Slides Agent\" width=\"100%\" />\n</td>\n</tr>\n<tr>\n<td align=\"center\">\n<b>Demo Agent: ChemScout</b><br>\n<sub>Runs in an isolated execution environment! A chemistry research assistant that searches compound databases, analyzes molecular structures, and summarizes findings into structured reports.</sub><br><br>\n<img src=\"docs/examples/chemscout-agent.gif\" alt=\"ChemScout Agent\" width=\"100%\" />\n</td>\n</tr>\n</table>\n\n## Architecture\n\n<p align=\"center\">\n  <img src=\"docs/images/architecture.png\" alt=\"Skill Compose Architecture\" width=\"700\" />\n</p>\n\n*Some features shown may still be in development.*\n\n## Quick Start\n\nGet started with Docker:\n\n```bash\ngit clone https://github.com/MooseGoose0701/skill-compose.git\ncd skill-compose/docker\n# Default model is Kimi 2.5 (thinking disabled, API key: MOONSHOT_API_KEY), add at least one LLM API key.\n# You can also set API KEYs manually in the Web UI \"Environment\" after launch.\ncp .env.example .env\ndocker compose up -d\n```\n\nOpen **http://localhost:62600** and click **\"Skill-Compose Your Agent\"**.\n\nStop services:\n\n```bash\ncd skill-compose/docker\ndocker compose down\n```\n\n<details>\n<summary>Build from source (for developers)</summary>\n\n```bash\ncd skill-compose/docker\ncp .env.example .env\n# Use docker-compose.dev.yaml to build ima",
      "default_branch": "main"
    },
    "fetched_at": "2026-02-22T03:32:33.380597"
  },
  {
    "basic_info": {
      "name": "FastCode",
      "full_name": "HKUDS/FastCode",
      "owner": "HKUDS",
      "description": "\"FastCode: Accelerating and Streamlining Your Code Understanding\"",
      "url": "https://github.com/HKUDS/FastCode",
      "clone_url": "https://github.com/HKUDS/FastCode.git",
      "ssh_url": "git@github.com:HKUDS/FastCode.git",
      "homepage": "",
      "created_at": "2026-02-13T06:43:54Z",
      "updated_at": "2026-02-21T21:02:23Z",
      "pushed_at": "2026-02-13T10:29:53Z"
    },
    "stats": {
      "stars": 895,
      "forks": 105,
      "watchers": 895,
      "open_issues": 5,
      "size": 1126
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 1095412,
        "HTML": 53258,
        "Shell": 26419,
        "TypeScript": 7851,
        "Dockerfile": 2486,
        "JavaScript": 1256
      },
      "license": null,
      "topics": []
    },
    "content": {
      "readme": "<div align=\"center\">\n\n<img src=\"assets/FastCode.svg\" alt=\"FastCode Logo\" width=\"200\"/>\n\n<!-- # FastCode -->\n\n### FastCode: Accelerating and Streamlining Your Code Understanding\n\n| **‚ö° High Performance** | **üí∞ Cost Efficient** | **üöÄ Fast & Scalable** |\n\n[![Python 3.12+](https://img.shields.io/badge/python-3.12+-blue.svg)](https://www.python.org/downloads/)\n[![License](https://img.shields.io/badge/license-MIT-green.svg)](LICENSE)\n[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)\n<p>\n  <a href=\"https://github.com/HKUDS/FastCode/issues/1\"><img src=\"https://img.shields.io/badge/üí¨WeChat-Group-07c160?style=for-the-badge&logo=wechat&logoColor=white&labelColor=1a1a2e\"></a>\n  <a href=\"https://github.com/HKUDS/FastCode/issues/2\"><img src=\"https://img.shields.io/badge/üí¨Feishu-Group-3370ff?style=for-the-badge&logo=bytedance&logoColor=white&labelColor=1a1a2e\"></a>\n</p>\n\n[Features](#-why-fastcode) ‚Ä¢ [Quick Start](#-quick-start) ‚Ä¢ [Installation](#-installation) ‚Ä¢ [Documentation](#-how-it-works)\n\n</div>\n\n---\n\n## üéØ Why FastCode?\n\nFastCode is a token-efficient framework for comprehensive code understanding and analysis: delivering **superior speed**, **exceptional accuracy**, and **cost-effectiveness** for large-scale codebases and software architectures.\n\nüöÄ **Superior Speed Advantage** - Runs 3x faster than Cursor and 4x faster than Claude Code.\n\nüí∞ **Significant Cost Savings** - Costs 55% less than Cursor and 44% less than Claude Code.\n\n‚ö° **Highest Accuracy** - Outperforms Cursor and Claude Code with the highest accuracy score.\n\n<div align=\"center\">\n<img src=\"assets/performance.png\" alt=\"FastCode Performance vs Cost\" width=\"850\"/>\n</div>\n\n---\n\n## Key Features of FastCode\n\n### üéØ Core Performance Advantages\n- 3-4x Faster than competitors (Cursor/Claude Code)\n- 44-55% Cost Reduction compared to alternatives\n- Highest Accuracy Score across benchmarks\n- Up to 10x Token Savings through smart navigation\n\n### üõ†Ô∏è Technical Capabilities\n- Large-Scale Repository Analysis - Handle massive codebases efficiently\n- Multi-Language Support - Python, JavaScript, TypeScript, Java, Go, C/C++, Rust, C#\n- Multi-Repository Reasoning - Cross-repo dependency analysis\n- Small Model Support - Local model compatibility (qwen3-coder-30b)\n\n### üíª User Experience\n- Beautiful Web UI - Intuitive codebase exploration\n- Flexible API - Easy workflow integration\n- Smart Structural Navigation - Load only what you need\n\n---\n\n## üé• See FastCode in Action\n\n<div align=\"center\">\n\n[![Watch FastCode Demo](https://img.youtube.com/vi/NwexLWHPBOY/0.jpg)](https://youtu.be/NwexLWHPBOY)\n\n**Click to watch FastCode in action** - See how FastCode analyzes complex codebases with lightning speed.\n\n---\n\n</div>\n\n### Core Technologies Behind FastCode\n\nFastCode introduces a three-phase framework that transforms how LLMs understand and navigate codebases:\n\n<p align=\"center\">\n  <img src=\"assets/framework.png\" alt=\"FastCode Framework\" width=\"100%\"/>\n</p>\n\n## üèóÔ∏è Semantic-Structural Code Representation\n\n### Multi-layered codebase understanding for comprehensive analysis\n\n- **üîç Hierarchical Code Units** ‚Äî Advanced multi-level indexing spanning files, classes, functions, and documentation using AST-based parsing across 8+ programming languages\n\n- **üîó Hybrid Index** ‚Äî Seamlessly combines semantic embeddings with keyword search (BM25) for robust and precise code retrieval\n\n- **üìä Multi-Layer Graph Modeling** ‚Äî Three interconnected relationship graphs (Call Graph, Dependency Graph, Inheritance Graph) enabling structural navigation across the entire codebase\n\n### üß≠ Lightning-Fast Codebase Navigation\n\nFinding the right code without opening every file - at lightning speed\n\n- **‚ö° Two-Stage Smart Search** ‚Äî Like having a research assistant that first finds potentially relevant code, then ranks and organizes the best matches for your specific question.\n\n- **üìÅ Safe File Browsing** ‚Äî Explores your project structure securely, understanding folder organization and file patterns without compromising security.\n\n- **üåê Following Code Connections** ‚Äî Traces how code pieces connect (up to 2 steps away), like following a trail of breadcrumbs through your codebase.\n\n- **üéØ Code Skimming** ‚Äî Instead of reading entire files, FastCode just looks at the \"headlines\" - function names, class definitions, and type hints. This is like reading a book's chapter titles instead of every page, saving massive amounts of processing power.\n\n### üí∞ Cost-Efficient Context Management\n\nGetting maximum insight while minimizing costs - automatically\n\n- **üìà Budget-Aware Decision Making** ‚Äî Considers five key factors before processing: confidence level, query complexity, codebase size, resource cost, and iteration count. Like a cost-conscious financial advisor that weighs all options before making decisions.\n\n- **üîÑ Resource-Optimized Learning** ‚Äî Continuously adapts its approach in real-time, getting more efficient about what information to gather and when to stop. Think of it a",
      "default_branch": "main"
    },
    "fetched_at": "2026-02-22T03:32:34.513598"
  },
  {
    "basic_info": {
      "name": "lingbot-depth",
      "full_name": "Robbyant/lingbot-depth",
      "owner": "Robbyant",
      "description": "Masked Depth Modeling for Spatial Perception",
      "url": "https://github.com/Robbyant/lingbot-depth",
      "clone_url": "https://github.com/Robbyant/lingbot-depth.git",
      "ssh_url": "git@github.com:Robbyant/lingbot-depth.git",
      "homepage": "https://technology.robbyant.com/lingbot-depth",
      "created_at": "2026-01-25T14:14:29Z",
      "updated_at": "2026-02-21T18:11:31Z",
      "pushed_at": "2026-02-14T16:39:34Z"
    },
    "stats": {
      "stars": 874,
      "forks": 63,
      "watchers": 874,
      "open_issues": 4,
      "size": 31479
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 144257
      },
      "license": "Apache License 2.0",
      "topics": [
        "depth",
        "depth-camera",
        "masked-image-modeling"
      ]
    },
    "content": {
      "readme": "# LingBot-Depth: Masked Depth Modeling for Spatial Perception\n\n\n[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](LICENSE)\n[![Python 3.9+](https://img.shields.io/badge/python-3.9+-blue.svg)](https://www.python.org/downloads/)\n[![PyTorch 2.6+](https://img.shields.io/badge/pytorch-2.6+-green.svg)](https://pytorch.org/)\n\nüìÑ **[Technical Report](https://github.com/Robbyant/lingbot-depth/blob/main/tech-report.pdf)** |\nüìÑ **[arXiv](https://arxiv.org/abs/2601.17895)** |\nüåê **[Project Page](https://technology.robbyant.com/lingbot-depth)** |\nüíª **[Code](https://github.com/robbyant/lingbot-depth)** |\nü§ó **[Hugging Face](https://huggingface.co/collections/robbyant/lingbot-depth)** |\nü§ñ **[ModelScope](https://www.modelscope.cn/collections/Robbyant/LingBot-Depth)** ÔΩú\nü§ñ **[Video](https://www.bilibili.com/video/BV1oa6uBXEyh)**\n\n\n**LingBot-Depth** transforms incomplete and noisy depth sensor data into high-quality, metric-accurate 3D measurements. By jointly aligning RGB appearance and depth geometry in a unified latent space, our model serves as a powerful spatial perception foundation for robot learning and 3D vision applications.\n\n<p align=\"center\">\n  <img src=\"assets/teaser/teaser-crop.png\" width=\"100%\">\n</p>\n\nOur approach refines raw sensor depth into clean, complete measurements, enabling:\n- **Depth Completion & Refinement**: Fills missing regions with metric accuracy and improved quality\n- **Scene Reconstruction**: High-fidelity indoor mapping with a strong depth prior\n- **4D Point Tracking**: Accurate dynamic tracking in metric space for robot learning\n- **Dexterous Manipulation**: Robust grasping with precise geometric understanding\n\n## News\n\n- **[2026.02.15]** Upload LingBot-Depth-v0.5 which fixes the bug in previous version.\n\n## Artifacts Release\n\n### Model Zoo\n\nWe provide pretrained models for different scenarios:\n\n| Model | Hugging Face Model | ModelScope Model | Description |\n|-------|-----------|-----------|-------------|\n| LingBot-Depth-v0.5 | [robbyant/lingbot-depth-pretrain-vitl-14-v0.5](https://huggingface.co/robbyant/lingbot-depth-pretrain-vitl-14-v0.5/tree/main) | [robbyant/lingbot-depth-pretrain-vitl-14-v0.5](https://www.modelscope.cn/models/Robbyant/lingbot-depth-pretrain-vitl-14-v0.5)| ‚≠ê **Recommended!** General-purpose depth refinement and completion (fixes the bug in LingBot-Depth-v0.1)|\n| LingBot-Depth-v0.1 | [robbyant/lingbot-depth-pretrain-vitl-14](https://huggingface.co/robbyant/lingbot-depth-pretrain-vitl-14/tree/main) | [robbyant/lingbot-depth-pretrain-vitl-14](https://www.modelscope.cn/models/Robbyant/lingbot-depth-pretrain-vitl-14)| General-purpose depth refinement |\n| LingBot-Depth-DC | [robbyant/lingbot-depth-postrain-dc-vitl14](https://huggingface.co/robbyant/lingbot-depth-postrain-dc-vitl14/tree/main) | [robbyant/lingbot-depth-postrain-dc-vitl14](https://www.modelscope.cn/models/Robbyant/lingbot-depth-postrain-dc-vitl14)| Optimized for sparse depth completion |\n\n### Data Release (Coming Soon)\n- The curated 3M RGB-D dataset will be released upon completion of the necessary licensing and approval procedures. \n- Expected release: **mid-March 2026**.\n\n## Installation\n\n### Requirements\n\n‚Ä¢ Python ‚â• 3.9 ‚Ä¢ PyTorch ‚â• 2.0.0 ‚Ä¢ CUDA-capable GPU (recommended)\n\n### From source\n\n```bash\ngit clone https://github.com/robbyant/lingbot-depth\ncd lingbot-depth\n\n# Install the package (use 'python -m pip' to ensure correct environment)\nconda create -n lingbot-depth python=3.9\nconda activate lingbot-depth\npython -m pip install -e .\n```\n## Quick Start\n\n**Inference:**\n\n```python\nimport torch\nimport cv2\nimport numpy as np\nfrom mdm.model.v2 import MDMModel\n\n# Load model\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = MDMModel.from_pretrained('robbyant/lingbot-depth-pretrain-vitl-14-v0.5').to(device)\n\n# Load and prepare inputs\nimage = cv2.cvtColor(cv2.imread('examples/0/rgb.png'), cv2.COLOR_BGR2RGB)\nh, w = image.shape[:2]\nimage = torch.tensor(image / 255, dtype=torch.float32, device=device).permute(2, 0, 1)[None]\n\ndepth = cv2.imread('examples/0/raw_depth.png', cv2.IMREAD_UNCHANGED).astype(np.float32) / 1000.0\ndepth = torch.tensor(depth, dtype=torch.float32, device=device)[None]\n\nintrinsics = np.loadtxt('examples/0/intrinsics.txt')\nintrinsics[0] /= w  # Normalize fx and cx by width\nintrinsics[1] /= h  # Normalize fy and cy by height\nintrinsics = torch.tensor(intrinsics, dtype=torch.float32, device=device)[None]\n\n# Run inference\noutput = model.infer(\n    image,\n    depth_in=depth,\n    intrinsics=intrinsics)\n\ndepth_pred = output['depth']  # Refined depth map\npoints = output['points']      # 3D point cloud\n```\n\n**Run example:**\n\nThe model is automatically downloaded from Hugging Face on first run (no manual download needed):\n\n```bash\n# Basic usage - processes example 0\npython example.py\n\n# Use a different example (0-7 available)\npython example.py --example 1\n\n# Use depth completion optimized model\npython example.py --model robbyant/lingbot-depth-postrain-dc-vitl14-v0.5\n\n#",
      "default_branch": "main"
    },
    "fetched_at": "2026-02-22T03:32:35.658990"
  },
  {
    "basic_info": {
      "name": "noodles",
      "full_name": "unslop-xyz/noodles",
      "owner": "unslop-xyz",
      "description": "Your codebase was probably AI-generated. Get a better handle on it. Noodles creates interactive diagrams that visualize how your code actually works, so you can understand what the AI built without reading every line.",
      "url": "https://github.com/unslop-xyz/noodles",
      "clone_url": "https://github.com/unslop-xyz/noodles.git",
      "ssh_url": "git@github.com:unslop-xyz/noodles.git",
      "homepage": null,
      "created_at": "2026-01-26T13:59:24Z",
      "updated_at": "2026-02-20T09:31:11Z",
      "pushed_at": "2026-02-04T19:13:19Z"
    },
    "stats": {
      "stars": 864,
      "forks": 113,
      "watchers": 864,
      "open_issues": 2,
      "size": 3320
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 85729,
        "HTML": 34320,
        "D2": 58
      },
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "# Noodles\n\n## TL;DR\n\nYour codebase was probably AI-generated. Get a better handle on it.\n\nNoodles creates interactive diagrams that visualize how your code actually works, so you can understand what the AI built without reading every line.\n\n![noodles demo](assets/demo.gif)\n\n## What it does\n\n- Scans a folder and builds a manifest of your code\n- Uses OpenAI to identify user-facing entry points (CLI commands, routes, UI components)\n- Generates D2 diagrams showing how code flows from entry to outcome\n- Renders an interactive overlay to explore the diagrams\n- Tracks changes and updates diagrams incrementally when code changes\n\n## Setup\n\n### Prerequisites\n\n- Python 3.9+\n- [uv](https://docs.astral.sh/uv/) (recommended) or pip\n- [d2](https://d2lang.com/) CLI for diagram rendering\n- OpenAI API key\n\n### Install dependencies\n\n```bash\n# Using uv (recommended)\nuv sync\n\n# Or using pip\npip install -e '.[dev]'\n```\n\n### Install d2\n\n```bash\n# macOS\nbrew install d2\n\n# Or download from https://d2lang.com\n```\n\n### Configure OpenAI API key\n\n**Option 1: `.env` file (recommended)**\n\nCreate a `.env` file in your project root:\n```\nOPENAI_API_KEY=your-key\n```\nThe CLI automatically loads `.env` files on startup.\n\n**Option 2: Environment variable**\n```bash\n# macOS/Linux\nexport OPENAI_API_KEY=\"your-key\"\n\n# Windows (PowerShell)\n$env:OPENAI_API_KEY=\"your-key\"\n\n# Windows (CMD)\nset OPENAI_API_KEY=your-key\n```\n\n### Optional configuration\n\n```bash\n# Point to d2 binary if not on PATH\nexport UNSLOP_D2_BIN=/opt/homebrew/bin/d2\n\n# Set log level (DEBUG, INFO, WARNING, ERROR)\nexport UNSLOP_LOG_LEVEL=INFO\n```\n\n## Usage\n\n```bash\nunslop run\n```\n\n- Use the overlay to select folders to analyze\n- Diagrams and manifests are stored in `<folder>/.unslop/`\n- Close the overlay to exit\n\n### Overlay controls\n\n- **Choose folder** - Select a codebase to analyze\n- **Update** - Regenerate diagram only if code changed (incremental)\n- **Rerun** - Full rebuild from scratch\n- **?** - Hover for keyboard shortcuts and icon meanings\n- Click any node to drill into details; hover for tooltips\n\n## Known rough edges\n\n- Speed of diagram generation; works best on projects with fewer than 100 files for now\n- UI is intentionally verbose for debugging; simplification planned\n- Diagram quality varies; prompt tuning ongoing\n",
      "default_branch": "main"
    },
    "fetched_at": "2026-02-22T03:32:36.821381"
  },
  {
    "basic_info": {
      "name": "paperbanana",
      "full_name": "llmsresearch/paperbanana",
      "owner": "llmsresearch",
      "description": "Open source implementation and extension of Google Research‚Äôs PaperBanana for automated academic figures, diagrams, and research visuals, expanded to new domains like slide generation.",
      "url": "https://github.com/llmsresearch/paperbanana",
      "clone_url": "https://github.com/llmsresearch/paperbanana.git",
      "ssh_url": "git@github.com:llmsresearch/paperbanana.git",
      "homepage": "",
      "created_at": "2026-02-04T14:46:47Z",
      "updated_at": "2026-02-22T03:31:10Z",
      "pushed_at": "2026-02-19T06:26:26Z"
    },
    "stats": {
      "stars": 853,
      "forks": 124,
      "watchers": 853,
      "open_issues": 11,
      "size": 16040
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 244132
      },
      "license": "MIT License",
      "topics": [
        "academic-diagrams",
        "academic-research",
        "agentic-ai",
        "arxiv",
        "diagram-generation",
        "gemini",
        "google-gemini",
        "llm",
        "llms",
        "mcp",
        "mcp-server",
        "multiagent",
        "neurips",
        "paperbanana",
        "python-ai-research-tools",
        "research-automation",
        "research-tools",
        "scientific-visualization",
        "text-to-image",
        "vlm"
      ]
    },
    "content": {
      "readme": "<!-- mcp-name: io.github.llmsresearch/paperbanana -->\n<table align=\"center\" width=\"100%\" style=\"border: none; border-collapse: collapse;\">\n  <tr>\n    <td width=\"220\" align=\"left\" valign=\"middle\" style=\"border: none;\">\n      <img src=\"https://dwzhu-pku.github.io/PaperBanana/static/images/logo.jpg\" alt=\"PaperBanana Logo\" width=\"180\"/>\n    </td>\n    <td align=\"left\" valign=\"middle\" style=\"border: none;\">\n      <h1>PaperBanana</h1>\n      <p><strong>Automated Academic Illustration for AI Scientists</strong></p>\n      <p>\n        <a href=\"https://github.com/llmsresearch/paperbanana/actions/workflows/ci.yml\"><img src=\"https://github.com/llmsresearch/paperbanana/actions/workflows/ci.yml/badge.svg\" alt=\"CI\"/></a>\n        <a href=\"https://pypi.org/project/paperbanana/\"><img src=\"https://img.shields.io/pypi/dm/paperbanana?label=PyPI%20downloads&logo=pypi&logoColor=white\" alt=\"PyPI Downloads\"/></a>\n        <a href=\"https://huggingface.co/spaces/llmsresearch/paperbanana\"><img src=\"https://img.shields.io/badge/Demo-HuggingFace-yellow?logo=huggingface&logoColor=white\" alt=\"Demo\"/></a>\n        <br/>\n        <a href=\"https://www.python.org/downloads/\"><img src=\"https://img.shields.io/badge/python-3.10%2B-blue?logo=python&logoColor=white\" alt=\"Python 3.10+\"/></a>\n        <a href=\"https://arxiv.org/abs/2601.23265\"><img src=\"https://img.shields.io/badge/arXiv-2601.23265-b31b1b?logo=arxiv&logoColor=white\" alt=\"arXiv\"/></a>\n        <a href=\"LICENSE\"><img src=\"https://img.shields.io/badge/license-MIT-green?logo=opensourceinitiative&logoColor=white\" alt=\"License: MIT\"/></a>\n        <br/>\n        <a href=\"https://pydantic.dev\"><img src=\"https://img.shields.io/badge/Pydantic-v2-e92063?logo=pydantic&logoColor=white\" alt=\"Pydantic v2\"/></a>\n        <a href=\"https://typer.tiangolo.com\"><img src=\"https://img.shields.io/badge/CLI-Typer-009688?logo=gnubash&logoColor=white\" alt=\"Typer\"/></a>\n        <a href=\"https://ai.google.dev/\"><img src=\"https://img.shields.io/badge/Gemini-Free%20Tier-4285F4?logo=google&logoColor=white\" alt=\"Gemini Free Tier\"/></a>\n      </p>\n    </td>\n  </tr>\n</table>\n\n---\n\n> **Disclaimer**: This is an **unofficial, community-driven open-source implementation** of the paper\n> *\"PaperBanana: Automating Academic Illustration for AI Scientists\"* by Dawei Zhu, Rui Meng, Yale Song,\n> Xiyu Wei, Sujian Li, Tomas Pfister, and Jinsung Yoon ([arXiv:2601.23265](https://arxiv.org/abs/2601.23265)).\n> This project is **not affiliated with or endorsed by** the original authors or Google Research.\n> The implementation is based on the publicly available paper and may differ from the original system.\n\nAn agentic framework for generating publication-quality academic diagrams and statistical plots from text descriptions. Supports OpenAI (GPT-5.2 + GPT-Image-1.5), Azure OpenAI / Foundry, and Google Gemini providers.\n\n- Two-phase multi-agent pipeline with iterative refinement\n- Multiple VLM and image generation providers (OpenAI, Azure, Gemini)\n- Input optimization layer for better generation quality\n- Auto-refine mode and run continuation with user feedback\n- CLI, Python API, and MCP server for IDE integration\n- Claude Code skills for `/generate-diagram`, `/generate-plot`, and `/evaluate-diagram`\n\n<p align=\"center\">\n  <img src=\"assets/img/hero_image.png\" alt=\"PaperBanana takes paper as input and provide diagram as output\" style=\"max-width: 960px; width: 100%; height: auto;\"/>\n</p>\n\n---\n\n## Quick Start\n\n### Prerequisites\n\n- Python 3.10+\n- An OpenAI API key ([platform.openai.com](https://platform.openai.com/api-keys)) or Azure OpenAI / Foundry endpoint\n- Or a Google Gemini API key (free, [Google AI Studio](https://makersuite.google.com/app/apikey))\n\n### Step 1: Install\n\n```bash\npip install paperbanana\n```\n\nOr install from source for development:\n\n```bash\ngit clone https://github.com/llmsresearch/paperbanana.git\ncd paperbanana\npip install -e \".[dev,openai,google]\"\n```\n\n### Step 2: Get Your API Key\n\n```bash\ncp .env.example .env\n# Edit .env and add your API key:\n#   OPENAI_API_KEY=your-key-here\n#\n# For Azure OpenAI / Foundry:\n#   OPENAI_BASE_URL=https://<resource>.openai.azure.com/openai/v1\n```\n\nOr use the setup wizard for Gemini:\n\n```bash\npaperbanana setup\n```\n\n### Step 3: Generate a Diagram\n\n```bash\npaperbanana generate \\\n  --input examples/sample_inputs/transformer_method.txt \\\n  --caption \"Overview of our encoder-decoder architecture with sparse routing\"\n```\n\nWith input optimization and auto-refine:\n\n```bash\npaperbanana generate \\\n  --input my_method.txt \\\n  --caption \"Overview of our encoder-decoder framework\" \\\n  --optimize --auto\n```\n\nOutput is saved to `outputs/run_<timestamp>/final_output.png` along with all intermediate iterations and metadata.\n\n---\n\n## How It Works\n\nPaperBanana implements a multi-agent pipeline with up to 7 specialized agents:\n\n**Phase 0 -- Input Optimization (optional, `--optimize`):**\n\n0. **Input Optimizer** runs two parallel VLM calls:\n   - **Context Enricher** structures raw methodology text into diagram-ready f",
      "default_branch": "main"
    },
    "fetched_at": "2026-02-22T03:32:37.972287"
  },
  {
    "basic_info": {
      "name": "lingbot-vla",
      "full_name": "Robbyant/lingbot-vla",
      "owner": "Robbyant",
      "description": "A Pragmatic VLA Foundation Model",
      "url": "https://github.com/Robbyant/lingbot-vla",
      "clone_url": "https://github.com/Robbyant/lingbot-vla.git",
      "ssh_url": "git@github.com:Robbyant/lingbot-vla.git",
      "homepage": "",
      "created_at": "2026-01-26T12:52:25Z",
      "updated_at": "2026-02-22T03:04:47Z",
      "pushed_at": "2026-02-05T13:13:09Z"
    },
    "stats": {
      "stars": 825,
      "forks": 62,
      "watchers": 825,
      "open_issues": 16,
      "size": 20944
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 962338,
        "Dockerfile": 1249,
        "Shell": 617,
        "Makefile": 350
      },
      "license": "Apache License 2.0",
      "topics": []
    },
    "content": {
      "readme": "<h1 align=\"center\">LingBot-VLA: A Pragmatic VLA Foundation Model</h1>\n\n<p align=\"center\">\n  <a href=\"assets/LingBot-VLA.pdf\"><img src=\"https://img.shields.io/static/v1?label=Paper&message=PDF&color=red&logo=arxiv\"></a>\n  <a href=\"https://technology.robbyant.com/lingbot-vla\"><img src=\"https://img.shields.io/badge/Project-Website-blue\"></a>\n  <a href=\"https://huggingface.co/collections/robbyant/lingbot-vla\"><img src=\"https://img.shields.io/static/v1?label=%F0%9F%A4%97%20Model&message=HuggingFace&color=yellow\"></a>\n  <a href=\"https://modelscope.cn/collections/Robbyant/LingBot-VLA\"><img src=\"https://img.shields.io/static/v1?label=%F0%9F%A4%96%20Model&message=ModelScope&color=purple\"></a>\n  <a href=\"LICENSE\"><img src=\"https://img.shields.io/badge/License-Apache--2.0-green\"></a>\n</p>\n\n<p align=\"center\">\n  <img src=\"assets/Teaser.png\" width=\"100%\">\n</p>\n\n## ü•≥ We are excited to introduce **LingBot-VLA**, a pragmatic Vision-Language-Action foundation model.\n\n**LingBot-VLA** has focused on **Pragmatic**:\n- **Large-scale Pre-training Data**: 20,000 hours of real-world\ndata from 9 popular dual-arm robot configurations.\n<p align=\"center\">\n  <img src=\"assets/scale_sr.png\" width=\"45%\" style=\"margin: 0 10px;\">\n  <img src=\"assets/scale_ps.png\" width=\"45%\" style=\"margin: 0 10px;\">\n</p>\n\n- **Strong Performance**: Achieve clear superiority over competitors on simulation and real-world benchmarks.\n- **Training Efficiency**: Represent a 1.5 ‚àº 2.8√ó (depending on the relied VLM base model) speedup over existing VLA-oriented codebases.\n\n## üöÄ News\n- **[2026-01-27]** LingBot-VLA Technical Report is available on Arxiv.\n- **[2026-01-27]** Weights and code released!\n\n\n---\n\n\n## üõ†Ô∏è Installation\nRequirements\n - Python 3.12.3\n - Pytorch 2.8.0\n - CUDA 12.8\n\n```bash\n# Install Lerobot\npip install torch==2.8.0 torchvision==0.23.0 torchaudio==2.8.0 --index-url https://download.pytorch.org/whl/cu128\nGIT_LFS_SKIP_SMUDGE=1 git clone https://github.com/huggingface/lerobot.git\ncd lerobot\ngit checkout 0cf864870cf29f4738d3ade893e6fd13fbd7cdb5\npip install -e .\n# Install flash attention\npip install /path/to/flash_attn-2.8.3+cu12torch2.8cxx11abiTRUE-cp312-cp312-linux_x86_64.whl\n\n# Clone the repository\ngit clone https://github.com/robbyant/lingbot-vla.git\ncd lingbot-vla/\ngit submodule update --remote --recursive\npip install -e .\npip install -r requirements.txt\n# Install LingBot-Depth dependency\ncd ./lingbotvla/models/vla/vision_models/lingbot-depth/\npip install -e . --no-deps\ncd ../MoGe\npip install -e .\n```\n\n---\n\n## üì¶ Model Download\nWe release LingBot-VLA pre-trained weights in two configurations: depth-free version and a depth-distillated version.\n- **Pretrained Checkpoints for Post-Training with and without depth**\n\n| Model Name | Huggingface | ModelScope | Description |\n| :--- | :---: | :---: | :---: |\n| LingBot-VLA-4B &nbsp; | [ü§ó lingbot-vla-4b](https://huggingface.co/robbyant/lingbot-vla-4b) | [ü§ñ lingbot-vla-4b](https://modelscope.cn/models/Robbyant/lingbot-vla-4b) | LingBot-VLA *w/o* Depth|\n| LingBot-VLA-4B-Depth | [ü§ó lingbot-vla-4b-depth](https://huggingface.co/robbyant/lingbot-vla-4b-depth) | [ü§ñ lingbot-vla-4b-depth](https://modelscope.cn/models/Robbyant/lingbot-vla-4b-depth) | LingBot-VLA *w/* Depth |\n\n\n\n\nTo train LingBot with our codebase, weights from [Qwen2.5-VL-3B-Instruct](https://huggingface.co/Qwen/Qwen2.5-VL-3B-Instruct), [MoGe-2-vitb-normal](https://huggingface.co/Ruicheng/moge-2-vitb-normal), and [LingBot-Depth](https://huggingface.co/robbyant/lingbot-depth-pretrain-vitl-14) also need to be prepared.\n- **Run Command**:\n```bash\npython3 scripts/download_hf_model.py --repo_id robbyant/lingbot-vla-4b --local_dir lingbot-vla-4b \n```\n---\n\n## üíª Post-Training Example\n\n- **Data Preparation**:\nPlease follow [RoboTwin2.0 Preparation](experiment/robotwin/README.md)\n\n- **Training Configuration**:\nWe provide the mixed post-training configuration in five RoboTwin 2.0 tasks (\"open_microwave\" \"click_bell\" \"stack_blocks_three\" \"place_shoe\" \"put_object_cabinet\").\n<details>\n<summary><b>Click to expand full YAML configuration</b></summary>\n\n```yaml\nmodel:\n  model_path: \"path/to/lingbot_vla_checkpoint\" # Path to pre-trained VLA foundation model (w/o or w depth)\n  tokenizer_path: \"path/to/Qwen2.5-VL-3B-Instruct\" \n  post_training: true            # Enable post-training/fine-tuning mode\n  adanorm_time: true\n  old_adanorm: true\n\ndata:\n  datasets_type: vla\n  data_name: robotwin_5_new      \n  train_path: \"path/to/lerobot_merged_data\" # merged data from 5 robotwin2.0 tasks\n  num_workers: 8\n  norm_type: bounds_99_woclip\n  norm_stats_file: assets/norm_stats/robotwin_50.json # file of normalization statistics\n\ntrain:\n  output_dir: \"path/to/output\"\n  loss_type: L1_fm               # we apply L1 flow-matching loss in robotwin2.0 finetuning\n  data_parallel_mode: fsdp2      # Use Fully Sharded Data Parallel (PyTorch FSDP2)\n  enable_full_shard: false       # Don't apply reshare after forward in FSDP2\n  module_fsdp_enable: true\n  use_compile: true              # Acceleration",
      "default_branch": "main"
    },
    "fetched_at": "2026-02-22T03:32:39.112495"
  },
  {
    "basic_info": {
      "name": "OpenPlanter",
      "full_name": "ShinMegamiBoson/OpenPlanter",
      "owner": "ShinMegamiBoson",
      "description": null,
      "url": "https://github.com/ShinMegamiBoson/OpenPlanter",
      "clone_url": "https://github.com/ShinMegamiBoson/OpenPlanter.git",
      "ssh_url": "git@github.com:ShinMegamiBoson/OpenPlanter.git",
      "homepage": null,
      "created_at": "2026-02-20T01:04:23Z",
      "updated_at": "2026-02-22T03:27:43Z",
      "pushed_at": "2026-02-22T03:27:39Z"
    },
    "stats": {
      "stars": 806,
      "forks": 118,
      "watchers": 806,
      "open_issues": 9,
      "size": 400
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 989277,
        "Shell": 3754,
        "Dockerfile": 296
      },
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "# OpenPlanter\n\nA recursive-language-model investigation agent with a terminal UI. OpenPlanter ingests heterogeneous datasets ‚Äî corporate registries, campaign finance records, lobbying disclosures, government contracts, and more ‚Äî resolves entities across them, and surfaces non-obvious connections through evidence-backed analysis. It operates autonomously with file I/O, shell execution, web search, and recursive sub-agent delegation.\n\n## Quickstart\n\n```bash\n# Install\npip install -e .\n\n# Configure API keys (interactive prompt)\nopenplanter-agent --configure-keys\n\n# Launch the TUI\nopenplanter-agent --workspace /path/to/your/project\n```\n\nOr run a single task headlessly:\n\n```bash\nopenplanter-agent --task \"Cross-reference vendor payments against lobbying disclosures and flag overlaps\" --workspace ./data\n```\n\n### Docker\n\n```bash\n# Add your API keys to .env, then:\ndocker compose up\n```\n\nThe container mounts `./workspace` as the agent's working directory.\n\n## Supported Providers\n\n| Provider | Default Model | Env Var |\n|----------|---------------|---------|\n| OpenAI | `gpt-5.2` | `OPENAI_API_KEY` |\n| Anthropic | `claude-opus-4-6` | `ANTHROPIC_API_KEY` |\n| OpenRouter | `anthropic/claude-sonnet-4-5` | `OPENROUTER_API_KEY` |\n| Cerebras | `qwen-3-235b-a22b-instruct-2507` | `CEREBRAS_API_KEY` |\n| Ollama | `llama3.2` | (none ‚Äî local) |\n\n### Local Models (Ollama)\n\n[Ollama](https://ollama.com) runs models locally with no API key. Install Ollama, pull a model (`ollama pull llama3.2`), then:\n\n```bash\nopenplanter-agent --provider ollama\nopenplanter-agent --provider ollama --model mistral\nopenplanter-agent --provider ollama --list-models\n```\n\nThe base URL defaults to `http://localhost:11434/v1` and can be overridden with `OPENPLANTER_OLLAMA_BASE_URL` or `--base-url`. The first request may be slow while Ollama loads the model into memory; a 120-second first-byte timeout is used automatically.\n\nAdditional service keys: `EXA_API_KEY` (web search), `VOYAGE_API_KEY` (embeddings).\n\nAll keys can also be set with an `OPENPLANTER_` prefix (e.g. `OPENPLANTER_OPENAI_API_KEY`), via `.env` files in the workspace, or via CLI flags.\n\n## Agent Tools\n\nThe agent has access to 19 tools, organized around its investigation workflow:\n\n**Dataset ingestion & workspace** ‚Äî `list_files`, `search_files`, `repo_map`, `read_file`, `write_file`, `edit_file`, `hashline_edit`, `apply_patch` ‚Äî load, inspect, and transform source datasets; write structured findings.\n\n**Shell execution** ‚Äî `run_shell`, `run_shell_bg`, `check_shell_bg`, `kill_shell_bg` ‚Äî run analysis scripts, data pipelines, and validation checks.\n\n**Web** ‚Äî `web_search` (Exa), `fetch_url` ‚Äî pull public records, verify entities, and retrieve supplementary data.\n\n**Planning & delegation** ‚Äî `think`, `subtask`, `execute`, `list_artifacts`, `read_artifact` ‚Äî decompose investigations into focused sub-tasks, each with acceptance criteria and independent verification.\n\nIn **recursive mode** (the default), the agent spawns sub-agents via `subtask` and `execute` to parallelize entity resolution, cross-dataset linking, and evidence-chain construction across large investigations.\n\n## CLI Reference\n\n```\nopenplanter-agent [options]\n```\n\n### Workspace & Session\n\n| Flag | Description |\n|------|-------------|\n| `--workspace DIR` | Workspace root (default: `.`) |\n| `--session-id ID` | Use a specific session ID |\n| `--resume` | Resume the latest (or specified) session |\n| `--list-sessions` | List saved sessions and exit |\n\n### Model Selection\n\n| Flag | Description |\n|------|-------------|\n| `--provider NAME` | `auto`, `openai`, `anthropic`, `openrouter`, `cerebras`, `ollama` |\n| `--model NAME` | Model name or `newest` to auto-select |\n| `--reasoning-effort LEVEL` | `low`, `medium`, `high`, or `none` |\n| `--list-models` | Fetch available models from the provider API |\n\n### Execution\n\n| Flag | Description |\n|------|-------------|\n| `--task OBJECTIVE` | Run a single task and exit (headless) |\n| `--recursive` | Enable recursive sub-agent delegation |\n| `--acceptance-criteria` | Judge subtask results with a lightweight model |\n| `--max-depth N` | Maximum recursion depth (default: 4) |\n| `--max-steps N` | Maximum steps per call (default: 100) |\n| `--timeout N` | Shell command timeout in seconds (default: 45) |\n\n### UI\n\n| Flag | Description |\n|------|-------------|\n| `--no-tui` | Plain REPL (no colors or spinner) |\n| `--headless` | Non-interactive mode (for CI) |\n| `--demo` | Censor entity names and workspace paths in output |\n\n### Persistent Defaults\n\nUse `--default-model`, `--default-reasoning-effort`, or per-provider variants like `--default-model-openai` to save workspace defaults to `.openplanter/settings.json`. View them with `--show-settings`.\n\n## TUI Commands\n\nInside the interactive REPL:\n\n| Command | Action |\n|---------|--------|\n| `/model` | Show current model and provider |\n| `/model NAME` | Switch model (aliases: `opus`, `sonnet`, `gpt5`, etc.) |\n| `/model NAME --save` | Switch and persist as default |\n| `/mod",
      "default_branch": "main"
    },
    "fetched_at": "2026-02-22T03:32:40.248199"
  }
]