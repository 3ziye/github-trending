[
  {
    "basic_info": {
      "name": "provenance-action",
      "full_name": "danielroe/provenance-action",
      "owner": "danielroe",
      "description": "GitHub Action that detects dependency provenance downgrades from lockfile changes (npm/pnpm/yarn).",
      "url": "https://github.com/danielroe/provenance-action",
      "clone_url": "https://github.com/danielroe/provenance-action.git",
      "ssh_url": "git@github.com:danielroe/provenance-action.git",
      "homepage": "",
      "created_at": "2025-09-16T11:08:14Z",
      "updated_at": "2025-09-17T10:36:56Z",
      "pushed_at": "2025-09-17T07:16:12Z"
    },
    "stats": {
      "stars": 194,
      "forks": 3,
      "watchers": 194,
      "open_issues": 1,
      "size": 83
    },
    "tech_info": {
      "language": "TypeScript",
      "languages": {
        "TypeScript": 44178
      },
      "license": null,
      "topics": []
    },
    "content": {
      "readme": "# `danielroe/provenance-action`\n\nDetect and fail CI when dependencies in your lockfile lose npm provenance or trusted publisher status.\n\n> [!WARNING]\n> This action is under active development and is only one tool to assist in securing your dependencies.\n\n## ✨ Features\n- supports `pnpm-lock.yaml`, `package-lock.json`, `yarn.lock` (v1 and v2+), `bun.lock`\n- handles transitives by comparing resolved versions\n- inline GitHub annotations at the lockfile line\n- JSON output and optional hard‑fail (default: on)\n- pure TypeScript, Node 24+\n\n👉 See it in action: [danielroe/provenance-action-test](https://github.com/danielroe/provenance-action-test)\n\n## 🚀 Quick start\n```yaml\nname: ci\non:\n  pull_request:\n    branches:\n      - main\npermissions:\n  contents: read\njobs:\n  check-provenance:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n        with:\n          fetch-depth: 0\n      - name: Check provenance downgrades\n        uses: danielroe/provenance-action@main\n        id: check\n        with:\n          fail-on-provenance-change: true # optional, default: false\n        #   lockfile: pnpm-lock.yaml      # optional\n        #   base-ref: origin/main         # optional, default: origin/main\n        #   fail-on-downgrade: true       # optional, default: true\n      - name: Print result\n        run: \"echo 'Downgraded: ${{ steps.check.outputs.downgraded }}'\"\n```\n\n## 🔧 Inputs\n- `lockfile` (optional): Path to the lockfile. Auto-detected if omitted.\n- `workspace-path` (optional): Path to workspace root. Default: `.`\n- `base-ref` (optional): Git ref to compare against. Default: `origin/main`.\n- `fail-on-downgrade` (optional): Controls failure behavior. Accepts `true`, `false`, `any`, or `only-provenance-loss`. Default: `true` (which is the same as `any`).\n- `fail-on-provenance-change` (optional): When `true`, fail on provenance repository/branch changes. Default: `false`.\n\n## 📤 Outputs\n- `downgraded`: JSON array of `{ name, from, to, downgradeType }` for detected downgrades. `downgradeType` is `provenance` or `trusted_publisher`.\n- `changed`: JSON array of provenance change events `{ name, from, to, type, previousRepository?, newRepository?, previousBranch?, newBranch? }`.\n\n## 🧠 How it works\n1. Diffs your lockfile against the base ref and collects changed resolved versions (including transitives).\n2. Checks npm provenance via the attestations API for each `name@version`.\n3. Falls back to version metadata for `dist.attestations`.\n4. Emits file+line annotations in the lockfile.\n5. If provenance exists for both the previous and new version, extracts GitHub `owner/repo` and branch from attestations and warns when they differ (repo changed or branch changed).\n\n## 🔒 Why this matters\nTrusted publishing links a package back to its source repo and build workflow, providing strong provenance guarantees. It helps ensure the package you install corresponds to audited source and CI.\n\nHowever, maintainers can still be phished or coerced into publishing without trusted publishing enabled, or switching to a non‑trusted path. In those cases, packages may still carry attestations, but the chain back to the trusted publisher can be weakened.\n\nThis action:\n- Detects when a dependency update loses npm provenance (no attestations) or loses trusted publisher (attestations but no trusted publisher marker), and\n- Fails CI by default (configurable), before that change lands in your main branch.\n\nThis is a stopgap until package managers enforce stronger policies natively. Until then, it offers a lightweight guardrail in CI.\n\n## ⚠️ Notes\n- Runs on Node 24+ and executes the TypeScript entrypoint directly.\n- `bun.lockb` is not supported. (You can generate a `bun.lock` with `bun install --save-text-lockfile`.)\n- Repository and branch change detection is best‑effort; attestation shapes vary and some packages omit repo/ref details.\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-17T10:39:42.122392"
  },
  {
    "basic_info": {
      "name": "GuitarPedal",
      "full_name": "torvalds/GuitarPedal",
      "owner": "torvalds",
      "description": null,
      "url": "https://github.com/torvalds/GuitarPedal",
      "clone_url": "https://github.com/torvalds/GuitarPedal.git",
      "ssh_url": "git@github.com:torvalds/GuitarPedal.git",
      "homepage": null,
      "created_at": "2025-09-17T01:01:29Z",
      "updated_at": "2025-09-17T10:39:21Z",
      "pushed_at": "2025-09-17T01:02:41Z"
    },
    "stats": {
      "stars": 191,
      "forks": 3,
      "watchers": 191,
      "open_issues": 4,
      "size": 322
    },
    "tech_info": {
      "language": null,
      "languages": {},
      "license": null,
      "topics": []
    },
    "content": {
      "readme": "## Random guitar pedal board design\n\n### Background\n\nThis is a personal toy project that has gone through several phases, but\nthe common theme has been that it makes absolutely no sense outside of\nthe very specific niche of \"Linus is trying to learn random things about\nelectronics\".\n\nSo keep that in mind: there is very little point to any of this to\nanybody else.  Don't expect some great useful guitar pedal experience.\n\nI call it my \"LEGO for adults\" hobby, because this got started when I\nwanted to extend my traditional after-Christmas activity (which was\nreceiving and building _actual_ LEGO kits, which has been a thing for me\nsince I was a wee tyke) with something else.\n\nSo for Christmas 2024, I got a new soldering iron and randomly started\ndoing guitar pedal kits.  And so over the next month or two, I built at\nleast two dozen kits, and had to literally look for victims to give them\naway to because I had no use for them myself.\n\n> [!NOTE]\n> Of all the kits I built, the ones I enjoyed the most were the Aion FX\n> ones, and if you are looking for a kit build of traditional analog\n> guitar pedals, I can heartily recommend them.\n>\n> The documentation, the customer service, the components, and the\n> enclosures were all top notch. See [\"Aion FX\"](https://aionfx.com/)\n\nAnyway, after building a lot of these traditional analog guitar pedal\nkits I decided I really wanted to actually understand what they did,\nbecause I really had very little experience with any analog circuits.\n\nWhile I've done some very limited electronics most of my life, almost\nall of it has been related to computers, so it's been either digital\nlogic or power supplies for them.\n\nAlso, I was looking for a different kind of soldering experience where\nthere was less snipping of legs of through-hole components.  I actually\nlike soldering SMT components, but that doesn't tend to be what those\nguitar pedal kits do.\n\nI had done some very limited PCB design with kicad a few years ago, so I\ndecided to just start learning more about analog circuits.  And then it\nkind of grew from that.\n\n### Electrical design\n\nThis is the \"fourth generation\" of my guitar pedal design journey, and\nis a new repository because the goal of the learning experience has\nevolved.\n\nWhat started out being about the analog circuits (and the power rails:\nthose were always a big thing) got to the point where I realized I\nreally want to do a mixed signal design: understanding what the circuits\ndo is one thing, re-creating some analog design from the 70s when you\ndon't actually care about the sound is another thing entirely.\n\nAlso, on the actual analog signal side, I started out using op-amps, but\nas I was attempting to learn how things actually worked, I had switched\nover to a \"discrete components only\" model, and this continues that\ntrend (except for the whole digital side, of course).\n\n> [!NOTE]\n> To me \"discrete components\" does include more optimized packages:\n> things like dual diodes or matched transistors, but not more complex\n> circuits like a op-amp (or a 555 timer or D Flip-flop or other classic\n> logic IC)\n\nAlso, because I don't typically *listen* to the end result, but look at\nit with a signal generator and an oscilloscope, I've grown to detest\npower supply noise.\n\nNot knowing what I was doing, quite a lot of my circuits have been very\nnoisy indeed, and have coupled in noise from the power supply into the\nsignal chain, and you can really see that on an oscilloscope even when\nit's not always audible.\n\nEven in op-amp designs, where the op-amp itself has a very high PSRR and\nisn't mixing power supply noise into the signal, my biasing circuits\nwere often not great, and so the op-amp would see not just the signal\nbut the power supply noise coming in through the DC biasing.\n\nAnd every time I tried a dual power rail (so that I could just keep the\nsignal ground-referenced), the noise from the switching ended up just\nalways noticeable, and the extra complexity was annoying when a lot of\neffects then didn't have any real use for the dual rail.\n\nFiltering obviously helps, but this is just a long-winded explanation\nfor why I ended up really appreciating the \"bias to ground\" JFET model\nfor the signal input side, and the common drain follower in particular.\n\nThat works with a single JFET (the MMBF5103 worked well for me), but my\nfavorite design so far is a dual-JFET LS844 with the second matched JFET\nused as a current sink.  It has basically infinite input impedance (and\ncould be DC coupled, although I do the coupling capacitor with resistor\nto ground) and gives a good output signal somewhere roughly in the\nmiddle of the single-supply 9V rail.\n\nSee [https://www.linearsystems.com/_files/ugd/7e8069_52b1022fbded45fab609459acb337629.pdf](LS844 Application note)\n\nWhy do I mention this in particular? Mainly because it's a great example\nof how completely *insane* my designs are.  That LS844 is used as a\nvoltage follower with a noticeable DC offset, and that single dual-JFET\nSOT-23-6 component is m",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-17T10:39:43.433288"
  },
  {
    "basic_info": {
      "name": "Asus-ROG-Aml-Deep-Dive",
      "full_name": "Zephkek/Asus-ROG-Aml-Deep-Dive",
      "owner": "Zephkek",
      "description": "A deep dive into the ACPI.sys DPC latency problems on Asus ROG laptops",
      "url": "https://github.com/Zephkek/Asus-ROG-Aml-Deep-Dive",
      "clone_url": "https://github.com/Zephkek/Asus-ROG-Aml-Deep-Dive.git",
      "ssh_url": "git@github.com:Zephkek/Asus-ROG-Aml-Deep-Dive.git",
      "homepage": null,
      "created_at": "2025-09-16T22:03:00Z",
      "updated_at": "2025-09-17T10:35:57Z",
      "pushed_at": "2025-09-17T09:53:43Z"
    },
    "stats": {
      "stars": 149,
      "forks": 4,
      "watchers": 149,
      "open_issues": 3,
      "size": 1410
    },
    "tech_info": {
      "language": "ASL",
      "languages": {
        "ASL": 8072935
      },
      "license": null,
      "topics": []
    },
    "content": {
      "readme": "# The ASUS Gaming Laptop ACPI Firmware Bug: A Deep Technical Investigation\r\n\r\n## If You're Here, You Know The Pain\r\n\r\nYou own a high-end ASUS ROG laptop perhaps a Strix, Scar, or Zephyrus. It's specifications are impressive: an RTX 30/40 series GPU, a top-tier Intel processor, and plenty of RAM. Yet, it stutters during basic tasks like watching a YouTube video, audio crackles and pops on Discord calls, the mouse cursor freezes for a split second, just long enough to be infuriating.\r\n\r\nYou've likely tried all the conventional fixes:\r\n- Updating every driver imaginable, multiple times.\r\n- Performing a \"clean\" reinstallation of Windows.\r\n- Disabling every conceivable power-saving option.\r\n- Manually tweaking processor interrupt affinities.\r\n- Following convoluted multi-step guides from Reddit threads.\r\n- Even installing Linux, only to find the problem persists.\r\n\r\nIf none of that worked, it's because the issue isn't with the operating system or a driver. The problem is far deeper, embedded in the machine's firmware, the BIOS.\r\n\r\n## Initial Symptoms and Measurement\r\n\r\n### The Pattern Emerges\r\n\r\nThe first tool in any performance investigator's toolkit for these symptoms is LatencyMon. It acts as a canary in the coal mine for system-wide latency issues. On an affected ASUS Zephyrus M16, the results are immediate and damning:\r\n\r\n```\r\nCONCLUSION\r\nYour system appears to be having trouble handling real-time audio and other tasks. \r\nYou are likely to experience buffer underruns appearing as drop outs, clicks or pops.\r\n\r\nHIGHEST MEASURED INTERRUPT TO PROCESS LATENCY\r\nHighest measured interrupt to process latency (μs):   65,816.60\r\nAverage measured interrupt to process latency (μs):   23.29\r\n\r\nHIGHEST REPORTED ISR ROUTINE EXECUTION TIME\r\nHighest ISR routine execution time (μs):              536.80\r\nDriver with highest ISR routine execution time:       ACPI.sys\r\n\r\nHIGHEST REPORTED DPC ROUTINE EXECUTION TIME  \r\nHighest DPC routine execution time (μs):              5,998.83\r\nDriver with highest DPC routine execution time:       ACPI.sys\r\n```\r\n\r\nThe data clearly implicates `ACPI.sys`. However, the per-CPU data reveals a more specific pattern:\r\n\r\n```\r\nCPU 0 Interrupt cycle time (s):                       208.470124\r\nCPU 0 ISR highest execution time (μs):                536.804674\r\nCPU 0 DPC highest execution time (μs):                5,998.834725\r\nCPU 0 DPC total execution time (s):                   90.558238\r\n```\r\n\r\nCPU 0 is taking the brunt of the impact, spending over 90 seconds processing interrupts while other cores remain largely unaffected. This isn't a failure of load balancing; it's a process locked to a single core.\r\n\r\nA similar test on a Scar 15 from 2022 shows the exact same culprit: high DPC latency originating from `ACPI.sys`.\r\n\r\n<img width=\"974\" height=\"511\" alt=\"latencymon\" src=\"https://github.com/user-attachments/assets/fdf6f26a-dda8-4561-82c7-349fc8c298ab\" />\r\n\r\nIt's easy to blame a Windows driver, but `ACPI.sys` is not a typical driver. It primarily functions as an interpreter for ACPI Machine Language (AML), the code provided by the laptop's firmware (BIOS). If `ACPI.sys` is slow, it's because the firmware is feeding it inefficient or flawed AML code to execute. These slowdowns are often triggered by General Purpose Events (GPEs) and traffic from the Embedded Controller (EC). To find the true source, we must dig deeper.\r\n\r\n## Capturing the Problem in More Detail: ETW Tracing\r\n\r\n### Setting Up Advanced ACPI Tracing\r\n\r\nTo understand what `ACPI.sys` is doing during these latency spikes, we can use Event Tracing for Windows (ETW) to capture detailed logs from the ACPI providers.\r\n\r\n```powershell\r\n# Find the relevant ACPI ETW providers\r\nlogman query providers | findstr /i acpi\r\n# This returns two key providers:\r\n# Microsoft-Windows-Kernel-Acpi {C514638F-7723-485B-BCFC-96565D735D4A}\r\n# Microsoft-ACPI-Provider {DAB01D4D-2D48-477D-B1C3-DAAD0CE6F06B}\r\n\r\n# Start a comprehensive trace session\r\nlogman start ACPITrace -p {DAB01D4D-2D48-477D-B1C3-DAAD0CE6F06B} 0xFFFFFFFF 5 -o C:\\Temp\\acpi.etl -ets\r\nlogman update ACPITrace -p {C514638F-7723-485B-BCFC-96565D735D4A} 0xFFFFFFFF 5 -ets\r\n\r\n# Then once we're done we can stop the trace and check the etl file and save the data in csv format aswell.\r\nlogman stop ACPITrace -ets\r\ntracerpt C:\\Temp\\acpi_providers.etl -o C:\\Temp\\acpi_events.csv -of CSV\r\n```\r\n\r\n### An Unexpected Discovery\r\n\r\nAnalyzing the resulting trace file in the Windows Performance Analyzer reveals a crucial insight. The spikes aren't random; they are periodic, occurring like clockwork every 30 to 60 seconds.\r\n\r\n<img width=\"1673\" height=\"516\" alt=\"61c7abb1-d7aa-4b69-9a88-22cca7352f00\" src=\"https://github.com/user-attachments/assets/2aac7320-3e06-4025-841c-86129f9d5b62\" />\r\n\r\nRandom interruptions often suggest hardware faults or thermal throttling. A perfectly repeating pattern points to a systemic issue, a timer or a scheduled event baked into the system's logic.\r\n\r\nThe raw event data confirms this pattern:\r\n```c",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-17T10:39:44.783305"
  },
  {
    "basic_info": {
      "name": "VoxCPM",
      "full_name": "OpenBMB/VoxCPM",
      "owner": "OpenBMB",
      "description": "VoxCPM: Tokenizer-Free TTS for Context-Aware Speech Generation and True-to-Life Voice Cloning",
      "url": "https://github.com/OpenBMB/VoxCPM",
      "clone_url": "https://github.com/OpenBMB/VoxCPM.git",
      "ssh_url": "git@github.com:OpenBMB/VoxCPM.git",
      "homepage": "",
      "created_at": "2025-09-16T03:41:49Z",
      "updated_at": "2025-09-17T10:38:42Z",
      "pushed_at": "2025-09-17T10:09:42Z"
    },
    "stats": {
      "stars": 126,
      "forks": 9,
      "watchers": 126,
      "open_issues": 1,
      "size": 1488
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 107143
      },
      "license": "Apache License 2.0",
      "topics": []
    },
    "content": {
      "readme": "## 🎙️ VoxCPM: Tokenizer-Free TTS for Context-Aware Speech Generation and True-to-Life Voice Cloning\n\n\n[![Project Page](https://img.shields.io/badge/Project%20Page-GitHub-blue)](https://github.com/OpenBMB/VoxCPM/) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-OpenBMB-yellow)](https://huggingface.co/openbmb/VoxCPM-0.5B) [![ModelScope](https://img.shields.io/badge/ModelScope-OpenBMB-purple)](https://modelscope.cn/models/OpenBMB/VoxCPM-0.5B)  [![Live Playground](https://img.shields.io/badge/Live%20PlayGround-Demo-orange)](https://huggingface.co/spaces/OpenBMB/VoxCPM-Demo) [![Samples](https://img.shields.io/badge/Page-Samples-red)](https://openbmb.github.io/VoxCPM-demopage)\n\n\n\n<div align=\"center\">\n  <img src=\"assets/voxcpm_logo.png\" alt=\"VoxCPM Logo\" width=\"40%\">\n</div>\n\n<div align=\"center\">\n\n👋 Contact us on [WeChat](assets/wechat.png)\n\n</div>\n\n## News \n* [2025.09.16] 🔥 🔥 🔥  We Open Source the VoxCPM-0.5B [weights](https://huggingface.co/openbmb/VoxCPM-0.5B)!\n* [2025.09.16] 🎉 🎉 🎉  We Provide the [Gradio PlayGround](https://huggingface.co/spaces/OpenBMB/VoxCPM-Demo) for VoxCPM-0.5B, try it now! \n\n## Overview\n\nVoxCPM is a novel tokenizer-free Text-to-Speech (TTS) system that redefines realism in speech synthesis. By modeling speech in a continuous space, it overcomes the limitations of discrete tokenization and enables two flagship capabilities: context-aware speech generation and true-to-life zero-shot voice cloning.\n\nUnlike mainstream approaches that convert speech to discrete tokens, VoxCPM uses an end-to-end diffusion autoregressive architecture that directly generates continuous speech representations from text. Built on [MiniCPM-4](https://huggingface.co/openbmb/MiniCPM4-0.5B) backbone, it achieves implicit semantic-acoustic decoupling through hierachical language modeling and FSQ constraints, greatly enhancing both expressiveness and generation stability.\n\n<div align=\"center\">\n  <img src=\"assets/voxcpm_model.png\" alt=\"VoxCPM Model Architecture\" width=\"90%\">\n</div>\n\n\n###  🚀 Key Features\n- **Context-Aware, Expressive Speech Generation** - VoxCPM comprehends text to infer and generate appropriate prosody, delivering speech with remarkable expressiveness and natural flow. It spontaneously adapts speaking style based on content, producing highly fitting vocal expression trained on a massive 1.8 million-hour bilingual corpus.\n- **True-to-Life Voice Cloning** - With only a short reference audio clip, VoxCPM performs accurate zero-shot voice cloning, capturing not only the speaker’s timbre but also fine-grained characteristics such as accent, emotional tone, rhythm, and pacing to create a faithful and natural replica.\n- **High-Efficiency Synthesis** - VoxCPM supports streaming synthesis with a Real-Time Factor (RTF) as low as 0.17 on a consumer-grade NVIDIA RTX 4090 GPU, making it possible for real-time applications.\n\n\n\n\n\n\n\n\n\n\n##  Quick Start\n\n### 🔧 Install from PyPI\n``` sh\npip install voxcpm\n```\n### 1.  Model Download (Optional)\nBy default, when you first run the script, the model will be downloaded automatically, but you can also download the model in advance.\n- Download VoxCPM-0.5B\n    ```\n    from huggingface_hub import snapshot_download\n    snapshot_download(\"openbmb/VoxCPM-0.5B\",local_files_only=local_files_only)\n    ```\n- Download ZipEnhancer and SenseVoice-Small. We use ZipEnhancer to enhance speech prompts and SenseVoice-Small for speech prompt ASR in the web demo.\n    ```\n    from modelscope import snapshot_download\n    snapshot_download('iic/speech_zipenhancer_ans_multiloss_16k_base')\n    snapshot_download('iic/SenseVoiceSmall')\n    ```\n\n### 2. Basic Usage\n```python\nimport soundfile as sf\nfrom voxcpm import VoxCPM\n\nmodel = VoxCPM.from_pretrained(\"openbmb/VoxCPM-0.5B\")\n\nwav = model.generate(\n    text=\"VoxCPM is an innovative end-to-end TTS model from ModelBest, designed to generate highly expressive speech.\",\n    prompt_wav_path=None,      # optional: path to a prompt speech for voice cloning\n    prompt_text=None,          # optional: reference text\n    cfg_value=2.0,             # LM guidance on LocDiT, higher for better adherence to the prompt, but maybe worse\n    inference_timesteps=10,   # LocDiT inference timesteps, higher for better result, lower for fast speed\n    normalize=True,           # enable external TN tool\n    denoise=True,             # enable external Denoise tool\n    retry_badcase=True,        # enable retrying mode for some bad cases (unstoppable)\n    retry_badcase_max_times=3,  # maximum retrying times\n    retry_badcase_ratio_threshold=6.0, # maximum length restriction for bad case detection (simple but effective), it could be adjusted for slow pace speech\n)\n\nsf.write(\"output.wav\", wav, 16000)\nprint(\"saved: output.wav\")\n```\n\n### 3. CLI Usage\n\nAfter installation, the entry point is `voxcpm` (or use `python -m voxcpm.cli`).\n\n```bash\n# 1) Direct synthesis (single text)\nvoxcpm --text \"VoxCPM is an innovative end-to-end TTS model from ModelBest, designed to ge",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-17T10:39:46.126923"
  },
  {
    "basic_info": {
      "name": "LLaVA-OneVision-1.5",
      "full_name": "EvolvingLMMs-Lab/LLaVA-OneVision-1.5",
      "owner": "EvolvingLMMs-Lab",
      "description": null,
      "url": "https://github.com/EvolvingLMMs-Lab/LLaVA-OneVision-1.5",
      "clone_url": "https://github.com/EvolvingLMMs-Lab/LLaVA-OneVision-1.5.git",
      "ssh_url": "git@github.com:EvolvingLMMs-Lab/LLaVA-OneVision-1.5.git",
      "homepage": null,
      "created_at": "2025-09-16T14:05:47Z",
      "updated_at": "2025-09-17T10:32:44Z",
      "pushed_at": "2025-09-17T07:52:19Z"
    },
    "stats": {
      "stars": 122,
      "forks": 7,
      "watchers": 122,
      "open_issues": 3,
      "size": 2643
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 7578354,
        "Jupyter Notebook": 521664,
        "Shell": 178138,
        "C++": 38717,
        "Cuda": 16941,
        "C": 2951,
        "Dockerfile": 2631,
        "HTML": 2625,
        "Makefile": 313
      },
      "license": null,
      "topics": []
    },
    "content": {
      "readme": "# LLaVA-OneVision-1.5: Fully Open Framework for Democratized Multimodal Training\n\n\n[🤗 Mid-Training-Data (Uploading!)](https://huggingface.co/datasets/lmms-lab/LLaVA-One-Vision-1.5-Mid-Training-85M) | \n[🤗 Insturct-Data (Uploading!)](https://huggingface.co/datasets/lmms-lab/LLaVA-One-Vision-1.5-Insturct-26M) \n\n**LLaVA-OneVision1.5** introduces a novel family of **fully open-source** Large Multimodal Models (LMMs) that achieves **state-of-the-art performance**  with substantially **lower cost** through training on **native resolution** images.\n\n1. **Superior Performance**\nA family of fully open-source large multimodal models demonstrating **superior performance** across multiple multimodal benchmarks, **outperforming Qwen2.5-VL** in most evaluation tasks.\n\n2. **High-Quality Data at Scale**\nMeticulously curated **pre-training and SFT data** with rigorous filtering and quality control, achieving **superior data efficiency** with only **64B tokens**.\n- Concept-balanced, highly diverse, high-quality caption data\n- Comprehensive instruction fine-tuning data covering a wide range of tasks\n\n3. **Ultra-Efficient Training Framework**\nComplete end-to-end training framework designed for maximum efficiency:\n- **$16K total budget** for full model training\n- **45% HFU efficiency** on A100 GPUs ($0.6 per GPU/Hour)\n- Built on **MegatronLM** with support for **MoE**, **FP8**, and **long sequence parallelization**\n- Optimized codebase for cost-effective scaling\n\n4. **Fully Open Framework** for community access and reproducibility:\n- ✅ High-quality pre-training & SFT data\n- ✅ Complete training framework & code\n- ✅ Training recipes & configurations\n- ✅ Base & instruct model checkpoints\n- ✅ Comprehensive training logs & metrics\n\n\n## Model\n\n| Model                  | #Vision Param | #Language Param | #Total Param | HF Link                                                                      |\n|------------------------|---------------|-----------------|--------------|------------------------------------------------------------------------------|\n| LLaVA-OV-1.5-4B-Instruct      | 0.3B          | 4.4B            | 4.7B         | [🤗 link]()                |\n| LLaVA-OV-1.5-8B-Instruct      | 0.3B          | 8.2B            | 8.5B         | [🤗 link](https://huggingface.co/lmms-lab/LLaVA-OneVision-1.5-8B-Instruct) |\n\n\n## Dataset\n\n![Dataset Visualization](asset/dataset.jpg)\n\n\n| Description | Link |\n|-------------|------|\n| Mid-training data for LLaVA-OneVision-1.5 | [🤗 Download (Uploading!)](https://huggingface.co/datasets/lmms-lab/LLaVA-One-Vision-1.5-Mid-Training-85M) |\n| SFT data for LLaVA-OneVision-1.5 | [🤗 Download (Uploading!)](https://huggingface.co/datasets/lmms-lab/LLaVA-OneVision-1.5-Insturct-Data) |\n\n\n## Evaluation Results\n\n\nAll evaluations were conducted using lmms_eval.\n\n|                                  | **LLaVA-OV-1.5-8B** | **Qwen2.5 VL 7B** | **LLaVA-OV-1.5-4B** | **Qwen2.5 VL 3B** |\n|:----------------------------------|:---------------:|:-------------:|:---------------:|:-------------:|\n| MMMU (Validation)                 |    **55.44**    |     51.33     |    **51.44**    |     46.44     |\n| MMMU-Pro (Standard)               |    **37.40**    |     36.30     |    **33.24**    |     31.10     |\n| MMMU-Pro (Vision)                 |      25.15      |   **32.83**   |    **23.53**    |     21.27     |\n| MMBench (English; Test)           |    **84.14**    |     83.40     |    **82.29**    |     77.97     |\n| MMBench (Chinese; Test)           |      81.00      |   **81.61**   |    **76.73**    |     74.55     |\n| MME-RealWorld (English)           |    **62.31**    |     57.33     |    **57.16**    |     51.60     |\n| MME-RealWorld (Chinese)           |    **56.11**    |     51.50     |      21.38      |   **45.38**   |\n| AI2D (With Mask)                  |    **84.16**    |     82.58     |    **84.62**    |     78.56     |\n| AI2D (Without Mask)               |    **94.11**    |     93.36     |    **92.84**    |     90.74     |\n| CV-Bench                          |    **80.82**    |     79.95     |    **74.00**    |     71.53     |\n| VL-RewardBench                    |      45.90      |   **49.65**   |    **45.90**    |     42.06     |\n| V*                                |    **78.01**    |     76.96     |      66.49      |   **69.63**   |\n| PixmoCount                        |      62.19      |   **63.33**   |    **59.17**    |     50.85     |\n| CountBench                        |    **88.19**    |     86.35     |    **77.80**    |     72.51     |\n| ChartQA                           |    **86.48**    |     84.08     |    **85.11**    |     83.36     |\n| CharXiv (Direct Questions)        |    **74.10**    |     69.80     |    **70.70**    |     58.20     |\n| DocVQA (Test)                     |    **95.00**    |     94.93     |    **93.48**    |     92.67     |\n| InfoVQA (Test)                    |      78.42      |   **81.67**   |    **75.27**    |     75.63     |\n| WeMath                            |    **33.62**    |     33.",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-17T10:39:47.440417"
  },
  {
    "basic_info": {
      "name": "VibeVoice-finetuning",
      "full_name": "voicepowered-ai/VibeVoice-finetuning",
      "owner": "voicepowered-ai",
      "description": "Unofficial WIP LoRa Finetuning repository for VibeVoice",
      "url": "https://github.com/voicepowered-ai/VibeVoice-finetuning",
      "clone_url": "https://github.com/voicepowered-ai/VibeVoice-finetuning.git",
      "ssh_url": "git@github.com:voicepowered-ai/VibeVoice-finetuning.git",
      "homepage": null,
      "created_at": "2025-09-16T10:57:57Z",
      "updated_at": "2025-09-17T10:03:52Z",
      "pushed_at": "2025-09-16T16:31:47Z"
    },
    "stats": {
      "stars": 103,
      "forks": 24,
      "watchers": 103,
      "open_issues": 2,
      "size": 189
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 312954
      },
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "\r\n  \r\n\r\n# Unofficial WIP Finetuning repo for VibeVoice\r\n\r\n  \r\n\r\n# Hardware requirements\r\n\r\n  \r\n\r\nTo train a VibeVoice 1.5B LoRa, a machine with at least 16gb VRAM is recommended.\r\n\r\nTo train a VibeVoice 7B LoRa, a machine with at least 48gb VRAM is recommended.\r\n\r\nKeep in mind longer audios increase VRAM requirements\r\n\r\n  \r\n\r\n# Installation\r\n\r\nIt is recommended to install this in a fresh environment. Specifically, the Dockerized environment `runpod/pytorch:2.8.0-py3.11-cuda12.8.1-cudnn-devel-ubuntu22.04` has been tested to work.\r\n\r\n  \r\n\r\nTransformers version 4.51.3 is known to work, while other versions have errors related to Qwen2 architecture.\r\n\r\n  \r\n```\r\ngit clone https://github.com/voicepowered-ai/VibeVoice-finetuning\r\n\r\npip install -e .\r\n\r\npip uninstall -y transformers && pip install transformers==4.51.3\r\n\r\n(OPTIONAL) wandb login\r\n\r\n(OPTIONAL) export HF_HOME=/workspace/hf_models\r\n```\r\n\r\n  \r\n\r\n# Usage\r\n\r\n  \r\n\r\n## VibeVoice 1.5B / 7B (LoRA) fine-tuning\r\n\r\n  \r\n\r\n  \r\n\r\nWe put some code together for training VibeVoice (7B) with LoRA. This uses the vendored VibeVoice model/processor and trains with a dual loss: masked CE on text tokens plus diffusion MSE on acoustic latents.\r\n\r\n  \r\n\r\n  \r\n\r\nRequirements:\r\n\r\n  \r\n\r\n- Download a compatible VibeVoice 7B or 1.5b checkpoint (config + weights) and its processor files (preprocessor_config.json) or run straight from HF model.\r\n\r\n- A 24khz audio dataset with audio files (target audio), text prompts (transcriptions) and optionally voice prompts (reference audio)\r\n\r\n  \r\n\r\n  \r\n  \r\n\r\n### Training with Hugging Face Dataset\r\n\r\n  \r\n```\r\npython -m src.finetune_vibevoice_lora \\\r\n\r\n--model_name_or_path aoi-ot/VibeVoice-Large \\\r\n\r\n--processor_name_or_path src/vibevoice/processor \\\r\n\r\n--dataset_name your/dataset \\\r\n\r\n--text_column_name text \\\r\n\r\n--audio_column_name audio \\\r\n\r\n--voice_prompts_column_name voice_prompts \\\r\n\r\n--output_dir outputTrain3 \\\r\n\r\n--per_device_train_batch_size 8 \\\r\n\r\n--gradient_accumulation_steps 16 \\\r\n\r\n--learning_rate 2.5e-5 \\\r\n\r\n--num_train_epochs 5 \\\r\n\r\n--logging_steps 10 \\\r\n\r\n--save_steps 100 \\\r\n\r\n--eval_steps 100 \\\r\n\r\n--report_to wandb \\\r\n\r\n--remove_unused_columns False \\\r\n\r\n--bf16 True \\\r\n\r\n--do_train \\\r\n\r\n--gradient_clipping \\\r\n\r\n--gradient_checkpointing False \\\r\n\r\n--ddpm_batch_mul 4 \\\r\n\r\n--diffusion_loss_weight 1.4 \\\r\n\r\n--train_diffusion_head True \\\r\n\r\n--ce_loss_weight 0.04 \\\r\n\r\n--voice_prompt_drop_rate 0.2 \\\r\n\r\n--lora_target_modules q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj \\\r\n\r\n--lr_scheduler_type cosine \\\r\n\r\n--warmup_ratio 0.03 \\\r\n\r\n--max_grad_norm 0.8\r\n```\r\n  \r\n\r\n----------\r\n\r\n  \r\n\r\n### Training with Local JSONL Dataset\r\n\r\n  \r\n```\r\npython -m src.finetune_vibevoice_lora \\\r\n\r\n--model_name_or_path aoi-ot/VibeVoice-Large \\\r\n\r\n--processor_name_or_path src/vibevoice/processor \\\r\n\r\n--train_jsonl prompts.jsonl \\\r\n\r\n--text_column_name text \\\r\n\r\n--audio_column_name audio \\\r\n\r\n--output_dir outputTrain3 \\\r\n\r\n--per_device_train_batch_size 8 \\\r\n\r\n--gradient_accumulation_steps 16 \\\r\n\r\n--learning_rate 2.5e-5 \\\r\n\r\n--num_train_epochs 5 \\\r\n\r\n--logging_steps 10 \\\r\n\r\n--save_steps 100 \\\r\n\r\n--report_to wandb \\\r\n\r\n--remove_unused_columns False \\\r\n\r\n--bf16 True \\\r\n\r\n--do_train \\\r\n\r\n--gradient_clipping \\\r\n\r\n--gradient_checkpointing False \\\r\n\r\n--ddpm_batch_mul 4 \\\r\n\r\n--diffusion_loss_weight 1.4 \\\r\n\r\n--train_diffusion_head True \\\r\n\r\n--ce_loss_weight 0.04 \\\r\n\r\n--voice_prompt_drop_rate 0.2 \\\r\n\r\n--lora_target_modules q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj \\\r\n\r\n--lr_scheduler_type cosine \\\r\n\r\n--warmup_ratio 0.03 \\\r\n\r\n--max_grad_norm 0.8\r\n```\r\n\r\n\r\n### JSONL format:\r\n\r\n  \r\n\r\nYou can provide an optional `voice_prompts` key. If it is omitted, a voice prompt will be automatically generated from the target audio.\r\n\r\n  \r\n\r\n**Example without a pre-defined voice prompt (will be auto-generated):**\r\n\r\n`{\"text\": \"Speaker 0: Speaker0 transcription.\", \"audio\": \"/workspace/wavs/segment_000000.wav\"}`\r\n\r\n  \r\n\r\n**Example with a pre-defined voice prompt:**\r\n\r\n`{\"text\": \"Speaker 0: Speaker0 transcription.\", \"audio\": \"/workspace/wavs/segment_000000.wav\", \"voice_prompts\": \"/path/to/a/different/prompt.wav\"}`\r\n\r\n  \r\n\r\n**Example with multiple speakers and voice prompts:**\r\n\r\n`{\"text\": \"Speaker 0: How is the project coming along?\\nSpeaker 1: It's going well, we should be finished by Friday.\", \"audio\": \"/data/conversations/convo_01.wav\", \"voice_prompts\": [\"/data/prompts/alice_voice_prompt.wav\", \"/data/prompts/bob_voice_prompt.wav\"]}`\r\n\r\n  \r\n  \r\n  \r\n\r\n# Notes:\r\n\r\n  \r\n\r\n- Audio is assumed to be 24 kHz; input audio will be loaded/resampled to 24 kHz.\r\n\r\n  \r\n\r\n- If you pass raw NumPy arrays or torch Tensors as audio (without sampling rate metadata), the collator assumes they are already 24 kHz. To trigger resampling, provide dicts like {\"array\": <np.ndarray>, \"sampling_rate\": <int>} or file paths.\r\n\r\n  \r\n\r\n- Tokenizers (acoustic/semantic) are frozen by default. LoRA is applied to the LLM (Qwen) and optionally to the diffusion head.\r\n\r\n  \r\n\r\n- The collator builds interleaved seq",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-17T10:39:48.784173"
  },
  {
    "basic_info": {
      "name": "aisdk-prompt-optimizer",
      "full_name": "Scale3-Labs/aisdk-prompt-optimizer",
      "owner": "Scale3-Labs",
      "description": "A tool kit for generating high quality prompts for AISDK using DSPy GEPA optimizer",
      "url": "https://github.com/Scale3-Labs/aisdk-prompt-optimizer",
      "clone_url": "https://github.com/Scale3-Labs/aisdk-prompt-optimizer.git",
      "ssh_url": "git@github.com:Scale3-Labs/aisdk-prompt-optimizer.git",
      "homepage": null,
      "created_at": "2025-09-16T17:54:46Z",
      "updated_at": "2025-09-17T10:21:29Z",
      "pushed_at": "2025-09-16T18:07:03Z"
    },
    "stats": {
      "stars": 65,
      "forks": 3,
      "watchers": 65,
      "open_issues": 0,
      "size": 106
    },
    "tech_info": {
      "language": "TypeScript",
      "languages": {
        "TypeScript": 87779,
        "Python": 8751,
        "CSS": 4475,
        "JavaScript": 605
      },
      "license": "Other",
      "topics": []
    },
    "content": {
      "readme": "# AISDK Prompt Optimizer\n\n[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)\n[![Open Source](https://badges.frapsoft.com/os/v1/open-source.svg?v=103)](https://opensource.org/)\n\nTransform your AI interactions with intelligent prompt optimization. Teach your AI, collect ideal samples, and generate optimized prompts using the powerful AISDK Prompt Optimizer.\n\n**Fully Open Source** - Built by the team that created [Langtrace AI](https://langtrace.ai) and [Zest](https://heyzest.ai)\n\n## What is GEPA?\n\n**GEPA** (Genetic-Pareto) is a reflective optimizer that adaptively evolves textual components (such as prompts) of AI systems. Unlike traditional optimization methods that only use scalar scores, GEPA leverages rich textual feedback to guide the optimization process, allowing it to propose high-performing prompts in very few rollouts.\n\nKey features of GEPA:\n- **Reflective Prompt Mutation**: Uses LLMs to reflect on execution traces and propose new instructions\n- **Rich Textual Feedback**: Leverages any textual feedback beyond just scalar rewards\n- **Pareto-based Selection**: Maintains a frontier of candidates that excel in different scenarios\n\nLearn more: [GEPA Documentation](https://dspy.ai/api/optimizers/GEPA/)\n\n## How It Works\n\n1. **Start Conversation**: Begin chatting with the AI and teach it desired behaviors through examples\n2. **Mark Examples**: Save ideal conversation samples that represent perfect responses\n3. **Run Optimization**: Let AISDK Prompt Optimizer analyze patterns and generate optimized prompts\n4. **Deploy Results**: Use the optimized prompts in your applications\n\n## Features\n\n- **Teach Your AI**: Guide your AI through interactive conversations and demonstrate the ideal responses you want to achieve\n- **Collect Ideal Samples**: Gather high-quality conversation examples that represent perfect AI behavior for your use case\n- **AISDK Prompt Optimizer**: Leverage advanced optimization algorithms to automatically generate and refine prompt candidates\n\n## Quick Start\n\n### Prerequisites\n- Node.js (18+ recommended)\n- `uv` package manager for Python\n- OpenAI API key\n- AI Gateway API key\n\n### Environment Setup\n\nBefore running the application, you need to set up your environment variables:\n\n1. Copy the example environment file:\n   ```bash\n   cp .env.example .env\n   ```\n\n2. Edit the `.env` file and add your API keys:\n   ```bash\n   # Required: OpenAI API key for AI model access\n   OPENAI_API_KEY=your_actual_openai_api_key_here\n   \n   # Required: AI Gateway API key for prompt optimization\n   AI_GATEWAY_API_KEY=your_actual_ai_gateway_api_key_here\n   ```\n\n**Important**: Never commit your actual API keys to version control. The `.env` file is already included in `.gitignore`.\n\n### Installation & Setup\n\n```bash\n# Clone the repository\ngit clone https://github.com/Scale3-Labs/aisdk-prompt-optimizer\ncd aisdk-prompt-optimizer\n\n# Install dependencies\nnpm install\n\n# Start both services (recommended)\nnpm run dev:all\n```\n\n### Alternative: Start Services Separately\n\n```bash\n# Terminal 1: Start the Python optimizer\ncd python_optimizer\nuv run app.py\n\n# Terminal 2: Start the web app\nnpm run dev\n```\n\nThe web app will be available at `http://localhost:3000` and the Python optimizer at `http://localhost:8000`. Both services need to be running for the optimization features to work.\n\n## Available Scripts\n\n- `npm run dev` - Start the Next.js development server\n- `npm run dev:py` - Start the Python optimizer server\n- `npm run dev:all` - Start both services concurrently\n\n## Architecture\n\n### Python Optimizer (dspy.GEPA)\n\nThe repository includes a lightweight Flask server exposing the `dspy.GEPA` optimizer, managed with `uv`. The Next.js `/api/optimize` route calls this server and writes optimization artifacts to:\n- `data/prompt.md` - Generated optimized prompts\n- `data/complete-optimization.json` - Complete optimization results and metadata\n\n### Web Application\n\nBuilt with Next.js and shadcn/ui components, the web interface provides:\n- Interactive chat interface for teaching the AI\n- Sample collection and management\n- Optimization trigger and results visualization\n- Modern, responsive UI with dark/light mode support\n\n## Technology Stack\n\n- **Frontend**: Next.js, TypeScript, Tailwind CSS, shadcn/ui\n- **Backend**: Python Flask server with dspy.GEPA optimizer\n- **Package Management**: npm (frontend), uv (Python)\n\n## Learn More\n\n- [DSPy Documentation](https://dspy.ai/)\n- [GEPA Optimizer API](https://dspy.ai/api/optimizers/GEPA/)\n- [Next.js Documentation](https://nextjs.org/docs)\n\n## Deployment\n\nThe easiest way to deploy your Next.js app is to use the [Vercel Platform](https://vercel.com/new?utm_medium=default-template&filter=next.js&utm_source=create-next-app&utm_campaign=create-next-app-readme) from the creators of Next.js.\n\nFor the Python optimizer, you'll need to deploy it to a Python-compatible hosting service and update the API endpoints accordingly.\n\nCheck out the [N",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-17T10:39:50.116884"
  },
  {
    "basic_info": {
      "name": "free-sqlite",
      "full_name": "fjb040911/free-sqlite",
      "owner": "fjb040911",
      "description": "Free SQLite for VSCode.Support writing SQL statements",
      "url": "https://github.com/fjb040911/free-sqlite",
      "clone_url": "https://github.com/fjb040911/free-sqlite.git",
      "ssh_url": "git@github.com:fjb040911/free-sqlite.git",
      "homepage": null,
      "created_at": "2025-09-16T06:20:19Z",
      "updated_at": "2025-09-17T10:39:26Z",
      "pushed_at": "2025-09-17T09:52:12Z"
    },
    "stats": {
      "stars": 63,
      "forks": 0,
      "watchers": 63,
      "open_issues": 0,
      "size": 9882
    },
    "tech_info": {
      "language": "TypeScript",
      "languages": {
        "TypeScript": 34158,
        "JavaScript": 16831
      },
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "# Free sqlite\n\nVSCode extension to explore and query SQLite databases.\nopen-source and free.\n\n## ✨ Features\n - 📃 Open any SQLite file - Simply click on .sqlite or .db files to open them instantly\n - 🛡 Table Explorer - Browse all tables in your database from an integrated sidebar\n - 🌈 Data Visualization - View table data in a clean, modern interface that adapts to your VS Code theme\n - 🖥 SQL statement editor - SQL statement editor, Automatically complete SQL keywords, table names, and fields\n - 📦 Query result export - The query results can be exported as Excel or CSV\n - ❤️ Favorites - Collect some of your most commonly used SQL statements\n\n## How to use\n\n### Install\nVSCode extension install!\n[Install free sqlite](https://marketplace.visualstudio.com/items?itemName=free-sqlite.free-sqlite)\n\n### Open database\nNow! Browse all tables in your database in the right panel\n![open](https://github.com/fjb040911/free-sqlite/blob/main/doc/open.gif?raw=true)\n\n### Multiple files\n![multiple](https://github.com/fjb040911/free-sqlite/blob/main/doc/multi.gif?raw=true)\n\n### SQL Editor\nAutomatically complete SQL keywords, table names, and fields\n![editor](https://github.com/fjb040911/free-sqlite/blob/main/doc/select.gif?raw=true)\n\n### Favorite\nQuickly execute or view your frequently used SQL\n![favorite](https://github.com/fjb040911/free-sqlite/blob/main/doc/favoites.gif?raw=true)\n\n### Export\n![export](https://github.com/fjb040911/free-sqlite/blob/main/doc/expot.gif?raw=true)\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-17T10:39:51.439751"
  },
  {
    "basic_info": {
      "name": "vibe-coding-playbook",
      "full_name": "RiyaParikh0112/vibe-coding-playbook",
      "owner": "RiyaParikh0112",
      "description": null,
      "url": "https://github.com/RiyaParikh0112/vibe-coding-playbook",
      "clone_url": "https://github.com/RiyaParikh0112/vibe-coding-playbook.git",
      "ssh_url": "git@github.com:RiyaParikh0112/vibe-coding-playbook.git",
      "homepage": null,
      "created_at": "2025-09-16T13:58:41Z",
      "updated_at": "2025-09-17T10:03:24Z",
      "pushed_at": "2025-09-16T17:58:27Z"
    },
    "stats": {
      "stars": 54,
      "forks": 23,
      "watchers": 54,
      "open_issues": 1,
      "size": 50
    },
    "tech_info": {
      "language": null,
      "languages": {},
      "license": null,
      "topics": []
    },
    "content": {
      "readme": "# vibe-coding-playbook",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-17T10:39:52.759948"
  },
  {
    "basic_info": {
      "name": "grok2api",
      "full_name": "VeroFess/grok2api",
      "owner": "VeroFess",
      "description": "rewrite grok2api",
      "url": "https://github.com/VeroFess/grok2api",
      "clone_url": "https://github.com/VeroFess/grok2api.git",
      "ssh_url": "git@github.com:VeroFess/grok2api.git",
      "homepage": null,
      "created_at": "2025-09-16T12:48:33Z",
      "updated_at": "2025-09-17T10:18:03Z",
      "pushed_at": "2025-09-17T07:19:37Z"
    },
    "stats": {
      "stars": 44,
      "forks": 19,
      "watchers": 44,
      "open_issues": 0,
      "size": 53
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 160146,
        "HTML": 33516,
        "Dockerfile": 306
      },
      "license": null,
      "topics": []
    },
    "content": {
      "readme": "# Grok API Gateway\n\n## 与原版差异\n\n本 fork 版本相较于原版增加了以下功能：\n\n0. **基本全部重写了...**\n1. **自动获取 x-statsig-id** - 使用 Playwright 自动获取并管理认证头\n2. **流模式标签过滤** - 自动移除响应中的 `<xaiArtifact` 等标签\n3. **增强统计功能** - 改进的令牌使用统计和监控\n4. **Grok4支持** - 反正我能用.jpg\n\n## 环境变量配置\n\n### 必需配置\n\n| 环境变量 | 描述 | 默认值 | 示例 |\n|---------|------|--------|------|\n| `API_KEY` | API 访问密钥 | `sk-123456` | `sk-your-api-key` |\n| `SSO` | Grok SSO 令牌（普通） | - | `token1,token2,token3` |\n| `SSO_SUPER` | Grok SSO 令牌（超级） | - | `super_token1,super_token2` |\n\n### 可选配置\n\n| 环境变量 | 描述 | 默认值 | 有效值 | 示例 |\n|---------|------|--------|--------|------|\n| `IS_CUSTOM_SSO` | 允许动态 SSO 令牌 | `false` | `true/false` | `true` |\n| `IS_TEMP_CONVERSATION` | 临时对话模式 | `true` | `true/false` | `false` |\n| `SHOW_THINKING` | 显示推理过程 | `false` | `true/false` | `true` |\n| `SHOW_SEARCH_RESULTS` | 显示搜索结果 | `true` | `true/false` | `false` |\n| `IS_SUPER_GROK` | 启用超级 Grok 功能 | `false` | `true/false` | `true` |\n| `MANAGER_SWITCH` | 启用 Web 管理界面 | - | `true/false` | `true` |\n| `ADMINPASSWORD` | 管理界面密码 | - | 任意字符串 | `admin123` |\n| `PORT` | 服务端口 | `5200` | 数字 | `8080` |\n| `PROXY` | 代理服务器 | - | HTTP/SOCKS5 URL | `http://127.0.0.1:1080` |\n| `CF_CLEARANCE` | Cloudflare 令牌 | - | CF 令牌字符串 | `cf_clearance_token` |\n| `DISABLE_DYNAMIC_HEADERS` | 禁用动态头部获取（禁用 Playwright 自动获取 x-statsig-id） | `false` | `true/false` | `true` |\n| `FILTERED_TAGS` | 过滤标签列表 | `xaiartifact,xai:tool_usage_card,grok:render,details,summary` | 逗号分隔 | `tag1,tag2,tag3` |\n| `TAG_CONFIG` | 过滤标签配置 | `{\"xaiartifact\":{\"behavior\":\"preserve_content\"},\"xai:tool_usage_card\":{\"behavior\":\"remove_all\"},\"grok:render\":{\"behavior\":\"remove_all\"},\"details\":{\"behavior\":\"preserve_content\"},\"summary\":{\"behavior\":\"preserve_content\"}}` | json | `{\"xaiartifact\":{\"behavior\":\"preserve_content\"},\"xai:tool_usage_card\":{\"behavior\":\"remove_all\"},\"grok:render\":{\"behavior\":\"remove_all\"},\"details\":{\"behavior\":\"preserve_content\"},\"summary\":{\"behavior\":\"preserve_content\"}}` |\n| `CONTENT_TYPE_MAPPINGS` | 过滤标签重写配置 | 太长了,看源码 | json | {\"text/plain\":{\"stag\":\"```\",\"etag\":\"```\"},\"text/python\":{\"stag\":\"```python\\n\",\"etag\":\"\\n```\"}} |\n\n### 标签过滤配置\n\n添加了高级标签过滤功能，可在流式响应中自动处理特定的 XML/HTML 标签。\n\n注意配置错误会直接破坏输出!!!\n\n#### FILTERED_TAGS\n\n**描述**：标签过滤列表, 当遇到不在列表中的标签时会立即放弃后续重写\n\n**格式**：逗号分隔的标签名称，小写\n\n**默认值**：`xaiartifact,xai:tool_usage_card,grok:render,details,summary`\n\n**示例**：\n```bash\nFILTERED_TAGS=xaiartifact,grok:render,grok:thinking\n```\n\n#### TAG_CONFIG\n\n**描述**：高级标签行为配置，支持为不同标签设置不同的处理策略。\n\n**格式**：JSON 对象，键为标签名称（小写），值为配置对象\n\n**配置选项**：\n- `behavior`: 标签行为\n  - `\"preserve_content\"`: 保留内容，添加格式化标记\n  - `\"remove_all\"`: 完全移除标签和内容\n\n**默认值**：基于 FILTERED_TAGS 自动生成\n\n**示例**：\n```json\n{\n  \"xaiartifact\": {\"behavior\": \"preserve_content\"},\n  \"xai:tool_usage_card\": {\"behavior\": \"remove_all\"},\n  \"grok:render\": {\"behavior\": \"remove_all\"},\n  \"details\": {\"behavior\": \"preserve_content\"},\n  \"summary\": {\"behavior\": \"preserve_content\"}\n}\n```\n\n**在 docker-compose.yml 中配置**：\n```yaml\nenvironment:\n  TAG_CONFIG: '{\"xaiartifact\":{\"behavior\":\"preserve_content\"},\"xai:tool_usage_card\":{\"behavior\":\"remove_all\"},\"grok:render\":{\"behavior\":\"remove_all\"},\"details\":{\"behavior\":\"preserve_content\"},\"summary\":{\"behavior\":\"preserve_content\"}}'\n```\n\n#### CONTENT_TYPE_MAPPINGS\n\n**描述**：内容类型映射配置，定义不同 contentType 的格式化标记。\n\n**格式**：JSON 对象，键为 MIME 类型，值为包含 stag（开始标记）和 etag（结束标记）的对象\n\n**默认映射**：\n```json\n{\n  \"text/plain\": {\"stag\": \"```\", \"etag\": \"```\"},\n  \"text/markdown\": {\"stag\": \"\", \"etag\": \"\"},\n  \"application/json\": {\"stag\": \"```json\\n\", \"etag\": \"\\n```\"}\n}\n```\n\n**示例配置**：\n```yaml\nenvironment:\n  CONTENT_TYPE_MAPPINGS: '{\"text/plain\":{\"stag\":\"```\",\"etag\":\"```\"},\"text/python\":{\"stag\":\"```python\\n\",\"etag\":\"\\n```\"}}'\n```\n\n**工作原理**：\n1. 当遇到 `preserve_content` 行为的标签时，会查找标签的 `contentType` 属性\n2. 根据 `contentType` 在映射表中查找对应的格式化标记\n3. 用 `stag` + 内容 + `etag` 替换原始标签和对应的封闭标签\n\n\n## 快速开始\n\n### 使用 Docker Hub 镜像\n\n现在可以直接从 Docker Hub 拉取预构建的镜像：\n\n```bash\n# 拉取镜像\ndocker pull verofess/grok2api\n\n# 运行容器\ndocker run -d \\\n  --name grok2api \\\n  -p 5200:5200 \\\n  -e API_KEY=sk-your-api-key \\\n  -e SSO=your-sso-token \\\n  verofess/grok2api\n\n# 或者使用 docker-compose\ndocker-compose up -d\n```\n\n### Docker Compose 示例\n\n```yaml\nservices:\n  grok2api:\n    image: verofess/grok2api\n    container_name: grok2api\n    ports:\n      - \"5200:5200\"\n    environment:\n      - API_KEY=sk-your-api-key\n      - SSO=your-sso-token\n      - IS_TEMP_CONVERSATION=true\n      - SHOW_THINKING=false\n    restart: unless-stopped\n```",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-17T10:39:54.090039"
  },
  {
    "basic_info": {
      "name": "Qwen3-ASR-Toolkit",
      "full_name": "QwenLM/Qwen3-ASR-Toolkit",
      "owner": "QwenLM",
      "description": "Official Python toolkit for the Qwen3-ASR API. Parallel high‑throughput calls, robust long‑audio transcription, multi‑sample‑rate support.",
      "url": "https://github.com/QwenLM/Qwen3-ASR-Toolkit",
      "clone_url": "https://github.com/QwenLM/Qwen3-ASR-Toolkit.git",
      "ssh_url": "git@github.com:QwenLM/Qwen3-ASR-Toolkit.git",
      "homepage": "",
      "created_at": "2025-09-16T09:03:49Z",
      "updated_at": "2025-09-17T10:38:39Z",
      "pushed_at": "2025-09-17T07:22:44Z"
    },
    "stats": {
      "stars": 41,
      "forks": 2,
      "watchers": 41,
      "open_issues": 0,
      "size": 10
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 11717
      },
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "# Qwen3-ASR-Toolkit\n\n[![PyPI version](https://badge.fury.io/py/qwen3-asr-toolkit.svg)](https://badge.fury.io/py/qwen3-asr-toolkit)\n[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://www.python.org/downloads/)\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n\nAn advanced, high-performance Python command-line toolkit for using the **Qwen-ASR API** (formerly Qwen3-ASR-Flash). This implementation overcomes the API's 3-minute audio length limitation by intelligently splitting long audio/video files and processing them in parallel, enabling rapid transcription of hours-long content.\n\n## 🚀 Key Features\n\n-   **Break the 3-Minute Limit**: Seamlessly transcribe audio and video files of any length by bypassing the official API's duration constraint.\n-   **Smart Audio Splitting**: Utilizes **Voice Activity Detection (VAD)** to split audio into meaningful chunks at natural silent pauses. This ensures that words and sentences are not awkwardly cut off.\n-   **High-Speed Parallel Processing**: Leverages multi-threading to send audio chunks to the Qwen-ASR API concurrently, dramatically reducing the total transcription time for long files.\n-   **Automatic Audio Resampling**: Automatically converts audio from any sample rate and channel count to the 16kHz mono format required by the Qwen-ASR API. You can use any audio file without worrying about pre-processing.\n-   **Universal Media Support**: Supports virtually any audio and video format (e.g., `.mp4`, `.mov`, `.mkv`, `.mp3`, `.wav`, `.m4a`) thanks to its reliance on FFmpeg.\n-   **Simple & Easy to Use**: A straightforward command-line interface allows you to get started with just a single command.\n\n## ⚙️ How It Works\n\nThis tool follows a robust pipeline to deliver fast and accurate transcriptions for long-form media:\n\n1.  **Media Loading**: The script first loads your local audio or video file.\n2.  **VAD-based Chunking**: It analyzes the audio stream using Voice Activity Detection (VAD) to identify silent segments.\n3.  **Intelligent Splitting**: The audio is then split into smaller chunks based on the detected silences. Each chunk is kept under the 3-minute API limit, preventing mid-sentence cuts.\n4.  **Parallel API Calls**: A thread pool is initiated to upload and process these chunks concurrently using the DashScope Qwen-ASR API.\n5.  **Result Aggregation**: The transcribed text segments from all chunks are collected, re-ordered, and saved.\n\n## 🏁 Getting Started\n\nFollow these steps to set up and run the project on your local machine.\n\n### Prerequisites\n\n-   Python 3.8 or higher.\n-   **FFmpeg**: The script requires FFmpeg to be installed on your system to handle media files.\n    -   **Ubuntu/Debian**: `sudo apt update && sudo apt install ffmpeg`\n    -   **macOS**: `brew install ffmpeg`\n    -   **Windows**: Download from the [official FFmpeg website](https://ffmpeg.org/download.html) and add it to your system's PATH.\n-   **DashScope API Key**: You need an API key from Alibaba Cloud's DashScope.\n    -   You can obtain one from the [DashScope Console](https://dashscope.console.aliyun.com/apiKey). If you are calling the API services of Tongyi Qwen for the first time, you can follow the tutorial on [this website](https://help.aliyun.com/zh/model-studio/first-api-call-to-qwen) to create your own API Key.\n    -   For better security and convenience, it is **highly recommended** to set your API key as an environment variable named `DASHSCOPE_API_KEY`. The script will automatically use it, and you won't need to pass the `--api-key` argument in the command.\n\n        **On Linux/macOS:**\n        ```bash\n        export DASHSCOPE_API_KEY=\"your_api_key_here\"\n        ```\n        *(To make this permanent, add the line to your `~/.bashrc`, `~/.zshrc`, or `~/.profile` file.)*\n\n        **On Windows (Command Prompt):**\n        ```cmd\n        set DASHSCOPE_API_KEY=\"your_api_key_here\"\n        ```\n\n        **On Windows (PowerShell):**\n        ```powershell\n        $env:DASHSCOPE_API_KEY=\"your_api_key_here\"\n        ```\n        *(For a permanent setting on Windows, search for \"Edit the system environment variables\" in the Start Menu and add `DASHSCOPE_API_KEY` to your user variables.)*\n\n### Installation\n\nWe recommend installing the tool directly from PyPI for the simplest setup.\n\n#### Option 1: Install from PyPI (Recommended)\n\nSimply run the following command in your terminal. This will install the package and make the `qwen3-asr` command available system-wide.\n\n```bash\npip install qwen3-asr-toolkit\n```\n\n#### Option 2: Install from Source\n\nIf you want to install the latest development version or contribute to the project, you can install from the source code.\n\n1.  Clone the repository:\n    ```bash\n    git clone https://github.com/QwenLM/Qwen3-ASR-Toolkit.git\n    cd Qwen3-ASR-Toolkit\n    ```\n\n2.  Install the package:\n    ```bash\n    pip install .\n    ```\n\n## 📖 Usage\n\nOnce installed, you can use the `qwen3-asr` command dire",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-17T10:39:55.436501"
  },
  {
    "basic_info": {
      "name": "CRT_Python_AI_A",
      "full_name": "gilshan-s/CRT_Python_AI_A",
      "owner": "gilshan-s",
      "description": "Coding",
      "url": "https://github.com/gilshan-s/CRT_Python_AI_A",
      "clone_url": "https://github.com/gilshan-s/CRT_Python_AI_A.git",
      "ssh_url": "git@github.com:gilshan-s/CRT_Python_AI_A.git",
      "homepage": null,
      "created_at": "2025-09-16T04:28:17Z",
      "updated_at": "2025-09-17T05:01:25Z",
      "pushed_at": "2025-09-16T05:24:03Z"
    },
    "stats": {
      "stars": 39,
      "forks": 37,
      "watchers": 39,
      "open_issues": 0,
      "size": 2
    },
    "tech_info": {
      "language": null,
      "languages": {},
      "license": null,
      "topics": []
    },
    "content": {
      "readme": "# CRT_Python_AI_A\nCoding\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-17T10:39:56.810831"
  },
  {
    "basic_info": {
      "name": "ECE_F_CRT_PYTHON",
      "full_name": "gilshan-s/ECE_F_CRT_PYTHON",
      "owner": "gilshan-s",
      "description": "CODING",
      "url": "https://github.com/gilshan-s/ECE_F_CRT_PYTHON",
      "clone_url": "https://github.com/gilshan-s/ECE_F_CRT_PYTHON.git",
      "ssh_url": "git@github.com:gilshan-s/ECE_F_CRT_PYTHON.git",
      "homepage": null,
      "created_at": "2025-09-17T04:49:52Z",
      "updated_at": "2025-09-17T05:02:14Z",
      "pushed_at": "2025-09-17T04:49:52Z"
    },
    "stats": {
      "stars": 38,
      "forks": 40,
      "watchers": 38,
      "open_issues": 0,
      "size": 0
    },
    "tech_info": {
      "language": null,
      "languages": {},
      "license": null,
      "topics": []
    },
    "content": {
      "readme": "# ECE_F_CRT_PYTHON\nCODING\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-17T10:39:58.185394"
  },
  {
    "basic_info": {
      "name": "autogen-financial-analysis",
      "full_name": "liangdabiao/autogen-financial-analysis",
      "owner": "liangdabiao",
      "description": "一个基于微软AutoGen框架的企业级金融分析系统，使用多Agent架构提供全面的财务分析、风险评估和量化投资分析功能。 核心功能: 多源数据收集 : 整合Yahoo Finance、Alpha Vantage、Quandl等多个金融数据源, 智能财务分析 : 基于AutoGen的多Agent协作分析, 风险评估 : VaR计算、压力测试、蒙特卡洛模拟, 量化分析 : 因子模型、投资组合优化、机器学习预测, 实时监控 : 系统性能监控和告警, 数据可视化 : 交互式图表和报告生成,",
      "url": "https://github.com/liangdabiao/autogen-financial-analysis",
      "clone_url": "https://github.com/liangdabiao/autogen-financial-analysis.git",
      "ssh_url": "git@github.com:liangdabiao/autogen-financial-analysis.git",
      "homepage": null,
      "created_at": "2025-09-16T08:42:33Z",
      "updated_at": "2025-09-17T10:27:00Z",
      "pushed_at": "2025-09-17T00:24:38Z"
    },
    "stats": {
      "stars": 34,
      "forks": 8,
      "watchers": 34,
      "open_issues": 0,
      "size": 349
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 873205,
        "HTML": 52025,
        "PLpgSQL": 7113,
        "Dockerfile": 917
      },
      "license": null,
      "topics": []
    },
    "content": {
      "readme": "# AutoGen Financial Analysis System\n\n一个基于微软AutoGen框架的企业级金融分析系统，使用多Agent架构提供全面的财务分析、风险评估和量化投资分析功能。\n\n## 🚀 功能特性\n\n### 🔍 核心功能\n- **多源数据收集**: 整合Yahoo Finance、Alpha Vantage等多个金融数据源\n- **智能财务分析**: 基于AutoGen的多Agent协作分析\n- **风险评估**: VaR计算、压力测试、蒙特卡洛模拟\n- **量化分析**: 因子模型、投资组合优化、策略回测、机器学习预测\n- **实时监控**: 系统性能监控和告警\n- **数据可视化**: 交互式图表和报告生成\n\n### 🏗️ 技术架构\n- **微服务架构**: 模块化设计，支持水平扩展\n- **异步处理**: 高性能异步任务处理\n- **缓存系统**: 多级缓存策略，提升响应速度\n- **安全性**: 完整的身份认证、授权和加密\n- **监控告警**: Prometheus + Grafana监控体系\n- **容器化**: Docker和Kubernetes部署支持\n\n## 📦 安装指南\n\n### 系统要求\n- Python 3.8+\n- Redis 6.0+\n- PostgreSQL 12+\n- Docker (可选)\n\n### 快速安装\n\n1. **克隆项目**\n```bash\ngit clone https://github.com/your-username/autogen-financial-analysis.git\ncd autogen-financial-analysis\n```\n\n2. **创建虚拟环境**\n```bash\npython -m venv venv\nsource venv/bin/activate  # Linux/Mac\n# 或\nvenv\\Scripts\\activate     # Windows\n```\n\n3. **安装依赖**\n```bash\npip install -r requirements.txt\n```\n\n4. **配置环境变量**\n```bash\ncp .env.example .env\n# 编辑 .env 文件，添加必要的API密钥\n```\n\n### Docker部署\n\n```bash\n# 构建并启动所有服务\ndocker-compose up -d\n\n# 查看服务状态\ndocker-compose ps\n\n# 查看日志\ndocker-compose logs -f\n```\n\n## 🛠️ 使用方法\n\n### 命令行界面\n\n#### 基本用法\n```bash\n# 分析单个公司\npython -m src.main analyze AAPL\n\n# 分析投资组合\npython -m src.main portfolio AAPL MSFT GOOG\n\n# 交互模式\npython -m src.main interactive\n```\n\n#### 高级选项\n```bash\n# 指定分析类型\npython -m src.main analyze AAPL --type comprehensive\n\n# 导出报告\npython -m src.main analyze AAPL --format html,pdf\n\n# 自定义配置\npython -m src.main analyze AAPL --config custom_config.yaml\n```\n\n#### 量化分析选项\n```bash\n# 对单个股票进行量化分析\npython -m src.main quant AAPL\n\n# 使用特定因子进行分析\npython -m src.main quant AAPL --factors momentum value growth\n\n# 使用特定因子模型\npython -m src.main quant AAPL --method carhart\n\n# 导出量化分析报告\npython -m src.main quant AAPL --format html,pdf,json\n```\n\n#### 策略回测选项\n```bash\n# 运行动量策略回测\npython -m src.main backtest --strategy momentum --start-date 2020-01-01 --end-date 2023-01-01\n\n# 设置回测参数\npython -m src.main backtest --strategy momentum --start-date 2020-01-01 --end-date 2023-01-01 --initial-capital 100000 --commission 0.001\n\n# 导出回测报告\npython -m src.main backtest --strategy momentum --start-date 2020-01-01 --end-date 2023-01-01 --format html,pdf\n```\n\n#### 策略优化选项\n```bash\n# 优化策略参数\npython -m src.main optimize --strategy momentum --param window=5,10,15,20\n\n# 设置优化时间范围\npython -m src.main optimize --strategy momentum --param window=5,10,15,20 --start-date 2020-01-01 --end-date 2023-01-01\n```\n\n#### 投资组合优化选项\n```bash\n# 使用均值-方差优化方法\npython -m src.main optimize-portfolio --symbols AAPL MSFT GOOG --method mean_variance\n\n# 使用风险平价优化方法\npython -m src.main optimize-portfolio --symbols AAPL MSFT GOOG --method risk_parity\n\n# 设置风险厌恶系数\npython -m src.main optimize-portfolio --symbols AAPL MSFT GOOG --method mean_variance --risk-aversion 1.5\n```\n\n### Web界面\n\n启动Web服务：\n```bash\npython -m src.api.app\n```\n\n访问 `http://localhost:8000` 使用Web界面。\n\n### API接口\n\n#### 创建分析任务\n```bash\ncurl -X POST \"http://localhost:8000/api/v1/analysis\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"symbols\": [\"AAPL\", \"MSFT\"],\n    \"analysis_type\": \"comprehensive\",\n    \"export_formats\": [\"html\", \"pdf\"]\n  }'\n```\n\n#### 查看任务状态\n```bash\ncurl -X GET \"http://localhost:8000/api/v1/analysis/{task_id}\"\n```\n\n#### WebSocket实时更新\n```javascript\nconst ws = new WebSocket('ws://localhost:8000/ws');\nws.onmessage = function(event) {\n    const data = JSON.parse(event.data);\n    console.log('任务更新:', data);\n};\n```\n\n## 📊 分析报告\n\n### 财务分析报告\n- **盈利能力分析**: ROE、ROA、毛利率、净利率\n- **偿债能力分析**: 资产负债率、流动比率、速动比率\n- **运营效率分析**: 总资产周转率、存货周转率\n- **成长性分析**: 收入增长率、利润增长率\n- **杜邦分析**: ROE分解为净利润率、资产周转率和权益乘数\n\n### 风险评估报告\n- **市场风险**: VaR、CVaR、Beta系数\n- **信用风险**: Z-Score、Altman模型\n- **流动性风险**: 流动性覆盖率、净稳定资金率\n- **操作风险**: 历史模拟、蒙特卡洛模拟\n- **压力测试**: 极端市场情景分析\n\n### 量化分析报告\n- **因子分析**: 多因子暴露、因子收益率、信息系数\n- **投资组合优化**: 有效前沿、风险平价、最大分散化\n- **策略回测**: 累计收益、最大回撤、夏普比率\n- **风险贡献分析**: 各资产对组合风险的贡献度\n- **绩效归因**: 收益来源分解\n\n### 投资组合分析报告\n- **有效前沿**: 风险收益最优化组合\n- **夏普比率**: 风险调整后收益\n- **最大回撤**: 历史最大损失\n- **相关系数**: 资产间相关性分析\n- **风险平价**: 风险贡献度优化\n\n## 🔧 配置说明\n\n### 主要配置文件\n\n#### config.yaml\n```yaml\n# AutoGen配置\nautogen:\n  gpt_model: \"gpt-4\"\n  temperature: 0.7\n  max_tokens: 4000\n\n# 数据源配置\ndata_sources:\n  yahoo_finance:\n    timeout: 30\n    retry_count: 3\n  alpha_vantage:\n    api_key: \"${ALPHA_VANTAGE_API_KEY}\"\n    calls_per_minute: 5\n```\n\n#### 环境变量\n```bash\n# API密钥\nYAHOO_FINANCE_API_KEY=your_key_here\nALPHA_VANTAGE_API_KEY=your_key_here\n\n# 数据库配置\nDATABASE_URL=postgresql://user:password@localhost:5432/autogen_financial\nREDIS_URL=redis://localhost:6379/0\n\n# 安全配置\nSECRET_KEY=your_secret_key_here\nJWT_SECRET=your_jwt_secret_here\n```\n\n## 🧪 测试\n\n### 运行测试\n```bash\n# 运行所有测试\npytest\n\n# 运行特定模块测试\npytest tests/test_data.py\n\n# 运行API测试\npytest tests/test_api.py\n\n# 生成覆盖率报告\npytest --cov=src --cov-report=html\n```\n\n### 测试覆盖率\n- 数据收集模块: 95%\n- 财务分析模块: 92%\n- 风险分析模块: 90%\n- API接口: 88%\n- 整体覆盖率: 93%\n\n## 📈 性能监控\n\n### 系统指标\n- CPU使用率\n- 内存使用量\n- 磁盘I/O\n- 网络吞吐量\n- 数据库连接数\n- Redis命中率\n\n### 业务指标\n- 数据收集成功率\n- 分析任务执行时间\n- API响应时间\n- 错误率\n- 用户活跃度\n\n### 访问监控界面\n```bash\n# Grafana仪表板\nhttp://localhost:3000\n\n# Prometheus查询界面\nhttp://localhost:9090\n```\n\n## 🔒 安",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-17T10:39:59.504145"
  },
  {
    "basic_info": {
      "name": "C3_CRT_Python",
      "full_name": "gilshan-s/C3_CRT_Python",
      "owner": "gilshan-s",
      "description": "Coding",
      "url": "https://github.com/gilshan-s/C3_CRT_Python",
      "clone_url": "https://github.com/gilshan-s/C3_CRT_Python.git",
      "ssh_url": "git@github.com:gilshan-s/C3_CRT_Python.git",
      "homepage": null,
      "created_at": "2025-09-16T09:17:49Z",
      "updated_at": "2025-09-16T09:46:04Z",
      "pushed_at": "2025-09-16T09:41:18Z"
    },
    "stats": {
      "stars": 30,
      "forks": 29,
      "watchers": 30,
      "open_issues": 0,
      "size": 2
    },
    "tech_info": {
      "language": null,
      "languages": {},
      "license": null,
      "topics": []
    },
    "content": {
      "readme": "# C3_CRT_Python\nCoding\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-17T10:40:00.869609"
  },
  {
    "basic_info": {
      "name": "InternVLA-M1",
      "full_name": "InternRobotics/InternVLA-M1",
      "owner": "InternRobotics",
      "description": null,
      "url": "https://github.com/InternRobotics/InternVLA-M1",
      "clone_url": "https://github.com/InternRobotics/InternVLA-M1.git",
      "ssh_url": "git@github.com:InternRobotics/InternVLA-M1.git",
      "homepage": null,
      "created_at": "2025-09-16T12:47:28Z",
      "updated_at": "2025-09-17T10:31:53Z",
      "pushed_at": "2025-09-17T10:31:50Z"
    },
    "stats": {
      "stars": 28,
      "forks": 0,
      "watchers": 28,
      "open_issues": 0,
      "size": 21157
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 595684,
        "Shell": 10860,
        "Makefile": 661
      },
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "# InternVLA-M1\n\n**InternVLA-M1** is an open-source, end-to-end **vision–language–action (VLA) framework** for building and researching generalist robot policies.\n\nhttps://github.com/user-attachments/assets/e83ae046-a503-46a8-95e4-ef381919b7f8\n\n[![Paper](https://img.shields.io/badge/Paper-arXiv-red.svg)](https://github.com/InternRobotics/InternVLA-M1/blob/InternVLA-M1/assets/InternVLA_M1.pdf) [![Website](https://img.shields.io/badge/Website-GitHub%20Pages-blue.svg)](https://internrobotics.github.io/internvla-m1.github.io) [![Demo](https://img.shields.io/badge/Demo-YouTube-red.svg)](https://youtu.be/n129VDqJCk4) [![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)\n\n![](assets/teaser.png)\n\n## 🔥 Key Features\n\n1. **Modular & Extensible**  \n   All core components (model architecture, training data, training strategies, evaluation pipeline) are fully decoupled, enabling independent development, debugging, and extension of each module.\n\n\n2. **Dual-System and Dual-Supervision**\n   InternVLA-M1 integrates both a language head and an action head under a unified framework, enabling collaborative training with dual supervision. \n\n3. **Efficient Training & Fast Convergence**\n   Learns spatial and visual priors from large-scale multimodal pretraining and transfers them via spatial prompt fine-tuning. Achieves strong performance (e.g., SOTA-level convergence on  in \\~2.5 epochs without separate action pretraining). \n\n## 🎯 Target Audience\n\n1. Users who want to leverage open-source VLMs (e.g., Qwen2.5-VL) for robot control.\n2. Teams co-training action datasets jointly with multimodal (vision–language) data.\n3. Researchers exploring alternative VLA architectures and training strategies.\n\n## 📊 Experimental Results\n|             | WindowX | Google Robot(VA) | Google Robot(VM) | LIBERO |\n|-------------|---------|------------------|------------------|--------|\n| pi0         | 27.1    | 54.8             | 58.8             | 94.2   |\n| gr00t       | 61.9    | 44.5             | 35.2             | 93.9   |\n| InterVLA-M1 |**71.7** |**76.0**          |**80.7**          |**95.9**|\n|             |         |                  |                  |        |\n\n\n\n\n\n\n# 🚀 Quick Start\n\n## 🛠 Environment Setup\n\n```bash\n# Clone the repo\ngit clone https://github.com/InternRobotics/InternVLA-M1\n\n# Create conda environment\nconda create -n internvla-m1 python=3.10 -y\nconda activate internvla-m1\n\n# Install requirements\npip install -r requirements.txt\n\n# Install FlashAttention2\npip install flash-attn --no-build-isolation\n\n# Install InternVLA-M1\npip install -e .\n```\n\n## 📘 Examples\n\nWe provide several end-to-end examples for reference:\n\n* **Reproduce InternVLA-M1 in simplerEnv**\n  [Example](/examples/simplerEnv/setup.md)\n\n* **Training/Deployment on real robots**\n  [Example](/examples/real_robot/setup.md)\n\n* **Extending InternVLA-M1**\n  [Example](examples/extending_m1/README.md)\n\n## 📈 Model Zoo\nWe will release a series of pretrained models and checkpoints to facilitate reproduction and downstream use.\n\n- Full list and download links: assets/MODEL_ZOO.md\n\nStatus: rolling release. If you need early access or encounter broken links, please open an issue.\n\n# 🗺️ Roadmap\n\n* [ ] Release model weights (Stay tuned, coming soon)\n* [ ] Add multi-task mixed training examples\n* [ ] Release real-robot demo\n* [ ] Unify evaluation scripts and metrics\n\n# 🤝 Contributing\n\nWe welcome contributions via Pull Requests or Issues.\nPlease include detailed logs and reproduction steps when reporting bugs.\n\n# 📜 Citation\n\nIf you find this useful in your research, please consider citing:\n\n```bibtex\n@misc{internvla2024,\n  title  = {InternVLA-M1: Latent Spatial Grounding for Instruction-Following Robotic Manipulation},\n  author = {InternVLA-M1 Contributors},\n  year   = {2025},\n  booktitle={arXiv},\n}\n```\n\n# 📬 Contact\n\n* Issues: Submit via GitHub Issues with detailed logs and steps\n\n# 🙏 Acknowledgements\n\nWe thank the open-source community for their inspiring work. This project builds upon and is inspired by the following projects (alphabetical order):\n- [IPEC-COMMUNITY](https://huggingface.co/IPEC-COMMUNITY): Curated OXE / LIBERO style multi-task datasets and formatting examples.\n- [Isaac-GR00T](https://github.com/NVIDIA/Isaac-GR00T): Standardized action data loader (GR00T-LeRobot).\n- [Qwen2.5-VL](https://github.com/QwenLM/Qwen2.5-VL/blob/main/qwen-vl-finetune/README.md): Multimodal input/output format, data loader, and pretrained VLM backbone.\n- [CogACT](https://github.com/microsoft/CogACT/tree/main/action_model): Reference for a DiT-style action head design.\n- [llavavla](https://github.com/JinhuiYE/llavavla): Baseline code structure and engineering design references.\n\n\nNotes:\n- If any required attribution or license header is missing, please open an issue and we will correct it promptly.\n- All third-party resources remain under their original licenses; users should comply with respective terms.\n\n\n---\n\nThanks for using **InternVLA-M1**! 🌟\nIf you find it useful, please co",
      "default_branch": "InternVLA-M1"
    },
    "fetched_at": "2025-09-17T10:40:02.197095"
  },
  {
    "basic_info": {
      "name": "ComfyUI-QI-QwenEditSafe",
      "full_name": "wallen0322/ComfyUI-QI-QwenEditSafe",
      "owner": "wallen0322",
      "description": "ComfyUI-QI-QwenEditSafe",
      "url": "https://github.com/wallen0322/ComfyUI-QI-QwenEditSafe",
      "clone_url": "https://github.com/wallen0322/ComfyUI-QI-QwenEditSafe.git",
      "ssh_url": "git@github.com:wallen0322/ComfyUI-QI-QwenEditSafe.git",
      "homepage": null,
      "created_at": "2025-09-16T09:17:36Z",
      "updated_at": "2025-09-17T07:33:18Z",
      "pushed_at": "2025-09-17T06:54:20Z"
    },
    "stats": {
      "stars": 23,
      "forks": 1,
      "watchers": 23,
      "open_issues": 0,
      "size": 83
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 15523
      },
      "license": null,
      "topics": []
    },
    "content": {
      "readme": "# ComfyUI-QI-QwenEditSafe\n\n[中文版本](README.zh.md) | English\n\n## Quick Guide (EN)\n- No‑resize grid padding; CLIP & VAE share the same source.\n- `reference_pixels` are **VAE recon** (same domain). Use `prompt_emphasis` to trade text adherence vs. reference strength.\n- Nodes: TextEncodeQwenImageEdit (EN), 文生图编辑 (CN), VAE Decode.\n\n**Wiring**: `CLIP/IMAGE/VAE` → TextEncodeQwenImageEdit → `(conditioning, image, latent)` → sampler → VAE Decode.\n\n**Key params**:\n- inject_mode: both / latents / pixels (default both)\n- pixels_source: recon (default) / input\n- pixels_shaping: colorfield_64 (default) / colorfield_32 / full\n- prompt_emphasis: 0–1 (default 0.5; ≥0.8 = strong text), enable mild high-frequency boost (radius 5, amount 0.10–0.15), and increase target total pixels when possible.\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-17T10:40:03.527564"
  },
  {
    "basic_info": {
      "name": "cartpole-ts",
      "full_name": "haydenkz/cartpole-ts",
      "owner": "haydenkz",
      "description": "gpt-5 codex cartpole ",
      "url": "https://github.com/haydenkz/cartpole-ts",
      "clone_url": "https://github.com/haydenkz/cartpole-ts.git",
      "ssh_url": "git@github.com:haydenkz/cartpole-ts.git",
      "homepage": "",
      "created_at": "2025-09-16T17:44:34Z",
      "updated_at": "2025-09-17T09:19:47Z",
      "pushed_at": "2025-09-17T09:19:43Z"
    },
    "stats": {
      "stars": 22,
      "forks": 5,
      "watchers": 22,
      "open_issues": 0,
      "size": 62
    },
    "tech_info": {
      "language": "TypeScript",
      "languages": {
        "TypeScript": 29448,
        "JavaScript": 605,
        "CSS": 550
      },
      "license": null,
      "topics": []
    },
    "content": {
      "readme": "GPT-5 Codex made this, don't expect bug fixes or upgrades ",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-17T10:40:04.869643"
  },
  {
    "basic_info": {
      "name": "activerecord_lazy_columns",
      "full_name": "fatkodima/activerecord_lazy_columns",
      "owner": "fatkodima",
      "description": "Adds support for lazy-loading columns in Active Record models",
      "url": "https://github.com/fatkodima/activerecord_lazy_columns",
      "clone_url": "https://github.com/fatkodima/activerecord_lazy_columns.git",
      "ssh_url": "git@github.com:fatkodima/activerecord_lazy_columns.git",
      "homepage": "",
      "created_at": "2025-09-16T13:19:18Z",
      "updated_at": "2025-09-17T06:14:00Z",
      "pushed_at": "2025-09-16T13:28:19Z"
    },
    "stats": {
      "stars": 21,
      "forks": 0,
      "watchers": 21,
      "open_issues": 0,
      "size": 6
    },
    "tech_info": {
      "language": "Ruby",
      "languages": {
        "Ruby": 4642
      },
      "license": "MIT License",
      "topics": [
        "activerecord",
        "gem",
        "rails",
        "ruby"
      ]
    },
    "content": {
      "readme": "# ActiverecordLazyColumns\n\n`activerecord_lazy_columns` is a gem that lets you specify columns to be loaded lazily in your Active Record models.\n\nBy default, Active Record loads all the columns in each model instance. This gem lets you specify columns\nto be excluded by default. This is useful to reduce the memory taken by Active Record when loading records\nthat have large columns if those particular columns are actually not required most of the time.\nIn this situation it can also greatly reduce the database query time because loading large BLOB/TEXT columns\ngenerally means seeking other database pages since they are not stored wholly in the record's page itself.\n\nNotice that a better approach can be moving those columns to new models, since Active Record loads related models\nlazily by default. This gem is an easy workaround.\n\n## Requirements\n\n- ruby 3.2+\n- activerecord 7.2+\n\n## Installation\n\nAdd this line to your application's Gemfile:\n\n```ruby\ngem \"activerecord_lazy_columns\"\n```\n\n## Usage\n\nUse `lazy_columns` in your Active Record models to define which columns should be loaded lazily:\n\n```ruby\nclass Action < ApplicationRecord\n  lazy_columns :comments\nend\n```\n\nNow, when you fetch some action, the comments are not loaded:\n\n```ruby\nAction.create!(title: \"Some action\", comments: \"Some comments\") # => <Action id: 1...>\naction = Action.find(1) # => <Action id: 1, title: \"Some action\">\n```\n\nAnd if you try to read the `comments` attribute, it will be loaded into the model:\n\n```ruby\naction.comments # => \"Some comments\"\naction # => <Action id: 1, title: \"Some action\", comments: \"Some comments\"\n```\n\n## How the gem works\n\nThis gem does two things:\n\n- Modifies the `default_scope` of the model so that it fetches all the attributes except the ones marked as lazy.\n- Defines a reader method per lazy attribute that will load the corresponding column under demand.\n\n### Eager loading of attributes defined as lazy\n\nThe first time you access a lazy attribute, a new database query will be executed to load it. If you are going\nto operate on a number of objects and want to have the lazy attributes eagerly loaded, use Active Record's\n`.select()` in the initial query. For example:\n\n```ruby\nAction.select(:comments)\n```\n\n## Credits\n\nThanks to the [`lazy_columns` gem](https://github.com/jorgemanrubia/lazy_columns) for the original idea.\n\n## Development\n\nTo install this gem onto your local machine, run `bundle exec rake install`. To release a new version, update\nthe version number in `version.rb`, and then run `bundle exec rake release`, which will create a git tag\nfor the version, push git commits and the created tag, and push the `.gem` file to [rubygems.org](https://rubygems.org).\n\n## Contributing\n\nBug reports and pull requests are welcome on GitHub at https://github.com/fatkodima/activerecord_lazy_columns.\n\n## License\n\nThe gem is available as open source under the terms of the [MIT License](https://opensource.org/licenses/MIT).\n",
      "default_branch": "master"
    },
    "fetched_at": "2025-09-17T10:40:06.217248"
  },
  {
    "basic_info": {
      "name": "Cyberlivre",
      "full_name": "pedrosilvaevangelista/Cyberlivre",
      "owner": "pedrosilvaevangelista",
      "description": "Curso completo de Segurança Cibernética com conteúdos gratuitos em português.",
      "url": "https://github.com/pedrosilvaevangelista/Cyberlivre",
      "clone_url": "https://github.com/pedrosilvaevangelista/Cyberlivre.git",
      "ssh_url": "git@github.com:pedrosilvaevangelista/Cyberlivre.git",
      "homepage": null,
      "created_at": "2025-09-16T12:41:59Z",
      "updated_at": "2025-09-17T02:51:23Z",
      "pushed_at": "2025-09-16T12:48:57Z"
    },
    "stats": {
      "stars": 21,
      "forks": 1,
      "watchers": 21,
      "open_issues": 0,
      "size": 515
    },
    "tech_info": {
      "language": null,
      "languages": {},
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "<p align=\"center\">\n  <img src=\"assets/CYBERLIVRE.png\" alt=\"Banner do Curso CiberLivre\" width=\"800\"/>\n</p>\n\n<div align=\"center\">\n\n# Curso CyberLivre\n\n</div>\n\n> **Um curso completo e gratuito para formar profissionais em Segurança Cibernética**\n\nEste projeto é um repositório inspirado no [Ciência da Computação](https://github.com/Universidade-Livre/ciencia-da-computacao) e consiste em um **curso completo para formar profissionais na área de segurança cibernética**, com conteúdo suficiente para você ingressar no mercado de trabalho e continuar se aprimorando cada vez mais.  \n\nUtilizamos materiais diversos encontrados em **plataformas gratuitas**, o que permite aprender sem gastar nada. Acreditamos que o acesso à educação deve ser possível para todos.  \n\nO conteúdo do curso pode ser feito **individualmente ou em grupo** — sinta-se livre para estudar no seu tempo e do jeito que achar melhor!  \n\nTodos os cursos escolhidos aqui são conhecidos por sua **qualidade e fácil entendimento**. Este projeto será constantemente aprimorado ao longo do tempo.\n\nEste curso não descarta a possibilidade de se aprofundar nos tópicos apresentados ainda mais, recomendamos que o faça.\n\nRecomenda-se a criação de atividades e desafios sobre os topicos estudados para melhor entendimento.\n\n**Bons estudos !!!**  \n\n## 🗺️ **Estrutura do Curso**\n\n| Nível | Foco | Duração Estimada | Status |\n|-------|------|------------------|--------|\n| [**0️⃣ Soft Skills**](#0️⃣-nível-0---soft-skills) | Comunicação, Liderança, Pensamento Crítico | 2-3 semanas | ✅ |\n| [**1️⃣ Hardware**](#1️⃣-nível-1---hardware) | Fundamentos de Hardware e Arquitetura | 2-3 semanas | ✅ |\n| [**2️⃣ Sistemas Operacionais**](#2️⃣-nível-2---sistemas-operacionais) | Windows, Linux, Virtualização | 4-6 semanas | ✅ |\n| [**3️⃣ Redes**](#3️⃣-nível-3---redes-de-computadores) | TCP/IP, Protocolos, Infraestrutura | 6-8 semanas | ✅ |\n| [**4️⃣ Programação**](#4️⃣-nível-4---programação) | Python, Automação, Scripting | 8-10 semanas | ✅ |\n| [**5️⃣ Segurança Cibernética**](#5️⃣-nível-5---segurança-cibernética) | Red/Blue Team, SOC, Pentest | 12+ semanas | ✅ |\n\n---\n\n## **0️⃣ Nível 0 - Soft Skills**\n\n**🎯 Objetivo:** Desenvolver habilidades interpessoais essenciais para o sucesso profissional.\n\nCompetências técnicas são fundamentais, mas soft skills fazem a diferença na carreira. Este nível desenvolve comunicação, liderança, pensamento crítico e inteligência emocional.\n\n### 📺 Vídeos/Playlists\n\n| Nº | Conteúdo | Canal | Descrição |\n|----|----------|-------|-----------|\n| 1 | [**Aulas de Filosofia**](https://youtube.com/playlist?list=PLhuwT6UtzNbnkUlRUZvY6DBgr9LInBtrZ&si=yg5hFpEGxui1FW8d) | Isto não é Filosofia | Pensamento crítico e análise lógica através da filosofia |\n| 2 | [**Técnicas de Vendas e Negociação**](https://youtube.com/playlist?list=PL5lL7Dm426seGa5Hmi1jIu0p8OYtNs2Xm&si=5D_Xs-e81_ZzBsE7) | Profissional Marketing | Persuasão, negociação e fechamento de negócios |\n| 3 | [**Apresentação de Slides**](https://youtube.com/playlist?list=PL6OEUipU7xC7vCVVaOFikOjVqf1BhTVR0&si=6vfhiXAG4Dtpy6Uv) | TechEnfim | Criação de apresentações impactantes |\n| 4 | [**Comunicação e Oratória**](https://youtube.com/playlist?list=PLF0WnKFaIjV2s2-vU2up7-cqpOACSx73v&si=KUKu_te4h9HpLadg) | El Professor da Oratória | Desenvolvimento da comunicação verbal |\n| 5 | [**Networking Profissional**](https://youtu.be/jr8rv8JCcI0) | Jovens de Negócios | Construção de relacionamentos estratégicos |\n| 6 | [**Construção da Identidade**](https://youtube.com/playlist?list=PLwinAdFkfTrVLsK1yBMkmf8SkTg-aR7r-&si=Jt1sK4TkqNiUvfDs) | Projeto Cão Pastor | Autoconhecimento e desenvolvimento pessoal |\n| 7 | [**Performance e Foco**](https://youtube.com/playlist?list=PLwinAdFkfTrU2l2iQuvw6U-n2xTdDWUZZ&si=F3aBeV1A1paIN0Yl) | Projeto Cão Pastor | Disciplina e alta performance |\n| 8 | [**Filosofia Aplicada**](https://youtube.com/playlist?list=PLN50oHosyDdg4P68_1pzRMqHpUnSfJGEF&si=JgmK4wbYFAS5ZxRT) | Clóvis de Barros Filho | Ética e reflexões práticas |\n\n### 🎓 Cursos Estruturados\n\n| Nº | Curso | Plataforma | Descrição |\n|----|-------|------------|-----------|\n| 1 | [**Vendas: Prospecção ao Fechamento**](https://kultivi.com/cursos/negocios/vendas-da-prospeccao-ao-fechamento) | Kultivi | Técnicas completas de vendas |\n| 2 | [**Escrita Criativa**](https://kultivi.com/cursos/negocios/curso-escrita-criativa) | Kultivi | Melhore sua comunicação escrita |\n| 3 | [**Networking Estratégico**](https://kultivi.com/cursos/negocios/conexoes-estrategicas-e-networking-intencional) | Kultivi | Relacionamentos profissionais eficazes |\n| 4 | [**Gestão de Compras**](https://kultivi.com/cursos/negocios/compras-iniciando-uma-carreira-de-sucesso) | Kultivi | Negociação e gestão estratégica |\n| 5 | [**Social Media Marketing**](https://kultivi.com/cursos/negocios/social-media) | Kultivi | Marketing digital e presença online |\n| 6 | [**Oratória de Alta Performance**](https://kultivi.com/cursos/negocios/oratoria-de-alta-performance) | Kultivi | Técnicas avançadas de a",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-17T10:40:07.629912"
  }
]