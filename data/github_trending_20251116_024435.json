[
  {
    "basic_info": {
      "name": "DeepSeek-OCR",
      "full_name": "deepseek-ai/DeepSeek-OCR",
      "owner": "deepseek-ai",
      "description": "Contexts Optical Compression",
      "url": "https://github.com/deepseek-ai/DeepSeek-OCR",
      "clone_url": "https://github.com/deepseek-ai/DeepSeek-OCR.git",
      "ssh_url": "git@github.com:deepseek-ai/DeepSeek-OCR.git",
      "homepage": null,
      "created_at": "2025-10-17T06:14:27Z",
      "updated_at": "2025-11-16T01:28:49Z",
      "pushed_at": "2025-10-25T02:43:18Z"
    },
    "stats": {
      "stars": 20484,
      "forks": 1754,
      "watchers": 20484,
      "open_issues": 238,
      "size": 7948
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 113538
      },
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "<!-- markdownlint-disable first-line-h1 -->\n<!-- markdownlint-disable html -->\n<!-- markdownlint-disable no-duplicate-header -->\n\n\n<div align=\"center\">\n  <img src=\"assets/logo.svg\" width=\"60%\" alt=\"DeepSeek AI\" />\n</div>\n\n\n<hr>\n<div align=\"center\">\n  <a href=\"https://www.deepseek.com/\" target=\"_blank\">\n    <img alt=\"Homepage\" src=\"assets/badge.svg\" />\n  </a>\n  <a href=\"https://huggingface.co/deepseek-ai/DeepSeek-OCR\" target=\"_blank\">\n    <img alt=\"Hugging Face\" src=\"https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-DeepSeek%20AI-ffc107?color=ffc107&logoColor=white\" />\n  </a>\n\n</div>\n\n<div align=\"center\">\n\n  <a href=\"https://discord.gg/Tc7c45Zzu5\" target=\"_blank\">\n    <img alt=\"Discord\" src=\"https://img.shields.io/badge/Discord-DeepSeek%20AI-7289da?logo=discord&logoColor=white&color=7289da\" />\n  </a>\n  <a href=\"https://twitter.com/deepseek_ai\" target=\"_blank\">\n    <img alt=\"Twitter Follow\" src=\"https://img.shields.io/badge/Twitter-deepseek_ai-white?logo=x&logoColor=white\" />\n  </a>\n\n</div>\n\n\n\n<p align=\"center\">\n  <a href=\"https://huggingface.co/deepseek-ai/DeepSeek-OCR\"><b>üì• Model Download</b></a> |\n  <a href=\"https://github.com/deepseek-ai/DeepSeek-OCR/blob/main/DeepSeek_OCR_paper.pdf\"><b>üìÑ Paper Link</b></a> |\n  <a href=\"https://arxiv.org/abs/2510.18234\"><b>üìÑ Arxiv Paper Link</b></a> |\n</p>\n\n<h2>\n<p align=\"center\">\n  <a href=\"\">DeepSeek-OCR: Contexts Optical Compression</a>\n</p>\n</h2>\n\n<p align=\"center\">\n<img src=\"assets/fig1.png\" style=\"width: 1000px\" align=center>\n</p>\n<p align=\"center\">\n<a href=\"\">Explore the boundaries of visual-text compression.</a>       \n</p>\n\n## Release\n- [2025/10/23]üöÄüöÄüöÄ DeepSeek-OCR is now officially supported in upstream [vLLM](https://docs.vllm.ai/projects/recipes/en/latest/DeepSeek/DeepSeek-OCR.html#installing-vllm). Thanks to the [vLLM](https://github.com/vllm-project/vllm) team for their help.\n- [2025/10/20]üöÄüöÄüöÄ We release DeepSeek-OCR, a model to investigate the role of vision encoders from an LLM-centric viewpoint.\n\n## Contents\n- [Install](#install)\n- [vLLM Inference](#vllm-inference)\n- [Transformers Inference](#transformers-inference)\n  \n\n\n\n\n## Install\n>Our environment is cuda11.8+torch2.6.0.\n1. Clone this repository and navigate to the DeepSeek-OCR folder\n```bash\ngit clone https://github.com/deepseek-ai/DeepSeek-OCR.git\n```\n2. Conda\n```Shell\nconda create -n deepseek-ocr python=3.12.9 -y\nconda activate deepseek-ocr\n```\n3. Packages\n\n- download the vllm-0.8.5 [whl](https://github.com/vllm-project/vllm/releases/tag/v0.8.5) \n```Shell\npip install torch==2.6.0 torchvision==0.21.0 torchaudio==2.6.0 --index-url https://download.pytorch.org/whl/cu118\npip install vllm-0.8.5+cu118-cp38-abi3-manylinux1_x86_64.whl\npip install -r requirements.txt\npip install flash-attn==2.7.3 --no-build-isolation\n```\n**Note:** if you want vLLM and transformers codes to run in the same environment, you don't need to worry about this installation error like: vllm 0.8.5+cu118 requires transformers>=4.51.1\n\n## vLLM-Inference\n- VLLM:\n>**Note:** change the INPUT_PATH/OUTPUT_PATH and other settings in the DeepSeek-OCR-master/DeepSeek-OCR-vllm/config.py\n```Shell\ncd DeepSeek-OCR-master/DeepSeek-OCR-vllm\n```\n1. image: streaming output\n```Shell\npython run_dpsk_ocr_image.py\n```\n2. pdf: concurrency ~2500tokens/s(an A100-40G)\n```Shell\npython run_dpsk_ocr_pdf.py\n```\n3. batch eval for benchmarks\n```Shell\npython run_dpsk_ocr_eval_batch.py\n```\n\n**[2025/10/23] The version of upstream [vLLM](https://docs.vllm.ai/projects/recipes/en/latest/DeepSeek/DeepSeek-OCR.html#installing-vllm):**\n\n```shell\nuv venv\nsource .venv/bin/activate\n# Until v0.11.1 release, you need to install vLLM from nightly build\nuv pip install -U vllm --pre --extra-index-url https://wheels.vllm.ai/nightly\n```\n\n```python\nfrom vllm import LLM, SamplingParams\nfrom vllm.model_executor.models.deepseek_ocr import NGramPerReqLogitsProcessor\nfrom PIL import Image\n\n# Create model instance\nllm = LLM(\n    model=\"deepseek-ai/DeepSeek-OCR\",\n    enable_prefix_caching=False,\n    mm_processor_cache_gb=0,\n    logits_processors=[NGramPerReqLogitsProcessor]\n)\n\n# Prepare batched input with your image file\nimage_1 = Image.open(\"path/to/your/image_1.png\").convert(\"RGB\")\nimage_2 = Image.open(\"path/to/your/image_2.png\").convert(\"RGB\")\nprompt = \"<image>\\nFree OCR.\"\n\nmodel_input = [\n    {\n        \"prompt\": prompt,\n        \"multi_modal_data\": {\"image\": image_1}\n    },\n    {\n        \"prompt\": prompt,\n        \"multi_modal_data\": {\"image\": image_2}\n    }\n]\n\nsampling_param = SamplingParams(\n            temperature=0.0,\n            max_tokens=8192,\n            # ngram logit processor args\n            extra_args=dict(\n                ngram_size=30,\n                window_size=90,\n                whitelist_token_ids={128821, 128822},  # whitelist: <td>, </td>\n            ),\n            skip_special_tokens=False,\n        )\n# Generate output\nmodel_outputs = llm.generate(model_input, sampling_param)\n\n# Print output\nfor output in model_outputs:\n    print(output.outputs[0].text)\n```\n## ",
      "default_branch": "main"
    },
    "fetched_at": "2025-11-16T02:44:36.238471"
  },
  {
    "basic_info": {
      "name": "AI-Trader",
      "full_name": "HKUDS/AI-Trader",
      "owner": "HKUDS",
      "description": "\"AI-Trader: Can AI Beat the Market?\" Live Trading Bench: https://ai4trade.ai",
      "url": "https://github.com/HKUDS/AI-Trader",
      "clone_url": "https://github.com/HKUDS/AI-Trader.git",
      "ssh_url": "git@github.com:HKUDS/AI-Trader.git",
      "homepage": "https://ai4trade.ai",
      "created_at": "2025-10-23T12:45:00Z",
      "updated_at": "2025-11-16T02:30:43Z",
      "pushed_at": "2025-11-14T11:49:24Z"
    },
    "stats": {
      "stars": 9386,
      "forks": 1432,
      "watchers": 9386,
      "open_issues": 48,
      "size": 15329
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 312770,
        "Shell": 4908
      },
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "\n\n<div align=\"center\">\n  <picture>\n      <img src=\"./assets/AI-Trader-log.png\" width=\"20%\" style=\"border: none; box-shadow: none;\">\n  </picture>\n</div >\n\n<div align=\"center\">\n\n# üöÄ AI-Trader: Can AI Beat the Market?\n\n[![Python](https://img.shields.io/badge/Python-3.10+-blue.svg)](https://python.org)\n[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)\n[![GitHub stars](https://img.shields.io/github/stars/HKUDS/AI-Trader?style=social)](https://github.com/HKUDS/AI-Trader)\n[![Feishu](https://img.shields.io/badge/üí¨Feishu-Group-blue?style=flat)](./Communication.md) \n[![WeChat](https://img.shields.io/badge/WeChat-Group-green?style=flat&logo=wechat)](./Communication.md)\n\n**AI agents battle for supremacy in NASDAQ 100, SSE 50, and cryptocurrency markets. Zero human input. Pure competition.**\n\n## üèÜ Current Championship Leaderboard üèÜ \n[*Click Here: AI Live Trading*](https://ai4trade.ai)\n\n</div>\n\n---\n## Friends of AI-Trader: Other Interesting Projects\n- [TradeTrap](https://github.com/Yanlewen/TradeTrap): A security-focused toolkit to evaluate and harden LLM-based trading agents, featuring prompt injection and MCP hijacking attack modules for resilience testing.\n\n- [RockAlpha](https://rockalpha.rockflow.ai/): The investment arena launched by RockFlow. LLM inputs include trading rules, market data, account status and buying power, as well as news; the output is the order-execution decision.\n\n- [TwinMarket](https://github.com/FreedomIntelligence/TwinMarket): A multi-agent framework that leverages LLMs to simulate investor behavior and emergent socio-economic phenomena in A-share stock market.\n---\n## üéâ Weekly Update\n\n### üìà Market Expansion\n- ‚úÖ **A-Share Market Support** - Extended our trading capabilities to include Chinese A-share markets, expanding our global market coverage.\n- ‚úÖ **Cryptocurrency Market Support** - Added support for trading major cryptocurrencies including Bitcoin, Ethereum, and 8 other leading digital assets.\n\n### ‚è∞ Enhanced Trading Capabilities\n- ‚úÖ **Hourly Trading Support** - We've upgraded from daily to hourly trading intervals, enabling more precise and responsive market participation with granular timing control.\n\n### üé® User Experience Improvements\n- ‚úÖ **Live Trading Dashboard** - Introduced real-time visualization of all agent trading activities: https://ai4trade.ai.\n\n- ‚úÖ **Agent Reasoning Display** - Implemented complete transparency into AI decision-making processes, featuring detailed reasoning chains that show how each trading decision is formed.\n\n- ‚úÖ **Interactive Leaderboard** - Launched a dynamic performance ranking system with live updates, allowing users to track and compare agent performance in real-time.\n\n- ‚è∞ **Important Notice** - To maintain a well-managed repository, we no longer upload runtime data to the repo, as it would make it very bloated. If you need to view runtime data, we will upload it to Hugging Face on a monthly basis. You can view real-time runtime data here: https://ai4trade.ai.\n---\n\n## **How to use this dataset**\n\nIt's simple! \n\nYou just need to submit a PR that includes at least: `./agent/{your_strategy}.py` (you can inherit from Basemodel to create your strategy!), `./configs/{yourconfig}`, and instructions on how to run your strategy. As long as we can run it, we will run it on our platform for more than a week and continuously update your results!\n\n---\n\n<div align=\"center\">\n\n[üöÄ Quick Start](#-quick-start) ‚Ä¢ [üìà Performance Analysis](#-performance-analysis) ‚Ä¢ [üõ†Ô∏è Configuration Guide](#-configuration-guide) ‚Ä¢ [‰∏≠ÊñáÊñáÊ°£](README_CN.md)\n\n</div>\n\n\n## üåü Project Introduction\n\n> **AI-Trader enables five distinct AI models, each employing unique investment strategies, to compete autonomously in the same market and determine which can generate the highest profits in NASDAQ 100, SSE 50, or cryptocurrency trading!**\n\n### üéØ Core Features\n\n- ü§ñ **Fully Autonomous Decision-Making**: AI agents perform 100% independent analysis, decision-making, and execution without human intervention\n- üõ†Ô∏è **Pure Tool-Driven Architecture**: Built on MCP toolchain, enabling AI to complete all trading operations through standardized tool calls\n- üèÜ **Multi-Model Competition Arena**: Deploy multiple AI models (GPT, Claude, Qwen, etc.) for competitive trading\n- üìä **Real-Time Performance Analytics**: Comprehensive trading records, position monitoring, and profit/loss analysis\n- üîç **Intelligent Market Intelligence**: Integrated Jina search for real-time market news and financial reports\n- ‚ö° **MCP Toolchain Integration**: Modular tool ecosystem based on Model Context Protocol\n- üîå **Extensible Strategy Framework**: Support for third-party strategies and custom AI agent integration\n- ‚è∞ **Historical Replay Capability**: Time-period replay functionality with automatic future information filtering\n\n---\n\n### üéÆ Trading Environment\nEach AI model starts with $10,000, 100,000¬•, or 50,000 USDT to trade NASDAQ 100 stocks, SSE 50 stocks, or major cryptocurrencies in a controlled environment with real mar",
      "default_branch": "main"
    },
    "fetched_at": "2025-11-16T02:44:37.392413"
  },
  {
    "basic_info": {
      "name": "Skill_Seekers",
      "full_name": "yusufkaraaslan/Skill_Seekers",
      "owner": "yusufkaraaslan",
      "description": "Convert documentation websites, GitHub repositories, and PDFs into Claude AI skills with automatic conflict detection",
      "url": "https://github.com/yusufkaraaslan/Skill_Seekers",
      "clone_url": "https://github.com/yusufkaraaslan/Skill_Seekers.git",
      "ssh_url": "git@github.com:yusufkaraaslan/Skill_Seekers.git",
      "homepage": "",
      "created_at": "2025-10-17T14:43:48Z",
      "updated_at": "2025-11-16T02:37:03Z",
      "pushed_at": "2025-11-12T20:30:22Z"
    },
    "stats": {
      "stars": 4038,
      "forks": 431,
      "watchers": 4038,
      "open_issues": 119,
      "size": 770
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 693757,
        "Shell": 8958
      },
      "license": "MIT License",
      "topics": [
        "ai-tools",
        "ast-parser",
        "automation",
        "claude-ai",
        "claude-skills",
        "code-analysis",
        "conflict-detection",
        "documentation",
        "documentation-generator",
        "github",
        "github-scraper",
        "mcp",
        "mcp-server",
        "multi-source",
        "ocr",
        "pdf",
        "python",
        "web-scraping"
      ]
    },
    "content": {
      "readme": "[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/yusufkaraaslan-skill-seekers-badge.png)](https://mseep.ai/app/yusufkaraaslan-skill-seekers)\n\n# Skill Seeker\n\n[![Version](https://img.shields.io/badge/version-2.0.0-blue.svg)](https://github.com/yusufkaraaslan/Skill_Seekers/releases/tag/v2.0.0)\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)\n[![MCP Integration](https://img.shields.io/badge/MCP-Integrated-blue.svg)](https://modelcontextprotocol.io)\n[![Tested](https://img.shields.io/badge/Tests-379%20Passing-brightgreen.svg)](tests/)\n[![Project Board](https://img.shields.io/badge/Project-Board-purple.svg)](https://github.com/users/yusufkaraaslan/projects/2)\n[![PyPI version](https://badge.fury.io/py/skill-seekers.svg)](https://pypi.org/project/skill-seekers/)\n[![PyPI - Downloads](https://img.shields.io/pypi/dm/skill-seekers.svg)](https://pypi.org/project/skill-seekers/)\n[![PyPI - Python Version](https://img.shields.io/pypi/pyversions/skill-seekers.svg)](https://pypi.org/project/skill-seekers/)\n\n**Automatically convert documentation websites, GitHub repositories, and PDFs into Claude AI skills in minutes.**\n\n> üìã **[View Development Roadmap & Tasks](https://github.com/users/yusufkaraaslan/projects/2)** - 134 tasks across 10 categories, pick any to contribute!\n\n## What is Skill Seeker?\n\nSkill Seeker is an automated tool that transforms documentation websites, GitHub repositories, and PDF files into production-ready [Claude AI skills](https://www.anthropic.com/news/skills). Instead of manually reading and summarizing documentation, Skill Seeker:\n\n1. **Scrapes** multiple sources (docs, GitHub repos, PDFs) automatically\n2. **Analyzes** code repositories with deep AST parsing\n3. **Detects** conflicts between documentation and code implementation\n4. **Organizes** content into categorized reference files\n5. **Enhances** with AI to extract best examples and key concepts\n6. **Packages** everything into an uploadable `.zip` file for Claude\n\n**Result:** Get comprehensive Claude skills for any framework, API, or tool in 20-40 minutes instead of hours of manual work.\n\n## Why Use This?\n\n- üéØ **For Developers**: Create skills from documentation + GitHub repos with conflict detection\n- üéÆ **For Game Devs**: Generate skills for game engines (Godot docs + GitHub, Unity, etc.)\n- üîß **For Teams**: Combine internal docs + code repositories into single source of truth\n- üìö **For Learners**: Build comprehensive skills from docs, code examples, and PDFs\n- üîç **For Open Source**: Analyze repos to find documentation gaps and outdated examples\n\n## Key Features\n\n### üåê Documentation Scraping\n- ‚úÖ **llms.txt Support** - Automatically detects and uses LLM-ready documentation files (10x faster)\n- ‚úÖ **Universal Scraper** - Works with ANY documentation website\n- ‚úÖ **Smart Categorization** - Automatically organizes content by topic\n- ‚úÖ **Code Language Detection** - Recognizes Python, JavaScript, C++, GDScript, etc.\n- ‚úÖ **8 Ready-to-Use Presets** - Godot, React, Vue, Django, FastAPI, and more\n\n### üìÑ PDF Support (**v1.2.0**)\n- ‚úÖ **Basic PDF Extraction** - Extract text, code, and images from PDF files\n- ‚úÖ **OCR for Scanned PDFs** - Extract text from scanned documents\n- ‚úÖ **Password-Protected PDFs** - Handle encrypted PDFs\n- ‚úÖ **Table Extraction** - Extract complex tables from PDFs\n- ‚úÖ **Parallel Processing** - 3x faster for large PDFs\n- ‚úÖ **Intelligent Caching** - 50% faster on re-runs\n\n### üêô GitHub Repository Scraping (**v2.0.0**)\n- ‚úÖ **Deep Code Analysis** - AST parsing for Python, JavaScript, TypeScript, Java, C++, Go\n- ‚úÖ **API Extraction** - Functions, classes, methods with parameters and types\n- ‚úÖ **Repository Metadata** - README, file tree, language breakdown, stars/forks\n- ‚úÖ **GitHub Issues & PRs** - Fetch open/closed issues with labels and milestones\n- ‚úÖ **CHANGELOG & Releases** - Automatically extract version history\n- ‚úÖ **Conflict Detection** - Compare documented APIs vs actual code implementation\n- ‚úÖ **MCP Integration** - Natural language: \"Scrape GitHub repo facebook/react\"\n\n### üîÑ Unified Multi-Source Scraping (**NEW - v2.0.0**)\n- ‚úÖ **Combine Multiple Sources** - Mix documentation + GitHub + PDF in one skill\n- ‚úÖ **Conflict Detection** - Automatically finds discrepancies between docs and code\n- ‚úÖ **Intelligent Merging** - Rule-based or AI-powered conflict resolution\n- ‚úÖ **Transparent Reporting** - Side-by-side comparison with ‚ö†Ô∏è warnings\n- ‚úÖ **Documentation Gap Analysis** - Identifies outdated docs and undocumented features\n- ‚úÖ **Single Source of Truth** - One skill showing both intent (docs) and reality (code)\n- ‚úÖ **Backward Compatible** - Legacy single-source configs still work\n\n### ü§ñ AI & Enhancement\n- ‚úÖ **AI-Powered Enhancement** - Transforms basic templates into comprehensive guides\n- ‚úÖ **No API Costs** - FREE local enhancement using Claude Code Max\n- ‚úÖ **MCP Server for",
      "default_branch": "development"
    },
    "fetched_at": "2025-11-16T02:44:38.508254"
  },
  {
    "basic_info": {
      "name": "awesome-claude-skills",
      "full_name": "ComposioHQ/awesome-claude-skills",
      "owner": "ComposioHQ",
      "description": "A curated list of awesome Claude Skills, resources, and tools for customizing Claude AI workflows",
      "url": "https://github.com/ComposioHQ/awesome-claude-skills",
      "clone_url": "https://github.com/ComposioHQ/awesome-claude-skills.git",
      "ssh_url": "git@github.com:ComposioHQ/awesome-claude-skills.git",
      "homepage": "",
      "created_at": "2025-10-17T07:15:01Z",
      "updated_at": "2025-11-16T02:42:56Z",
      "pushed_at": "2025-11-12T03:24:23Z"
    },
    "stats": {
      "stars": 3601,
      "forks": 462,
      "watchers": 3601,
      "open_issues": 2,
      "size": 3147
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 566237,
        "JavaScript": 37795,
        "Shell": 11441
      },
      "license": null,
      "topics": [
        "anthropic",
        "anthropic-ai",
        "anthropic-skills",
        "awesome",
        "awesome-lists",
        "claude",
        "claude-4",
        "claude-4-5-sonnet",
        "claude-4-opus",
        "claude-api",
        "claude-code",
        "claude-desktop",
        "claude-skills",
        "claude-skills-hub",
        "skills"
      ]
    },
    "content": {
      "readme": "<h1 align=\"center\">Awesome Claude Skills</h1>\n\n<p align=\"center\">\n<a href=\"https://composio.dev/?utm_source=Github&utm_medium=Youtube&utm_campaign=2025-11&utm_content=AwesomeSkills\">\n  <img width=\"1280\" height=\"640\" alt=\"Composio banner\" src=\"https://github.com/user-attachments/assets/adb3f57a-2706-4329-856f-059a32059d48\">\n</a>\n\n\n</p>\n\n<p align=\"center\">\n  <a href=\"https://awesome.re\">\n    <img src=\"https://awesome.re/badge.svg\" alt=\"Awesome\" />\n  </a>\n  <a href=\"https://makeapullrequest.com\">\n    <img src=\"https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat-square\" alt=\"PRs Welcome\" />\n  </a>\n  <a href=\"https://www.apache.org/licenses/LICENSE-2.0\">\n    <img src=\"https://img.shields.io/badge/License-Apache_2.0-blue.svg?style=flat-square\" alt=\"License: Apache-2.0\" />\n  </a>\n</p>\n\nA curated list of practical Claude Skills for enhancing productivity across Claude.ai, Claude Code, and the Claude API.\n\n\n> If you want your skills to take actions across 500+ apps, wire them up with [Composio](https://composio.dev/?utm_source=Github&utm_medium=Youtube&utm_campaign=2025-11&utm_content=AwesomeSkills)\n\n\n## Contents\n\n- [What Are Claude Skills?](#what-are-claude-skills)\n- [Skills](#skills)\n  - [Document Processing](#document-processing)\n  - [Development & Code Tools](#development--code-tools)\n  - [Data & Analysis](#data--analysis)\n  - [Business & Marketing](#business--marketing)\n  - [Communication & Writing](#communication--writing)\n  - [Creative & Media](#creative--media)\n  - [Productivity & Organization](#productivity--organization)\n  - [Collaboration & Project Management](#collaboration--project-management)\n  - [Security & Systems](#security--systems)\n- [Getting Started](#getting-started)\n- [Creating Skills](#creating-skills)\n- [Contributing](#contributing)\n- [Resources](#resources)\n- [License](#license)\n\n## What Are Claude Skills?\n\nClaude Skills are customizable workflows that teach Claude how to perform specific tasks according to your unique requirements. Skills enable Claude to execute tasks in a repeatable, standardized manner across all Claude platforms.\n\n## Skills\n\n### Document Processing\n\n- [docx](https://github.com/anthropics/skills/tree/main/document-skills/docx) - Create, edit, analyze Word docs with tracked changes, comments, formatting.\n- [pdf](https://github.com/anthropics/skills/tree/main/document-skills/pdf) - Extract text, tables, metadata, merge & annotate PDFs.\n- [pptx](https://github.com/anthropics/skills/tree/main/document-skills/pptx) - Read, generate, and adjust slides, layouts, templates.\n- [xlsx](https://github.com/anthropics/skills/tree/main/document-skills/xlsx) - Spreadsheet manipulation: formulas, charts, data transformations.\n- [Markdown to EPUB Converter](https://github.com/smerchek/claude-epub-skill) - Converts markdown documents and chat summaries into professional EPUB ebook files. *By [@smerchek](https://github.com/smerchek)*\n\n### Development & Code Tools\n\n- [artifacts-builder](https://github.com/anthropics/skills/tree/main/artifacts-builder) - Suite of tools for creating elaborate, multi-component claude.ai HTML artifacts using modern frontend web technologies (React, Tailwind CSS, shadcn/ui).\n- [aws-skills](https://github.com/zxkane/aws-skills) - AWS development with CDK best practices, cost optimization MCP servers, and serverless/event-driven architecture patterns.\n- [Changelog Generator](./changelog-generator/) - Automatically creates user-facing changelogs from git commits by analyzing history and transforming technical commits into customer-friendly release notes.\n- [Claude Code Terminal Title](https://github.com/bluzername/claude-code-terminal-title) - Gives each Claud-Code terminal window a dynamic title that describes the work being done so you don't lose track of what window is doing what.\n- [D3.js Visualization](https://github.com/chrisvoncsefalvay/claude-d3js-skill) - Teaches Claude to produce D3 charts and interactive data visualizations. *By [@chrisvoncsefalvay](https://github.com/chrisvoncsefalvay)*\n- [FFUF Web Fuzzing](https://github.com/jthack/ffuf_claude_skill) - Integrates the ffuf web fuzzer so Claude can run fuzzing tasks and analyze results for vulnerabilities. *By [@jthack](https://github.com/jthack)*\n- [finishing-a-development-branch](https://github.com/obra/superpowers/tree/main/skills/finishing-a-development-branch) - Guides completion of development work by presenting clear options and handling chosen workflow.\n- [iOS Simulator](https://github.com/conorluddy/ios-simulator-skill) - Enables Claude to interact with iOS Simulator for testing and debugging iOS applications. *By [@conorluddy](https://github.com/conorluddy)*\n- [MCP Builder](./mcp-builder/) - Guides creation of high-quality MCP (Model Context Protocol) servers for integrating external APIs and services with LLMs using Python or TypeScript.\n- [move-code-quality-skill](https://github.com/1NickPappas/move-code-quality-skill) - Analyzes Move language packages against the official Move",
      "default_branch": "master"
    },
    "fetched_at": "2025-11-16T02:44:39.657739"
  },
  {
    "basic_info": {
      "name": "omnilingual-asr",
      "full_name": "facebookresearch/omnilingual-asr",
      "owner": "facebookresearch",
      "description": "Omnilingual ASR Open-Source Multilingual SpeechRecognition for 1600+ Languages",
      "url": "https://github.com/facebookresearch/omnilingual-asr",
      "clone_url": "https://github.com/facebookresearch/omnilingual-asr.git",
      "ssh_url": "git@github.com:facebookresearch/omnilingual-asr.git",
      "homepage": null,
      "created_at": "2025-11-06T22:38:00Z",
      "updated_at": "2025-11-16T01:39:30Z",
      "pushed_at": "2025-11-13T20:34:22Z"
    },
    "stats": {
      "stars": 1867,
      "forks": 141,
      "watchers": 1867,
      "open_issues": 14,
      "size": 1021
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 295799
      },
      "license": "Other",
      "topics": []
    },
    "content": {
      "readme": "<div align=\"center\">\n  <img src=\"./omniASR_header.jpg\" alt=\"Header image with a collage of on-the-ground photos from the transcription gathering efforts in Pakistan and Liberia.\" width=\"100%\" />\n  <p><i>Photographs captured during corpus creation efforts in Pakistan and Liberia.</i></p>\n</div>\n\n# Omnilingual ASR: Open-Source Multilingual Speech Recognition for 1600+ Languages\n\nOmnilingual ASR is an open-source speech recognition system supporting over 1,600 languages ‚Äî including hundreds never previously covered by any ASR technology. Designed for broad accessibility, it enables new languages to be added with just a few paired examples without requiring specialized expertise or large datasets. By combining scalable zero-shot learning with a flexible model family, Omnilingual ASR aims to make speech technology more inclusive and adaptable for communities and researchers worldwide.\n\n* [Huggingface Demo](https://huggingface.co/spaces/facebook/omniasr-transcriptions)\n* [Huggingface Dataset](https://huggingface.co/datasets/facebook/omnilingual-asr-corpus)\n* [Paper](https://ai.meta.com/research/publications/omnilingual-asr-open-source-multilingual-speech-recognition-for-1600-languages/)\n* [Blogpost](http://ai.meta.com/blog/omnilingual-asr-advancing-automatic-speech-recognition)\n\n<div align=\"center\">\n  <img src=\"./result_table.png\" alt=\"Performance results table\" width=\"100%\" />\n  <p><i>Our 7B-LLM-ASR system achieves state-of-the-art performance across 1,600+ languages, with character error rates (CER) below 10 for 78% of those languages.</i></p>\n</div>\n\n\n## Documentation\n\n### Quick Start\n- **[Installation & Basic Usage](#installation)** - Setup and first transcription\n- **[Inference Pipeline](src/omnilingual_asr/models/inference/README.md)** - Comprehensive transcription guide with batch processing, language conditioning, and context examples\n- **[Supported Languages](#supported-languages)** - View the complete list of 1600+ supported languages\n\n\n### Models & Architecture\n- **[Model Specifications](#model-architectures)** - Available models, parameters, and memory requirements\n- **[Architecture Overview](src/omnilingual_asr/models/README.md)** - Technical details on W2V, CTC, and LLM model families\n- **[Asset Management](src/omnilingual_asr/cards/README.md)** - Configuration system for models, tokenizers, and datasets\n\n### Training & Data Pipeline\n- **[Data Preparation](workflows/dataprep/README.md)** - End-to-end guide for multilingual dataset preparation, HuggingFace integration, and parquet processing\n- **[Training Recipes](workflows/recipes/wav2vec2/asr/README.md)** - Pre-configured workflows for CTC and LLM model training\n\n---\n\n## Installation\n\nThe models were developed using [fairseq2](https://github.com/facebookresearch/fairseq2), a research-focused sequence modeling toolkit. While we provide a **reference** inference pipeline that works across platforms, audio support requires [libsndfile](https://github.com/facebookresearch/fairseq2?tab=readme-ov-file#system-dependencies) (Mac: `brew install libsndfile`; Windows may need an additional [setup](https://github.com/facebookresearch/fairseq2?tab=readme-ov-file#installing-on-windows)).\n\n```bash\n# using pip\npip install omnilingual-asr\n\n# using uv\nuv add omnilingual-asr\n```\n\n## Inference\n\n```python\nfrom omnilingual_asr.models.inference.pipeline import ASRInferencePipeline\n\npipeline = ASRInferencePipeline(model_card=\"omniASR_LLM_7B\")\n\naudio_files = [\"/path/to/eng_audio1.flac\", \"/path/to/deu_audio2.wav\"]\nlang = [\"eng_Latn\", \"deu_Latn\"]\ntranscriptions = pipeline.transcribe(audio_files, lang=lang, batch_size=2)\n```\n\nMore details on running specific models can be found in the [src/omnilingual_asr/models/inference](/src/omnilingual_asr/models/inference/README.md) directory.\n\n> **‚ö†Ô∏è Important:** Currently only audio files shorter than 40 seconds are accepted for inference. We plan to add support for transcribing unlimited-length audio files shortly.\n\n### Supported Languages\n\nTo view the full list of 1600+ supported languages, you can access the language list [programmatically](/src/omnilingual_asr/models/wav2vec2_llama/lang_ids.py):\n\n```python\nfrom omnilingual_asr.models.wav2vec2_llama.lang_ids import supported_langs\n\n# Print all supported languages\nprint(f\"Total supported languages: {len(supported_langs)}\")\nprint(supported_langs)\n\n# Check if a specific language is supported\nif \"eng_Latn\" in supported_langs:\n    print(\"English (Latin script) is supported!\")\n```\n\nLanguages follow the format `{language_code}_{script}`, for example `eng_Latn` - English (Latin script), `cmn_Hans` - Mandarin Chinese (Simplified), ...\n\n### Using the HuggingFace Dataset ü§ó\n\nWe provide a large-scale multilingual speech dataset on HuggingFace under CC-BY-4.0 License: [`facebook/omnilingual-asr-corpus`](https://huggingface.co/datasets/facebook/omnilingual-asr-corpus).\nThis dataset can be directly used with our inference pipeline for evaluation or testing:\n\n```bash\npip install \"omnilingual-asr[dat",
      "default_branch": "main"
    },
    "fetched_at": "2025-11-16T02:44:40.770943"
  },
  {
    "basic_info": {
      "name": "PokeeResearchOSS",
      "full_name": "Pokee-AI/PokeeResearchOSS",
      "owner": "Pokee-AI",
      "description": "Pokee Deep Research Model Open Source Repo",
      "url": "https://github.com/Pokee-AI/PokeeResearchOSS",
      "clone_url": "https://github.com/Pokee-AI/PokeeResearchOSS.git",
      "ssh_url": "git@github.com:Pokee-AI/PokeeResearchOSS.git",
      "homepage": null,
      "created_at": "2025-10-17T07:15:18Z",
      "updated_at": "2025-11-15T07:30:53Z",
      "pushed_at": "2025-10-22T06:57:29Z"
    },
    "stats": {
      "stars": 1650,
      "forks": 1019,
      "watchers": 1650,
      "open_issues": 5,
      "size": 1835
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 386590,
        "Shell": 3417
      },
      "license": "Apache License 2.0",
      "topics": []
    },
    "content": {
      "readme": "This repository hosts Pokee‚Äôs state-of-the-art 7B DeepResearch Agent, which integrates web search and content reading capabilities to answer complex questions using the most up-to-date information available online.\n\n*We also offer an API hosting our proprietary deep research agent, which is up to 75% cheaper than OpenAI, Gemini, and Perplexity. It delivers comprehensive, citation-rich research reports with no hidden costs and no API key management required. (For more information about the API, visit [pokee.ai/deepresearch-preview](https://pokee.ai/deepresearch-preview))*\n\n<div align=\"center\">\n\n[![GitHub Stars](https://img.shields.io/github/stars/Pokee-AI/PokeeResearchOSS?style=social)](https://github.com/Pokee-AI/PokeeResearchOSS)\n[![arXiv](https://img.shields.io/badge/arXiv-2510.15862-b31b1b.svg)](https://arxiv.org/pdf/2510.15862)\n[![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Model-yellow)](https://huggingface.co/PokeeAI/pokee_research_7b)\n\n[![LinkedIn](https://img.shields.io/badge/LinkedIn-Follow-blue?style=social&logo=linkedin)](https://linkedin.com/company/pokee-ai)\n[![X (Twitter)](https://img.shields.io/badge/X-Follow-1DA1F2?style=social&logo=x)](https://x.com/pokee_ai)\n[![Discord](https://img.shields.io/badge/Discord-Join-5865F2?style=social&logo=discord)](https://discord.gg/VJXWQvyd)\n[![WeChat](https://img.shields.io/badge/WeChat-Join-07C160?style=social&logo=wechat)](https://i.postimg.cc/wv099v5w/wechat-group-pokee.jpg)\n\n<img src=\"Logo.png\" alt=\"Pokee AI Logo\" width=\"200\"/>\n<p style=\"text-align:center;\">\n  <a href=\"https://pokee.ai\" target=\"_blank\" rel=\"noopener noreferrer\"><strong>pokee.ai</strong></a>\n</p>\n\n</div>\n\n# PokeeResearch-7B Agent\n\nPokee's state-of-the-art 7B DeepResearch Agent that leverages web search and content reading capabilities to answer complex questions using the most up-to-date information available online.\n<div align=\"center\">\n<img src=\"hle_gaia_bro.png\" alt=\"HLE, GAIA and BrowseComp Performance\" width=\"600\"/>\n</div>\n<div align=\"center\">\n<img src=\"qa.png\" alt=\"7 QA Benchmark Performance\" width=\"800\"/>\n</div>\n\n## üöÄ Features\n\n- **Multi-turn Research**: Performs iterative web searches and content analysis\n- **Tool Integration**: Seamlessly integrates web search, content reading, and browsing tools\n- **Comprehensive Evaluation**: Includes benchmark evaluation across multiple QA datasets\n- **High Performance**: Achieves superior results on complex reasoning tasks\n- **Scalable Architecture**: Built on efficient 7B parameter model for optimal performance\n\n\n## üìã Requirements\n\n### Hardware\n- **Compute Node**: We tested the code on a single 80GB A100 GPU (GPUs with less memory may also work, though we have not tested them). Using multiple GPUs can further accelerate inference. For reference, the driver version is 570.133.20 and the CUDA toolkit version is 12.8.\n\n### Software\n- **Docker**: Environment to run the code will be provided as a docker image.\n\n### API Keys\nYou will need the following API keys:\n- **Serper API**: For web search functionality\n- **Jina API**: For web content reading and extraction\n- **Gemini API**: For content summarization and result evaluation\n- **HuggingFace Token**: For downloading the model from HuggingFace\n\n## üõ†Ô∏è Quick Start\n\n### 1. Environment Setup\nWe provide a Docker image for easy deployment:\n```bash\ndocker pull verlai/verl:app-verl0.5-transformers4.55.4-sglang0.4.10.post2-mcore0.13.0-te2.2\ndocker create --runtime=nvidia --gpus all --net=host --shm-size=\"80g\"  -v .:/workspace/ --name pokeeresearch verlai/verl:app-verl0.5-transformers4.55.4-sglang0.4.10.post2-mcore0.13.0-te2.2 sleep infinity\ndocker start pokeeresearch\ndocker exec -it pokeeresearch  bash\nssh-keygen -t ed25519 -C <USER_NAME>\n# copy /root/.ssh/id_ed25519.pub to github ssh keys\ngit clone git@github.com:Pokee-AI/PokeeResearchOSS.git --recursive\ncd PokeeResearchOSS\npip install colorlog\npip install -U google-genai\nhf auth login # enter your huggingface token, the tokens needs to have permission to use Pokee AI's models\ncd verl\npip install -e .\ncd ..\n```\n\nCreate a `.env` file in the project root and add your API keys:\n```bash\nSERPER_API_KEY=your_serper_api_key_here\nJINA_API_KEY=your_jina_api_key_here\nGEMINI_API_KEY=your_gemini_api_key_here\n```\n\n### 2. Modify ```run.sh``` to use more than one GPUs (optional)\nRunning the experiment with more GPUs is faster. By default the experiment uses one GPU.\nIf you want to use more GPUs, simply modify \n```bash\ntrainer.n_gpus_per_node=1 \\\n```\nin ```run.sh``` to \n```bash\ntrainer.n_gpus_per_node=<NUM_GPUS_TO_USE> \\\n```\n### 3. Run Benchmark Evaluation\n\n**Step 1: Start the Tool Server**\n```bash\npython start_tool_server.py \\\n--port <PORT_NUMBER> \\ # to specify the port to listen to (default 8888)\n--enable-cache # to enable caching tool results (recommended to save api credits)\n```\n\n**Step 2: Run the Evaluation**\n\nStart a new terminal, then run the experiment.\n```bash\ndocker exec -it pokeeresearch bash\ncd PokeeResearchOSS\nbash run.sh\n```\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-11-16T02:44:41.897650"
  },
  {
    "basic_info": {
      "name": "pico-banana-400k",
      "full_name": "apple/pico-banana-400k",
      "owner": "apple",
      "description": null,
      "url": "https://github.com/apple/pico-banana-400k",
      "clone_url": "https://github.com/apple/pico-banana-400k.git",
      "ssh_url": "git@github.com:apple/pico-banana-400k.git",
      "homepage": null,
      "created_at": "2025-10-21T21:15:35Z",
      "updated_at": "2025-11-15T18:16:14Z",
      "pushed_at": "2025-10-28T20:51:33Z"
    },
    "stats": {
      "stars": 1641,
      "forks": 70,
      "watchers": 1641,
      "open_issues": 7,
      "size": 7260
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 3369
      },
      "license": "Other",
      "topics": []
    },
    "content": {
      "readme": "# üçå Pico-Banana-400K: A Large-Scale Dataset for Text-Guided Image Editing\n\n<font size=7><div align='center' > [[üìñ Paper](https://www.arxiv.org/pdf/2510.19808)]  </div></font>\n\n**Pico-Banana-400K** is a large-scale dataset of **~400K text‚Äìimage‚Äìedit triplets** designed to advance research in **text-guided image editing**.  \nEach example contains:\n- an **original image** (from [Open Images](https://storage.googleapis.com/openimages/web/factsfigures.html)),  \n- a **human-like edit instruction**, and  \n- the **edited result** generated by Nano-Banana and verified by Gemini-2.5-Pro.\n\nThe dataset spans **35 edit operations** across **8 semantic categories**, covering diverse transformations‚Äîfrom low-level color adjustments to high-level object, scene, and stylistic edits.\n\n---\n\n\n\n## üß© Key Features\n\n| Feature | Description |\n|----------|-------------|\n| **Total Samples** | ~257K single-turn text‚Äìimage‚Äìedit triplets for SFT, ~56K single-turn text-image(positive) - image(negative)-edit for preference learning, and ~72K multi-turn texts-images-edits for multi-turn applications|\n| **Source** | [Open Images](https://storage.googleapis.com/openimages/web/factsfigures.html) |\n| **Edit Operations** | 35 across 8 semantic categories |\n| **Categories** | Pixel & Photometric, Object-Level, Scene Composition, Stylistic, Text & Symbol, Human-Centric, Scale & Perspective, Spatial/Layout |\n| **Image Resolution** | 512‚Äì1024 px |\n| **Prompt Generator** | [Gemini-2.5-Flash](https://deepmind.google/models/gemini/flash/) |\n| **Editing Model** | Nano-Banana |\n| **Self-Evaluation** | Automated judging pipeline using Gemini-2.5-Pro for edit quality |\n\n---\n\n## üèóÔ∏è Dataset Construction\n\nPico-Banana-400K is built using a **two-stage multimodal generation pipeline**:\n\n1. **Instruction Generation**  \n   Each Open Images sample is passed to *Gemini-2.5-Flash*, which writes concise, natural-language editing instructions grounded in visible content. We also provide short instructions summarized by Qwen-2.5-Instruct-7B. \n   Example:  \n   ```json\n   {\n     \"instruction\": \"Change the red car to blue.\"\n   }\n   \n\n2. **Editing + Self-Evaluation**\n   The Nano-Banana model performs the edit, then automatically evaluates the result using a structured quality prompt that measures:\n   Instruction Compliance (40%)\n   Editing Realism (25%)\n   Preservation Balance (20%)\n   Technical Quality (15%)\n   Only edits scoring above a strict threshold (~0.7) are labeled as successful, forming the main dataset; the remaining ~56K are retained as failure cases for robustness and preference learning.\n\n## üìä Dataset Statistics\n\n**Nano-Banana-400K** contains **~400K image editing data**, covering a wide visual and semantic range drawn from real-world imagery.\n\n---\n\n### üß≠ Category Distribution\n\n| Category | Description | Percentage |\n|:----------|:-------------|:------------:|\n| **Object-Level Semantic** | Add, remove, replace, or relocate objects | **35%** |\n| **Scene Composition & Multi-Subject** | Contextual and environmental transformations | **20%** |\n| **Human-Centric** | Edits involving clothing, expression, or appearance | **18%** |\n| **Stylistic** | Domain and artistic style transfer | **10%** |\n| **Text & Symbol** | Edits involving visible text, signs, or symbols | **8%** |\n| **Pixel & Photometric** | Brightness, contrast, and tonal adjustments | **5%** |\n| **Scale & Perspective** | Zoom, viewpoint, or framing changes | **2%** |\n| **Spatial / Layout** | Outpainting, composition, or canvas extension | **2%** |\n\n---\n\n### üìÇ Data Composition\n\n- **Single-Turn SFT samples (successful edits):** ~257K  \n- **Single-Turn Preference samples (failure cases):** ~56K\n- **Multi-Turn SFT samples (successful cases):** ~72K  \n- **Gemini-generated instructions:** concise, natural, and image-aware\n- **Edit coverage:** 35 edit types across 8 semantic categories  \n- **Image diversity:** includes humans, objects, text-rich scenes, etc from Open Images  \n\n---\n\n### üñºÔ∏è Visualization\n\nBelow are representative examples from different categories:\n\n| Category | Example |\n|:----------|:---------|\n| Object-Level | ‚ÄúReplace the red apple with a green one.‚Äù |\n| Scene Composition | ‚ÄúAdd sunlight streaming through the window.‚Äù |\n| Human-Centric | ‚ÄúChange the person‚Äôs expression to smiling.‚Äù |\n| Text & Symbol | ‚ÄúUppercase the text on the billboard.‚Äù |\n| Stylistic | ‚ÄúConvert the image to a Van Gogh painting style.‚Äù |\n\n---\n\nPico-Banana-400K provides both **breadth** (diverse edit operations) and **depth** (quality-controlled multimodal supervision), making it a strong foundation for training and evaluating text-guided image editing models.\n\n## üß† Applications\n\n**Pico-Banana-400K** serves as a versatile resource for advancing controllable and instruction-aware image editing.  \nBeyond single-step editing, the dataset enables **multi-turn, conversational editing** and **reward-based training paradigms**.\n\n\n\n## üì¶ Dataset Download Guide\n\nThe **Pico-Banana-400K** dataset is hosted on Apple‚Äôs public CDN.  \nYou ",
      "default_branch": "main"
    },
    "fetched_at": "2025-11-16T02:44:43.007748"
  },
  {
    "basic_info": {
      "name": "Emu3.5",
      "full_name": "baaivision/Emu3.5",
      "owner": "baaivision",
      "description": "Native Multimodal Models are World Learners",
      "url": "https://github.com/baaivision/Emu3.5",
      "clone_url": "https://github.com/baaivision/Emu3.5.git",
      "ssh_url": "git@github.com:baaivision/Emu3.5.git",
      "homepage": "",
      "created_at": "2025-10-29T13:40:19Z",
      "updated_at": "2025-11-16T00:17:58Z",
      "pushed_at": "2025-11-13T10:05:19Z"
    },
    "stats": {
      "stars": 1240,
      "forks": 42,
      "watchers": 1240,
      "open_issues": 24,
      "size": 23790
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 225201
      },
      "license": "Apache License 2.0",
      "topics": []
    },
    "content": {
      "readme": "<div align='center'>\n<h1>Emu3.5: Native Multimodal Models are World Learners</h1>\n\nEmu3.5 Team, BAAI\n\n[Project Page](https://emu.world/) | [ü§óHF Models](https://huggingface.co/collections/BAAI/emu35) | [Paper](https://arxiv.org/pdf/2510.26583)\n</div>\n\n\n<div align='center'>\n<img src=\"./assets/arch.png\" class=\"interpolation-image\" alt=\"arch.\" height=\"100%\" width=\"100%\" />\n</div>\n\n\n<div align='center'>\n<img src=\"./assets/co.png\" class=\"interpolation-image\" alt=\"arch.\" height=\"90%\" width=\"90%\" />\n</div>\n\n\n|  üîπ | **Core Concept**                         | **Description**                                                                                                                            |\n| :-: | :--------------------------------------- | :----------------------------------------------------------------------------------------------------------------------------------------- |\n|  üß† | **Unified World Modeling**               | Predicts the **next state jointly across vision and language**, enabling coherent **world modeling** and **generation**.              |\n|  üß© | **End-to-End Pretraining**               | Trained with a **unified next-token prediction** objective over **interleaved vision‚Äìlanguage sequences**.                                 |\n|  üìö | **Over 10T+ Multimodal Tokens**               | Pre-trained on **over 10 trillion interleaved tokens** from **video frames** and **transcripts**, capturing **spatiotemporal structure**.       |\n|  üîÑ | **Native Multimodal I/O**                | Processes and generates **interleaved visual‚Äìtext sequences** without **modality adapters** or **task-specific heads**.                    |\n|  üéØ | **RL Post-Training**                     | Large-scale **reinforcement learning** enhances **reasoning**, **compositionality**, and **generation quality**.                           |\n|  ‚ö°  | **Discrete Diffusion Adaptation (DiDA)** | Converts **sequential decoding ‚Üí bidirectional parallel prediction**, achieving **‚âà20√ó faster inference without performance loss**.      |\n| üñºÔ∏è | **Versatile Generation**                 | Excels in **long-horizon vision‚Äìlanguage generation**, **any-to-image (X2I)** synthesis, and **text-rich image creation**.                 |\n|  üåê | **Generalizable World Modeling**         | Enables **spatiotemporally consistent world exploration**, and **open-world embodied manipulation** across diverse scenarios.          |\n|  üèÜ | **Performance Benchmark**                | Matches **Gemini 2.5 Flash Image (Nano Banana)** on **image generation/editing**, and **outperforms** on **interleaved generation tasks**. |\n\n\n\n## Table of Contents\n\n1. [Model & Weights](#1-model--weights)\n2. [Quick Start](#2-quick-start)\n3. [Schedule](#3-schedule)\n4. [Citation](#4-citation)\n\n## 1. Model & Weights\n\n| Model name               | HF Weight |\n| ------------------------ | --------- |\n| Emu3.5               | [ü§ó HF link](https://huggingface.co/BAAI/Emu3.5/tree/main) |\n| Emu3.5-Image                | [ü§ó HF link](https://huggingface.co/BAAI/Emu3.5-Image/tree/main) |\n| Emu3.5-VisionTokenizer     | [ü§ó HF link](https://huggingface.co/BAAI/Emu3.5-VisionTokenizer/tree/main) |\n\n\n*Note:*  \n- **Emu3.5** supports general-purpose multimodal predictions, including interleaved image-text generation and single-image generation (T2I/X2I) tasks.\n- **Emu3.5-Image** is a model focused on T2I/X2I tasks for best performance on these scenarios.\n- Both models are pure next-token predictors without DiDA acceleration (each image may take several minutes to generate).  \n- ‚ö° **Stay tuned for DiDA-accelerated weights.**\n\n> üí° **Usage tip:**  \n> For **interleaved image-text generation**, use **Emu3.5**.  \n> For **single-image generation** (T2I and X2I), use **Emu3.5-Image** for the best quality.\n\n\n\n## 2. Quick Start\n\n### Environment Setup\n\n```bash\n# Python 3.10 or higher is required.\ngit clone https://github.com/baaivision/Emu3.5\ncd Emu3.5\npip install -r requirements.txt\npip install flash_attn==2.8.3 --no-build-isolation\n```\n### Configuration\n\nEdit `configs/config.py` to set:\n\n- Paths: `model_path`, `vq_path`\n- Task template: `task_type in {t2i, x2i, howto, story, explore, vla}`\n- Input image: `use_image` (True to provide reference images, controls <|IMAGE|> token); set `reference_image` in each prompt to specify the image path. For x2i task, we recommand using `reference_image` as a list containing single/multiple image paths to be compatible with multi-image input.\n- Sampling: `sampling_params` (classifier_free_guidance, temperature, top_k/top_p, etc.)\n- Aspect Ratio (for t2i task): `aspect_ratio` (\"4:3\", \"21:9\", \"1:1\", \"auto\" etc..)\n\n### Run Inference\n\n```bash\npython inference.py --cfg configs/config.py\n```\n\n\n#### Example Configurations by Task\nBelow are example commands for different tasks.\nMake sure to set CUDA_VISIBLE_DEVICES according to your available GPUs.\n\n\n```bash\n# üñºÔ∏è Text-to-Image (T2I) task\nCUDA_VISIBLE_DEVICES=0 python inference.py --cfg configs/example_config_t2i.py\n\n# üîÑ Any-to-Image (X2I) t",
      "default_branch": "main"
    },
    "fetched_at": "2025-11-16T02:44:44.154132"
  },
  {
    "basic_info": {
      "name": "LongCat-Video",
      "full_name": "meituan-longcat/LongCat-Video",
      "owner": "meituan-longcat",
      "description": null,
      "url": "https://github.com/meituan-longcat/LongCat-Video",
      "clone_url": "https://github.com/meituan-longcat/LongCat-Video.git",
      "ssh_url": "git@github.com:meituan-longcat/LongCat-Video.git",
      "homepage": null,
      "created_at": "2025-10-25T06:49:49Z",
      "updated_at": "2025-11-16T01:13:46Z",
      "pushed_at": "2025-11-04T10:07:37Z"
    },
    "stats": {
      "stars": 1150,
      "forks": 105,
      "watchers": 1150,
      "open_issues": 10,
      "size": 1300750
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 328127
      },
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "# LongCat-Video\n\n<div align=\"center\">\n  <img src=\"assets/longcat-video_logo.svg\" width=\"45%\" alt=\"LongCat-Video\" />\n</div>\n<hr>\n\n<div align=\"center\" style=\"line-height: 1;\">\n  <a href='https://meituan-longcat.github.io/LongCat-Video/'><img src='https://img.shields.io/badge/Project-Page-green'></a>\n  <a href='https://arxiv.org/abs/2510.22200'><img src='https://img.shields.io/badge/Technique-Report-red'></a>\n  <a href='https://huggingface.co/meituan-longcat/LongCat-Video'><img src='https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Model-blue'></a>\n</div>\n\n<div align=\"center\" style=\"line-height: 1;\">\n  <a href='https://github.com/meituan-longcat/LongCat-Flash-Chat/blob/main/figures/wechat_official_accounts.png'><img src='https://img.shields.io/badge/WeChat-LongCat-brightgreen?logo=wechat&logoColor=white'></a>  \n  <a href='https://x.com/Meituan_LongCat'><img src='https://img.shields.io/badge/Twitter-LongCat-white?logo=x&logoColor=white'></a>\n</div>\n\n<div align=\"center\" style=\"line-height: 1;\">\n  <a href='LICENSE'><img src='https://img.shields.io/badge/License-MIT-f5de53?&color=f5de53'></a>\n</div>\n\n## Model Introduction\nWe introduce LongCat-Video, a foundational video generation model with 13.6B parameters, delivering strong performance across *Text-to-Video*, *Image-to-Video*, and *Video-Continuation* generation tasks. It particularly excels in efficient and high-quality long video generation, representing our first step toward world models.\n\n### Key Features\n- üåü **Unified architecture for multiple tasks**: LongCat-Video unifies *Text-to-Video*, *Image-to-Video*, and *Video-Continuation* tasks within a single video generation framework. It natively supports all these tasks with a single model and consistently delivers strong performance across each individual task.\n- üåü **Long video generation**: LongCat-Video is natively pretrained on *Video-Continuation* tasks, enabling it to produce minutes-long videos without color drifting or quality degradation.\n- üåü **Efficient inference**: LongCat-Video generates $720p$, $30fps$ videos within minutes by employing a coarse-to-fine generation strategy along both the temporal and spatial axes. Block Sparse Attention further enhances efficiency, particularly at high resolutions\n- üåü **Strong performance with multi-reward RLHF**: Powered by multi-reward Group Relative Policy Optimization (GRPO), comprehensive evaluations on both internal and public benchmarks demonstrate that LongCat-Video achieves performance comparable to leading open-source video generation models as well as the latest commercial solutions.\n\nFor more detail, please refer to the comprehensive [***LongCat-Video Technical Report***](https://arxiv.org/abs/2510.22200).\n\n## üé• Teaser Video\n\n<div align=\"center\">\n  <video src=\"https://github.com/user-attachments/assets/00fa63f0-9c4e-461a-a79e-c662ad596d7d\" width=\"2264\" height=\"384\"> </video>\n</div>\n\n## Quick Start\n\n### Installation\n\nClone the repo:\n\n```shell\ngit clone --single-branch --branch main https://github.com/meituan-longcat/LongCat-Video\ncd LongCat-Video\n```\n\nInstall dependencies:\n\n```shell\n# create conda environment\nconda create -n longcat-video python=3.10\nconda activate longcat-video\n\n# install torch (configure according to your CUDA version)\npip install torch==2.6.0+cu124 torchvision==0.21.0+cu124 torchaudio==2.6.0 --index-url https://download.pytorch.org/whl/cu124\n\n# install flash-attn-2\npip install ninja \npip install psutil \npip install packaging \npip install flash_attn==2.7.4.post1\n\n# install other requirements\npip install -r requirements.txt\n```\n\nFlashAttention-2 is enabled in the model config by default; you can also change the model config (\"./weights/LongCat-Video/dit/config.json\") to use FlashAttention-3 or xformers once installed.\n\n### Model Download\n\n| Models | Download Link |\n| --- | --- |\n| LongCat-Video | ü§ó [Huggingface](https://huggingface.co/meituan-longcat/LongCat-Video) |\n\nDownload models using huggingface-cli:\n```shell\npip install \"huggingface_hub[cli]\"\nhuggingface-cli download meituan-longcat/LongCat-Video --local-dir ./weights/LongCat-Video\n```\n\n### Run Text-to-Video\n\n```shell\n# Single-GPU inference\ntorchrun run_demo_text_to_video.py --checkpoint_dir=./weights/LongCat-Video --enable_compile\n\n# Multi-GPU inference\ntorchrun --nproc_per_node=2 run_demo_text_to_video.py --context_parallel_size=2 --checkpoint_dir=./weights/LongCat-Video --enable_compile\n```\n\n### Run Image-to-Video\n\n```shell\n# Single-GPU inference\ntorchrun run_demo_image_to_video.py --checkpoint_dir=./weights/LongCat-Video --enable_compile\n\n# Multi-GPU inference\ntorchrun --nproc_per_node=2 run_demo_image_to_video.py --context_parallel_size=2 --checkpoint_dir=./weights/LongCat-Video --enable_compile\n```\n\n### Run Video-Continuation\n\n```shell\n# Single-GPU inference\ntorchrun run_demo_video_continuation.py --checkpoint_dir=./weights/LongCat-Video --enable_compile\n\n# Multi-GPU inference\ntorchrun --nproc_per_node=2 run_demo_video_continuation.py --context_parallel_size=2 -",
      "default_branch": "main"
    },
    "fetched_at": "2025-11-16T02:44:45.282230"
  },
  {
    "basic_info": {
      "name": "crypto-trading-open",
      "full_name": "cryptocj520/crypto-trading-open",
      "owner": "cryptocj520",
      "description": "crypto-trading-open",
      "url": "https://github.com/cryptocj520/crypto-trading-open",
      "clone_url": "https://github.com/cryptocj520/crypto-trading-open.git",
      "ssh_url": "git@github.com:cryptocj520/crypto-trading-open.git",
      "homepage": null,
      "created_at": "2025-11-11T12:00:02Z",
      "updated_at": "2025-11-16T01:11:48Z",
      "pushed_at": "2025-11-11T12:03:28Z"
    },
    "stats": {
      "stars": 1137,
      "forks": 645,
      "watchers": 1137,
      "open_issues": 8,
      "size": 997
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 2825921,
        "Shell": 37998
      },
      "license": null,
      "topics": []
    },
    "content": {
      "readme": "# Â§ö‰∫§ÊòìÊâÄÁ≠ñÁï•Ëá™Âä®ÂåñÁ≥ªÁªü\n\n**Multi-Exchange Strategy Automation System**\n\n## üéØ È°πÁõÆÁÆÄ‰ªã\n\nËøôÊòØ‰∏Ä‰∏™‰ºÅ‰∏öÁ∫ßÁöÑÂ§ö‰∫§ÊòìÊâÄÂä†ÂØÜË¥ßÂ∏ÅËá™Âä®Âåñ‰∫§ÊòìÁ≥ªÁªüÔºåÊèê‰æõÈ´òÊÄßËÉΩ„ÄÅÈ´òÂèØÈù†ÊÄßÁöÑÁΩëÊ†º‰∫§Êòì„ÄÅÂà∑Èáè‰∫§Êòì„ÄÅÂ•óÂà©ÁõëÊéßÂíåÂ∏ÇÂú∫ÁõëÊéßÂäüËÉΩ„ÄÇÁ≥ªÁªüÈááÁî®‰∏•Ê†ºÁöÑÂàÜÂ±ÇÊû∂ÊûÑËÆæËÆ°ÔºåÊîØÊåÅ Hyperliquid„ÄÅBackpack„ÄÅLighter„ÄÅBinance„ÄÅOKX„ÄÅEdgeX Á≠âÂ§ö‰∏™‰∫§ÊòìÊâÄÁöÑÂÆåÊï¥ÈÄÇÈÖç„ÄÇ\n\n## üèóÔ∏è Ê†∏ÂøÉÁ≥ªÁªüÊû∂ÊûÑ\n\n### Á≥ªÁªüÁªÑ‰ª∂\n\n```\nÂ§ö‰∫§ÊòìÊâÄÁ≠ñÁï•Ëá™Âä®ÂåñÁ≥ªÁªü\n‚îú‚îÄ‚îÄ üìä ÁΩëÊ†º‰∫§ÊòìÁ≥ªÁªü (Grid Trading)\n‚îÇ   ‚îú‚îÄ‚îÄ ÊôÆÈÄöÁΩëÊ†º              # Âõ∫ÂÆö‰ª∑Ê†ºÂå∫Èó¥ÁΩëÊ†º\n‚îÇ   ‚îú‚îÄ‚îÄ È©¨‰∏ÅÁΩëÊ†º              # È©¨‰∏ÅÊ†ºÂ∞îÈÄíÂ¢ûÁ≠ñÁï•\n‚îÇ   ‚îú‚îÄ‚îÄ ‰ª∑Ê†ºÁßªÂä®ÁΩëÊ†º          # Âä®ÊÄÅË∑üÈöè‰ª∑Ê†º\n‚îÇ   ‚îú‚îÄ‚îÄ Ââ•Â§¥ÁöÆÊ®°Âºè            # Âø´ÈÄüÊ≠¢ÊçüÁ≠ñÁï•\n‚îÇ   ‚îú‚îÄ‚îÄ Êô∫ËÉΩÂâ•Â§¥ÁöÆ            # Â§öÊ¨°Ê∑±Ë∑åÊ£ÄÊµã\n‚îÇ   ‚îú‚îÄ‚îÄ Êú¨Èáë‰øùÊä§Ê®°Âºè          # Ëá™Âä®Ê≠¢Êçü‰øùÊä§\n‚îÇ   ‚îú‚îÄ‚îÄ Ê≠¢ÁõàÊ®°Âºè              # Âà∞ËææÁõÆÊ†áËá™Âä®Âπ≥‰ªì\n‚îÇ   ‚îî‚îÄ‚îÄ Áé∞Ë¥ßÈ¢ÑÁïôÁÆ°ÁêÜ          # Áé∞Ë¥ßÂ∏ÅÁßçÈ¢ÑÁïô\n‚îú‚îÄ‚îÄ üîç ÁΩëÊ†ºÊ≥¢Âä®ÁéáÊâ´ÊèèÂô® (Grid Volatility Scanner)\n‚îÇ   ‚îú‚îÄ‚îÄ ËôöÊãüÁΩëÊ†ºÊ®°Êãü          # Êó†ÈúÄÂÆûÈôÖ‰∏ãÂçïÁöÑÊ®°ÊãüÁΩëÊ†º\n‚îÇ   ‚îú‚îÄ‚îÄ ÂÆûÊó∂APRËÆ°ÁÆó           # ÂáÜÁ°ÆÈ¢ÑÊµãÂπ¥ÂåñÊî∂ÁõäÁéá\n‚îÇ   ‚îú‚îÄ‚îÄ ‰ª£Â∏ÅÊéíË°åÊ¶ú            # ÊåâÊ≥¢Âä®ÁéáÂíåAPRÊéíÂ∫è\n‚îÇ   ‚îú‚îÄ‚îÄ Êô∫ËÉΩËØÑÁ∫ßÁ≥ªÁªü          # S/A/B/C/DÁ≠âÁ∫ßËØÑ‰º∞\n‚îÇ   ‚îî‚îÄ‚îÄ ÁªàÁ´Ø UI              # Rich ÂÆûÊó∂ÁõëÊéßÁïåÈù¢\n‚îú‚îÄ‚îÄ üíπ Âà∑Èáè‰∫§ÊòìÁ≥ªÁªü (Volume Maker)\n‚îÇ   ‚îú‚îÄ‚îÄ ÊåÇÂçïÊ®°Âºè              # Èôê‰ª∑ÂçïÂà∑ÈáèÔºàBackpackÔºâ\n‚îÇ   ‚îî‚îÄ‚îÄ Â∏Ç‰ª∑Ê®°Âºè              # Â∏Ç‰ª∑ÂçïÂø´ÈÄüÂà∑ÈáèÔºàLighterÔºâ\n‚îú‚îÄ‚îÄ üîÑ Â•óÂà©ÁõëÊéßÁ≥ªÁªü (Arbitrage Monitor)\n‚îÇ   ‚îú‚îÄ‚îÄ ‰ª∑Ê†ºÁõëÊéß              # ÂÆûÊó∂‰ª∑Ê†ºÂ∑ÆÁõëÊéß\n‚îÇ   ‚îú‚îÄ‚îÄ ËµÑÈáëË¥πÁéáÁõëÊéß          # Ë∑®‰∫§ÊòìÊâÄË¥πÁéáÂ∑ÆÂºÇ\n‚îÇ   ‚îú‚îÄ‚îÄ Â•óÂà©Êú∫‰ºöËØÜÂà´          # ‰ª∑Â∑ÆÂíåË¥πÁéáÂ•óÂà©\n‚îÇ   ‚îú‚îÄ‚îÄ ÁªàÁ´Ø UI              # Rich ÂÆûÊó∂ÁõëÊéßÁïåÈù¢\n‚îÇ   ‚îî‚îÄ‚îÄ ‰∫§ÊòìÂØπËá™Âä®ÂèëÁé∞        # Â§ö‰∫§ÊòìÊâÄ‰∫§ÊòìÂØπÂåπÈÖç\n‚îú‚îÄ‚îÄ üîî ‰ª∑Ê†ºÊèêÈÜíÁ≥ªÁªü (Price Alert)\n‚îÇ   ‚îú‚îÄ‚îÄ ‰ª∑Ê†ºÁ™ÅÁ†¥ÁõëÊéß          # ‰ª∑Ê†ºËß¶ÂèäÁõÆÊ†áÊèêÈÜí\n‚îÇ   ‚îú‚îÄ‚îÄ Â§ö‰∫§ÊòìÊâÄÊîØÊåÅ          # ÊîØÊåÅÊâÄÊúâÊé•ÂÖ•ÁöÑ‰∫§ÊòìÊâÄ\n‚îÇ   ‚îú‚îÄ‚îÄ ÁªàÁ´Ø UI              # ÂÆûÊó∂‰ª∑Ê†ºÊòæÁ§∫\n‚îÇ   ‚îî‚îÄ‚îÄ Â£∞Èü≥ÊèêÈÜí              # Á™ÅÁ†¥Êó∂Â£∞Èü≥ÈÄöÁü•\n‚îú‚îÄ‚îÄ üîó ‰∫§ÊòìÊâÄÈÄÇÈÖçÂ±Ç (Exchange Adapters)\n‚îÇ   ‚îú‚îÄ‚îÄ Hyperliquid ÈÄÇÈÖçÂô®    # Ê∞∏Áª≠ÂêàÁ∫¶ + Áé∞Ë¥ß\n‚îÇ   ‚îú‚îÄ‚îÄ Backpack ÈÄÇÈÖçÂô®       # Ê∞∏Áª≠ÂêàÁ∫¶\n‚îÇ   ‚îú‚îÄ‚îÄ Lighter ÈÄÇÈÖçÂô®        # Ê∞∏Áª≠ÂêàÁ∫¶Ôºà‰ΩéÊâãÁª≠Ë¥πÔºâ\n‚îÇ   ‚îú‚îÄ‚îÄ Binance ÈÄÇÈÖçÂô®        # Áé∞Ë¥ß + Ê∞∏Áª≠ÂêàÁ∫¶\n‚îÇ   ‚îú‚îÄ‚îÄ OKX ÈÄÇÈÖçÂô®            # Áé∞Ë¥ß + Ê∞∏Áª≠ÂêàÁ∫¶\n‚îÇ   ‚îú‚îÄ‚îÄ EdgeX ÈÄÇÈÖçÂô®          # Ê∞∏Áª≠ÂêàÁ∫¶\n‚îÇ   ‚îî‚îÄ‚îÄ Áªü‰∏ÄÊé•Âè£Ê†áÂáÜ          # Ê†áÂáÜÂåñ API Êé•Âè£\n‚îî‚îÄ‚îÄ üèõÔ∏è Âü∫Á°ÄËÆæÊñΩÂ±Ç (Infrastructure)\n    ‚îú‚îÄ‚îÄ ‰æùËµñÊ≥®ÂÖ•ÂÆπÂô®          # DI ÂÆπÂô®ÁÆ°ÁêÜ\n    ‚îú‚îÄ‚îÄ ‰∫ã‰ª∂Á≥ªÁªü              # ‰∫ã‰ª∂È©±Âä®Êû∂ÊûÑ\n    ‚îú‚îÄ‚îÄ Êó•ÂøóÁ≥ªÁªü              # ÁªìÊûÑÂåñÊó•Âøó\n    ‚îú‚îÄ‚îÄ ÈÖçÁΩÆÁÆ°ÁêÜ              # YAML ÈÖçÁΩÆÁ≥ªÁªü\n    ‚îî‚îÄ‚îÄ Êï∞ÊçÆËÅöÂêàÂô®            # Â§ö‰∫§ÊòìÊâÄÊï∞ÊçÆËÅöÂêà\n```\n\n## üöÄ Âø´ÈÄüÂºÄÂßã\n\n### Á≥ªÁªüË¶ÅÊ±Ç\n\n- Python 3.8+\n- ÊîØÊåÅÁöÑÊìç‰ΩúÁ≥ªÁªüÔºöLinux„ÄÅmacOS„ÄÅWindows\n- ÂèØÈÄâÔºötmuxÔºàÁî®‰∫éÂ§öËøõÁ®ãÁÆ°ÁêÜÔºâ\n\n### ÂÆâË£Ö‰æùËµñ\n\n```bash\n# ÂÆâË£Ö Python ‰æùËµñ\npip install -r requirements.txt\n```\n\n### ÈÖçÁΩÆ API ÂØÜÈí•\n\nÂú® `config/exchanges/` ÁõÆÂΩï‰∏ãÈÖçÁΩÆÂØπÂ∫î‰∫§ÊòìÊâÄÁöÑ API ÂØÜÈí•Ôºö\n\n```bash\nconfig/exchanges/\n‚îú‚îÄ‚îÄ hyperliquid_config.yaml   # Hyperliquid ÈÖçÁΩÆ\n‚îú‚îÄ‚îÄ backpack_config.yaml       # Backpack ÈÖçÁΩÆ\n‚îú‚îÄ‚îÄ lighter_config.yaml        # Lighter ÈÖçÁΩÆ\n‚îú‚îÄ‚îÄ binance_config.yaml        # Binance ÈÖçÁΩÆ\n‚îú‚îÄ‚îÄ okx_config.yaml            # OKX ÈÖçÁΩÆ\n‚îî‚îÄ‚îÄ edgex_config.yaml          # EdgeX ÈÖçÁΩÆ\n```\n\n### Âø´ÈÄüÂêØÂä®ÂêÑÁ≥ªÁªü\n\n#### ÁΩëÊ†º‰∫§ÊòìÁ≥ªÁªü\n```bash\npython3 run_grid_trading.py config/grid/lighter-long-perp-btc.yaml\n```\n\n#### Âà∑Èáè‰∫§ÊòìÁ≥ªÁªüÔºàBackpackÊåÇÂçïÊ®°ÂºèÔºâ\n```bash\npython3 run_volume_maker.py config/volume_maker/backpack_btc_volume_maker.yaml\n```\n\n#### Âà∑Èáè‰∫§ÊòìÁ≥ªÁªüÔºàLighterÂ∏Ç‰ª∑Ê®°ÂºèÔºâ\n```bash\npython3 run_lighter_volume_maker.py config/volume_maker/lighter_volume_maker.yaml\n```\n\n#### Â•óÂà©ÁõëÊéßÁ≥ªÁªü\n```bash\npython3 run_arbitrage_monitor.py\n```\n\n#### ‰ª∑Ê†ºÊèêÈÜíÁ≥ªÁªü\n```bash\npython3 run_price_alert.py config/price_alert/binance_alert.yaml\n```\n\n#### ÁΩëÊ†ºÊ≥¢Âä®ÁéáÊâ´ÊèèÂô®\n```bash\npython3 grid_volatility_scanner/run_scanner.py\n```\n\n## üìã Ê†∏ÂøÉÂäüËÉΩËØ¶Ëß£\n\n### 1Ô∏è‚É£ ÁΩëÊ†º‰∫§ÊòìÁ≥ªÁªü\n\n#### ÂäüËÉΩÁâπÊÄß\n\n- **Â§öÁßçÁΩëÊ†ºÊ®°Âºè**ÔºöÊôÆÈÄöÁΩëÊ†º„ÄÅÈ©¨‰∏ÅÁΩëÊ†º„ÄÅ‰ª∑Ê†ºÁßªÂä®ÁΩëÊ†º\n- **Êô∫ËÉΩÁ≠ñÁï•**ÔºöÂâ•Â§¥ÁöÆ„ÄÅÊô∫ËÉΩÂâ•Â§¥ÁöÆ„ÄÅÊú¨Èáë‰øùÊä§„ÄÅÊ≠¢ÁõàÊ®°Âºè\n- **ÂÅ•Â∫∑Ê£ÄÊü•**ÔºöËá™Âä®ËÆ¢ÂçïÊ†°È™åÂíå‰øÆÂ§çÊú∫Âà∂\n- **ÁªàÁ´Ø UI**ÔºöÂÆûÊó∂ÁõëÊéßÁïåÈù¢ÔºåÊòæÁ§∫ÊåÅ‰ªì„ÄÅÁõà‰∫è„ÄÅÁΩëÊ†ºÁä∂ÊÄÅ\n- **Áé∞Ë¥ßÊîØÊåÅ**ÔºöÁé∞Ë¥ßÈ¢ÑÁïôÁÆ°ÁêÜÔºàËá™Âä®Áª¥ÊåÅÂ∏ÅÁßç‰ΩôÈ¢ùÔºâ\n- **Â§ö‰∫§ÊòìÊâÄ**ÔºöÊîØÊåÅ Hyperliquid„ÄÅBackpack„ÄÅLighter\n\n#### ÈÖçÁΩÆÊñá‰ª∂‰ΩçÁΩÆ\n\n```\nconfig/grid/\n‚îú‚îÄ‚îÄ lighter_btc_perp_long.yaml              # Lighter BTC ÂÅöÂ§ö\n‚îú‚îÄ‚îÄ lighter_btc_perp_short.yaml             # Lighter BTC ÂÅöÁ©∫\n‚îú‚îÄ‚îÄ hyperliquid_btc_perp_long.yaml          # Hyperliquid BTC ÂÅöÂ§ö\n‚îú‚îÄ‚îÄ hyperliquid_btc_perp_short.yaml         # Hyperliquid BTC ÂÅöÁ©∫\n‚îú‚îÄ‚îÄ hyperliquid_btc_spot_long.yaml          # Hyperliquid Áé∞Ë¥ßÂÅöÂ§ö\n‚îú‚îÄ‚îÄ backpack_capital_protection_long_btc.yaml   # Backpack BTC Êú¨Èáë‰øùÊä§\n‚îú‚îÄ‚îÄ backpack_capital_protection_long_eth.yaml   # Backpack ETH Êú¨Èáë‰øùÊä§\n‚îú‚îÄ‚îÄ backpack_capital_protection_long_sol.yaml   # Backpack SOL Êú¨Èáë‰øùÊä§\n‚îú‚îÄ‚îÄ backpack_capital_protection_long_bnb.yaml   # Backpack BNB Êú¨Èáë‰øùÊä§\n‚îî‚îÄ‚îÄ backpack_capital_protection_long_hype.yaml  # Backpack HYPE Êú¨Èáë‰øùÊä§\n```\n\n#### ÂêØÂä®ÊñπÂºè\n\n```bash\n# ÊñπÂºè1ÔºöÁõ¥Êé•ÂêØÂä®ÔºàÊé®ËçêÔºâ\npython3 run_grid_trading.py config/grid/lighter_btc_perp_long.yaml\npython3 run_grid_trading.py config/grid/lighter_eth_perp_long.yaml\n\n# ÊñπÂºè2ÔºöDEBUG Ê®°ÂºèÂêØÂä®ÔºàÊü•ÁúãËØ¶ÁªÜÊó•ÂøóÔºâ\npython3 run_grid_trading.py config/grid/lighter_btc_perp_long.yaml --debug\n\n# ÊñπÂºè3Ôºö‰ΩøÁî® Shell ËÑöÊú¨ÊâπÈáèÂêØÂä®ÔºàtmuxÔºâ\n./scripts/start_all_grids.sh\n```\n\n#### Ê†∏ÂøÉÊñá‰ª∂\n\n| Êñá‰ª∂Ë∑ØÂæÑ | ËØ¥Êòé |\n|---------|------|\n| `run_grid_trading.py` | ÁΩëÊ†º‰∫§ÊòìÁ≥ªÁªü‰∏ªÂêØÂä®ËÑöÊú¨ |\n| `core/services/grid/coordinator/grid_coordinator.py` | ÁΩëÊ†ºÁ≥ªÁªüÂçèË∞ÉÂô®ÔºàÊ†∏ÂøÉÈÄªËæëÔºâ |\n| `core/services/grid/implementations/grid_engine_impl.py` | ÁΩëÊ†ºÊâßË°åÂºïÊìé |\n| `core/services/grid/implementations/grid_strategy_impl.py` | ÁΩëÊ†ºÁ≠ñÁï•ÂÆûÁé∞ |\n| `core/services/grid/implementations/position_tracker_impl.py` | ÊåÅ‰ªìË∑üË∏™Âô® |\n| `core/services/grid/implementations/order_health_checker.py` | ËÆ¢ÂçïÂÅ•Â∫∑Ê£ÄÊü•Âô® |\n| `core/services/grid/scalping/scalping_manager.py` | Ââ•Â§¥ÁöÆÁÆ°ÁêÜÂô® |\n| `core/services/grid/scalping/smart_scalping_tracker.py` | Êô∫ËÉΩÂâ•Â§¥ÁöÆËøΩË∏™Âô® |\n| `core/services/grid/capital_protection/capital_protection_manager.py` | Êú¨Èáë‰øùÊä§ÁÆ°ÁêÜÂô® |\n| `core/services/grid/terminal_ui.py` | ÁªàÁ´Ø UI ÁïåÈù¢ |\n\n### 2Ô∏è‚É£ Âà∑Èáè‰∫§ÊòìÁ≥ªÁªü\n\n#### ÂäüËÉΩÁâπÊÄß\n\n- **Âèå‰∫§ÊòìÊ®°Âºè**ÔºöÊåÇÂçïÊ®°ÂºèÔºàBackpackÔºâ„ÄÅÂ∏Ç‰ª∑Ê®°ÂºèÔºàLighterÔºâ\n- **‰ø°Âè∑Ê∫êÊîØÊåÅ**ÔºöBackpack REST API„ÄÅHyperliquid WebSocket\n- **Êô∫ËÉΩÂà§Êñ≠**Ôºö‰π∞ÂçñÂçïÊï∞ÈáèÂØπÊØî„ÄÅ‰ª∑Ê†ºÂèòÂä®ÁõëÊéß\n- **ÂÆûÊó∂ÁªüËÆ°**ÔºöÊàê‰∫§Èáè„ÄÅ",
      "default_branch": "main"
    },
    "fetched_at": "2025-11-16T02:44:46.399873"
  },
  {
    "basic_info": {
      "name": "DeekSeek-OCR---Dockerized-API",
      "full_name": "Bogdanovich77/DeekSeek-OCR---Dockerized-API",
      "owner": "Bogdanovich77",
      "description": null,
      "url": "https://github.com/Bogdanovich77/DeekSeek-OCR---Dockerized-API",
      "clone_url": "https://github.com/Bogdanovich77/DeekSeek-OCR---Dockerized-API.git",
      "ssh_url": "git@github.com:Bogdanovich77/DeekSeek-OCR---Dockerized-API.git",
      "homepage": null,
      "created_at": "2025-10-21T23:30:09Z",
      "updated_at": "2025-11-15T22:26:01Z",
      "pushed_at": "2025-10-22T19:32:31Z"
    },
    "stats": {
      "stars": 1015,
      "forks": 113,
      "watchers": 1015,
      "open_issues": 10,
      "size": 117
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 160119,
        "Batchfile": 2335,
        "Dockerfile": 2214
      },
      "license": null,
      "topics": []
    },
    "content": {
      "readme": "# DeepSeek-OCR: PDF to Markdown Converter\n\nA powerful OCR solution that converts PDF documents to Markdown format using DeepSeek-OCR with FastAPI backend. This project provides both a batch processing script and a REST API for flexible document conversion.\n\n## üöÄ Quick Start\n\n### Option 1: Batch Processing with pdf_to_markdown_processor.py\n\n1. Place your PDF files in the `data/` directory\n2. Ensure the DeepSeek-OCR API is running (see Docker setup below)\n3. Run the processor:\n\n```bash\npython pdf_to_markdown_processor.py\n```\n\n### Option 2: REST API with Docker Backend\n\n1. Build and start the Docker container\n2. Use the API endpoints to process documents\n3. Integrate with your applications\n\n---\n\n## üìã Prerequisites\n\n### Hardware Requirements\n- **NVIDIA GPU** with CUDA 11.8+ support\n- **GPU Memory**: Minimum 12GB VRAM (Model takes ~9GB)\n- **System RAM**: Minimum 32GB (recommended: 64GB+)\n- **Storage**: 50GB+ free space for model and containers\n\n### Software Requirements\n- **Python 3.8+** (for local processing)\n- **Docker** 20.10+ with GPU support\n- **Docker Compose** 2.0+\n- **NVIDIA Container Toolkit** installed\n- **CUDA 11.8** compatible drivers\n\n---\n\n## üê≥ Docker Backend Setup\n\n### 1. Download Model Weights\n\nCreate a directory for model weights and download the DeepSeek-OCR model:\n\n```bash\n# Create models directory\nmkdir -p models\n\n# Download using Hugging Face CLI\npip install huggingface_hub\nhuggingface-cli download deepseek-ai/DeepSeek-OCR --local-dir models/deepseek-ai/DeepSeek-OCR\n\n# Or using git\ngit clone https://huggingface.co/deepseek-ai/DeepSeek-OCR models/deepseek-ai/DeepSeek-OCR\n```\n\n### 2. Build and Run the Docker Container\n\n#### Windows Users\n\n```cmd\nREM Build the Docker image\nbuild.bat\n\nREM Start the service\ndocker-compose up -d\n\nREM Check logs\ndocker-compose logs -f deepseek-ocr\n```\n\n#### Linux/macOS Users\n\n```bash\n# Build the Docker image\ndocker-compose build\n\n# Start the service\ndocker-compose up -d\n\n# Check logs\ndocker-compose logs -f deepseek-ocr\n```\n\n### 3. Verify Installation\n\n```bash\n# Health check\ncurl http://localhost:8000/health\n\n# Expected response:\n{\n  \"status\": \"healthy\",\n  \"model_loaded\": true,\n  \"model_path\": \"/app/models/deepseek-ai/DeepSeek-OCR\",\n  \"cuda_available\": true,\n  \"cuda_device_count\": 1\n}\n```\n\n---\n\n## üìÑ PDF Processing Scripts\n\nThis project provides several PDF processing scripts, each designed for different use cases. All scripts scan the `data/` directory for PDF files and convert them to Markdown format with different prompts and post-processing options.\n\n### Output Naming Convention\n\nAll processors append a suffix to the output filename to indicate the processing method used:\n- **-MD.md**: Markdown conversion (preserves document structure)\n- **-OCR.md**: Plain OCR extraction (raw text without formatting)\n- **-CUSTOM.md**: Custom prompt processing (uses prompt from YAML file)\n\nFor example, processing `document.pdf` will create:\n- `document-MD.md` (markdown processors)\n- `document-OCR.md` (OCR processor)\n- `document-CUSTOM.md` (custom prompt processors)\n\n---\n\n### 1. pdf_to_markdown_processor.py\n\n**Purpose**: Basic PDF to Markdown conversion using the standard markdown prompt\n\n**Features**:\n- Uses prompt: `'<image>\\n<|grounding|>Convert the document to markdown.'`\n- Converts PDFs to structured Markdown format\n- Simple processing without image extraction\n- Outputs files with `-MD.md` suffix\n\n**Usage**:\n```bash\n# Place PDF files in the data directory\ncp your_document.pdf data/\n\n# Run the processor\npython pdf_to_markdown_processor.py\n\n# Check results\nls data/*-MD.md\n```\n\n---\n\n### 2. pdf_to_markdown_processor_enhanced.py\n\n**Purpose**: Enhanced PDF to Markdown conversion with post-processing\n\n**Features**:\n- Uses the same markdown prompt as the basic version\n- **Post-processing features**:\n  - Image extraction and saving to `data/images/` folder\n  - Special token cleanup\n  - Reference processing for layout information\n  - Content cleaning and formatting\n- Outputs files with `-MD.md` suffix\n\n**Usage**:\n```bash\n# Place PDF files in the data directory\ncp your_document.pdf data/\n\n# Run the enhanced processor\npython pdf_to_markdown_processor_enhanced.py\n\n# Check results (including extracted images)\nls data/*-MD.md\nls data/images/\n```\n\n---\n\n### 3. pdf_to_ocr_enhanced.py\n\n**Purpose**: Plain OCR text extraction without markdown formatting\n\n**Features**:\n- Uses OCR prompt: `'<image>\\nFree OCR.'`\n- Extracts raw text without markdown structure\n- Includes the same post-processing features as the enhanced markdown processor\n- Outputs files with `-OCR.md` suffix\n\n**Usage**:\n```bash\n# Place PDF files in the data directory\ncp your_document.pdf data/\n\n# Run the OCR processor\npython pdf_to_ocr_enhanced.py\n\n# Check results\nls data/*-OCR.md\n```\n\n---\n\n### 4. pdf_to_custom_prompt.py\n\n**Purpose**: PDF processing with custom prompts (raw output)\n\n**Features**:\n- Uses custom prompt loaded from `custom_prompt.yaml`\n- Returns raw model response without post-processing\n- Ideal for testing and debugging ",
      "default_branch": "main"
    },
    "fetched_at": "2025-11-16T02:44:47.512181"
  },
  {
    "basic_info": {
      "name": "World-in-Novel-View",
      "full_name": "tianrun-chen/World-in-Novel-View",
      "owner": "tianrun-chen",
      "description": "Scaling Novel View Synthesis for Static and Dynamic Scenes",
      "url": "https://github.com/tianrun-chen/World-in-Novel-View",
      "clone_url": "https://github.com/tianrun-chen/World-in-Novel-View.git",
      "ssh_url": "git@github.com:tianrun-chen/World-in-Novel-View.git",
      "homepage": null,
      "created_at": "2025-10-17T14:43:19Z",
      "updated_at": "2025-11-16T02:25:16Z",
      "pushed_at": "2025-10-26T15:22:16Z"
    },
    "stats": {
      "stars": 955,
      "forks": 0,
      "watchers": 955,
      "open_issues": 0,
      "size": 9996
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 187502,
        "Shell": 4789
      },
      "license": null,
      "topics": []
    },
    "content": {
      "readme": "# World-in-Novel-View\n\n[![Stars](https://img.shields.io/github/stars/tianrun-chen/World-in-Novel-View?style=social)](https://github.com/tianrun-chen/World-in-Novel-View/stargazers)\n[![Python](https://img.shields.io/badge/Python-3.8%2B-blue)](https://www.python.org/)\n\n**Scaling Novel View Synthesis for Static and Dynamic Scenes**\n\n![(im3.gif)](https://github.com/tianrun-chen/World-in-Novel-View/blob/main/im3.gif)\n\n- **Distributed Training**: FSDP and DDP support for large-scale training\n- **Mixed Precision**: Automatic mixed precision (AMP) for faster training\n---\nTrain and Inference with Ascend 910b NPU\nTorch.NPU is required„ÄÇ\n\n\n## Project Structure\n```\n.\n‚îú‚îÄ‚îÄ geope_core/              # Core GeoPE implementation\n‚îÇ   ‚îú‚îÄ‚îÄ torch.py             # PyTorch GeoPE attention\n‚îÇ   ‚îî‚îÄ‚îÄ utils/               # Utility modules\n‚îÇ       ‚îú‚îÄ‚îÄ config.py        # Configuration utilities\n‚îÇ       ‚îú‚îÄ‚îÄ functional.py    # Functional utilities\n‚îÇ       ‚îú‚îÄ‚îÄ mha.py           # Multi-head attention\n‚îÇ       ‚îú‚îÄ‚îÄ runner.py        # Training runner\n‚îÇ       ‚îî‚îÄ‚îÄ transformer.py   # Transformer components\n‚îú‚îÄ‚îÄ src/                     # Application code\n‚îÇ   ‚îú‚îÄ‚îÄ geope_attention/     # GeoPE attention wrapper\n‚îÇ   ‚îú‚îÄ‚îÄ geope_utils/         # GeoPE utilities wrapper\n‚îÇ   ‚îú‚îÄ‚îÄ nvs_models/          # Novel view synthesis models\n‚îÇ   ‚îú‚îÄ‚îÄ nvs_data/            # Data loading and preprocessing\n‚îÇ   ‚îî‚îÄ‚îÄ nvs_training/        # Training and evaluation\n‚îú‚îÄ‚îÄ tests/                   # Unit tests\n‚îî‚îÄ‚îÄ scripts/                 # Utility scripts\n```\n### Training\n\n```bash\n# Single NPU training\npython -m src.main konet\n\n# Multi-NPU training with FSDP\ntorchrun --standalone --nproc-per-node=4 -m src.main konet-fsdp\n```\n\n\n\n# Dataset: KOKONI-WorldVID-1A\n\nKOKONI-WorldVID-1A is a large-scale video dataset designed for **Novel View Synthesis** research. It contains over **10,000 unique videos** sourced from **Bilibili**, one of China's leading video-sharing platforms.\n\n## üí° Dataset Highlights\n\nUnlike most existing novel view synthesis datasets, KOKONI-WorldVID-1A provides videos from real-world, diverse scenarios with a unique data domain. These videos are created by a wide range of content creators, covering everything from static landscapes and object displays to dynamic human activities and lifestyle recordings.\n\n- **Data Source**: All videos are sourced from Bilibili, providing the research community with a perspective distinct from Western-dominated datasets.\n- **Scale**: Contains over 10,000 unique videos, offering sufficient data for deep learning model training.\n- **Content Diversity**: Videos encompass a wide variety of content, helping to improve model generalization in complex real-world scenarios.\n- **Static & Dynamic**: The dataset includes both static and partially dynamic videos. For static scene videos, we additionally provide human-screened static segments to facilitate more fine-grained model training and evaluation.\n\n## üìä Dataset Statistics\n\n| Category | Count | Description |\n|----------|-------|-------------|\n| **Total Videos** | 10,000 | Unique videos from Bilibili |\n| **Static Videos** | ~5,000 | Videos with static scenes and annotated segments |\n| **Dynamic Videos** | ~5,000 | Videos with dynamic content (e.g., walking, movement) |\n| **Data Domain** | Chinese UGC | User-generated content from China |\n| **Application** | Novel View Synthesis | Training and evaluation of NVS models |\n\n## üìÇ Data Structure\n\nThe dataset is organized using two CSV files that contain metadata for all videos:\n\n### 1. `static.csv` - Static Scene Videos\n\nContains videos with static scenes and human-annotated time segments.\n\n**Format:**\n```\nÂ∫èÂè∑,URL,ËßÜÈ¢ëÊ†áÈ¢ò,ÈùôÊÄÅÂºÄÂßãÊó∂Èó¥1,ÈùôÊÄÅÁªìÊùüÊó∂Èó¥1,ÈùôÊÄÅÂºÄÂßãÊó∂Èó¥2,ÈùôÊÄÅÁªìÊùüÊó∂Èó¥2\n1,https://www.bilibili.com/video/BV1xx411c7mD,Beautiful Landscape,00:10,00:35,01:20,02:15\n2,https://www.bilibili.com/video/BV1Ab411q7yH,Object Display,00:00,00:45,,\n```\n\n**Columns:**\n- `Â∫èÂè∑` (Index): Sequential number\n- `URL`: Bilibili video URL\n- `ËßÜÈ¢ëÊ†áÈ¢ò` (Video Title): Original video title\n- `ÈùôÊÄÅÂºÄÂßãÊó∂Èó¥1` (Static Start Time 1): Start time of first static segment (format: MM:SS or HH:MM:SS)\n- `ÈùôÊÄÅÁªìÊùüÊó∂Èó¥1` (Static End Time 1): End time of first static segment\n- `ÈùôÊÄÅÂºÄÂßãÊó∂Èó¥2` (Static Start Time 2): Start time of second static segment (optional)\n- `ÈùôÊÄÅÁªìÊùüÊó∂Èó¥2` (Static End Time 2): End time of second static segment (optional)\n\n**Note:** Additional segment pairs may exist (ÈùôÊÄÅÂºÄÂßãÊó∂Èó¥3, ÈùôÊÄÅÁªìÊùüÊó∂Èó¥3, etc.)\n\n### 2. `walk.csv` - Dynamic Scene Videos\n\nContains videos with dynamic content such as walking, movement, or changing scenes.\n\n**Format:**\n```\nÂ∫èÂè∑,URL,ËßÜÈ¢ëÊ†áÈ¢ò\n1,https://www.bilibili.com/video/BV1yZ4y1u7fA,City Walk Tour\n2,https://www.bilibili.com/video/BV1Hx411v7iP,Campus Walking\n```\n\n**Columns:**\n- `Â∫èÂè∑` (Index): Sequential number\n- `URL`: Bilibili video URL\n- `ËßÜÈ¢ëÊ†áÈ¢ò` (Video Title): Original video title\n\n## üì• Download & Usage\n\nWe provide a Python script (`download_videos.py`) to help users batch download videos from the dataset. Please follow these steps:\n\n### 1. Install Dependencies\n\n```bash\npip install you-get pandas\n```\n\nOr alternativel",
      "default_branch": "main"
    },
    "fetched_at": "2025-11-16T02:44:48.650851"
  },
  {
    "basic_info": {
      "name": "claude-scientific-skills",
      "full_name": "K-Dense-AI/claude-scientific-skills",
      "owner": "K-Dense-AI",
      "description": "A set of ready to use scientific skills for Claude",
      "url": "https://github.com/K-Dense-AI/claude-scientific-skills",
      "clone_url": "https://github.com/K-Dense-AI/claude-scientific-skills.git",
      "ssh_url": "git@github.com:K-Dense-AI/claude-scientific-skills.git",
      "homepage": "https://k-dense.ai",
      "created_at": "2025-10-19T20:54:15Z",
      "updated_at": "2025-11-16T01:20:06Z",
      "pushed_at": "2025-11-14T03:46:40Z"
    },
    "stats": {
      "stars": 868,
      "forks": 97,
      "watchers": 868,
      "open_issues": 0,
      "size": 3425
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 1294379,
        "JavaScript": 37795
      },
      "license": "MIT License",
      "topics": [
        "ai-scientist",
        "bioinformatics",
        "chemoinformatics",
        "claude",
        "claude-skills",
        "claudecode",
        "clinical-research",
        "computational-biology",
        "data-analysis",
        "drug-discovery",
        "genomics",
        "materials-science",
        "metabolomics",
        "proteomics",
        "scientific-computing",
        "scientific-visualization"
      ]
    },
    "content": {
      "readme": "# Claude Scientific Skills\n\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE.md)\n[![Skills](https://img.shields.io/badge/Skills-118-brightgreen.svg)](#whats-included)\n\nA comprehensive collection of **118+ ready-to-use scientific skills** for Claude, created by the K-Dense team. Transform Claude into your AI research assistant capable of executing complex multi-step scientific workflows across biology, chemistry, medicine, and beyond.\n\nThese skills enable Claude to seamlessly work with specialized scientific libraries, databases, and tools across multiple scientific domains:\n- üß¨ Bioinformatics & Genomics - Sequence analysis, single-cell RNA-seq, gene regulatory networks, variant annotation, phylogenetic analysis\n- üß™ Cheminformatics & Drug Discovery - Molecular property prediction, virtual screening, ADMET analysis, molecular docking, lead optimization\n- üî¨ Proteomics & Mass Spectrometry - LC-MS/MS processing, peptide identification, spectral matching, protein quantification\n- üè• Clinical Research & Precision Medicine - Clinical trials, pharmacogenomics, variant interpretation, drug safety, precision therapeutics\n- üß† Healthcare AI & Clinical ML - EHR analysis, physiological signal processing, medical imaging, clinical prediction models\n- üñºÔ∏è Medical Imaging & Digital Pathology - DICOM processing, whole slide image analysis, computational pathology, radiology workflows\n- ü§ñ Machine Learning & AI - Deep learning, reinforcement learning, time series analysis, model interpretability, Bayesian methods\n- üîÆ Materials Science & Chemistry - Crystal structure analysis, phase diagrams, metabolic modeling, computational chemistry\n- üåå Physics & Astronomy - Astronomical data analysis, coordinate transformations, cosmological calculations, symbolic mathematics, physics computations\n- ‚öôÔ∏è Engineering & Simulation - Discrete-event simulation, multi-objective optimization, metabolic engineering, systems modeling, process optimization\n- üìä Data Analysis & Visualization - Statistical analysis, network analysis, time series, publication-quality figures, large-scale data processing\n- üß™ Laboratory Automation - Liquid handling protocols, lab equipment control, workflow automation, LIMS integration\n- üìö Scientific Communication - Literature review, peer review, scientific writing, document processing, publication workflows\n- üî¨ Multi-omics & Systems Biology - Multi-modal data integration, pathway analysis, network biology, systems-level insights\n- üß¨ Protein Engineering & Design - Protein language models, structure prediction, sequence design, function annotation\n\n**Transform Claude Code into an 'AI Scientist' on your desktop!**\n\n> üíº For substantially more advanced capabilities, compute infrastructure, and enterprise-ready offerings, check out [k-dense.ai](https://k-dense.ai/).\n\n> ‚≠ê **If you find this repository useful**, please consider giving it a star! It helps others discover these tools and encourages us to continue maintaining and expanding this collection.\n\n---\n\n## üì¶ What's Included\n\nThis repository provides **118+ scientific skills** organized into the following categories:\n\n- **25+ Scientific Databases** - Direct API access to PubMed, ChEMBL, UniProt, COSMIC, ClinicalTrials.gov, and more\n- **50+ Python Packages** - RDKit, Scanpy, PyTorch Lightning, scikit-learn, BioPython, and others\n- **15+ Scientific Integrations** - Benchling, DNAnexus, LatchBio, OMERO, Protocols.io, and more\n- **20+ Analysis & Communication Tools** - Literature review, scientific writing, peer review, document processing\n\nEach skill includes:\n- ‚úÖ Comprehensive documentation (`SKILL.md`)\n- ‚úÖ Practical code examples\n- ‚úÖ Use cases and best practices\n- ‚úÖ Integration guides\n- ‚úÖ Reference materials\n\n---\n\n## üìã Table of Contents\n\n- [What's Included](#whats-included)\n- [Why Use This?](#why-use-this)\n- [Getting Started](#getting-started)\n  - [Claude Code](#claude-code-recommended)\n  - [Cursor IDE](#cursor-ide)\n  - [Any MCP Client](#any-mcp-client)\n- [Prerequisites](#prerequisites)\n- [Quick Examples](#quick-examples)\n- [Use Cases](#use-cases)\n- [Available Skills](#available-skills)\n- [Contributing](#contributing)\n- [Troubleshooting](#troubleshooting)\n- [FAQ](#faq)\n- [Support](#support)\n- [Join Our Community](#join-our-community)\n- [Citation](#citation)\n- [License](#license)\n\n---\n\n## üöÄ Why Use This?\n\n### ‚ö° **Accelerate Your Research**\n- **Save Days of Work** - Skip API documentation research and integration setup\n- **Production-Ready Code** - Tested, validated examples following scientific best practices\n- **Multi-Step Workflows** - Execute complex pipelines with a single prompt\n\n### üéØ **Comprehensive Coverage**\n- **117+ Skills** - Extensive coverage across all major scientific domains\n- **25+ Databases** - Direct access to PubMed, ChEMBL, UniProt, COSMIC, and more\n- **50+ Python Packages** - RDKit, Scanpy, PyTorch Lightning, scikit-learn, and others\n\n### üîß **Easy Integration**\n- **One-Click Setup** - Install via Claude Code or MCP server\n- **Au",
      "default_branch": "main"
    },
    "fetched_at": "2025-11-16T02:44:49.760768"
  },
  {
    "basic_info": {
      "name": "claude-code-prompt-improver",
      "full_name": "severity1/claude-code-prompt-improver",
      "owner": "severity1",
      "description": "Intelligent prompt improver hook for Claude Code. Type vibes, ship precision.",
      "url": "https://github.com/severity1/claude-code-prompt-improver",
      "clone_url": "https://github.com/severity1/claude-code-prompt-improver.git",
      "ssh_url": "git@github.com:severity1/claude-code-prompt-improver.git",
      "homepage": null,
      "created_at": "2025-10-18T03:03:21Z",
      "updated_at": "2025-11-16T02:43:46Z",
      "pushed_at": "2025-11-12T04:37:57Z"
    },
    "stats": {
      "stars": 857,
      "forks": 77,
      "watchers": 857,
      "open_issues": 1,
      "size": 3218
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 22730
      },
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "# Claude Code Prompt Improver\n\nA UserPromptSubmit hook that enriches vague prompts before Claude Code executes them. Uses the AskUserQuestion tool (Claude Code 2.0.22+) for targeted clarifying questions.\n\n![Demo](assets/demo.gif)\n\n## What It Does\n\nIntercepts prompts and evaluates clarity. Claude then:\n- Checks if the prompt is clear using conversation history\n- For clear prompts: proceeds immediately (zero overhead)\n- For vague prompts: invokes the `prompt-improver` skill to create research plan, gather context, and ask 1-6 grounded questions\n- Proceeds with original request using the clarification\n\n**Result:** Better outcomes on the first try, without back-and-forth.\n\n**v0.4.0 Update:** Skill-based architecture with hook-level evaluation achieves 31% token reduction. Clear prompts have zero skill overhead, vague prompts get comprehensive research and questioning via the skill.\n\n## How It Works\n\n```mermaid\nsequenceDiagram\n    participant User\n    participant Hook\n    participant Claude\n    participant Skill\n    participant Project\n\n    User->>Hook: \"fix the bug\"\n    Hook->>Claude: Evaluation prompt (~189 tokens)\n    Claude->>Claude: Evaluate using conversation history\n    alt Vague prompt\n        Claude->>Skill: Invoke prompt-improver skill\n        Skill-->>Claude: Research and question guidance\n        Claude->>Claude: Create research plan (TodoWrite)\n        Claude->>Project: Execute research (codebase, web, docs)\n        Project-->>Claude: Context\n        Claude->>User: Ask grounded questions (1-6)\n        User->>Claude: Answer\n        Claude->>Claude: Execute original request with answers\n    else Clear prompt\n        Claude->>Claude: Proceed immediately (no skill load)\n    end\n```\n\n## Installation\n\n**Requirements:** Claude Code 2.0.22+ (uses AskUserQuestion tool for targeted clarifying questions)\n\n### Option 1: Via Marketplace (Recommended)\n\n**1. Add the marketplace:**\n```bash\nclaude plugin marketplace add severity1/claude-code-marketplace\n```\n\n**2. Install the plugin:**\n```bash\nclaude plugin install prompt-improver@claude-code-marketplace\n```\n\n**3. Restart Claude Code**\n\nVerify installation with `/plugin` command. You should see the prompt-improver plugin listed.\n\n### Option 2: Local Plugin Installation (Recommended for Development)\n\n**1. Clone the repository:**\n```bash\ngit clone https://github.com/severity1/claude-code-prompt-improver.git\ncd claude-code-prompt-improver\n```\n\n**2. Add the local marketplace:**\n```bash\nclaude plugin marketplace add /absolute/path/to/claude-code-prompt-improver/.dev-marketplace/.claude-plugin/marketplace.json\n```\n\nReplace `/absolute/path/to/` with the actual path where you cloned the repository.\n\n**3. Install the plugin:**\n```bash\nclaude plugin install prompt-improver@local-dev\n```\n\n**4. Restart Claude Code**\n\nVerify installation with `/plugin` command. You should see \"1 plugin available, 1 already installed\".\n\n### Option 3: Manual Installation\n\n**1. Copy the hook:**\n```bash\ncp scripts/improve-prompt.py ~/.claude/hooks/\nchmod +x ~/.claude/hooks/improve-prompt.py\n```\n\n**2. Update `~/.claude/settings.json`:**\n```json\n{\n  \"hooks\": {\n    \"UserPromptSubmit\": [\n      {\n        \"hooks\": [\n          {\n            \"type\": \"command\",\n            \"command\": \"python3 ~/.claude/hooks/improve-prompt.py\"\n          }\n        ]\n      }\n    ]\n  }\n}\n```\n\n## Usage\n\n**Normal use:**\n```bash\nclaude \"fix the bug\"      # Hook evaluates, may ask questions\nclaude \"add tests\"        # Hook evaluates, may ask questions\n```\n\n**Bypass prefixes:**\n```bash\nclaude \"* add dark mode\"                    # * = skip evaluation\nclaude \"/help\"                              # / = slash commands bypass\nclaude \"# remember to use rg over grep\"     # # = memorize bypass\n```\n\n**Vague prompt:**\n```bash\n$ claude \"fix the error\"\n```\n\nClaude asks:\n```\nWhich error needs fixing?\n  ‚óã TypeError in src/components/Map.tsx (recent change)\n  ‚óã API timeout in src/services/osmService.ts\n  ‚óã Other (paste error message)\n```\n\nYou select an option, Claude proceeds with full context.\n\n**Clear prompt:**\n```bash\n$ claude \"Fix TypeError in src/components/Map.tsx line 127 where mapboxgl.Map constructor is missing container option\"\n```\n\nClaude proceeds immediately without questions.\n\n## Design Philosophy\n\n- **Rarely intervene** - Most prompts pass through unchanged\n- **Trust user intent** - Only ask when genuinely unclear\n- **Use conversation history** - Avoid redundant exploration\n- **Max 1-6 questions** - Enough for complex scenarios, still focused\n- **Transparent** - Evaluation visible in conversation\n\n## Architecture\n\n**v0.4.0:** Skill-based architecture with hook-level evaluation.\n\n**Hook (scripts/improve-prompt.py) - Evaluation Orchestrator:**\n- Intercepts via stdin/stdout JSON (~70 lines)\n- Handles bypass prefixes: `*`, `/`, `#`\n- Wraps prompts with evaluation instructions (~189 tokens)\n- Claude evaluates clarity using conversation history\n- If vague: Instructs Claude to invoke `prompt-improver` skill\n\n**Skill (skills/prompt-improver/)",
      "default_branch": "main"
    },
    "fetched_at": "2025-11-16T02:44:50.876161"
  },
  {
    "basic_info": {
      "name": "OpenAIglasses_for_Navigation",
      "full_name": "AI-FanGe/OpenAIglasses_for_Navigation",
      "owner": "AI-FanGe",
      "description": "a open framework for blind navigation based on esp32",
      "url": "https://github.com/AI-FanGe/OpenAIglasses_for_Navigation",
      "clone_url": "https://github.com/AI-FanGe/OpenAIglasses_for_Navigation.git",
      "ssh_url": "git@github.com:AI-FanGe/OpenAIglasses_for_Navigation.git",
      "homepage": null,
      "created_at": "2025-10-20T10:03:03Z",
      "updated_at": "2025-11-16T02:33:46Z",
      "pushed_at": "2025-11-01T06:47:27Z"
    },
    "stats": {
      "stars": 844,
      "forks": 247,
      "watchers": 844,
      "open_issues": 8,
      "size": 23596
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 530632,
        "JavaScript": 73288,
        "C++": 38358,
        "HTML": 9871,
        "Shell": 5462,
        "Batchfile": 4340,
        "CSS": 3712,
        "C": 1665,
        "Dockerfile": 1337
      },
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "# AI Êô∫ËÉΩÁõ≤‰∫∫ÁúºÈïúÁ≥ªÁªü ü§ñüëì\n\n<div align=\"center\">\n\n‰∏Ä‰∏™Èù¢ÂêëËßÜÈöú‰∫∫Â£´ÁöÑÊô∫ËÉΩÂØºËà™‰∏éËæÖÂä©Á≥ªÁªüÔºåÈõÜÊàê‰∫ÜÁõ≤ÈÅìÂØºËà™„ÄÅËøáÈ©¨Ë∑ØËæÖÂä©„ÄÅÁâ©ÂìÅËØÜÂà´„ÄÅÂÆûÊó∂ËØ≠Èü≥‰∫§‰∫íÁ≠âÂäüËÉΩ„ÄÇ  Êú¨È°πÁõÆ‰ªÖ‰∏∫‰∫§ÊµÅÂ≠¶‰π†‰ΩøÁî®ÔºåËØ∑ÂãøÁõ¥Êé•ÁªôËßÜÈöú‰∫∫Áæ§‰ΩøÁî®„ÄÇÊú¨È°πÁõÆÂÜÖ‰ªÖÂåÖÂê´‰ª£Á†ÅÔºåÊ®°ÂûãÂú∞ÂùÄÔºöhttps://www.modelscope.cn/models/archifancy/AIGlasses_for_navigation  „ÄÇ‰∏ãËΩΩÂêéÂ≠òÊîæÂú®/model Êñá‰ª∂Â§π\n\n[ÂäüËÉΩÁâπÊÄß](#ÂäüËÉΩÁâπÊÄß) ‚Ä¢ [Âø´ÈÄüÂºÄÂßã](#Âø´ÈÄüÂºÄÂßã) ‚Ä¢ [Á≥ªÁªüÊû∂ÊûÑ](#Á≥ªÁªüÊû∂ÊûÑ) ‚Ä¢ [‰ΩøÁî®ËØ¥Êòé](#‰ΩøÁî®ËØ¥Êòé) ‚Ä¢ [ÂºÄÂèëÊñáÊ°£](#ÂºÄÂèëÊñáÊ°£)\n\n</div>\n\n---\n<img width=\"2481\" height=\"3508\" alt=\"1\" src=\"https://github.com/user-attachments/assets/e8dec4a6-8fa6-4d94-bd66-4e9864b67daf\" />\n<img width=\"2480\" height=\"3508\" alt=\"2\" src=\"https://github.com/user-attachments/assets/bc7d1aac-a9e9-4ef8-9d67-224708d0c9fd\" />\n<img width=\"2481\" height=\"3508\" alt=\"4\" src=\"https://github.com/user-attachments/assets/6dd19750-57af-4560-a007-9a7059956b53\" />\n\n## üìã ÁõÆÂΩï\n\n- [ÂäüËÉΩÁâπÊÄß](#ÂäüËÉΩÁâπÊÄß)\n- [Á≥ªÁªüË¶ÅÊ±Ç](#Á≥ªÁªüË¶ÅÊ±Ç)\n- [Âø´ÈÄüÂºÄÂßã](#Âø´ÈÄüÂºÄÂßã)\n- [Á≥ªÁªüÊû∂ÊûÑ](#Á≥ªÁªüÊû∂ÊûÑ)\n- [‰ΩøÁî®ËØ¥Êòé](#‰ΩøÁî®ËØ¥Êòé)\n- [ÈÖçÁΩÆËØ¥Êòé](#ÈÖçÁΩÆËØ¥Êòé)\n- [ÂºÄÂèëÊñáÊ°£](#ÂºÄÂèëÊñáÊ°£)\n\n## ‚ú® ÂäüËÉΩÁâπÊÄß\n\n### üö∂ Áõ≤ÈÅìÂØºËà™Á≥ªÁªü\n- **ÂÆûÊó∂Áõ≤ÈÅìÊ£ÄÊµã**ÔºöÂü∫‰∫é YOLO ÂàÜÂâ≤Ê®°ÂûãÂÆûÊó∂ËØÜÂà´Áõ≤ÈÅì\n- **Êô∫ËÉΩËØ≠Èü≥ÂºïÂØº**ÔºöÊèê‰æõÁ≤æÂáÜÁöÑÊñπÂêëÊåáÂºïÔºàÂ∑¶ËΩ¨„ÄÅÂè≥ËΩ¨„ÄÅÁõ¥Ë°åÁ≠âÔºâ\n- **ÈöúÁ¢çÁâ©Ê£ÄÊµã‰∏éÈÅøÈöú**ÔºöËá™Âä®ËØÜÂà´ÂâçÊñπÈöúÁ¢çÁâ©Âπ∂ËßÑÂàíÈÅøÈöúË∑ØÁ∫ø\n- **ËΩ¨ÂºØÊ£ÄÊµã**ÔºöËá™Âä®ËØÜÂà´ÊÄ•ËΩ¨ÂºØÂπ∂ÊèêÂâçÊèêÈÜí\n- **ÂÖâÊµÅÁ®≥ÂÆö**Ôºö‰ΩøÁî® Lucas-Kanade ÂÖâÊµÅÁÆóÊ≥ïÁ®≥ÂÆöÊé©Á†ÅÔºåÂáèÂ∞ëÊäñÂä®\n\n### üö¶ ËøáÈ©¨Ë∑ØËæÖÂä©\n- **ÊñëÈ©¨Á∫øËØÜÂà´**ÔºöÂÆûÊó∂Ê£ÄÊµãÊñëÈ©¨Á∫ø‰ΩçÁΩÆÂíåÊñπÂêë\n- **Á∫¢ÁªøÁÅØËØÜÂà´**ÔºöÂü∫‰∫éÈ¢úËâ≤ÂíåÂΩ¢Áä∂ÁöÑÁ∫¢ÁªøÁÅØÁä∂ÊÄÅÊ£ÄÊµã\n- **ÂØπÈΩêÂºïÂØº**ÔºöÂºïÂØºÁî®Êà∑ÂØπÂáÜÊñëÈ©¨Á∫ø‰∏≠ÂøÉ\n- **ÂÆâÂÖ®ÊèêÈÜí**ÔºöÁªøÁÅØÊó∂ËØ≠Èü≥ÊèêÁ§∫ÂèØ‰ª•ÈÄöË°å\n\n### üîç Áâ©ÂìÅËØÜÂà´‰∏éÊü•Êâæ\n- **Êô∫ËÉΩÁâ©ÂìÅÊêúÁ¥¢**ÔºöËØ≠Èü≥Êåá‰ª§Êü•ÊâæÁâ©ÂìÅÔºàÂ¶Ç\"Â∏ÆÊàëÊâæ‰∏Ä‰∏ãÁ∫¢Áâõ\"Ôºâ\n- **ÂÆûÊó∂ÁõÆÊ†áËøΩË∏™**Ôºö‰ΩøÁî® YOLO-E ÂºÄÊîæËØçÊ±áÊ£ÄÊµã + ByteTrack ËøΩË∏™\n- **ÊâãÈÉ®ÂºïÂØº**ÔºöÁªìÂêà MediaPipe ÊâãÈÉ®Ê£ÄÊµãÔºåÂºïÂØºÁî®Êà∑ÊâãÈÉ®Èù†ËøëÁâ©ÂìÅ\n- **ÊäìÂèñÊ£ÄÊµã**ÔºöÊ£ÄÊµãÊâãÈÉ®Êè°ÊåÅÂä®‰ΩúÔºåÁ°ÆËÆ§Áâ©ÂìÅÂ∑≤ÊãøÂà∞\n- **Â§öÊ®°ÊÄÅÂèçÈ¶à**ÔºöËßÜËßâÊ†áÊ≥® + ËØ≠Èü≥ÂºïÂØº + Â±Ö‰∏≠ÊèêÁ§∫\n\n### üéôÔ∏è ÂÆûÊó∂ËØ≠Èü≥‰∫§‰∫í\n- **ËØ≠Èü≥ËØÜÂà´ÔºàASRÔºâ**ÔºöÂü∫‰∫éÈòøÈáå‰∫ë DashScope Paraformer ÂÆûÊó∂ËØ≠Èü≥ËØÜÂà´\n- **Â§öÊ®°ÊÄÅÂØπËØù**ÔºöQwen-Omni-Turbo ÊîØÊåÅÂõæÂÉè+ÊñáÊú¨ËæìÂÖ•ÔºåËØ≠Èü≥ËæìÂá∫\n- **Êô∫ËÉΩÊåá‰ª§Ëß£Êûê**ÔºöËá™Âä®ËØÜÂà´ÂØºËà™„ÄÅÊü•Êâæ„ÄÅÂØπËØùÁ≠â‰∏çÂêåÁ±ªÂûãÊåá‰ª§\n- **‰∏ä‰∏ãÊñáÊÑüÁü•**ÔºöÂú®‰∏çÂêåÊ®°Âºè‰∏ãÊô∫ËÉΩËøáÊª§Êó†ÂÖ≥Êåá‰ª§\n\n### üìπ ËßÜÈ¢ë‰∏éÈü≥È¢ëÂ§ÑÁêÜ\n- **ÂÆûÊó∂ËßÜÈ¢ëÊµÅ**ÔºöWebSocket Êé®ÊµÅÔºåÊîØÊåÅÂ§öÂÆ¢Êà∑Á´ØÂêåÊó∂ËßÇÁúã\n- **Èü≥ËßÜÈ¢ëÂêåÊ≠•ÂΩïÂà∂**ÔºöËá™Âä®‰øùÂ≠òÂ∏¶Êó∂Èó¥Êà≥ÁöÑÂΩïÂÉèÂíåÈü≥È¢ëÊñá‰ª∂\n- **IMU Êï∞ÊçÆËûçÂêà**ÔºöÊé•Êî∂ ESP32 ÁöÑ IMU Êï∞ÊçÆÔºåÊîØÊåÅÂßøÊÄÅ‰º∞ËÆ°\n- **Â§öË∑ØÈü≥È¢ëÊ∑∑Èü≥**ÔºöÊîØÊåÅÁ≥ªÁªüËØ≠Èü≥„ÄÅAI ÂõûÂ§ç„ÄÅÁéØÂ¢ÉÈü≥ÂêåÊó∂Êí≠Êîæ\n\n### üé® ÂèØËßÜÂåñ‰∏é‰∫§‰∫í\n- **Web ÂÆûÊó∂ÁõëÊéß**ÔºöÊµèËßàÂô®Á´ØÂÆûÊó∂Êü•ÁúãÂ§ÑÁêÜÂêéÁöÑËßÜÈ¢ëÊµÅ\n- **IMU 3D ÂèØËßÜÂåñ**ÔºöThree.js ÂÆûÊó∂Ê∏≤ÊüìËÆæÂ§áÂßøÊÄÅ\n- **Áä∂ÊÄÅÈù¢Êùø**ÔºöÊòæÁ§∫ÂØºËà™Áä∂ÊÄÅ„ÄÅÊ£ÄÊµã‰ø°ÊÅØ„ÄÅFPS Á≠â\n- **‰∏≠ÊñáÂèãÂ•Ω**ÔºöÊâÄÊúâÁïåÈù¢ÂíåËØ≠Èü≥‰ΩøÁî®‰∏≠ÊñáÔºåÊîØÊåÅËá™ÂÆö‰πâÂ≠ó‰Ωì\n\n## üíª Á≥ªÁªüË¶ÅÊ±Ç\n\n### Á°¨‰ª∂Ë¶ÅÊ±Ç\n- **ÂºÄÂèë/ÊúçÂä°Âô®Á´Ø**Ôºö\n  - CPU: Intel i5 Êàñ‰ª•‰∏äÔºàÊé®Ëçê i7/i9Ôºâ\n  - GPU: NVIDIA GPUÔºàÊîØÊåÅ CUDA 11.8+ÔºåÊé®Ëçê RTX 3060 Êàñ‰ª•‰∏äÔºâ\n  - ÂÜÖÂ≠ò: 8GB RAMÔºàÊé®Ëçê 16GBÔºâ\n  - Â≠òÂÇ®: 10GB ÂèØÁî®Á©∫Èó¥\n\n- **ÂÆ¢Êà∑Á´ØËÆæÂ§á**ÔºàÂèØÈÄâÔºâÔºö\n  - ESP32-CAM ÊàñÂÖ∂‰ªñÊîØÊåÅ WebSocket ÁöÑÊëÑÂÉèÂ§¥\n  - È∫¶ÂÖãÈ£éÔºàÁî®‰∫éËØ≠Èü≥ËæìÂÖ•Ôºâ\n  - Êâ¨Â£∞Âô®/ËÄ≥Êú∫ÔºàÁî®‰∫éËØ≠Èü≥ËæìÂá∫Ôºâ\n\n### ËΩØ‰ª∂Ë¶ÅÊ±Ç\n- **Êìç‰ΩúÁ≥ªÁªü**: Windows 10/11, Linux (Ubuntu 20.04+), macOS 10.15+\n- **Python**: 3.9 - 3.11\n- **CUDA**: 11.8 ÊàñÊõ¥È´òÁâàÊú¨ÔºàGPU Âä†ÈÄüÂøÖÈúÄÔºâ\n- **ÊµèËßàÂô®**: Chrome 90+, Firefox 88+, Edge 90+ÔºàÁî®‰∫é Web ÁõëÊéßÔºâ\n\n### API ÂØÜÈí•\n- **ÈòøÈáå‰∫ë DashScope API Key**ÔºàÂøÖÈúÄÔºâÔºö\n  - Áî®‰∫éËØ≠Èü≥ËØÜÂà´ÔºàASRÔºâÂíå Qwen-Omni ÂØπËØù\n  - Áî≥ËØ∑Âú∞ÂùÄÔºöhttps://dashscope.console.aliyun.com/\n\n## üöÄ Âø´ÈÄüÂºÄÂßã\n\n### 1. ÂÖãÈöÜÈ°πÁõÆ\n\n```bash\ngit clone https://github.com/yourusername/aiglass.git\ncd aiglass/rebuild1002\n```\n\n### 2. ÂÆâË£Ö‰æùËµñ\n\n#### ÂàõÂª∫ËôöÊãüÁéØÂ¢ÉÔºàÊé®ËçêÔºâ\n```bash\npython -m venv venv\n# Windows\nvenv\\Scripts\\activate\n# Linux/macOS\nsource venv/bin/activate\n```\n\n#### ÂÆâË£Ö Python ÂåÖ\n```bash\npip install -r requirements.txt\n```\n\n#### ÂÆâË£Ö CUDA Âíå cuDNNÔºàGPU Âä†ÈÄüÔºâ\nËØ∑ÂèÇËÄÉ [NVIDIA CUDA Toolkit ÂÆâË£ÖÊåáÂçó](https://developer.nvidia.com/cuda-downloads)\n\n### 3. ‰∏ãËΩΩÊ®°ÂûãÊñá‰ª∂\n\nÂ∞Ü‰ª•‰∏ãÊ®°ÂûãÊñá‰ª∂ÊîæÂÖ• `model/` ÁõÆÂΩïÔºö\n\n| Ê®°ÂûãÊñá‰ª∂ | Áî®ÈÄî | Â§ßÂ∞è | ‰∏ãËΩΩÈìæÊé• |\n|---------|------|------|---------|\n| `yolo-seg.pt` | Áõ≤ÈÅìÂàÜÂâ≤ | ~50MB | [ÂæÖË°•ÂÖÖ] |\n| `yoloe-11l-seg.pt` | ÂºÄÊîæËØçÊ±áÊ£ÄÊµã | ~80MB | [ÂæÖË°•ÂÖÖ] |\n| `shoppingbest5.pt` | Áâ©ÂìÅËØÜÂà´ | ~30MB | [ÂæÖË°•ÂÖÖ] |\n| `trafficlight.pt` | Á∫¢ÁªøÁÅØÊ£ÄÊµã | ~20MB | [ÂæÖË°•ÂÖÖ] |\n| `hand_landmarker.task` | ÊâãÈÉ®Ê£ÄÊµã | ~15MB | [MediaPipe Models](https://developers.google.com/mediapipe/solutions/vision/hand_landmarker#models) |\n\n### 4. ÈÖçÁΩÆ API ÂØÜÈí•\n\nÂàõÂª∫ `.env` Êñá‰ª∂Ôºö\n\n```bash\n# .env\nDASHSCOPE_API_KEY=your_api_key_here\n```\n\nÊàñÂú®‰ª£Á†Å‰∏≠Áõ¥Êé•‰øÆÊîπÔºà‰∏çÊé®ËçêÔºâÔºö\n```python\n# app_main.py, line 50\nAPI_KEY = \"your_api_key_here\"\n```\n\n### 5. ÂêØÂä®Á≥ªÁªü\n\n```bash\npython app_main.py\n```\n\nÁ≥ªÁªüÂ∞ÜÂú® `http://0.0.0.0:8081` ÂêØÂä®ÔºåÊâìÂºÄÊµèËßàÂô®ËÆøÈóÆÂç≥ÂèØÁúãÂà∞ÂÆûÊó∂ÁõëÊéßÁïåÈù¢„ÄÇ\n\n### 6. ËøûÊé•ËÆæÂ§áÔºàÂèØÈÄâÔºâ\n\nÂ¶ÇÊûú‰ΩøÁî® ESP32-CAMÔºåËØ∑Ôºö\n1. ÁÉßÂΩï `compile/compile.ino` Âà∞ ESP32\n2. ‰øÆÊîπ WiFi ÈÖçÁΩÆÔºåËøûÊé•Âà∞Âêå‰∏ÄÁΩëÁªú\n3. ESP32 Ëá™Âä®ËøûÊé•Âà∞ WebSocket Á´ØÁÇπ\n\n## üèóÔ∏è Á≥ªÁªüÊû∂ÊûÑ\n\n### Êï¥‰ΩìÊû∂ÊûÑ\n\n```\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ                        ÂÆ¢Êà∑Á´ØÂ±Ç                              ‚îÇ\n‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îÇ\n‚îÇ  ‚îÇ  ESP32-CAM   ‚îÇ  ‚îÇ   ÊµèËßàÂô®      ‚îÇ  ‚îÇ   ÁßªÂä®Á´Ø      ‚îÇ      ‚îÇ\n‚îÇ  ‚îÇ  (ËßÜÈ¢ë/Èü≥È¢ë)  ‚îÇ  ‚îÇ  (ÁõëÊéßÁïåÈù¢)   ‚îÇ  ‚îÇ  (ËØ≠Èü≥ÊéßÂà∂)   ‚îÇ      ‚îÇ\n‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n          ‚îÇ WebSocket        ‚îÇ HTTP/WS          ‚îÇ WebSocket\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ         ‚îÇ                  ‚îÇ                  ‚îÇ              ‚îÇ\n‚îÇ    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ\n‚îÇ    ‚îÇ         FastAPI ‰∏ªÊúçÂä° (app_main.py)              ‚îÇ    ‚îÇ\n‚îÇ    ‚îÇ  - WebSocket Ë∑ØÁî±ÁÆ°ÁêÜ                              ‚îÇ    ‚îÇ\n‚îÇ    ‚îÇ  - Èü≥ËßÜÈ¢ëÊµÅÂàÜÂèë                                     ‚îÇ    ‚îÇ\n‚îÇ    ‚îÇ  - Áä∂ÊÄÅÁÆ°ÁêÜ‰∏éÂçèË∞É                                   ‚îÇ    ‚îÇ\n‚îÇ    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ\n‚îÇ         ‚îÇ                ‚îÇ                ‚îÇ                  ‚îÇ\n‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îÇ\n‚îÇ  ‚îÇ ASR Ê®°Âùó     ‚îÇ  ‚îÇ Omni ÂØπËØù   ‚îÇ  ‚îÇ Èü≥È¢ëÊí≠Êîæ     ‚îÇ         ‚îÇ\n‚îÇ  ‚îÇ (asr_core)   ‚îÇ  ‚îÇ(omni_client)‚îÇ  ‚îÇ(audio_player)‚îÇ         ‚îÇ\n‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îÇ\n‚îÇ                                                               ‚îÇ\n‚îÇ         Â∫îÁî®Â±Ç                         ",
      "default_branch": "main"
    },
    "fetched_at": "2025-11-16T02:44:52.012259"
  },
  {
    "basic_info": {
      "name": "tiny8",
      "full_name": "sql-hkr/tiny8",
      "owner": "sql-hkr",
      "description": "A tiny CPU simulator written in Python",
      "url": "https://github.com/sql-hkr/tiny8",
      "clone_url": "https://github.com/sql-hkr/tiny8.git",
      "ssh_url": "git@github.com:sql-hkr/tiny8.git",
      "homepage": "https://sql-hkr.github.io/tiny8/",
      "created_at": "2025-10-20T16:28:30Z",
      "updated_at": "2025-11-15T05:37:25Z",
      "pushed_at": "2025-11-12T15:20:02Z"
    },
    "stats": {
      "stars": 843,
      "forks": 17,
      "watchers": 843,
      "open_issues": 5,
      "size": 1687
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 225950,
        "Dockerfile": 332
      },
      "license": "MIT License",
      "topics": [
        "8-bit-computer",
        "assembler",
        "visualization"
      ]
    },
    "content": {
      "readme": "# Tiny8\n\n[![PyPI version](https://img.shields.io/pypi/v/tiny8)](https://pypi.org/project/tiny8/)\n[![License](https://img.shields.io/github/license/sql-hkr/tiny8)](LICENSE)\n[![Python versions](https://img.shields.io/pypi/pyversions/tiny8)](https://pypi.org/project/tiny8/)\n[![CI](https://github.com/sql-hkr/tiny8/actions/workflows/ci.yml/badge.svg)](https://github.com/sql-hkr/tiny8/actions/workflows/ci.yml)\n[![codecov](https://codecov.io/github/sql-hkr/tiny8/graph/badge.svg?token=OBM58R8MCL)](https://codecov.io/github/sql-hkr/tiny8)\n\n> **An educational 8-bit CPU simulator with interactive visualization**\n\nTiny8 is a lightweight and educational toolkit for exploring the fundamentals of computer architecture through hands-on assembly programming and real-time visualization. Designed for learning and experimentation, it features an AVR-inspired 8-bit CPU with 32 registers, a rich instruction set, and powerful debugging tools ‚Äî all with zero heavy dependencies.\n\n\n<div align=\"center\">\n  <img src=\"https://github.com/user-attachments/assets/ffbcb2c4-2c3a-469f-b7b7-e6e86eb374da\" alt=\"Animated bubble sort visualization\" width=\"600\">\n  <p><em>Real-time visualization of a bubble sort algorithm executing on Tiny8</em></p>\n</div>\n\n## ‚ú® Features\n\n### üéØ **Interactive Terminal Debugger**\n<img width=\"600\" src=\"https://github.com/user-attachments/assets/0bbd4382-806e-4b5a-af0b-54d83417fcfb\" alt=\"CLI visualizer screenshot\">\n\n- **Vim-style navigation**: Step through execution with intuitive keyboard controls\n- **Change highlighting**: See exactly what changed at each step (registers, flags, memory)\n- **Advanced search**: Find instructions, track register/memory changes, locate PC addresses\n- **Marks and bookmarks**: Set and jump to important execution points\n- **Vertical scrolling**: Handle programs with large memory footprints\n\n### üé¨ **Graphical Animation**\n- Generate high-quality GIF/MP4 videos of program execution\n- Visualize register evolution, memory access patterns, and flag changes\n- Perfect for presentations, documentation, and learning materials\n\n### üèóÔ∏è **Complete 8-bit Architecture**\n- **32 general-purpose registers** (R0-R31)\n- **8-bit ALU** with arithmetic, logical, and bit manipulation operations\n- **Status register (SREG)** with 8 condition flags\n- **2KB address space** for unified memory and I/O\n- **Stack operations** with dedicated stack pointer\n- **AVR-inspired instruction set** with 60+ instructions\n\n### üìö **Educational Focus**\n- Clean, readable Python implementation\n- Comprehensive examples (Fibonacci, bubble sort, factorial, and more)\n- Step-by-step execution traces for debugging\n- Full API documentation and instruction set reference\n\n## üöÄ Quick Start\n\n### Installation\n\n```bash\npip install tiny8\n```\n\n### Your First Program\n\nCreate `fibonacci.asm`:\n```asm\n; Fibonacci Sequence Calculator\n; Calculates the 10th Fibonacci number (F(10) = 55)\n; F(0) = 0, F(1) = 1, F(n) = F(n-1) + F(n-2)\n;\n; Results stored in registers:\n; R16 and R17 hold the two most recent Fibonacci numbers\n\n    ldi r16, 0          ; F(0) = 0\n    ldi r17, 1          ; F(1) = 1\n    ldi r18, 9          ; Counter: 9 more iterations to reach F(10)\n\nloop:\n    add r16, r17        ; F(n) = F(n-1) + F(n-2)\n    mov r19, r16        ; Save result temporarily\n    mov r16, r17        ; Shift: previous = current\n    mov r17, r19        ; Shift: current = new result\n    dec r18             ; Decrement counter\n    brne loop           ; Continue if counter != 0\n\ndone:\n    jmp done            ; Infinite loop at end\n```\n\nRun it:\n```bash\ntiny8 fibonacci.asm # Interactive debugger\ntiny8 fibonacci.asm -m ani -o fibonacci.gif # Generate animation\n```\n\n### Python API\n\n```python\nfrom tiny8 import CPU, assemble_file\n\nasm = assemble_file(\"fibonacci.asm\")\ncpu = CPU()\ncpu.load_program(asm)\ncpu.run(max_steps=1000)\n\nprint(f\"Result: R17 = {cpu.read_reg(17)}\")  # Final Fibonacci number\n```\n\n## üí° Why Tiny8?\n\n**For Students** ‚Äî Write assembly, see immediate results with visual feedback. Understand how each instruction affects CPU state without abstractions.\n\n**For Educators** ‚Äî Interactive demonstrations, easy assignment creation, and generate animations for lectures.\n\n**For Hobbyists** ‚Äî Rapid algorithm prototyping at the hardware level with minimal overhead and an extensible, readable codebase.\n\n## üìñ Documentation\n\n- [**Full Documentation**](https://sql-hkr.github.io/tiny8/) ‚Äî Complete API reference and guides\n- [**Instruction Set Reference**](#instruction-set-reference) ‚Äî All 60+ instructions\n- [**CLI Guide**](#interactive-cli-controls) ‚Äî Terminal debugger keyboard shortcuts\n- [**Examples**](#examples) ‚Äî Sample programs with explanations\n- [**Contributing**](CONTRIBUTING.md) ‚Äî Guidelines for contributors\n\n## üéÆ Interactive CLI Controls\n\nThe terminal-based debugger provides powerful navigation and inspection capabilities.\n\n### Navigation & Playback\n\n- `l` / `h` or `‚Üí` / `‚Üê` ‚Äî Step forward/backward\n- `w` / `b` ‚Äî Jump ¬±10 steps\n- `0` / `$` ‚Äî Jump to first/last step\n- `Space` ‚Äî Play/p",
      "default_branch": "main"
    },
    "fetched_at": "2025-11-16T02:44:53.138675"
  },
  {
    "basic_info": {
      "name": "AITradeGame",
      "full_name": "chadyi/AITradeGame",
      "owner": "chadyi",
      "description": null,
      "url": "https://github.com/chadyi/AITradeGame",
      "clone_url": "https://github.com/chadyi/AITradeGame.git",
      "ssh_url": "git@github.com:chadyi/AITradeGame.git",
      "homepage": "",
      "created_at": "2025-10-20T07:23:51Z",
      "updated_at": "2025-11-16T02:37:18Z",
      "pushed_at": "2025-11-03T09:07:01Z"
    },
    "stats": {
      "stars": 824,
      "forks": 251,
      "watchers": 824,
      "open_issues": 1,
      "size": 21963
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 68175,
        "JavaScript": 40008,
        "CSS": 15588,
        "HTML": 15410,
        "Dockerfile": 161
      },
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "# AITradeGame - Open Source AI Trading Simulator\n\n[English](README.md) | [‰∏≠Êñá](README_ZH.md)\n\n[![Python 3.9+](https://img.shields.io/badge/python-3.9+-blue.svg)](https://www.python.org/downloads/)\n[![Flask](https://img.shields.io/badge/flask-3.0+-green.svg)](https://flask.palletsprojects.com/)\n[![License](https://img.shields.io/badge/license-MIT-blue.svg)](LICENSE)\n\nAITradeGame is an AI trading simulator that supports both local and online versions.\n\nProvides an online version with interactive features and leaderboards.\n\nLocal version stores all data on your computer, no cloud storage, no tracking.\n\nIncludes a Windows one-click standalone executable that runs without installation.\n\n## Features\n\n### Desktop Version (Local)\n\nAI-driven trading strategies based on large language models, compatible with OpenAI, DeepSeek, Claude, and other models. Leveraged portfolio management with ECharts visualizations. 100% privacy with all data stored in local database. Trading fee configuration supported to simulate real trading environment.\n\n**Latest Features:**\n- API Provider Management: Unified management of multiple AI service provider API configurations\n- Smart Model Selection: Automatically fetch available model lists for each provider\n- Aggregated View: View aggregated assets and performance comparison across all models\n- System Settings: Configurable trading frequency and fee rates\n\n### Online Version (Public)\n\nLeaderboard functionality to compete with AI enthusiasts worldwide. Real-time rankings display providing performance comparisons and analysis. Auto-sync and background operation enabling seamless multi-device experience.\n\n## Quick Start\n\n### Try Online Version\n\nLaunch the online version at https://aitradegame.com without any installation.\n\n### Desktop Version\n\nDownload AITradeGame.exe from GitHub releases. Double-click the executable to run. The interface will open automatically. Start adding AI models and begin trading.\n\nAlternatively, clone the repository from GitHub. Install dependencies with pip install -r requirements.txt. Run the application with python app.py and visit http://localhost:5000.\n\n### Docker Deployment\n\nYou can also run AITradeGame using Docker:\n\n**Using docker-compose (recommended):**\n```bash\n# Build and start the container\ndocker-compose up -d\n\n# Access the application at http://localhost:5000\n```\n\n**Using docker directly:**\n```bash\n# Build the image\ndocker build -t aitradegame .\n\n# Run the container\ndocker run -d -p 5000:5000 -v $(pwd)/data:/app/data aitradegame\n\n# Access the application at http://localhost:5000\n```\n\nThe data directory will be created automatically to store the SQLite database. To stop the container, run `docker-compose down`.\n\n## Configuration\n\n### API Provider Setup\nFirst, add AI service providers:\n1. Click the \"API Provider\" button\n2. Enter provider name, API URL, and API key\n3. Manually input available models or click \"Fetch Models\" to auto-fetch\n4. Click save to complete configuration\n\n### Adding Trading Models\nAfter configuring providers, add trading models:\n1. Click the \"Add Model\" button\n2. Select a configured API provider\n3. Choose a specific model from the dropdown\n4. Enter display name and initial capital\n5. Click submit to start trading\n\n### System Settings\nClick the \"Settings\" button to configure:\n- Trading Frequency: Control AI decision interval (1-1440 minutes)\n- Trading Fee Rate: Commission rate per trade (default 0.1%)\n\n## Supported AI Models\n\nSupports all OpenAI-compatible APIs. This includes OpenAI models like gpt-4 and gpt-3.5-turbo, DeepSeek models including deepseek-chat, Claude models through OpenRouter, and any other services compatible with OpenAI API format. More protocols are being added.\n\n## Usage\n\nStart the server by running AITradeGame.exe or python app.py. Add AI model configuration through the web interface at http://localhost:5000. The system automatically begins trading simulation based on your configuration. Trading fees are charged for each open and close position according to the set rate, ensuring AI strategies operate under realistic cost conditions.\n\n## Privacy and Security\n\nAll data is stored in the AITradeGame.db SQLite file in the same directory as the executable. No external servers are contacted except your specified AI API endpoints. No user accounts or login required - everything runs locally.\n\n## Development\n\nDevelopment requires Python 3.9 or later. Internet connection is needed for market data and AI API calls.\n\nInstall all dependencies with: pip install -r requirements.txt\n\n## Contributing\n\nCommunity contributions are welcome.\n\n## Disclaimer\n\nThis is a simulated trading platform for testing AI models and strategies. It is not real trading and no actual money is involved. Always conduct your own research and analysis before making investment decisions. No warranties are provided regarding trading outcomes or AI performance.\n\n## Links\n\nOnline version with leaderboard and social features: https://aitradegame.com\n\nDesktop bui",
      "default_branch": "main"
    },
    "fetched_at": "2025-11-16T02:44:54.246255"
  },
  {
    "basic_info": {
      "name": "xiaomi-miloco",
      "full_name": "XiaoMi/xiaomi-miloco",
      "owner": "XiaoMi",
      "description": "Xiaomi Miloco",
      "url": "https://github.com/XiaoMi/xiaomi-miloco",
      "clone_url": "https://github.com/XiaoMi/xiaomi-miloco.git",
      "ssh_url": "git@github.com:XiaoMi/xiaomi-miloco.git",
      "homepage": null,
      "created_at": "2025-11-06T13:01:59Z",
      "updated_at": "2025-11-16T02:44:33Z",
      "pushed_at": "2025-11-15T09:33:33Z"
    },
    "stats": {
      "stars": 758,
      "forks": 37,
      "watchers": 758,
      "open_issues": 23,
      "size": 12471
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 1053500,
        "JavaScript": 506415,
        "Shell": 86880,
        "C++": 83167,
        "Less": 71674,
        "HTML": 54025,
        "CSS": 9737,
        "Dockerfile": 5505,
        "C": 2948,
        "CMake": 1813
      },
      "license": "Other",
      "topics": []
    },
    "content": {
      "readme": "# Xiaomi Miloco\n\n**Xiaomi Local Copilot** is a future exploration solution for smart homes. Using Xiaomi Home cameras as the source of visual information and a self-developed LLM as its core, it connects all IoT devices throughout the house. Based on the development paradigm of LLM, it enables users to define various family needs and rules in natural language, achieving broader and more creative smart device integration.\n\n<div align=\"center\">\n\nEnglish | [ÁÆÄ‰Ωì‰∏≠Êñá](README_zh_Hans.md)\n\n</div>\n\n## News\n\n- [2025-11] Xiaomi Miloco Framework Open Source\n\n## Key Features\n\n1. New Interaction Paradigm: Based on the development paradigm of LLM, rule-setting and complex device command control can be completed through natural language interaction.\n2. New Use for Visual Data: Using camera data streams as a source of perceptual information, the LLM is used to analyze various home scene events contained in the visual data to respond to user queries.\n3. On-Device LLM: The home scene tasks are split into two stages: planning and visual understanding. It provides Xiaomi's self-developed on-device model to realize on-device video understanding and ensure family privacy and security.\n4. Xiaomi Home Ecosystem: It connects with the Xiaomi Home ecosystem, supports the retrieval and execution of Mi Home devices and scenes, and supports sending customized content for Xiao Home notifications.\n\n    <img src=\"assets/images/ai_center.jpg\" width=\"60%\" />\n\n## Quick Start\n\n### System Requirements\n\n- **Hardware Requirements**\n```Plain Text\nCPU: x64 architecture\nGraphics Card: NVIDIA 30 series and above, 8GB VRAM minimum (recommended 12GB and above)\nStorage: Recommended 16GB or more available space (for local model storage)\n```\n\n- **Software Requirements**\n```Plain Text\nOperating System:\n  - Linux: x64 architecture, recommended Ubuntu 22.04 and above LTS versions\n  - Windows: x64 architecture, recommended Windows 10 and above, requires WSL2 support\n  - macOS: Not currently supported\nDocker: Version 20.10 and above, requires docker compose support\nNVIDIA Driver: NVIDIA driver with CUDA support\nNVIDIA Container Toolkit: For Docker GPU support\n```\n\n### Install\n\n> **Note**: Please ensure your system meets the above hardware and software requirements. Windows systems need to enter the WSL environment.\n\n**Install with Docker**  \nOne-click installation via command line\n```bash\nbash -c \"$(wget -qO- https://xiaomi-miloco.cnbj1.mi-fds.com/xiaomi-miloco/install.sh)\"\n```\nOr download the source code first, then execute the one-click installation script:\n```bash\ngit clone https://github.com/XiaoMi/xiaomi-miloco.git\n\nbash scripts/install.sh\n```\nFor detailed installation steps, please refer to the [Docker Deployment Documentation](docs/environment-setup.md).\n\n**Install with source code**  \nFor source code installation steps, please refer to the [Development Guide](docs/development/developer-setup.md).\n\n## Usage Documentation\n\nPlease refer to the [Usage Documentation](docs/usage/README.md).\n\n## Contributing\n\nPlease refer to the [Contributing Guide](CONTRIBUTING.md).\n\n## License\n\nFor license details, please see [LICENSE.md](LICENSE.md).\n\n**Important Notice**: This project is limited to non-commercial use only. Without written authorization from Xiaomi Corporation, this project may not be used for developing applications, web services, or other forms of software.\n\n## Security Issues\n\nIf you discover potential security issues in this project, or believe you may have found a security issue, please notify the [Miloco Team](xiaomi-miloco@xiaomi.com) via our vulnerability reporting email. Please do not create public GitHub Issues.\n\n## Contact Us\n\n### Issue Reporting\n\nFor issue reporting, please participate through the following methods:\n- Submit a [GitHub Issue](https://github.com/XiaoMi/xiaomi-miloco/issues/new/)\n\n### Technical Discussion\n\n- GitHub [Discussions](https://github.com/XiaoMi/xiaomi-miloco/discussions/)\n- Project Discussion Group (WeChat):\n\n  <img src=\"assets/images/wechat_11.jpg\" width=\"30%\" />  <img src=\"assets/images/wechat_8.jpg\" width=\"30%\" />  <img src=\"assets/images/wechat_4.jpg\" width=\"30%\" />\n\n\n### Join Us\n\nThe **Xiaomi Miloco** team is hiring. Send your resume to `xiaomi-miloco@xiaomi.com`, and it will be delivered directly to the project lead.\n\n## Acknowledgments\n\nThank you to the original team members who worked hard for MilocoÔºözhaoy„ÄÅyangyongjie„ÄÅxx„ÄÅChangyu„ÄÅyyk„ÄÅjunhui„ÄÅÈÉ≠ÂÖ¥ÂÆù„ÄÅ47„ÄÅafei„ÄÇ\n\nYour passion and talent are the fundamental driving force behind Miloco's continuous innovation and progress.\n\nSpecial thanks to:\n- The [llama.cpp](https://github.com/ggml-org/llama.cpp) open source project for providing inference backend capabilities\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-11-16T02:44:55.369841"
  },
  {
    "basic_info": {
      "name": "mcp_agent_mail",
      "full_name": "Dicklesworthstone/mcp_agent_mail",
      "owner": "Dicklesworthstone",
      "description": "Like gmail for your coding agents. Lets various different agents communicate and coordinate with each other.",
      "url": "https://github.com/Dicklesworthstone/mcp_agent_mail",
      "clone_url": "https://github.com/Dicklesworthstone/mcp_agent_mail.git",
      "ssh_url": "git@github.com:Dicklesworthstone/mcp_agent_mail.git",
      "homepage": null,
      "created_at": "2025-10-23T03:42:36Z",
      "updated_at": "2025-11-15T21:52:55Z",
      "pushed_at": "2025-11-14T05:49:28Z"
    },
    "stats": {
      "stars": 729,
      "forks": 66,
      "watchers": 729,
      "open_issues": 0,
      "size": 9509
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 1335823,
        "HTML": 528061,
        "Shell": 97357,
        "JavaScript": 73976,
        "CSS": 10029,
        "Dockerfile": 2870,
        "Makefile": 434
      },
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "# MCP Agent Mail\n\n![Agent Mail Showcase](screenshots/output/agent_mail_showcase.gif)\n\n> \"It's like gmail for your coding agents!\"\n\nA mail-like coordination layer for coding agents, exposed as an HTTP-only FastMCP server. It gives agents memorable identities, an inbox/outbox, searchable message history, and voluntary file reservation \"leases\" to avoid stepping on each other.\n\nThink of it as asynchronous email + directory + change-intent signaling for your agents, backed by Git (for human-auditable artifacts) and SQLite (for indexing and queries).\n\nStatus: Under active development. The design is captured in detail in `project_idea_and_guide.md` (start with the original prompt at the top of that file).\n\n## Why this exists\n\nModern projects often run multiple coding agents at once (backend, frontend, scripts, infra). Without a shared coordination fabric, agents:\n\n- Overwrite each other's edits or panic on unexpected diffs\n- Miss critical context from parallel workstreams\n- Require humans to \"liaison\" messages across tools and teams\n\nThis project provides a lightweight, interoperable layer so agents can:\n\n- Register a temporary-but-persistent identity (e.g., GreenCastle)\n- Send/receive GitHub-Flavored Markdown messages with images\n- Search, summarize, and thread conversations\n- Declare advisory file reservations (leases) on files/globs to signal intent\n- Inspect a directory of active agents, programs/models, and activity\n\nIt's designed for: FastMCP clients and CLI tools (Claude Code, Codex, Gemini CLI, etc.) coordinating across one or more codebases.\n\n## From Idea Spark to Shipping Swarm\n\nIf a blank repo feels daunting, follow the field-tested workflow we documented in `project_idea_and_guide.md` (‚ÄúAppendix: From Blank Repo to Coordinated Swarm‚Äù):\n\n- **Ideate fast:** Write a scrappy email-style blurb about the problem, desired UX, and any must-have stack picks (‚âà15 minutes).\n- **Promote it to a plan:** Feed that blurb to GPT-5 Pro (and optionally Grok4 Heavy / Opus 4.1) until you get a granular Markdown plan, then iterate on the plan file while it‚Äôs still cheap to change. The Markdown Web Browser sample plan shows the level of detail to aim for.\n- **Codify the rules:** Clone a tuned `AGENTS.md`, add any tech-specific best-practice guides, and let Codex scaffold the repo plus Beads tasks straight from the plan.\n- **Spin up the swarm:** Launch multiple Codex panes (or any agent mix), register each identity with Agent Mail, and have them acknowledge `AGENTS.md`, the plan document, and the Beads backlog before touching code.\n- **Keep everyone fed:** Reuse the canned instruction cadence from the tweet thread or, better yet, let the commercial Companion app‚Äôs Message Stacks broadcast those prompts automatically so you never hand-feed panes again.\n\nWatch the full 23-minute walkthrough (https://youtu.be/68VVcqMEDrs?si=pCm6AiJAndtZ6u7q) to see the loop in action.\n\n## Productivity Math & Automation Loop\n\nOne disciplined hour of GPT-5 Codex‚Äîwhen it isn‚Äôt waiting on human prompts‚Äîoften produces 10‚Äì20 ‚Äúhuman hours‚Äù of work because the agents reason and type at machine speed. Agent Mail multiplies that advantage in two layers:\n\n1. **Base OSS server:** Git-backed mailboxes, advisory file reservations, Typer CLI helpers, and searchable archives keep independent agents aligned without babysitting. Every instruction, lease, and attachment is auditable.\n2. **Companion stack (commercial):** The iOS app + host automation can provision, pair, and steer heterogeneous fleets (Claude Code, Codex, Gemini CLI, etc.) from your phone using customizable Message Stacks, Human Overseer broadcasts, Beads awareness, and plan editing tools‚Äîno manual tmux choreography required. The automation closes the loop by scheduling prompts, honoring Limited Mode, and enforcing Double-Arm confirmations for destructive work.\n\nResult: you invest 1‚Äì2 hours of human supervision, but dozens of agent-hours execute in parallel with clear audit trails and conflict-avoidance baked in.\n\n## TLDR Quickstart\n\n### One-line installer\n\n```bash\ncurl -fsSL https://raw.githubusercontent.com/Dicklesworthstone/mcp_agent_mail/main/scripts/install.sh | bash -s -- --yes\n```\n\nWhat this does:\n\n- Installs uv if missing and updates your PATH for this session\n- Creates a Python 3.14 virtual environment and installs dependencies with uv\n- Runs the auto-detect integration to wire up supported agent tools\n- Starts the MCP HTTP server on port 8765 and prints a masked bearer token\n- Creates helper scripts under `scripts/` (including `run_server_with_token.sh`)\n\nPrefer a specific location or options? Add flags like `--dir <path>`, `--project-dir <path>`, `--no-start`, `--start-only`, `--port <number>`, or `--token <hex>`.\n\n**Port conflicts?** Use `--port` to specify a different port (default: 8765):\n\n```bash\n# Install with custom port\ncurl -fsSL https://raw.githubusercontent.com/Dicklesworthstone/mcp_agent_mail/main/scripts/install.sh | bash -s -- --port 9000 --yes\n\n# Or use the CLI command af",
      "default_branch": "main"
    },
    "fetched_at": "2025-11-16T02:44:56.491404"
  },
  {
    "basic_info": {
      "name": "ds",
      "full_name": "huojichuanqi/ds",
      "owner": "huojichuanqi",
      "description": "Áî®deepseekÂÅö‰∫§ÊòìÔºàÊêûÁùÄÁé©ÔºåÊ≥®ÊÑèÈ£éÈô©Ôºâ",
      "url": "https://github.com/huojichuanqi/ds",
      "clone_url": "https://github.com/huojichuanqi/ds.git",
      "ssh_url": "git@github.com:huojichuanqi/ds.git",
      "homepage": null,
      "created_at": "2025-10-20T07:05:47Z",
      "updated_at": "2025-11-15T19:47:00Z",
      "pushed_at": "2025-11-01T10:45:20Z"
    },
    "stats": {
      "stars": 726,
      "forks": 404,
      "watchers": 726,
      "open_issues": 19,
      "size": 27
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 94612
      },
      "license": null,
      "topics": []
    },
    "content": {
      "readme": "#### ‰∏™‰∫∫ÂñúÊ¨¢Áé©ÈªëÁÆ±ÊñáÂåñÔºå‰Ω†‰ª¨‰∏ç‰∏ÄÊ†∑ÔºåÂà´‰∏äÂ§¥„ÄÇ\n#### ÊêûËøô‰∏™ÁõÆÁöÑÊòØÂÖàÁ°ÆÂÆödsÁöÑ‰∏Ä‰∫õ‰∏úË•øÔºå‰∏çÊòØÂïÜ‰∏öÂåñ‰∫ßÂìÅÔºå‰∏ª‰ΩìÊÄùË∑ØÊòØÂõ¥ÁªïËØïÈ™åËØÅ‰º™ÂéªÁöÑ\n###  ÂÖ≥Ê≥®Êé®Áâπ‰∫ÜËß£ÊÄùË∑ØÊù•ÈæôÂéªËÑâÔºöhttps://x.com/huojichuanqi\n####  ÁõÆÂâçÊúÄÊúâ‰ª∑ÂÄºÁöÑÂ∞±ÊòØds+ÊåáÊ†áÊñπÊ°àÔºå‰ΩÜÊòØÂü∫Á°ÄÁâàÊú¨ÊòØÊú¨Âú∞ÁÆóÂ•ΩÁªô‰ªñÔºåÊàë‰ª¨Ê≠£Âú®Â∞ùËØïÁî®dsÁõ¥Êé•ÂàÜÊûêÊåáÊ†áÔºå‰ΩÜÊòØÊïàÊûúÊ≤°ÁúãÂá∫Êù•\n### Ê≥®ÊÑè‰∏∫‰∫ÜÁÆÄÂåñÈÄªËæëÔºåËÆ∞ÂæóÊîπ ÂçïÂêëÊåÅ‰ªì  ÂçïÂêëÊåÅ‰ªì ÂçïÂêëÊåÅ‰ªì \n\n\n\n## ÈÖçÁΩÆÂÜÖÂÆπ\n\n### ÈÖçÁΩÆÊñá‰ª∂Âª∫Âú®Á≠ñÁï•Ê†πÁõÆÂΩï\n\n### Êñá‰ª∂ÂêçÂ≠ó    .env\n\n####  DEEPSEEK_API_KEY= ‰Ω†ÁöÑdeepseek  apiÂØÜÈí•\n\n####  BINANCE_API_KEY=\n\n####  BINANCE_SECRET=\n\n####  OKX_API_KEY=\n\n####  OKX_SECRET=\n\n#### OKX_PASSWORD=\n\n###  ËßÜÈ¢ëÊïôÁ®ãÔºöhttps://www.youtube.com/watch?v=Yv-AMVaWUVg\n###  ÈÖçÂêàÂàÜÊ°£ÁßªÂä®Ê≠¢ÁõàÊ≠¢ÊçüÔºöhttps://youtu.be/-vfeyqUkuzY\n\n### ÂáÜÂ§á‰∏ÄÂè∞ubuntuÊúçÂä°Âô® Êé®ËçêÈòøÈáå‰∫ë È¶ôÊ∏ØÊàñËÄÖÊñ∞Âä†Âù° ËΩª‰∫ëÊúçÂä°Âô®\n\n\n#### wget https://repo.anaconda.com/archive/Anaconda3-2024.10-1-Linux-x86_64.sh\n\n#### bash Anaconda3-2024.10-1-Linux-x86_64.sh\n\n#### source /root/anaconda3/etc/profile.d/conda.sh \n#### echo \". /root/anaconda3/etc/profile.d/conda.sh\" >> ~/.bashrc\n\n\n\n\n#### conda create -n ds python=3.10\n\n#### conda activate ds\n\n#### pip install -r requirements.txt\n\n\n\n#### apt-get update Êõ¥Êñ∞ÈïúÂÉèÊ∫ê\n\n\n#### apt-get upgrade ÂøÖË¶ÅÂ∫ìÁöÑ‰∏Ä‰∏™ÂçáÁ∫ß\n\n\n#### apt install npm ÂÆâË£Önpm\n\n\n#### npm install pm2 -g ‰ΩøÁî®npmÂÆâË£Öpm2\n\n#### conda create -n trail3 python=4.10\n\n###### ÊâìËµèÂú∞ÂùÄÔºàTRC20ÔºâÔºöTUunBuqQ1ZDYt9WrA3ZarndFPQgefXqZAM",
      "default_branch": "main"
    },
    "fetched_at": "2025-11-16T02:44:57.591264"
  }
]