[
  {
    "basic_info": {
      "name": "spec-kit",
      "full_name": "github/spec-kit",
      "owner": "github",
      "description": "💫 Toolkit to help you get started with Spec-Driven Development",
      "url": "https://github.com/github/spec-kit",
      "clone_url": "https://github.com/github/spec-kit.git",
      "ssh_url": "git@github.com:github/spec-kit.git",
      "homepage": "",
      "created_at": "2025-08-21T22:54:31Z",
      "updated_at": "2025-09-20T02:10:39Z",
      "pushed_at": "2025-09-20T01:24:35Z"
    },
    "stats": {
      "stars": 22541,
      "forks": 1805,
      "watchers": 22541,
      "open_issues": 186,
      "size": 2603
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 42852,
        "PowerShell": 14700,
        "Shell": 13469
      },
      "license": "MIT License",
      "topics": [
        "ai",
        "copilot",
        "development",
        "engineering",
        "prd",
        "spec",
        "spec-driven"
      ]
    },
    "content": {
      "readme": "<div align=\"center\">\n    <img src=\"./media/logo_small.webp\"/>\n    <h1>🌱 Spec Kit</h1>\n    <h3><em>Build high-quality software faster.</em></h3>\n</div>\n\n<p align=\"center\">\n    <strong>An effort to allow organizations to focus on product scenarios rather than writing undifferentiated code with the help of Spec-Driven Development.</strong>\n</p>\n\n[![Release](https://github.com/github/spec-kit/actions/workflows/release.yml/badge.svg)](https://github.com/github/spec-kit/actions/workflows/release.yml)\n\n---\n\n## Table of Contents\n\n- [🤔 What is Spec-Driven Development?](#-what-is-spec-driven-development)\n- [⚡ Get started](#-get-started)\n- [📽️ Video Overview](#️-video-overview)\n- [🔧 Specify CLI Reference](#-specify-cli-reference)\n- [📚 Core philosophy](#-core-philosophy)\n- [🌟 Development phases](#-development-phases)\n- [🎯 Experimental goals](#-experimental-goals)\n- [🔧 Prerequisites](#-prerequisites)\n- [📖 Learn more](#-learn-more)\n- [📋 Detailed process](#-detailed-process)\n- [🔍 Troubleshooting](#-troubleshooting)\n- [👥 Maintainers](#-maintainers)\n- [💬 Support](#-support)\n- [🙏 Acknowledgements](#-acknowledgements)\n- [📄 License](#-license)\n\n## 🤔 What is Spec-Driven Development?\n\nSpec-Driven Development **flips the script** on traditional software development. For decades, code has been king — specifications were just scaffolding we built and discarded once the \"real work\" of coding began. Spec-Driven Development changes this: **specifications become executable**, directly generating working implementations rather than just guiding them.\n\n## ⚡ Get started\n\n### 1. Install Specify\n\nInitialize your project depending on the coding agent you're using:\n\n```bash\nuvx --from git+https://github.com/github/spec-kit.git specify init <PROJECT_NAME>\n```\n\n### 2. Establish project principles\n\nUse the **`/constitution`** command to create your project's governing principles and development guidelines that will guide all subsequent development.\n\n```bash\n/constitution Create principles focused on code quality, testing standards, user experience consistency, and performance requirements\n```\n\n### 3. Create the spec\n\nUse the **`/specify`** command to describe what you want to build. Focus on the **what** and **why**, not the tech stack.\n\n```bash\n/specify Build an application that can help me organize my photos in separate photo albums. Albums are grouped by date and can be re-organized by dragging and dropping on the main page. Albums are never in other nested albums. Within each album, photos are previewed in a tile-like interface.\n```\n\n### 4. Create a technical implementation plan\n\nUse the **`/plan`** command to provide your tech stack and architecture choices.\n\n```bash\n/plan The application uses Vite with minimal number of libraries. Use vanilla HTML, CSS, and JavaScript as much as possible. Images are not uploaded anywhere and metadata is stored in a local SQLite database.\n```\n\n### 5. Break down into tasks\n\nUse **`/tasks`** to create an actionable task list from your implementation plan.\n\n```bash\n/tasks\n```\n\n### 6. Execute implementation\n\nUse **`/implement`** to execute all tasks and build your feature according to the plan.\n\n```bash\n/implement\n```\n\nFor detailed step-by-step instructions, see our [comprehensive guide](./spec-driven.md).\n\n## 📽️ Video Overview\n\nWant to see Spec Kit in action? Watch our [video overview](https://www.youtube.com/watch?v=a9eR1xsfvHg&pp=0gcJCckJAYcqIYzv)!\n\n[![Spec Kit video header](/media/spec-kit-video-header.jpg)](https://www.youtube.com/watch?v=a9eR1xsfvHg&pp=0gcJCckJAYcqIYzv)\n\n## 🔧 Specify CLI Reference\n\nThe `specify` command supports the following options:\n\n### Commands\n\n| Command     | Description                                                    |\n|-------------|----------------------------------------------------------------|\n| `init`      | Initialize a new Specify project from the latest template      |\n| `check`     | Check for installed tools (`git`, `claude`, `gemini`, `code`/`code-insiders`, `cursor-agent`, `windsurf`, `qwen`, `opencode`) |\n\n### `specify init` Arguments & Options\n\n| Argument/Option        | Type     | Description                                                                  |\n|------------------------|----------|------------------------------------------------------------------------------|\n| `<project-name>`       | Argument | Name for your new project directory (optional if using `--here`)            |\n| `--ai`                 | Option   | AI assistant to use: `claude`, `gemini`, `copilot`, `cursor`, `qwen`, `opencode`, or `windsurf` |\n| `--script`             | Option   | Script variant to use: `sh` (bash/zsh) or `ps` (PowerShell)                 |\n| `--ignore-agent-tools` | Flag     | Skip checks for AI agent tools like Claude Code                             |\n| `--no-git`             | Flag     | Skip git repository initialization                                          |\n| `--here`               | Flag     | Initialize project in the current directory instead of creating a",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-20T02:14:06.024436"
  },
  {
    "basic_info": {
      "name": "Awesome-Nano-Banana-images",
      "full_name": "PicoTrex/Awesome-Nano-Banana-images",
      "owner": "PicoTrex",
      "description": "A curated collection of fun and creative examples generated with Nano Banana🍌, Gemini-2.5-flash-image based model. We also release Nano-consistent-150K openly to support the community's development of image generation and unified models(click to website to see our blog)",
      "url": "https://github.com/PicoTrex/Awesome-Nano-Banana-images",
      "clone_url": "https://github.com/PicoTrex/Awesome-Nano-Banana-images.git",
      "ssh_url": "git@github.com:PicoTrex/Awesome-Nano-Banana-images.git",
      "homepage": "https://picotrex.github.io/Awesome-Nano-Banana-images/",
      "created_at": "2025-08-28T17:03:09Z",
      "updated_at": "2025-09-20T01:54:05Z",
      "pushed_at": "2025-09-18T16:32:53Z"
    },
    "stats": {
      "stars": 9695,
      "forks": 988,
      "watchers": 9695,
      "open_issues": 12,
      "size": 150241
    },
    "tech_info": {
      "language": null,
      "languages": {},
      "license": "Apache License 2.0",
      "topics": [
        "awesome",
        "gemini-2-5-flash-image",
        "nano-banana"
      ]
    },
    "content": {
      "readme": "<div align=\"center\">\n\n<img src=\"images/logo.jpg\"  alt=\"输入图片\"> \n\n[![License: CC BY 4.0](https://img.shields.io/badge/License-CC_BY_4.0-lightgrey.svg)](LICENSE) \n[![简体中文](https://img.shields.io/badge/简体中文-点击查看-orange)](README.md)\n[![English](https://img.shields.io/badge/English-Click_to_View-yellow)](README_en.md)\n[![Japanese](https://img.shields.io/badge/日本語-クリックして表示-green)](README_ja.md)\n[![Korean](https://img.shields.io/badge/한국어-눌러서_보기-blue)](README_kr.md)\n[![Turkish](https://img.shields.io/badge/Türkçe-Görüntülemek_için_Tıklayın-red)](README_tr.md)\n\n</div>\n\n> [!NOTE]\n> 我们提出 Nano-consistent-150k——首个基于 Nano-Banana 构建、规模超过 150k 的高质量数据集，专为在多样而复杂的编辑场景中保持人物身份一致性而设计。其一大特点是卓越的身份一致性：针对同一人像，我们在多种任务与指令下提供了 35 种以上不同的编辑结果。以一致的人物身份为锚点，该数据集使得围绕同一主体在多种编辑任务、指令与模态之间无缝衔接的交错（interleaved）数据构建成为可能。\n<a href='https://picotrex.github.io/Awesome-Nano-Banana-images/'><img src='https://img.shields.io/badge/🌐 Website-Blog-orange' height=\"25\"></a>\n<a href='https://huggingface.co/datasets/Yejy53/Nano-consistent-150k'><img src='https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Dataset-yellow' height=\"25\"></a>\n\n## 🍌 Introduction\n\n欢迎来到 Nano-banana 精选图片库！🤗 \n\n**我们收集了Nano-banana在各个任务场景下生成的令人惊艳的图片和提示词**，全方位展示Google在图像生成与编辑的无限可能。希望能帮助你更好地了解Nano-banana。快一起解锁 Nano-banana 的多图融合与创意编辑力量吧！✨\n\n这些案例主要来源于Twitter/ X 🐦、小红书📕等自媒体平台。\n\n喜欢就点 ⭐ Star 收藏起来吧！\n\n## 📰 News\n\n- **2025年9月18日：** 我们发布了 [**Nano-consistent-150k**](https://picotrex.github.io/Awesome-Nano-Banana-images/) 数据集\n- **2025年9月16日：** 4️⃣ 第四次仓库更新\n- **2025年9月9日：** 3️⃣ 第三次仓库更新\n- **2025年9月3日：** 2️⃣ 第二次仓库更新\n- **2025年8月28日：** 🎉 1️⃣ ${\\color{red} 第一次\\ Awesome-Nano-Banana-images \\ 更新!}$\n\n## 📑 Menu\n\n- [🍌 Introduction](#-introduction)\n- [📰 News](#-news)\n- [📑 Menu](#-menu)\n- [🖼️ 例子](#️-例子)\n  - [例 1: 插画变手办（by @ZHO\\_ZHO\\_ZHO）](#例-1-插画变手办by-zho_zho_zho)\n  - [例 2: 根据地图箭头生成地面视角图片（by @tokumin）](#例-2-根据地图箭头生成地面视角图片by-tokumin)\n  - [例 3: 真实世界的AR信息化（by @bilawalsidhu）](#例-3-真实世界的ar信息化by-bilawalsidhu)\n  - [例 4: 分离出3D建筑/制作等距模型（by @Zieeett）](#例-4-分离出3d建筑制作等距模型by-zieeett)\n  - [例 5: 不同时代自己的照片（by @AmirMushich）](#例-5-不同时代自己的照片by-amirmushich)\n  - [例 6: 多参考图像生成（by @MrDavids1）](#例-6-多参考图像生成by-mrdavids1)\n  - [例 7: 自动修图（by @op7418）](#例-7-自动修图by-op7418)\n  - [例 8: 手绘图控制多角色姿态（by @op7418）](#例-8-手绘图控制多角色姿态by-op7418)\n  - [例 9: 跨视角图像生成（by @op7418）](#例-9-跨视角图像生成by-op7418)\n  - [例 10: 定制人物贴纸（by @op7418）](#例-10-定制人物贴纸by-op7418)\n  - [例 11: 动漫转真人Coser（by @ZHO\\_ZHO\\_ZHO）](#例-11-动漫转真人coserby-zho_zho_zho)\n  - [例 12: 生成角色设定（by @ZHO\\_ZHO\\_ZHO）](#例-12-生成角色设定by-zho_zho_zho)\n  - [例 13: 色卡线稿上色（by @ZHO\\_ZHO\\_ZHO）](#例-13-色卡线稿上色by-zho_zho_zho)\n  - [例 14: 文章信息图（by @黄建同学）](#例-14-文章信息图by-黄建同学)\n  - [例 15: 更换多种发型（by @balconychy）](#例-15-更换多种发型by-balconychy)\n  - [例 16: 模型标注讲解图（by @berryxia\\_ai）](#例-16-模型标注讲解图by-berryxia_ai)\n  - [例 17: 定制大理石雕塑（by @umesh\\_ai）](#例-17-定制大理石雕塑by-umesh_ai)\n  - [例 18: 根据食材做菜（by @Gdgtify）](#例-18-根据食材做菜by-gdgtify)\n  - [例 19: 数学题推理（by @Gorden Sun）](#例-19-数学题推理by-gorden-sun)\n  - [例 20: 旧照片上色（by @GeminiApp）](#例-20-旧照片上色by-geminiapp)\n  - [例 21: OOTD穿搭（by @302.AI）](#例-21-ootd穿搭by-302ai)\n  - [例 22: 人物换衣（by @skirano）](#例-22-人物换衣by-skirano)\n  - [例 23: 多视图结果生成（by @Error\\_HTTP\\_404）](#例-23-多视图结果生成by-error_http_404)\n  - [例 24: 电影分镜（by @GeminiApp）](#例-24-电影分镜by-geminiapp)\n  - [例 25: 人物姿势修改（by @arrakis\\_ai）](#例-25-人物姿势修改by-arrakis_ai)\n  - [例 26: 线稿图生成图像（by @ZHO\\_ZHO\\_ZHO）](#例-26-线稿图生成图像by-zho_zho_zho)\n  - [例 27: 为图像添加水印（by @AiMachete）](#例-27-为图像添加水印by-aimachete)\n  - [例 28: 知识推理生成图像（by @icreatelife）](#例-28-知识推理生成图像by-icreatelife)\n  - [例 29: 红笔批注（by @AiMachete）](#例-29-红笔批注by-aimachete)\n  - [例 30: 爆炸的食物（by @icreatelife）](#例-30-爆炸的食物by-icreatelife)\n  - [例 31: 制作漫画书（by @icreatelife）](#例-31-制作漫画书by-icreatelife)\n  - [例 32: 动作人偶（by @icreatelife）](#例-32-动作人偶by-icreatelife)\n  - [例 33: 地图生成等距建筑（by @demishassabis）](#例-33-地图生成等距建筑by-demishassabis)\n  - [例 34: 参考图控制人物表情（by @ZHO\\_ZHO\\_ZHO）](#例-34-参考图控制人物表情by-zho_zho_zho)\n  - [例 35: 插画绘画过程四格（by @ZHO\\_ZHO\\_ZHO）](#例-35-插画绘画过程四格by-zho_zho_zho)\n  - [例 36: 虚拟试妆（by @ZHO\\_ZHO\\_ZHO）](#例-36-虚拟试妆by-zho_zho_zho)\n  - [例 37: 妆面分析（by @ZHO\\_ZHO\\_ZHO）](#例-37-妆面分析by-zho_zho_zho)\n  - [例 38: Google地图视角下的中土世界（by @TechHallo）](#例-38-google地图视角下的中土世界by-techhallo)\n  - [例 39: 印刷插画生成（by @Umesh）](#例-39-印刷插画生成by-umesh)\n  - [例 40: 超多人物姿势生成（by @tapehead\\_Lab）](#例-40-超多人物姿势生成by-tapehead_lab)\n  - [例 41: 物品包装生成（by @ZHO\\_ZHO\\_ZHO）](#例-41-物品包装生成by-zho_zho_zho)\n  - [例 42: 叠加滤镜/材质（by @ZHO\\_ZHO\\_ZHO）](#例-42-叠加滤镜材质by-zho_zho_zho)\n  - [例 43: 控制人物脸型（by @ZHO\\_ZHO\\_ZHO）](#例-43-控制人物脸型by-zho_zho_zho)\n  - [例 44: 光影控制（by @ZHO\\_ZHO\\_ZHO）](#例-44-光影控制by-zho_zho_zho)\n  - [例 45: 乐高玩具小人（by @ZHO\\_ZHO\\_ZHO）](#例-45-乐高玩具小人by-zho_zho_zho)\n  - [例 46: 高达模型小人（by @ZHO\\_ZHO\\_ZHO）](#例-46-高达模型小人by-zho_zho_zho)\n  - [例 47: 硬件拆解图（by @AIimagined）](#例-47-硬件拆解图by-aiimagined)\n  - [例 48: 食物卡路里标注（by @icreatelife）](#例-48-食物卡路里标注by-icreatelife)\n  - [例 49: 提取信息并放置透明图层（by @nglprz）](#例-49-提取信息并放置透明图层by-nglprz)\n  - [例 50: 图像外扩修复（by @bwabbage）](#例-50-图像外扩修复by-bwabbage)\n  - [例 51: 古老地图生成古代场景（by @levelsio）](#例-51-古老地图生成古代场景by-levelsio)\n  - [例 52: 时尚服装拼贴画（by @tetumemo）](#例-52-时尚服装拼贴画by-tetume",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-20T02:14:07.141055"
  },
  {
    "basic_info": {
      "name": "VibeVoice",
      "full_name": "microsoft/VibeVoice",
      "owner": "microsoft",
      "description": "Frontier Open-Source Text-to-Speech",
      "url": "https://github.com/microsoft/VibeVoice",
      "clone_url": "https://github.com/microsoft/VibeVoice.git",
      "ssh_url": "git@github.com:microsoft/VibeVoice.git",
      "homepage": "",
      "created_at": "2025-08-25T13:24:01Z",
      "updated_at": "2025-09-20T01:40:27Z",
      "pushed_at": "2025-09-05T15:02:35Z"
    },
    "stats": {
      "stars": 9034,
      "forks": 1056,
      "watchers": 9034,
      "open_issues": 25,
      "size": 55066
    },
    "tech_info": {
      "language": null,
      "languages": {},
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "<div align=\"center\">\n\n## 🎙️ VibeVoice: A Frontier Long Conversational Text-to-Speech Model\n[![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=microsoft)](https://microsoft.github.io/VibeVoice)\n[![Hugging Face](https://img.shields.io/badge/HuggingFace-Collection-orange?logo=huggingface)](https://huggingface.co/collections/microsoft/vibevoice-68a2ef24a875c44be47b034f)\n[![Technical Report](https://img.shields.io/badge/Technical-Report-red?logo=adobeacrobatreader)](https://arxiv.org/pdf/2508.19205)\n\n\n</div>\n\n\n<div align=\"center\">\n<picture>\n  <source media=\"(prefers-color-scheme: dark)\" srcset=\"Figures/VibeVoice_logo_white.png\">\n  <img src=\"Figures/VibeVoice_logo.png\" alt=\"VibeVoice Logo\" width=\"300\">\n</picture>\n</div>\n\n\n**2025-09-05: VibeVoice is an open-source research framework intended to advance collaboration in the speech synthesis community. After release, we discovered instances where the tool was used in ways inconsistent with the stated intent. Since responsible use of AI is one of Microsoft’s guiding principles, we have disabled this repo until we are confident that out-of-scope use is no longer possible.**\n\n<br>\n\nVibeVoice is a novel framework designed for generating **expressive**, **long-form**, **multi-speaker** conversational audio, such as podcasts, from text. It addresses significant challenges in traditional Text-to-Speech (TTS) systems, particularly in scalability, speaker consistency, and natural turn-taking.\n\nA core innovation of VibeVoice is its use of continuous speech tokenizers (Acoustic and Semantic) operating at an ultra-low frame rate of 7.5 Hz. These tokenizers efficiently preserve audio fidelity while significantly boosting computational efficiency for processing long sequences. VibeVoice employs a [next-token diffusion](https://arxiv.org/abs/2412.08635) framework, leveraging a Large Language Model (LLM) to understand textual context and dialogue flow, and a diffusion head to generate high-fidelity acoustic details.\n\nThe model can synthesize speech up to **90 minutes** long with up to **4 distinct speakers**, surpassing the typical 1-2 speaker limits of many prior models. \n\n\n<p align=\"left\">\n  <img src=\"Figures/MOS-preference.png\" alt=\"MOS Preference Results\" height=\"260px\">\n  <img src=\"Figures/VibeVoice.jpg\" alt=\"VibeVoice Overview\" height=\"250px\" style=\"margin-right: 10px;\">\n</p>\n\n\n### 🎵 Demo Examples\n\n\n**Video Demo**\n\nWe produced this video with [Wan2.2](https://github.com/Wan-Video/Wan2.2). We sincerely appreciate the Wan-Video team for their great work.\n\n**English**\n<div align=\"center\">\n\nhttps://github.com/user-attachments/assets/0967027c-141e-4909-bec8-091558b1b784\n\n</div>\n\n\n**Chinese**\n<div align=\"center\">\n\nhttps://github.com/user-attachments/assets/322280b7-3093-4c67-86e3-10be4746c88f\n\n</div>\n\n**Cross-Lingual**\n<div align=\"center\">\n\nhttps://github.com/user-attachments/assets/838d8ad9-a201-4dde-bb45-8cd3f59ce722\n\n</div>\n\n**Spontaneous Singing**\n<div align=\"center\">\n\nhttps://github.com/user-attachments/assets/6f27a8a5-0c60-4f57-87f3-7dea2e11c730\n\n</div>\n\n\n**Long Conversation with 4 people**\n<div align=\"center\">\n\nhttps://github.com/user-attachments/assets/a357c4b6-9768-495c-a576-1618f6275727\n\n</div>\n\nFor more examples, see the [Project Page](https://microsoft.github.io/VibeVoice).\n\n\n\n## Risks and limitations\n\nWhile efforts have been made to optimize it through various techniques, it may still produce outputs that are unexpected, biased, or inaccurate. VibeVoice inherits any biases, errors, or omissions produced by its base model (specifically, Qwen2.5 1.5b in this release).\nPotential for Deepfakes and Disinformation: High-quality synthetic speech can be misused to create convincing fake audio content for impersonation, fraud, or spreading disinformation. Users must ensure transcripts are reliable, check content accuracy, and avoid using generated content in misleading ways. Users are expected to use the generated content and to deploy the models in a lawful manner, in full compliance with all applicable laws and regulations in the relevant jurisdictions. It is best practice to disclose the use of AI when sharing AI-generated content.\n\nEnglish and Chinese only: Transcripts in languages other than English or Chinese may result in unexpected audio outputs.\n\nNon-Speech Audio: The model focuses solely on speech synthesis and does not handle background noise, music, or other sound effects.\n\nOverlapping Speech: The current model does not explicitly model or generate overlapping speech segments in conversations.\n\nWe do not recommend using VibeVoice in commercial or real-world applications without further testing and development. This model is intended for research and development purposes only. Please use responsibly.\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-20T02:14:08.262130"
  },
  {
    "basic_info": {
      "name": "term.everything",
      "full_name": "mmulet/term.everything",
      "owner": "mmulet",
      "description": "Run any GUI app in the terminal❗",
      "url": "https://github.com/mmulet/term.everything",
      "clone_url": "https://github.com/mmulet/term.everything.git",
      "ssh_url": "git@github.com:mmulet/term.everything.git",
      "homepage": "",
      "created_at": "2025-09-07T02:52:48Z",
      "updated_at": "2025-09-20T02:10:08Z",
      "pushed_at": "2025-09-18T04:30:07Z"
    },
    "stats": {
      "stars": 5668,
      "forks": 118,
      "watchers": 5668,
      "open_issues": 16,
      "size": 53541
    },
    "tech_info": {
      "language": "TypeScript",
      "languages": {
        "TypeScript": 219331,
        "C++": 39626,
        "JavaScript": 2607,
        "Meson": 2286,
        "Shell": 1291,
        "C": 314
      },
      "license": "GNU Affero General Public License v3.0",
      "topics": [
        "alacritty",
        "cli",
        "foss",
        "iterm2",
        "kitty",
        "linux",
        "ssh",
        "terminal",
        "wayland",
        "wayland-compositor"
      ]
    },
    "content": {
      "readme": "\n\n\n<table>\n  <tr>\n    <td valign=\"middle\">\n      <img width=\"128\" height=\"128\" alt=\"icon2\" src=\"./resources/icon.png\" />\n    </td>\n    <td><h1>Term.Everything❗</h1></td>\n    <td><a href=\"https://github.com/mmulet/term.everything/releases\">Download the beta test now!</a></td>\n    <td><a href=\"./resources/HowIDidIt.md\">HowIDidIt.md</a></td>\n  </tr>\n  <tr>\n    <td></td>\n    <td>Works on both x11 and Wayland host systems.</td>\n    <td></td>\n    <td></td>\n  </tr>\n</table>\n\n## Run every GUI app in the terminal!\n\n![warp_into_terminal0001-0195](./resources/graphics/warp_in_2.gif)\n\n## Even over ssh!\nBehold as I play a [video game in a font](https://github.com/mmulet/font-game-engine) in a web browser in a terminal transmitted over ssh (with one hand tied behind my back)!\n\n![ssh_example](./resources/graphics/ssh_example.gif)\n\n### Read about how it works!\nCheck out [HowIDidIt.md](./resources/HowIDidIt.md)\n\n## More Examples\nThe quality of the window is limited to the number of rows and columns in your\nterminal. If you increase the resolution (ctrl - in alacritty, check your\nterminal) the quality will go up, (but performance may go down).\n\nHere I open up the Wing It! movie, and increase the quality until I get both\na good frame rate and resolution:\n\n![increase resolution](./resources/graphics/show_increase_res.gif)\n\n----------------\n\nIf your terminal supports images (like [kitty](https://sw.kovidgoyal.net/kitty/)\nor [iterm2](https://iterm2.com/)) you can render windows at full resolution\n(performance may degrade).\n\nIn this example, on my mac, I open iTerm2 ssh into ubuntu and open firefox\nat full resolution:\n\n![full_resultion](resources/graphics/full_resultion.gif)\n\n------------\n\nI feel like every single day I hear about another terminal file viewer. I say, stop making terminal file viewers because you can just use the file viewer you already have! In your terminal!\n\n![file_manager](./resources/graphics/file_manager.gif)\n\n-------------\n\nTerminal in a terminal in a terminal in a terminal in a terminal.... it's terminals all the way down.\n![terminal_in_terminal](./resources/graphics/terminal_in_terminal.gif)\n\n-------------\nWith only a small amount hacking, it can run Doom (shareware episode)!\n\n![Doom](./resources/graphics/doom.gif)\n------\nRun an entire Desktop in your terminal!\n[@ismail-yilmaz](https://github.com/ismail-yilmaz) is running Firefox, on [KDE Neon](https://neon.kde.org) in a [VM](https://gitlab.gnome.org/GNOME/gnome-boxes) on [Bobcat](https://github.com/ismail-yilmaz/Bobcat)\n![Desktop in VM](./resources/graphics/desktop_in_vm.gif)\n\nAnd this isn't even full resolution! Checkout the [full vid in in the discussions](https://github.com/mmulet/term.everything/discussions/16#discussioncomment-14390137)\n\n## About\n`term.everything❗` is a Linux CLI program to run GUI windows in your terminal. Specifically, `term.everything❗` is a built-from-scratch [Wayland](https://wiki.archlinux.org/title/Wayland) compositor that outputs to a terminal rather than your monitor.\n\n>Don't know what Wayland is or just want to know more about how this works? Then, head over to [HowIDidIt.md](./resources/HowIDidIt.md) where I will explain how everything works in detail.\n\n## Try it out!\n[Download the beta test now!](https://github.com/mmulet/term.everything/releases)\n\n## Roadmap\n1. [x] Term some things <--- This is where we are at\n  - Some apps or (even most apps) may fail to launch or even crash! Please create [an issue]( https://github.com/mmulet/term.everything/issues) if you have problems.\n2. [ ] Term most things\n3. [ ] Term everything❗\n\n## Help and Usage\nCheck out the [help file here](./resources/help.md) for a usage guide on how to use `term.everything❗`\n\n## Contributing\nterm.everything❗ is written in developer friendly [Typescript](https://www.typescriptlang.org/) using the [bun](https://bun.com/) engine, with a just a smidge of C++.\nSee [./Contributing.md](./Contributing.md).\n\n## Legal:\n\nterm.everything❗ copyright 2025 Late for Dinner Studios, LLC\n---\nFontemon copyright 2021 Late for Dinner Studios, LLC\n---\nWing It! movie is licensed under the Creative Commons Attribution 4.0 license\n[Wing it licensing page](https://studio.blender.org/projects/wing-it/pages/licensing/)\nAttribution:\n(CC) Blender Foundation | studio.blender.org\n---\nDoom shareware episode is copyright 1993 id Software\n---\n\n## Bonus:\nThis is Gwerm the Term Worm.\n\n![this is gwern](./resources/graphics/this_is_gwern.gif)\n\nHe is doing okay. Thanks for asking.\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-20T02:14:09.381028"
  },
  {
    "basic_info": {
      "name": "LidAngleSensor",
      "full_name": "samhenrigold/LidAngleSensor",
      "owner": "samhenrigold",
      "description": "tfw when you when your lid when uhh angle your lid sensor",
      "url": "https://github.com/samhenrigold/LidAngleSensor",
      "clone_url": "https://github.com/samhenrigold/LidAngleSensor.git",
      "ssh_url": "git@github.com:samhenrigold/LidAngleSensor.git",
      "homepage": "https://samhenri.gold",
      "created_at": "2025-09-06T19:07:20Z",
      "updated_at": "2025-09-20T00:29:05Z",
      "pushed_at": "2025-09-08T21:30:26Z"
    },
    "stats": {
      "stars": 3262,
      "forks": 122,
      "watchers": 3262,
      "open_issues": 31,
      "size": 1486
    },
    "tech_info": {
      "language": "Objective-C",
      "languages": {
        "Objective-C": 49586
      },
      "license": "Apache License 2.0",
      "topics": []
    },
    "content": {
      "readme": "# Lid Angle Sensor\n\nHi, I’m Sam Gold. Did you know that you have ~rights~ a lid angle sensor in your MacBook? [The ~Constitution~ human interface device utility says you do.](https://youtu.be/wqnHtGgVAUE?t=21)\n\nThis is a little utility that shows the angle from the sensor and, optionally, plays a wooden door creaking sound if you adjust it reeaaaaaal slowly.\n\n## FAQ\n\n**What is a lid angle sensor?**\n\nDespite what the name would have you believe, it is a sensor that detects the angle of the lid.\n\n**Which devices have a lid angle sensor?**\n\nIt was introduced with the 2019 16-inch MacBook Pro. If your laptop is newer, you probably have it. [People have reported](https://github.com/samhenrigold/LidAngleSensor/issues/13) that it **does not work on M1 devices**, I have not yet figured out a fix.\n\n**My laptop should have it, why doesn't it show up?**\n\nI've only tested this on my M4 MacBook Pro and have hard-coded it to look for a specific sensor. If that doesn't work, try running [this script](https://gist.github.com/samhenrigold/42b5a92d1ee8aaf2b840be34bff28591) and report the output in [an issue](https://github.com/samhenrigold/LidAngleSensor/issues/new/choose).\n\nKnown problematic models:\n\n- M1 MacBook Air\n- M1 MacBook Pro\n\n**Can I use this on my iMac?**\n\n~~Not yet tested. Feel free to slam your computer into your desk and make a PR with your results.~~\n\n[It totally works](https://github.com/samhenrigold/LidAngleSensor/issues/33). If it doesn't work for you, try slamming your computer harder?\n\n**Why?**\n\nA lot of free time. I'm open to full-time work in NYC or remote. I'm a designer/design-engineer. https://samhenri.gold\n\n**No I mean like why does my laptop need to know the exact angle of its lid?**\n\nOh. I don't know.\n\n**Can I contribute?**\n\nI guess.\n\n**Why does it say it's by Lisa?**\n\nI signed up for my developer account when I was a kid, used my mom's name, and now it's stuck that way forever and I can't change it. That's life.\n\n**How come the audio feels kind of...weird?**\n\nI'm bad at audio.\n\n**Where did the sound effect come from?**\n\nLEGO Batman 3: Beyond Gotham. But you knew that already.\n\n**Can I turn off the sound?**\n\nYes, never click \"Start Audio\". But this energy isn't encouraged.\n\n## Building\n\nAccording to [this issue](https://github.com/samhenrigold/LidAngleSensor/issues/12), building requires having Xcode installed. I've only tested this on Xcode 26. YMMV.\n\n## Installation\n\nVia Homebrew:\n\n```shell\nbrew install lidanglesensor\n```\n\n## Related projects\n\n- [Python library that taps into this sensor](https://github.com/tcsenpai/pybooklid)\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-20T02:14:10.504269"
  },
  {
    "basic_info": {
      "name": "growchief",
      "full_name": "growchief/growchief",
      "owner": "growchief",
      "description": "The Ultimate all-in social media automation (outreach) tool 🤖",
      "url": "https://github.com/growchief/growchief",
      "clone_url": "https://github.com/growchief/growchief.git",
      "ssh_url": "git@github.com:growchief/growchief.git",
      "homepage": "https://growchief.com",
      "created_at": "2025-08-21T08:06:06Z",
      "updated_at": "2025-09-19T21:13:44Z",
      "pushed_at": "2025-09-09T16:29:45Z"
    },
    "stats": {
      "stars": 3051,
      "forks": 198,
      "watchers": 3051,
      "open_issues": 0,
      "size": 851
    },
    "tech_info": {
      "language": "TypeScript",
      "languages": {
        "TypeScript": 834458,
        "JavaScript": 7990,
        "CSS": 3660,
        "Shell": 517,
        "HTML": 506
      },
      "license": "GNU Affero General Public License v3.0",
      "topics": [
        "automation",
        "n8n",
        "nestjs",
        "nodejs",
        "outreach",
        "social-media",
        "temporal"
      ]
    },
    "content": {
      "readme": "<p align=\"center\">\n  <a href=\"https://growchief.com/\" target=\"_blank\">\n  <picture>\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://github.com/user-attachments/assets/1ba8233b-856e-448e-899b-5f9445e65d85\">\n    <img alt=\"GrowChief Logo\" src=\"https://github.com/user-attachments/assets/50401d55-d870-478a-a5c4-ef6b474e6ccc\" width=\"280\"/>\n  </picture>\n  </a>\n</p>\n\n<p align=\"center\">\n<a href=\"https://opensource.org/license/agpl-v3\">\n  <img src=\"https://img.shields.io/badge/License-AGPL%203.0-blue.svg\" alt=\"License\">\n</a>\n</p>\n\n<div align=\"center\">\n  <strong>\n  <h2>Your ultimate social media automation tool (outreach tool)</h2><br />\n  <a href=\"https://growchief.com\">GrowChief</a>: An alternative to: Phantom Buster, Expandi, Zopto, LinkedIn Helper, Meet Alfred, etc.<br /><br />\n  </strong>\n  GrowChief is an API based tool to automate your social media accounts such as<br />sending connection requests and follow-up messages. Perfect for n8n / Make / Zapier users\n</div>\n\n<div class=\"flex\" align=\"center\">\n  <br />\n  <img alt=\"Linkedin\" src=\"https://postiz.com/svgs/socials/Linkedin.svg\" width=\"32\">\n  <img alt=\"X\" src=\"https://postiz.com/svgs/socials/X.svg\" width=\"32\">\n</div>\n<p align=\"center\">\n  <br />\n  <a href=\"https://docs.growchief.com\" rel=\"dofollow\"><strong>Explore the docs »</strong></a>\n  <br />\n</p>\n<p align=\"center\">\n  <a href=\"https://www.npmjs.com/package/n8n-nodes-growchief\">N8N node</a>\n  ·\n  <a href=\"https://platform.growchief.com\">Register</a>\n  ·\n  <a href=\"https://discord.growchief.com\">Join Our Discord (devs only)</a>\n</p>\n\n## ✨ Features\n\n| ![Image 1](https://github.com/user-attachments/assets/492ffc23-98ff-4d1b-a812-34debc0d2161) | ![Image 2](https://github.com/user-attachments/assets/1dd33597-dc87-45a7-8380-f31c102c3687) |\n| ------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------- |\n| ![Image 3](https://github.com/user-attachments/assets/ba7d377a-8ede-424e-9c5e-6f7741d97f81) | ![Image 4](https://github.com/user-attachments/assets/17e903c8-b32c-4f6c-b565-dc30f240f069) |\n\n---\n\n## Introduction\n\nGrowchief is an open-source social media automation tool (aka social scraper). It allows you to create a workflow (step-by-step) for interacting with different accounts on social media, such as sending a connection request, following up with a message, and so on.\nWe do not encourage spam (perfect for API/n8n automations).\n\n## Why is GrowChief so good?\n\n* It takes care of concurrency — even if you create 10 workflows with the same account and trigger all of them at the same time, it will make an action every 10 minutes, never having multiple scrapings happening at the same time.\n\n* Enrichment waterful — when you don't provide the account URL but other parameters like email, we use multiple provider to figure out the profile URL.\n\n* It takes care of your working hours — you can use the API to keep adding leads to your workflows, but they will only be processed during working hours.\n\n* Proxies are allowed — you can add your own proxies or create one using proxy providers to keep you safe.\n\n* Human-like automation — GrowChief uses natural mouse movements and clicks on different parts of the screen. It never triggers clicks by `\"document.querySelector('x').click()\"`.\n\n* It uses [Playwright](https://github.com/microsoft/playwright) together with [Patchright](https://github.com/Kaliiiiiiiiii-Vinyzu/patchright) for maximum stealthiness.\n\n* It uses a special technology to authenticate your accounts — you never need to put your username and password directly into the system.\n\n* It always runs in headful — our Docker image is already built with `xvfb` for real human automation.\n\n## Things you should know\n\n* Social media automation is a common practice in businesses, from small ones to enterprises. Yes, even the biggest companies in the world do it.\n  However, it violates the terms of service of the platforms and can result in a ban. Use it at your own risk and connect only with leads you know.\n\n* GrowChief Docker can work great without scale, as every time you start automation it opens a Chrome browser (perfect for 1–2 accounts).\n  However, once you scale, you need a smarter system (remote browser) with an option to scale, as Chrome consumes a lot of memory. For that, you can use GrowChief Cloud.\n\n\n## Tech Stack\n\n* PNPM (Monorepo)\n* React (Vite)\n* NestJS (Backend, Workers)\n* Prisma (Default to PostgreSQL)\n* Temporal (Orchestrator)\n\n## QuickStart / Installation\n\nView https://docs.growchief.com\n\n## Sponsorship\n\nThis can be very valuable for Proxies / Lead enrichment companies, feel free to check our sponsorship page.\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-20T02:14:11.621870"
  },
  {
    "basic_info": {
      "name": "youtu-agent",
      "full_name": "TencentCloudADP/youtu-agent",
      "owner": "TencentCloudADP",
      "description": "A simple yet powerful agent framework that delivers with open-source models",
      "url": "https://github.com/TencentCloudADP/youtu-agent",
      "clone_url": "https://github.com/TencentCloudADP/youtu-agent.git",
      "ssh_url": "git@github.com:TencentCloudADP/youtu-agent.git",
      "homepage": "https://tencentcloudadp.github.io/youtu-agent/",
      "created_at": "2025-08-21T07:58:13Z",
      "updated_at": "2025-09-19T22:31:31Z",
      "pushed_at": "2025-09-19T13:43:41Z"
    },
    "stats": {
      "stars": 2934,
      "forks": 280,
      "watchers": 2934,
      "open_issues": 25,
      "size": 10171
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 485594,
        "TypeScript": 209405,
        "CSS": 50866,
        "Jinja": 12981,
        "JavaScript": 1085,
        "Dockerfile": 871,
        "Makefile": 786,
        "HTML": 716,
        "Shell": 219
      },
      "license": "Other",
      "topics": [
        "agent-framework",
        "agents",
        "openai-agents",
        "python"
      ]
    },
    "content": {
      "readme": "# <img src=\"docs/assets/logo.svg\" alt=\"Youtu-agent Logo\" height=\"24px\"> Youtu-Agent: A simple yet powerful agent framework that delivers with open-source models\n\n<div align=\"center\">\n<a href=\"https://tencentcloudadp.github.io/youtu-agent/\"><img src=https://img.shields.io/badge/📖-Documentation-blue.svg></a>\n<!-- <a href=https://arxiv.org/abs/2502.14345><img src=https://img.shields.io/badge/arXiv-2502.14345-b31b1b.svg></a> -->\n<a href=https://github.com/TencentCloudADP/youtu-agent><img src=https://img.shields.io/badge/GitHub-Tencent-blue.svg></a>\n<a href=https://deepwiki.com/TencentCloudADP/youtu-agent><img src=https://img.shields.io/badge/DeepWiki-Tencent-blue.svg></a>\n</div>\n\n<p align=\"center\">\n| <a href=\"README_ZH.md\"><b>中文</b></a>\n| <a href=\"README_JA.md\"><b>日本語</b></a>\n| <a href=\"#-benchmark-performance\"><b>🌟 Performance</b></a> \n| <a href=\"#-examples\"><b>💡 Examples</b> </a> \n| <a href=\"#-features\"><b>✨ Features</b> </a> \n| <a href=\"#-getting-started\"><b>🚀 Getting Started</b> </a> \n| <a href=\"https://discord.gg/svwuqgUx\"><b>📢 Join Community</b> </a> \n</p>\n\n\n`Youtu-Agent` is a flexible, high-performance framework for building, running, and evaluating autonomous agents. Beyond topping the benchmarks, this framework delivers powerful agent capabilities, e.g. data analysis, file processing, and deep research, all with open-source models.\n\n<img src=\"docs/assets/mascot.png\" alt=\"Youtu-agent Logo\" width=\"200\" align=\"left\" style=\"margin-right:20px;\">\n\nKey highlights:\n- **Verified performance**: Achieved 71.47% on WebWalkerQA (pass@1) and 72.8% on GAIA (text-only subset, pass@1), using purely `DeepSeek-V3` series models (without Claude or GPT), establishing a strong open-source starting point.\n- **Open-source friendly & cost-aware**: Optimized for accessible, low-cost deployment without reliance on closed models.\n- **Practical use cases**: Out-of-the-box support for tasks like CSV analysis, literature review, personal file organization, and podcast and video generation (coming soon).\n- **Flexible architecture**: Built on [openai-agents](https://github.com/openai/openai-agents-python), with extensible support for diverse model APIs (form `DeepSeek` to `gpt-oss`), tool integrations, and framework implementations.\n- **Automation & simplicity**: YAML-based configs, auto agent generation, and streamlined setup reduce manual overhead.\n\n## 🗞️ News\n\n- 📺 [2025-09-09] We hosted a live sharing the design philosophy and basic usage of `Youtu-Agent`. [[video](https://www.bilibili.com/video/BV1mypqz4EvS)] [[documentation](https://doc.weixin.qq.com/doc/w3_AcMATAZtAPICNLgt3CbnxRWaYWnW4)].\n- 🎁 [2025-09-02] [Tencent Cloud International](https://www.tencentcloud.com/) offers new users of the DeepSeek API **3 million free tokens** (**Sep 1 – Oct 31, 2025**). [Try it out](https://www.tencentcloud.com/document/product/1255/70381) for free if you want to use DeepSeek models in `Youtu-Agent`! For enterprise agent solutions, also check out [Agent Development Platform](https://adp.tencentcloud.com) (ADP).\n- 📺 [2025-08-28] We hosted a live sharing updates about DeepSeek-V3.1 and how to use it in the `Youtu-Agent` framework. [[video](https://www.bilibili.com/video/BV1XwayzrETi/)] [[documentation](https://doc.weixin.qq.com/doc/w3_AcMATAZtAPICNvcLaY5FvTOuo7MwF)].\n\n## 🌟 Benchmark Performance\n\n`Youtu-Agent` is built on open-source models and lightweight tools, demonstrating strong results on challenging deep search and tool use benchmarks.\n\n- **[WebWalkerQA](https://huggingface.co/datasets/callanwu/WebWalkerQA)**: Achieved 60.71% accuracy with `DeepSeek-V3-0324`， using new released `DeepSeek-V3.1` can further improve to 71.47%, setting a new SOTA performance.\n- **[GAIA](https://gaia-benchmark-leaderboard.hf.space/)**: Achieved 72.8% pass@1 on the [text-only validation subset](https://github.com/sunnynexus/WebThinker/blob/main/data/GAIA/dev.json) using `DeepSeek-V3-0324` (including models used within tools). We are actively extending evaluation to the full GAIA benchmark with multimodal tools, and will release the trajectories in the near future. Stay tuned! ✨\n\n![WebWalkerQA](docs/assets/images/benchmark_webwalkerqa.png)\n\n## 💡 Examples\n\nClick on the images to view detailed videos.\n\n<table>\n  <tr>\n    <td style=\"border: 1px solid black; padding: 10px; width: 50%; vertical-align: top;\">\n      <strong>Data Analysis</strong><br>Analyzes a CSV file and generates an HTML report.\n    </td>\n    <td style=\"border: 1px solid black; padding: 10px; width: 50%; vertical-align: top;\">\n      <strong>File Management</strong><br>Renames and categorizes local files for the user.\n    </td>\n  </tr>\n  <tr>\n    <td style=\"border: 1px solid black; padding: 10px; width: 50%; vertical-align: top;\">\n      <video src=\"https://github.com/user-attachments/assets/60193435-b89d-47d3-8153-5799d6ff2920\" \n             poster=\"https://img.youtube.com/vi/r9we4m1cB6M/sddefault.jpg\" \n             controls muted preload=\"metadata\" \n             width=\"100%\" height=\"300\"\n             ",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-20T02:14:12.753866"
  },
  {
    "basic_info": {
      "name": "shimmy",
      "full_name": "Michael-A-Kuykendall/shimmy",
      "owner": "Michael-A-Kuykendall",
      "description": "⚡ Python-free Rust inference server — OpenAI-API compatible. GGUF + SafeTensors, hot model swap, auto-discovery, single binary. FREE now, FREE forever.",
      "url": "https://github.com/Michael-A-Kuykendall/shimmy",
      "clone_url": "https://github.com/Michael-A-Kuykendall/shimmy.git",
      "ssh_url": "git@github.com:Michael-A-Kuykendall/shimmy.git",
      "homepage": "",
      "created_at": "2025-08-28T22:55:46Z",
      "updated_at": "2025-09-20T01:46:55Z",
      "pushed_at": "2025-09-20T01:29:48Z"
    },
    "stats": {
      "stars": 2440,
      "forks": 166,
      "watchers": 2440,
      "open_issues": 1,
      "size": 213258
    },
    "tech_info": {
      "language": "Rust",
      "languages": {
        "Rust": 536594,
        "C": 158306,
        "C++": 77564,
        "Shell": 46795,
        "TOML": 18128,
        "Python": 14810,
        "TypeScript": 10203,
        "JavaScript": 7526,
        "YAML": 6387,
        "Dockerfile": 4809,
        "Batchfile": 4747,
        "Ruby": 931
      },
      "license": "MIT License",
      "topics": [
        "api-server",
        "command-line-tool",
        "developer-tools",
        "gguf",
        "huggingface",
        "huggingface-models",
        "huggingface-transformers",
        "inference-server",
        "llama",
        "llamacpp",
        "llm-inference",
        "local-ai",
        "lora",
        "machine-learning",
        "ollama-api",
        "openai-compatible",
        "rust",
        "rust-crate",
        "transformers"
      ]
    },
    "content": {
      "readme": "<div align=\"center\">\r\n  <img src=\"assets/shimmy-logo.png\" alt=\"Shimmy Logo\" width=\"300\" height=\"auto\" />\r\n  \r\n  # The Privacy-First Alternative to Ollama\r\n  \r\n  ### 🔒 Local AI Without the Lock-in 🚀\r\n\r\n  [![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\r\n  [![Security](https://img.shields.io/badge/Security-Audited-green)](https://github.com/Michael-A-Kuykendall/shimmy/security)\r\n  [![Crates.io](https://img.shields.io/crates/v/shimmy.svg)](https://crates.io/crates/shimmy)\r\n  [![Downloads](https://img.shields.io/crates/d/shimmy.svg)](https://crates.io/crates/shimmy)\r\n  [![Rust](https://img.shields.io/badge/rust-stable-brightgreen.svg)](https://rustup.rs/)\r\n  [![GitHub Stars](https://img.shields.io/github/stars/Michael-A-Kuykendall/shimmy?style=social)](https://github.com/Michael-A-Kuykendall/shimmy/stargazers)\r\n  \r\n  [![💝 Sponsor this project](https://img.shields.io/badge/💝_Sponsor_this_project-ea4aaa?style=for-the-badge&logo=github&logoColor=white)](https://github.com/sponsors/Michael-A-Kuykendall)\r\n</div>\r\n\r\n**Shimmy will be free forever.** No asterisks. No \"free for now.\" No pivot to paid.\r\n\r\n### 💝 Support Shimmy's Growth\r\n\r\n🚀 **If Shimmy helps you, consider [sponsoring](https://github.com/sponsors/Michael-A-Kuykendall) — 100% of support goes to keeping it free forever.**\r\n\r\n- **$5/month**: Coffee tier ☕ - Eternal gratitude + sponsor badge  \r\n- **$25/month**: Bug prioritizer 🐛 - Priority support + name in [SPONSORS.md](SPONSORS.md)\r\n- **$100/month**: Corporate backer 🏢 - Logo placement + monthly office hours  \r\n- **$500/month**: Infrastructure partner 🚀 - Direct support + roadmap input\r\n\r\n[**🎯 Become a Sponsor**](https://github.com/sponsors/Michael-A-Kuykendall) | See our amazing [sponsors](SPONSORS.md) 🙏\r\n\r\n---\r\n\r\n## Drop-in OpenAI API Replacement for Local LLMs\r\n\r\nShimmy is a **5.1MB single-binary** that provides **100% OpenAI-compatible endpoints** for GGUF models. Point your existing AI tools to Shimmy and they just work — locally, privately, and free.\r\n\r\n## 🤔 What are you building with Shimmy?\r\n\r\n**New developer tools and specifications included!** Whether you're forking Shimmy for your application or integrating it as a service, we now provide:\r\n\r\n- **🔧 Integration Templates**: Copy-paste guidance for embedding Shimmy in your projects\r\n- **📋 Development Specifications**: GitHub Spec-Kit methodology for planning Shimmy-based features\r\n- **🛡️ Architectural Guarantees**: Constitutional principles ensuring Shimmy stays reliable and lightweight\r\n- **📖 Complete Documentation**: Everything you need to build on Shimmy's foundation\r\n\r\n**Building something cool with Shimmy?** These tools help you do it systematically and reliably.\r\n\r\n### 🚀 **GitHub Spec-Kit Integration**\r\nShimmy now includes [GitHub's brand-new Spec-Kit methodology](https://github.com/github/spec-kit) – specification-driven development that just launched in September 2025! Get professional-grade development workflows:\r\n\r\n- **🏗️ Systematic Development**: `/specify` → `/plan` → `/tasks` → implement\r\n- **🤖 AI-Native Workflow**: Works with Claude Code, GitHub Copilot, and other AI assistants  \r\n- **📋 Professional Templates**: Complete specification and planning frameworks\r\n- **🛡️ Constitutional Protection**: Built-in governance and architectural validation\r\n\r\n[**📖 Complete Developer Guide →**](DEVELOPERS.md) • [**🛠️ Learn GitHub Spec-Kit →**](https://github.com/github/spec-kit)\r\n\r\n### Try it in 30 seconds\r\n\r\n```bash\r\n# 1) Install + run\r\ncargo install shimmy --features huggingface\r\nshimmy serve &\r\n\r\n# 2) See models and pick one\r\nshimmy list\r\n\r\n# 3) Smoke test the OpenAI API\r\ncurl -s http://127.0.0.1:11435/v1/chat/completions \\\r\n  -H 'Content-Type: application/json' \\\r\n  -d '{\r\n        \"model\":\"REPLACE_WITH_MODEL_FROM_list\",\r\n        \"messages\":[{\"role\":\"user\",\"content\":\"Say hi in 5 words.\"}],\r\n        \"max_tokens\":32\r\n      }' | jq -r '.choices[0].message.content'\r\n```\r\n\r\n## 🚀 Works with Your Existing Tools\r\n\r\n**No code changes needed** - just change the API endpoint:\r\n\r\n- **VSCode Extensions**: Point to `http://localhost:11435`\r\n- **Cursor Editor**: Built-in OpenAI compatibility  \r\n- **Continue.dev**: Drop-in model provider\r\n- **Any OpenAI client**: Python, Node.js, curl, etc.\r\n\r\n### Use with OpenAI SDKs\r\n\r\n- Node.js (openai v4)\r\n\r\n```ts\r\nimport OpenAI from \"openai\";\r\n\r\nconst openai = new OpenAI({\r\n  baseURL: \"http://127.0.0.1:11435/v1\",\r\n  apiKey: \"sk-local\", // placeholder, Shimmy ignores it\r\n});\r\n\r\nconst resp = await openai.chat.completions.create({\r\n  model: \"REPLACE_WITH_MODEL\",\r\n  messages: [{ role: \"user\", content: \"Say hi in 5 words.\" }],\r\n  max_tokens: 32,\r\n});\r\n\r\nconsole.log(resp.choices[0].message?.content);\r\n```\r\n\r\n- Python (openai>=1.0.0)\r\n\r\n```python\r\nfrom openai import OpenAI\r\n\r\nclient = OpenAI(base_url=\"http://127.0.0.1:11435/v1\", api_key=\"sk-local\")\r\n\r\nresp = client.chat.completions.create(\r\n    model=\"REPLACE_WITH_MODEL\",\r\n    messages=[{\"role\": \"user\", \"content\": \"Say hi i",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-20T02:14:13.867646"
  },
  {
    "basic_info": {
      "name": "dionysus",
      "full_name": "pewdiepie-archdaemon/dionysus",
      "owner": "pewdiepie-archdaemon",
      "description": "laptop ",
      "url": "https://github.com/pewdiepie-archdaemon/dionysus",
      "clone_url": "https://github.com/pewdiepie-archdaemon/dionysus.git",
      "ssh_url": "git@github.com:pewdiepie-archdaemon/dionysus.git",
      "homepage": null,
      "created_at": "2025-08-27T22:09:28Z",
      "updated_at": "2025-09-19T20:02:19Z",
      "pushed_at": "2025-09-01T06:43:55Z"
    },
    "stats": {
      "stars": 2437,
      "forks": 81,
      "watchers": 2437,
      "open_issues": 73,
      "size": 25034
    },
    "tech_info": {
      "language": "Shell",
      "languages": {
        "Shell": 44000,
        "SCSS": 10593,
        "GLSL": 8416,
        "Python": 5585,
        "CSS": 4914
      },
      "license": null,
      "topics": []
    },
    "content": {
      "readme": "",
      "default_branch": "dionysus"
    },
    "fetched_at": "2025-09-20T02:14:14.974092"
  },
  {
    "basic_info": {
      "name": "RustGPT",
      "full_name": "tekaratzas/RustGPT",
      "owner": "tekaratzas",
      "description": "An transformer based LLM. Written completely in Rust",
      "url": "https://github.com/tekaratzas/RustGPT",
      "clone_url": "https://github.com/tekaratzas/RustGPT.git",
      "ssh_url": "git@github.com:tekaratzas/RustGPT.git",
      "homepage": null,
      "created_at": "2025-09-13T22:05:55Z",
      "updated_at": "2025-09-20T01:43:02Z",
      "pushed_at": "2025-09-17T14:27:56Z"
    },
    "stats": {
      "stars": 2408,
      "forks": 182,
      "watchers": 2408,
      "open_issues": 4,
      "size": 91
    },
    "tech_info": {
      "language": "Rust",
      "languages": {
        "Rust": 66254
      },
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "# 🦀 Rust LLM from Scratch\n\nhttps://github.com/user-attachments/assets/ec4a4100-b03a-4b3c-a7d6-806ea54ed4ed\n\nA complete **Large Language Model implementation in pure Rust** with no external ML frameworks. Built from the ground up using only `ndarray` for matrix operations.\n\n## 🚀 What This Is\n\nThis project demonstrates how to build a transformer-based language model from scratch in Rust, including:\n- **Pre-training** on factual text completion\n- **Instruction tuning** for conversational AI\n- **Interactive chat mode** for testing\n- **Full backpropagation** with gradient clipping\n- **Modular architecture** with clean separation of concerns\n\n## ❌ What This Isn't\n\nThis is not a production grade LLM. It is so far away from the larger models.\n\nThis is just a toy project that demonstrates how these models work under the hood.\n\n## 🔍 Key Files to Explore\n\nStart with these two core files to understand the implementation:\n\n- **[`src/main.rs`](src/main.rs)** - Training pipeline, data preparation, and interactive mode\n- **[`src/llm.rs`](src/llm.rs)** - Core LLM implementation with forward/backward passes and training logic\n\n## 🏗️ Architecture\n\nThe model uses a **transformer-based architecture** with the following components:\n\n```\nInput Text → Tokenization → Embeddings → Transformer Blocks → Output Projection → Predictions\n```\n\n### Project Structure\n\n```\nsrc/\n├── main.rs              # 🎯 Training pipeline and interactive mode\n├── llm.rs               # 🧠 Core LLM implementation and training logic\n├── lib.rs               # 📚 Library exports and constants\n├── transformer.rs       # 🔄 Transformer block (attention + feed-forward)\n├── self_attention.rs    # 👀 Multi-head self-attention mechanism  \n├── feed_forward.rs      # ⚡ Position-wise feed-forward networks\n├── embeddings.rs        # 📊 Token embedding layer\n├── output_projection.rs # 🎰 Final linear layer for vocabulary predictions\n├── vocab.rs            # 📝 Vocabulary management and tokenization\n├── layer_norm.rs       # 🧮 Layer normalization\n└── adam.rs             # 🏃 Adam optimizer implementation\n\ntests/\n├── llm_test.rs         # Tests for core LLM functionality\n├── transformer_test.rs # Tests for transformer blocks\n├── self_attention_test.rs # Tests for attention mechanisms\n├── feed_forward_test.rs # Tests for feed-forward layers\n├── embeddings_test.rs  # Tests for embedding layers\n├── vocab_test.rs       # Tests for vocabulary handling\n├── adam_test.rs        # Tests for optimizer\n└── output_projection_test.rs # Tests for output layer\n```\n\n## 🧪 What The Model Learns\n\nThe implementation includes two training phases:\n\n1. **Pre-training**: Learns basic world knowledge from factual statements\n   - \"The sun rises in the east and sets in the west\"\n   - \"Water flows downhill due to gravity\"\n   - \"Mountains are tall and rocky formations\"\n\n2. **Instruction Tuning**: Learns conversational patterns\n   - \"User: How do mountains form? Assistant: Mountains are formed through tectonic forces...\"\n   - Handles greetings, explanations, and follow-up questions\n\n## 🚀 Quick Start\n\n```bash\n# Clone and run\ngit clone https://github.com/tekaratzas/RustGPT.git \ncd RustGPT\ncargo run\n\n# The model will:\n# 1. Build vocabulary from training data\n# 2. Pre-train on factual statements (100 epochs)  \n# 3. Instruction-tune on conversational data (100 epochs)\n# 4. Enter interactive mode for testing\n```\n\n## 🎮 Interactive Mode\n\nAfter training, test the model interactively:\n\n```\nEnter prompt: How do mountains form?\nModel output: Mountains are formed through tectonic forces or volcanism over long geological time periods\n\nEnter prompt: What causes rain?\nModel output: Rain is caused by water vapor in clouds condensing into droplets that become too heavy to remain airborne\n```\n\n## 🧮 Technical Implementation\n\n### Model Configuration\n- **Vocabulary Size**: Dynamic (built from training data)\n- **Embedding Dimension**: 128\n- **Hidden Dimension**: 256  \n- **Max Sequence Length**: 80 tokens\n- **Architecture**: 3 Transformer blocks + embeddings + output projection\n\n### Training Details\n- **Optimizer**: Adam with gradient clipping\n- **Pre-training LR**: 0.0005 (100 epochs)\n- **Instruction Tuning LR**: 0.0001 (100 epochs)\n- **Loss Function**: Cross-entropy loss\n- **Gradient Clipping**: L2 norm capped at 5.0\n\n### Key Features\n- **Custom tokenization** with punctuation handling\n- **Greedy decoding** for text generation\n- **Gradient clipping** for training stability\n- **Modular layer system** with clean interfaces\n- **Comprehensive test coverage** for all components\n\n## 🔧 Development\n\n```bash\n# Run all tests\ncargo test\n\n# Test specific components\ncargo test --test llm_test\ncargo test --test transformer_test\ncargo test --test self_attention_test\n\n# Build optimized version\ncargo build --release\n\n# Run with verbose output\ncargo test -- --nocapture\n```\n\n## 🧠 Learning Resources\n\nThis implementation demonstrates key ML concepts:\n- **Transformer architecture** (attention, feed-forward, layer norm)\n- **Backpropagation** through ne",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-20T02:14:16.097977"
  },
  {
    "basic_info": {
      "name": "ZHO-nano-banana-Creation",
      "full_name": "ZHO-ZHO-ZHO/ZHO-nano-banana-Creation",
      "owner": "ZHO-ZHO-ZHO",
      "description": "我的 nano-banana 创意玩法大合集！  持续更新中！",
      "url": "https://github.com/ZHO-ZHO-ZHO/ZHO-nano-banana-Creation",
      "clone_url": "https://github.com/ZHO-ZHO-ZHO/ZHO-nano-banana-Creation.git",
      "ssh_url": "git@github.com:ZHO-ZHO-ZHO/ZHO-nano-banana-Creation.git",
      "homepage": null,
      "created_at": "2025-08-28T13:09:00Z",
      "updated_at": "2025-09-20T02:12:51Z",
      "pushed_at": "2025-09-18T22:04:39Z"
    },
    "stats": {
      "stars": 2377,
      "forks": 233,
      "watchers": 2377,
      "open_issues": 3,
      "size": 85
    },
    "tech_info": {
      "language": null,
      "languages": {},
      "license": "GNU General Public License v3.0",
      "topics": []
    },
    "content": {
      "readme": "<img width=\"1152\" height=\"1536\" alt=\"Mask group封面\" src=\"https://github.com/user-attachments/assets/48d727e7-7998-4d2a-aa37-59beb41ac23e\" />\n\n\n<div align=\"center\">\n   \n# Nano-Banana Creation ZHO\n   \n我的 Nano-Banana 原创玩法大合集 | [完整帖](https://x.com/ZHO_ZHO_ZHO/status/1961073677681983926) | My Nano-Banana Creation collection\n\n！！！注意标明出处哦！！！\n\n\n\n<img width=\"1032\" height=\"1373\" alt=\"Group 320\" src=\"https://github.com/user-attachments/assets/8f6ec719-1097-43f5-b56f-ef68123e2203\" />\n\n\n\n<div align=\"left\">\n\n\n# （1）目录（46项）\n\n- [1️⃣ 出圈/火的用法：图片变手办/手办视频](#1️⃣-出圈火的用法图片变手办手办视频)\n- [2️⃣ 名人/指定人物（上传图片）超写实照片级生成](#2️⃣-名人指定人物上传图片超写实照片级生成)\n- [3️⃣ 指定人物短视频：人像特征保持 + 切换视角 + veo3 首尾帧](#3️⃣-指定人物短视频人像特征保持--切换视角--veo3-首尾帧)\n- [4️⃣ 建筑图转模型/建模](#4️⃣-建筑图转模型建模)\n- [5️⃣ 连续编辑 + 物体组合 + 背景设计](#5️⃣-连续编辑--物体组合--背景设计)\n- [6️⃣ 高清修复](#6️⃣-高清修复)\n- [7️⃣ 物体组合/版本对比](#7️⃣-物体组合版本对比)\n- [8️⃣ 商品广告短片：指定人物 + 商品](#8️⃣-商品广告短片指定人物--商品)\n- [9️⃣ 人群中分离指定模糊人物 + 高清生成](#9️⃣-人群中分离指定模糊人物--高清生成)\n- [1️⃣0️⃣ 图转线稿 + 色卡上色](#1️⃣0️⃣-图转线稿--色卡上色)\n- [1️⃣1️⃣ 一句话生成一套角色设定/故事书](#1️⃣1️⃣-一句话生成一套角色设定故事书)\n- [1️⃣2️⃣ 虚实结合/跨次元：插画人物探店](#1️⃣2️⃣-虚实结合跨次元插画人物探店)\n- [1️⃣3️⃣ 指定人物 + 设计 实景体验/效果图](#1️⃣3️⃣-指定人物--设计-实景体验效果图)\n- [1️⃣4️⃣ 精准替换视频人物](#1️⃣4️⃣-精准替换视频人物)\n- [1️⃣5️⃣ 动漫转真人（接近 1:1 还原）](#1️⃣5️⃣-动漫转真人接近-11-还原)\n- [1️⃣6️⃣ 高质量摄影：指定人物 + 高质量姿势参考](#1️⃣6️⃣-高质量摄影指定人物--高质量姿势参考)\n- [1️⃣7️⃣ 图片转人偶玩具](#1️⃣7️⃣-图片转人偶玩具)\n- [1️⃣8️⃣ 图片转-funko-pop-公仔](#1️⃣8️⃣-图片转-funko-pop-公仔)\n- [1️⃣9️⃣ 图片转乐高](#1️⃣9️⃣-图片转乐高)\n- [2️⃣0️⃣ 图片转针织玩偶](#2️⃣0️⃣-图片转针织玩偶)\n- [2️⃣1️⃣ 图片转芭比娃娃](#2️⃣1️⃣-图片转芭比娃娃)\n- [2️⃣2️⃣ 万物变高达](#2️⃣2️⃣-万物变高达)\n- [2️⃣3️⃣ 赛博生娃？！两张人脸生成孩子脸](#2️⃣3️⃣-赛博生娃两张人脸生成孩子脸)\n- [2️⃣4️⃣ 产品设计图转真实效果/渲染](#2️⃣4️⃣-产品设计图转真实效果渲染)\n- [2️⃣5️⃣ 随手拍秒变专业摄影大片？！nano-banana-拯救你的废片](#2️⃣5️⃣-随手拍秒变专业摄影大片nano-banana-拯救你的废片)\n- [2️⃣6️⃣ 光影参考](#2️⃣6️⃣-光影参考)\n- [2️⃣7️⃣ 使用光影人偶做打光参考](#2️⃣7️⃣-使用光影人偶做打光参考)\n- [2️⃣8️⃣ 生成绘画/渲染过程四宫格](#2️⃣8️⃣-生成绘画渲染过程四宫格)\n- [2️⃣9️⃣ 一句话-照片变插画-还附带绘画过程](#2️⃣9️⃣-一句话-照片变插画-还附带绘画过程)\n- [3️⃣0️⃣ 脸型参考/控制，秒变卡通形象](#3️⃣0️⃣-脸型参考控制秒变卡通形象)\n- [3️⃣1️⃣ 一句咒语任何风格变写实](#3️⃣1️⃣-一句咒语任何风格变写实)\n- [3️⃣2️⃣ 曲面屏贴图](#3️⃣2️⃣-曲面屏贴图)\n- [3️⃣3️⃣ 直接为图片中的曲面大屏生成-指定的-裸眼-3d-内容](#3️⃣3️⃣-直接为图片中的曲面大屏生成-指定的-裸眼-3d-内容)\n- [3️⃣4️⃣ 任意图片（明星/动漫）变挂件挂在自己/女朋友包包上](#3️⃣4️⃣-任意图片明星动漫变挂件挂在自己女朋友包包上)\n- [3️⃣5️⃣ 叠加指定-材质质感/效果](#3️⃣5️⃣-叠加指定-材质质感效果)\n- [3️⃣6️⃣ 照片变娃娃](#3️⃣6️⃣-照片变娃娃)\n- [3️⃣7️⃣ 产品包装贴合](#3️⃣7️⃣-产品包装贴合)\n- [3️⃣8️⃣ 把指定图片贴在大阶梯上](#3️⃣8️⃣-把指定图片贴在大阶梯上)\n- [3️⃣9️⃣ 虚拟试妆化指定妆面](#3️⃣9️⃣-虚拟试妆化指定妆面)\n- [4️⃣0️⃣ 妆面分析-+-优化建议](#4️⃣0️⃣-妆面分析--优化建议)\n- [4️⃣1️⃣ 工业设计-手绘-秒变-实景效果](#4️⃣1️⃣-工业设计-手绘-秒变-实景效果)\n- [4️⃣2️⃣ 工业设计套图马克笔、水彩、分析图、渲染图](#4️⃣2️⃣-工业设计套图马克笔水彩分析图渲染图)\n- [4️⃣3️⃣ 表情准确参考动漫、真人都没问题](#4️⃣3️⃣-表情准确参考动漫真人都没问题)\n- [4️⃣4️⃣ 动物拟人表情](#4️⃣4️⃣-动物拟人表情)\n- [4️⃣5️⃣ 绝美卡片设计](#4️⃣5️⃣-绝美卡片设计)\n- [4️⃣6️⃣ 多人物插画集](#4️⃣6️⃣-多人物插画集)\n\n\n# （2）在线体验/本地部署（开源）：🍌 三件套\n\n\n- 窗口式：[1️⃣ Nano Bananary｜香蕉超市](#1️⃣-nano-bananary香蕉超市)\n\n- 白板/画布式：[2️⃣ BananaPod｜香蕉铺子](#2️⃣-bananapod香蕉铺子)\n\n- 节点/工作流式：[3️⃣ BananaFlow｜香蕉工厂](#3️⃣-bananaflow香蕉工厂)\n\n\n\n## 1️⃣ Nano Bananary｜香蕉超市\n\n<img width=\"1251\" height=\"2051\" alt=\"Group 336\" src=\"https://github.com/user-attachments/assets/51a70ae5-9f94-4dc3-88ca-8ef688d1ee5c\" />\n\n\n所有玩法已陆续上线我自己的开源 APP：Nano Bananary｜香蕉超市，无需提示词，丝滑衔接，已上线 视频生成 功能\n\n\n一键直达：[Nano Bananary｜香蕉超市](https://github.com/ZHO-ZHO-ZHO/Nano-Bananary)\n\n\n\n## 2️⃣ BananaPod｜香蕉铺子\n\n\n<img width=\"2261\" height=\"1410\" alt=\"screenshot-20250909-020322\" src=\"https://github.com/user-attachments/assets/9997b969-c773-4f7b-9c0b-dfeed5cf9e71\" />\n\n各种玩法已内置，支持手绘生图，支持多图框选，轻松构建创意画板\n\n一键直达：[BananaPod｜香蕉铺子](https://github.com/ZHO-ZHO-ZHO/BananaPod)\n\n\n\n## 3️⃣ BananaFlow｜香蕉工厂\n\n<img width=\"2667\" height=\"3518\" alt=\"Group 357\" src=\"https://github.com/user-attachments/assets/a934c87c-90a6-4628-8866-428c07eb434d\" />\n\nNanoBanana + Veo 的开源工作流创意平台，工作流 + 窗口 双模式纵享丝滑，各种玩法已内置，轻松构建创意工作流\n\n一键直达：[BananaFlow｜香蕉工厂](https://github.com/ZHO-ZHO-ZHO/BananaFlow-ZHO)\n\n\n\n# （3）原创玩法 + 提示词\n\n## 1️⃣ 出圈/火的用法：图片变手办/手办视频\n\nhttps://github.com/user-attachments/assets/6440b1eb-4ecb-4d77-8dba-63c6d61c1ad3\n\n1）变手办 Prompt：\n\n```\nturn this photo into a character figure. Behind it, place a box with the character’s image printed on it, and a computer showing the Blender modeling process on its screen. In front of the box, add a round plastic base with the character figure standing on it. set the scene indoors if possible\n```\n\n2）变视频 Prompt（Veo3）：\n\n```\nA pair of hands picks up the figurine and examines it closely\n```\n\n<details>\n<summary>我的原帖</summary>\n\n  \n[图片变手办](https://x.com/ZHO_ZHO_ZHO/status/1958539464994959715)\n\n[图片变手办优化版](https://x.com/ZHO_ZHO_ZHO/status/1959524430520135749)\n\n[手办变视频](https://x.com/ZHO_ZHO_ZHO/status/1958550998815023573)\n\n\n</details>\n\n\n\n## 2️⃣ 名人/指定人物（上传图片）超写实照片级生成\n\n\n<img width=\"894\" height=\"900\" alt=\"image\" src=\"https://github.com/user-attachments/assets/0a0fd607-66c4-4993-bd10-fe3f1b734ef0\" />\n\n\nPrompt：\n\n```\n画面采用中景近乎半身的构图，镜头与人物几乎平视，但透视感强烈，但因为主体微微前倾，视觉上产生一种略带俯视感的压缩效果，让观者与模特之间的距离显得亲密而直接。人物微微抬头冲向镜头，有种拽姐的感觉。闪光灯从正面偏左上方打来，制造出硬朗的高光与深重阴影——墨镜镜片上有明显高光反射，人物后方墙面出现淡淡的投影，整体呈现典型的“直闪”质感：颗粒感轻微，可见胶片风格或高感光度数码拍摄的粗粝纹理。人物面部稍稍有点过度曝光\n\n色彩基调以低饱和的中性色为主：黑色的宽肩丝绒西装外套占据画面最大面积，面料似乎是丝绒，带有细腻的绒面纹路；内搭的黑色丝绸吊带（搬运不标出处死妈）。模特下装是黑色超短裙和薄透质感的连裤袜。背景左侧堆放的墨绿色与咖色棒球帽、右侧叠放的亮蓝色夹克及黑色头盔等杂物，在柔和阴",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-20T02:14:17.204473"
  },
  {
    "basic_info": {
      "name": "KatelyaTV",
      "full_name": "katelya77/KatelyaTV",
      "owner": "katelya77",
      "description": "一个基于MoonTV(LunaTV)二次开发的项目。",
      "url": "https://github.com/katelya77/KatelyaTV",
      "clone_url": "https://github.com/katelya77/KatelyaTV.git",
      "ssh_url": "git@github.com:katelya77/KatelyaTV.git",
      "homepage": "https://tv.katelya.eu.org",
      "created_at": "2025-08-27T06:04:23Z",
      "updated_at": "2025-09-20T00:53:05Z",
      "pushed_at": "2025-09-05T08:21:49Z"
    },
    "stats": {
      "stars": 1807,
      "forks": 2323,
      "watchers": 1807,
      "open_issues": 48,
      "size": 29311
    },
    "tech_info": {
      "language": "TypeScript",
      "languages": {
        "TypeScript": 661598,
        "JavaScript": 58289,
        "CSS": 39181,
        "Dockerfile": 2444,
        "Shell": 328
      },
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "<div align=\"center\">\n  <img src=\"public/logo.png\" alt=\"KatelyaTV Logo\" width=\"128\" />\n\n  <h1>KatelyaTV</h1>\n  <p><strong>跨平台 · 聚合搜索 · 即开即用 · 自托管影视聚合播放器</strong></p>\n  <p>基于 <code>Next.js 14</code> · <code>TypeScript</code> · <code>Tailwind CSS</code> · 多源聚合 / 播放记录 / 收藏同步 / 跳过片头片尾 / PWA</p>\n  \n  <p>\n    <a href=\"#-快速开始\">🚀 快速开始</a> ·\n    <a href=\"#-功能特性\">✨ 功能特性</a> ·\n    <a href=\"#-部署方案\">📋 部署方案</a> ·\n    <a href=\"#-配置说明\">⚙️ 配置说明</a>\n  </p>\n</div>\n\n---\n\n## 📰 项目声明\n\n本项目自「MoonTV」演进而来，为其二创/继承版本，持续维护与改进功能与体验。保留并致谢原作者与社区贡献者。\n\n> **🔔 重要变更**：应用户社区建议，为确保项目长期稳定运行和合规性，内置视频源已移除。现需要用户自行配置资源站以使用完整功能。我们提供了经过测试的推荐配置文件，让您快速上手使用。\n\n---\n\n## ✨ 功能特性\n\n### 🎬 核心功能\n\n- **🔍 聚合搜索**：整合多个影视资源站，一键搜索全网内容\n- **📺 高清播放**：基于 ArtPlayer 的强大播放器，支持多种格式\n- **⏭️ 智能跳过**：自动检测并跳过片头片尾，手动设置跳过时间段\n- **🎯 断点续播**：自动记录播放进度，跨设备同步观看位置\n- **📱 响应式设计**：完美适配手机、平板、电脑各种屏幕\n\n### 💾 数据管理\n\n- **⭐ 收藏功能**：收藏喜欢的影视作品，支持跨设备同步\n- **📖 播放历史**：自动记录观看历史，快速找回看过的内容\n- **👥 多用户支持**：独立的用户系统，每个用户独享个人数据\n- **🔄 数据同步**：支持多种存储后端（LocalStorage、Redis、D1、Upstash）\n- **🔒 内容过滤**：智能成人内容过滤系统，默认开启安全保护\n\n### 🚀 部署特性\n\n- **🐳 Docker 一键部署**：提供完整的 Docker 镜像，开箱即用\n- **☁️ 多平台支持**：Vercel、Docker、Cloudflare Pages 全兼容\n- **🔧 灵活配置**：支持自定义资源站、代理设置、主题配置\n- **📱 PWA 支持**：可安装为桌面/手机应用\n- **📺 TVBox 兼容**：支持 TVBox 配置接口\n\n---\n\n## 🚀 快速开始\n\n### 💡 方案选择指南\n\n| 使用场景     | 推荐方案         | 存储类型     | 成人内容过滤 | 多用户 | 部署难度 |\n| ------------ | ---------------- | ------------ | ------------ | ------ | -------- |\n| **个人使用** | Docker 单容器    | localstorage | ❌           | ❌     | ⭐       |\n| **家庭使用** | Docker + Redis   | redis        | ✅           | ✅     | ⭐⭐     |\n| **免费部署** | Vercel + Upstash | upstash      | ✅           | ✅     | ⭐⭐⭐   |\n| **生产环境** | Docker + Kvrocks | kvrocks      | ✅           | ✅     | ⭐⭐     |\n| **全球加速** | Cloudflare Pages | d1           | ✅           | ✅     | ⭐⭐⭐⭐ |\n\n> 💡 **重要提示**：成人内容过滤功能需要数据库存储支持，不支持 `localstorage` 方式\n\n---\n\n## 📋 部署方案\n\n### 方案一：Docker 单容器（最简单）\n\n**特点**：5 分钟部署，个人使用，无多用户功能\n\n```bash\ndocker run -d \\\n  --name katelyatv \\\n  -p 3000:3000 \\\n  -e PASSWORD=your_password \\\n  --restart unless-stopped \\\n  ghcr.io/katelya77/katelyatv:latest\n```\n\n**挂载自定义配置**（可选）：\n\n```bash\ndocker run -d \\\n  --name katelyatv \\\n  -p 3000:3000 \\\n  -e PASSWORD=your_password \\\n  -v $(pwd)/config.json:/app/config.json:ro \\\n  --restart unless-stopped \\\n  ghcr.io/katelya77/katelyatv:latest\n```\n\n### 方案二：Docker + Redis（推荐家庭使用）\n\n**特点**：完整功能，多用户支持，成人内容过滤\n\n```bash\n# 1. 下载配置文件\ncurl -O https://raw.githubusercontent.com/katelya77/KatelyaTV/main/docker-compose.redis.yml\ncurl -O https://raw.githubusercontent.com/katelya77/KatelyaTV/main/.env.redis.example\n\n# 2. 配置环境变量\ncp .env.redis.example .env\n```\n\n**编辑 .env 文件**：\n\n```bash\n# 管理员账号（必填）\nUSERNAME=admin\nPASSWORD=your_secure_password\n\n# 存储配置\nNEXT_PUBLIC_STORAGE_TYPE=redis\nREDIS_URL=redis://katelyatv-redis:6379\n\n# 功能开关\nNEXT_PUBLIC_ENABLE_REGISTER=true\n```\n\n```bash\n# 3. 启动服务\ndocker compose -f docker-compose.redis.yml up -d\n```\n\n### 方案三：Docker + Kvrocks（生产环境）\n\n**特点**：极高可靠性，数据持久化到磁盘，节省内存\n\n```bash\n# 1. 下载配置文件\ncurl -O https://raw.githubusercontent.com/katelya77/KatelyaTV/main/docker-compose.kvrocks.yml\ncurl -O https://raw.githubusercontent.com/katelya77/KatelyaTV/main/.env.kvrocks.example\n\n# 2. 配置环境变量\ncp .env.kvrocks.example .env\n```\n\n**编辑 .env 文件**：\n\n```bash\n# 管理员账号（必填，否则无法登录）\nUSERNAME=admin\nPASSWORD=your_secure_password\n\n# 存储配置\nNEXT_PUBLIC_STORAGE_TYPE=kvrocks\nKVROCKS_URL=redis://kvrocks:6666\n\n# 功能开关\nNEXT_PUBLIC_ENABLE_REGISTER=true\n```\n\n```bash\n# 3. 启动服务\ndocker compose -f docker-compose.kvrocks.yml up -d\n```\n\n### 方案四：Vercel + Upstash（免费推荐）\n\n**特点**：完全免费，自动 HTTPS，全球 CDN\n\n#### 基础部署\n\n1. **Fork 项目** → [GitHub 仓库](https://github.com/katelya77/KatelyaTV)\n2. **部署到 Vercel**：\n   - 登录 [Vercel](https://vercel.com/)\n   - 导入刚 Fork 的仓库\n   - 添加环境变量：`PASSWORD=your_password`\n   - 点击 Deploy\n\n#### 多用户配置\n\n3. **创建 Upstash 数据库**：\n\n   - 访问 [Upstash](https://upstash.com/)\n   - 创建免费 Redis 数据库\n   - 获取 `UPSTASH_URL` 和 `UPSTASH_TOKEN`\n\n4. **添加环境变量**：\n\n```bash\n# 存储配置\nNEXT_PUBLIC_STORAGE_TYPE=upstash\nUPSTASH_URL=https://xxx.upstash.io\nUPSTASH_TOKEN=your_token\n\n# 管理员账号\nUSERNAME=admin\nPASSWORD=your_password\n\n# 功能开关\nNEXT_PUBLIC_ENABLE_REGISTER=true\n```\n\n5. **重新部署** → Vercel Dashboard → Redeploy\n\n### 方案五：Cloudflare Pages + D1（全球加速）\n\n**特点**：全球 CDN，无限带宽，免费 SSL\n\n#### 快速部署\n\n1. **Fork 项目** → [GitHub 仓库](https://github.com/katelya77/KatelyaTV)\n2. **创建 Pages 项目**：\n\n   - 登录 [Cloudflare Dashboard](https://dash.cloudflare.com/)\n   - Pages → Connect to Git → 选择仓库\n   - 构建设置：\n     ```\n     Build command: pnpm install --frozen-lockfile && pnpm run pages:build\n     Build output directory: .vercel/output/static\n     ```\n   - 兼容性标志：`nodejs_compat`\n\n3. **环境变量配置**：\n\n```bash\n# 管理员账号\nUSERNAME=admin\nPASSWORD=your_password\n\n# 存储配置\nNEXT_PUBLIC_STORAGE_TYPE=d1\n\n# 功能开关\nNEXT_PUBLIC_ENABLE_REGISTER=true\n```\n\n4. **创建 D1 数据库**（多用户支持）：\n\n```bash\n# 安装Wrangler CLI\nnpm install -g wrangler\nwrangler auth login\n\n# 创建数据库\nwrangler d1 create katelyatv-db\n# ⚠️ 重要：确保在项目根目录下运行此命令\n# 如果遇到文件路径错误，请参考 D1_MIGRATION.md 排查指南\nwrangler d1 execute katelyatv-db --file=./scripts/d1-init.sql\n```\n\n5. **配置数据库绑定** → 在 `wr",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-20T02:14:18.307416"
  },
  {
    "basic_info": {
      "name": "linear-algebra",
      "full_name": "little-book-of/linear-algebra",
      "owner": "little-book-of",
      "description": "There is hardly any theory which is more elementary than linear algebra, in spite of the fact that generations of professors and textbook writers have obscured its simplicity by preposterous calculations with matrices. —Jean Dieudonne",
      "url": "https://github.com/little-book-of/linear-algebra",
      "clone_url": "https://github.com/little-book-of/linear-algebra.git",
      "ssh_url": "git@github.com:little-book-of/linear-algebra.git",
      "homepage": "",
      "created_at": "2025-09-02T03:28:30Z",
      "updated_at": "2025-09-19T19:28:03Z",
      "pushed_at": "2025-09-04T07:55:49Z"
    },
    "stats": {
      "stars": 1671,
      "forks": 52,
      "watchers": 1671,
      "open_issues": 3,
      "size": 1159
    },
    "tech_info": {
      "language": "TeX",
      "languages": {
        "TeX": 159618
      },
      "license": "Other",
      "topics": []
    },
    "content": {
      "readme": "# The Little Book of Linear Algebra\n\nA concise, beginner-friendly introduction to the core ideas of linear algebra.\n\n## Formats\n\n- [Download PDF](book.pdf) – print-ready version\n- [Download EPUB](book.epub) – e-reader friendly\n- [View LaTeX](book.tex) – Latex source\n\n# Chapter 1. Vectors\n\n## 1.1 Scalars and Vectors\n\nA scalar is a single numerical quantity, most often taken from the real numbers, denoted by $\\mathbb{R}$. Scalars are\nthe fundamental building blocks of arithmetic: they can be added, subtracted, multiplied, and, except in the case of\nzero, divided. In linear algebra, scalars play the role of coefficients, scaling factors, and entries of larger\nstructures such as vectors and matrices. They provide the weights by which more complex objects are measured and\ncombined. A vector is an ordered collection of scalars, arranged either in a row or a column. When the scalars are real\nnumbers, the vector is said to belong to *real* $n$-dimensional space, written\n\n$$\n\\mathbb{R}^n = \\{ (x_1, x_2, \\dots, x_n) \\mid x_i \\in \\mathbb{R} \\}.\n$$\n\nAn element of $\\mathbb{R}^n$ is called a vector of dimension $n$ or an *n*-vector. The number $n$ is called the\ndimension of the vector space. Thus $\\mathbb{R}^2$ is the space of all ordered pairs of real numbers, $\\mathbb{R}^3$ the\nspace of all ordered triples, and so on.\n\nExample 1.1.1.\n\n- A 2-dimensional vector: $(3, -1) \\in \\mathbb{R}^2$.\n- A 3-dimensional vector: $(2, 0, 5) \\in \\mathbb{R}^3$.\n- A 1-dimensional vector: $(7) \\in \\mathbb{R}^1$, which corresponds to the scalar \\$7$ itself.\n\nVectors are often written vertically in column form, which emphasizes their role in matrix multiplication:\n\n$$\n\\mathbf{v} = \\begin{bmatrix}\n2 \\\\\n0 \\\\\n5 \\end{bmatrix} \\in \\mathbb{R}^3.\n$$\n\nThe vertical layout makes the structure clearer when we consider linear combinations or multiply matrices by vectors.\n\n### Geometric Interpretation\n\nIn $\\mathbb{R}^2$, a vector $(x_1, x_2)$ can be visualized as an arrow starting at the origin $(0,0)$ and ending at the\npoint $(x_1, x_2)$. Its length corresponds to the distance from the origin, and its orientation gives a direction in the\nplane. In $\\mathbb{R}^3$, the same picture extends into three dimensions: a vector is an arrow from the origin\nto $(x_1, x_2, x_3)$. Beyond three dimensions, direct visualization is no longer possible, but the algebraic rules of\nvectors remain identical. Even though we cannot draw a vector in $\\mathbb{R}^{10}$, it behaves under addition, scaling,\nand transformation exactly as a 2- or 3-dimensional vector does. This abstract point of view is what allows linear\nalgebra to apply to data science, physics, and machine learning, where data often lives in very high-dimensional spaces.\nThus a vector may be regarded in three complementary ways:\n\n1. As a point in space, described by its coordinates.\n2. As a displacement or arrow, described by a direction and a length.\n3. As an abstract element of a vector space, whose properties follow algebraic rules independent of geometry.\n\n### Notation\n\n- Vectors are written in boldface lowercase letters: $\\mathbf{v}, \\mathbf{w}, \\mathbf{x}$.\n- The *i*-th entry of a vector $\\mathbf{v}$ is written $v_i$, where indices begin at 1.\n- The set of all *n*-dimensional vectors over $\\mathbb{R}$ is denoted $\\mathbb{R}^n$.\n- Column vectors will be the default form unless otherwise stated.\n\n### Why begin here?\n\nScalars and vectors form the atoms of linear algebra. Every structure we will build-vector spaces, linear\ntransformations, matrices, eigenvalues-relies on the basic notions of number and ordered collection of numbers. Once\nvectors are understood, we can define operations such as addition and scalar multiplication, then generalize to\nsubspaces, bases, and coordinate systems. Eventually, this framework grows into the full theory of linear algebra, with\npowerful applications to geometry, computation, and data.\n\n### Exercises 1.1\n\n1. Write three different vectors in $\\mathbb{R}^2$ and sketch them as arrows from the origin. Identify their coordinates\n   explicitly.\n2. Give an example of a vector in $\\mathbb{R}^4$. Can you visualize it directly? Explain why high-dimensional\n   visualization is challenging.\n3. Let $\\mathbf{v} = (4, -3, 2)$. Write $\\mathbf{v}$ in column form and state $v_1, v_2, v_3$.\n4. In what sense is the set $\\mathbb{R}^1$ both a line and a vector space? Illustrate with examples.\n5. Consider the vector $\\mathbf{u} = (1,1,\\dots,1) \\in \\mathbb{R}^n$. What is special about this vector when $n$ is\n   large? What might it represent in applications?\n\n## 1.2 Vector Addition and Scalar Multiplication\n\nVectors in linear algebra are not static objects; their power comes from the operations we can perform on them. Two\nfundamental operations define the structure of vector spaces: addition and scalar multiplication. These operations\nsatisfy simple but far-reaching rules that underpin the entire subject.\n\n### Vector Addition\n\nGiven two vectors of the same dimension, their sum is obtained by adding",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-20T02:14:19.453950"
  },
  {
    "basic_info": {
      "name": "port-kill",
      "full_name": "kagehq/port-kill",
      "owner": "kagehq",
      "description": "Port Kill helps you find and free ports blocking your dev work.",
      "url": "https://github.com/kagehq/port-kill",
      "clone_url": "https://github.com/kagehq/port-kill.git",
      "ssh_url": "git@github.com:kagehq/port-kill.git",
      "homepage": "https://portkill.com",
      "created_at": "2025-08-24T02:58:20Z",
      "updated_at": "2025-09-20T01:10:18Z",
      "pushed_at": "2025-09-19T16:09:36Z"
    },
    "stats": {
      "stars": 1495,
      "forks": 35,
      "watchers": 1495,
      "open_issues": 1,
      "size": 12767
    },
    "tech_info": {
      "language": "Rust",
      "languages": {
        "Rust": 336933,
        "Vue": 156718,
        "TypeScript": 92260,
        "Shell": 23719,
        "JavaScript": 12794,
        "Batchfile": 10726,
        "CSS": 2100
      },
      "license": "Other",
      "topics": []
    },
    "content": {
      "readme": "# Port Kill\n\nPort Kill helps you find and free ports blocking your dev work. It works on macOS, Linux, and Windows, locally or over SSH with a simple CLI, status bar and an optional dashboard.\n\n![Port Kill Status Bar Icon](image-short.png)\n\n## Community & Support\n\nJoin our Discord community for discussions, support, and updates:\n\n[![Discord](https://img.shields.io/badge/Discord-Join%20our%20community-7289DA?style=for-the-badge&logo=discord&logoColor=white)](https://discord.gg/KqdBcqRk5E)\n\n\n## Install\n\n```bash\n# macOS/Linux (when releases are available)\ncurl -fsSL https://raw.githubusercontent.com/kagehq/port-kill/main/install-release.sh | bash\n\n# Windows (PowerShell)\nInvoke-WebRequest -Uri 'https://raw.githubusercontent.com/kagehq/port-kill/main/install-release.bat' -OutFile 'install-release.bat'; .\\install-release.bat\n\n# From source\ngit clone https://github.com/kagehq/port-kill.git && cd port-kill && ./install.sh --all\n```\n\n## Quick start (CLI)\n\n```bash\n# See what's using common dev ports\n./target/release/port-kill-console --console --ports 3000,8000,8080\n\n# Scan port ranges (new!)\n./target/release/port-kill-console --console --json --ports '6000-9999'\n\n# Mixed individual ports and ranges\n./target/release/port-kill-console --console --ports '3000,6000-6002,8000'\n\n# Free up the usual suspects\n./target/release/port-kill-console --reset\n\n# Remote over SSH\n./target/release/port-kill-console --remote user@host --ports 3000,8000\n\n# Guard mode (watch + auto-resolve)\n./target/release/port-kill-console --guard-mode --auto-resolve\n\n# Security audit (JSON)\n./target/release/port-kill-console --audit --json\n\n# Endpoint monitoring (send data to external endpoint)\n./target/release/port-kill-console --monitor-endpoint https://api.company.com/port-status\n```\n\n## Dashboard (optional)\n\n![Port Kill Dashboard](dashboard/assets/img/portkill-dashboard.png)\n\n```bash\ncd dashboard\nnpm install\nnpm run dev   # http://localhost:3000\n```\n\n## MCP (use Port Kill from Cursor, Claude etc.)\n\nAdd `npx -y 'https://gitpkg.vercel.app/kagehq/port-kill/mcp?main'` to your MCP config.\n\nFor example for Cursor add to `.cursor/mcp.json`:\n```\n{\n   \"mcpServers\": {\n      \"port-kill-mcp\": {\n         \"command\": \"npx\",\n         \"args\": [\"-y\", \"https://gitpkg.vercel.app/kagehq/port-kill/mcp?main\"]\n      }\n   }\n}\n```\n\nNotes:\n- The server shells out to `./target/release/port-kill-console` or `port-kill-console` if it is on the PATH. If yours lives elsewhere, set `PORT_KILL_BIN=/absolute/path/to/port-kill-console`. e.g.\n```\n{\n   \"mcpServers\": {\n      \"port-kill-mcp\": {\n         \"command\": \"npx\",\n         \"args\": [\"-y\", \"https://gitpkg.vercel.app/kagehq/port-kill/mcp?main\"],\n         \"env\": {\n            \"PORT_KILL_BIN\": \"/absolute/path/to/port-kill-console\"\n         }\n      }\n   }\n}\n```\n\nSee [mcp/README.md](mcp/README.md) for more information on port-kill-mcp including how to install from source.\n\n## Features\n\n- Real‑time process detection on specific ports or ranges\n- One‑shot cleanup: `--reset`\n- Smart filtering and ignore lists\n- Port Guard Mode (watch/reserve/auto‑resolve)\n- Security Audit Mode (suspicious ports, risk score, JSON)\n- Remote Mode over SSH\n- Works with Docker; console mode works everywhere\n\n## Common flags\n\n```bash\n--ports 3000,8000,8080          # specific ports\n--ports '6000-9999'             # port ranges (new!)\n--ports '3000,6000-6002,8000'   # mixed individual ports and ranges\n--start-port 3000 --end-port 9000\n--ignore-ports 5353,5000,7000\n--ignore-processes Chrome,rapportd\n--guard-mode --auto-resolve\n--audit --json\n--remote user@server\n```\n\n\n### Manual Installation\n\n1. Clone the repository:\n```bash\ngit clone <repository-url>\ncd port-kill\n```\n\n2. Install and build (recommended):\n```bash\n./install.sh\n```\n\nOr manually:\n```bash\n./build-macos.sh\n./run.sh\n```\n\n### Linux Installation\n\n1. Clone the repository:\n```bash\ngit clone <repository-url>\ncd port-kill\n```\n\n2. Install required packages:\n```bash\n# Ubuntu/Debian\nsudo apt-get install libatk1.0-dev libgdk-pixbuf2.0-dev libgtk-3-dev libappindicator3-dev\n\n# Fedora/RHEL\nsudo dnf install atk-devel gdk-pixbuf2-devel gtk3-devel libappindicator-gtk3-devel\n\n# Arch Linux\nsudo pacman -S atk gdk-pixbuf2 gtk3 libappindicator-gtk3\n```\n\n3. Install and build (recommended):\n```bash\n./install.sh\n```\n\nOr manually:\n```bash\n./build-linux.sh\n./run-linux.sh\n```\n\n### Windows Installation\n\n1. Clone the repository:\n```bash\ngit clone <repository-url>\ncd port-kill\n```\n\n2. Install Rust (if not already installed):\n```bash\n# Download and run rustup-init.exe from https://rustup.rs/\n```\n\n3. Install and build (recommended):\n```bash\n./install.sh\n```\n\nOr manually:\n```bash\nbuild-windows.bat\nrun-windows.bat\n```\n\n## Usage\n\n### Basic Usage\n\n**Platform-Specific Run Scripts:**\n- **macOS**: Use `./run.sh` \n- **Linux**: Use `./run-linux.sh`\n- **Windows**: Use `run-windows.bat`\n\n1. **Start the Application**: Run the appropriate script for your platform with default settings (ports 2000-6000)\n2. **Monitor Status**: Check the status bar fo",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-20T02:14:20.634133"
  },
  {
    "basic_info": {
      "name": "vimmaster",
      "full_name": "renzorlive/vimmaster",
      "owner": "renzorlive",
      "description": "VIM Master: in-browser game that teaches core Vim motions and editing commands through short, focused levels. ",
      "url": "https://github.com/renzorlive/vimmaster",
      "clone_url": "https://github.com/renzorlive/vimmaster.git",
      "ssh_url": "git@github.com:renzorlive/vimmaster.git",
      "homepage": "https://vim-master.com",
      "created_at": "2025-08-27T14:53:51Z",
      "updated_at": "2025-09-19T09:56:25Z",
      "pushed_at": "2025-09-02T18:11:17Z"
    },
    "stats": {
      "stars": 1322,
      "forks": 54,
      "watchers": 1322,
      "open_issues": 5,
      "size": 16027
    },
    "tech_info": {
      "language": "JavaScript",
      "languages": {
        "JavaScript": 204630,
        "HTML": 28685,
        "CSS": 8117
      },
      "license": "MIT License",
      "topics": [
        "coding",
        "education",
        "game",
        "indie-game",
        "vim"
      ]
    },
    "content": {
      "readme": "# VIM Master\n\nVIM Master -- in-browser game that teaches core Vim motions and editing commands through short, focused levels. \n\n## Try the Online Demo\n[![Demo Online](https://img.shields.io/badge/demo-online-brightgreen?logo=github&style=for-the-badge)](https://renzorlive.github.io/vimmaster/)\n\n> 💡 **Tip:** For the best experience, use a desktop/laptop (full keyboard support).\n\n## Screenshot\n![VIM Master Screenshot](images/vm.gif)\n\n## Features\n- **Normal/Insert modes** with an on-screen status bar\n- **Command log** showing your keystrokes\n- **16 Progressive Levels** that validate your action outcomes (not just keystrokes)\n- **Complete Vim Support**: `h j k l`, `w b e`, `gg G`, `0 $`, `x`, `dd`, `dw`, `yy`, `p`, `i`, `a`, `o/O`, `cw`, `D`, `r`, ex-commands `:q`, `:wq`\n- **Numeric counts** for motions/operators (e.g., `3w`, `2dd`, `5x`, `5G`)\n- **Undo/redo support** (`u`, `Ctrl+r`)\n- **Vim-style search**: `/` and `?`, with `n`/`N` navigation and match highlighting\n- **Challenge Mode**: Fast-paced timed challenges to test your Vim skills\n- **Cheat Mode**: Interactive command reference with instant practice sessions\n- **Progress Management**: Auto-save, export/import codes, and progress tracking\n- **Badge System**: Earn visual badges as you learn (Beginner, Search Master)\n- **Profile Page**: Beautiful showcase of achievements with social media sharing\n- **Canvas-Based Achievement Cards**: Generate downloadable and shareable images\n- **Modular Architecture**: Clean, maintainable codebase for easy development\n\n## Recent UI/UX Improvements\n- **Streamlined Layout**: ASCII logo at top, title under text editor, buttons above instructions\n- **Compact Achievements**: Achievements container positioned before instructions for better flow\n- **Collapsible Progress Management**: Click to expand/collapse progress information, reducing UI clutter\n- **Responsive Design**: Optimized layout for better focus on gameplay elements\n- **Auto-Focus Editor**: Editor automatically focuses when lessons start for seamless UX\n- **Challenge Points Integration**: Challenge points now properly tracked and displayed in progress summary\n- **Enhanced Cheat Mode**: All cheat mode lessons now work with proper auto-focus and completion tracking\n\n## Latest Bug Fixes & Improvements\n- **Fixed \"undefined challenge points\"**: Challenge points now properly display in progress summary\n- **Enhanced Cheat Mode**: All practice lessons now auto-focus and validate completion correctly\n- **Improved Challenge Mode**: Better validation and scoring system for challenge tasks\n- **Auto-Focus UX**: Editor automatically focuses when starting lessons for better user experience\n- **Progress System**: Robust error handling and fallback values for all progress data\n\n## Progress Management System\nVIM Master features a comprehensive client-side progress tracking system that works entirely in your browser:\n\n### **Features**\n- **Auto-save**: Progress automatically saved every 5 seconds and after earning badges\n- **Export/Import Codes**: Generate compact Base64-encoded progress codes for backup and sharing\n- **Local Storage**: Progress persists between browser sessions\n- **Progress Summary**: Real-time display of current level, badges earned, and commands practiced\n- **Clear Progress**: Reset all progress with a single click\n\n### **How It Works**\n1. **Export Progress**: Click \"Export Progress\" to generate a shareable code\n2. **Import Progress**: Paste a code and click \"Import Progress\" to restore your game state\n3. **Progress Codes**: Compact, shareable strings containing all your achievements and progress\n4. **Privacy First**: All data stays on your device - no accounts or backend required\n\n### **Progress Data Tracked**\n- Current level and challenge mode status\n- Earned badges (Beginner, Search Master, etc.)\n- Commands practiced during gameplay\n- Challenge points earned from challenge mode\n- Timestamp of last save\n\n## Profile Page & Social Sharing\nShowcase your VIM mastery journey with a beautiful profile page and share achievements on social media.\n\n### **Profile Page Features**\n- **Beautiful Achievement Cards**: Eye-catching cards for each earned badge\n- **Progress Overview**: Visual representation of your learning journey with circular progress indicators\n- **ASCII Logo Integration**: The iconic VIM Master logo prominently displayed\n- **Social Media Integration**: Share achievements on Twitter, Facebook, and other platforms\n- **Progress Code Management**: Copy and share your progress codes easily\n- **GitHub Integration**: Links to view source code and contribute to the project\n\n### **Canvas-Based Achievement Cards**\n- **Dynamic Image Generation**: Create custom achievement cards using HTML5 Canvas\n- **Downloadable Images**: Save achievement cards as PNG files for sharing\n- **Social Media Ready**: Optimized dimensions and styling for social platforms\n- **Custom Branding**: Features the VIM Master ASCII logo and your progress data\n- **Professional Design**: Beautiful blue/pu",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-20T02:14:21.752958"
  },
  {
    "basic_info": {
      "name": "semantic-router",
      "full_name": "vllm-project/semantic-router",
      "owner": "vllm-project",
      "description": "Intelligent Mixture-of-Models Router for Efficient LLM Inference",
      "url": "https://github.com/vllm-project/semantic-router",
      "clone_url": "https://github.com/vllm-project/semantic-router.git",
      "ssh_url": "git@github.com:vllm-project/semantic-router.git",
      "homepage": "https://vllm-semantic-router.com",
      "created_at": "2025-08-26T21:49:50Z",
      "updated_at": "2025-09-20T01:01:30Z",
      "pushed_at": "2025-09-19T21:13:54Z"
    },
    "stats": {
      "stars": 1265,
      "forks": 122,
      "watchers": 1265,
      "open_issues": 56,
      "size": 5556
    },
    "tech_info": {
      "language": "Go",
      "languages": {
        "Go": 667927,
        "Python": 644711,
        "Rust": 189496,
        "JavaScript": 85364,
        "CSS": 63646,
        "Shell": 57681,
        "Makefile": 23181,
        "Dockerfile": 1176
      },
      "license": "Apache License 2.0",
      "topics": [
        "ai-gateway",
        "bert-classification",
        "envoy-ext-proc",
        "envoyproxy",
        "fine-tuning",
        "golang",
        "huggingface-candle",
        "huggingface-transformers",
        "kubernetes",
        "llm-tool-call",
        "mixture-of-models",
        "pii-detection",
        "prompt-guard",
        "python",
        "rust",
        "semantic-router",
        "vllm"
      ]
    },
    "content": {
      "readme": "<div align=\"center\">\n\n<img src=\"website/static/img/repo.png\" alt=\"vLLM Semantic Router\" width=\"80%\"/>\n\n[![Documentation](https://img.shields.io/badge/docs-read%20the%20docs-blue)](https://vllm-semantic-router.com)\n[![Hugging Face](https://img.shields.io/badge/🤗%20Hugging%20Face-Community-yellow)](https://huggingface.co/LLM-Semantic-Router)\n[![License](https://img.shields.io/badge/license-Apache%202.0-blue.svg)](LICENSE)\n[![Crates.io](https://img.shields.io/crates/v/candle-semantic-router.svg)](https://crates.io/crates/candle-semantic-router)\n![Test And Build](https://github.com/vllm-project/semantic-router/workflows/Test%20And%20Build/badge.svg)\n\n**📚 [Complete Documentation](https://vllm-semantic-router.com) | 🚀 [Quick Start](https://vllm-semantic-router.com/docs/getting-started/installation) | 📣 [Blog](https://vllm-semantic-router.com/blog/) | 📖 [API Reference](https://vllm-semantic-router.com/docs/api/router/)**\n\n![code](./website/static/img/code.png)\n\n</div>\n\n## Innovations ✨\n\n![architecture](./website/static/img/architecture.png)\n\n### Intelligent Routing 🧠\n\n#### Auto-Reasoning and Auto-Selection of Models\n\nAn **Mixture-of-Models** (MoM) router that intelligently directs OpenAI API requests to the most suitable models from a defined pool based on **Semantic Understanding** of the request's intent (Complexity, Task, Tools).\n\nThis is achieved using BERT classification. Conceptually similar to Mixture-of-Experts (MoE) which lives *within* a model, this system selects the best *entire model* for the nature of the task.\n\nAs such, the overall inference accuracy is improved by using a pool of models that are better suited for different types of tasks:\n\n![Model Accuracy](./website/static/img/category_accuracies.png)\n\nThe screenshot below shows the LLM Router dashboard in Grafana.\n\n![LLM Router Dashboard](./website/static/img/grafana_screenshot.png)\n\nThe router is implemented in two ways: \n\n- Golang (with Rust FFI based on the [candle](https://github.com/huggingface/candle) rust ML framework)\n- Python\nBenchmarking will be conducted to determine the best implementation.\n\n#### Auto-Selection of Tools\n\nSelect the tools to use based on the prompt, avoiding the use of tools that are not relevant to the prompt so as to reduce the number of prompt tokens and improve tool selection accuracy by the LLM.\n\n### Enterprise Security 🔒\n\n#### PII detection\n\nDetect PII in the prompt, avoiding sending PII to the LLM so as to protect the privacy of the user.\n\n#### Prompt guard\n\nDetect if the prompt is a jailbreak prompt, avoiding sending jailbreak prompts to the LLM so as to prevent the LLM from misbehaving.\n\n### Similarity Caching ⚡️\n\nCache the semantic representation of the prompt so as to reduce the number of prompt tokens and improve the overall inference latency.\n\n## Documentation 📖\n\nFor comprehensive documentation including detailed setup instructions, architecture guides, and API references, visit:\n\n**👉 [Complete Documentation at Read the Docs](https://vllm-semantic-router.com/)**\n\nThe documentation includes:\n\n- **[Installation Guide](https://vllm-semantic-router.com/docs/getting-started/installation/)** - Complete setup instructions\n- **[System Architecture](https://vllm-semantic-router.com/docs/architecture/system-architecture/)** - Technical deep dive\n- **[Model Training](https://vllm-semantic-router.com/docs/training/training-overview/)** - How classification models work\n- **[API Reference](https://vllm-semantic-router.com/docs/api/router/)** - Complete API documentation\n\n## Community 👋\n\nFor questions, feedback, or to contribute, please join `#semantic-router` channel in vLLM Slack.\n\n## Citation\n\nIf you find Semantic Router helpful in your research or projects, please consider citing it:\n\n```\n@misc{semanticrouter2025,\n  title={vLLM Semantic Router},\n  author={vLLM Semantic Router Team},\n  year={2025},\n  howpublished={\\url{https://github.com/vllm-project/semantic-router}},\n}\n```\n\n## Star History 🔥\n\nWe opened the project at Aug 31, 2025. We love open source  and collaboration ❤️\n\n[![Star History Chart](https://api.star-history.com/svg?repos=vllm-project/semantic-router&type=Date)](https://www.star-history.com/#vllm-project/semantic-router&Date)\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-20T02:14:22.851886"
  },
  {
    "basic_info": {
      "name": "multi-agent-coding-system",
      "full_name": "Danau5tin/multi-agent-coding-system",
      "owner": "Danau5tin",
      "description": "Reached #13 on Stanford's Terminal Bench leaderboard. Orchestrator, explorer & coder agents working together with intelligent context sharing.",
      "url": "https://github.com/Danau5tin/multi-agent-coding-system",
      "clone_url": "https://github.com/Danau5tin/multi-agent-coding-system.git",
      "ssh_url": "git@github.com:Danau5tin/multi-agent-coding-system.git",
      "homepage": "",
      "created_at": "2025-08-30T19:23:45Z",
      "updated_at": "2025-09-20T01:16:31Z",
      "pushed_at": "2025-09-07T16:01:23Z"
    },
    "stats": {
      "stars": 1183,
      "forks": 145,
      "watchers": 1183,
      "open_issues": 2,
      "size": 849
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 139397,
        "Shell": 483
      },
      "license": null,
      "topics": []
    },
    "content": {
      "readme": "# 🤓 Orchestrator: A multi-agent AI coder. Reached #13 on Stanford's TerminalBench. Open sourced!\n\nTL;DR:\n- Over the weekend, quite unexpectedly, I made a multi-agent AI system that places slightly higher than Claude Code on Stanford's TerminalBench leaderboard (13th place).\n- This AI system consists of an orchestration agent that dispatches multiple explorer and coder agents to do all the work.\n- The orchestrator explicitly defines what knowledge artifacts subagents must return, then reuses and synthesises these artifacts across future tasks - creating compound intelligence where each action builds meaningfully on previous discoveries.\n\n![Orchestrator with claude-sonnet-4 on standford's terminal bench](./readme_imgs/orchestrator-sonnet-4-stanford-terminal-bench-leaderboard.png)\n\n## How the System Works\n\n![System architecture overview](readme_imgs/orch_agent_sys_arch.png)\n\nThe orchestrator acts as the brain of the operation - it receives the user's task but never touches code directly. Instead, it:\n\n1. **Analyses** the task and breaks it into focused subtasks\n2. **Dispatches** explorer agents to understand the system\n3. **Delegates** implementation work to coder agents with precise instructions\n4. **Verifies** all changes through additional explorer agents\n5. **Maintains** the context store with all discovered knowledge\n\nThe orchestrator's lack of direct code access forces proper delegation and verification patterns, leading to more strategic solutions.\n\nFor a full breakdown of this project's code structure, see [here](./PROJECT_STRUCTURE.md)\n\n\n## 📈 Evaluation Results\n\n### Performance on TerminalBench\n\n[Terminal bench](https://www.tbench.ai/) is a brilliant benchmark created by Stanford and [Laude Institute](https://www.laude.org/) to quantify agents' ability to complete complex tasks in the terminal. My Orchestrator system achieved **13th place** on the leaderboard, demonstrating competitive performance against leading AI coding assistants.\n\nI ran the Orchestrator evaluations with both Claude-4-Sonnet and also Qwen3-Coder-480B-A35B:\n\n![Performance comparison chart](readme_imgs/perf_chart.png)\n![Orchestrator with qwen-3-coder on standford's terminal bench](./readme_imgs/orchestrator-qwen-3-coder-stanford-terminal-bench-leaderboard.png)\n\nThis image shows Qwen-3-Coder performance on the benchmark. The screenshot towards the top of this README shows Sonnet-4 performance.\n\n### Cost & Efficiency\n\nOne of the most striking results is the amount of tokens used by Sonnet-4 as opposed to Qwen3-Coder.\n\nThe below table shows the total tokens (input and output included) processed across the TerminalBench evaluation run (5 attempts at 80 tasks = 400 trajectories).\n\n| Model | Success Rate | Total Evaluation Cost | Token Usage |\n|-------|--------------|------------|-------------|\n| **Claude Sonnet-4** | 37.0% | $263.56* | 93.2M tokens |\n| **Qwen-3-Coder** | 19.7% | $217.83 | 14.7M tokens |\n\n*Claude Sonnet-4 costs reflect heavy caching usage, reducing actual API costs\n\n\n## 🤖 The Agents\n\nWhile all agents use the same underlying LLM, each operates with its own context window, specialised system message, and distinct toolset. This creates functionally different agents optimised for their specific roles.\n\n### 🎯 Orchestrator Agent\n[System message](./src/agents/system_msgs/md_files/orchestrator_sys_msg_v0.1.md)\n**Role:** Strategic coordinator and persistent intelligence layer  \n**Capabilities:** Task decomposition, context management, subagent delegation  \n**Tools:** Task creation, subagent launching, context store management  \n**Restrictions:** Cannot read or modify code directly - operates purely at architectural level  \n\nThe orchestrator maintains the complete picture across all tasks, tracking discoveries and progress. It crafts precise task descriptions that explicitly specify what contexts subagents should return, ensuring focused and valuable information gathering.\n\n**Trust Calibration Strategy:**  \nThe orchestrator employs adaptive delegation based on task complexity:\n- **Low Complexity Tasks**: Grants extremely high autonomy to the coder agent for simple modifications and bug fixes\n- **Medium/Large Tasks**: Maintains strong trust but uses iterative decomposition - breaking complex problems into atomic, verifiable steps\n- **Verification Philosophy**: Uses explorer agents liberally to verify progress, especially when tasks involve critical functionality\n\n\n### 🔍 Explorer Agent \n[System message](./src/agents/system_msgs/md_files/explorer_sys_msg_v0.1.md) \n**Role:** Read-only investigation and verification specialist  \n**Capabilities:** System exploration, code analysis, test execution, verification  \n**Tools:** File reading, search operations (grep/glob), bash commands, temporary script creation  \n**Restrictions:** Cannot modify existing files - strictly read-only operations  \n\nExplorers gather intelligence about the codebase, verify implementations, and discover system behaviors. They create knowledge artifacts that eliminat",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-20T02:14:23.974333"
  },
  {
    "basic_info": {
      "name": "Super-Mario-Bros.-Remastered-Public",
      "full_name": "JHDev2006/Super-Mario-Bros.-Remastered-Public",
      "owner": "JHDev2006",
      "description": null,
      "url": "https://github.com/JHDev2006/Super-Mario-Bros.-Remastered-Public",
      "clone_url": "https://github.com/JHDev2006/Super-Mario-Bros.-Remastered-Public.git",
      "ssh_url": "git@github.com:JHDev2006/Super-Mario-Bros.-Remastered-Public.git",
      "homepage": null,
      "created_at": "2025-09-13T15:29:58Z",
      "updated_at": "2025-09-20T02:08:57Z",
      "pushed_at": "2025-09-19T19:59:31Z"
    },
    "stats": {
      "stars": 1147,
      "forks": 137,
      "watchers": 1147,
      "open_issues": 158,
      "size": 51559
    },
    "tech_info": {
      "language": "GDScript",
      "languages": {
        "GDScript": 937173,
        "GAP": 36590,
        "C#": 10724,
        "GDShader": 2322
      },
      "license": "GNU General Public License v3.0",
      "topics": []
    },
    "content": {
      "readme": "# Super Mario Bros Remastered\nA Remake / Celebration of the original 'Super Mario Bros.' games. Features new levels, custom modes, new characters, alongside a full level editor / custom level system!\n\n<img width=\"3840\" height=\"2160\" alt=\"SMB1R_BANNER_printable\" src=\"https://github.com/user-attachments/assets/ed0e97a8-614a-44e2-b69f-2654fca6196c\" />\n\n### Art by [@krystalphantasm.bsky.social](https://bsky.app/profile/krystalphantasm.bsky.social/post/3lvgmgvjeks2f)\n\n### Download: https://github.com/JHDev2006/Super-Mario-Bros.-Remastered-Public/releases\n\n# Requires an original SMB1 NES ROM to play! None of the original assets are contained in the source code, unless it was originally made by us!\n\n# This does NOT act as a replacement for the original Super Mario Bros. games. Super Mario Bros. & Super Mario Bros.: The Lost Levels, can be played now on Nintendo Switch, through Nintendo Switch Online\n\n## Features\n- Super Mario Bros., Super Mario Bros.: The Lost Levels, Super Mario Bros. Special and All Night Nippon: Super Mario Bros. Fully recreated from the ground up!\n- Improved physics / level design\n- Resource Packs! Fully customize how the game looks and sounds.\n- Custom Characters - Add in your own characters to use in game.\n- Fully Open Source!\n- Level Share Square Partnered\n\n## Downloading\n1. Go to the 'Releases' page\n2. Look for the latest version\n3. Download the .zip for your OS\n4. Extract and run\n5. Enjoy!\n\n## Importing for editing\n1. Download the source\n2. Download Godot 4.5 beta 3\n3. Import the project\n4. Enjoy!\n\n## Contributing\nYou are more than welcome to contribute any fixes / improvements you'd like, simply open a pull request, and I'll review it ASAP!\n\n## System Requirements\n\nPlease refer to the Godot engine requirements for minimum and recommended hardware specifications.\n\n[Minimum Requirements](https://docs.godotengine.org/en/latest/about/system_requirements.html#desktop-or-laptop-pc-minimum)\n\n[Recommended Requirements](https://docs.godotengine.org/en/latest/about/system_requirements.html#id3)\n\n\n## Issues\nWhen opening an issue, please keep it to one report, per post, and try and be as helpful as possible, when telling me what has occured, so that its as easy to fix as possible.\nPlease do not open issues, for feature requests, suggestions, or opinions. BUG REPORTS ONLY\n\n## Known Issues\nThere are a couple known issues, mainly due to being built off of Godot, and these issues existing in the engine itself.\n- Steam deck controls do not work natively, you can circumvent this by setting up controller bindings to emulate keys instead, apologies.\n- Physics are weird, when interacting with corners + the camera barrier\n- Drop shadows jitter when playing with \"Smooth Rendering\"\n- Several entities jitter at times.\n- Blocks + coins, respawn when reloading resource packs\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-20T02:14:25.094706"
  },
  {
    "basic_info": {
      "name": "HunyuanWorld-Voyager",
      "full_name": "Tencent-Hunyuan/HunyuanWorld-Voyager",
      "owner": "Tencent-Hunyuan",
      "description": "Voyager is an interactive RGBD video generation model conditioned on camera input, and supports real-time 3D reconstruction.",
      "url": "https://github.com/Tencent-Hunyuan/HunyuanWorld-Voyager",
      "clone_url": "https://github.com/Tencent-Hunyuan/HunyuanWorld-Voyager.git",
      "ssh_url": "git@github.com:Tencent-Hunyuan/HunyuanWorld-Voyager.git",
      "homepage": "https://3d-models.hunyuan.tencent.com/world/",
      "created_at": "2025-08-27T09:34:10Z",
      "updated_at": "2025-09-20T01:01:58Z",
      "pushed_at": "2025-09-10T08:21:15Z"
    },
    "stats": {
      "stars": 1139,
      "forks": 74,
      "watchers": 1139,
      "open_issues": 14,
      "size": 850180
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 468553,
        "Shell": 1060
      },
      "license": "Other",
      "topics": [
        "3d",
        "3d-generation",
        "aigc",
        "hunyuan3d",
        "image-to-3d",
        "image-to-video",
        "scene-generation",
        "world-model",
        "world-models"
      ]
    },
    "content": {
      "readme": "[中文阅读](README_zh.md)\n\n# **HunyuanWorld-Voyager**\n\n<p align=\"center\">\n  <img src=\"assets/teaser.png\">\n</p>\n\n<div align=\"center\">\n  <a href=\"https://3d-models.hunyuan.tencent.com/world/\" target=\"_blank\"><img src=\"https://img.shields.io/static/v1?label=Project%20Page&message=Web&color=green\" height=22px></a>\n  <a href=\"https://3d-models.hunyuan.tencent.com/voyager/voyager_en/assets/HYWorld_Voyager.pdf\" target=\"_blank\"><img src=\"https://img.shields.io/static/v1?label=Tech%20Report&message=arxiv&color=red\" height=22px></a>\n  <a href=\"https://huggingface.co/tencent/HunyuanWorld-Voyager\" target=\"_blank\"><img src=\"https://img.shields.io/static/v1?label=HunyuanWorld-Voyager&message=HuggingFace&color=yellow\" height=22px></a>\n</div>\n\n-----\n\nWe introduce HunyuanWorld-Voyager, a novel video diffusion framework that generates world-consistent 3D point-cloud sequences from a single image with user-defined camera path. Voyager can generate 3D-consistent scene videos for world exploration following custom camera trajectories. It can also generate aligned depth and RGB video for efficient and direct 3D reconstruction.\n\n\n## 🔥🔥🔥 News!!\n* Sep 2, 2025: 👋 We release the code and model weights of HunyuanWorld-Voyager. [Download](ckpts/README.md).\n\n> Join our **[Wechat](#)** and **[Discord](https://discord.gg/dNBrdrGGMa)** group to discuss and find help from us.\n\n| Wechat Group                                     | Xiaohongshu                                           | X                                           | Discord                                           |\n|--------------------------------------------------|-------------------------------------------------------|---------------------------------------------|---------------------------------------------------|\n| <img src=\"assets/qrcode/wechat.png\"  height=140> | <img src=\"assets/qrcode/xiaohongshu.png\"  height=140> | <img src=\"assets/qrcode/x.png\"  height=140> | <img src=\"assets/qrcode/discord.png\"  height=140> | \n\n## 🎥 Demo\n### Demo Video\n\n<div align=\"center\">\n  <video src=\"https://github.com/user-attachments/assets/2eb844c9-30ba-4770-8066-189c123affee\" width=\"80%\" poster=\"\"> </video>\n</div>\n\n### Camera-Controllable Video Generation\n\n|  Input | Generated Video  |\n|:----------------:|:----------------:|\n|  <img src=\"assets/demo/camera/input1.png\" width=\"80%\">        |       <video src=\"https://github.com/user-attachments/assets/2b03ecd5-9a8f-455c-bf04-c668d3a61b04\" width=\"100%\"> </video>        |\n| <img src=\"assets/demo/camera/input2.png\" width=\"80%\">         |       <video src=\"https://github.com/user-attachments/assets/45844ac0-c65a-4e04-9f7d-4c72d47e0339\" width=\"100%\"> </video>        | \n| <img src=\"assets/demo/camera/input3.png\" width=\"80%\">         |       <video src=\"https://github.com/user-attachments/assets/f7f48473-3bb5-4a30-bd22-af3ca95ee8dc\" width=\"100%\"> </video>        |\n\n### Multiple Applications\n\n- Video Reconstruction\n\n| Generated Video | Reconstructed Point Cloud |\n|:---------------:|:--------------------------------:|\n| <video src=\"https://github.com/user-attachments/assets/72a41804-63fc-4596-963d-1497e68f7790\" width=\"100%\"> </video> | <video src=\"https://github.com/user-attachments/assets/67574e9c-9e21-4ed6-9503-e65d187086a2\" width=\"100%\"> </video> |\n\n- Image-to-3D Generation\n\n| | |\n|:---------------:|:---------------:|\n| <video src=\"https://github.com/user-attachments/assets/886aa86d-990e-4b86-97a5-0b9110862d14\" width=\"100%\"> </video> | <video src=\"https://github.com/user-attachments/assets/4c1734ba-4e78-4979-b30e-3c8c97aa984b\" width=\"100%\"> </video> |\n\n- Video Depth Estimation\n\n| | |\n|:---------------:|:---------------:|\n| <video src=\"https://github.com/user-attachments/assets/e4c8b729-e880-4be3-826f-429a5c1f12cd\" width=\"100%\"> </video> | <video src=\"https://github.com/user-attachments/assets/7ede0745-cde7-42f1-9c28-e4dca90dac52\" width=\"100%\"> </video> |\n\n\n## ☯️ **HunyuanWorld-Voyager Introduction**\n###  Architecture\n\nVoyager consists of two key components:\n\n(1) World-Consistent Video Diffusion: A unified architecture that jointly generates aligned RGB and depth video sequences, conditioned on existing world observation to ensure global coherence.\n\n(2) Long-Range World Exploration: An efficient world cache with point culling and an auto-regressive inference with smooth video sampling for iterative scene extension with context-aware consistency.\n\nTo train Voyager, we propose a scalable data engine, i.e., a video reconstruction pipeline that automates camera pose estimation and metric depth prediction for arbitrary videos, enabling large-scale, diverse training data curation without manual 3D annotations. Using this pipeline, we compile a dataset of over 100,000 video clips, combining real-world captures and synthetic Unreal Engine renders.\n\n<p align=\"center\">\n  <img src=\"assets/backbone.jpg\"  height=500>\n</p>\n\n### Performance\n\n<table class=\"comparison-table\">\n  <thead>\n    <tr>\n      <th>Method</th>\n      <th>WorldScore Average</th>\n      <th>Camera Con",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-20T02:14:26.224159"
  },
  {
    "basic_info": {
      "name": "map-anything",
      "full_name": "facebookresearch/map-anything",
      "owner": "facebookresearch",
      "description": "MapAnything: Universal Feed-Forward Metric 3D Reconstruction",
      "url": "https://github.com/facebookresearch/map-anything",
      "clone_url": "https://github.com/facebookresearch/map-anything.git",
      "ssh_url": "git@github.com:facebookresearch/map-anything.git",
      "homepage": "",
      "created_at": "2025-09-04T14:37:36Z",
      "updated_at": "2025-09-20T01:56:12Z",
      "pushed_at": "2025-09-19T10:17:35Z"
    },
    "stats": {
      "stars": 1127,
      "forks": 42,
      "watchers": 1127,
      "open_issues": 11,
      "size": 6164
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 1727697,
        "Shell": 157713
      },
      "license": "Apache License 2.0",
      "topics": [
        "3d-reconstruction",
        "ai",
        "calibration",
        "depth-completion",
        "depth-estimation",
        "image-to-3d",
        "multi-view-stereo",
        "robotics",
        "sfm"
      ]
    },
    "content": {
      "readme": "<div align=\"center\">\n<h1>MapAnything: Universal Feed-Forward Metric <br>3D Reconstruction</h1>\n<a href=\"https://map-anything.github.io/assets/MapAnything.pdf\"><img src=\"https://img.shields.io/badge/Paper-blue\" alt=\"Paper\"></a>\n<a href=\"https://arxiv.org/abs/2509.13414\"><img src=\"https://img.shields.io/badge/arXiv-2509.13414-b31b1b\" alt=\"arXiv\"></a>\n<a href=\"https://map-anything.github.io/\"><img src=\"https://img.shields.io/badge/Project_Page-green\" alt=\"Project Page\"></a>\n<a href=\"https://huggingface.co/spaces/facebook/map-anything\"><img src='https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Demo-blue'></a>\n<br>\n<br>\n<strong>\n<a href=\"https://nik-v9.github.io/\">Nikhil Keetha<sup>1,2</sup></a>\n&nbsp;&nbsp;\n<a href=\"https://sirwyver.github.io/\">Norman Müller<sup>1</sup></a>\n&nbsp;&nbsp;\n<a href=\"https://demuc.de/\">Johannes Schönberger<sup>1</sup></a>\n&nbsp;&nbsp;\n<a href=\"https://www.linkedin.com/in/lorenzoporzi\">Lorenzo Porzi<sup>1</sup></a>\n&nbsp;&nbsp;\n<a href=\"https://infinity1096.github.io/\">Yuchen Zhang<sup>2</sup></a>\n<br>\n<a href=\"https://tobiasfshr.github.io/\">Tobias Fischer<sup>1</sup></a>\n&nbsp;&nbsp;\n<a href=\"https://www.linkedin.com/in/arno-knapitsch\">Arno Knapitsch<sup>1</sup></a>\n&nbsp;&nbsp;\n<a href=\"https://www.linkedin.com/in/duncan-zauss\">Duncan Zauss<sup>1</sup></a>\n&nbsp;&nbsp;\n<a href=\"https://ethanweber.me/\">Ethan Weber<sup>1</sup></a>\n&nbsp;&nbsp;\n<a href=\"https://www.linkedin.com/in/nelsonantunes7\">Nelson Antunes<sup>1</sup></a>\n<br>\n<a href=\"https://x.com/jonathonluiten?lang=en\">Jonathon Luiten<sup>1</sup></a>\n&nbsp;&nbsp;\n<a href=\"https://m.lopezantequera.com/\">Manuel Lopez-Antequera<sup>1</sup></a>\n&nbsp;&nbsp;\n<a href=\"https://scholar.google.com/citations?user=484sccEAAAAJ\">Samuel Rota Bulò<sup>1</sup></a>\n&nbsp;&nbsp;\n<a href=\"https://richardt.name/\">Christian Richardt<sup>1</sup></a>\n<br>\n<a href=\"https://www.cs.cmu.edu/~deva/\">Deva Ramanan<sup>2</sup></a>\n&nbsp;&nbsp;\n<a href=\"https://theairlab.org/team/sebastian/\">Sebastian Scherer<sup>2</sup></a>\n&nbsp;&nbsp;\n<a href=\"https://www.linkedin.com/in/peter-kontschieder-2a6410134\">Peter Kontschieder<sup>1</sup></a>\n<br>\n<br>\n<sup>1</sup> Meta &nbsp;&nbsp;\n<sup>2</sup> Carnegie Mellon University\n</strong>\n\n</div>\n\n## Overview\n\nMapAnything is a simple, end-to-end trained transformer model that directly regresses the factored metric 3D geometry of a scene given various types of inputs (images, calibration, poses, or depth). A single feed-forward model supports over 12 different 3D reconstruction tasks including multi-image sfm, multi-view stereo, monocular metric depth estimation, registration, depth completion and more.\n\n![Overview](./assets/teaser.png)\n\n## Table of Contents\n\n- [Quick Start](#quick-start)\n  - [Installation](#installation)\n  - [Image-Only Inference](#image-only-inference)\n  - [Multi-Modal Inference](#multi-modal-inference)\n- [Interactive Demos](#interactive-demos)\n  - [Online Demo](#online-demo)\n  - [Local Gradio Demo](#local-gradio-demo)\n  - [Rerun Demo](#rerun-demo)\n- [COLMAP & GSplat Support](#colmap--gsplat-support)\n  - [Exporting to COLMAP Format](#exporting-to-colmap-format)\n  - [Integration with Gaussian Splatting](#integration-with-gaussian-splatting)\n- [Data Processing for Training & Benchmarking](#data-processing-for-training--benchmarking)\n- [Training](#training)\n- [Benchmarking](#benchmarking)\n- [Code License](#code-license)\n- [Models](#models)\n- [Building Blocks for MapAnything](#building-blocks-for-mapanything)\n- [Acknowledgments](#acknowledgments)\n- [Citation](#citation)\n\n## Quick Start\n\n### Installation\n\n```bash\ngit clone https://github.com/facebookresearch/map-anything.git\ncd map-anything\n\n# Create and activate conda environment\nconda create -n mapanything python=3.12 -y\nconda activate mapanything\n\n# Optional: Install torch, torchvision & torchaudio specific to your system\n# Install MapAnything\npip install -e .\n\n# For all optional dependencies\n# See pyproject.toml for more details\npip install -e \".[all]\"\npre-commit install\n```\n\nNote that we don't pin a specific version of PyTorch or CUDA in our requirements. Please feel free to install PyTorch based on your specific system.\n\n### Image-Only Inference\n\nFor metric 3D reconstruction from images without additional geometric inputs:\n\n```python\n# Optional config for better memory efficiency\nimport os\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n\n# Required imports\nimport torch\nfrom mapanything.models import MapAnything\nfrom mapanything.utils.image import load_images\n\n# Get inference device\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# Init model - This requries internet access or the huggingface hub cache to be pre-downloaded\n# For Apache 2.0 license model, use \"facebook/map-anything-apache\"\nmodel = MapAnything.from_pretrained(\"facebook/map-anything\").to(device)\n\n# Load and preprocess images from a folder or list of paths\nimages = \"path/to/your/images/\"  # or [\"path/to/img1.jpg\", \"path/to/img2.jpg\", ...]\nviews = loa",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-20T02:14:27.359249"
  }
]