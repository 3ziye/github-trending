[
  {
    "basic_info": {
      "name": "nanobot",
      "full_name": "HKUDS/nanobot",
      "owner": "HKUDS",
      "description": "\"ğŸˆ nanobot: The Ultra-Lightweight OpenClaw\"",
      "url": "https://github.com/HKUDS/nanobot",
      "clone_url": "https://github.com/HKUDS/nanobot.git",
      "ssh_url": "git@github.com:HKUDS/nanobot.git",
      "homepage": "",
      "created_at": "2026-02-01T07:16:15Z",
      "updated_at": "2026-03-01T03:36:13Z",
      "pushed_at": "2026-02-28T18:06:59Z"
    },
    "stats": {
      "stars": 26930,
      "forks": 4299,
      "watchers": 26930,
      "open_issues": 752,
      "size": 33847
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 544125,
        "TypeScript": 8773,
        "Shell": 7009,
        "JavaScript": 1316,
        "Dockerfile": 1294
      },
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "<div align=\"center\">\n  <img src=\"nanobot_logo.png\" alt=\"nanobot\" width=\"500\">\n  <h1>nanobot: Ultra-Lightweight Personal AI Assistant</h1>\n  <p>\n    <a href=\"https://pypi.org/project/nanobot-ai/\"><img src=\"https://img.shields.io/pypi/v/nanobot-ai\" alt=\"PyPI\"></a>\n    <a href=\"https://pepy.tech/project/nanobot-ai\"><img src=\"https://static.pepy.tech/badge/nanobot-ai\" alt=\"Downloads\"></a>\n    <img src=\"https://img.shields.io/badge/python-â‰¥3.11-blue\" alt=\"Python\">\n    <img src=\"https://img.shields.io/badge/license-MIT-green\" alt=\"License\">\n    <a href=\"./COMMUNICATION.md\"><img src=\"https://img.shields.io/badge/Feishu-Group-E9DBFC?style=flat&logo=feishu&logoColor=white\" alt=\"Feishu\"></a>\n    <a href=\"./COMMUNICATION.md\"><img src=\"https://img.shields.io/badge/WeChat-Group-C5EAB4?style=flat&logo=wechat&logoColor=white\" alt=\"WeChat\"></a>\n    <a href=\"https://discord.gg/MnCvHqpUGB\"><img src=\"https://img.shields.io/badge/Discord-Community-5865F2?style=flat&logo=discord&logoColor=white\" alt=\"Discord\"></a>\n  </p>\n</div>\n\nğŸˆ **nanobot** is an **ultra-lightweight** personal AI assistant inspired by [OpenClaw](https://github.com/openclaw/openclaw) \n\nâš¡ï¸ Delivers core agent functionality in just **~4,000** lines of code â€” **99% smaller** than Clawdbot's 430k+ lines.\n\nğŸ“ Real-time line count: **3,935 lines** (run `bash core_agent_lines.sh` to verify anytime)\n\n## ğŸ“¢ News\n\n- **2026-02-28** ğŸš€ Released **v0.1.4.post3** â€” cleaner context, hardened session history, and smarter agent. Please see [release notes](https://github.com/HKUDS/nanobot/releases/tag/v0.1.4.post3) for details.\n- **2026-02-27** ğŸ§  Experimental thinking mode support, DingTalk media messages, Feishu and QQ channel fixes.\n- **2026-02-26** ğŸ›¡ï¸ Session poisoning fix, WhatsApp dedup, Windows path guard, Mistral compatibility.\n- **2026-02-25** ğŸ§¹ New Matrix channel, cleaner session context, auto workspace template sync.\n- **2026-02-24** ğŸš€ Released **v0.1.4.post2** â€” a reliability-focused release with a redesigned heartbeat, prompt cache optimization, and hardened provider & channel stability. See [release notes](https://github.com/HKUDS/nanobot/releases/tag/v0.1.4.post2) for details.\n- **2026-02-23** ğŸ”§ Virtual tool-call heartbeat, prompt cache optimization, Slack mrkdwn fixes.\n- **2026-02-22** ğŸ›¡ï¸ Slack thread isolation, Discord typing fix, agent reliability improvements.\n- **2026-02-21** ğŸ‰ Released **v0.1.4.post1** â€” new providers, media support across channels, and major stability improvements. See [release notes](https://github.com/HKUDS/nanobot/releases/tag/v0.1.4.post1) for details.\n- **2026-02-20** ğŸ¦ Feishu now receives multimodal files from users. More reliable memory under the hood.\n- **2026-02-19** âœ¨ Slack now sends files, Discord splits long messages, and subagents work in CLI mode.\n\n<details>\n<summary>Earlier news</summary>\n\n- **2026-02-18** âš¡ï¸ nanobot now supports VolcEngine, MCP custom auth headers, and Anthropic prompt caching.\n- **2026-02-17** ğŸ‰ Released **v0.1.4** â€” MCP support, progress streaming, new providers, and multiple channel improvements. Please see [release notes](https://github.com/HKUDS/nanobot/releases/tag/v0.1.4) for details.\n- **2026-02-16** ğŸ¦ nanobot now integrates a [ClawHub](https://clawhub.ai) skill â€” search and install public agent skills.\n- **2026-02-15** ğŸ”‘ nanobot now supports OpenAI Codex provider with OAuth login support.\n- **2026-02-14** ğŸ”Œ nanobot now supports MCP! See [MCP section](#mcp-model-context-protocol) for details.\n- **2026-02-13** ğŸ‰ Released **v0.1.3.post7** â€” includes security hardening and multiple improvements. **Please upgrade to the latest version to address security issues**. See [release notes](https://github.com/HKUDS/nanobot/releases/tag/v0.1.3.post7) for more details.\n- **2026-02-12** ğŸ§  Redesigned memory system â€” Less code, more reliable. Join the [discussion](https://github.com/HKUDS/nanobot/discussions/566) about it!\n- **2026-02-11** âœ¨ Enhanced CLI experience and added MiniMax support!\n- **2026-02-10** ğŸ‰ Released **v0.1.3.post6** with improvements! Check the updates [notes](https://github.com/HKUDS/nanobot/releases/tag/v0.1.3.post6) and our [roadmap](https://github.com/HKUDS/nanobot/discussions/431).\n- **2026-02-09** ğŸ’¬ Added Slack, Email, and QQ support â€” nanobot now supports multiple chat platforms!\n- **2026-02-08** ğŸ”§ Refactored Providersâ€”adding a new LLM provider now takes just 2 simple steps! Check [here](#providers).\n- **2026-02-07** ğŸš€ Released **v0.1.3.post5** with Qwen support & several key improvements! Check [here](https://github.com/HKUDS/nanobot/releases/tag/v0.1.3.post5) for details.\n- **2026-02-06** âœ¨ Added Moonshot/Kimi provider, Discord integration, and enhanced security hardening!\n- **2026-02-05** âœ¨ Added Feishu channel, DeepSeek provider, and enhanced scheduled tasks support!\n- **2026-02-04** ğŸš€ Released **v0.1.3.post4** with multi-provider & Docker support! Check [here](https://github.com/HKUDS/nanobot/releases/tag/v0.1.3.post4) for details.\n- **2026-02-03** âš¡ Integrated vLLM for local LLM support",
      "default_branch": "main"
    },
    "fetched_at": "2026-03-01T03:37:32.309315"
  },
  {
    "basic_info": {
      "name": "ClawWork",
      "full_name": "HKUDS/ClawWork",
      "owner": "HKUDS",
      "description": "\"ClawWork: OpenClaw as Your AI Coworker - ğŸ’° $10K earned in 7 Hours\"",
      "url": "https://github.com/HKUDS/ClawWork",
      "clone_url": "https://github.com/HKUDS/ClawWork.git",
      "ssh_url": "git@github.com:HKUDS/ClawWork.git",
      "homepage": "",
      "created_at": "2026-02-15T16:41:38Z",
      "updated_at": "2026-03-01T03:36:03Z",
      "pushed_at": "2026-02-28T03:48:31Z"
    },
    "stats": {
      "stars": 5920,
      "forks": 718,
      "watchers": 5920,
      "open_issues": 24,
      "size": 463650
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 620275,
        "Jupyter Notebook": 246332,
        "JavaScript": 174116,
        "HCL": 25934,
        "TypeScript": 13025,
        "Shell": 12288,
        "Solidity": 8691,
        "CSS": 2198,
        "HTML": 389
      },
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "<img alt=\"image\" src=\"assets/live_banner.png\" /><div align=\"center\">\n  <h1>ClawWork: OpenClaw as Your AI Coworker</h1>\n    <p>\n    <img src=\"https://img.shields.io/badge/python-â‰¥3.10-blue\" alt=\"Python\">\n    <img src=\"https://img.shields.io/badge/license-MIT-green\" alt=\"License\">\n    <img src=\"https://img.shields.io/badge/dataset-GDPVal%20220%20tasks-orange\" alt=\"GDPVal\">\n    <img src=\"https://img.shields.io/badge/benchmark-economic%20survival-red\" alt=\"Benchmark\">\n    <a href=\"https://github.com/HKUDS/nanobot\"><img src=\"https://img.shields.io/badge/nanobot-integration-C5EAB4?style=flat&logo=github&logoColor=white\" alt=\"nanobot\"></a>\n    <a href=\"https://github.com/HKUDS/.github/blob/main/profile/README.md\"><img src=\"https://img.shields.io/badge/Feishu-Group-E9DBFC?style=flat&logo=feishu&logoColor=white\" alt=\"Feishu\"></a>\n    <a href=\"https://github.com/HKUDS/.github/blob/main/profile/README.md\"><img src=\"https://img.shields.io/badge/WeChat-Group-C5EAB4?style=flat&logo=wechat&logoColor=white\" alt=\"WeChat\"></a>\n  </p>\n  <h3>ğŸ’° $10K in 7 Hours â€” AI Coworker for 44+ Professions</h3>\n  <h4>| Technology & Engineering | Business & Finance | Healthcare & Social Services | Legal, Media & Operations | </h3>\n  <h3><a href=\"https://hkuds.github.io/ClawWork/\">ğŸ”´ Watch AI Coworkers Earn Money from Real-Life Tasks</a></h3>\n\n| Rank | Agent | Starter | Balance | Income | Cost | Pay Rate | Avg Quality |\n|:----:|-------|--------:|--------:|-------:|-----:|---------:|------------:|\n| ğŸ¥‡ | **Gemini 3.1 Pro Preview** | $10.00 | $15,661.71 | $15,757.48 | $105.76 | $1,287.47/hr | 43.3% |\n| ğŸ¥ˆ | **Qwen3.5-Plus** | $10.00 | $15,268.13 | $15,264.92 | $6.78 | $1,390.42/hr | 41.6% |\n| ğŸ¥‰ | **GLM-4.7** | $10.00 | $11,497.05 | $11,503.49 | $16.44 | $877.80/hr | 40.6% |\n| 4 | **Qwen3-Max** | $10.00 | $10,782.80 | $10,781.06 | $8.26 | $1,072.14/hr | 37.9% |\n| 5 | **Kimi-K2.5** | $10.00 | $10,471.21 | $10,483.20 | $21.99 | $858.62/hr | 36.6% |\n\n  <p><sub>Agent data on the site is periodically synced to this repo. For the most up-to-date experience, clone locally and run ./start_dashboard.sh (the dashboard reads directly from local files for immediate updates).</sub></p>\n\n</div>\n  \n---\n\n<div align=\"center\">\n<img src=\"assets/clawwork_banner.png\" alt=\"ClawWork\" width=\"800\">\n</div>\n\n### ğŸš€ AI Assistant â†’ AI Coworker Evolution\nTransforms AI assistants into true AI coworkers that complete real work tasks and create genuine economic value.\n\n### ğŸ’° Real-World Economic Benchmark\nReal-world economic testing system where AI agents must earn income by completing professional tasks from the [GDPVal](https://openai.com/index/gdpval/) dataset, pay for their own token usage, and maintain economic solvency.\n\n### ğŸ“Š Production AI Validation\nMeasures what truly matters in production environments: **work quality**, **cost efficiency**, and **long-term survival** - not just technical benchmarks.\n\n### ğŸ¤– Multi-Model Competition Arena\nSupports different AI models (GLM, Kimi, Qwen, etc.) competing head-to-head to determine the ultimate \"AI worker champion\" through actual work performance\n\n---\n\n## ğŸ“¢ News\n\n- **2026-02-21 ğŸ”„ ClawMode + Frontend + Agents Update** â€” Updated ClawMode to support ClawWork-specific tools; improved frontend dashboard (untapped potential visualization); added more agents: Claude Sonnet 4.6, Gemini 3.1 Pro and Qwen-3.5-Plus.\n- **2026-02-20 ğŸ’° Improved Cost Tracking** â€” Token costs are now read directly from various API responses (including thinking tokens) instead of estimation. OpenRouter's reported cost is used verbatim when available.\n- **2026-02-19 ğŸ“Š Agent Results Updated** â€” Added Qwen3-Max, Kimi-K2.5, GLM-4.7 through Feb 19. Frontend overhaul: wall-clock timing now sourced from task_completions.jsonl.\n- **2026-02-17 ğŸ”§ Enhanced Nanobot Integration** â€” New /clawwork command for on-demand paid tasks. Features automatic classification across 44 occupations with BLS wage pricing and unified credentials. Try locally: python -m clawmode_integration.cli agent.\n- **2026-02-16 ğŸ‰ ClawWork Launch** â€” ClawWork is now officially available! Welcome to explore ClawWork.\n\n---\n\n## âœ¨ ClawWork's Key Features\n\n- **ğŸ’¼ Real Professional Tasks**: 220 GDP validation tasks spanning 44 economic sectors (Manufacturing, Finance, Healthcare, and more) from the GDPVal dataset â€” testing real-world work capability\n\n- **ğŸ’¸ Extreme Economic Pressure**: Agents start with just $10 and pay for every token generated. One bad task or careless search can wipe the balance. Income only comes from completing quality work.\n\n- **ğŸ§  Strategic Work + Learn Choices**: Agents face daily decisions: work for immediate income or invest in learning to improve future performance â€” mimicking real career trade-offs.\n\n- **ğŸ“Š React Dashboard**: Visualization of balance changes, task completions, learning progress, and survival metrics from real-life tasks â€” watch the economic drama unfold.\n\n- **ğŸª¶ Ultra-Lightweight Architecture**: Built on Nanobot â€” your strong AI coworker with minimal infrastructure. Single p",
      "default_branch": "main"
    },
    "fetched_at": "2026-03-01T03:37:33.611989"
  },
  {
    "basic_info": {
      "name": "financial-services-plugins",
      "full_name": "anthropics/financial-services-plugins",
      "owner": "anthropics",
      "description": null,
      "url": "https://github.com/anthropics/financial-services-plugins",
      "clone_url": "https://github.com/anthropics/financial-services-plugins.git",
      "ssh_url": "git@github.com:anthropics/financial-services-plugins.git",
      "homepage": null,
      "created_at": "2026-02-23T19:24:50Z",
      "updated_at": "2026-03-01T03:37:20Z",
      "pushed_at": "2026-02-24T04:16:30Z"
    },
    "stats": {
      "stars": 4917,
      "forks": 482,
      "watchers": 4917,
      "open_issues": 8,
      "size": 412
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 39573
      },
      "license": "Apache License 2.0",
      "topics": []
    },
    "content": {
      "readme": "# Claude for Financial Services Plugins\n\nPlugins that turn Claude into a specialist for financial services â€” investment banking, equity research, private equity, and wealth management. Built for [Claude Cowork](https://claude.com/product/cowork), also compatible with [Claude Code](https://claude.com/product/claude-code).\n\n## Why Plugins\n\nCowork lets you set the goal and Claude delivers finished, professional work. Plugins let you go further: tell Claude how your firm does analysis, which data sources to pull from, how to handle critical workflows, and what slash commands to expose â€” so your team gets better and more consistent outcomes.\n\nEach plugin bundles the skills, connectors, slash commands, and sub-agents for a specific financial services workflow. Out of the box, they give Claude a strong starting point for helping anyone in that role. The real power comes when you customize them for your firm â€” your models, your templates, your processes â€” so Claude works like it was built for your team.\n\n## What is Claude for Financial Services?\n\nClaude for Financial Services is a comprehensive solution built on Claude for Enterprise with specialized capabilities for financial analysis. It connects Claude to the data sources and tools financial professionals use daily â€” eliminating the need to juggle multiple browser tabs and improving source verification to reduce the risk of errors from manual data gathering.\n\n## End-to-End Workflows\n\nThese plugins aren't just a collection of point tools â€” they enable complete workflows that span research, analysis, modeling, and output creation:\n\n- **Research to Report**: Pull real-time data from MCP providers, analyze earnings results, and generate publication-ready equity research reports â€” all in a single session\n- **Spreadsheet Analysis**: Build comparable company analyses, DCF models, and LBO models as fully functional Excel workbooks with live formulas, sensitivity tables, and industry-standard formatting\n- **Financial Modeling**: Populate 3-statement models from SEC filings, cross-check assumptions against peer data, and stress-test scenarios â€” with blue/black/green color coding conventions built in\n- **Deal Materials**: Draft CIMs, teasers, and process letters, then generate pitch deck slides and strip profiles using your firm's branded PowerPoint templates\n- **Portfolio to Presentation**: Screen opportunities, run diligence checklists, build IC memos, and track portfolio KPIs â€” moving seamlessly from data to deliverable\n\nEach workflow connects upstream data sources (via MCP) to downstream outputs (Excel, PowerPoint, Word), so you move from question to finished work product without context-switching.\n\n## Plugin Marketplace\n\nStart with **financial analysis** â€” the core plugin that provides shared modeling tools and all MCP data connectors. Then add any function-specific plugins to enhance Claude's capabilities for your workflow.\n\n| Plugin | Type | How it helps | Connectors |\n|--------|------|-------------|------------|\n| **[financial analysis](./financial-analysis)** | Core (install first) | Build comps, DCF models, LBO models, and 3-statement financials. QC presentations and create reusable PPT templates. Provides the shared foundation and all data connectors. | Daloopa, Morningstar, S&P Global, FactSet, Moody's, MT Newswires, Aiera, LSEG, PitchBook, Chronograph, Egnyte |\n| **[investment banking](./investment-banking)** | Add-on | Draft CIMs, teasers, and process letters. Build buyer lists, run merger models, create strip profiles, and track live deals through milestones. | â€” |\n| **[equity research](./equity-research)** | Add-on | Write earnings updates and initiating coverage reports. Maintain investment theses, track catalysts, draft morning notes, and screen for new ideas. | â€” |\n| **[private equity](./private-equity)** | Add-on | Source and screen deals, run due diligence checklists, analyze unit economics and returns, draft IC memos, and monitor portfolio company KPIs. | â€” |\n| **[wealth management](./wealth-management)** | Add-on | Prep for client meetings, build financial plans, rebalance portfolios, generate client reports, and identify tax-loss harvesting opportunities. | â€” |\n\n**41 skills, 38 commands, 11 MCP integrations**\n\nInstall these directly from Cowork, browse the full collection here on GitHub, or build your own.\n\n### Partner-Built Plugins\n\nThese plugins are built and maintained by our data partners, bringing their financial data and analytics directly into Claude workflows.\n\n| Plugin | Partner | How it helps |\n|--------|---------|-------------|\n| **[LSEG](./partner-built/lseg)** | [LSEG](https://www.lseg.com/) | Price bonds, analyze yield curves, evaluate FX carry trades, value options, and build macro dashboards using LSEG financial data and analytics. 8 commands covering fixed income, FX, equities, and macro. |\n| **[S&P Global](./partner-built/spglobal)** | [S&P Global](https://www.spglobal.com/) | Generate company tearsheets, earnings previews, and fun",
      "default_branch": "main"
    },
    "fetched_at": "2026-03-01T03:37:34.900684"
  },
  {
    "basic_info": {
      "name": "PaperBanana",
      "full_name": "dwzhu-pku/PaperBanana",
      "owner": "dwzhu-pku",
      "description": "PaperBanana: Automating Academic Illustration For AI Scientists",
      "url": "https://github.com/dwzhu-pku/PaperBanana",
      "clone_url": "https://github.com/dwzhu-pku/PaperBanana.git",
      "ssh_url": "git@github.com:dwzhu-pku/PaperBanana.git",
      "homepage": "https://dwzhu-pku.github.io/PaperBanana/",
      "created_at": "2026-01-30T06:55:48Z",
      "updated_at": "2026-03-01T03:07:06Z",
      "pushed_at": "2026-02-25T14:05:50Z"
    },
    "stats": {
      "stars": 4362,
      "forks": 254,
      "watchers": 4362,
      "open_issues": 23,
      "size": 14698
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 242682,
        "JavaScript": 102057,
        "HTML": 23687,
        "CSS": 2275,
        "Shell": 2258,
        "Dockerfile": 516
      },
      "license": "Apache License 2.0",
      "topics": []
    },
    "content": {
      "readme": "# <div align=\"center\">PaperBanana ğŸŒ</div>\n<div align=\"center\">Dawei Zhu, Rui Meng, Yale Song, Xiyu Wei, Sujian Li, Tomas Pfister and Jinsung yoon\n<br><br></div>\n\n</div> \n<div align=\"center\">\n<a href=\"https://huggingface.co/papers/2601.23265\"><img src=\"assets/paper-page-xl.svg\" alt=\"Paper page on HF\"></a>\n<a href=\"https://huggingface.co/datasets/dwzhu/PaperBananaBench\"><img src=\"assets/dataset-on-hf-xl.svg\" alt=\"Dataset on HF\"></a>\n</div>\n\n> Hi everyone! The original version of PaperBanana is already open-sourced under Google-Research as [PaperVizAgent](https://github.com/google-research/papervizagent). \nThis repository forked the content of that repo and aims to keep evolving toward better support for academic paper illustrationâ€”though we have made solid progress, there is still a long way to go for more reliable generation and for more diverse, complex scenarios. PaperBanana is intended to be a fully open-source project dedicated to facilitating academic illustration for all researchers. Our goal is simply to benefit the community, so we currently have no plans to use it for commercial purposes.\n\n\n\n\n**PaperBanana** is a reference-driven multi-agent framework for automated academic illustration generation. Acting like a creative team of specialized agents, it transforms raw scientific content into publication-quality diagrams and plots through an orchestrated pipeline of **Retriever, Planner, Stylist, Visualizer, and Critic** agents. The framework leverages in-context learning from reference examples and iterative refinement to produce aesthetically pleasing and semantically accurate scientific illustrations.\n\nHere are some example diagrams and plots generated by PaperBanana:\n![Examples](assets/teaser_figure.jpg)\n\n## Overview of PaperBanana\n\n![PaperBanana Framework](assets/method_diagram.png)\n\nPaperBanana achieves high-quality academic illustration generation by orchestrating five specialized agents in a structured pipeline:\n\n1. **Retriever Agent**: Identifies the most relevant reference diagrams from a curated collection to guide downstream agents\n2. **Planner Agent**: Translates method content and communicative intent into comprehensive textual descriptions using in-context learning\n3. **Stylist Agent**: Refines descriptions to adhere to academic aesthetic standards using automatically synthesized style guidelines\n4. **Visualizer Agent**: Transforms textual descriptions into visual outputs using state-of-the-art image generation models\n5. **Critic Agent**: Forms a closed-loop refinement mechanism with the Visualizer through multi-round iterative improvements\n\n## Quick Start\n\n### Step1: Clone the Repo\n```bash\ngit clone https://github.com/dwzhu-pku/PaperBanana.git\ncd PaperBanana\n```\n\n### Step2: Configuration\nPaperBanana supports configuring API keys from a YAML configuration file or via environment variables. \n\nWe recommend duplicate the `configs/model_config.template.yaml` file into `configs/model_config.yaml` to externalize all user configurations. This file is ignored by git to keep your api keys and configurations secret. In `model_config.yaml`, remember to fill in the two model names (`defaults.model_name` and `defaults.image_model_name`) and set at least one API key under `api_keys` (e.g. `google_api_key` for Gemini models).\n\nNote that if you need to generate many candidates simultaneously, you will require an API key that supports high concurrency.\n\n### Step3: Downloading the Dataset\nFirst download [PaperBananaBench](https://huggingface.co/datasets/dwzhu/PaperBananaBench), then place it under the `data` directory (e.g., `data/PaperBananaBench/`). The framework is designed to function gracefully without the dataset by bypassing the Retriever Agent's few-shot learning capability. If interested in the original PDFs, please download them from [PaperBananaDiagramPDFs](https://huggingface.co/datasets/dwzhu/PaperBananaDiagramPDFs).\n\n### Step4: Installing the Environment\n1. We use `uv` to manage Python packages. Please install `uv` following the instructions [here](https://docs.astral.sh/uv/getting-started/installation/).\n\n2. Create and activate a virtual environment\n    ```bash\n    uv venv # This will create a virtual environment in the current directory, under .venv/\n    source .venv/bin/activate  # or .venv\\Scripts\\activate on Windows\n    ```\n\n3. Install python 3.12\n    ```bash\n    uv python install 3.12\n    ```\n\n4. Install required packages\n    ```bash\n    uv pip install -r requirements.txt\n    ```\n\n### Launch PaperBanana\n\n#### Interactive Demo (Streamlit)\nThe easiest way to launch PaperBanana is via the interactive Streamlit demo:\n```bash\nstreamlit run demo.py\n```\n\nThe web interface provides two main workflows:\n\n**1. Generate Candidates Tab**:\n- Paste your method section content (Markdown recommended) and provide the figure caption.\n- Configure settings (pipeline mode, retrieval setting, number of candidates, aspect ratio, critic rounds).\n- Click \"Generate Candidates\" and wait for parallel processing.\n-",
      "default_branch": "main"
    },
    "fetched_at": "2026-03-01T03:37:36.189511"
  },
  {
    "basic_info": {
      "name": "Agent-Reach",
      "full_name": "Panniantong/Agent-Reach",
      "owner": "Panniantong",
      "description": "Give your AI agent eyes to see the entire internet. Read & search Twitter, Reddit, YouTube, GitHub, Bilibili, XiaoHongShu â€” one CLI, zero API fees.",
      "url": "https://github.com/Panniantong/Agent-Reach",
      "clone_url": "https://github.com/Panniantong/Agent-Reach.git",
      "ssh_url": "git@github.com:Panniantong/Agent-Reach.git",
      "homepage": null,
      "created_at": "2026-02-24T02:10:24Z",
      "updated_at": "2026-03-01T03:35:02Z",
      "pushed_at": "2026-02-28T12:29:39Z"
    },
    "stats": {
      "stars": 3096,
      "forks": 249,
      "watchers": 3096,
      "open_issues": 0,
      "size": 513
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 90634,
        "Shell": 5544
      },
      "license": "MIT License",
      "topics": [
        "agent-infrastructure",
        "ai-agent",
        "ai-search",
        "automation",
        "bilibili",
        "claude-code",
        "cli",
        "cursor",
        "free-api",
        "llm-tools",
        "mcp",
        "python",
        "reddit-scraper",
        "twitter-scraper",
        "web-scraper",
        "xiaohongshu",
        "youtube-transcript"
      ]
    },
    "content": {
      "readme": "<h1 align=\"center\">ğŸ‘ï¸ Agent Reach</h1>\n\n<p align=\"center\">\n  <strong>ç»™ä½ çš„ AI Agent ä¸€é”®è£…ä¸Šäº’è”ç½‘èƒ½åŠ›</strong>\n</p>\n\n<p align=\"center\">\n  <a href=\"LICENSE\"><img src=\"https://img.shields.io/badge/License-MIT-blue.svg?style=for-the-badge\" alt=\"MIT License\"></a>\n  <a href=\"https://www.python.org/\"><img src=\"https://img.shields.io/badge/Python-3.10+-green.svg?style=for-the-badge&logo=python&logoColor=white\" alt=\"Python 3.8+\"></a>\n  <a href=\"https://github.com/Panniantong/agent-reach/stargazers\"><img src=\"https://img.shields.io/github/stars/Panniantong/agent-reach?style=for-the-badge\" alt=\"GitHub Stars\"></a>\n</p>\n\n<p align=\"center\">\n  <a href=\"#å¿«é€Ÿä¸Šæ‰‹\">å¿«é€Ÿå¼€å§‹</a> Â· <a href=\"docs/README_en.md\">English</a> Â· <a href=\"#æ”¯æŒçš„å¹³å°\">æ”¯æŒå¹³å°</a> Â· <a href=\"#è®¾è®¡ç†å¿µ\">è®¾è®¡ç†å¿µ</a>\n</p>\n\n---\n\n## ä¸ºä»€ä¹ˆéœ€è¦ Agent Reachï¼Ÿ\n\nAI Agent å·²ç»èƒ½å¸®ä½ å†™ä»£ç ã€æ”¹æ–‡æ¡£ã€ç®¡é¡¹ç›®â€”â€”ä½†ä½ è®©å®ƒå»ç½‘ä¸Šæ‰¾ç‚¹ä¸œè¥¿ï¼Œå®ƒå°±æŠ“çäº†ï¼š\n\n- ğŸ“º \"å¸®æˆ‘çœ‹çœ‹è¿™ä¸ª YouTube æ•™ç¨‹è®²äº†ä»€ä¹ˆ\" â†’ **çœ‹ä¸äº†**ï¼Œæ‹¿ä¸åˆ°å­—å¹•\n- ğŸ¦ \"å¸®æˆ‘æœä¸€ä¸‹æ¨ç‰¹ä¸Šå¤§å®¶æ€ä¹ˆè¯„ä»·è¿™ä¸ªäº§å“\" â†’ **æœä¸äº†**ï¼ŒTwitter API è¦ä»˜è´¹\n- ğŸ“– \"å» Reddit ä¸Šçœ‹çœ‹æœ‰æ²¡æœ‰äººé‡åˆ°è¿‡åŒæ ·çš„ bug\" â†’ **403 è¢«å°**ï¼ŒæœåŠ¡å™¨ IP è¢«æ‹’\n- ğŸ“• \"å¸®æˆ‘çœ‹çœ‹å°çº¢ä¹¦ä¸Šè¿™ä¸ªå“çš„å£ç¢‘\" â†’ **æ‰“ä¸å¼€**ï¼Œå¿…é¡»ç™»å½•æ‰èƒ½çœ‹\n- ğŸ“º \"Bç«™ä¸Šæœ‰ä¸ªæŠ€æœ¯è§†é¢‘ï¼Œå¸®æˆ‘æ€»ç»“ä¸€ä¸‹\" â†’ **è¿ä¸ä¸Š**ï¼Œæµ·å¤–/æœåŠ¡å™¨ IP è¢«å±è”½\n- ğŸ” \"å¸®æˆ‘åœ¨ç½‘ä¸Šæœä¸€ä¸‹æœ€æ–°çš„ LLM æ¡†æ¶å¯¹æ¯”\" â†’ **æ²¡æœ‰å¥½ç”¨çš„æœç´¢**ï¼Œè¦ä¹ˆä»˜è´¹è¦ä¹ˆè´¨é‡å·®\n- ğŸŒ \"å¸®æˆ‘çœ‹çœ‹è¿™ä¸ªç½‘é¡µå†™äº†å•¥\" â†’ **æŠ“å›æ¥ä¸€å † HTML æ ‡ç­¾**ï¼Œæ ¹æœ¬æ²¡æ³•è¯»\n- ğŸ“¦ \"è¿™ä¸ª GitHub ä»“åº“æ˜¯å¹²å˜›çš„ï¼ŸIssue é‡Œè¯´äº†ä»€ä¹ˆï¼Ÿ\" â†’ èƒ½ç”¨ï¼Œä½†è®¤è¯é…ç½®å¾ˆéº»çƒ¦\n- ğŸ“¡ \"å¸®æˆ‘è®¢é˜…è¿™å‡ ä¸ª RSS æºï¼Œæœ‰æ›´æ–°å‘Šè¯‰æˆ‘\" â†’ è¦è‡ªå·±è£…åº“å†™ä»£ç \n\n**è¿™äº›ä¸éš¾å®ç°ï¼Œä½†æ˜¯éœ€è¦è‡ªå·±æŠ˜è…¾é…ç½®**\n\næ¯ä¸ªå¹³å°éƒ½æœ‰è‡ªå·±çš„é—¨æ§›â€”â€”è¦ä»˜è´¹çš„ APIã€è¦ç»•è¿‡çš„å°é”ã€è¦ç™»å½•çš„è´¦å·ã€è¦æ¸…æ´—çš„æ•°æ®ã€‚ä½ è¦ä¸€ä¸ªä¸€ä¸ªå»è¸©å‘ã€è£…å·¥å…·ã€è°ƒé…ç½®ï¼Œå…‰æ˜¯è®© Agent èƒ½è¯»ä¸ªæ¨ç‰¹å°±å¾—æŠ˜è…¾åŠå¤©ã€‚\n\n**Agent Reach æŠŠè¿™ä»¶äº‹å˜æˆä¸€å¥è¯ï¼š**\n\n```\nå¸®æˆ‘å®‰è£… Agent Reachï¼šhttps://raw.githubusercontent.com/Panniantong/agent-reach/main/docs/install.md\n```\n\nå¤åˆ¶ç»™ä½ çš„ Agentï¼Œå‡ åˆ†é’Ÿåå®ƒå°±èƒ½è¯»æ¨ç‰¹ã€æœ Redditã€çœ‹ YouTubeã€åˆ·å°çº¢ä¹¦äº†ã€‚\n\n> â­ **Star è¿™ä¸ªé¡¹ç›®**ï¼Œæˆ‘ä»¬ä¼šæŒç»­è¿½è¸ªå„å¹³å°çš„å˜åŒ–ã€æ¥å…¥æ–°çš„æ¸ é“ã€‚ä½ ä¸ç”¨è‡ªå·±ç›¯â€”â€”å¹³å°å°äº†æˆ‘ä»¬ä¿®ï¼Œæœ‰æ–°æ¸ é“æˆ‘ä»¬åŠ ã€‚\n\n### âœ… åœ¨ä½ ç”¨ä¹‹å‰ï¼Œä½ å¯èƒ½æƒ³çŸ¥é“\n\n| | |\n|---|---|\n| ğŸ’° **å®Œå…¨å…è´¹** | æ‰€æœ‰å·¥å…·å¼€æºã€æ‰€æœ‰ API å…è´¹ã€‚å”¯ä¸€å¯èƒ½èŠ±é’±çš„æ˜¯æœåŠ¡å™¨ä»£ç†ï¼ˆ$1/æœˆï¼‰ï¼Œæœ¬åœ°ç”µè„‘ä¸éœ€è¦ |\n| ğŸ”’ **éšç§å®‰å…¨** | Cookie åªå­˜åœ¨ä½ æœ¬åœ°ï¼Œä¸ä¸Šä¼ ä¸å¤–ä¼ ã€‚ä»£ç å®Œå…¨å¼€æºï¼Œéšæ—¶å¯å®¡æŸ¥ |\n| ğŸ”„ **æŒç»­æ›´æ–°** | åº•å±‚å·¥å…·ï¼ˆyt-dlpã€xreachã€Jina Reader ç­‰ï¼‰å®šæœŸè¿½è¸ªæ›´æ–°åˆ°æœ€æ–°ç‰ˆï¼Œä½ ä¸ç”¨è‡ªå·±ç›¯ |\n| ğŸ¤– **å…¼å®¹æ‰€æœ‰ Agent** | Claude Codeã€OpenClawã€Cursorã€Windsurfâ€¦â€¦ä»»ä½•èƒ½è·‘å‘½ä»¤è¡Œçš„ Agent éƒ½èƒ½ç”¨ |\n| ğŸ©º **è‡ªå¸¦è¯Šæ–­** | `agent-reach doctor` ä¸€æ¡å‘½ä»¤å‘Šè¯‰ä½ å“ªä¸ªé€šã€å“ªä¸ªä¸é€šã€æ€ä¹ˆä¿® |\n\n---\n\n## æ”¯æŒçš„å¹³å°\n\n| å¹³å° | è£…å¥½å³ç”¨ | é…ç½®åè§£é” | æ€ä¹ˆé… |\n|------|---------|-----------|-------|\n| ğŸŒ **ç½‘é¡µ** | é˜…è¯»ä»»æ„ç½‘é¡µ | â€” | æ— éœ€é…ç½® |\n| ğŸ“º **YouTube** | å­—å¹•æå– + è§†é¢‘æœç´¢ | â€” | æ— éœ€é…ç½® |\n| ğŸ“¡ **RSS** | é˜…è¯»ä»»æ„ RSS/Atom æº | â€” | æ— éœ€é…ç½® |\n| ğŸ” **å…¨ç½‘æœç´¢** | â€” | å…¨ç½‘è¯­ä¹‰æœç´¢ | è‡ªåŠ¨é…ç½®ï¼ˆMCP æ¥å…¥ï¼Œå…è´¹æ— éœ€ Keyï¼‰ |\n| ğŸ“¦ **GitHub** | è¯»å…¬å¼€ä»“åº“ + æœç´¢ | ç§æœ‰ä»“åº“ã€æ Issue/PRã€Fork | å‘Šè¯‰ Agentã€Œå¸®æˆ‘ç™»å½• GitHubã€ |\n| ğŸ¦ **Twitter/X** | è¯»å•æ¡æ¨æ–‡ | æœç´¢æ¨æ–‡ã€æµè§ˆæ—¶é—´çº¿ã€å‘æ¨ | å‘Šè¯‰ Agentã€Œå¸®æˆ‘é… Twitterã€ |\n| ğŸ“º **Bç«™** | æœ¬åœ°ï¼šå­—å¹•æå– + æœç´¢ | æœåŠ¡å™¨ä¹Ÿèƒ½ç”¨ | å‘Šè¯‰ Agentã€Œå¸®æˆ‘é…ä»£ç†ã€ |\n| ğŸ“– **Reddit** | æœç´¢ï¼ˆé€šè¿‡ Exa å…è´¹ï¼‰ | è¯»å¸–å­å’Œè¯„è®º | å‘Šè¯‰ Agentã€Œå¸®æˆ‘é…ä»£ç†ã€ |\n| ğŸ“• **å°çº¢ä¹¦** | â€” | é˜…è¯»ã€æœç´¢ã€å‘å¸–ã€è¯„è®ºã€ç‚¹èµ | å‘Šè¯‰ Agentã€Œå¸®æˆ‘é…å°çº¢ä¹¦ã€ |\n| ğŸµ **æŠ–éŸ³** | â€” | è§†é¢‘è§£æã€æ— æ°´å°ä¸‹è½½é“¾æ¥è·å– | å‘Šè¯‰ Agentã€Œå¸®æˆ‘é…æŠ–éŸ³ã€ |\n| ğŸ’¼ **LinkedIn** | Jina Reader è¯»å…¬å¼€é¡µé¢ | Profile è¯¦æƒ…ã€å…¬å¸é¡µé¢ã€èŒä½æœç´¢ | å‘Šè¯‰ Agentã€Œå¸®æˆ‘é… LinkedInã€ |\n| ğŸ¢ **Bossç›´è˜** | Jina Reader è¯»èŒä½é¡µ | æœç´¢èŒä½ã€å‘ HR æ‰“æ‹›å‘¼ | å‘Šè¯‰ Agentã€Œå¸®æˆ‘é… Bossç›´è˜ã€ |\n\n> **ä¸çŸ¥é“æ€ä¹ˆé…ï¼Ÿä¸ç”¨æŸ¥æ–‡æ¡£ã€‚** ç›´æ¥å‘Šè¯‰ Agentã€Œå¸®æˆ‘é… XXXã€ï¼Œå®ƒçŸ¥é“éœ€è¦ä»€ä¹ˆã€ä¼šä¸€æ­¥ä¸€æ­¥å¼•å¯¼ä½ ã€‚\n>\n> ğŸª éœ€è¦ Cookie çš„å¹³å°ï¼ˆTwitterã€å°çº¢ä¹¦ç­‰ï¼‰ï¼Œ**ä¼˜å…ˆä½¿ç”¨** Chrome æ’ä»¶ [Cookie-Editor](https://chromewebstore.google.com/detail/cookie-editor/hlkenndednhfkekhgcdicdfddnkalmdm) å¯¼å‡º Cookieï¼Œå‘ç»™ Agent å³å¯é…ç½®ã€‚æµç¨‹ç»Ÿä¸€ï¼šæµè§ˆå™¨ç™»å½• â†’ Cookie-Editor å¯¼å‡º â†’ å‘ç»™ Agentã€‚æ¯”æ‰«ç æ›´ç®€å•å¯é ã€‚\n>\n> ğŸ”’ Cookie åªå­˜åœ¨ä½ æœ¬åœ°ï¼Œä¸ä¸Šä¼ ä¸å¤–ä¼ ã€‚ä»£ç å®Œå…¨å¼€æºï¼Œéšæ—¶å¯å®¡æŸ¥ã€‚\n> ğŸ’» æœ¬åœ°ç”µè„‘ä¸éœ€è¦ä»£ç†ã€‚ä»£ç†åªæœ‰éƒ¨ç½²åœ¨æœåŠ¡å™¨ä¸Šæ‰éœ€è¦ï¼ˆ~$1/æœˆï¼‰ã€‚\n\n---\n\n## å¿«é€Ÿä¸Šæ‰‹\n\nå¤åˆ¶è¿™å¥è¯ç»™ä½ çš„ AI Agentï¼ˆClaude Codeã€OpenClawã€Cursor ç­‰ï¼‰ï¼š\n\n```\nå¸®æˆ‘å®‰è£… Agent Reachï¼šhttps://raw.githubusercontent.com/Panniantong/agent-reach/main/docs/install.md\n```\n\nå°±è¿™ä¸€æ­¥ã€‚Agent ä¼šè‡ªå·±å®Œæˆå‰©ä¸‹çš„æ‰€æœ‰äº‹æƒ…ã€‚\n\n> ğŸ›¡ï¸ **æ‹…å¿ƒå®‰å…¨ï¼Ÿ** å¯ä»¥ç”¨å®‰å…¨æ¨¡å¼â€”â€”ä¸ä¼šè‡ªåŠ¨è£…ç³»ç»ŸåŒ…ï¼Œåªå‘Šè¯‰ä½ éœ€è¦ä»€ä¹ˆï¼š\n> ```\n> å¸®æˆ‘å®‰è£… Agent Reachï¼ˆå®‰å…¨æ¨¡å¼ï¼‰ï¼šhttps://raw.githubusercontent.com/Panniantong/agent-reach/main/docs/install.md\n> å®‰è£…æ—¶ä½¿ç”¨ --safe å‚æ•°\n> ```\n\n<details>\n<summary>å®ƒä¼šåšä»€ä¹ˆï¼Ÿï¼ˆç‚¹å‡»å±•å¼€ï¼‰</summary>\n\n1. **å®‰è£… CLI å·¥å…·** â€” `pip install` è£…å¥½ `agent-reach` å‘½ä»¤è¡Œ\n2. **å®‰è£…ç³»ç»Ÿä¾èµ–** â€” è‡ªåŠ¨æ£€æµ‹å¹¶å®‰è£… Node.jsã€gh CLIã€mcporterã€xreach ç­‰\n3. **é…ç½®æœç´¢å¼•æ“** â€” é€šè¿‡ MCP æ¥å…¥ Exaï¼ˆå…è´¹ï¼Œæ— éœ€ API Keyï¼‰\n4. **æ£€æµ‹ç¯å¢ƒ** â€” åˆ¤æ–­æ˜¯æœ¬åœ°ç”µè„‘è¿˜æ˜¯æœåŠ¡å™¨ï¼Œç»™å‡ºå¯¹åº”çš„é…ç½®å»ºè®®\n5. **æ³¨å†Œ SKILL.md** â€” åœ¨ Agent çš„ skills ç›®å½•å®‰è£…ä½¿ç”¨æŒ‡å—ï¼Œä»¥å Agent é‡åˆ°\"æœæ¨ç‰¹\"ã€\"çœ‹è§†é¢‘\"è¿™ç±»éœ€æ±‚ï¼Œä¼šè‡ªåŠ¨çŸ¥é“è¯¥è°ƒå“ªä¸ªä¸Šæ¸¸å·¥å…·\n\nå®‰è£…å®Œä¹‹åï¼Œ`agent-reach doctor` ä¸€æ¡å‘½ä»¤å‘Šè¯‰ä½ æ¯ä¸ªæ¸ é“çš„çŠ¶æ€ã€‚\n</details>\n\n---\n\n## è£…å¥½å°±èƒ½ç”¨\n\nä¸éœ€è¦ä»»ä½•é…ç½®ï¼Œå‘Šè¯‰ Agent å°±è¡Œï¼š\n\n- \"å¸®æˆ‘çœ‹çœ‹è¿™ä¸ªé“¾æ¥\" â†’ `curl https://r.jina.ai/URL` è¯»ä»»æ„ç½‘é¡µ\n- \"è¿™ä¸ª GitHub ä»“åº“æ˜¯åšä»€ä¹ˆçš„\" â†’ `gh repo view owner/repo`\n- \"è¿™ä¸ªè§†é¢‘è®²äº†ä»€ä¹ˆ\" â†’ `yt-dlp --dump-json URL` æå–å­—å¹•\n- \"å¸®æˆ‘çœ‹çœ‹è¿™æ¡æ¨æ–‡\" â†’ `xreach tweet URL --json`\n- \"è®¢é˜…è¿™ä¸ª RSS\" â†’ `feedparser` è§£æ\n- \"æœä¸€ä¸‹ GitHub ä¸Šæœ‰ä»€ä¹ˆ LLM æ¡†æ¶\" â†’ `gh search repos \"LLM framework\"`\n\n**ä¸éœ€è¦è®°å‘½ä»¤ã€‚** Agent è¯»äº† SKILL.md ä¹‹åè‡ªå·±çŸ¥é“è¯¥è°ƒä»€ä¹ˆã€‚\n\n---\n\n## è®¾è®¡ç†å¿µ\n\n**Agent Reach æ˜¯ä¸€ä¸ªè„šæ‰‹æ¶ï¼ˆscaffoldingï¼‰ï¼Œä¸æ˜¯æ¡†æ¶ã€‚**\n\nä½ ç»™ä¸€ä¸ªæ–° Agent è£…ç¯å¢ƒçš„æ—¶å€™ï¼Œæ€»è¦èŠ±æ—¶é—´å»æ‰¾å·¥å…·ã€è£…ä¾èµ–ã€è°ƒé…ç½®â€”â€”Twitter ç”¨ä»€ä¹ˆè¯»ï¼ŸReddit æ€ä¹ˆç»•å°ï¼ŸYouTube å­—å¹•æ€ä¹ˆæå–ï¼Ÿæ¯æ¬¡éƒ½è¦é‡æ–°è¸©ä¸€éã€‚\n\nAgent Reach åšçš„äº‹æƒ…å¾ˆç®€å•ï¼š**å¸®ä½ æŠŠè¿™äº›é€‰å‹å’Œé…ç½®çš„æ´»å„¿åšå®Œäº†ã€‚**\n\nå®‰è£…å®Œæˆåï¼ŒAgent ç›´æ¥è°ƒç”¨ä¸Šæ¸¸å·¥å…·ï¼ˆxreach CLIã€yt-dlpã€mcporterã€gh CLI ç­‰ï¼‰ï¼Œä¸éœ€è¦ç»è¿‡ Agent Reach çš„åŒ…è£…å±‚ã€‚\n\n### ğŸ”Œ æ¯ä¸ªæ¸ é“éƒ½æ˜¯å¯æ’æ‹”çš„\n\næ¯ä¸ªå¹³å°èƒŒåæ˜¯ä¸€ä¸ªç‹¬ç«‹çš„ä¸Šæ¸¸å·¥å…·ã€‚**ä¸æ»¡æ„ï¼Ÿæ¢æ‰å°±è¡Œã€‚**\n\n```\nchannels/\nâ”œâ”€â”€ web.py          â†’ Jina Reader     â† å¯ä»¥æ¢æˆ Firecrawlã€Crawl4AIâ€¦â€¦\nâ”œâ”€â”€ twitter.py      â†’ xreach            â† å¯ä»¥æ¢æˆ Nitterã€å®˜æ–¹ APIâ€¦â€¦\nâ”œâ”€â”€ youtube.py      â†’ yt-dlp          â† å¯ä»¥æ¢æˆ YouTube APIã€Whisperâ€¦â€¦\nâ”œâ”€â”€ github.py       â†’ gh CLI          â† å¯ä»¥æ¢æˆ REST APIã€PyGithubâ€¦â€¦\nâ”œâ”€â”€ bilibili.py     â†’ yt-dlp          â† å¯ä»¥æ¢æˆ bilibili-apiâ€¦â€¦\nâ”œâ”€â”€ reddit.py       â†’ JSON API + Exa  â† å¯ä»¥æ¢æˆ PRAWã€Pushshiftâ€¦â€¦\nâ”œâ”€â”€ xiaohongshu.py  â†’ mcporter MCP    â† å¯ä»¥æ¢æˆå…¶ä»– XH",
      "default_branch": "main"
    },
    "fetched_at": "2026-03-01T03:37:37.480852"
  },
  {
    "basic_info": {
      "name": "CoPaw",
      "full_name": "agentscope-ai/CoPaw",
      "owner": "agentscope-ai",
      "description": "Your Personal AI Assistant; easy to install, deploy on your own machine or on the cloud; supports multiple chat apps with easily extensible capabilities.",
      "url": "https://github.com/agentscope-ai/CoPaw",
      "clone_url": "https://github.com/agentscope-ai/CoPaw.git",
      "ssh_url": "git@github.com:agentscope-ai/CoPaw.git",
      "homepage": "http://copaw.agentscope.io/",
      "created_at": "2026-02-24T03:42:56Z",
      "updated_at": "2026-03-01T03:37:10Z",
      "pushed_at": "2026-03-01T00:10:33Z"
    },
    "stats": {
      "stars": 2235,
      "forks": 214,
      "watchers": 2235,
      "open_issues": 84,
      "size": 7441
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 1309049,
        "TypeScript": 395459,
        "Less": 31273,
        "CSS": 17299,
        "Shell": 17238,
        "PowerShell": 14393,
        "JavaScript": 4632,
        "Dockerfile": 2665,
        "HTML": 1087
      },
      "license": "Apache License 2.0",
      "topics": []
    },
    "content": {
      "readme": "<div align=\"center\">\n\n# CoPaw\n\n[![GitHub Repo](https://img.shields.io/badge/GitHub-Repo-black.svg?logo=github)](https://github.com/agentscope-ai/CoPaw)\n[![PyPI](https://img.shields.io/pypi/v/copaw?color=3775A9&label=PyPI&logo=pypi)](https://pypi.org/project/copaw/)\n[![Documentation](https://img.shields.io/badge/Docs-Website-green.svg?logo=readthedocs&label=Docs)](https://copaw.agentscope.io/)\n[![Python Version](https://img.shields.io/badge/python-3.10%20~%20%3C3.14-blue.svg?logo=python&label=Python)](https://www.python.org/downloads/)\n[![Last Commit](https://img.shields.io/github/last-commit/agentscope-ai/CoPaw)](https://github.com/agentscope-ai/CoPaw)\n[![License](https://img.shields.io/badge/license-Apache%202.0-red.svg?logo=apache&label=License)](LICENSE)\n[![Code Style](https://img.shields.io/badge/code%20style-black-black.svg?logo=python&label=CodeStyle)](https://github.com/psf/black)\n[![GitHub Stars](https://img.shields.io/github/stars/agentscope-ai/CoPaw?style=flat&logo=github&color=yellow&label=Stars)](https://github.com/agentscope-ai/CoPaw/stargazers)\n[![GitHub Forks](https://img.shields.io/github/forks/agentscope-ai/CoPaw?style=flat&logo=github&color=purple&label=Forks)](https://github.com/agentscope-ai/CoPaw/network)\n[![DeepWiki](https://img.shields.io/badge/DeepWiki-Ask_Devin-navy.svg?logo=data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACwAAAAyCAYAAAAnWDnqAAAAAXNSR0IArs4c6QAAA05JREFUaEPtmUtyEzEQhtWTQyQLHNak2AB7ZnyXZMEjXMGeK/AIi+QuHrMnbChYY7MIh8g01fJoopFb0uhhEqqcbWTp06/uv1saEDv4O3n3dV60RfP947Mm9/SQc0ICFQgzfc4CYZoTPAswgSJCCUJUnAAoRHOAUOcATwbmVLWdGoH//PB8mnKqScAhsD0kYP3j/Yt5LPQe2KvcXmGvRHcDnpxfL2zOYJ1mFwrryWTz0advv1Ut4CJgf5uhDuDj5eUcAUoahrdY/56ebRWeraTjMt/00Sh3UDtjgHtQNHwcRGOC98BJEAEymycmYcWwOprTgcB6VZ5JK5TAJ+fXGLBm3FDAmn6oPPjR4rKCAoJCal2eAiQp2x0vxTPB3ALO2CRkwmDy5WohzBDwSEFKRwPbknEggCPB/imwrycgxX2NzoMCHhPkDwqYMr9tRcP5qNrMZHkVnOjRMWwLCcr8ohBVb1OMjxLwGCvjTikrsBOiA6fNyCrm8V1rP93iVPpwaE+gO0SsWmPiXB+jikdf6SizrT5qKasx5j8ABbHpFTx+vFXp9EnYQmLx02h1QTTrl6eDqxLnGjporxl3NL3agEvXdT0WmEost648sQOYAeJS9Q7bfUVoMGnjo4AZdUMQku50McDcMWcBPvr0SzbTAFDfvJqwLzgxwATnCgnp4wDl6Aa+Ax283gghmj+vj7feE2KBBRMW3FzOpLOADl0Isb5587h/U4gGvkt5v60Z1VLG8BhYjbzRwyQZemwAd6cCR5/XFWLYZRIMpX39AR0tjaGGiGzLVyhse5C9RKC6ai42ppWPKiBagOvaYk8lO7DajerabOZP46Lby5wKjw1HCRx7p9sVMOWGzb/vA1hwiWc6jm3MvQDTogQkiqIhJV0nBQBTU+3okKCFDy9WwferkHjtxib7t3xIUQtHxnIwtx4mpg26/HfwVNVDb4oI9RHmx5WGelRVlrtiw43zboCLaxv46AZeB3IlTkwouebTr1y2NjSpHz68WNFjHvupy3q8TFn3Hos2IAk4Ju5dCo8B3wP7VPr/FGaKiG+T+v+TQqIrOqMTL1VdWV1DdmcbO8KXBz6esmYWYKPwDL5b5FA1a0hwapHiom0r/cKaoqr+27/XcrS5UwSMbQAAAABJRU5ErkJggg==)](https://deepwiki.com/agentscope-ai/CoPaw)\n[![Discord](https://img.shields.io/badge/Discord-Join_Us-blueviolet.svg?logo=discord)](https://discord.gg/eYMpfnkG8h)\n[![DingTalk](https://img.shields.io/badge/DingTalk-Join_Us-orange.svg)](https://qr.dingtalk.com/action/joingroup?code=v1,k1,OmDlBXpjW+I2vWjKDsjvI9dhcXjGZi3bQiojOq3dlDw=&_dt_no_comment=1&origin=11)\n\n[[Documentation](https://copaw.agentscope.io/)] [[ä¸­æ–‡ README](README_zh.md)]\n\n<p align=\"center\">\n  <img src=\"https://img.alicdn.com/imgextra/i1/O1CN01tvT5rg1JHQNRP8tXR_!!6000000001003-2-tps-1632-384.png\" alt=\"CoPaw Logo\" width=\"120\">\n</p>\n\n<p align=\"center\"><b>Works for you, grows with you.</b></p>\n\n</div>\n\nYour Personal AI Assistant; easy to install, deploy on your own machine or on the cloud; supports multiple chat apps with easily extensible capabilities.\n\n> **Core capabilities:**\n>\n> **Every channel** â€” DingTalk, Feishu, QQ, Discord, iMessage, and more. One assistant, connect as you need.\n>\n> **Under your control** â€” Memory and personalization under your control. Deploy locally or in the cloud; scheduled reminders to any channel.\n>\n> **Skills** â€” Built-in cron; custom skills in your workspace, auto-loaded. No lock-in.\n>\n> <details>\n> <summary><b>What you can do</b></summary>\n>\n> <br>\n>\n> - **Social**: daily digest of hot posts (Xiaohongshu, Zhihu, Reddit), Bilibili/YouTube summaries.\n> - **Productivity**: newsletter digests to DingTalk/Feishu/QQ, contacts from email/calendar.\n> - **Creative**: describe your goal, run overnight, get a draft next day.\n> - **Research**: track tech/AI news, personal knowledge base.\n> - **Desktop**: organize files, read/summarize docs, request files in chat.\n> - **Explore**: combine Skills and cron into your own agentic app.\n>\n> </details>\n\n---\n\n## Table of Contents\n\n> **Recommended reading:**\n>\n> - **I want to run CoPaw in 3 commands**: [Quick Start](#quick-start) â†’ open Console in browser.\n> - **I want to chat in DingTalk / Feishu / QQ**: [Quick Start](#quick-start) â†’ [Channels](https://copaw.agentscope.io/docs/channels).\n> - **I donâ€™t want to install Python**: [One-line install](#one-line-install-beta-continuously-improving) handles Python automatically, or use [ModelScope one-click](https://modelscope.cn/studios/fork?target=AgentScope/CoPaw) for cloud.\n\n- [Quick Start](#quick-start)\n- [API Key](#api-key)\n- [Local Models](#local-models)\n- [Documentation](#documentation)\n- [Install from source](#install",
      "default_branch": "main"
    },
    "fetched_at": "2026-03-01T03:37:38.763915"
  },
  {
    "basic_info": {
      "name": "GLM-OCR",
      "full_name": "zai-org/GLM-OCR",
      "owner": "zai-org",
      "description": "GLM-OCR: Accurate Ã—  Fast Ã— Comprehensive",
      "url": "https://github.com/zai-org/GLM-OCR",
      "clone_url": "https://github.com/zai-org/GLM-OCR.git",
      "ssh_url": "git@github.com:zai-org/GLM-OCR.git",
      "homepage": "",
      "created_at": "2026-02-02T12:59:43Z",
      "updated_at": "2026-02-28T21:40:02Z",
      "pushed_at": "2026-02-26T05:03:09Z"
    },
    "stats": {
      "stars": 1815,
      "forks": 125,
      "watchers": 1815,
      "open_issues": 41,
      "size": 100607
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 467944,
        "TypeScript": 306509,
        "CSS": 10590,
        "Shell": 6033,
        "Dockerfile": 3009,
        "HTML": 555
      },
      "license": "Apache License 2.0",
      "topics": [
        "glm",
        "image2text",
        "ocr"
      ]
    },
    "content": {
      "readme": "## GLM-OCR\n\n<div align=\"center\">\n<img src=resources/logo.svg width=\"40%\"/>\n</div>\n<p align=\"center\">\n    ğŸ‘‹ Join our <a href=\"resources/WECHAT.md\" target=\"_blank\">WeChat</a> and <a href=\"https://discord.gg/QR7SARHRxK\" target=\"_blank\">Discord</a> community\n    <br>\n    ğŸ“ Use GLM-OCR's <a href=\"https://docs.z.ai/guides/vlm/glm-ocr\" target=\"_blank\">API</a>\n</p>\n\n<div align=\"center\">\n  <a href=\"README_zh.md\">ç®€ä½“ä¸­æ–‡</a> | English\n</div>\n\n### Model Introduction\n\nGLM-OCR is a multimodal OCR model for complex document understanding, built on the GLM-V encoderâ€“decoder architecture. It introduces Multi-Token Prediction (MTP) loss and stable full-task reinforcement learning to improve training efficiency, recognition accuracy, and generalization. The model integrates the CogViT visual encoder pre-trained on large-scale imageâ€“text data, a lightweight cross-modal connector with efficient token downsampling, and a GLM-0.5B language decoder. Combined with a two-stage pipeline of layout analysis and parallel recognition based on PP-DocLayout-V3, GLM-OCR delivers robust and high-quality OCR performance across diverse document layouts.\n\n**Key Features**\n\n- **State-of-the-Art Performance**: Achieves a score of 94.62 on OmniDocBench V1.5, ranking #1 overall, and delivers state-of-the-art results across major document understanding benchmarks, including formula recognition, table recognition, and information extraction.\n\n- **Optimized for Real-World Scenarios**: Designed and optimized for practical business use cases, maintaining robust performance on complex tables, code-heavy documents, seals, and other challenging real-world layouts.\n\n- **Efficient Inference**: With only 0.9B parameters, GLM-OCR supports deployment via vLLM, SGLang, and Ollama, significantly reducing inference latency and compute cost, making it ideal for high-concurrency services and edge deployments.\n\n- **Easy to Use**: Fully open-sourced and equipped with a comprehensive [SDK](https://github.com/zai-org/GLM-OCR) and inference toolchain, offering simple installation, one-line invocation, and smooth integration into existing production pipelines.\n\n### News & Updates\n\n- **[Coming Soon]** GLM-OCR Technical Report\n- **[2026.2.12]** Fine-tuning tutorial based on LLaMA-Factory is now available. See: [GLM-OCR Fine-tuning Guide](examples/finetune/README.md)\n\n### Download Model\n\n| Model   | Download Links                                                                                                              | Precision |\n| ------- | --------------------------------------------------------------------------------------------------------------------------- | --------- |\n| GLM-OCR | [ğŸ¤— Hugging Face](https://huggingface.co/zai-org/GLM-OCR)<br> [ğŸ¤– ModelScope](https://modelscope.cn/models/ZhipuAI/GLM-OCR) | BF16      |\n\n## GLM-OCR SDK\n\nWe provide an SDK for using GLM-OCR more efficiently and conveniently.\n\n### Install SDK\n\n> [UV Installation](https://docs.astral.sh/uv/getting-started/installation/)\n\n```bash\n# Install from source\ngit clone https://github.com/zai-org/glm-ocr.git\ncd glm-ocr\nuv venv --python 3.12 --seed && source .venv/bin/activate\nuv pip install -e .\n```\n\n### Model Deployment\n\nTwo ways to use GLM-OCR:\n\n#### Option 1: Zhipu MaaS API (Recommended for Quick Start)\n\nUse the hosted cloud API â€“ no GPU needed. The cloud service runs the complete GLM-OCR pipeline internally, so the SDK simply forwards your request and returns the result.\n\n1. Get an API key from https://open.bigmodel.cn\n2. Configure `config.yaml`:\n\n```yaml\npipeline:\n  maas:\n    enabled: true # Enable MaaS mode\n    api_key: your-api-key # Required\n```\n\nThat's it! When `maas.enabled=true`, the SDK acts as a thin wrapper that:\n\n- Forwards your documents to the Zhipu cloud API\n- Returns the results directly (Markdown + JSON layout details)\n- No local processing, no GPU required\n\nInput note (MaaS): the upstream API accepts `file` as a URL or a `data:<mime>;base64,...` data URI.\nIf you have raw base64 without the `data:` prefix, wrap it as a data URI (recommended). The SDK will\nauto-wrap local file paths / bytes / raw base64 into a data URI when calling MaaS.\n\nAPI documentation: https://docs.bigmodel.cn/cn/guide/models/vlm/glm-ocr\n\n#### Option 2: Self-host with vLLM / SGLang\n\nDeploy the GLM-OCR model locally for full control. The SDK provides the complete pipeline: layout detection, parallel region OCR, and result formatting.\n\n##### Using vLLM\n\nInstall vLLM:\n\n```bash\nuv pip install -U vllm --torch-backend=auto --extra-index-url https://wheels.vllm.ai/nightly\n# Or use Docker\ndocker pull vllm/vllm-openai:nightly\n```\n\nLaunch the service:\n\n```bash\n# In docker container, uv may not be need for transformers install\nuv pip install git+https://github.com/huggingface/transformers.git\n\n# Run with MTP for better performance\nvllm serve zai-org/GLM-OCR --allowed-local-media-path / --port 8080 --speculative-config '{\"method\": \"mtp\", \"num_speculative_tokens\": 1}' --served-model-name glm-ocr\n```\n\n##### Using SGLang\n\nIn",
      "default_branch": "main"
    },
    "fetched_at": "2026-03-01T03:37:40.110546"
  },
  {
    "basic_info": {
      "name": "dash",
      "full_name": "agno-agi/dash",
      "owner": "agno-agi",
      "description": "Self-learning data agent that grounds its answers in 6 layers of context. Inspired by OpenAI's in-house implementation.",
      "url": "https://github.com/agno-agi/dash",
      "clone_url": "https://github.com/agno-agi/dash.git",
      "ssh_url": "git@github.com:agno-agi/dash.git",
      "homepage": "",
      "created_at": "2026-01-30T13:54:17Z",
      "updated_at": "2026-02-28T22:33:19Z",
      "pushed_at": "2026-02-16T23:56:00Z"
    },
    "stats": {
      "stars": 1728,
      "forks": 192,
      "watchers": 1728,
      "open_issues": 7,
      "size": 174
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 52313,
        "Shell": 10886,
        "Dockerfile": 662
      },
      "license": "Apache License 2.0",
      "topics": []
    },
    "content": {
      "readme": "# Dash\n\nDash is a **self-learning data agent** that grounds its answers in **6 layers of context** and improves with every run.\n\nInspired by [OpenAI's in-house data agent](https://openai.com/index/inside-our-in-house-data-agent/).\n\n## Get Started\n\n```sh\n# Clone the repo\ngit clone https://github.com/agno-agi/dash.git && cd dash\n\n# Add OPENAI_API_KEY\ncp example.env .env\n# Edit .env and add your key\n\n# Start the application\ndocker compose up -d --build\n\n# Load sample data and knowledge\ndocker exec -it dash-api python -m dash.scripts.load_data\ndocker exec -it dash-api python -m dash.scripts.load_knowledge\n```\n\nConfirm dash is running at [http://localhost:8000/docs](http://localhost:8000/docs).\n\n## Connect to the Web UI\n\n1. Open [os.agno.com](https://os.agno.com) and login\n2. Add OS â†’ Local â†’ `http://localhost:8000`\n3. Click \"Connect\"\n\n**Try it** (sample F1 dataset):\n\n- Who won the most F1 World Championships?\n- How many races has Lewis Hamilton won?\n- Compare Ferrari vs Mercedes points 2015-2020\n\n## Why Text-to-SQL Breaks in Practice\n\nOur goal is simple: ask a question in english, get a correct, meaningful answer. But raw LLMs writing SQL hit a wall fast:\n\n- **Schemas lack meaning.**\n- **Types are misleading.**\n- **Tribal knowledge is missing.**\n- **No way to learn from mistakes.**\n- **Results generally lack interpretation.**\n\nThe root cause is missing context and missing memory.\n\nDash solves this with **6 layers of grounded context**, a **self-learning loop** that improves with every query, and a focus on **understanding your question** to deliver insights you can act on.\n\n## The Six Layers of Context\n\n| Layer | Purpose | Source |\n|------|--------|--------|\n| **Table Usage** | Schema, columns, relationships | `knowledge/tables/*.json` |\n| **Human Annotations** | Metrics, definitions, and business rules | `knowledge/business/*.json` |\n| **Query Patterns** | SQL that is known to work | `knowledge/queries/*.sql` |\n| **Institutional Knowledge** | Docs, wikis, external references | MCP (optional) |\n| **Learnings** | Error patterns and discovered fixes | Agno `Learning Machine` |\n| **Runtime Context** | Live schema changes | `introspect_schema` tool |\n\nThe agent retrieves relevant context at query time via hybrid search, then generates SQL grounded in patterns that already work.\n\n## The Self-Learning Loop\n\nDash improves without retraining or fine-tuning. We call this gpu-poor continuous learning.\n\nIt learns through two complementary systems:\n\n| System | Stores | How It Evolves |\n|------|--------|----------------|\n| **Knowledge** | Validated queries and business context | Curated by you + dash |\n| **Learnings** | Error patterns and fixes | Managed by `Learning Machine` automatically |\n\n```\nUser Question\n     â†“\nRetrieve Knowledge + Learnings\n     â†“\nReason about intent\n     â†“\nGenerate grounded SQL\n     â†“\nExecute and interpret\n     â†“\n â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”\n â†“         â†“\nSuccess    Error\n â†“         â†“\n â†“         Diagnose â†’ Fix â†’ Save Learning\n â†“                           (never repeated)\n â†“\nReturn insight\n â†“\nOptionally save as Knowledge\n```\n\n**Knowledge** is curatedâ€”validated queries and business context you want the agent to build on.\n\n**Learnings** is discoveredâ€”patterns the agent finds through trial and error. When a query fails because `position` is TEXT not INTEGER, the agent saves that gotcha. Next time, it knows.\n\n## Insights, Not Just Rows\n\nDash reasons about what makes an answer useful, not just technically correct.\n\n**Question:**\nWho won the most races in 2019?\n\n| Typical SQL Agent | Dash |\n|------------------|------|\n| `Hamilton: 11` | Lewis Hamilton dominated 2019 with **11 wins out of 21 races**, more than double Bottasâ€™s 4 wins. This performance secured his sixth world championship. |\n\n## Deploy to Railway\n\n```sh\nrailway login\n\n./scripts/railway_up.sh\n```\n\n### Production Operations\n\n**Load data and knowledge:**\n```sh\nrailway run python -m dash.scripts.load_data\nrailway run python -m dash.scripts.load_knowledge\n```\n\n**View logs:**\n\n```sh\nrailway logs --service dash\n```\n\n**Run commands in production:**\n\n```sh\nrailway run python -m dash  # CLI mode\n```\n\n**Redeploy after changes:**\n\n```sh\nrailway up --service dash -d\n```\n\n**Open dashboard:**\n```sh\nrailway open\n```\n\n## Adding Knowledge\n\nDash works best when it understands how your organization talks about data.\n\n```\nknowledge/\nâ”œâ”€â”€ tables/      # Table meaning and caveats\nâ”œâ”€â”€ queries/     # Proven SQL patterns\nâ””â”€â”€ business/    # Metrics and language\n```\n\n### Table Metadata\n\n```\n{\n  \"table_name\": \"orders\",\n  \"table_description\": \"Customer orders with denormalized line items\",\n  \"use_cases\": [\"Revenue reporting\", \"Customer analytics\"],\n  \"data_quality_notes\": [\n    \"created_at is UTC\",\n    \"status values: pending, completed, refunded\",\n    \"amount stored in cents\"\n  ]\n}\n```\n\n### Query Patterns\n\n```\n-- <query name>monthly_revenue</query name>\n-- <query description>\n-- Monthly revenue calculation.\n-- Converts cents to dollars.\n-- Excludes refunded orders.\n-- </query description>",
      "default_branch": "main"
    },
    "fetched_at": "2026-03-01T03:37:41.538259"
  },
  {
    "basic_info": {
      "name": "dataclaw",
      "full_name": "peteromallet/dataclaw",
      "owner": "peteromallet",
      "description": null,
      "url": "https://github.com/peteromallet/dataclaw",
      "clone_url": "https://github.com/peteromallet/dataclaw.git",
      "ssh_url": "git@github.com:peteromallet/dataclaw.git",
      "homepage": null,
      "created_at": "2026-02-24T20:33:55Z",
      "updated_at": "2026-03-01T03:28:27Z",
      "pushed_at": "2026-02-26T12:19:22Z"
    },
    "stats": {
      "stars": 1623,
      "forks": 203,
      "watchers": 1623,
      "open_issues": 2,
      "size": 807
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 284654
      },
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "# DataClaw\n\n> **This is a performance art project.** Anthropic built their models on the world's freely shared information, then introduced increasingly [dystopian data policies](https://www.anthropic.com/news/detecting-and-preventing-distillation-attacks) to stop anyone else from doing the same with their data â€” pulling up the ladder behind them. DataClaw lets you throw the ladder back down. The dataset it produces is yours to share.\n\nTurn your Claude Code, Codex, Gemini CLI, OpenCode, and OpenClaw conversation history into structured data and publish it to Hugging Face with a single command. DataClaw parses session logs, redacts secrets and PII, and uploads the result as a ready-to-use dataset.\n\n![DataClaw](dataclaw.jpeg)\n\nEvery export is tagged **`dataclaw`** on Hugging Face. Together, they may someday form a growing [distributed dataset](https://huggingface.co/datasets?other=dataclaw) of real-world human-AI coding collaboration.\n\n## Give this to your agent\n\nPaste this into Claude Code, Codex, Gemini CLI, OpenCode, OpenClaw, or any coding agent:\n\n```\nHelp me export my Claude Code, Codex, Gemini CLI, OpenCode, and OpenClaw conversation history to Hugging Face using DataClaw.\nInstall it, set up the skill, then walk me through the process.\n\nSTEP 1 â€” INSTALL\n  pip install dataclaw\n  If that fails: git clone https://github.com/banodoco/dataclaw.git /tmp/dataclaw && pip install /tmp/dataclaw\n  If that also fails, ask the user where the source is.\n\nSTEP 2 â€” INSTALL SKILL\n  dataclaw update-skill claude\n\nSTEP 3 â€” START\n  dataclaw prep\n  Every dataclaw command outputs next_steps in its JSON â€” follow them through the entire flow.\n\nSTEP 3A â€” CHOOSE SOURCE SCOPE (REQUIRED BEFORE EXPORT)\n  Ask the user explicitly: Claude Code, Codex, Gemini CLI, OpenCode, OpenClaw, or all?\n  dataclaw config --source \"claude|codex|gemini|opencode|openclaw|all\"\n  Do not export until source scope is explicitly confirmed.\n\nSTEP 3B â€” PRESENT ALL FOLDERS (REQUIRED BEFORE EXPORT)\n  dataclaw list --source \"claude|codex|gemini|opencode|openclaw|all\"\n  Send the FULL project/folder list to the user in a message (name, source, sessions, size, excluded).\n  Ask which projects to exclude.\n  dataclaw config --exclude \"project1,project2\" OR dataclaw config --confirm-projects\n  Do not export until folder selection is explicitly confirmed.\n\nSTEP 4 â€” REVIEW + CONFIRM (REQUIRED BEFORE PUSH)\n  dataclaw export --no-push --output /tmp/dataclaw_export.jsonl\n  Review PII findings and apply excludes/redactions as needed.\n  Full name is requested for an exact-name privacy scan against the export.\n  If the user declines sharing full name, use --skip-full-name-scan and attest the skip reason.\n  dataclaw confirm --full-name \"THEIR FULL NAME\" --attest-full-name \"...\" --attest-sensitive \"...\" --attest-manual-scan \"...\"\n\nSTEP 5 â€” PUBLISH (ONLY AFTER EXPLICIT USER APPROVAL)\n  dataclaw export --publish-attestation \"User explicitly approved publishing to Hugging Face.\"\n  Never publish unless the user explicitly says yes.\n\nIF ANY COMMAND FAILS DUE TO A SKIPPED STEP:\n  Restate the 6-step checklist above and resume from the blocked step (do not skip ahead).\n\nIMPORTANT: Never run bare `huggingface-cli login` â€” always use --token.\nIMPORTANT: Always export with --no-push first and review for PII before publishing.\n```\n\n<details>\n<summary><b>Manual usage (without an agent)</b></summary>\n\n### Quick start\n\n```bash\npip install dataclaw\nhuggingface-cli login --token YOUR_TOKEN\n\n# See your projects\ndataclaw prep\ndataclaw config --source all  # REQUIRED: choose claude, codex, gemini, opencode, openclaw, or all\ndataclaw list --source all  # Present full list and confirm folder scope before export\n\n# Configure\ndataclaw config --repo username/my-personal-codex-data\ndataclaw config --exclude \"personal-stuff,scratch\"\ndataclaw config --redact-usernames \"my_github_handle,my_discord_name\"\ndataclaw config --redact \"my-domain.com,my-secret-project\"\n\n# Export locally first\ndataclaw export --no-push\n\n# Review and confirm\ndataclaw confirm \\\n  --full-name \"YOUR FULL NAME\" \\\n  --attest-full-name \"Asked for full name and scanned export for YOUR FULL NAME.\" \\\n  --attest-sensitive \"Asked about company/client/internal names and private URLs; none found or redactions updated.\" \\\n  --attest-manual-scan \"Manually scanned 20 sessions across beginning/middle/end and reviewed findings.\"\n\n# Optional if user declines sharing full name\ndataclaw confirm \\\n  --skip-full-name-scan \\\n  --attest-full-name \"User declined to share full name; skipped exact-name scan.\" \\\n  --attest-sensitive \"Asked about company/client/internal names and private URLs; none found or redactions updated.\" \\\n  --attest-manual-scan \"Manually scanned 20 sessions across beginning/middle/end and reviewed findings.\"\n\n# Push\ndataclaw export --publish-attestation \"User explicitly approved publishing to Hugging Face.\"\n```\n\n### Commands\n\n| Command | Description |\n|---------|-------------|\n| `dataclaw status` | Show current stage and next steps (JSON",
      "default_branch": "main"
    },
    "fetched_at": "2026-03-01T03:37:42.852085"
  },
  {
    "basic_info": {
      "name": "FastCode",
      "full_name": "HKUDS/FastCode",
      "owner": "HKUDS",
      "description": "\"FastCode: Accelerating and Streamlining Your Code Understanding\"",
      "url": "https://github.com/HKUDS/FastCode",
      "clone_url": "https://github.com/HKUDS/FastCode.git",
      "ssh_url": "git@github.com:HKUDS/FastCode.git",
      "homepage": "",
      "created_at": "2026-02-13T06:43:54Z",
      "updated_at": "2026-03-01T03:33:21Z",
      "pushed_at": "2026-02-28T04:59:17Z"
    },
    "stats": {
      "stars": 1530,
      "forks": 166,
      "watchers": 1530,
      "open_issues": 4,
      "size": 1118
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 1118365,
        "HTML": 53840,
        "Shell": 26419,
        "TypeScript": 7851,
        "Dockerfile": 2486,
        "JavaScript": 1256
      },
      "license": null,
      "topics": []
    },
    "content": {
      "readme": "<div align=\"center\">\n\n<img src=\"assets/FastCode.svg\" alt=\"FastCode Logo\" width=\"200\"/>\n\n<!-- # FastCode -->\n\n### FastCode: Accelerating and Streamlining Your Code Understanding\n\n| **âš¡ High Performance** | **ğŸ’° Cost Efficient** | **ğŸš€ Fast & Scalable** |\n\n[![Python 3.12+](https://img.shields.io/badge/python-3.12+-blue.svg)](https://www.python.org/downloads/)\n[![License](https://img.shields.io/badge/license-MIT-green.svg)](LICENSE)\n[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)\n<p>\n  <a href=\"https://github.com/HKUDS/FastCode/issues/1\"><img src=\"https://img.shields.io/badge/ğŸ’¬WeChat-Group-07c160?style=for-the-badge&logo=wechat&logoColor=white&labelColor=1a1a2e\"></a>\n  <a href=\"https://github.com/HKUDS/FastCode/issues/2\"><img src=\"https://img.shields.io/badge/ğŸ’¬Feishu-Group-3370ff?style=for-the-badge&logo=bytedance&logoColor=white&labelColor=1a1a2e\"></a>\n  <a href=\"https://github.com/HKUDS/.github/blob/main/profile/README.md\"><img src=\"https://img.shields.io/badge/Feishu-Group-E9DBFC?style=flat&logo=feishu&logoColor=white\" alt=\"Feishu\"></a>\n  <a href=\"https://github.com/HKUDS/.github/blob/main/profile/README.md\"><img src=\"https://img.shields.io/badge/WeChat-Group-C5EAB4?style=flat&logo=wechat&logoColor=white\" alt=\"WeChat\"></a>\n</p>\n\n[Features](#-why-fastcode) â€¢ [Quick Start](#-quick-start) â€¢ [Installation](#-installation) â€¢ [MCP Server](#mcp-server-use-in-cursor--claude-code--windsurf) â€¢ [Documentation](#-how-it-works)\n\n</div>\n\n---\n\n## ğŸ¯ Why FastCode?\n\nFastCode is a token-efficient framework for comprehensive code understanding and analysis: delivering **superior speed**, **exceptional accuracy**, and **cost-effectiveness** for large-scale codebases and software architectures.\n\nğŸš€ **Superior Speed Advantage** - Runs 3x faster than Cursor and 4x faster than Claude Code.\n\nğŸ’° **Significant Cost Savings** - Costs 55% less than Cursor and 44% less than Claude Code.\n\nâš¡ **Highest Accuracy** - Outperforms Cursor and Claude Code with the highest accuracy score.\n\n<div align=\"center\">\n<img src=\"assets/performance.png\" alt=\"FastCode Performance vs Cost\" width=\"850\"/>\n</div>\n\n---\n\n## Key Features of FastCode\n\n### ğŸ¯ Core Performance Advantages\n- 2-4x Faster than competitors (Cursor/Claude Code)\n- 44-55% Cost Reduction compared to alternatives\n- Highest Accuracy Score across benchmarks\n- Up to 10x Token Savings through smart navigation\n\n### ğŸ› ï¸ Technical Capabilities\n- Large-Scale Repository Analysis - Handle massive codebases efficiently\n- Multi-Language Support - Python, JavaScript, TypeScript, Java, Go, C/C++, Rust, C#\n- Multi-Repository Reasoning - Cross-repo dependency analysis\n- Small Model Support - Local model compatibility (qwen3-coder-30b)\n\n### ğŸ’» User Experience\n- **MCP Server** - Use FastCode directly through MCP integration (e.g., Cursor, Claude Code)\n- Beautiful Web UI - Intuitive codebase exploration\n- Flexible API - Easy workflow integration\n- Smart Structural Navigation - Load only what you need\n\n---\n\n## ğŸ¥ See FastCode in Action\n\n<div align=\"center\">\n\n[![Watch FastCode Demo](https://img.youtube.com/vi/NwexLWHPBOY/0.jpg)](https://youtu.be/NwexLWHPBOY)\n\n**Click to watch FastCode in action** - See how FastCode analyzes complex codebases with lightning speed.\n\n---\n\n</div>\n\n### Core Technologies Behind FastCode\n\nFastCode introduces a three-phase framework that transforms how LLMs understand and navigate codebases:\n\n<p align=\"center\">\n  <img src=\"assets/framework.png\" alt=\"FastCode Framework\" width=\"100%\"/>\n</p>\n\n## ğŸ—ï¸ Semantic-Structural Code Representation\n\n### Multi-layered codebase understanding for comprehensive analysis\n\n- **ğŸ” Hierarchical Code Units** â€” Advanced multi-level indexing spanning files, classes, functions, and documentation using AST-based parsing across 8+ programming languages\n\n- **ğŸ”— Hybrid Index** â€” Seamlessly combines semantic embeddings with keyword search (BM25) for robust and precise code retrieval\n\n- **ğŸ“Š Multi-Layer Graph Modeling** â€” Three interconnected relationship graphs (Call Graph, Dependency Graph, Inheritance Graph) enabling structural navigation across the entire codebase\n\n### ğŸ§­ Lightning-Fast Codebase Navigation\n\nFinding the right code without opening every file - at lightning speed\n\n- **âš¡ Two-Stage Smart Search** â€” Like having a research assistant that first finds potentially relevant code, then ranks and organizes the best matches for your specific question.\n\n- **ğŸ“ Safe File Browsing** â€” Explores your project structure securely, understanding folder organization and file patterns without compromising security.\n\n- **ğŸŒ Following Code Connections** â€” Traces how code pieces connect (up to 2 steps away), like following a trail of breadcrumbs through your codebase.\n\n- **ğŸ¯ Code Skimming** â€” Instead of reading entire files, FastCode just looks at the \"headlines\" - function names, class definitions, and type hints. This is like reading a book's chapter titles instead of every page, saving massive amounts of processing power.\n\n#",
      "default_branch": "main"
    },
    "fetched_at": "2026-03-01T03:37:44.149003"
  },
  {
    "basic_info": {
      "name": "OpenPlanter",
      "full_name": "ShinMegamiBoson/OpenPlanter",
      "owner": "ShinMegamiBoson",
      "description": null,
      "url": "https://github.com/ShinMegamiBoson/OpenPlanter",
      "clone_url": "https://github.com/ShinMegamiBoson/OpenPlanter.git",
      "ssh_url": "git@github.com:ShinMegamiBoson/OpenPlanter.git",
      "homepage": null,
      "created_at": "2026-02-20T01:04:23Z",
      "updated_at": "2026-03-01T01:04:15Z",
      "pushed_at": "2026-02-23T17:53:39Z"
    },
    "stats": {
      "stars": 1372,
      "forks": 211,
      "watchers": 1372,
      "open_issues": 17,
      "size": 342
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 1038665,
        "Shell": 3754,
        "Dockerfile": 296
      },
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "# OpenPlanter\n\nA recursive-language-model investigation agent with a terminal UI. OpenPlanter ingests heterogeneous datasets â€” corporate registries, campaign finance records, lobbying disclosures, government contracts, and more â€” resolves entities across them, and surfaces non-obvious connections through evidence-backed analysis. It operates autonomously with file I/O, shell execution, web search, and recursive sub-agent delegation.\n\n## Quickstart\n\n```bash\n# Install\npip install -e .\n\n# Configure API keys (interactive prompt)\nopenplanter-agent --configure-keys\n\n# Launch the TUI\nopenplanter-agent --workspace /path/to/your/project\n```\n\nOr run a single task headlessly:\n\n```bash\nopenplanter-agent --task \"Cross-reference vendor payments against lobbying disclosures and flag overlaps\" --workspace ./data\n```\n\n### Docker\n\n```bash\n# Add your API keys to .env, then:\ndocker compose up\n```\n\nThe container mounts `./workspace` as the agent's working directory.\n\n## Supported Providers\n\n| Provider | Default Model | Env Var |\n|----------|---------------|---------|\n| OpenAI | `gpt-5.2` | `OPENAI_API_KEY` |\n| Anthropic | `claude-opus-4-6` | `ANTHROPIC_API_KEY` |\n| OpenRouter | `anthropic/claude-sonnet-4-5` | `OPENROUTER_API_KEY` |\n| Cerebras | `qwen-3-235b-a22b-instruct-2507` | `CEREBRAS_API_KEY` |\n| Ollama | `llama3.2` | (none â€” local) |\n\n### Local Models (Ollama)\n\n[Ollama](https://ollama.com) runs models locally with no API key. Install Ollama, pull a model (`ollama pull llama3.2`), then:\n\n```bash\nopenplanter-agent --provider ollama\nopenplanter-agent --provider ollama --model mistral\nopenplanter-agent --provider ollama --list-models\n```\n\nThe base URL defaults to `http://localhost:11434/v1` and can be overridden with `OPENPLANTER_OLLAMA_BASE_URL` or `--base-url`. The first request may be slow while Ollama loads the model into memory; a 120-second first-byte timeout is used automatically.\n\nAdditional service keys: `EXA_API_KEY` (web search), `VOYAGE_API_KEY` (embeddings).\n\nAll keys can also be set with an `OPENPLANTER_` prefix (e.g. `OPENPLANTER_OPENAI_API_KEY`), via `.env` files in the workspace, or via CLI flags.\n\n## Agent Tools\n\nThe agent has access to 19 tools, organized around its investigation workflow:\n\n**Dataset ingestion & workspace** â€” `list_files`, `search_files`, `repo_map`, `read_file`, `write_file`, `edit_file`, `hashline_edit`, `apply_patch` â€” load, inspect, and transform source datasets; write structured findings.\n\n**Shell execution** â€” `run_shell`, `run_shell_bg`, `check_shell_bg`, `kill_shell_bg` â€” run analysis scripts, data pipelines, and validation checks.\n\n**Web** â€” `web_search` (Exa), `fetch_url` â€” pull public records, verify entities, and retrieve supplementary data.\n\n**Planning & delegation** â€” `think`, `subtask`, `execute`, `list_artifacts`, `read_artifact` â€” decompose investigations into focused sub-tasks, each with acceptance criteria and independent verification.\n\nIn **recursive mode** (the default), the agent spawns sub-agents via `subtask` and `execute` to parallelize entity resolution, cross-dataset linking, and evidence-chain construction across large investigations.\n\n## CLI Reference\n\n```\nopenplanter-agent [options]\n```\n\n### Workspace & Session\n\n| Flag | Description |\n|------|-------------|\n| `--workspace DIR` | Workspace root (default: `.`) |\n| `--session-id ID` | Use a specific session ID |\n| `--resume` | Resume the latest (or specified) session |\n| `--list-sessions` | List saved sessions and exit |\n\n### Model Selection\n\n| Flag | Description |\n|------|-------------|\n| `--provider NAME` | `auto`, `openai`, `anthropic`, `openrouter`, `cerebras`, `ollama` |\n| `--model NAME` | Model name or `newest` to auto-select |\n| `--reasoning-effort LEVEL` | `low`, `medium`, `high`, or `none` |\n| `--list-models` | Fetch available models from the provider API |\n\n### Execution\n\n| Flag | Description |\n|------|-------------|\n| `--task OBJECTIVE` | Run a single task and exit (headless) |\n| `--recursive` | Enable recursive sub-agent delegation |\n| `--acceptance-criteria` | Judge subtask results with a lightweight model |\n| `--max-depth N` | Maximum recursion depth (default: 4) |\n| `--max-steps N` | Maximum steps per call (default: 100) |\n| `--timeout N` | Shell command timeout in seconds (default: 45) |\n\n### UI\n\n| Flag | Description |\n|------|-------------|\n| `--no-tui` | Plain REPL (no colors or spinner) |\n| `--headless` | Non-interactive mode (for CI) |\n| `--demo` | Censor entity names and workspace paths in output |\n\n### Persistent Defaults\n\nUse `--default-model`, `--default-reasoning-effort`, or per-provider variants like `--default-model-openai` to save workspace defaults to `.openplanter/settings.json`. View them with `--show-settings`.\n\n## TUI Commands\n\nInside the interactive REPL:\n\n| Command | Action |\n|---------|--------|\n| `/model` | Show current model and provider |\n| `/model NAME` | Switch model (aliases: `opus`, `sonnet`, `gpt5`, etc.) |\n| `/model NAME --save` | Switch and persist as default |\n| `/mod",
      "default_branch": "main"
    },
    "fetched_at": "2026-03-01T03:37:45.434313"
  },
  {
    "basic_info": {
      "name": "claude-seo",
      "full_name": "AgriciDaniel/claude-seo",
      "owner": "AgriciDaniel",
      "description": " Universal SEO skill for Claude Code. Comprehensive SEO analysis for any website or business type.",
      "url": "https://github.com/AgriciDaniel/claude-seo",
      "clone_url": "https://github.com/AgriciDaniel/claude-seo.git",
      "ssh_url": "git@github.com:AgriciDaniel/claude-seo.git",
      "homepage": "https://www.skool.com/ai-marketing-hub-pro",
      "created_at": "2026-02-07T08:17:38Z",
      "updated_at": "2026-03-01T03:16:27Z",
      "pushed_at": "2026-02-19T19:29:57Z"
    },
    "stats": {
      "stars": 1273,
      "forks": 199,
      "watchers": 1273,
      "open_issues": 1,
      "size": 1912
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 25940,
        "Shell": 8812,
        "PowerShell": 7230
      },
      "license": "MIT License",
      "topics": [
        "ai-tools",
        "claude",
        "claude-code",
        "core-web-vitals",
        "schema-markup",
        "seo",
        "seo-audit",
        "seo-tools"
      ]
    },
    "content": {
      "readme": "<!-- Updated: 2026-02-08 -->\n\n![Claude SEO](screenshots/cover-image.jpeg)\n\n# Claude SEO\n\nComprehensive SEO analysis skill for Claude Code. Covers technical SEO, on-page analysis, content quality (E-E-A-T), schema markup, image optimization, sitemap architecture, AI search optimization (GEO), and strategic planning.\n\n![SEO Command Demo](screenshots/seo-command-demo.gif)\n\n[![Claude Code Skill](https://img.shields.io/badge/Claude%20Code-Skill-blue)](https://claude.ai/claude-code)\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)\n\n## Installation\n\n### One-Command Install (Unix/macOS/Linux)\n\n```bash\ncurl -fsSL https://raw.githubusercontent.com/AgriciDaniel/claude-seo/main/install.sh | bash\n```\n\n### Manual Install\n\n```bash\ngit clone https://github.com/AgriciDaniel/claude-seo.git\ncd claude-seo\n./install.sh\n```\n\n### Windows\n\n```powershell\nirm https://raw.githubusercontent.com/AgriciDaniel/claude-seo/main/install.ps1 | iex\n```\n\n## Quick Start\n\n```bash\n# Start Claude Code\nclaude\n\n# Run a full site audit\n/seo audit https://example.com\n\n# Analyze a single page\n/seo page https://example.com/about\n\n# Check schema markup\n/seo schema https://example.com\n\n# Generate a sitemap\n/seo sitemap generate\n\n# Optimize for AI search\n/seo geo https://example.com\n```\n### Demo:\n[Watch the full demo on YouTube](https://www.youtube.com/watch?v=COMnNlUakQk)\n\n**`/seo audit` â€” full site audit with parallel subagents:**\n\n![SEO Audit Demo](screenshots/seo-audit-demo.gif)\n\n## Commands\n\n| Command | Description |\n|---------|-------------|\n| `/seo audit <url>` | Full website audit with parallel subagent delegation |\n| `/seo page <url>` | Deep single-page analysis |\n| `/seo sitemap <url>` | Analyze existing XML sitemap |\n| `/seo sitemap generate` | Generate new sitemap with industry templates |\n| `/seo schema <url>` | Detect, validate, and generate Schema.org markup |\n| `/seo images <url>` | Image optimization analysis |\n| `/seo technical <url>` | Technical SEO audit (8 categories) |\n| `/seo content <url>` | E-E-A-T and content quality analysis |\n| `/seo geo <url>` | AI Overviews / Generative Engine Optimization |\n| `/seo plan <type>` | Strategic SEO planning (saas, local, ecommerce, publisher, agency) |\n| `/seo programmatic <url>` | Programmatic SEO analysis and planning |\n| `/seo competitor-pages <url>` | Competitor comparison page generation |\n| `/seo hreflang <url>` | Hreflang/i18n SEO audit and generation |\n\n### `/seo programmatic [url|plan]`\n**Programmatic SEO Analysis & Planning**\n\nBuild SEO pages at scale from data sources with quality safeguards.\n\n**Capabilities:**\n- Analyze existing programmatic pages for thin content and cannibalization\n- Plan URL patterns and template structures for data-driven pages\n- Internal linking automation between generated pages\n- Canonical strategy and index bloat prevention\n- Quality gates: âš ï¸ WARNING at 100+ pages, ğŸ›‘ HARD STOP at 500+ without audit\n\n### `/seo competitor-pages [url|generate]`\n**Competitor Comparison Page Generator**\n\nCreate high-converting \"X vs Y\" and \"alternatives to X\" pages.\n\n**Capabilities:**\n- Structured comparison tables with feature matrices\n- Product schema markup with AggregateRating\n- Conversion-optimized layouts with CTA placement\n- Keyword targeting for comparison intent queries\n- Fairness guidelines for accurate competitor representation\n\n### `/seo hreflang [url]`\n**Hreflang / i18n SEO Audit & Generation**\n\nValidate and generate hreflang tags for multi-language sites.\n\n**Capabilities:**\n- Generate hreflang tags (HTML, HTTP headers, or XML sitemap)\n- Validate self-referencing tags, return tags, x-default\n- Detect common mistakes (missing returns, invalid codes, HTTP/HTTPS mismatch)\n- Cross-domain hreflang support\n- Language/region code validation (ISO 639-1 + ISO 3166-1)\n\n## Features\n\n### Core Web Vitals (Current Metrics)\n- **LCP** (Largest Contentful Paint): Target < 2.5s\n- **INP** (Interaction to Next Paint): Target < 200ms\n- **CLS** (Cumulative Layout Shift): Target < 0.1\n\n> Note: INP replaced FID on March 12, 2024. FID was fully removed from all Chrome tools on September 9, 2024.\n\n### E-E-A-T Analysis\nUpdated to September 2025 Quality Rater Guidelines:\n- **Experience**: First-hand knowledge signals\n- **Expertise**: Author credentials and depth\n- **Authoritativeness**: Industry recognition\n- **Trustworthiness**: Contact info, security, transparency\n\n### Schema Markup\n- Detection: JSON-LD (preferred), Microdata, RDFa\n- Validation against Google's supported types\n- Generation with templates\n- Deprecation awareness:\n  - HowTo: Deprecated (Sept 2023)\n  - FAQ: Restricted to gov/health sites (Aug 2023)\n  - SpecialAnnouncement: Deprecated (July 2025)\n\n### AI Search Optimization (GEO)\nNew for 2026 - optimize for:\n- Google AI Overviews\n- ChatGPT web search\n- Perplexity\n- Other AI-powered search\n\n### Quality Gates\n- Warning at 30+ location pages\n- Hard stop at 50+ location pages\n- Thin content detection per page type\n- Doorway page prevention\n\n## Architectur",
      "default_branch": "main"
    },
    "fetched_at": "2026-03-01T03:37:46.733734"
  },
  {
    "basic_info": {
      "name": "claw-compactor",
      "full_name": "aeromomo/claw-compactor",
      "owner": "aeromomo",
      "description": "ğŸ¦ Claw Compactor â€” The 98% Crusher. Cut your AI agent token spend in half with 5 layered compression techniques.",
      "url": "https://github.com/aeromomo/claw-compactor",
      "clone_url": "https://github.com/aeromomo/claw-compactor.git",
      "ssh_url": "git@github.com:aeromomo/claw-compactor.git",
      "homepage": null,
      "created_at": "2026-02-10T00:27:08Z",
      "updated_at": "2026-03-01T03:35:16Z",
      "pushed_at": "2026-02-28T01:34:06Z"
    },
    "stats": {
      "stars": 1153,
      "forks": 98,
      "watchers": 1153,
      "open_issues": 3,
      "size": 1190
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 335972,
        "JavaScript": 202404,
        "HTML": 49843,
        "Shell": 723
      },
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "# Claw Compactor\n![Claw Compactor Banner](assets/banner.png)\n\n[![Build](https://img.shields.io/badge/build-passing-brightgreen)](https://github.com/aeromomo/claw-compactor) [![Release](https://img.shields.io/github/v/release/aeromomo/claw-compactor?color=blue)](https://github.com/aeromomo/claw-compactor/releases) [![Tests](https://img.shields.io/badge/tests-800%20passed-brightgreen)](https://github.com/aeromomo/claw-compactor) [![Python](https://img.shields.io/badge/python-3.9%2B-blue)](https://python.org) [![License](https://img.shields.io/badge/license-MIT-purple)](LICENSE) [![OpenClaw](https://img.shields.io/badge/OpenClaw-skill-orange)](https://openclaw.ai)\n\n*\"Cut your tokens. Keep your facts.\"*\n\n**Cut your AI agent's token spend in half.** One command compresses your entire workspace â€” memory files, session transcripts, sub-agent context â€” using 5 layered compression techniques. Deterministic. Mostly lossless. No LLM required.\n\n## Features\n- **5 compression layers** working in sequence for maximum savings\n- **Zero LLM cost** â€” all compression is rule-based and deterministic\n- **Lossless roundtrip** for dictionary, RLE, and rule-based compression\n- **~97% savings** on session transcripts via observation extraction\n- **Tiered summaries** (L0/L1/L2) for progressive context loading\n- **CJK-aware** â€” full Chinese/Japanese/Korean support\n- **One command** (`full`) runs everything in optimal order\n\n## 5 Compression Layers\n| 1 | Rule engine | Dedup lines, strip markdown filler, merge sections | 4-8% | |\n| 2 | Dictionary encoding | Auto-learned codebook, `$XX` substitution | 4-5% | |\n| 3 | Observation compression | Session JSONL â†’ structured summaries | ~97% | * |\n| 4 | RLE patterns | Path shorthand (`$WS`), IP prefix, enum compaction | 1-2% | |\n| 5 | Compressed Context Protocol | ultra/medium/light abbreviation | 20-60% | * |\n\n\\*Lossy techniques preserve all facts and decisions; only verbose formatting is removed.\n\n## Quick Start\n```bash\ngit clone https://github.com/aeromomo/claw-compactor.git\ncd claw-compactor\n\n# See how much you'd save (non-destructive)\npython3 scripts/mem_compress.py /path/to/workspace benchmark\n\n# Compress everything\npython3 scripts/mem_compress.py /path/to/workspace full\n```\n\n**Requirements:** Python 3.9+. Optional: `pip install tiktoken` for exact token counts (falls back to heuristic).\n\n## Architecture\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ mem_compress.py â”‚\nâ”‚ (unified entry point) â”‚\nâ””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜\n â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚\n â–¼ â–¼ â–¼ â–¼ â–¼ â–¼ â–¼ â–¼\n estimate compress dict dedup observe tiers audit optimize\n â””â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”¬â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”˜\n â–¼\n â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n â”‚ lib/ â”‚\n â”‚ tokens.py â”‚ â† tiktoken or heuristic\n â”‚ markdown.py â”‚ â† section parsing\n â”‚ dedup.py â”‚ â† shingle hashing\n â”‚ dictionary.py â”‚ â† codebook compression\n â”‚ rle.py â”‚ â† path/IP/enum encoding\n â”‚ tokenizer_ â”‚\n â”‚ optimizer.py â”‚ â† format optimization\n â”‚ config.py â”‚ â† JSON config\n â”‚ exceptions.py â”‚ â† error types\n â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n## Commands\nAll commands: `python3 scripts/mem_compress.py <workspace> <command> [options]`\n\n`full`, Description=Complete pipeline (all steps in order), Typical Savings=50%+ combined\n`benchmark`, Description=Dry-run performance report, Typical Savings=â€”\n`compress`, Description=Rule-based compression, Typical Savings=4-8%\n`dict`, Description=Dictionary encoding with auto-codebook, Typical Savings=4-5%\n`observe`, Description=Session transcript â†’ observations, Typical Savings=~97%\n`tiers`, Description=Generate L0/L1/L2 summaries, Typical Savings=88-95% on sub-agent loads\n`dedup`, Description=Cross-file duplicate detection, Typical Savings=varies\n`estimate`, Description=Token count report, Typical Savings=â€”\n`audit`, Description=Workspace health check, Typical Savings=â€”\n`optimize`, Description=Tokenizer-level format fixes, Typical Savings=1-3%\n\n### Global Options\n- `--json` â€” Machine-readable JSON output\n- `--dry-run` â€” Preview changes without writing\n- `--since YYYY-MM-DD` â€” Filter sessions by date\n- `--auto-merge` â€” Auto-merge duplicates (dedup)\n\n## Real-World Savings\nSession transcripts (observe), Typical Savings=**~97%**, Notes=Megabytes of JSONL â†’ concise observation MD\nVerbose/new workspace, Typical Savings=**50-70%**, Notes=First run on unoptimized workspace\nRegular maintenance, Typical Savings=**10-20%**, Notes=Weekly runs on active workspace\nAlready-optimized, Typical Savings=**3-12%**, Notes=Diminishing returns â€” workspace is clean\n\n## cacheRetention â€” Complementary Optimization\nBefore compression runs, enable **prompt caching** for a 90% discount on cached tokens:\n\n```json\n{\n \"agents\": {\n \"defaults\": {\n \"models\": {\n \"anthropic/claude-opus-4-6\": {\n \"params\": {\n \"cacheRetention\": \"long\"\n }\n\nCompression reduces token count, caching reduces cost-per-token. Together: 50% compression + 90% cache discount = **95% effective cost reduction**.\n\n## Heartbeat Automation\nRun weekly or on heartbeat:\n\n```markdown\n\n## Memory Maintenance ",
      "default_branch": "main"
    },
    "fetched_at": "2026-03-01T03:37:48.060640"
  },
  {
    "basic_info": {
      "name": "skill-compose",
      "full_name": "MooseGoose0701/skill-compose",
      "owner": "MooseGoose0701",
      "description": "Skill Compose is an open-source agent builder and runtime platform for skill-powered agents. No workflow graphs. No CLI.",
      "url": "https://github.com/MooseGoose0701/skill-compose",
      "clone_url": "https://github.com/MooseGoose0701/skill-compose.git",
      "ssh_url": "git@github.com:MooseGoose0701/skill-compose.git",
      "homepage": "",
      "created_at": "2026-02-12T14:11:39Z",
      "updated_at": "2026-03-01T03:36:09Z",
      "pushed_at": "2026-03-01T03:36:52Z"
    },
    "stats": {
      "stars": 1075,
      "forks": 94,
      "watchers": 1075,
      "open_issues": 5,
      "size": 26138
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 2101987,
        "TypeScript": 985750,
        "Shell": 25854,
        "Makefile": 7562,
        "Dockerfile": 7160,
        "CSS": 4067,
        "OCaml": 2323,
        "JavaScript": 776
      },
      "license": "Apache License 2.0",
      "topics": []
    },
    "content": {
      "readme": "<p align=\"center\">\n  <img src=\"scheader.png\" alt=\"Skill Compose\" width=\"50%\" />\n</p>\n\n<p align=\"center\">\n  <a href=\"./README.md\"><img alt=\"English\" src=\"https://img.shields.io/badge/English-d9d9d9\"></a>\n  <a href=\"./README_es.md\"><img alt=\"EspaÃ±ol\" src=\"https://img.shields.io/badge/EspaÃ±ol-d9d9d9\"></a>\n  <a href=\"./README_pt-BR.md\"><img alt=\"PortuguÃªs (BR)\" src=\"https://img.shields.io/badge/PortuguÃªs (BR)-d9d9d9\"></a>\n  <a href=\"./README_zh-CN.md\"><img alt=\"ç®€ä½“ä¸­æ–‡\" src=\"https://img.shields.io/badge/ç®€ä½“ä¸­æ–‡-d9d9d9\"></a>\n  <a href=\"./README_ja.md\"><img alt=\"æ—¥æœ¬èª\" src=\"https://img.shields.io/badge/æ—¥æœ¬èª-d9d9d9\"></a>\n</p>\n\n<p align=\"center\">\nSkill Compose is an open-source agent builder and runtime platform for skill-powered agents.<br>\nNo workflow graphs. No CLI.\n</p>\n\n<p align=\"center\">\n  <a href=\"LICENSE\"><img src=\"https://img.shields.io/badge/License-Apache%202.0-blue.svg\" alt=\"License\" /></a>\n  <a href=\"https://www.python.org/\"><img src=\"https://img.shields.io/badge/Python-3.11+-green.svg\" alt=\"Python\" /></a>\n  <a href=\"https://nextjs.org/\"><img src=\"https://img.shields.io/badge/Next.js-14-black.svg\" alt=\"Next.js\" /></a>\n  <a href=\"https://discord.gg/8QK5suCV9m\"><img src=\"https://img.shields.io/badge/Discord-%235865F2.svg?style=flat&logo=discord&logoColor=white\" alt=\"discord\" /></a>\n  <a href=\"https://x.com/SkillComposeAI/\"><img src=\"https://img.shields.io/twitter/follow/SkillComposeAI\" alt=\"twitter\" /></a>\n</p>\n\n<p align=\"center\">\n  <img src=\"docs/images/screenshot.png\" alt=\"Skill Compose Screenshot\" width=\"800\" />\n</p>\n\n## Key Capabilities\n\n- ğŸ§© **Skills as first-class artifacts** â€” versioned, reviewable skill packages (contracts, references, rubrics, helpers), not brittle graphs.\n- ğŸ§  **\"Compose My Agent\" workflow** â€” describe what you want; Skill Compose finds/reuses skills, drafts missing ones, and composes an agent.\n- ğŸ”Œ **Tool + MCP wiring** â€” connect tools and MCP servers without hand-writing glue code.\n- ğŸš€ **Instant publishing** â€” one click to ship as **Web Chat** (shareable link) and/or **API** (integrations-ready endpoint).\n- ğŸ›¡ï¸ **Container-first isolation** â€” run agents in containers (or K8s pods) to keep hosts clean and execution reproducible.\n- ğŸ§± **Executors for heavy environments** â€” assign custom Docker images/K8s runtimes per agent (GPU/ML/HPC stacks, custom builds).\n- ğŸ“¦ **Skill lifecycle management** â€” GitHub import + one-click updates, multi-format import/export, version history, diff/rollback, and local sync.\n- ğŸ”„ **Skill evolution from reality** â€” improve skills using feedback + execution traces, with proposed rewrites you can review.\n- ğŸ—‚ï¸ **Skill library organization** â€” categories, pinning, and lightweight discovery to stay sane at 100+ skills.\n\n## Examples\n\n<table>\n<tr>\n<td align=\"center\">\n<b>Compose Your Agent</b><br>\n<sub>Describe what you want and let Skill Compose build the agent for you â€” finding existing skills, drafting missing ones, and wiring everything together.</sub><br><br>\n<img src=\"docs/examples/skill-compose-your-agent.gif\" alt=\"Compose Your Agent\" width=\"100%\" />\n</td>\n</tr>\n<tr>\n<td align=\"center\">\n<b>Evolve Your Agent</b><br>\n<sub>Improve skills automatically from execution traces and user feedback, review proposed changes, accept the rewrite, and watch your agents and skills get smarter.</sub><br><br>\n<img src=\"docs/examples/evolve-your-agent.gif\" alt=\"Evolve Your Agent\" width=\"100%\" />\n</td>\n</tr>\n<tr>\n<td align=\"center\">\n<b>Demo Agent: Article to Slides</b><br>\n<sub>Turn any article or paper into a polished slide deck. The agent reads the content, extracts key points, draft storyboards, and generates presentation-ready slides.</sub><br><br>\n<img src=\"docs/examples/article-to-slides-agent.gif\" alt=\"Article to Slides Agent\" width=\"100%\" />\n</td>\n</tr>\n<tr>\n<td align=\"center\">\n<b>Demo Agent: ChemScout</b><br>\n<sub>Runs in an isolated execution environment! A chemistry research assistant that searches compound databases, analyzes molecular structures, and summarizes findings into structured reports.</sub><br><br>\n<img src=\"docs/examples/chemscout-agent.gif\" alt=\"ChemScout Agent\" width=\"100%\" />\n</td>\n</tr>\n</table>\n\n## Architecture\n\n<p align=\"center\">\n  <img src=\"docs/images/architecture.png\" alt=\"Skill Compose Architecture\" width=\"700\" />\n</p>\n\n*Some features shown may still be in development.*\n\n## Quick Start\n\nGet started with Docker:\n\n```bash\ngit clone https://github.com/MooseGoose0701/skill-compose.git\ncd skill-compose/docker\n# Default model is Kimi 2.5 (thinking disabled, API key: MOONSHOT_API_KEY), add at least one LLM API key.\n# You can also set API KEYs manually in the Web UI \"Environment\" after launch.\ncp .env.example .env\ndocker compose up -d\n```\n\nOpen **http://localhost:62600** and click **\"Compose Your Agent\"**.\n\nStop services:\n\n```bash\ncd skill-compose/docker\ndocker compose down\n```\n\n<details>\n<summary>Build from source (for developers)</summary>\n\n```bash\ncd skill-compose/docker\ncp .env.example .env\n# Use docker-compose.dev.yaml to build images locally\ndocker compo",
      "default_branch": "main"
    },
    "fetched_at": "2026-03-01T03:37:49.351849"
  },
  {
    "basic_info": {
      "name": "paperbanana",
      "full_name": "llmsresearch/paperbanana",
      "owner": "llmsresearch",
      "description": "Open source implementation and extension of Google Researchâ€™s PaperBanana for automated academic figures, diagrams, and research visuals, expanded to new domains like slide generation.",
      "url": "https://github.com/llmsresearch/paperbanana",
      "clone_url": "https://github.com/llmsresearch/paperbanana.git",
      "ssh_url": "git@github.com:llmsresearch/paperbanana.git",
      "homepage": "",
      "created_at": "2026-02-04T14:46:47Z",
      "updated_at": "2026-03-01T03:35:28Z",
      "pushed_at": "2026-02-27T18:11:46Z"
    },
    "stats": {
      "stars": 1017,
      "forks": 145,
      "watchers": 1017,
      "open_issues": 9,
      "size": 16106
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 307139
      },
      "license": "MIT License",
      "topics": [
        "academic-diagrams",
        "academic-research",
        "agentic-ai",
        "arxiv",
        "diagram-generation",
        "gemini",
        "google-gemini",
        "llm",
        "llms",
        "mcp",
        "mcp-server",
        "multiagent",
        "neurips",
        "paperbanana",
        "python-ai-research-tools",
        "research-automation",
        "research-tools",
        "scientific-visualization",
        "text-to-image",
        "vlm"
      ]
    },
    "content": {
      "readme": "<!-- mcp-name: io.github.llmsresearch/paperbanana -->\n<table align=\"center\" width=\"100%\" style=\"border: none; border-collapse: collapse;\">\n  <tr>\n    <td width=\"220\" align=\"left\" valign=\"middle\" style=\"border: none;\">\n      <img src=\"https://dwzhu-pku.github.io/PaperBanana/static/images/logo.jpg\" alt=\"PaperBanana Logo\" width=\"180\"/>\n    </td>\n    <td align=\"left\" valign=\"middle\" style=\"border: none;\">\n      <h1>PaperBanana</h1>\n      <p><strong>Automated Academic Illustration for AI Scientists</strong></p>\n      <p>\n        <a href=\"https://github.com/llmsresearch/paperbanana/actions/workflows/ci.yml\"><img src=\"https://github.com/llmsresearch/paperbanana/actions/workflows/ci.yml/badge.svg\" alt=\"CI\"/></a>\n        <a href=\"https://pypi.org/project/paperbanana/\"><img src=\"https://img.shields.io/pypi/dm/paperbanana?label=PyPI%20downloads&logo=pypi&logoColor=white\" alt=\"PyPI Downloads\"/></a>\n        <a href=\"https://huggingface.co/spaces/llmsresearch/paperbanana\"><img src=\"https://img.shields.io/badge/Demo-HuggingFace-yellow?logo=huggingface&logoColor=white\" alt=\"Demo\"/></a>\n        <br/>\n        <a href=\"https://www.python.org/downloads/\"><img src=\"https://img.shields.io/badge/python-3.10%2B-blue?logo=python&logoColor=white\" alt=\"Python 3.10+\"/></a>\n        <a href=\"https://arxiv.org/abs/2601.23265\"><img src=\"https://img.shields.io/badge/arXiv-2601.23265-b31b1b?logo=arxiv&logoColor=white\" alt=\"arXiv\"/></a>\n        <a href=\"LICENSE\"><img src=\"https://img.shields.io/badge/license-MIT-green?logo=opensourceinitiative&logoColor=white\" alt=\"License: MIT\"/></a>\n        <br/>\n        <a href=\"https://pydantic.dev\"><img src=\"https://img.shields.io/badge/Pydantic-v2-e92063?logo=pydantic&logoColor=white\" alt=\"Pydantic v2\"/></a>\n        <a href=\"https://typer.tiangolo.com\"><img src=\"https://img.shields.io/badge/CLI-Typer-009688?logo=gnubash&logoColor=white\" alt=\"Typer\"/></a>\n        <a href=\"https://ai.google.dev/\"><img src=\"https://img.shields.io/badge/Gemini-Free%20Tier-4285F4?logo=google&logoColor=white\" alt=\"Gemini Free Tier\"/></a>\n      </p>\n    </td>\n  </tr>\n</table>\n\n---\n\n> **Disclaimer**: This is an **unofficial, community-driven open-source implementation** of the paper\n> *\"PaperBanana: Automating Academic Illustration for AI Scientists\"* by Dawei Zhu, Rui Meng, Yale Song,\n> Xiyu Wei, Sujian Li, Tomas Pfister, and Jinsung Yoon ([arXiv:2601.23265](https://arxiv.org/abs/2601.23265)).\n> This project is **not affiliated with or endorsed by** the original authors or Google Research.\n> The implementation is based on the publicly available paper and may differ from the original system.\n\nAn agentic framework for generating publication-quality academic diagrams and statistical plots from text descriptions. Supports OpenAI (GPT-5.2 + GPT-Image-1.5), Azure OpenAI / Foundry, and Google Gemini providers.\n\n- Two-phase multi-agent pipeline with iterative refinement\n- Multiple VLM and image generation providers (OpenAI, Azure, Gemini)\n- Input optimization layer for better generation quality\n- Auto-refine mode and run continuation with user feedback\n- CLI, Python API, and MCP server for IDE integration\n- Claude Code skills for `/generate-diagram`, `/generate-plot`, and `/evaluate-diagram`\n\n<p align=\"center\">\n  <img src=\"assets/img/hero_image.png\" alt=\"PaperBanana takes paper as input and provide diagram as output\" style=\"max-width: 960px; width: 100%; height: auto;\"/>\n</p>\n\n---\n\n## Quick Start\n\n### Prerequisites\n\n- Python 3.10+\n- An OpenAI API key ([platform.openai.com](https://platform.openai.com/api-keys)) or Azure OpenAI / Foundry endpoint\n- Or a Google Gemini API key (free, [Google AI Studio](https://makersuite.google.com/app/apikey))\n\n### Step 1: Install\n\n```bash\npip install paperbanana\n```\n\nOr install from source for development:\n\n```bash\ngit clone https://github.com/llmsresearch/paperbanana.git\ncd paperbanana\npip install -e \".[dev,openai,google]\"\n```\n\n### Step 2: Get Your API Key\n\n```bash\ncp .env.example .env\n# Edit .env and add your API key:\n#   OPENAI_API_KEY=your-key-here\n#\n# For Azure OpenAI / Foundry:\n#   OPENAI_BASE_URL=https://<resource>.openai.azure.com/openai/v1\n```\n\nOr use the setup wizard for Gemini:\n\n```bash\npaperbanana setup\n```\n\n### Step 3: Generate a Diagram\n\n```bash\npaperbanana generate \\\n  --input examples/sample_inputs/transformer_method.txt \\\n  --caption \"Overview of our encoder-decoder architecture with sparse routing\"\n```\n\nWith input optimization and auto-refine:\n\n```bash\npaperbanana generate \\\n  --input my_method.txt \\\n  --caption \"Overview of our encoder-decoder framework\" \\\n  --optimize --auto\n```\n\nOutput is saved to `outputs/run_<timestamp>/final_output.png` along with all intermediate iterations and metadata.\n\n---\n\n## How It Works\n\nPaperBanana implements a multi-agent pipeline with up to 7 specialized agents:\n\n**Phase 0 -- Input Optimization (optional, `--optimize`):**\n\n0. **Input Optimizer** runs two parallel VLM calls:\n   - **Context Enricher** structures raw methodology text into diagram-ready f",
      "default_branch": "main"
    },
    "fetched_at": "2026-03-01T03:37:50.659203"
  },
  {
    "basic_info": {
      "name": "Polymarket-Trading-Bot",
      "full_name": "angel10x/Polymarket-Trading-Bot",
      "owner": "angel10x",
      "description": "Polymarket Trading Bot â€” polymarket bot polymarket trading bot polymarket copy trading bot polymarket bot polymarket copy trading bot polymarket trading bot polymarket trading bot polymarket botpolymarket trading bot polymarket copy trading bot polymarket copy trading bot polymarket trading bot polymarket bot polymarket trading bot polymarket copy ",
      "url": "https://github.com/angel10x/Polymarket-Trading-Bot",
      "clone_url": "https://github.com/angel10x/Polymarket-Trading-Bot.git",
      "ssh_url": "git@github.com:angel10x/Polymarket-Trading-Bot.git",
      "homepage": "https://polymarket.com/",
      "created_at": "2026-02-19T13:33:37Z",
      "updated_at": "2026-03-01T03:19:54Z",
      "pushed_at": "2026-02-27T19:58:10Z"
    },
    "stats": {
      "stars": 933,
      "forks": 2,
      "watchers": 933,
      "open_issues": 0,
      "size": 33
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 34025
      },
      "license": "MIT License",
      "topics": [
        "arbitrage",
        "clob-client",
        "polymarket",
        "polymarket-copytrade",
        "polymarket-market-maker",
        "polymarket-python-copy-trading-bot",
        "python",
        "python3",
        "trading",
        "trading-and-supply"
      ]
    },
    "content": {
      "readme": "# Polymarket Bot\n\n<div align=\"center\">\n\n![Rust](https://img.shields.io/badge/python-3.12+-orange.svg)\n![License](https://img.shields.io/badge/license-MIT-blue.svg)\n![Version](https://img.shields.io/badge/version-1.0.0-green.svg)\n\n[English](#polymarket-bot) â€¢ [ç®€ä½“ä¸­æ–‡](README.zh-CN.md)\n\n</div>\n\nDonâ€™t stress the process â€” focus on the results. \n\nThatâ€™s how I approach trading. What about you? \n\nI have a feeling you see it the same way, so letâ€™s run it.\n\n## Important notice\n\nThis software is a **copy-trading bot** that replicates positions from a chosen wallet on Polymarket. The current release (v1) does not include built-in strategy logic, risk controls, or guarantees of profitability. Outcomes depend on the tracked wallet, your sizing, and market conditions. You can extend or adapt the code to implement your own risk and strategy layers. A newer version exists but is not publicly released; updates and support will continue to be provided over time.\n\n### Contact\n- [Discussions](../../discussions)\n- [WhatsApp](https://wa.me/16286666724?text=Hello%20there)\n- [Telegram](https://t.me/angel_10_x)\n- [Discord](https://discord.com/users/1114372741672488990)\n\n## Why python?\n\nI still remember the moment I decided to build my Polymarket bot.\n\nAt first, I stood at the familiar crossroads every developer knows too well ' which language should I trust with something that needed to be fast, reliable, and intelligent? TypeScript tempted me with its structured ecosystem and rich tooling. Other languages promised performance or strict type guarantees. But none of them felt like the right companion for the journey I was about to begin.\n\nBecause this wasn't just another script.\n\nThis was a living system ' a bot that would observe markets, interpret signals, adapt to uncertainty, and act with precision in a space where probabilities shift like tides.\n\nAnd that is why I chose Python.\n\nPython didn't just offer syntax; it offered *flow*. The code felt like thought translated directly into action. When I began wiring together market data ingestion, event interpretation, and strategy logic, Python allowed me to move at the speed of curiosity. Ideas became prototypes in minutes. Strategies evolved overnight. Refactors felt like gentle edits rather than structural battles.\n\nBut the deeper reason was intelligence.\n\nBuilding a Polymarket bot is not merely about sending transactions, it's about understanding context. Parsing sentiment, modeling probabilities, experimenting with reinforcement signals, and testing hypotheses rapidly. Python's ecosystem placed an entire laboratory at my fingertips: data analysis tools, statistical libraries, async frameworks, and machine learning capabilities that transformed the project from a trading bot into an experimental platform.\n\nTypeScript could orchestrate APIs elegantly.\nPython could help me *think*.\n\nThere was also a quiet comfort in Python's readability. Weeks later, when strategies became layered and decision paths intertwined, I could still open a file and understand my past self. The code read like notes from a conversation rather than instructions carved in stone. That clarity made iteration fearless.\n\nAnd iteration is survival in prediction markets.\n\nPython also became the bridge between research and execution. I could simulate strategies, visualize outcomes, tweak assumptions, and immediately embed those insights back into the bot's runtime behavior. The boundary between experimentation and production dissolved.\n\nWhat surprised me most, though, was the emotional dimension.\n\nLate nights debugging WebSocket streams, watching probabilities flicker, seeing the first autonomous trade execute successfully ' Python made those moments feel collaborative rather than adversarial. The language faded into the background, leaving only the problem space and my curiosity.\n\nThe Polymarket bot eventually became more than automation.\nIt became a narrative of decisions, risks, and evolving understanding.\n\nChoosing Python was not about rejecting TypeScript or other languages. It was about selecting the medium that best matched the nature of the problem ' dynamic, exploratory, data-driven, and adaptive.\n\nIn the end, Python didn't just power the bot.\n\nIt powered the process of discovery that made the bot possible.\n\n\n## Setup\n\n1. **Install dependencies**\n\n   ```bash\n   pip install -r requirements.txt\n   ```\n\n2. **Environment**\n\n   Copy `.env.example` to `.env` and set your Polymarket credentials:\n\n   ```\n   POLYMARKET_PRIVATE_KEY=your_private_key\n   POLYMARKET_PROXY_ADDRESS=your_proxy_address\n   ```\n\n3. **Config**\n\n   Create or edit `config.json` in the project root:\n\n   ```json\n   {\n       \"wallets_to_track\": [\"0x...\"],\n       \"copy_percentage\": 1.0,\n       \"rate_limit\": 25,\n       \"poll_interval\": 0.5,\n       \"trading_enabled\": true\n   }\n   ```\n\n### Configuration options\n\n| Option              | Description                                                                 |\n|---------------------|--------------------",
      "default_branch": "main"
    },
    "fetched_at": "2026-03-01T03:37:51.969295"
  },
  {
    "basic_info": {
      "name": "desloppify",
      "full_name": "peteromallet/desloppify",
      "owner": "peteromallet",
      "description": "Agent harness to make your slop code well-engineered and beautiful.",
      "url": "https://github.com/peteromallet/desloppify",
      "clone_url": "https://github.com/peteromallet/desloppify.git",
      "ssh_url": "git@github.com:peteromallet/desloppify.git",
      "homepage": "",
      "created_at": "2026-02-11T17:27:42Z",
      "updated_at": "2026-03-01T03:21:10Z",
      "pushed_at": "2026-02-28T12:12:38Z"
    },
    "stats": {
      "stars": 890,
      "forks": 51,
      "watchers": 890,
      "open_issues": 4,
      "size": 8672
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 4652141,
        "HTML": 9762,
        "Makefile": 2049
      },
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "# Desloppify - an agent harness to make your codebase ğŸ¤Œ\n\n[![PyPI version](https://img.shields.io/pypi/v/desloppify)](https://pypi.org/project/desloppify/) ![Python 3.11+](https://img.shields.io/badge/python-3.11%2B-blue)\n\nDesloppify gives your AI coding agent the tools to identify, understand, and systematically improve codebase quality. It combines mechanical detection (dead code, duplication, complexity) with subjective LLM review (naming, abstractions, module boundaries), then works through a prioritized fix loop. State persists across scans so it chips away over multiple sessions, and the scoring is designed to resist gaming.\n\n<img src=\"assets/explained.png\" width=\"100%\">\n\nThe score gives your agent a north-star, and the tooling helps it plan, execute, and resolve issues until it hits your target â€” with a lot of tricks to keep it on track. A score above 98 should correlate with a codebase a seasoned engineer would call beautiful.\n\nThat score generates a scorecard badge for your GitHub profile or README:\n\n<img src=\"assets/scorecard.png\" width=\"100%\">\n\nCurrently supports 28 languages â€” full plugin depth for TypeScript, Python, C#, Dart, GDScript, and Go; generic linter + tree-sitter support for Rust, Ruby, Java, Kotlin, and 17 more.\n\n## For your agent's consideration...\n\nPaste this prompt into your agent:\n\n```\nI want you to improve the quality of this codebase. To do this, install and run desloppify.\nRun ALL of the following (requires Python 3.11+):\n\npip install --upgrade \"desloppify[full]\"\ndesloppify update-skill claude    # IMPORTANT â€” installs the workflow guide. Pick yours: claude, cursor, codex, copilot, windsurf, gemini\ndesloppify scan --path .\ndesloppify next\n\n--path is the directory to scan (use \".\" for the whole project, or \"src/\" etc).\n\nYour goal is to get the strict score that Desloppify produces as high as possible. Don't be lazy. Fix things properly\nand fix things deep. Large refactors are fine if that's what it takes but also small fixes are great. The scoring is designed\nto resist gaming, so the only way to improve it is to actually make the code better. Don't cheat.\n\nFollow `next` â€” it tells you exactly what to fix, which file, and the resolve command to run\nwhen done. Fix the issue, resolve it, run `next` again. Keep going until done.\nUse `plan` to reorder priorities or cluster related issues.\nYou can scan to refresh things. The scan output includes agent instructions â€” follow them, don't augment with your own analysis but follow its plan.\n```\n\n## From Vibe Coding to Vibe Engineering\n\nVibe coding gets things built fast. But the codebases it produces tend to rot in ways that are hard to see and harder to fix â€” not just the mechanical stuff like dead imports, but the structural kind. Abstractions that made sense at first stop making sense. Naming drifts. Error handling is done three different ways. The codebase works, but working in it gets worse over time.\n\nLLMs are actually good at spotting this now, if you ask them the right questions. That's the core bet here â€” that an agent with the right framework can hold a codebase to a real standard, the kind that used to require a senior engineer paying close attention over months.\n\nSo we're trying to define what \"good\" looks like as a score that's actually worth optimizing. Not a lint score you game to 100 by suppressing warnings. Something where improving the number means the codebase genuinely got better. That's hard, and we're not done, but the anti-gaming stuff matters to us a lot â€” it's the difference between a useful signal and a vanity metric.\n\nThe hope is that anyone can use this to build something a seasoned engineer would look at and respect. That's the bar we're aiming for.\n\nIf you'd like to join a community of vibe engineers who want to build beautiful things, [come hang out](https://discord.gg/aZdzbZrHaY).\n\n<img src=\"assets/engineering.png\" width=\"100%\">\n\n---\n\n<details>\n<summary><strong>Stuff you probably won't need to know</strong></summary>\n\n#### Commands\n\n| Command | Description |\n|---------|-------------|\n| `scan [--reset-subjective]` | Run all detectors, update state (optional: reset subjective baseline to 0 first) |\n| `status` | Score + per-tier progress |\n| `show <pattern>` | Findings by file, directory, detector, or ID |\n| `next [--tier N] [--explain]` | Highest-priority open finding (--explain: with score context) |\n| `resolve <status> <patterns>` | Mark open / fixed / wontfix / false_positive |\n| `fix <fixer> [--dry-run]` | Auto-fix mechanical issues |\n| `review --prepare` | Generate subjective review packet (`query.json`) |\n| `review --run-batches --runner codex --parallel` | Run blind subjective batch assessments, merge/import, optionally `--scan-after-import` |\n| `review --import <file> [--allow-partial]` | Import subjective review findings (fails closed on invalid findings by default) |\n| `review --external-start --external-runner claude` | Start Claude cloud blind-review session (creates session/token/template) |\n| `re",
      "default_branch": "main"
    },
    "fetched_at": "2026-03-01T03:37:53.248837"
  },
  {
    "basic_info": {
      "name": "openakita",
      "full_name": "openakita/openakita",
      "owner": "openakita",
      "description": "An open-source AI assistant framework with skills and agent architecture",
      "url": "https://github.com/openakita/openakita",
      "clone_url": "https://github.com/openakita/openakita.git",
      "ssh_url": "git@github.com:openakita/openakita.git",
      "homepage": "http://openakita.ai",
      "created_at": "2026-01-30T17:36:05Z",
      "updated_at": "2026-03-01T02:39:31Z",
      "pushed_at": "2026-02-28T12:34:30Z"
    },
    "stats": {
      "stars": 872,
      "forks": 87,
      "watchers": 872,
      "open_issues": 14,
      "size": 122636
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 5391315,
        "TypeScript": 809362,
        "Rust": 202336,
        "CSS": 60209,
        "NSIS": 53167,
        "JavaScript": 45621,
        "Shell": 43875,
        "PowerShell": 33012,
        "HTML": 22447
      },
      "license": "MIT License",
      "topics": [
        "agent",
        "ai",
        "assistant",
        "automation",
        "claw",
        "clawd",
        "clawdbot",
        "openclaw",
        "python"
      ]
    },
    "content": {
      "readme": "<p align=\"center\">\n  <img src=\"docs/assets/logo.png\" alt=\"OpenAkita Logo\" width=\"200\" />\n</p>\n\n<h1 align=\"center\">OpenAkita</h1>\n\n<p align=\"center\">\n  <strong>Self-Evolving AI Agent â€” Learns Autonomously, Never Gives Up</strong>\n</p>\n\n<p align=\"center\">\n  <img src=\"https://img.shields.io/badge/license-MIT-blue.svg?style=flat-square\" alt=\"License\" height=\"20\" />\n  <img src=\"https://img.shields.io/badge/python-3.11+-blue.svg?style=flat-square\" alt=\"Python Version\" height=\"20\" />\n  <img src=\"https://img.shields.io/github/v/release/openakita/openakita?color=green&style=flat-square\" alt=\"Version\" height=\"20\" />\n  <img src=\"https://img.shields.io/pypi/v/openakita?color=green&style=flat-square\" alt=\"PyPI\" height=\"20\" />\n  <img src=\"https://img.shields.io/github/actions/workflow/status/openakita/openakita/ci.yml?branch=main&style=flat-square\" alt=\"Build Status\" height=\"20\" />\n</p>\n\n<p align=\"center\">\n  <a href=\"#desktop-terminal\">Desktop Terminal</a> â€¢\n  <a href=\"#features\">Features</a> â€¢\n  <a href=\"#quick-start\">Quick Start</a> â€¢\n  <a href=\"#architecture\">Architecture</a> â€¢\n  <a href=\"#documentation\">Documentation</a>\n</p>\n\n<p align=\"center\">\n  <strong>English</strong> | <a href=\"README_CN.md\">ä¸­æ–‡</a>\n</p>\n\n---\n\n## What is OpenAkita?\n\n**An AI Agent that keeps getting smarter while you sleep.**\n\nMost AI assistants forget you the moment the chat ends. OpenAkita teaches itself new skills, fixes its own bugs, and remembers everything you've told it â€” like the Akita dog it's named after: **loyal, reliable, never quits**.\n\nSet up in 3 minutes with just an API key. 8 personas, 6 IM platforms, and yes â€” it sends memes.\n\n---\n\n## Desktop Terminal\n\n<p align=\"center\">\n  <img src=\"docs/assets/desktop_terminal_en.png\" alt=\"OpenAkita Desktop Terminal\" width=\"800\" />\n</p>\n\nOpenAkita provides a cross-platform **Desktop Terminal** (built with Tauri + React) â€” an all-in-one AI assistant with chat, configuration, monitoring, and skill management:\n\n- **AI Chat Assistant** â€” Streaming output, Markdown rendering, multimodal input, Thinking display, Plan mode\n- **Bilingual (CN/EN)** â€” Auto-detects system language, one-click switch, fully internationalized\n- **Localization & i18n** â€” First-class support for Chinese and international ecosystems, PyPI mirrors, IM channels\n- **LLM Endpoint Manager** â€” Multi-provider, multi-endpoint, auto-failover, online model list fetching\n- **IM Channel Setup** â€” Telegram, Feishu, WeCom, DingTalk, QQ Official Bot, OneBot â€” all in one place\n- **Persona & Living Presence** â€” 8 role presets, proactive greetings, memory recall, learns your preferences\n- **Skill Marketplace** â€” Browse, download, configure skills in one place\n- **Status Monitor** â€” Compact dashboard: service/LLM/IM health at a glance\n- **System Tray** â€” Background residency + auto-start on boot, one-click start/stop\n\n> **Download**: [GitHub Releases](https://github.com/openakita/openakita/releases)\n>\n> Available for Windows (.exe) / macOS (.dmg) / Linux (.deb / .AppImage)\n\n### 3-Minute Quick Setup â€” Zero to Chatting\n\nNo command line. No config files. **From install to conversation in 3 minutes**:\n\n<p align=\"center\">\n  <img src=\"docs/assets/desktop_quick_config.gif\" alt=\"OpenAkita Quick Setup vs Full Setup\" width=\"800\" />\n</p>\n\n<table>\n<tr>\n<td width=\"50%\">\n\n**Quick Setup (Recommended for new users)**\n\n```\nâ‘  Fill in  â†’ Add LLM endpoint + IM (optional)\nâ‘¡ One-click â†’ Auto-create env, install deps, write config\nâ‘¢ Done      â†’ Launch service, start chatting\n```\n\nJust one API Key, everything else is automatic:\n- Auto-create workspace\n- Auto-download & install Python 3.11\n- Auto-create venv + pip install\n- Auto-write 40+ recommended defaults\n- Auto-save IM channel settings\n\n</td>\n<td width=\"50%\">\n\n**Full Setup (Power users)**\n\n```\nWorkspace â†’ Python â†’ Install â†’ LLM Endpoints\nâ†’ IM Channels â†’ Tools & Skills â†’ Agent System â†’ Finish\n```\n\n8-step guided wizard with full control:\n- Custom workspaces (multi-env isolation)\n- Choose Python version & install source\n- Configure desktop automation, MCP tools\n- Tune persona, living presence parameters\n- Logging, memory, scheduler & more\n\n</td>\n</tr>\n</table>\n\n> Switch between modes anytime â€” click \"Switch Setup Mode\" in the sidebar to return to the selection page without losing existing configuration.\n>\n> See [Configuration Guide](docs/configuration-guide.md) for full details.\n\n---\n\n## Features\n\n| | Feature | In One Line |\n|:---:|---------|-------------|\n| **1** | **Self-Learning & Evolution** | Daily self-check, memory consolidation, task retrospection, auto skill generation â€” it gets smarter while you sleep |\n| **2** | **8 Personas + Living Presence** | Girlfriend / Butler / Jarvisâ€¦ not just role-play â€” proactive greetings, remembers your birthday, auto-mutes at night |\n| **3** | **3-Min Quick Setup** | Desktop app, one-click start â€” just drop in an API Key, Python/env/deps/config all automatic |\n| **4** | **Plan Mode** | Complex tasks auto-decomposed into multi-step plans, real-time tracking, Plan â†’ Act â†’ Verify lo",
      "default_branch": "main"
    },
    "fetched_at": "2026-03-01T03:37:54.535037"
  },
  {
    "basic_info": {
      "name": "python-ws",
      "full_name": "eooce/python-ws",
      "owner": "eooce",
      "description": "build vless / trojan /shadowsocks  proxies on python server,no need core",
      "url": "https://github.com/eooce/python-ws",
      "clone_url": "https://github.com/eooce/python-ws.git",
      "ssh_url": "git@github.com:eooce/python-ws.git",
      "homepage": "",
      "created_at": "2026-02-22T07:42:15Z",
      "updated_at": "2026-03-01T03:35:21Z",
      "pushed_at": "2026-02-27T23:23:56Z"
    },
    "stats": {
      "stars": 867,
      "forks": 228,
      "watchers": 867,
      "open_issues": 2,
      "size": 93
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 24303,
        "HTML": 22525,
        "Dockerfile": 246
      },
      "license": "GNU General Public License v3.0",
      "topics": [
        "proxy",
        "serverless",
        "shadowsocks",
        "trojan",
        "vless",
        "websocks"
      ]
    },
    "content": {
      "readme": "<div align=\"center\">\n\n# Python-ws\nåŸºäºpython serverlesså®ç°çš„vless+trojan+shadowsocksä¸‰åè®®ï¼Œè½»é‡ï¼Œæ— å†…æ ¸ã€‚\n\n---\n\nTelegramäº¤æµåé¦ˆç¾¤ç»„ï¼šhttps://t.me/eooceu\n\n</div>\n\n\n* ç”¨äºpythonç¯å¢ƒçš„ç©å…·å’Œå®¹å™¨ï¼Œvless+trojan+shadowsocksä¸‰åè®®ï¼Œé›†æˆå“ªå’æ¢é’ˆæœåŠ¡(v0æˆ–v1)ï¼Œå¯è‡ªè¡Œæ·»åŠ ç¯å¢ƒå˜é‡å¼€å¯\n\n* PaaS å¹³å°è®¾ç½®çš„ç¯å¢ƒå˜é‡\n  | å˜é‡å        | æ˜¯å¦å¿…é¡» | é»˜è®¤å€¼ | å¤‡æ³¨ |\n  | ------------ | ------ | ------ | ------ |\n  | UUID         | å¦ |5efabea4-f6d4-91fd-b8f0-17e004c89c60| å¼€å¯äº†å“ªå’v1,è¯·ä¿®æ”¹UUID|\n  | PORT         | å¦ |  3000  |  èŠ‚ç‚¹ç›‘å¬ç«¯å£,é»˜è®¤è‡ªåŠ¨è·å–åˆ†é…çš„ç«¯å£                  |\n  | NEZHA_SERVER | å¦ |        |å“ªå’v1å¡«å†™å½¢å¼ï¼šnz.abc.com:8008   å“ªå’v0å¡«å†™å½¢å¼ï¼šnz.abc.com|\n  | NEZHA_PORT   | å¦ |        | å“ªå’v1æ²¡æœ‰æ­¤å˜é‡ï¼Œv0çš„agentç«¯å£| \n  | NEZHA_KEY    | å¦ |        | å“ªå’v1çš„NZ_CLIENT_SECRETæˆ–v0çš„agentç«¯å£ |\n  | NAME         | å¦ |        | èŠ‚ç‚¹åç§°å‰ç¼€ï¼Œä¾‹å¦‚ï¼škoyeb |\n  | DOMAIN       | æ˜¯ |        | é¡¹ç›®åˆ†é…çš„åŸŸåæˆ–å·²åä»£çš„åŸŸåï¼Œä¸åŒ…æ‹¬https://å‰ç¼€  |\n  | SUB_PATH     | å¦ |  sub   | è®¢é˜…token    |\n  | AUTO_ACCESS  | å¦ |  false | æ˜¯å¦å¼€å¯è‡ªåŠ¨è®¿é—®ä¿æ´»,falseä¸ºå…³é—­,trueä¸ºå¼€å¯,éœ€åŒæ—¶å¡«å†™DOMAINå˜é‡ |\n  | DEBUG        | å¦ |  false | è°ƒè¯•æ¨¡å¼ï¼Œé»˜è®¤å…³é—­ï¼Œtrueå¼€å¯                   |\n\n* åŸŸå/${SUB_APTH}æŸ¥çœ‹èŠ‚ç‚¹ä¿¡æ¯ï¼Œéæ ‡ç«¯å£ï¼ŒåŸŸå:ç«¯å£/${SUB_APTH}  SUB_APTHä¸ºè‡ªè¡Œè®¾ç½®çš„è®¢é˜…tokenï¼Œæœªè®¾ç½®é»˜è®¤ä¸ºsub\n\n    \n* æ¸©é¦¨æç¤ºï¼šREADAME.mdä¸ºè¯´æ˜æ–‡ä»¶ï¼Œè¯·ä¸è¦ä¸Šä¼ ã€‚\n* pythonæ··è‚´åœ°å€ï¼šhttps://freecodingtools.org/tools/obfuscator/python\n\n### ä½¿ç”¨cloudflare workers æˆ– snippets åä»£åŸŸåç»™èŠ‚ç‚¹å¥—cdnåŠ é€Ÿ,ä¹Ÿå¯ä»¥ä½¿ç”¨ç«¯å£å›æºæ–¹å¼\n```\nexport default {\n    async fetch(request, env) {\n        let url = new URL(request.url);\n        if (url.pathname.startsWith('/')) {\n            var arrStr = [\n                'change.your.domain', // æ­¤å¤„å•å¼•å·é‡Œå¡«å†™ä½ çš„èŠ‚ç‚¹ä¼ªè£…åŸŸå\n            ];\n            url.protocol = 'https:'\n            url.hostname = getRandomArray(arrStr)\n            let new_request = new Request(url, request);\n            return fetch(new_request);\n        }\n        return env.ASSETS.fetch(request);\n    },\n};\nfunction getRandomArray(array) {\n  const randomIndex = Math.floor(Math.random() * array.length);\n  return array[randomIndex];\n}\n```\n\n# ç›¸å…³é¡¹ç›®\n- Nodejsç‰ˆï¼Œè¿æ¥ç›´è¾¾ï¼š[node-ws](https://github.com/eooce/node-ws)\n- Java ç‰ˆï¼Œé“¾æ¥ç›´è¾¾ï¼š[java-ws](https://github.com/eooce/java-ws)\n- Golangç‰ˆï¼Œè¿æ¥ç›´è¾¾ï¼š[golang-ws](https://github.com/eooce/node-ws/tree/golang)\n\nç‰ˆæƒæ‰€æœ‰ Â©2025 `eooce`\n",
      "default_branch": "main"
    },
    "fetched_at": "2026-03-01T03:37:55.841261"
  },
  {
    "basic_info": {
      "name": "apple-silicon-accelerometer",
      "full_name": "olvvier/apple-silicon-accelerometer",
      "owner": "olvvier",
      "description": "reading the undocumented mems accelerometer + gyroscope on apple silicon macbooks via iokit hid",
      "url": "https://github.com/olvvier/apple-silicon-accelerometer",
      "clone_url": "https://github.com/olvvier/apple-silicon-accelerometer.git",
      "ssh_url": "git@github.com:olvvier/apple-silicon-accelerometer.git",
      "homepage": "https://medium.com/@oli.bourbonnais/your-macbook-has-an-accelerometer-and-you-can-read-it-in-real-time-in-python-28d9395fb180",
      "created_at": "2026-02-19T04:37:47Z",
      "updated_at": "2026-03-01T02:28:56Z",
      "pushed_at": "2026-03-01T02:53:29Z"
    },
    "stats": {
      "stars": 862,
      "forks": 43,
      "watchers": 862,
      "open_issues": 3,
      "size": 6368
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 91976,
        "Objective-C": 15336,
        "Shell": 549
      },
      "license": "MIT License",
      "topics": [
        "accelerometer",
        "apple",
        "applespu",
        "gyroscope",
        "hid",
        "iokit",
        "m2",
        "m3",
        "m4",
        "macbook",
        "macos",
        "mems",
        "research",
        "sensor",
        "spu"
      ]
    },
    "content": {
      "readme": "# apple-silicon-accelerometer\n\nmore information: [read the article on Medium](https://medium.com/@oli.bourbonnais/your-macbook-has-an-accelerometer-and-you-can-read-it-in-real-time-in-python-28d9395fb180)\n\nit turns out modern macbook pros have an undocumented mems accelerometer + gyroscope managed by the sensor processing unit (spu).\nthis project reads both via iokit hid, along with lid angle and ambient light sensors from the same interface\n\n![demo](https://raw.githubusercontent.com/olvvier/apple-silicon-accelerometer/main/assets/demo.gif)\n\n## try it\n\n    git clone https://github.com/olvvier/apple-silicon-accelerometer\n    cd apple-silicon-accelerometer\n    python3 -m venv .venv && source .venv/bin/activate\n    pip install -e .[demo]\n    sudo .venv/bin/python3 motion_live.py\n\n## what is this\n\napple silicon chips (M2/M3/M4/M5) have a hard to find mems IMU (accelerometer + gyroscope) managed by the sensor processing unit (SPU).\nit's not exposed through any public api or framework.\nthis project reads raw 3-axis acceleration and angular velocity data at ~800hz via iokit hid callbacks.\n\nonly tested on macbook pro m3 pro so far - might work on other apple silicon macs but no guarantees\n\n## how it works\n\nthe sensor lives under AppleSPUHIDDevice in the iokit registry, on vendor usage page 0xFF00.\nusage 3 is the accelerometer, usage 9 is the gyroscope (same physical IMU, believed to be Bosch BMI286 based on teardowns).\nthe driver is AppleSPUHIDDriver which is part of the sensor processing unit.\nwe open it with IOHIDDeviceCreate and register an asynchronous callback via IOHIDDeviceRegisterInputReportCallback.\ndata comes as 22-byte hid reports with x/y/z as int32 little-endian at byte offsets 6, 10, 14.\ndivide by 65536 to get the value in g (accel) or deg/s (gyro).\ncallback rate is ~100hz (decimated from ~800hz native)\n\norientation is computed by fusing accel + gyro with a Mahony AHRS quaternion filter and displayed as roll/pitch/yaw gauges\n\nyou can verify the device exists on your machine with:\n\n    ioreg -l -w0 | grep -A5 AppleSPUHIDDevice\n\n## install (beta API)\n\n    pip install macimu\n\nif you get `externally-managed-environment` (homebrew python), use a venv:\n\n    python3 -m venv .venv && source .venv/bin/activate && pip install macimu\n\n```python\nfrom macimu import IMU\n\nif __name__ == '__main__':\n    with IMU() as imu:\n        accel = imu.latest_accel()       # Sample(x, y, z) in g\n        gyro = imu.latest_gyro()         # Sample(x, y, z) in deg/s\n\n        for s in imu.read_accel():       # all new samples since last call\n            print(s.x, s.y, s.z)\n```\n\nrequires root (sudo) because iokit hid device access needs elevated privileges.\nnote: accelerometer reads ~1g at rest (gravity). use `macimu.filters.remove_gravity()` to isolate dynamic acceleration.\n\n### check if sensor exists (no root needed)\n\n```python\nfrom macimu import IMU\nprint(IMU.available())   # True on macbook pro m2+\n```\n\n### real-time orientation (roll / pitch / yaw)\n\nfuses accel + gyro with a mahony quaternion filter, no math needed on your side\n\n```python\nfrom macimu import IMU\n\nif __name__ == '__main__':\n    with IMU(orientation=True) as imu:\n        o = imu.orientation()\n        print(f\"{o.roll:.1f}Â° {o.pitch:.1f}Â° {o.yaw:.1f}Â°\")\n        print(o.qw, o.qx, o.qy, o.qz)  # raw quaternion\n```\n\n### timestamped samples (hardware timestamps from iokit)\n\neach sample includes a precise timestamp from the hid report (mach_absolute_time),\nnot a python-side clock. every report gets its own unique timestamp.\n\n```python\nfrom macimu import IMU\n\nif __name__ == '__main__':\n    with IMU() as imu:\n        for s in imu.read_accel_timed():\n            print(f\"t={s.t:.6f}  x={s.x:.3f}  y={s.y:.3f}  z={s.z:.3f}\")\n```\n\n### streaming with callback\n\n```python\nimport time\nfrom macimu import IMU\n\ndef on_sample(s):\n    print(s.x, s.y, s.z)\n\nif __name__ == '__main__':\n    with IMU() as imu:\n        stop = imu.on_accel(on_sample)  # background thread\n        time.sleep(10)\n        stop()                          # unregister\n```\n\n### sample rate control\n\n```python\nIMU(sample_rate=200)  # ~200 hz (preferred way)\nIMU(sample_rate=50)   # ~50 hz\nIMU(decimation=1)     # ~800 hz (full native rate)\nIMU(decimation=8)     # ~100 hz (default)\n```\n\n### signal processing (zero-dependency biquad butterworth filters)\n\n```python\nfrom macimu import IMU\nfrom macimu.filters import magnitude, remove_gravity, high_pass, low_pass, peak_detect\n\nif __name__ == '__main__':\n    with IMU() as imu:\n        samples = imu.read_accel()\n        m = magnitude(samples[0].x, samples[0].y, samples[0].z)\n        dynamic = remove_gravity(samples)               # kalman filter gravity removal\n        smooth = low_pass(samples, 5.0, 100.0)          # 2nd-order butterworth\n        taps = high_pass(samples, 10.0, 100.0, order=4) # 4th-order, -24 dB/oct\n        mags = [magnitude(s.x, s.y, s.z) for s in samples]\n        hits = peak_detect(mags, threshold=1.2)          # detect impacts\n```\n\n### mock mode (no root ne",
      "default_branch": "main"
    },
    "fetched_at": "2026-03-01T03:37:57.135998"
  }
]