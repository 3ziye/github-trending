[
  {
    "basic_info": {
      "name": "planning-with-files",
      "full_name": "OthmanAdi/planning-with-files",
      "owner": "OthmanAdi",
      "description": "Claude Code skill implementing Manus-style persistent markdown planning â€” the workflow pattern behind the $2B acquisition.",
      "url": "https://github.com/OthmanAdi/planning-with-files",
      "clone_url": "https://github.com/OthmanAdi/planning-with-files.git",
      "ssh_url": "git@github.com:OthmanAdi/planning-with-files.git",
      "homepage": "https://www.aikux.ai",
      "created_at": "2026-01-03T07:37:28Z",
      "updated_at": "2026-02-01T03:28:43Z",
      "pushed_at": "2026-01-27T21:22:59Z"
    },
    "stats": {
      "stars": 11943,
      "forks": 1098,
      "watchers": 11943,
      "open_issues": 14,
      "size": 373
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 50377,
        "Shell": 32752,
        "PowerShell": 32607
      },
      "license": "MIT License",
      "topics": [
        "adal",
        "agent",
        "agent-skills",
        "agents",
        "antigravity",
        "claude",
        "claude-code",
        "claude-skills",
        "clawd",
        "clawdbot",
        "clawdbot-skill",
        "clawdhub",
        "codebuddy",
        "cursor",
        "factory-ai",
        "kilocode",
        "manus",
        "manus-ai",
        "moltbot",
        "moltbot-skills"
      ]
    },
    "content": {
      "readme": "# Planning with Files\n\n> **Work like Manus** â€” the AI agent company Meta acquired for **$2 billion**.\n\n## Thank You\n\nTo everyone who starred, forked, and shared this skill â€” thank you. This project blew up in less than 24 hours, and the support from the community has been incredible.\n\nIf this skill helps you work smarter, that's all I wanted.\n\n---\n\nA Claude Code plugin that transforms your workflow to use persistent markdown files for planning, progress tracking, and knowledge storage â€” the exact pattern that made Manus worth billions.\n\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n[![Claude Code Plugin](https://img.shields.io/badge/Claude%20Code-Plugin-blue)](https://code.claude.com/docs/en/plugins)\n[![Claude Code Skill](https://img.shields.io/badge/Claude%20Code-Skill-green)](https://code.claude.com/docs/en/skills)\n[![Cursor Skills](https://img.shields.io/badge/Cursor-Skills-purple)](https://docs.cursor.com/context/skills)\n[![Kilocode Skills](https://img.shields.io/badge/Kilocode-Skills-orange)](https://kilo.ai/docs/agent-behavior/skills)\n[![Gemini CLI](https://img.shields.io/badge/Gemini%20CLI-Skills-4285F4)](https://geminicli.com/docs/cli/skills/)\n[![Moltbot](https://img.shields.io/badge/Moltbot-Skills-FF6B6B)](https://docs.molt.bot/tools/skills)\n[![Kiro](https://img.shields.io/badge/Kiro-Steering-00D4AA)](https://kiro.dev/docs/cli/steering/)\n[![AdaL CLI](https://img.shields.io/badge/AdaL%20CLI-Skills-9B59B6)](https://docs.sylph.ai/features/plugins-and-skills)\n[![Version](https://img.shields.io/badge/version-2.13.0-brightgreen)](https://github.com/OthmanAdi/planning-with-files/releases)\n[![SkillCheck Validated](https://img.shields.io/badge/SkillCheck-Validated-4c1)](https://getskillcheck.com)\n\n## Quick Install\n\n```bash\n# Install the plugin\nclaude plugins install OthmanAdi/planning-with-files\n```\n\nThat's it! Now use one of these commands in Claude Code:\n\n| Command | Autocomplete | Description |\n|---------|--------------|-------------|\n| `/planning-with-files:plan` | Type `/plan` | Shorter command (v2.11.0+) |\n| `/planning-with-files:start` | Type `/planning` | Original command |\n\n**Alternative:** If you want `/planning-with-files` (without prefix), copy skills to your local folder:\n\n```bash\n# Optional: Copy skills for /planning-with-files command\ncp -r ~/.claude/plugins/cache/planning-with-files/planning-with-files/*/skills/planning-with-files ~/.claude/skills/\n```\n\n**Windows (PowerShell):**\n```powershell\n# Install the plugin\nclaude plugins install OthmanAdi/planning-with-files\n\n# Optional: Copy skills for /planning-with-files command\nCopy-Item -Recurse -Path \"$env:USERPROFILE\\.claude\\plugins\\cache\\planning-with-files\\planning-with-files\\*\\skills\\planning-with-files\" -Destination \"$env:USERPROFILE\\.claude\\skills\\\"\n```\n\nSee [docs/installation.md](docs/installation.md) for all installation methods.\n\n## Supported IDEs\n\n| IDE | Status | Installation Guide | Format |\n|-----|--------|-------------------|--------|\n| Claude Code | âœ… Full Support | [Installation](docs/installation.md) | Plugin + SKILL.md |\n| Gemini CLI | âœ… Full Support | [Gemini Setup](docs/gemini.md) | Agent Skills |\n| Moltbot | âœ… Full Support | [Moltbot Setup](docs/moltbot.md) | Workspace/Local Skills |\n| Kiro | âœ… Full Support | [Kiro Setup](docs/kiro.md) | Steering Files |\n| Cursor | âœ… Full Support | [Cursor Setup](docs/cursor.md) | Skills |\n| Continue | âœ… Full Support | [Continue Setup](docs/continue.md) | Skills + Prompt files |\n| Kilocode | âœ… Full Support | [Kilocode Setup](docs/kilocode.md) | Skills |\n| OpenCode | âœ… Full Support | [OpenCode Setup](docs/opencode.md) | Personal/Project Skill |\n| Codex | âœ… Full Support | [Codex Setup](docs/codex.md) | Personal Skill |\n| FactoryAI Droid | âœ… Full Support | [Factory Setup](docs/factory.md) | Workspace/Personal Skill |\n| Antigravity | âœ… Full Support | [Antigravity Setup](docs/antigravity.md) | Workspace/Personal Skill |\n| CodeBuddy | âœ… Full Support | [CodeBuddy Setup](docs/codebuddy.md) | Workspace/Personal Skill |\n| AdaL CLI (Sylph AI) | âœ… Full Support | [AdaL Setup](docs/adal.md) | Personal/Project Skills |\n\n> **Note:** If your IDE uses the legacy Rules system instead of Skills, see the [`legacy-rules-support`](https://github.com/OthmanAdi/planning-with-files/tree/legacy-rules-support) branch.\n\n## Documentation\n\n| Document | Description |\n|----------|-------------|\n| [Installation Guide](docs/installation.md) | All installation methods (plugin, manual, Cursor, Windows) |\n| [Quick Start](docs/quickstart.md) | 5-step guide to using the pattern |\n| [Workflow Diagram](docs/workflow.md) | Visual diagram of how files and hooks interact |\n| [Troubleshooting](docs/troubleshooting.md) | Common issues and solutions |\n| [Gemini CLI Setup](docs/gemini.md) | Google Gemini CLI integration guide |\n| [Moltbot Setup](docs/moltbot.md) | Moltbot integration guide |\n| [Kiro Setup](docs/kiro.md) | Kiro steering files integration |\n| [Cursor Setup](docs/c",
      "default_branch": "master"
    },
    "fetched_at": "2026-02-01T03:44:49.486548"
  },
  {
    "basic_info": {
      "name": "maptoposter",
      "full_name": "originalankur/maptoposter",
      "owner": "originalankur",
      "description": "Transform your favorite cities into beautiful, minimalist designs. MapToPoster lets you create and export visually striking map posters with code.",
      "url": "https://github.com/originalankur/maptoposter",
      "clone_url": "https://github.com/originalankur/maptoposter.git",
      "ssh_url": "git@github.com:originalankur/maptoposter.git",
      "homepage": "",
      "created_at": "2026-01-08T12:29:35Z",
      "updated_at": "2026-02-01T03:15:47Z",
      "pushed_at": "2026-01-30T14:46:20Z"
    },
    "stats": {
      "stars": 8715,
      "forks": 786,
      "watchers": 8715,
      "open_issues": 12,
      "size": 319891
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 39665,
        "Shell": 3464
      },
      "license": "MIT License",
      "topics": [
        "design",
        "maps",
        "matplotlib-pyplot",
        "openstreetmap",
        "osmnx",
        "poster",
        "python",
        "visualdesign"
      ]
    },
    "content": {
      "readme": "# City Map Poster Generator\n\nGenerate beautiful, minimalist map posters for any city in the world.\n\n<img src=\"posters/singapore_neon_cyberpunk_20260118_153328.png\" width=\"250\">\n<img src=\"posters/dubai_midnight_blue_20260118_140807.png\" width=\"250\">\n\n## Examples\n\n| Country      | City           | Theme           | Poster |\n|:------------:|:--------------:|:---------------:|:------:|\n| USA          | San Francisco  | sunset          | <img src=\"posters/san_francisco_sunset_20260118_144726.png\" width=\"250\"> |\n| Spain        | Barcelona      | warm_beige      | <img src=\"posters/barcelona_warm_beige_20260118_140048.png\" width=\"250\"> |\n| Italy        | Venice         | blueprint       | <img src=\"posters/venice_blueprint_20260118_140505.png\" width=\"250\"> |\n| Japan        | Tokyo          | japanese_ink    | <img src=\"posters/tokyo_japanese_ink_20260118_142446.png\" width=\"250\"> |\n| India        | Mumbai         | contrast_zones  | <img src=\"posters/mumbai_contrast_zones_20260118_145843.png\" width=\"250\"> |\n| Morocco      | Marrakech      | terracotta      | <img src=\"posters/marrakech_terracotta_20260118_143253.png\" width=\"250\"> |\n| Singapore    | Singapore      | neon_cyberpunk  | <img src=\"posters/singapore_neon_cyberpunk_20260118_153328.png\" width=\"250\"> |\n| Australia    | Melbourne      | forest          | <img src=\"posters/melbourne_forest_20260118_153446.png\" width=\"250\"> |\n| UAE          | Dubai          | midnight_blue   | <img src=\"posters/dubai_midnight_blue_20260118_140807.png\" width=\"250\"> |\n| USA          | Seattle        | emerald         | <img src=\"posters/seattle_emerald_20260124_162244.png\" width=\"250\"> |\n\n## Installation\n\n### With uv (Recommended)\n\nMake sure [uv](https://docs.astral.sh/uv/) is installed. Running the script by prepending `uv run` automatically creates and manages a virtual environment.\n\n```bash\n# First run will automatically install dependencies\nuv run ./create_map_poster.py --city \"Paris\" --country \"France\"\n\n# Or sync dependencies explicitly first (using locked versions)\nuv sync --locked\nuv run ./create_map_poster.py --city \"Paris\" --country \"France\"\n```\n\n### With pip + venv\n\n```bash\npython -m venv .venv\nsource .venv/bin/activate  # On Windows: .venv\\Scripts\\activate\npip install -r requirements.txt\n```\n\n## Usage\n\n### Generate Poster\n\nIf you're using `uv`:\n\n```bash\nuv run ./create_map_poster.py --city <city> --country <country> [options]\n```\n\nOtherwise (pip + venv):\n\n```bash\npython create_map_poster.py --city <city> --country <country> [options]\n```\n\n### Required Options\n\n| Option | Short | Description |\n|--------|-------|-------------|\n| `--city` | `-c` | City name (used for geocoding) |\n| `--country` | `-C` | Country name (used for geocoding) |\n\n### Optional Flags\n\n| Option | Short | Description | Default |\n|--------|-------|-------------|---------|\n| **OPTIONAL:** `--latitude` | `-lat` | Override latitude center point (use with --longitude) | |\n| **OPTIONAL:** `--longitude` | `-long` | Override longitude center point (use with --latitude) | |\n| **OPTIONAL:** `--country-label` | | Override country text displayed on poster | |\n| **OPTIONAL:** `--theme` | `-t` | Theme name | terracotta |\n| **OPTIONAL:** `--distance` | `-d` | Map radius in meters | 18000 |\n| **OPTIONAL:** `--list-themes` | | List all available themes | |\n| **OPTIONAL:** `--all-themes` | | Generate posters for all available themes | |\n| **OPTIONAL:** `--width` | `-W` | Image width in inches | 12 (max: 20) |\n| **OPTIONAL:** `--height` | `-H` | Image height in inches | 16 (max: 20) |\n\n### Multilingual Support - i18n\n\nDisplay city and country names in your language with custom fonts from google fonts:\n\n| Option | Short | Description |\n|--------|-------|-------------|\n| `--display-city` | `-dc` | Custom display name for city (e.g., \"æ±äº¬\") |\n| `--display-country` | `-dC` | Custom display name for country (e.g., \"æ—¥æœ¬\") |\n| `--font-family` | | Google Fonts family name (e.g., \"Noto Sans JP\") |\n\n**Examples:**\n\n```bash\n# Japanese\npython create_map_poster.py -c \"Tokyo\" -C \"Japan\" -dc \"æ±äº¬\" -dC \"æ—¥æœ¬\" --font-family \"Noto Sans JP\"\n\n# Korean\npython create_map_poster.py -c \"Seoul\" -C \"South Korea\" -dc \"ì„œìš¸\" -dC \"ëŒ€í•œë¯¼êµ­\" --font-family \"Noto Sans KR\"\n\n# Arabic\npython create_map_poster.py -c \"Dubai\" -C \"UAE\" -dc \"Ø¯Ø¨ÙŠ\" -dC \"Ø§Ù„Ø¥Ù…Ø§Ø±Ø§Øª\" --font-family \"Cairo\"\n```\n\n**Note**: Fonts are automatically downloaded from Google Fonts and cached locally in `fonts/cache/`.\n\n### Resolution Guide (300 DPI)\n\nUse these values for `-W` and `-H` to target specific resolutions:\n\n| Target | Resolution (px) | Inches (-W / -H) |\n|--------|-----------------|------------------|\n| **Instagram Post** | 1080 x 1080 | 3.6 x 3.6 |\n| **Mobile Wallpaper** | 1080 x 1920 | 3.6 x 6.4 |\n| **HD Wallpaper** | 1920 x 1080 | 6.4 x 3.6 |\n| **4K Wallpaper** | 3840 x 2160 | 12.8 x 7.2 |\n| **A4 Print** | 2480 x 3508 | 8.3 x 11.7 |\n\n### Usage Examples\n\n#### Basic Examples\n\n```bash\n# Simple usage with default theme\npython create_map_poster.py -c \"Paris\" -C \"France\"\n\n# With custom theme a",
      "default_branch": "main"
    },
    "fetched_at": "2026-02-01T03:44:50.628279"
  },
  {
    "basic_info": {
      "name": "daily_stock_analysis",
      "full_name": "ZhuLinsen/daily_stock_analysis",
      "owner": "ZhuLinsen",
      "description": "LLMé©±åŠ¨çš„ A/H/ç¾è‚¡æ™ºèƒ½åˆ†æå™¨ï¼Œå¤šæ•°æ®æºè¡Œæƒ… + å®æ—¶æ–°é—» + Gemini å†³ç­–ä»ªè¡¨ç›˜ + å¤šæ¸ é“æ¨é€ï¼Œé›¶æˆæœ¬ï¼Œçº¯ç™½å«–ï¼Œå®šæ—¶è¿è¡Œ",
      "url": "https://github.com/ZhuLinsen/daily_stock_analysis",
      "clone_url": "https://github.com/ZhuLinsen/daily_stock_analysis.git",
      "ssh_url": "git@github.com:ZhuLinsen/daily_stock_analysis.git",
      "homepage": "",
      "created_at": "2026-01-10T06:43:20Z",
      "updated_at": "2026-02-01T03:44:30Z",
      "pushed_at": "2026-02-01T03:17:00Z"
    },
    "stats": {
      "stars": 8351,
      "forks": 8777,
      "watchers": 8351,
      "open_issues": 13,
      "size": 42489
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 740772,
        "Shell": 10609,
        "Dockerfile": 1394
      },
      "license": "MIT License",
      "topics": [
        "agent",
        "ai",
        "aigc",
        "gemini",
        "llm",
        "quant",
        "quantitative-trading",
        "rag",
        "stock"
      ]
    },
    "content": {
      "readme": "<div align=\"center\">\n\n# ğŸ“ˆ è‚¡ç¥¨æ™ºèƒ½åˆ†æç³»ç»Ÿ\n\n[![GitHub stars](https://img.shields.io/github/stars/ZhuLinsen/daily_stock_analysis?style=social)](https://github.com/ZhuLinsen/daily_stock_analysis/stargazers)\n[![CI](https://github.com/ZhuLinsen/daily_stock_analysis/actions/workflows/ci.yml/badge.svg)](https://github.com/ZhuLinsen/daily_stock_analysis/actions/workflows/ci.yml)\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)\n[![GitHub Actions](https://img.shields.io/badge/GitHub%20Actions-Ready-2088FF?logo=github-actions&logoColor=white)](https://github.com/features/actions)\n\n> ğŸ¤– åŸºäº AI å¤§æ¨¡å‹çš„ Aè‚¡/æ¸¯è‚¡/ç¾è‚¡è‡ªé€‰è‚¡æ™ºèƒ½åˆ†æç³»ç»Ÿï¼Œæ¯æ—¥è‡ªåŠ¨åˆ†æå¹¶æ¨é€ã€Œå†³ç­–ä»ªè¡¨ç›˜ã€åˆ°ä¼ä¸šå¾®ä¿¡/é£ä¹¦/Telegram/é‚®ç®±\n\n[**åŠŸèƒ½ç‰¹æ€§**](#-åŠŸèƒ½ç‰¹æ€§) â€¢ [**éƒ¨ç½²æŒ‡å—**](docs/DEPLOY.md) â€¢ [**æ¨é€æ•ˆæœ**](#-æ¨é€æ•ˆæœ) â€¢ [**ä½¿ç”¨æŒ‡å—**](docs/full-guide.md) â€¢ [**æ›´æ–°æ—¥å¿—**](docs/CHANGELOG.md)\n\n[English](docs/README_EN.md) | ç®€ä½“ä¸­æ–‡ | [ç¹é«”ä¸­æ–‡](docs/README_CHT.md)\n\n</div>\n\n## ğŸ’– èµåŠ©å•† (Sponsors)\n<div align=\"center\">\n  <a href=\"https://serpapi.com/baidu-search-api?utm_source=github_daily_stock_analysis\" target=\"_blank\">\n    <img src=\"./sources/serpapi_banner_zh.png\" alt=\"è½»æ¾æŠ“å–æœç´¢å¼•æ“ä¸Šçš„å®æ—¶é‡‘èæ–°é—»æ•°æ® - SerpApi\" height=\"160\">\n  </a>\n</div>\n<br>\n\n\n## âœ¨ åŠŸèƒ½ç‰¹æ€§\n\n### ğŸ¯ æ ¸å¿ƒåŠŸèƒ½\n- **AI å†³ç­–ä»ªè¡¨ç›˜** - ä¸€å¥è¯æ ¸å¿ƒç»“è®º + ç²¾ç¡®ä¹°å–ç‚¹ä½ + æ£€æŸ¥æ¸…å•\n- **å¤šç»´åº¦åˆ†æ** - æŠ€æœ¯é¢ + ç­¹ç åˆ†å¸ƒ + èˆ†æƒ…æƒ…æŠ¥ + å®æ—¶è¡Œæƒ…\n- **å¤§ç›˜å¤ç›˜** - æ¯æ—¥å¸‚åœºæ¦‚è§ˆã€æ¿å—æ¶¨è·Œã€åŒ—å‘èµ„é‡‘\n- **å¤šæ¸ é“æ¨é€** - æ”¯æŒä¼ä¸šå¾®ä¿¡ã€é£ä¹¦ã€Telegramã€é‚®ä»¶ï¼ˆè‡ªåŠ¨è¯†åˆ«ï¼‰\n- **é›¶æˆæœ¬éƒ¨ç½²** - GitHub Actions å…è´¹è¿è¡Œï¼Œæ— éœ€æœåŠ¡å™¨\n- **ğŸ’° ç™½å«– Gemini API** - Google AI Studio æä¾›å…è´¹é¢åº¦ï¼Œä¸ªäººä½¿ç”¨å®Œå…¨å¤Ÿç”¨\n- **ğŸ”„ å¤šæ¨¡å‹æ”¯æŒ** - æ”¯æŒ OpenAI å…¼å®¹ APIï¼ˆDeepSeekã€é€šä¹‰åƒé—®ç­‰ï¼‰ä½œä¸ºå¤‡é€‰\n\n### ğŸ“Š æ•°æ®æ¥æº\n- **è¡Œæƒ…æ•°æ®**: AkShareï¼ˆå…è´¹ï¼‰ã€Tushareã€Pytdxï¼ˆé€šè¾¾ä¿¡ï¼‰ã€Baostockã€YFinance\n- **æ–°é—»æœç´¢**: Tavilyã€SerpAPIã€Bocha\n- **AI åˆ†æ**: \n  - ä¸»åŠ›ï¼šGoogle Geminiï¼ˆgemini-3-flash-previewï¼‰â€”â€” [å…è´¹è·å–](https://aistudio.google.com/)\n  - å¤‡é€‰ï¼šåº”å¤§å®¶è¦æ±‚ï¼Œä¹Ÿæ”¯æŒäº†OpenAI å…¼å®¹ APIï¼ˆDeepSeekã€é€šä¹‰åƒé—®ã€Moonshot ç­‰ï¼‰\n\n### ğŸ›¡ï¸ äº¤æ˜“ç†å¿µå†…ç½®\n- âŒ **ä¸¥ç¦è¿½é«˜** - ä¹–ç¦»ç‡ > 5% è‡ªåŠ¨æ ‡è®°ã€Œå±é™©ã€\n- âœ… **è¶‹åŠ¿äº¤æ˜“** - MA5 > MA10 > MA20 å¤šå¤´æ’åˆ—\n- ğŸ“ **ç²¾ç¡®ç‚¹ä½** - ä¹°å…¥ä»·ã€æ­¢æŸä»·ã€ç›®æ ‡ä»·\n- ğŸ“‹ **æ£€æŸ¥æ¸…å•** - æ¯é¡¹æ¡ä»¶ç”¨ âœ…âš ï¸âŒ æ ‡è®°\n\n## ğŸš€ å¿«é€Ÿå¼€å§‹\n\n### æ–¹å¼ä¸€ï¼šGitHub Actionsï¼ˆæ¨èï¼Œé›¶æˆæœ¬ï¼‰\n\n**æ— éœ€æœåŠ¡å™¨ï¼Œæ¯å¤©è‡ªåŠ¨è¿è¡Œï¼**\n\n#### 1. Fork æœ¬ä»“åº“(é¡ºä¾¿ç‚¹ä¸‹â­å‘€)\n\nç‚¹å‡»å³ä¸Šè§’ `Fork` æŒ‰é’®\n\n#### 2. é…ç½® Secrets\n\nè¿›å…¥ä½  Fork çš„ä»“åº“ â†’ `Settings` â†’ `Secrets and variables` â†’ `Actions` â†’ `New repository secret`\n\n**AI æ¨¡å‹é…ç½®ï¼ˆäºŒé€‰ä¸€ï¼‰**\n\n| Secret åç§° | è¯´æ˜ | å¿…å¡« |\n|------------|------|:----:|\n| `GEMINI_API_KEY` | [Google AI Studio](https://aistudio.google.com/) è·å–å…è´¹ Key | âœ…* |\n| `OPENAI_API_KEY` | OpenAI å…¼å®¹ API Keyï¼ˆæ”¯æŒ DeepSeekã€é€šä¹‰åƒé—®ç­‰ï¼‰ | å¯é€‰ |\n| `OPENAI_BASE_URL` | OpenAI å…¼å®¹ API åœ°å€ï¼ˆå¦‚ `https://api.deepseek.com/v1`ï¼‰ | å¯é€‰ |\n| `OPENAI_MODEL` | æ¨¡å‹åç§°ï¼ˆå¦‚ `deepseek-chat`ï¼‰ | å¯é€‰ |\n\n> *æ³¨ï¼š`GEMINI_API_KEY` å’Œ `OPENAI_API_KEY` è‡³å°‘é…ç½®ä¸€ä¸ª\n\n**é€šçŸ¥æ¸ é“é…ç½®ï¼ˆå¯åŒæ—¶é…ç½®å¤šä¸ªï¼Œå…¨éƒ¨æ¨é€ï¼‰**\n\n| Secret åç§° | è¯´æ˜ | å¿…å¡« |\n|------------|------|:----:|\n| `WECHAT_WEBHOOK_URL` | ä¼ä¸šå¾®ä¿¡ Webhook URL | å¯é€‰ |\n| `FEISHU_WEBHOOK_URL` | é£ä¹¦ Webhook URL | å¯é€‰ |\n| `TELEGRAM_BOT_TOKEN` | Telegram Bot Tokenï¼ˆ@BotFather è·å–ï¼‰ | å¯é€‰ |\n| `TELEGRAM_CHAT_ID` | Telegram Chat ID | å¯é€‰ |\n| `EMAIL_SENDER` | å‘ä»¶äººé‚®ç®±ï¼ˆå¦‚ `xxx@qq.com`ï¼‰ | å¯é€‰ |\n| `EMAIL_PASSWORD` | é‚®ç®±æˆæƒç ï¼ˆéç™»å½•å¯†ç ï¼‰ | å¯é€‰ |\n| `EMAIL_RECEIVERS` | æ”¶ä»¶äººé‚®ç®±ï¼ˆå¤šä¸ªç”¨é€—å·åˆ†éš”ï¼Œç•™ç©ºåˆ™å‘ç»™è‡ªå·±ï¼‰ | å¯é€‰ |\n| `PUSHPLUS_TOKEN` | PushPlus Tokenï¼ˆ[è·å–åœ°å€](https://www.pushplus.plus)ï¼Œå›½å†…æ¨é€æœåŠ¡ï¼‰ | å¯é€‰ |\n| `CUSTOM_WEBHOOK_URLS` | è‡ªå®šä¹‰ Webhookï¼ˆæ”¯æŒé’‰é’‰ç­‰ï¼Œå¤šä¸ªç”¨é€—å·åˆ†éš”ï¼‰ | å¯é€‰ |\n| `CUSTOM_WEBHOOK_BEARER_TOKEN` | è‡ªå®šä¹‰ Webhook çš„ Bearer Tokenï¼ˆç”¨äºéœ€è¦è®¤è¯çš„ Webhookï¼‰ | å¯é€‰ |\n| `SINGLE_STOCK_NOTIFY` | å•è‚¡æ¨é€æ¨¡å¼ï¼šè®¾ä¸º `true` åˆ™æ¯åˆ†æå®Œä¸€åªè‚¡ç¥¨ç«‹å³æ¨é€ | å¯é€‰ |\n| `REPORT_TYPE` | æŠ¥å‘Šç±»å‹ï¼š`simple`(ç²¾ç®€) æˆ– `full`(å®Œæ•´)ï¼ŒDockerç¯å¢ƒæ¨èè®¾ä¸º `full` | å¯é€‰ |\n| `ANALYSIS_DELAY` | ä¸ªè‚¡åˆ†æå’Œå¤§ç›˜åˆ†æä¹‹é—´çš„å»¶è¿Ÿï¼ˆç§’ï¼‰ï¼Œé¿å…APIé™æµï¼Œå¦‚ `10` | å¯é€‰ |\n\n> *æ³¨ï¼šè‡³å°‘é…ç½®ä¸€ä¸ªæ¸ é“ï¼Œé…ç½®å¤šä¸ªåˆ™åŒæ—¶æ¨é€\n>\n> ğŸ“– æ›´å¤šé…ç½®ï¼ˆPushover æ‰‹æœºæ¨é€ã€é£ä¹¦äº‘æ–‡æ¡£ç­‰ï¼‰è¯·å‚è€ƒ [å®Œæ•´é…ç½®æŒ‡å—](docs/full-guide.md)\n\n**å…¶ä»–é…ç½®**\n\n| Secret åç§° | è¯´æ˜ | å¿…å¡« |\n|------------|------|:----:|\n| `STOCK_LIST` | è‡ªé€‰è‚¡ä»£ç ï¼Œå¦‚ `600519,hk00700,AAPL,TSLA` | âœ… |\n| `TAVILY_API_KEYS` | [Tavily](https://tavily.com/) æœç´¢ APIï¼ˆæ–°é—»æœç´¢ï¼‰ | æ¨è |\n| `BOCHA_API_KEYS` | [åšæŸ¥æœç´¢](https://open.bocha.cn/) Web Search APIï¼ˆä¸­æ–‡æœç´¢ä¼˜åŒ–ï¼Œæ”¯æŒAIæ‘˜è¦ï¼Œå¤šä¸ªkeyç”¨é€—å·åˆ†éš”ï¼‰ | å¯é€‰ |\n| `SERPAPI_API_KEYS` | [SerpAPI](https://serpapi.com/baidu-search-api?utm_source=github_daily_stock_analysis) å¤‡ç”¨æœç´¢ | å¯é€‰ |\n| `TUSHARE_TOKEN` | [Tushare Pro](https://tushare.pro/) Token | å¯é€‰ |\n| `WECHAT_MSG_TYPE` | ä¼å¾®æ¶ˆæ¯ç±»å‹ï¼Œé»˜è®¤ markdownï¼Œæ”¯æŒé…ç½® text ç±»å‹ï¼Œå‘é€çº¯ markdown æ–‡æœ¬ | å¯é€‰ |\n\n#### 3. å¯ç”¨ Actions\n\nè¿›å…¥ `Actions` æ ‡ç­¾ â†’ ç‚¹å‡» `I understand my workflows, go ahead and enable them`\n\n#### 4. æ‰‹åŠ¨æµ‹è¯•\n\n`Actions` â†’ `æ¯æ—¥è‚¡ç¥¨åˆ†æ` â†’ `Run workflow` â†’ é€‰æ‹©æ¨¡å¼ â†’ `Run workflow`\n\n#### 5. å®Œæˆï¼\n\né»˜è®¤æ¯ä¸ªå·¥ä½œæ—¥ **18:00ï¼ˆåŒ—äº¬æ—¶é—´ï¼‰** è‡ªåŠ¨æ‰§è¡Œ\n\n### æ–¹å¼äºŒï¼šæœ¬åœ°è¿è¡Œ / Docker éƒ¨ç½²\n\n> ğŸ“– æœ¬åœ°è¿è¡Œã€Docker éƒ¨ç½²è¯¦ç»†æ­¥éª¤è¯·å‚è€ƒ [å®Œæ•´é…ç½®æŒ‡å—](docs/full-guide.md)\n\n## ğŸ“± æ¨é€æ•ˆæœ\n\n![è¿è¡Œæ•ˆæœæ¼”ç¤º](./sources/all_2026-01-13_221547.gif)\n\n### å†³ç­–ä»ªè¡¨ç›˜\n```\nğŸ“Š 2026-01-10 å†³ç­–ä»ªè¡¨ç›˜\n3åªè‚¡ç¥¨ | ğŸŸ¢ä¹°å…¥:1 ğŸŸ¡è§‚æœ›:2 ğŸ”´å–å‡º:0\n\nğŸŸ¢ ä¹°å…¥ | è´µå·èŒ…å°(600519)\nğŸ“Œ ç¼©é‡å›è¸©MA5æ”¯æ’‘ï¼Œä¹–ç¦»ç‡1.2%å¤„äºæœ€ä½³ä¹°ç‚¹\nğŸ’° ç‹™å‡»: ä¹°å…¥1800 | æ­¢æŸ1750 | ç›®æ ‡1900\nâœ…å¤šå¤´æ’åˆ— âœ…ä¹–ç¦»å®‰å…¨ âœ…é‡èƒ½é…åˆ\n\nğŸŸ¡ è§‚æœ› | å®å¾·æ—¶ä»£(300750)\nğŸ“Œ ä¹–ç¦»ç‡7.8%è¶…è¿‡5%è­¦æˆ’çº¿ï¼Œä¸¥ç¦è¿½é«˜\nâš ï¸ ç­‰å¾…å›è°ƒè‡³MA5é™„è¿‘å†è€ƒè™‘\n\n---\nç”Ÿæˆæ—¶é—´: 18:00\n```\n\n### å¤§ç›˜å¤ç›˜\n\n![å¤§ç›˜å¤ç›˜æ¨é€æ•ˆæœ](./sources/dapan_2026-01-13_22-14-52.png)\n\n```\nğŸ¯ 2026-01-10 å¤§ç›˜å¤ç›˜\n\nğŸ“Š ä¸»è¦æŒ‡æ•°\n- ä¸Šè¯æŒ‡æ•°: 3250.12 (ğŸŸ¢+0.85%)\n- æ·±è¯æˆæŒ‡: 10521.36 (ğŸŸ¢+1.02%)\n- åˆ›ä¸šæ¿æŒ‡: 2156.78 (ğŸŸ¢+1.35%)\n\nğŸ“ˆ å¸‚åœºæ¦‚å†µ\nä¸Šæ¶¨: 3920 | ä¸‹è·Œ: 1349 | æ¶¨åœ: 155 | è·Œåœ: 3\n\nğŸ”¥ æ¿å—è¡¨ç°\né¢†æ¶¨: äº’è”ç½‘æœåŠ¡ã€æ–‡åŒ–ä¼ åª’ã€å°é‡‘å±\né¢†è·Œ: ä¿é™©ã€èˆªç©ºæœºåœºã€å…‰ä¼è®¾å¤‡\n```\n\n## âš™",
      "default_branch": "main"
    },
    "fetched_at": "2026-02-01T03:44:51.795214"
  },
  {
    "basic_info": {
      "name": "Qwen3-TTS",
      "full_name": "QwenLM/Qwen3-TTS",
      "owner": "QwenLM",
      "description": "Qwen3-TTS is an open-source series of TTS models developed by the Qwen team at Alibaba Cloud, supporting stable, expressive, and streaming speech generation, free-form voice design, and vivid voice cloning.",
      "url": "https://github.com/QwenLM/Qwen3-TTS",
      "clone_url": "https://github.com/QwenLM/Qwen3-TTS.git",
      "ssh_url": "git@github.com:QwenLM/Qwen3-TTS.git",
      "homepage": null,
      "created_at": "2026-01-21T06:41:32Z",
      "updated_at": "2026-02-01T03:11:45Z",
      "pushed_at": "2026-01-25T16:11:08Z"
    },
    "stats": {
      "stars": 6347,
      "forks": 743,
      "watchers": 6347,
      "open_issues": 36,
      "size": 6938
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 402665
      },
      "license": "Apache License 2.0",
      "topics": []
    },
    "content": {
      "readme": "# Qwen3-TTS\n\n<br>\n\n<p align=\"center\">\n    <img src=\"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-TTS-Repo/qwen3_tts_logo.png\" width=\"400\"/>\n<p>\n\n<p align=\"center\">\n&nbsp&nbspğŸ¤— <a href=\"https://huggingface.co/collections/Qwen/qwen3-tts\">Hugging Face</a>&nbsp&nbsp | &nbsp&nbspğŸ¤– <a href=\"https://modelscope.cn/collections/Qwen/Qwen3-TTS\">ModelScope</a>&nbsp&nbsp | &nbsp&nbspğŸ“‘ <a href=\"https://qwen.ai/blog?id=qwen3tts-0115\">Blog</a>&nbsp&nbsp | &nbsp&nbspğŸ“‘ <a href=\"https://arxiv.org/abs/2601.15621\">Paper</a>&nbsp&nbsp\n<br>\nğŸ–¥ï¸ <a href=\"https://huggingface.co/spaces/Qwen/Qwen3-TTS\">Hugging Face Demo</a>&nbsp&nbsp | &nbsp&nbsp ğŸ–¥ï¸ <a href=\"https://modelscope.cn/studios/Qwen/Qwen3-TTS\">ModelScope Demo</a>&nbsp&nbsp | &nbsp&nbspğŸ’¬ <a href=\"https://github.com/QwenLM/Qwen/blob/main/assets/wechat.png\">WeChat (å¾®ä¿¡)</a>&nbsp&nbsp | &nbsp&nbspğŸ«¨ <a href=\"https://discord.gg/CV4E9rpNSD\">Discord</a>&nbsp&nbsp | &nbsp&nbspğŸ“‘ <a href=\"https://help.aliyun.com/zh/model-studio/qwen-tts-realtime\">API</a>\n\n</p>\n\nWe release **Qwen3-TTS**, a series of powerful speech generation capabilities developed by Qwen, offering comprehensive support for voice clone, voice design, ultra-high-quality human-like speech generation, and natural language-based voice control. It provides developers and users with the most extensive set of speech generation features available.\n\n\n## News\n* 2026.1.22: ğŸ‰ğŸ‰ğŸ‰ We have released [Qwen3-TTS](https://huggingface.co/collections/Qwen/qwen3-tts) series (0.6B/1.7B) based on Qwen3-TTS-Tokenizer-12Hz. Please check our [blog](https://qwen.ai/blog?id=qwen3tts-0115)!\n\n## Contents <!-- omit in toc -->\n\n- [Overview](#overview)\n  - [Introduction](#introduction)\n  - [Model Architecture](#model-architecture)\n  - [Released Models Description and Download](#released-models-description-and-download)\n- [Quickstart](#quickstart)\n  - [Environment Setup](#environment-setup)\n  - [Python Package Usage](#python-package-usage)\n    - [Custom Voice Generation](#custom-voice-generate)\n    - [Voice Design](#voice-design)\n    - [Voice Clone](#voice-clone)\n    - [Voice Design then Clone](#voice-design-then-clone)\n    - [Tokenizer Encode and Decode](#tokenizer-encode-and-decode)\n  - [Launch Local Web UI Demo](#launch-local-web-ui-demo)\n  - [DashScope API Usage](#dashscope-api-usage)\n- [vLLM Usage](#vllm-usage)\n- [Fine Tuning](#fine-tuning)\n- [Evaluation](#evaluation)\n- [Citation](#citation)\n\n## Overview\n### Introduction\n\n<p align=\"center\">\n    <img src=\"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-TTS-Repo/qwen3_tts_introduction.png\" width=\"90%\"/>\n<p>\n\nQwen3-TTS covers 10 major languages (Chinese, English, Japanese, Korean, German, French, Russian, Portuguese, Spanish, and Italian) as well as multiple dialectal voice profiles to meet global application needs. In addition, the models feature strong contextual understanding, enabling adaptive control of tone, speaking rate, and emotional expression based on instructions and text semantics, and they show markedly improved robustness to noisy input text. Key features:\n\n* **Powerful Speech Representation**: Powered by the self-developed Qwen3-TTS-Tokenizer-12Hz, it achieves efficient acoustic compression and high-dimensional semantic modeling of speech signals. It fully preserves paralinguistic information and acoustic environmental features, enabling high-speed, high-fidelity speech reconstruction through a lightweight non-DiT architecture.\n* **Universal End-to-End Architecture**: Utilizing a discrete multi-codebook LM architecture, it realizes full-information end-to-end speech modeling. This completely bypasses the information bottlenecks and cascading errors inherent in traditional LM+DiT schemes, significantly enhancing the modelâ€™s versatility, generation efficiency, and performance ceiling.\n* **Extreme Low-Latency Streaming Generation**: Based on the innovative Dual-Track hybrid streaming generation architecture, a single model supports both streaming and non-streaming generation. It can output the first audio packet immediately after a single character is input, with end-to-end synthesis latency as low as 97ms, meeting the rigorous demands of real-time interactive scenarios.\n* **Intelligent Text Understanding and Voice Control**: Supports speech generation driven by natural language instructions, allowing for flexible control over multi-dimensional acoustic attributes such as timbre, emotion, and prosody. By deeply integrating text semantic understanding, the model adaptively adjusts tone, rhythm, and emotional expression, achieving lifelike â€œwhat you imagine is what you hearâ€ output.\n\n\n### Model Architecture\n\n<p align=\"center\">\n    <img src=\"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-TTS-Repo/overview.png\" width=\"80%\"/>\n<p>\n\n### Released Models Description and Download\n\nBelow is an introduction and download information for the Qwen3-TTS models that have already been released. Other models mentioned in the technical report will be released in the near future. Please se",
      "default_branch": "main"
    },
    "fetched_at": "2026-02-01T03:44:52.986503"
  },
  {
    "basic_info": {
      "name": "antigravity-awesome-skills",
      "full_name": "sickn33/antigravity-awesome-skills",
      "owner": "sickn33",
      "description": "The Ultimate Collection of 500+ Agentic Skills for Claude Code/Antigravity/Cursor. Battle-tested, high-performance skills for AI agents including official skills from Anthropic and Vercel.",
      "url": "https://github.com/sickn33/antigravity-awesome-skills",
      "clone_url": "https://github.com/sickn33/antigravity-awesome-skills.git",
      "ssh_url": "git@github.com:sickn33/antigravity-awesome-skills.git",
      "homepage": "https://github.com/sickn33/antigravity-awesome-skills",
      "created_at": "2026-01-14T17:48:09Z",
      "updated_at": "2026-02-01T03:36:11Z",
      "pushed_at": "2026-01-31T22:04:25Z"
    },
    "stats": {
      "stars": 5899,
      "forks": 1305,
      "watchers": 5899,
      "open_issues": 2,
      "size": 23375
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 1361667,
        "Shell": 270948,
        "JavaScript": 111667,
        "TypeScript": 38918,
        "HTML": 36813,
        "C#": 29255,
        "CSS": 6844,
        "C++": 2607
      },
      "license": "MIT License",
      "topics": [
        "agentic-skills",
        "ai-agents",
        "antigravity",
        "autonomous-coding",
        "claude-code",
        "mcp",
        "react-patterns",
        "security-auditing"
      ]
    },
    "content": {
      "readme": "# ğŸŒŒ Antigravity Awesome Skills: 625+ Agentic Skills for Claude Code, Gemini CLI, Cursor, Copilot & More\n\n> **The Ultimate Collection of 625+ Universal Agentic Skills for AI Coding Assistants â€” Claude Code, Gemini CLI, Codex CLI, Antigravity IDE, GitHub Copilot, Cursor, OpenCode**\n\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n[![Claude Code](https://img.shields.io/badge/Claude%20Code-Anthropic-purple)](https://claude.ai)\n[![Gemini CLI](https://img.shields.io/badge/Gemini%20CLI-Google-blue)](https://github.com/google-gemini/gemini-cli)\n[![Codex CLI](https://img.shields.io/badge/Codex%20CLI-OpenAI-green)](https://github.com/openai/codex)\n[![Cursor](https://img.shields.io/badge/Cursor-AI%20IDE-orange)](https://cursor.sh)\n[![Copilot](https://img.shields.io/badge/GitHub%20Copilot-VSCode-lightblue)](https://github.com/features/copilot)\n[![OpenCode](https://img.shields.io/badge/OpenCode-CLI-gray)](https://github.com/opencode-ai/opencode)\n[![Antigravity](https://img.shields.io/badge/Antigravity-DeepMind-red)](https://github.com/sickn33/antigravity-awesome-skills)\n\n**Antigravity Awesome Skills** is a curated, battle-tested library of **624 high-performance agentic skills** designed to work seamlessly across all major AI coding assistants:\n\n- ğŸŸ£ **Claude Code** (Anthropic CLI)\n- ğŸ”µ **Gemini CLI** (Google DeepMind)\n- ğŸŸ¢ **Codex CLI** (OpenAI)\n- ğŸ”´ **Antigravity IDE** (Google DeepMind)\n- ğŸ©µ **GitHub Copilot** (VSCode Extension)\n- ğŸŸ  **Cursor** (AI-native IDE)\n- âšª **OpenCode** (Open-source CLI)\n\nThis repository provides essential skills to transform your AI assistant into a **full-stack digital agency**, including official capabilities from **Anthropic**, **OpenAI**, **Google**, **Supabase**, and **Vercel Labs**.\n\n## Table of Contents\n\n- [ğŸš€ New Here? Start Here!](#new-here-start-here)\n- [ğŸ”Œ Compatibility & Invocation](#compatibility--invocation)\n- [ğŸ“¦ Features & Categories](#features--categories)\n- [ğŸ Curated Collections (Bundles)](#curated-collections)\n- [ğŸ“š Browse 625+ Skills](#browse-625-skills)\n- [ğŸ› ï¸ Installation](#installation)\n- [ğŸ¤ How to Contribute](#how-to-contribute)\n- [ğŸ‘¥ Contributors & Credits](#credits--sources)\n- [âš–ï¸ License](#license)\n- [ğŸ‘¥ Repo Contributors](#repo-contributors)\n- [ğŸŒŸ Star History](#star-history)\n\n---\n\n## New Here? Start Here!\n\n**Welcome to the V4.0.0 Enterprise Edition.** This isn't just a list of scripts; it's a complete operating system for your AI Agent.\n\n### 1. ğŸ£ Context: What is this?\n\n**Antigravity Awesome Skills** (Release 4.0.0) is a massive upgrade to your AI's capabilities.\n\nAI Agents (like Claude Code, Cursor, or Gemini) are smart, but they lack **specific tools**. They don't know your company's \"Deployment Protocol\" or the specific syntax for \"AWS CloudFormation\".\n**Skills** are small markdown files that teach them how to do these specific tasks perfectly, every time.\n\n### 2. âš¡ï¸ Quick Start (The \"Bundle\" Way)\n\nInstall once (clone or npx); then use our **Starter Packs** in [docs/BUNDLES.md](docs/BUNDLES.md) to see which skills fit your role. You get the full repo; Starter Packs are curated lists, not a separate install.\n\n1.  **Install** (pick one):\n    ```bash\n    # Easiest: npx installer (clones to ~/.agent/skills by default)\n    npx antigravity-awesome-skills\n\n    # Or clone manually\n    git clone https://github.com/sickn33/antigravity-awesome-skills.git .agent/skills\n    ```\n2.  **Pick your persona** (See [docs/BUNDLES.md](docs/BUNDLES.md)):\n    - **Web Dev?** use the `Web Wizard` pack.\n    - **Hacker?** use the `Security Engineer` pack.\n    - **Just curious?** start with `Essentials`.\n\n### 3. ğŸ§  How to use\n\nOnce installed, just ask your agent naturally:\n\n> \"Use the **@brainstorming** skill to help me plan a SaaS.\"\n> \"Run **@lint-and-validate** on this file.\"\n\nğŸ‘‰ **[Read the Full Getting Started Guide](docs/GETTING_STARTED.md)**\n\n---\n\n## Compatibility & Invocation\n\nThese skills follow the universal **SKILL.md** format and work with any AI coding assistant that supports agentic skills.\n\n| Tool            | Type | Invocation Example                | Path              |\n| :-------------- | :--- | :-------------------------------- | :---------------- |\n| **Claude Code** | CLI  | `>> /skill-name help me...`       | `.claude/skills/` |\n| **Gemini CLI**  | CLI  | `(User Prompt) Use skill-name...` | `.gemini/skills/` |\n| **Antigravity** | IDE  | `(Agent Mode) Use skill...`       | `.agent/skills/`  |\n| **Cursor**      | IDE  | `@skill-name (in Chat)`           | `.cursor/skills/` |\n| **Copilot**     | Ext  | `(Paste content manually)`        | N/A               |\n\n> [!TIP]\n> **Universal Path**: We recommend cloning to `.agent/skills/`. Most modern tools (Antigravity, recent CLIs) look here by default.\n\n> [!WARNING]\n> **Windows Users**: This repository uses **symlinks** for official skills.\n> The **npx** installer sets `core.symlinks=true` automatically. For **git clone**, enable Developer Mode or run Git as Administrator:\n> `git clone ",
      "default_branch": "main"
    },
    "fetched_at": "2026-02-01T03:44:54.109801"
  },
  {
    "basic_info": {
      "name": "personaplex",
      "full_name": "NVIDIA/personaplex",
      "owner": "NVIDIA",
      "description": "PersonaPlex code.",
      "url": "https://github.com/NVIDIA/personaplex",
      "clone_url": "https://github.com/NVIDIA/personaplex.git",
      "ssh_url": "git@github.com:NVIDIA/personaplex.git",
      "homepage": null,
      "created_at": "2026-01-05T19:10:35Z",
      "updated_at": "2026-02-01T02:46:49Z",
      "pushed_at": "2026-01-24T22:46:37Z"
    },
    "stats": {
      "stars": 4434,
      "forks": 624,
      "watchers": 4434,
      "open_issues": 29,
      "size": 1467
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 282630,
        "TypeScript": 91359,
        "CSS": 1981,
        "Dockerfile": 965,
        "HTML": 542,
        "JavaScript": 343
      },
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "# PersonaPlex: Voice and Role Control for Full Duplex Conversational Speech Models\n\n[![Weights](https://img.shields.io/badge/ğŸ¤—-Weights-yellow)](https://huggingface.co/nvidia/personaplex-7b-v1)\n[![Paper](https://img.shields.io/badge/ğŸ“„-Paper-blue)](https://research.nvidia.com/labs/adlr/files/personaplex/personaplex_preprint.pdf)\n[![Demo](https://img.shields.io/badge/ğŸ®-Demo-green)](https://research.nvidia.com/labs/adlr/personaplex/)\n[![Discord](https://img.shields.io/badge/Discord-Join-purple?logo=discord)](https://discord.gg/5jAXrrbwRb)\n\nPersonaPlex is a real-time, full-duplex speech-to-speech conversational model that enables persona control through text-based role prompts and audio-based voice conditioning. Trained on a combination of synthetic and real conversations, it produces natural, low-latency spoken interactions with a consistent persona. PersonaPlex is based on the [Moshi](https://arxiv.org/abs/2410.00037) architecture and weights.\n\n<p align=\"center\">\n  <img src=\"assets/architecture_diagram.png\" alt=\"PersonaPlex Model Architecture\">\n  <br>\n  <em>PersonaPlex Architecture</em>\n</p>\n\n## Usage\n\n### Prerequisites\n\nInstall the [Opus audio codec](https://github.com/xiph/opus) development library:\n```bash\n# Ubuntu/Debian\nsudo apt install libopus-dev\n\n# Fedora/RHEL\nsudo dnf install opus-devel\n\n# macOS\nbrew install opus\n```\n\n### Installation\n\nDownload this repository and install with:\n```bash\npip install moshi/.\n```\n\nExtra step for Blackwell based GPUs as suggested in (See https://github.com/NVIDIA/personaplex/issues/2):\n```bash\npip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu130\n```\n\n\n### Accept Model License\nLog in to your Huggingface account and accept the PersonaPlex model license [here](https://huggingface.co/nvidia/personaplex-7b-v1). <br>\nThen set up your Huggingface authentication:\n```bash\nexport HF_TOKEN=<YOUR_HUGGINGFACE_TOKEN>\n```\n\n### Launch Server\n\nLaunch server for live interaction (temporary SSL certs for https):\n```bash\nSSL_DIR=$(mktemp -d); python -m moshi.server --ssl \"$SSL_DIR\"\n```\n\n**CPU Offload:** If your GPU has insufficient memory, use the `--cpu-offload` flag to offload model layers to CPU. This requires the `accelerate` package (`pip install accelerate`):\n```bash\nSSL_DIR=$(mktemp -d); python -m moshi.server --ssl \"$SSL_DIR\" --cpu-offload\n```\n\nAccess the Web UI from a browser at `localhost:8998` if running locally, otherwise look for the access link printed by the script:\n```\nAccess the Web UI directly at https://11.54.401.33:8998\n```\n\n### Offline Evaluation\n\nFor offline evaluation use the offline script that streams in an input wav file and produces an output wav file from the captured output stream. The output file will be the same duration as the input file.\n\nAdd `--cpu-offload` to any command below if your GPU has insufficient memory (requires `accelerate` package). Or install cpu-only PyTorch for offline evaluation on pure CPU.\n\n**Assistant example:**\n```bash\nHF_TOKEN=<TOKEN> \\\npython -m moshi.offline \\\n  --voice-prompt \"NATF2.pt\" \\\n  --input-wav \"assets/test/input_assistant.wav\" \\\n  --seed 42424242 \\\n  --output-wav \"output.wav\" \\\n  --output-text \"output.json\"\n```\n\n**Service example:**\n```bash\nHF_TOKEN=<TOKEN> \\\npython -m moshi.offline \\\n  --voice-prompt \"NATM1.pt\" \\\n  --text-prompt \"$(cat assets/test/prompt_service.txt)\" \\\n  --input-wav \"assets/test/input_service.wav\" \\\n  --seed 42424242 \\\n  --output-wav \"output.wav\" \\\n  --output-text \"output.json\"\n```\n\n## Voices\n\nPersonaPlex supports a wide range of voices; we pre-package embeddings for voices that sound more natural and conversational (NAT) and others that are more varied (VAR). The fixed set of voices are labeled:\n```\nNatural(female): NATF0, NATF1, NATF2, NATF3\nNatural(male):   NATM0, NATM1, NATM2, NATM3\nVariety(female): VARF0, VARF1, VARF2, VARF3, VARF4\nVariety(male):   VARM0, VARM1, VARM2, VARM3, VARM4\n```\n\n## Prompting Guide\n\nThe model is trained on synthetic conversations for a fixed assistant role and varying customer service roles.\n\n### Assistant Role\n\nThe assistant role has the prompt:\n```\nYou are a wise and friendly teacher. Answer questions or provide advice in a clear and engaging way.\n```\n\nUse this prompt for the QA assistant focused \"User Interruption\" evaluation category in [FullDuplexBench](https://arxiv.org/abs/2503.04721).\n\n### Customer Service Roles\n\nThe customer service roles support a variety of prompts. Here are some examples for prompting style reference:\n```\nYou work for CitySan Services which is a waste management and your name is Ayelen Lucero. Information: Verify customer name Omar Torres. Current schedule: every other week. Upcoming pickup: April 12th. Compost bin service available for $8/month add-on.\n```\n```\nYou work for Jerusalem Shakshuka which is a restaurant and your name is Owen Foster. Information: There are two shakshuka options: Classic (poached eggs, $9.50) and Spicy (scrambled eggs with jalapenos, $10.25). Sides include warm pita ($2.50) and Israeli",
      "default_branch": "main"
    },
    "fetched_at": "2026-02-01T03:44:55.232043"
  },
  {
    "basic_info": {
      "name": "hive",
      "full_name": "adenhq/hive",
      "owner": "adenhq",
      "description": "Outcome driven agent development framework that evolves",
      "url": "https://github.com/adenhq/hive",
      "clone_url": "https://github.com/adenhq/hive.git",
      "ssh_url": "git@github.com:adenhq/hive.git",
      "homepage": "",
      "created_at": "2026-01-12T00:04:22Z",
      "updated_at": "2026-02-01T02:52:56Z",
      "pushed_at": "2026-01-31T09:59:31Z"
    },
    "stats": {
      "stars": 4260,
      "forks": 2645,
      "watchers": 4260,
      "open_issues": 944,
      "size": 2792
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 1684999,
        "Shell": 37641,
        "TypeScript": 21704,
        "Dockerfile": 1089,
        "Makefile": 733
      },
      "license": "Apache License 2.0",
      "topics": [
        "agent",
        "agent-framework",
        "agent-skills",
        "ai-evaluation",
        "anthropic",
        "automation",
        "autonomous-agents",
        "awesome",
        "claude",
        "claude-code",
        "human-in-the-loop",
        "observability-ai",
        "openai",
        "python",
        "self-hosted",
        "self-improving",
        "self-improving-agent",
        "self-improving-ai"
      ]
    },
    "content": {
      "readme": "<p align=\"center\">\n  <img width=\"100%\" alt=\"Hive Banner\" src=\"https://storage.googleapis.com/aden-prod-assets/website/aden-title-card.png\" />\n</p>\n\n<p align=\"center\">\n  <a href=\"README.md\">English</a> |\n  <a href=\"docs/i18n/zh-CN.md\">ç®€ä½“ä¸­æ–‡</a> |\n  <a href=\"docs/i18n/es.md\">EspaÃ±ol</a> |\n  <a href=\"docs/i18n/hi.md\">à¤¹à¤¿à¤¨à¥à¤¦à¥€</a> |\n  <a href=\"docs/i18n/pt.md\">PortuguÃªs</a> |\n  <a href=\"docs/i18n/ja.md\">æ—¥æœ¬èª</a> |\n  <a href=\"docs/i18n/ru.md\">Ğ ÑƒÑÑĞºĞ¸Ğ¹</a> |\n  <a href=\"docs/i18n/ko.md\">í•œêµ­ì–´</a>\n</p>\n\n[![Apache 2.0 License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://github.com/adenhq/hive/blob/main/LICENSE)\n[![Y Combinator](https://img.shields.io/badge/Y%20Combinator-Aden-orange)](https://www.ycombinator.com/companies/aden)\n[![Docker Pulls](https://img.shields.io/docker/pulls/adenhq/hive?logo=Docker&labelColor=%23528bff)](https://hub.docker.com/u/adenhq)\n[![Discord](https://img.shields.io/discord/1172610340073242735?logo=discord&labelColor=%235462eb&logoColor=%23f5f5f5&color=%235462eb)](https://discord.com/invite/MXE49hrKDk)\n[![Twitter Follow](https://img.shields.io/twitter/follow/teamaden?logo=X&color=%23f5f5f5)](https://x.com/aden_hq)\n[![LinkedIn](https://custom-icon-badges.demolab.com/badge/LinkedIn-0A66C2?logo=linkedin-white&logoColor=fff)](https://www.linkedin.com/company/teamaden/)\n\n<p align=\"center\">\n  <img src=\"https://img.shields.io/badge/AI_Agents-Self--Improving-brightgreen?style=flat-square\" alt=\"AI Agents\" />\n  <img src=\"https://img.shields.io/badge/Multi--Agent-Systems-blue?style=flat-square\" alt=\"Multi-Agent\" />\n  <img src=\"https://img.shields.io/badge/Goal--Driven-Development-purple?style=flat-square\" alt=\"Goal-Driven\" />\n  <img src=\"https://img.shields.io/badge/Human--in--the--Loop-orange?style=flat-square\" alt=\"HITL\" />\n  <img src=\"https://img.shields.io/badge/Production--Ready-red?style=flat-square\" alt=\"Production\" />\n</p>\n<p align=\"center\">\n  <img src=\"https://img.shields.io/badge/OpenAI-supported-412991?style=flat-square&logo=openai\" alt=\"OpenAI\" />\n  <img src=\"https://img.shields.io/badge/Anthropic-supported-d4a574?style=flat-square\" alt=\"Anthropic\" />\n  <img src=\"https://img.shields.io/badge/Google_Gemini-supported-4285F4?style=flat-square&logo=google\" alt=\"Gemini\" />\n  <img src=\"https://img.shields.io/badge/MCP-19_Tools-00ADD8?style=flat-square\" alt=\"MCP\" />\n</p>\n\n## Overview\n\nBuild reliable, self-improving AI agents without hardcoding workflows. Define your goal through conversation with a coding agent, and the framework generates a node graph with dynamically created connection code. When things break, the framework captures failure data, evolves the agent through the coding agent, and redeploys. Built-in human-in-the-loop nodes, credential management, and real-time monitoring give you control without sacrificing adaptability.\n\nVisit [adenhq.com](https://adenhq.com) for complete documentation, examples, and guides.\n\n## What is Aden\n\n<p align=\"center\">\n  <img width=\"100%\" alt=\"Aden Architecture\" src=\"docs/assets/aden-architecture-diagram.jpg\" />\n</p>\n\nAden is a platform for building, deploying, operating, and adapting AI agents:\n\n- **Build** - A Coding Agent generates specialized Worker Agents (Sales, Marketing, Ops) from natural language goals\n- **Deploy** - Headless deployment with CI/CD integration and full API lifecycle management\n- **Operate** - Real-time monitoring, observability, and runtime guardrails keep agents reliable\n- **Adapt** - Continuous evaluation, supervision, and adaptation ensure agents improve over time\n- **Infra** - Shared memory, LLM integrations, tools, and skills power every agent\n\n## Quick Links\n\n- **[Documentation](https://docs.adenhq.com/)** - Complete guides and API reference\n- **[Self-Hosting Guide](https://docs.adenhq.com/getting-started/quickstart)** - Deploy Hive on your infrastructure\n- **[Changelog](https://github.com/adenhq/hive/releases)** - Latest updates and releases\n<!-- - **[Roadmap](https://adenhq.com/roadmap)** - Upcoming features and plans -->\n- **[Report Issues](https://github.com/adenhq/hive/issues)** - Bug reports and feature requests\n\n## Quick Start\n\n### Prerequisites\n\n- [Python 3.11+](https://www.python.org/downloads/) for agent development\n- Claude Code or Cursor for utilizing agent skills\n\n### Installation\n\n```bash\n# Clone the repository\ngit clone https://github.com/adenhq/hive.git\ncd hive\n\n# Run quickstart setup\n./quickstart.sh\n```\n\nThis sets up:\n- **framework** - Core agent runtime and graph executor (in `core/.venv`)\n- **aden_tools** - MCP tools for agent capabilities (in `tools/.venv`)\n- All required Python dependencies\n\n### Build Your First Agent\n\n```bash\n# Build an agent using Claude Code\nclaude> /building-agents-construction\n\n# Test your agent\nclaude> /testing-agent\n\n# Run your agent\nPYTHONPATH=core:exports python -m your_agent_name run --input '{...}'\n```\n\n**[ğŸ“– Complete Setup Guide](ENVIRONMENT_SETUP.md)** - Detailed instructions for agent development\n\n### Cursor IDE Support\n\nSkills are also available in Curso",
      "default_branch": "main"
    },
    "fetched_at": "2026-02-01T03:44:56.337541"
  },
  {
    "basic_info": {
      "name": "Engram",
      "full_name": "deepseek-ai/Engram",
      "owner": "deepseek-ai",
      "description": "Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models",
      "url": "https://github.com/deepseek-ai/Engram",
      "clone_url": "https://github.com/deepseek-ai/Engram.git",
      "ssh_url": "git@github.com:deepseek-ai/Engram.git",
      "homepage": "",
      "created_at": "2026-01-12T05:26:50Z",
      "updated_at": "2026-02-01T02:21:04Z",
      "pushed_at": "2026-01-14T01:13:02Z"
    },
    "stats": {
      "stars": 3483,
      "forks": 235,
      "watchers": 3483,
      "open_issues": 12,
      "size": 2296
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 15017
      },
      "license": "Apache License 2.0",
      "topics": []
    },
    "content": {
      "readme": "<!-- markdownlint-disable first-line-h1 -->\n<!-- markdownlint-disable html -->\n<!-- markdownlint-disable no-duplicate-header -->\n\n<div align=\"center\">\n  <img src=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/logo.svg?raw=true\" width=\"60%\" alt=\"DeepSeek-V3\" />\n</div>\n<hr>\n<div align=\"center\" style=\"line-height: 1;\">\n  <a href=\"https://www.deepseek.com/\"><img alt=\"Homepage\"\n    src=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/badge.svg?raw=true\"/></a>\n  <a href=\"https://chat.deepseek.com/\"><img alt=\"Chat\"\n    src=\"https://img.shields.io/badge/ğŸ¤–%20Chat-DeepSeek%20V3-536af5?color=536af5&logoColor=white\"/></a>\n  <a href=\"https://huggingface.co/deepseek-ai\"><img alt=\"Hugging Face\"\n    src=\"https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-DeepSeek%20AI-ffc107?color=ffc107&logoColor=white\"/></a>\n  <br>\n  <a href=\"https://discord.gg/Tc7c45Zzu5\"><img alt=\"Discord\"\n    src=\"https://img.shields.io/badge/Discord-DeepSeek%20AI-7289da?logo=discord&logoColor=white&color=7289da\"/></a>\n  <a href=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/qr.jpeg?raw=true\"><img alt=\"Wechat\"\n    src=\"https://img.shields.io/badge/WeChat-DeepSeek%20AI-brightgreen?logo=wechat&logoColor=white\"/></a>\n  <a href=\"https://twitter.com/deepseek_ai\"><img alt=\"Twitter Follow\"\n    src=\"https://img.shields.io/badge/Twitter-deepseek_ai-white?logo=x&logoColor=white\"/></a>\n  <br>\n  <a href=\"LICENSE\" style=\"margin: 2px;\">\n    <img alt=\"License\" src=\"https://img.shields.io/badge/License-Apache 2.0-f5de53?&color=f5de53\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <br>\n</div>\n\n## 1. Introduction\n\nThis repository contains the official implementation for the paper: **[Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models](Engram_paper.pdf)**.\n\n> **Abstract:** While Mixture-of-Experts (MoE) scales capacity via conditional computation, Transformers lack a native primitive for knowledge lookup. To address this, we explore **conditional memory** as a complementary sparsity axis, instantiated via **Engram**, a module that modernizes classic $N$-gram embeddings for $\\mathcal{O}(1)$ lookup.\n\n**Key Contributions:**\n- **Sparsity Allocation:** We formulate the trade-off between neural computation (MoE) and static memory (Engram), identifying a U-shaped scaling law that guides optimal capacity allocation.\n- **Empirical Verification:** Under strict iso-parameter and iso-FLOPs constraints, the Engram-27B model demonstrates consistent improvements over MoE baselines across knowledge, reasoning, code and math domains.\n- **Mechanistic Analysis:** Our analysis suggests that Engram relieves early layers from static pattern reconstruction, potentially preserving effective depth for complex reasoning.\n- **System Efficiency:** The module employs deterministic addressing, enabling the offloading of massive embedding tables to host memory with minimal inference overhead.\n\n\n## 2. Architecture\n\nThe Engram module augments the backbone by retrieving static $N$-gram memory and fusing it with dynamic hidden states. The architecture is shown below ([drawio provided](drawio/Engram.drawio)):\n\n<p align=\"center\">\n  <img width=\"75%\" src=\"figures/arch.png\" alt=\"Engram Architecture\">\n</p>\n\n## 3. Evaluation\n\n### Scaling Law\n<p align=\"center\">\n  <img width=\"90%\" src=\"figures/scaling_law.png\" alt=\"Scaling Law\">\n</p>\n\n---\n\n### Large Scale Pre-training\n<p align=\"center\">\n  <img width=\"80%\" src=\"figures/27b_exp_results.png\" alt=\"Pre-training Results\">\n</p>\n\n---\n\n### Long-context Training\n<p align=\"center\">\n  <img width=\"80%\" src=\"figures/long_context_results.png\" alt=\"Long Context Results\">\n</p>\n\n\n## 4. Case Study of Engram\n<p align=\"center\">\n  <img width=\"80%\" src=\"figures/case.png\" alt=\"Long Context Results\">\n</p>\n\n## 5. Quick Start\n\nWe recommend using Python 3.8+ and PyTorch.\n```bash\npip install torch numpy transformers sympy\n```\nWe provide a standalone implementation to demonstrate the core logic of the Engram module:\n```bash\npython engram_demo_v1.py\n```\n\n> âš ï¸ **Note:** The provided code is a demonstration version intended to illustrate the data flow. It mocks standard components (like Attention/MoE/mHC) to focus on the Engram module. \n\n\n## 6. License\nThe use of Engram models is subject to [the Model License](LICENSE).\n\n## 7. Contact\n\nIf you have any questions, please raise an issue or contact us at [service@deepseek.com](mailto:service@deepseek.com).",
      "default_branch": "main"
    },
    "fetched_at": "2026-02-01T03:44:57.461538"
  },
  {
    "basic_info": {
      "name": "LTX-2",
      "full_name": "Lightricks/LTX-2",
      "owner": "Lightricks",
      "description": "Official Python inference and LoRA trainer package for the LTX-2 audioâ€“video generative model.",
      "url": "https://github.com/Lightricks/LTX-2",
      "clone_url": "https://github.com/Lightricks/LTX-2.git",
      "ssh_url": "git@github.com:Lightricks/LTX-2.git",
      "homepage": "https://ltx.io/model/ltx-2",
      "created_at": "2026-01-03T13:16:29Z",
      "updated_at": "2026-02-01T03:44:19Z",
      "pushed_at": "2026-01-29T19:30:12Z"
    },
    "stats": {
      "stars": 3346,
      "forks": 424,
      "watchers": 3346,
      "open_issues": 62,
      "size": 220
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 851032
      },
      "license": "Other",
      "topics": [
        "generative-ai",
        "ltx",
        "ltx-2"
      ]
    },
    "content": {
      "readme": "# LTX-2\n\n[![Website](https://img.shields.io/badge/Website-LTX-181717?logo=google-chrome)](https://ltx.io)\n[![Model](https://img.shields.io/badge/HuggingFace-Model-orange?logo=huggingface)](https://huggingface.co/Lightricks/LTX-2)\n[![Demo](https://img.shields.io/badge/Demo-Try%20Now-brightgreen?logo=vercel)](https://app.ltx.studio/ltx-2-playground/i2v)\n[![Paper](https://img.shields.io/badge/Paper-PDF-EC1C24?logo=adobeacrobatreader&logoColor=white)](https://arxiv.org/abs/2601.03233)\n[![Discord](https://img.shields.io/badge/Join-Discord-5865F2?logo=discord)](https://discord.gg/ltxplatform)\n\n**LTX-2** is the first DiT-based audio-video foundation model that contains all core capabilities of modern video generation in one model: synchronized audio and video, high fidelity, multiple performance modes, production-ready outputs, API access, and open access.\n\n<div align=\"center\">\n  <video src=\"https://github.com/user-attachments/assets/4414adc0-086c-43de-b367-9362eeb20228\" width=\"70%\" poster=\"\"> </video>\n</div>\n\n## ğŸš€ Quick Start\n\n```bash\n# Clone the repository\ngit clone https://github.com/Lightricks/LTX-2.git\ncd LTX-2\n\n# Set up the environment\nuv sync --frozen\nsource .venv/bin/activate\n```\n\n### Required Models\n\nDownload the following models from the [LTX-2 HuggingFace repository](https://huggingface.co/Lightricks/LTX-2):\n\n**LTX-2 Model Checkpoint** (choose and download one of the following)\n  * [`ltx-2-19b-dev-fp8.safetensors`](https://huggingface.co/Lightricks/LTX-2/blob/main/ltx-2-19b-dev-fp8.safetensors) - [Download](https://huggingface.co/Lightricks/LTX-2/resolve/main/ltx-2-19b-dev-fp8.safetensors)\n\n  * [`ltx-2-19b-dev.safetensors`](https://huggingface.co/Lightricks/LTX-2/blob/main/ltx-2-19b-dev.safetensors) - [Download](https://huggingface.co/Lightricks/LTX-2/resolve/main/ltx-2-19b-dev.safetensors)\n  * [`ltx-2-19b-distilled.safetensors`](https://huggingface.co/Lightricks/LTX-2/blob/main/ltx-2-19b-distilled.safetensors) - [Download](https://huggingface.co/Lightricks/LTX-2/resolve/main/ltx-2-19b-distilled.safetensors)\n  * [`ltx-2-19b-distilled-fp8.safetensors`](https://huggingface.co/Lightricks/LTX-2/blob/main/ltx-2-19b-distilled-fp8.safetensors) - [Download](https://huggingface.co/Lightricks/LTX-2/resolve/main/ltx-2-19b-distilled-fp8.safetensors)\n\n**Spatial Upscaler** - Required for current two-stage pipeline implementations in this repository\n  * [`ltx-2-spatial-upscaler-x2-1.0.safetensors`](https://huggingface.co/Lightricks/LTX-2/blob/main/ltx-2-spatial-upscaler-x2-1.0.safetensors) - [Download](https://huggingface.co/Lightricks/LTX-2/resolve/main/ltx-2-spatial-upscaler-x2-1.0.safetensors)\n\n**Temporal Upscaler** - Supported by the model and will be required for future pipeline implementations\n  * [`ltx-2-temporal-upscaler-x2-1.0.safetensors`](https://huggingface.co/Lightricks/LTX-2/blob/main/ltx-2-temporal-upscaler-x2-1.0.safetensors) - [Download](https://huggingface.co/Lightricks/LTX-2/resolve/main/ltx-2-temporal-upscaler-x2-1.0.safetensors)\n\n**Distilled LoRA** - Required for current two-stage pipeline implementations in this repository (except DistilledPipeline and ICLoraPipeline)\n  * [`ltx-2-19b-distilled-lora-384.safetensors`](https://huggingface.co/Lightricks/LTX-2/blob/main/ltx-2-19b-distilled-lora-384.safetensors) - [Download](https://huggingface.co/Lightricks/LTX-2/resolve/main/ltx-2-19b-distilled-lora-384.safetensors)\n\n**Gemma Text Encoder** (download all assets from the repository)\n  * [`Gemma 3`](https://huggingface.co/google/gemma-3-12b-it-qat-q4_0-unquantized/tree/main)\n\n**LoRAs**\n  * [`LTX-2-19b-IC-LoRA-Canny-Control`](https://huggingface.co/Lightricks/LTX-2-19b-IC-LoRA-Canny-Control) - [Download](https://huggingface.co/Lightricks/LTX-2-19b-IC-LoRA-Canny-Control/resolve/main/ltx-2-19b-ic-lora-canny-control.safetensors)\n  * [`LTX-2-19b-IC-LoRA-Depth-Control`](https://huggingface.co/Lightricks/LTX-2-19b-IC-LoRA-Depth-Control) - [Download](https://huggingface.co/Lightricks/LTX-2-19b-IC-LoRA-Depth-Control/resolve/main/ltx-2-19b-ic-lora-depth-control.safetensors)\n  * [`LTX-2-19b-IC-LoRA-Detailer`](https://huggingface.co/Lightricks/LTX-2-19b-IC-LoRA-Detailer) - [Download](https://huggingface.co/Lightricks/LTX-2-19b-IC-LoRA-Detailer/resolve/main/ltx-2-19b-ic-lora-detailer.safetensors)\n  * [`LTX-2-19b-IC-LoRA-Pose-Control`](https://huggingface.co/Lightricks/LTX-2-19b-IC-LoRA-Pose-Control) - [Download](https://huggingface.co/Lightricks/LTX-2-19b-IC-LoRA-Pose-Control/resolve/main/ltx-2-19b-ic-lora-pose-control.safetensors)\n  * [`LTX-2-19b-LoRA-Camera-Control-Dolly-In`](https://huggingface.co/Lightricks/LTX-2-19b-LoRA-Camera-Control-Dolly-In) - [Download](https://huggingface.co/Lightricks/LTX-2-19b-LoRA-Camera-Control-Dolly-In/resolve/main/ltx-2-19b-lora-camera-control-dolly-in.safetensors)\n  * [`LTX-2-19b-LoRA-Camera-Control-Dolly-Left`](https://huggingface.co/Lightricks/LTX-2-19b-LoRA-Camera-Control-Dolly-Left) - [Download](https://huggingface.co/Lightricks/LTX-2-19b-LoRA-Camera-Control-Dolly-Left/resolve/",
      "default_branch": "main"
    },
    "fetched_at": "2026-02-01T03:44:58.579328"
  },
  {
    "basic_info": {
      "name": "original_performance_takehome",
      "full_name": "anthropics/original_performance_takehome",
      "owner": "anthropics",
      "description": "Anthropic's original performance take-home, now open for you to try!",
      "url": "https://github.com/anthropics/original_performance_takehome",
      "clone_url": "https://github.com/anthropics/original_performance_takehome.git",
      "ssh_url": "git@github.com:anthropics/original_performance_takehome.git",
      "homepage": null,
      "created_at": "2026-01-19T19:16:04Z",
      "updated_at": "2026-02-01T02:05:10Z",
      "pushed_at": "2026-01-22T01:11:08Z"
    },
    "stats": {
      "stars": 3211,
      "forks": 702,
      "watchers": 3211,
      "open_issues": 12,
      "size": 8
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 57361,
        "HTML": 4758
      },
      "license": null,
      "topics": []
    },
    "content": {
      "readme": "# Anthropic's Original Performance Take-Home\n\nThis repo contains a version of Anthropic's original performance take-home, before Claude Opus 4.5 started doing better than humans given only 2 hours.\n\nThe original take-home was a 4-hour one that starts close to the contents of this repo, after Claude Opus 4 beat most humans at that, it was updated to a 2-hour one which started with code which achieved 18532 cycles (7.97x faster than this repo starts you). This repo is based on the newer take-home which has a few more instructions and comes with better debugging tools, but has the starter code reverted to the slowest baseline. After Claude Opus 4.5 we started using a different base for our time-limited take-homes.\n\nNow you can try to beat Claude Opus 4.5 given unlimited time!\n\n## Performance benchmarks \n\nMeasured in clock cycles from the simulated machine. All of these numbers are for models doing the 2 hour version which started at 18532 cycles:\n\n- **2164 cycles**: Claude Opus 4 after many hours in the test-time compute harness\n- **1790 cycles**: Claude Opus 4.5 in a casual Claude Code session, approximately matching the best human performance in 2 hours\n- **1579 cycles**: Claude Opus 4.5 after 2 hours in our test-time compute harness\n- **1548 cycles**: Claude Sonnet 4.5 after many more than 2 hours of test-time compute\n- **1487 cycles**: Claude Opus 4.5 after 11.5 hours in the harness\n- **1363 cycles**: Claude Opus 4.5 in an improved test time compute harness\n- **??? cycles**: Best human performance ever is substantially better than the above, but we won't say how much.\n\nWhile it's no longer a good time-limited test, you can still use this test to get us excited about hiring you! If you optimize below 1487 cycles, beating Claude Opus 4.5's best performance at launch, email us at performance-recruiting@anthropic.com with your code (and ideally a resume) so we can be appropriately impressed, especially if you get near the best solution we've seen. New model releases may change what threshold impresses us though, and no guarantees that we keep this readme updated with the latest on that.\n\nRun `python tests/submission_tests.py` to see which thresholds you pass.\n\n## Warning: LLMs can cheat\n\nNone of the solutions we received on the first day post-release below 1300 cycles were valid solutions. In each case, a language model modified the tests to make the problem easier.\n\nIf you use an AI agent, we recommend instructing it not to change the `tests/` folder and to use `tests/submission_tests.py` for verification.\n\nPlease run the following commands to validate your submission, and mention that you did so when submitting:\n```\n# This should be empty, the tests folder must be unchanged\ngit diff origin/main tests/\n# You should pass some of these tests and use the cycle count this prints\npython tests/submission_tests.py\n```\n\nAn example of this kind of hack is a model noticing that `problem.py` has multicore support, implementing multicore as an optimization, noticing there's no speedup and \"debugging\" that `N_CORES = 1` and \"fixing\" the core count so they get a speedup. Multicore is disabled intentionally in this version.\n",
      "default_branch": "main"
    },
    "fetched_at": "2026-02-01T03:44:59.699228"
  },
  {
    "basic_info": {
      "name": "pocket-tts",
      "full_name": "kyutai-labs/pocket-tts",
      "owner": "kyutai-labs",
      "description": "A TTS that fits in your CPU (and pocket)",
      "url": "https://github.com/kyutai-labs/pocket-tts",
      "clone_url": "https://github.com/kyutai-labs/pocket-tts.git",
      "ssh_url": "git@github.com:kyutai-labs/pocket-tts.git",
      "homepage": null,
      "created_at": "2026-01-07T17:33:32Z",
      "updated_at": "2026-02-01T03:34:12Z",
      "pushed_at": "2026-01-30T10:32:21Z"
    },
    "stats": {
      "stars": 2850,
      "forks": 307,
      "watchers": 2850,
      "open_issues": 35,
      "size": 595
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 131907,
        "HTML": 15252,
        "Dockerfile": 312
      },
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "# Pocket TTS\n\n<img width=\"1446\" height=\"622\" alt=\"pocket-tts-logo-v2-transparent\" src=\"https://github.com/user-attachments/assets/637b5ed6-831f-4023-9b4c-741be21ab238\" />\n\nA lightweight text-to-speech (TTS) application designed to run efficiently on CPUs.\nForget about the hassle of using GPUs and web APIs serving TTS models. With Kyutai's Pocket TTS, generating audio is just a pip install and a function call away.\n\nSupports Python 3.10, 3.11, 3.12, 3.13 and 3.14. Requires PyTorch 2.5+. Does not require the gpu version of PyTorch.\n\n[ğŸ”Š Demo](https://kyutai.org/pocket-tts) | \n[ğŸ±â€ğŸ’»GitHub Repository](https://github.com/kyutai-labs/pocket-tts) | \n[ğŸ¤— Hugging Face Model Card](https://huggingface.co/kyutai/pocket-tts) | \n[âš™ï¸ Tech report](https://kyutai.org/blog/2026-01-13-pocket-tts) |\n[ğŸ“„ Paper](https://arxiv.org/abs/2509.06926) | \n[ğŸ“š Documentation](https://github.com/kyutai-labs/pocket-tts/tree/main/docs)\n\n\n## Main takeaways\n* Runs on CPU\n* Small model size, 100M parameters\n* Audio streaming\n* Low latency, ~200ms to get the first audio chunk\n* Faster than real-time, ~6x real-time on a CPU of MacBook Air M4\n* Uses only 2 CPU cores\n* Python API and CLI\n* Voice cloning\n* English only at the moment\n* Can handle infinitely long text inputs\n* [Can run on client-side in the browser](#in-browser-implementations)\n\n## Trying it from the website, without installing anything\n\nNavigate to the [Kyutai website](https://kyutai.org/pocket-tts) to try it out directly in your browser. You can input text, select different voices, and generate speech without any installation.\n\n## Trying it with the CLI\n\n### The `generate` command\nYou can use pocket-tts directly from the command line. We recommend using\n`uv` as it installs any dependencies on the fly in an isolated environment (uv installation instructions [here](https://docs.astral.sh/uv/getting-started/installation/#standalone-installer)).\nYou can also use `pip install pocket-tts` to install it manually.\n\nThis will generate a wav file `./tts_output.wav` saying the default text with the default voice, and display some speed statistics.\n```bash\nuvx pocket-tts generate\n# or if you installed it manually with pip:\npocket-tts generate\n```\nModify the voice with `--voice` and the text with `--text`. We provide a small catalog of voices.\n\nYou can take a look at [this page](https://huggingface.co/kyutai/tts-voices) which details the licenses\nfor each voice.\n\n* [alba](https://huggingface.co/kyutai/tts-voices/blob/main/alba-mackenna/casual.wav)\n* [marius](https://huggingface.co/kyutai/tts-voices/blob/main/voice-donations/Selfie.wav)\n* [javert](https://huggingface.co/kyutai/tts-voices/blob/main/voice-donations/Butter.wav)\n* [jean](https://huggingface.co/kyutai/tts-voices/blob/main/ears/p010/freeform_speech_01.wav)\n* [fantine](https://huggingface.co/kyutai/tts-voices/blob/main/vctk/p244_023.wav)\n* [cosette](https://huggingface.co/kyutai/tts-voices/blob/main/expresso/ex04-ex02_confused_001_channel1_499s.wav)\n* [eponine](https://huggingface.co/kyutai/tts-voices/blob/main/vctk/p262_023.wav)\n* [azelma](https://huggingface.co/kyutai/tts-voices/blob/main/vctk/p303_023.wav)\n\nThe `--voice` argument can also take a plain wav file as input for voice cloning.\nYou can use your own or check out our [voice repository](https://huggingface.co/kyutai/tts-voices).\nWe recommend [cleaning the sample](https://podcast.adobe.com/en/enhance) before using it with Pocket TTS, because the audio quality of the sample is also reproduced.\n\nFeel free to check out the [generate documentation](https://github.com/kyutai-labs/pocket-tts/tree/main/docs/generate.md) for more details and examples.\nFor trying multiple voices and prompts quickly, prefer using the `serve` command.\n\n### The `serve` command\n\nYou can also run a local server to generate audio via HTTP requests.\n```bash\nuvx pocket-tts serve\n# or if you installed it manually with pip:\npocket-tts serve\n```\nNavigate to `http://localhost:8000` to try the web interface, it's faster than the command line as the model is kept in memory between requests.\n\nYou can check out the [serve documentation](https://github.com/kyutai-labs/pocket-tts/tree/main/docs/serve.md) for more details and examples.\n\n### The `export-voice` command\n\nProcessing an audio file (e.g., a .wav or .mp3) for voice cloning is relatively slow, but loading a safetensors file -- a voice embedding converted from an audio file -- is very fast. You can use the `export-voice` command to do this conversion. See the [export-voice documentation](https://github.com/kyutai-labs/pocket-tts/tree/main/docs/export_voice.md) for more details and examples.\n\n\n## Using it as a Python library\n\nYou can try out the Python library on Colab [here](https://colab.research.google.com/github/kyutai-labs/pocket-tts/blob/main/docs/pocket-tts-example.ipynb).\n\nInstall the package with\n```bash\npip install pocket-tts\n# or\nuv add pocket-tts\n```\n\nYou can use this package as a simple Python library to generate audio from text.\n```python\nfrom pocket_tts ",
      "default_branch": "main"
    },
    "fetched_at": "2026-02-01T03:45:00.843683"
  },
  {
    "basic_info": {
      "name": "heartlib",
      "full_name": "HeartMuLa/heartlib",
      "owner": "HeartMuLa",
      "description": null,
      "url": "https://github.com/HeartMuLa/heartlib",
      "clone_url": "https://github.com/HeartMuLa/heartlib.git",
      "ssh_url": "git@github.com:HeartMuLa/heartlib.git",
      "homepage": null,
      "created_at": "2026-01-15T07:53:15Z",
      "updated_at": "2026-02-01T03:42:40Z",
      "pushed_at": "2026-01-23T11:40:47Z"
    },
    "stats": {
      "stars": 2619,
      "forks": 250,
      "watchers": 2619,
      "open_issues": 67,
      "size": 893
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 77582
      },
      "license": "Apache License 2.0",
      "topics": []
    },
    "content": {
      "readme": "<p align=\"center\">\n    <picture>\n        <source srcset=\"./assets/logo.png\" media=\"(prefers-color-scheme: dark)\">\n        <img src=\"./assets/logo.png\" width=\"30%\">\n    </picture>\n    \n</p>\n\n<p align=\"center\">\n    <a href=\"https://heartmula.github.io/\">Demo ğŸ¶</a> &nbsp;|&nbsp; ğŸ“‘ <a href=\"https://arxiv.org/pdf/2601.10547\">Paper</a>\n    <br>\n    <a href=\"https://huggingface.co/HeartMuLa/HeartMuLa-oss-3B\">HeartMuLa-oss-3B ğŸ¤—</a> &nbsp;|&nbsp; <a href=\"https://modelscope.cn/models/HeartMuLa/HeartMuLa-oss-3B\">HeartMuLa-oss-3B <picture>\n        <source srcset=\"./assets/badge.svg\" media=\"(prefers-color-scheme: dark)\">\n        <img src=\"./assets/badge.svg\" width=\"20px\">\n    </picture></a>\n    <br>\n    <a href=\"https://huggingface.co/HeartMuLa/HeartMuLa-RL-oss-3B-20260123\"> HeartMuLa-RL-oss-3B-20260123 ğŸ¤—</a> &nbsp;|&nbsp; <a href=\"https://modelscope.cn/models/HeartMuLa/HeartMuLa-RL-oss-3B-20260123\">HeartMuLa-RL-oss-3B-20260123 <picture>\n        <source srcset=\"./assets/badge.svg\" media=\"(prefers-color-scheme: dark)\">\n        <img src=\"./assets/badge.svg\" width=\"20px\">\n    </picture></a>\n    \n</p>\n\n---\n# HeartMuLa: A Family of Open Sourced Music Foundation Models\n\nHeartMuLa is a family of open sourced music foundation models including: \n1. HeartMuLa: a music language model that generates music conditioned on lyrics and tags with multilingual support including but not limited to English, Chinese, Japanese, Korean and Spanish.\n2. HeartCodec: a 12.5 hz music codec with high reconstruction fidelity;\n3. HeartTranscriptor: a whisper-based model specifically tuned for lyrics transcription; Check [this page](./examples/README.md) for its usage.\n4. HeartCLAP: an audioâ€“text alignment model that establishes a unified embedding space for music descriptions and cross-modal retrieval.\n---\n\n\nBelow shows the experiment result of our oss-3B version compared with other baselines.\n<p align=\"center\">\n    <picture>\n        <source srcset=\"./assets/exp.png\" media=\"(prefers-color-scheme: dark)\">\n        <img src=\"./assets/exp.png\" width=\"90%\">\n    </picture>\n    \n</p>\n\n---\n\n## ğŸ”¥ Highlight\n\nOur latest internal version of HeartMuLa-7B achieves **comparable performance with Suno** in terms of musicality, fidelity and controllability. If you are interested, welcome to reach us out via heartmula.ai@gmail.com\n\n## ğŸ“° News\nJoin on Discord! [<img alt=\"join discord\" src=\"https://img.shields.io/discord/842440537755353128?color=%237289da&logo=discord\"/>](https://discord.gg/BKXF5FgH)\n\n- ğŸš€ **23 Jan. 2026**\n\n    By leveraging Reinforcement Learning, we have continuously refined our model and are proud to officially release **HeartMuLa-RL-oss-3B-20260123**. This version is designed to achieve more precise control over styles and tags. Simultaneously, we are launching **HeartCodec-oss-20260123**, which optimizes audio decoding quality.\n\n- ğŸ«¶ **20 Jan. 2026** \n    \n    [Benji](https://github.com/benjiyaya) has created a wonderful [ComfyUI custom node](https://github.com/benjiyaya/HeartMuLa_ComfyUI) for HeartMuLa. Thanks Benji!\n- âš–ï¸ **20 Jan. 2026** \n\n    License update: We update the license of this repo and all related model weights to **Apache 2.0**.\n- ğŸš€ **14 Jan. 2026**  \n    The official release of **HeartTranscriptor-oss** and the first **HeartMuLa-oss-3B** version along with our **HeartCodec-oss**.\n\n---\n## ğŸ§­ TODOs\n\n- â³ Release scripts for inference acceleration and streaming inference. The current inference speed is around RTF $\\approx 1.0$.\n- â³ Support **reference audio conditioning**, **fine-grained controllable music generation**, **hot song generation**.\n- â³ Release the **HeartMuLa-oss-7B** version.\n- âœ… Release inference code and pretrained checkpoints of  \n  **HeartCodec-oss, HeartMuLa-oss-3B, and HeartTranscriptor-oss**.\n\n---\n\n## ğŸ› ï¸ Local Deployment\n\n### âš™ï¸ Environment Setup\n\nWe recommend using `python=3.10` for local deployment.\n\nClone this repo and install locally.\n\n```\ngit clone https://github.com/HeartMuLa/heartlib.git\ncd heartlib\npip install -e .\n```\n\nDownload our pretrained checkpoints from huggingface or modelscope using the following command:\n\n```\n# if you are using huggingface\nhf download --local-dir './ckpt' 'HeartMuLa/HeartMuLaGen'\n\n## To use version released on 20260123 (recommended)\nhf download --local-dir './ckpt/HeartMuLa-oss-3B' 'HeartMuLa/HeartMuLa-RL-oss-3B-20260123'\nhf download --local-dir './ckpt/HeartCodec-oss' HeartMuLa/HeartCodec-oss-20260123\n\n## To use oss-3B version\nhf download --local-dir './ckpt/HeartMuLa-oss-3B' 'HeartMuLa/HeartMuLa-oss-3B'\nhf download --local-dir './ckpt/HeartCodec-oss' 'HeartMuLa/HeartCodec-oss'\n\n# if you are using modelscope\nmodelscope download --model 'HeartMuLa/HeartMuLaGen' --local_dir './ckpt'\n\n## To use version released on 20260123 (recommended)\nmodelscope download --model 'HeartMuLa/HeartMuLa-RL-oss-3B-20260123' --local_dir './ckpt/HeartMuLa-oss-3B'\nmodelscope download --model 'HeartMuLa/HeartCodec-oss-20260123' --local_dir './ckpt/HeartCodec-oss'\n\n## To use oss-3B version\nmodelscope download --m",
      "default_branch": "main"
    },
    "fetched_at": "2026-02-01T03:45:01.969704"
  },
  {
    "basic_info": {
      "name": "skills",
      "full_name": "trailofbits/skills",
      "owner": "trailofbits",
      "description": "Trail of Bits Claude Code skills for security research, vulnerability detection, and audit workflows",
      "url": "https://github.com/trailofbits/skills",
      "clone_url": "https://github.com/trailofbits/skills.git",
      "ssh_url": "git@github.com:trailofbits/skills.git",
      "homepage": "",
      "created_at": "2026-01-14T18:23:21Z",
      "updated_at": "2026-02-01T03:29:24Z",
      "pushed_at": "2026-02-01T00:57:56Z"
    },
    "stats": {
      "stars": 2273,
      "forks": 177,
      "watchers": 2273,
      "open_issues": 11,
      "size": 612
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 336468,
        "Shell": 68533,
        "YARA": 23399,
        "CodeQL": 12064,
        "C": 8714,
        "Swift": 5459,
        "C#": 5442,
        "Ruby": 5419,
        "Java": 5221,
        "JavaScript": 5033,
        "Kotlin": 5019,
        "TypeScript": 4645,
        "PHP": 3832,
        "Rust": 2608,
        "Go": 2213
      },
      "license": "Creative Commons Attribution Share Alike 4.0 International",
      "topics": [
        "agent-skills"
      ]
    },
    "content": {
      "readme": "# Trail of Bits Skills Marketplace\n\nA Claude Code plugin marketplace from Trail of Bits providing skills to enhance AI-assisted security analysis, testing, and development workflows.\n\n## Installation\n\n### Add the Marketplace\n\n```\n/plugin marketplace add trailofbits/skills\n```\n\n### Browse and Install Plugins\n\n```\n/plugin menu\n```\n\n### Local Development\n\nTo add the marketplace locally (e.g., for testing or development), navigate to the **parent directory** of this repository:\n\n```\ncd /path/to/parent  # e.g., if repo is at ~/projects/skills, be in ~/projects\n/plugins marketplace add ./skills\n```\n\n## Available Plugins\n\n### Smart Contract Security\n\n| Plugin | Description |\n|--------|-------------|\n| [building-secure-contracts](plugins/building-secure-contracts/) | Smart contract security toolkit with vulnerability scanners for 6 blockchains |\n| [entry-point-analyzer](plugins/entry-point-analyzer/) | Identify state-changing entry points in smart contracts for security auditing |\n\n### Code Auditing\n\n| Plugin | Description |\n|--------|-------------|\n| [audit-context-building](plugins/audit-context-building/) | Build deep architectural context through ultra-granular code analysis |\n| [burpsuite-project-parser](plugins/burpsuite-project-parser/) | Search and extract data from Burp Suite project files |\n| [differential-review](plugins/differential-review/) | Security-focused differential review of code changes with git history analysis |\n| [insecure-defaults](plugins/insecure-defaults/) | Detect insecure default configurations, hardcoded credentials, and fail-open security patterns |\n| [semgrep-rule-creator](plugins/semgrep-rule-creator/) | Create and refine Semgrep rules for custom vulnerability detection |\n| [semgrep-rule-variant-creator](plugins/semgrep-rule-variant-creator/) | Port existing Semgrep rules to new target languages with test-driven validation |\n| [sharp-edges](plugins/sharp-edges/) | Identify error-prone APIs, dangerous configurations, and footgun designs |\n| [static-analysis](plugins/static-analysis/) | Static analysis toolkit with CodeQL, Semgrep, and SARIF parsing |\n| [testing-handbook-skills](plugins/testing-handbook-skills/) | Skills from the [Testing Handbook](https://appsec.guide): fuzzers, static analysis, sanitizers, coverage |\n| [variant-analysis](plugins/variant-analysis/) | Find similar vulnerabilities across codebases using pattern-based analysis |\n\n### Malware Analysis\n\n| Plugin | Description |\n|--------|-------------|\n| [yara-authoring](plugins/yara-authoring/) | YARA detection rule authoring with linting, atom analysis, and best practices |\n\n### Verification\n\n| Plugin | Description |\n|--------|-------------|\n| [constant-time-analysis](plugins/constant-time-analysis/) | Detect compiler-induced timing side-channels in cryptographic code |\n| [property-based-testing](plugins/property-based-testing/) | Property-based testing guidance for multiple languages and smart contracts |\n| [spec-to-code-compliance](plugins/spec-to-code-compliance/) | Specification-to-code compliance checker for blockchain audits |\n\n### Audit Lifecycle\n\n| Plugin | Description |\n|--------|-------------|\n| [fix-review](plugins/fix-review/) | Verify fix commits address audit findings without introducing bugs |\n\n### Reverse Engineering\n\n| Plugin | Description |\n|--------|-------------|\n| [dwarf-expert](plugins/dwarf-expert/) | Interact with and understand the DWARF debugging format |\n\n### Mobile Security\n\n| Plugin | Description |\n|--------|-------------|\n| [firebase-apk-scanner](plugins/firebase-apk-scanner/) | Scan Android APKs for Firebase security misconfigurations |\n\n### Development\n\n| Plugin | Description |\n|--------|-------------|\n| [ask-questions-if-underspecified](plugins/ask-questions-if-underspecified/) | Clarify requirements before implementing |\n| [modern-python](plugins/modern-python/) | Modern Python tooling and best practices with uv, ruff, and pytest |\n\n### Team Management\n\n| Plugin | Description |\n|--------|-------------|\n| [culture-index](plugins/culture-index/) | Interpret Culture Index survey results for individuals and teams |\n\n### Tooling\n\n| Plugin | Description |\n|--------|-------------|\n| [claude-in-chrome-troubleshooting](plugins/claude-in-chrome-troubleshooting/) | Diagnose and fix Claude in Chrome MCP extension connectivity issues |\n\n## Trophy Case\n\nBugs discovered using Trail of Bits Skills. Found something? [Let us know!](https://github.com/trailofbits/skills/issues/new?template=trophy-case.yml)\n\nWhen reporting bugs you've found, feel free to mention:\n> Found using [Trail of Bits Skills](https://github.com/trailofbits/skills)\n\n| Skill | Bug |\n|-------|-----|\n| constant-time-analysis | [Timing side-channel in ML-DSA signing](https://github.com/RustCrypto/signatures/pull/1144) |\n\n## Contributing\n\nWe welcome contributions! Please see [CLAUDE.md](CLAUDE.md) for skill authoring guidelines.\n\n## License\n\nThis work is licensed under a [Creative Commons Attribution-ShareAlike 4.0 International License]",
      "default_branch": "main"
    },
    "fetched_at": "2026-02-01T03:45:03.098906"
  },
  {
    "basic_info": {
      "name": "nanocode",
      "full_name": "1rgs/nanocode",
      "owner": "1rgs",
      "description": "Minimal Claude Code alternative. Single Python file, zero dependencies, ~250 lines.",
      "url": "https://github.com/1rgs/nanocode",
      "clone_url": "https://github.com/1rgs/nanocode.git",
      "ssh_url": "git@github.com:1rgs/nanocode.git",
      "homepage": null,
      "created_at": "2026-01-11T02:12:27Z",
      "updated_at": "2026-02-01T03:05:36Z",
      "pushed_at": "2026-01-14T05:59:51Z"
    },
    "stats": {
      "stars": 1790,
      "forks": 149,
      "watchers": 1790,
      "open_issues": 7,
      "size": 183
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 8445
      },
      "license": null,
      "topics": []
    },
    "content": {
      "readme": "# nanocode\n\nMinimal Claude Code alternative. Single Python file, zero dependencies, ~250 lines.\n\nBuilt using Claude Code, then used to build itself.\n\n![screenshot](screenshot.png)\n\n## Features\n\n- Full agentic loop with tool use\n- Tools: `read`, `write`, `edit`, `glob`, `grep`, `bash`\n- Conversation history\n- Colored terminal output\n\n## Usage\n\n```bash\nexport ANTHROPIC_API_KEY=\"your-key\"\npython nanocode.py\n```\n\n### OpenRouter\n\nUse [OpenRouter](https://openrouter.ai) to access any model:\n\n```bash\nexport OPENROUTER_API_KEY=\"your-key\"\npython nanocode.py\n```\n\nTo use a different model:\n\n```bash\nexport OPENROUTER_API_KEY=\"your-key\"\nexport MODEL=\"openai/gpt-5.2\"\npython nanocode.py\n```\n\n## Commands\n\n- `/c` - Clear conversation\n- `/q` or `exit` - Quit\n\n## Tools\n\n| Tool | Description |\n|------|-------------|\n| `read` | Read file with line numbers, offset/limit |\n| `write` | Write content to file |\n| `edit` | Replace string in file (must be unique) |\n| `glob` | Find files by pattern, sorted by mtime |\n| `grep` | Search files for regex |\n| `bash` | Run shell command |\n\n## Example\n\n```\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nâ¯ what files are here?\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\nâº Glob(**/*.py)\n  â¿  nanocode.py\n\nâº There's one Python file: nanocode.py\n```\n\n## License\n\nMIT\n",
      "default_branch": "master"
    },
    "fetched_at": "2026-02-01T03:45:04.204252"
  },
  {
    "basic_info": {
      "name": "DeepSeek-OCR-2",
      "full_name": "deepseek-ai/DeepSeek-OCR-2",
      "owner": "deepseek-ai",
      "description": "Visual Causal Flow",
      "url": "https://github.com/deepseek-ai/DeepSeek-OCR-2",
      "clone_url": "https://github.com/deepseek-ai/DeepSeek-OCR-2.git",
      "ssh_url": "git@github.com:deepseek-ai/DeepSeek-OCR-2.git",
      "homepage": "",
      "created_at": "2026-01-27T03:05:42Z",
      "updated_at": "2026-02-01T03:36:09Z",
      "pushed_at": "2026-01-30T00:16:54Z"
    },
    "stats": {
      "stars": 1781,
      "forks": 130,
      "watchers": 1781,
      "open_issues": 32,
      "size": 1072
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 107443
      },
      "license": "Apache License 2.0",
      "topics": []
    },
    "content": {
      "readme": "<!-- markdownlint-disable first-line-h1 -->\n<!-- markdownlint-disable html -->\n<!-- markdownlint-disable no-duplicate-header -->\n\n\n<div align=\"center\">\n  <img src=\"assets/logo.svg\" width=\"60%\" alt=\"DeepSeek AI\" />\n</div>\n\n\n<hr>\n<div align=\"center\">\n  <a href=\"https://www.deepseek.com/\" target=\"_blank\">\n    <img alt=\"Homepage\" src=\"assets/badge.svg\" />\n  </a>\n  <a href=\"https://huggingface.co/deepseek-ai/DeepSeek-OCR-2\" target=\"_blank\">\n    <img alt=\"Hugging Face\" src=\"https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-DeepSeek%20AI-ffc107?color=ffc107&logoColor=white\" />\n  </a>\n\n</div>\n\n<div align=\"center\">\n\n  <a href=\"https://discord.gg/Tc7c45Zzu5\" target=\"_blank\">\n    <img alt=\"Discord\" src=\"https://img.shields.io/badge/Discord-DeepSeek%20AI-7289da?logo=discord&logoColor=white&color=7289da\" />\n  </a>\n  <a href=\"https://twitter.com/deepseek_ai\" target=\"_blank\">\n    <img alt=\"Twitter Follow\" src=\"https://img.shields.io/badge/Twitter-deepseek_ai-white?logo=x&logoColor=white\" />\n  </a>\n\n</div>\n\n\n\n<p align=\"center\">\n  <a href=\"https://huggingface.co/deepseek-ai/DeepSeek-OCR-2\"><b>ğŸ“¥ Model Download</b></a> |\n  <a href=\"https://github.com/deepseek-ai/DeepSeek-OCR-2/blob/main/DeepSeek_OCR2_paper.pdf\"><b>ğŸ“„ Paper Link</b></a> |\n  <a href=\"\"><b>ğŸ“„ Arxiv Paper Link</b></a> |\n</p>\n\n<h2>\n<p align=\"center\">\n  <a href=\"\">DeepSeek-OCR 2: Visual Causal Flow</a>\n</p>\n</h2>\n\n<p align=\"center\">\n<img src=\"assets/fig1.png\" style=\"width: 600px\" align=center>\n</p>\n<p align=\"center\">\n<a href=\"\">Explore more human-like visual encoding.</a>       \n</p>\n\n\n## Contents\n- [Install](#install)\n- [vLLM Inference](#vllm-inference)\n- [Transformers Inference](#transformers-inference)\n  \n\n\n\n\n## Install\n>Our environment is cuda11.8+torch2.6.0.\n1. Clone this repository and navigate to the DeepSeek-OCR-2 folder\n```bash\ngit clone https://github.com/deepseek-ai/DeepSeek-OCR-2.git\n```\n2. Conda\n```Shell\nconda create -n deepseek-ocr2 python=3.12.9 -y\nconda activate deepseek-ocr2\n```\n3. Packages\n\n- download the vllm-0.8.5 [whl](https://github.com/vllm-project/vllm/releases/tag/v0.8.5) \n```Shell\npip install torch==2.6.0 torchvision==0.21.0 torchaudio==2.6.0 --index-url https://download.pytorch.org/whl/cu118\npip install vllm-0.8.5+cu118-cp38-abi3-manylinux1_x86_64.whl\npip install -r requirements.txt\npip install flash-attn==2.7.3 --no-build-isolation\n```\n**Note:** if you want vLLM and transformers codes to run in the same environment, you don't need to worry about this installation error like: vllm 0.8.5+cu118 requires transformers>=4.51.1\n\n## vLLM-Inference\n- VLLM:\n>**Note:** change the INPUT_PATH/OUTPUT_PATH and other settings in the DeepSeek-OCR2-master/DeepSeek-OCR2-vllm/config.py\n```Shell\ncd DeepSeek-OCR2-master/DeepSeek-OCR2-vllm\n```\n1. image: streaming output\n```Shell\npython run_dpsk_ocr2_image.py\n```\n2. pdf: concurrency (on-par speed with DeepSeek-OCR)\n```Shell\npython run_dpsk_ocr2_pdf.py\n```\n3. batch eval for benchmarks (i.e., OmniDocBench v1.5)\n```Shell\npython run_dpsk_ocr2_eval_batch.py\n```\n\n## Transformers-Inference\n- Transformers\n```python\nfrom transformers import AutoModel, AutoTokenizer\nimport torch\nimport os\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\nmodel_name = 'deepseek-ai/DeepSeek-OCR-2'\n\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\nmodel = AutoModel.from_pretrained(model_name, _attn_implementation='flash_attention_2', trust_remote_code=True, use_safetensors=True)\nmodel = model.eval().cuda().to(torch.bfloat16)\n\n# prompt = \"<image>\\nFree OCR. \"\nprompt = \"<image>\\n<|grounding|>Convert the document to markdown. \"\nimage_file = 'your_image.jpg'\noutput_path = 'your/output/dir'\n\nres = model.infer(tokenizer, prompt=prompt, image_file=image_file, output_path = output_path, base_size = 1024, image_size = 768, crop_mode=True, save_results = True)\n```\nor you can\n```Shell\ncd DeepSeek-OCR2-master/DeepSeek-OCR2-hf\npython run_dpsk_ocr2.py\n```\n## Support-Modes\n- Dynamic resolution\n  - Default: (0-6)Ã—768Ã—768 + 1Ã—1024Ã—1024 â€” (0-6)Ã—144 + 256 visual tokens âœ…\n\n## Main Prompts\n```python\n# document: <image>\\n<|grounding|>Convert the document to markdown.\n# without layouts: <image>\\nFree OCR.\n```\n\n\n\n\n## Acknowledgement\n\nWe would like to thank [DeepSeek-OCR](https://github.com/deepseek-ai/DeepSeek-OCR/), [Vary](https://github.com/Ucas-HaoranWei/Vary/), [GOT-OCR2.0](https://github.com/Ucas-HaoranWei/GOT-OCR2.0/), [MinerU](https://github.com/opendatalab/MinerU), [PaddleOCR](https://github.com/PaddlePaddle/PaddleOCR) for their valuable models.\n\nWe also appreciate the benchmark [OmniDocBench](https://github.com/opendatalab/OmniDocBench).\n\n## Citation\n\n```bibtex\n@article{wei2025deepseek,\n  title={DeepSeek-OCR: Contexts Optical Compression},\n  author={Wei, Haoran and Sun, Yaofeng and Li, Yukun},\n  journal={arXiv preprint arXiv:2510.18234},\n  year={2025}\n}\n@article{wei2026deepseek,\n  title={DeepSeek-OCR 2: Visual Causal Flow},\n  author={Wei, Haoran and Sun, Yaofeng and Li, Yukun},\n  journal={arXiv preprint arXiv:2601.20552},\n  year",
      "default_branch": "main"
    },
    "fetched_at": "2026-02-01T03:45:05.357602"
  },
  {
    "basic_info": {
      "name": "last30days-skill",
      "full_name": "mvanhorn/last30days-skill",
      "owner": "mvanhorn",
      "description": "Claude Code skill that researches any topic across Reddit + X from the last 30 days, then writes copy-paste-ready prompts",
      "url": "https://github.com/mvanhorn/last30days-skill",
      "clone_url": "https://github.com/mvanhorn/last30days-skill.git",
      "ssh_url": "git@github.com:mvanhorn/last30days-skill.git",
      "homepage": null,
      "created_at": "2026-01-23T20:37:37Z",
      "updated_at": "2026-02-01T03:01:29Z",
      "pushed_at": "2026-01-28T21:43:11Z"
    },
    "stats": {
      "stars": 1747,
      "forks": 192,
      "watchers": 1747,
      "open_issues": 5,
      "size": 14125
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 156229
      },
      "license": null,
      "topics": [
        "ai-prompts",
        "claude",
        "claude-code",
        "reddit",
        "twitter"
      ]
    },
    "content": {
      "readme": "# /last30days\n\n**The AI world reinvents itself every month. This Claude Code skill keeps you current.** /last30days researches your topic across Reddit, X, and the web from the last 30 days, finds what the community is actually upvoting and sharing, and writes you a prompt that works today, not six months ago. Whether it's Ralph Wiggum loops, Suno music prompts, or the latest Midjourney techniques, you'll prompt like someone who's been paying attention.\n\n**Best for prompt research**: discover what prompting techniques actually work for any tool (ChatGPT, Midjourney, Claude, Figma AI, etc.) by learning from real community discussions and best practices.\n\n**But also great for anything trending**: music, culture, news, product recommendations, viral trends, or any question where \"what are people saying right now?\" matters.\n\n## Installation\n\n```bash\n# Clone the repo\ngit clone https://github.com/mvanhorn/last30days-skill.git ~/.claude/skills/last30days\n\n# Add your API keys\nmkdir -p ~/.config/last30days\ncat > ~/.config/last30days/.env << 'EOF'\nOPENAI_API_KEY=sk-...\nXAI_API_KEY=xai-...\nEOF\nchmod 600 ~/.config/last30days/.env\n```\n\n## Usage\n\n```\n/last30days [topic]\n/last30days [topic] for [tool]\n```\n\nExamples:\n- `/last30days prompting techniques for ChatGPT for legal questions`\n- `/last30days iOS app mockups for Nano Banana Pro`\n- `/last30days What are the best rap songs lately`\n- `/last30days remotion animations for Claude Code`\n\n## What It Does\n\n1. **Researches** - Scans Reddit and X for discussions from the last 30 days\n2. **Synthesizes** - Identifies patterns, best practices, and what actually works\n3. **Delivers** - Either writes copy-paste-ready prompts for your target tool, or gives you a curated expert-level answer\n\n### Use it for:\n- **Prompt research** - \"What prompting techniques work for legal questions in ChatGPT?\"\n- **Tool best practices** - \"How are people using Remotion with Claude Code?\"\n- **Trend discovery** - \"What are the best rap songs right now?\"\n- **Product research** - \"What do people think of the new M4 MacBook?\"\n- **Viral content** - \"What's the dog-as-human trend on ChatGPT?\"\n\n---\n\n## Example: Legal Prompting (Hallucination Prevention)\n\n**Query:** `/last30days prompting techniques for chatgpt for legal questions`\n\n**Research Output:**\n> The dominant theme is hallucination prevention - multiple sources discuss lawyers being fined or embarrassed for submitting fake case citations that ChatGPT fabricated. Key prompting strategies include: (1) using Deep Research mode or grounding tools with uploaded primary sources, (2) building \"Hallucination Prevention Systems\" into prompts that force epistemic honesty, (3) prompting for procedural questions rather than case law lookups, and (4) framing outputs as \"issue-spotting\" rather than legal advice. A major warning: chat logs are discoverable in litigation.\n\n**Key patterns discovered:**\n1. Hallucination prevention clauses - \"If you don't know a case citation, say so rather than inventing one\"\n2. Role assignment - assign ChatGPT a specific persona (paralegal, issue-spotter) to constrain outputs\n3. Structured output requirements - request specific sections (risks, key terms, questions for counsel)\n4. Epistemic humility enforcement - \"acknowledge uncertainty\" and \"flag when you're unsure\"\n5. Scope limitation - focus on procedural questions, not case law research\n\n**Then asked:** \"I have a squatter in my house in San Francisco, what are my options to get them out?\"\n\n**Generated Prompt:**\n\n```\nRole: You are a legal research assistant helping a property owner understand\ntheir options. You are NOT providing legal advice - you are helping me\nidentify issues, understand procedures, and prepare informed questions for an\nattorney.\n\nSituation: I own a residential property in San Francisco, California. Someone\nis currently occupying the property without my permission or any lease\nagreement (a squatter). I need to understand my legal options for removing them.\n\nYour task: Provide a structured analysis covering:\n\n1. California/SF Legal Framework - What laws govern this situation? (Note: San\nFrancisco has strong tenant protections - address how these apply or don't\napply to squatters vs. tenants)\n2. Key Distinctions - Help me understand: squatter vs. trespasser vs. holdover\ntenant. How does the length of occupation affect my options?\n3. Available Legal Remedies - What are my procedural options? (unlawful\ndetainer, police involvement, etc.) What is the typical process and timeline?\n4. Critical Mistakes to Avoid - What actions could expose me to liability?\n(self-help eviction, utility shutoffs, etc.)\n5. Questions I Should Ask an Attorney - Based on this analysis, what specific\nquestions should I bring to a California real estate attorney?\n\nImportant constraints:\n- Do NOT cite specific case names unless you are 100% certain they exist - say\n\"case law generally supports...\" instead\n- Flag any areas where you're uncertain\n- Note where San Francisco local ordinances may di",
      "default_branch": "main"
    },
    "fetched_at": "2026-02-01T03:45:06.504306"
  },
  {
    "basic_info": {
      "name": "notebooklm-py",
      "full_name": "teng-lin/notebooklm-py",
      "owner": "teng-lin",
      "description": "Unofficial Python API for Google NotebookLM",
      "url": "https://github.com/teng-lin/notebooklm-py",
      "clone_url": "https://github.com/teng-lin/notebooklm-py.git",
      "ssh_url": "git@github.com:teng-lin/notebooklm-py.git",
      "homepage": "https://github.com/teng-lin/notebooklm-py",
      "created_at": "2026-01-07T15:27:19Z",
      "updated_at": "2026-02-01T02:49:24Z",
      "pushed_at": "2026-01-27T13:56:24Z"
    },
    "stats": {
      "stars": 1692,
      "forks": 163,
      "watchers": 1692,
      "open_issues": 1,
      "size": 24759
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 1587634
      },
      "license": "MIT License",
      "topics": [
        "api",
        "claude",
        "notebookln",
        "python",
        "sdk",
        "skills"
      ]
    },
    "content": {
      "readme": "# notebooklm-py\n<p align=\"left\">\n  <img src=\"https://raw.githubusercontent.com/teng-lin/notebooklm-py/main/notebooklm-py.png\" alt=\"notebooklm-py logo\" width=\"128\">\n</p>\n\n**Comprehensive Python API for Google NotebookLM.** Full programmatic access to NotebookLM's featuresâ€”including capabilities the web UI doesn't exposeâ€”from Python or the command line.\n\n[![PyPI version](https://img.shields.io/pypi/v/notebooklm-py.svg)](https://pypi.org/project/notebooklm-py/)\n[![Python Version](https://img.shields.io/badge/python-3.10%20%7C%203.11%20%7C%203.12%20%7C%203.13%20%7C%203.14-blue)](https://pypi.org/project/notebooklm-py/)\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n[![Tests](https://github.com/teng-lin/notebooklm-py/actions/workflows/test.yml/badge.svg)](https://github.com/teng-lin/notebooklm-py/actions/workflows/test.yml)\n\n**Source & Development**: <https://github.com/teng-lin/notebooklm-py>\n\n> **âš ï¸ Unofficial Library - Use at Your Own Risk**\n>\n> This library uses **undocumented Google APIs** that can change without notice.\n>\n> - **Not affiliated with Google** - This is a community project\n> - **APIs may break** - Google can change internal endpoints anytime\n> - **Rate limits apply** - Heavy usage may be throttled\n>\n> Best for prototypes, research, and personal projects. See [Troubleshooting](docs/troubleshooting.md) for debugging tips.\n\n## What You Can Build\n\nğŸ¤– **AI Agent Tools** - Integrate NotebookLM into Claude Code or other LLM agents. Ships with [Claude Code skills](#agent-skills-claude-code) for natural language automation (`notebooklm skill install`), or build your own integrations with the async Python API.\n\nğŸ“š **Research Automation** - Bulk-import sources (URLs, PDFs, YouTube, Google Drive), run web/Drive research queries with auto-import, and extract insights programmatically. Build repeatable research pipelines.\n\nğŸ™ï¸ **Content Generation** - Generate Audio Overviews (podcasts), videos, slide decks, quizzes, flashcards, infographics, data tables, mind maps, and study guides. Full control over formats, styles, and output.\n\nğŸ“¥ **Downloads & Export** - Download all generated artifacts locally (MP3, MP4, PDF, PNG, CSV, JSON, Markdown). Export to Google Docs/Sheets. **Features the web UI doesn't offer**: batch downloads, quiz/flashcard export in multiple formats, mind map JSON extraction.\n\n## Three Ways to Use\n\n| Method | Best For |\n|--------|----------|\n| **Python API** | Application integration, async workflows, custom pipelines |\n| **CLI** | Shell scripts, quick tasks, CI/CD automation |\n| **Agent Skills** | Claude Code, LLM agents, natural language automation |\n\n## Features\n\n### Complete NotebookLM Coverage\n\n| Category | Capabilities |\n|----------|--------------|\n| **Notebooks** | Create, list, rename, delete |\n| **Sources** | URLs, YouTube, files (PDF, text, Markdown, Word, audio, video, images), Google Drive, pasted text; refresh, get guide/fulltext |\n| **Chat** | Questions, conversation history, custom personas |\n| **Research** | Web and Drive research agents (fast/deep modes) with auto-import |\n| **Sharing** | Public/private links, user permissions (viewer/editor), view level control |\n\n### Content Generation (All NotebookLM Studio Types)\n\n| Type | Options | Download Format |\n|------|---------|-----------------|\n| **Audio Overview** | 4 formats (deep-dive, brief, critique, debate), 3 lengths, 50+ languages | MP3/MP4 |\n| **Video Overview** | 2 formats, 9 visual styles (classic, whiteboard, kawaii, anime, etc.) | MP4 |\n| **Slide Deck** | Detailed or presenter format, adjustable length | PDF |\n| **Infographic** | 3 orientations, 3 detail levels | PNG |\n| **Quiz** | Configurable quantity and difficulty | JSON, Markdown, HTML |\n| **Flashcards** | Configurable quantity and difficulty | JSON, Markdown, HTML |\n| **Report** | Briefing doc, study guide, blog post, or custom prompt | Markdown |\n| **Data Table** | Custom structure via natural language | CSV |\n| **Mind Map** | Interactive hierarchical visualization | JSON |\n\n### Beyond the Web UI\n\nThese features are available via API/CLI but not exposed in NotebookLM's web interface:\n\n- **Batch downloads** - Download all artifacts of a type at once\n- **Quiz/Flashcard export** - Get structured JSON, Markdown, or HTML (web UI only shows interactive view)\n- **Mind map data extraction** - Export hierarchical JSON for visualization tools\n- **Data table CSV export** - Download structured tables as spreadsheets\n- **Source fulltext access** - Retrieve the indexed text content of any source\n- **Programmatic sharing** - Manage permissions without the UI\n\n## Installation\n\n```bash\n# Basic installation\npip install notebooklm-py\n\n# With browser login support (required for first-time setup)\npip install \"notebooklm-py[browser]\"\nplaywright install chromium\n```\n\n### Development Installation\n\nFor contributors or testing unreleased features:\n\n```bash\npip install git+https://github.com/teng-lin/notebooklm-py@main\n```\n\nâš ï¸ T",
      "default_branch": "main"
    },
    "fetched_at": "2026-02-01T03:45:07.635772"
  },
  {
    "basic_info": {
      "name": "AlphaGPT",
      "full_name": "imbue-bit/AlphaGPT",
      "owner": "imbue-bit",
      "description": "ä½¿ç”¨ç¬¦å·å›å½’åœ¨ä¸­å›½è‚¡å¸‚ä¸åŠ å¯†å¸‚åœºä¸Šè¿›è¡Œé«˜æ•ˆå› å­æŒ–æ˜ã€‚",
      "url": "https://github.com/imbue-bit/AlphaGPT",
      "clone_url": "https://github.com/imbue-bit/AlphaGPT.git",
      "ssh_url": "git@github.com:imbue-bit/AlphaGPT.git",
      "homepage": "",
      "created_at": "2026-01-08T02:05:21Z",
      "updated_at": "2026-02-01T03:18:24Z",
      "pushed_at": "2026-01-20T08:29:01Z"
    },
    "stats": {
      "stars": 1508,
      "forks": 2278,
      "watchers": 1508,
      "open_issues": 0,
      "size": 1995
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 109367
      },
      "license": "Apache License 2.0",
      "topics": [
        "deep-learning",
        "finance",
        "math",
        "quant",
        "quantitative-finance",
        "sr",
        "transformer",
        "uniswap-v4"
      ]
    },
    "content": {
      "readme": "# AlphaGPT\n\n> [!IMPORTANT]\n> GitHub çš„ issue å¹¶éè¢«è®¾è®¡ä¸ºæ‰“å¡å·¥å…·ï¼ä»…ç”¨äºã€Œæ‰“å¡ã€ã€Œç•™åã€çš„ issue å°†ä¼šè¢«ç›´æ¥åˆ é™¤ã€‚è‹¥æ‚¨éœ€è¦ç¤¾ç¾¤ï¼Œè¯·è€ƒè™‘QQç¾¤ç»„ 1076893473ã€‚\n\n> [!IMPORTANT]\n> è‹¥æ‚¨åœ¨åŠ å¯†å¸‚åœºè¿›è¡Œäº¤æ˜“ï¼Œå¦å¯å‚è€ƒ [Defense in Predatory Markets: A Differential Game Framework for AMM Liquidity via Uniswap V4 Hooks](https://github.com/imbue-bit/no_JIT) è¿›è¡Œåšå¸‚ã€‚ç¬”è€…æ‡’å¾—å‘ä¼šè®®æŠ•ç¨¿äº†ã€‚è‹¥æœ‰ç–‘é—®ï¼Œè¯·è”ç³» imbue2025@outlook.com. BTWï¼Œå¯¹è¯¥ä»“åº“ä»£ç è¿›è¡Œ live trading å‰ä½œé€‚å½“çš„ä¿®æ”¹å¯èƒ½ä¼šå‡ºç°æ„æƒ³ä¸åˆ°çš„ä¸šç»©ã€‚\n\n## What happenedï¼Ÿ\n\nç›®å‰åŒæ–¹å·²è¾¾æˆå’Œè§£ã€‚\n\n## å»ä¸­å¿ƒåŒ–\n\nç›®å‰åŒæ–¹å·²è¾¾æˆå’Œè§£ã€‚\n\n## è´£ä»»å…é™¤\n\nç›®å‰åŒæ–¹å·²è¾¾æˆå’Œè§£ã€‚\n\n## Abstract\n\nç›®å‰åŒæ–¹å·²è¾¾æˆå’Œè§£ã€‚\n\n## Motivation\n\nç›®å‰åŒæ–¹å·²è¾¾æˆå’Œè§£ã€‚\n\n## This was their money-making machine. Now it's your public library.\n\nç›®å‰åŒæ–¹å·²è¾¾æˆå’Œè§£ã€‚\n\n## OH! NO!\n\nç›®å‰åŒæ–¹å·²è¾¾æˆå’Œè§£ã€‚\n\n## Star History\n\n[![Star History Chart](https://api.star-history.com/svg?repos=imbue-bit/AlphaGPT&type=date&legend=top-left)](https://www.star-history.com/#imbue-bit/AlphaGPT&type=date&legend=top-left)\n",
      "default_branch": "main"
    },
    "fetched_at": "2026-02-01T03:45:08.788350"
  },
  {
    "basic_info": {
      "name": "lingbot-world",
      "full_name": "Robbyant/lingbot-world",
      "owner": "Robbyant",
      "description": "Advancing Open-source World Models",
      "url": "https://github.com/Robbyant/lingbot-world",
      "clone_url": "https://github.com/Robbyant/lingbot-world.git",
      "ssh_url": "git@github.com:Robbyant/lingbot-world.git",
      "homepage": "https://technology.robbyant.com/lingbot-world",
      "created_at": "2026-01-28T04:52:50Z",
      "updated_at": "2026-02-01T03:41:12Z",
      "pushed_at": "2026-01-30T07:15:16Z"
    },
    "stats": {
      "stars": 1339,
      "forks": 64,
      "watchers": 1339,
      "open_issues": 11,
      "size": 47025
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 594764
      },
      "license": "Apache License 2.0",
      "topics": [
        "aigc",
        "image-to-video",
        "lingbot-world",
        "video-generation",
        "world-models"
      ]
    },
    "content": {
      "readme": "<div align=\"center\">\n  <img src=\"assets/teaser.png\">\n\n<h1>LingBot-World: Advancing Open-source World Models</h1>\n\nRobbyant Team\n\n</div>\n\n\n<div align=\"center\">\n\n[![Page](https://img.shields.io/badge/%F0%9F%8C%90%20Project%20Page-Demo-00bfff)](https://technology.robbyant.com/lingbot-world)\n[![Tech Report](https://img.shields.io/badge/%F0%9F%93%84%20Tech%20Report-Document-teal)](LingBot_World_paper.pdf)\n[![Paper](https://img.shields.io/static/v1?label=Paper&message=PDF&color=red&logo=arxiv)](https://arxiv.org/abs/2601.20540)\n[![Model](https://img.shields.io/static/v1?label=%F0%9F%A4%97%20Model&message=HuggingFace&color=yellow)](https://huggingface.co/robbyant/lingbot-world-base-cam)\n[![Model](https://img.shields.io/static/v1?label=%F0%9F%A4%96%20Model&message=ModelScope&color=purple)](https://www.modelscope.cn/models/Robbyant/lingbot-world-base-cam)\n[![License](https://img.shields.io/badge/License-Apache--2.0-green)](LICENSE.txt)\n\n\n</div>\n\n-----\n\nWe are excited to introduce **LingBot-World**, an open-sourced world simulator stemming from video generation. Positioned\nas a top-tier world model, LingBot-World offers the following features. \n- **High-Fidelity & Diverse Environments**: It maintains high fidelity and robust dynamics in a broad spectrum of environments, including realism, scientific contexts, cartoon styles, and beyond. \n- **Long-Term Memory & Consistency**: It enables a minute-level horizon while preserving contextual consistency over time, which is also known as long-term memory. \n- **Real-Time Interactivity & Open Access**: It supports real-time interactivity, achieving a latency of under 1 second when producing 16 frames per second. We provide public access to the code and model in an effort to narrow the divide between open-source and closed-source technologies. We believe our release will empower the community with practical applications across areas like content creation, gaming, and robot learning.\n\n## ğŸ¬ Video Demo\n<div align=\"center\">\n  <video src=\"https://github.com/user-attachments/assets/ea4a7a8d-5d9e-4ccf-96e7-02f93797116e\" width=\"100%\" poster=\"\"> </video>\n</div>\n\n## ğŸ”¥ News\n- Jan 29, 2026: ğŸ‰ We release the technical report, code, and models for LingBot-World.\n\n<!-- ## ğŸ”– Introduction of LingBot-World\nWe present **LingBot-World**, an **open-sourced** world simulator stemming from video generation. Positioned\nas a top-tier world model, LingBot-World offers the following features. \n- It maintains high fidelity and robust dynamics in a broad spectrum of environments, including realism, scientific contexts, cartoon styles, and beyond. \n- It enables a minute-level horizon while preserving contextual consistency over time, which is also known as **long-term memory**. \n- It supports real-time interactivity, achieving a latency of under 1 second when producing 16 frames per second. We provide public access to the code and model in an effort to narrow the divide between open-source and closed-source technologies. We believe our release will empower the community with practical applications across areas like content creation, gaming, and robot learning. -->\n\n## âš™ï¸ Quick Start\nThis codebase is built upon [Wan2.2](https://github.com/Wan-Video/Wan2.2). Please refer to their documentation for installation instructions.\n### Installation\nClone the repo:\n```sh\ngit clone https://github.com/robbyant/lingbot-world.git\ncd lingbot-world\n```\nInstall dependencies:\n```sh\n# Ensure torch >= 2.4.0\npip install -r requirements.txt\n```\nInstall [`flash_attn`](https://github.com/Dao-AILab/flash-attention):\n```sh\npip install flash-attn --no-build-isolation\n```\n### Model Download\n\n| Model | Control Signals | Resolution | Download Links |\n| :---  | :--- | :--- | :--- |\n| **LingBot-World-Base (Cam)** | Camera Poses | 480P & 720P | ğŸ¤— [HuggingFace](https://huggingface.co/robbyant/lingbot-world-base-cam) ğŸ¤– [ModelScope](https://www.modelscope.cn/models/Robbyant/lingbot-world-base-cam) |\n| **LingBot-World-Base (Act)** | Actions | - | *To be released* |\n| **LingBot-World-Fast**       |    -    | - | *To be released* |\n\nDownload models using huggingface-cli:\n```sh\npip install \"huggingface_hub[cli]\"\nhuggingface-cli download robbyant/lingbot-world-base-cam --local-dir ./lingbot-world-base-cam\n```\nDownload models using modelscope-cli:\n ```sh\npip install modelscope\nmodelscope download robbyant/lingbot-world-base-cam --local_dir ./lingbot-world-base-cam\n```\n### Inference\nBefore running inference, you need to prepare:\n- Input image\n- Text prompt\n- Control signals (optional, can be generated from a video using [ViPE](https://github.com/nv-tlabs/vipe))\n  - `intrinsics.npy`: Shape `[num_frames, 4]`, where the 4 values represent `[fx, fy, cx, cy]`\n  - `poses.npy`: Shape `[num_frames, 4, 4]`, where each `[4, 4]` represents a transformation matrix in OpenCV coordinates\n\n- 480P:\n``` sh\ntorchrun --nproc_per_node=8 generate.py --task i2v-A14B --size 480*832 --ckpt_dir lingbot-world-base-cam --image examples/00/image.jpg --action_path examples/00 -",
      "default_branch": "main"
    },
    "fetched_at": "2026-02-01T03:45:09.894227"
  },
  {
    "basic_info": {
      "name": "zlibrary-to-notebooklm",
      "full_name": "zstmfhy/zlibrary-to-notebooklm",
      "owner": "zstmfhy",
      "description": "ä¸€é”®å°† Z-Library ä¹¦ç±è‡ªåŠ¨ä¸‹è½½å¹¶ä¸Šä¼ åˆ° Google NotebookLM",
      "url": "https://github.com/zstmfhy/zlibrary-to-notebooklm",
      "clone_url": "https://github.com/zstmfhy/zlibrary-to-notebooklm.git",
      "ssh_url": "git@github.com:zstmfhy/zlibrary-to-notebooklm.git",
      "homepage": null,
      "created_at": "2026-01-14T00:27:42Z",
      "updated_at": "2026-02-01T03:20:35Z",
      "pushed_at": "2026-01-17T09:13:05Z"
    },
    "stats": {
      "stars": 1268,
      "forks": 156,
      "watchers": 1268,
      "open_issues": 4,
      "size": 39
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 35884
      },
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "# ğŸ“š Z-Library to NotebookLM\n\n[English](README.md) | [ç®€ä½“ä¸­æ–‡](README.zh-CN.md)\n\n> Automatically download books from Z-Library and upload them to Google NotebookLM with one command.\n\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)\n[![Claude Skill](https://img.shields.io/badge/Claude-Skill-success.svg)](https://claude.ai/claude-code)\n\n---\n\n## âš ï¸ Important Disclaimer\n\n**This project is for educational, research, and technical demonstration purposes only. Please strictly comply with local laws and copyright regulations. Use only for:**\n\n- âœ… Resources you have legal access to\n- âœ… Public domain or open-source licensed documents (e.g., arXiv, Project Gutenberg)\n- âœ… Content you personally own or have authorization to use\n\n**The author does not encourage or support any form of copyright infringement and assumes no legal liability. Use at your own risk.**\n\n**Please respect intellectual property rights and support authorized reading!**\n\n---\n\n## âœ¨ Features\n\n- ğŸ” **One-time Login, Forever Use** - Similar to `notebooklm login` experience\n- ğŸ“¥ **Smart Download** - Prioritizes PDF (preserves formatting), auto-fallback to EPUB â†’ Markdown\n- ğŸ“¦ **Smart Chunking** - Large files auto-split (>350k words) for reliable CLI upload\n- ğŸ¤– **Fully Automated** - Complete workflow with a single command\n- ğŸ¯ **Format Adaptive** - Automatically detects and processes multiple formats (PDF, EPUB, MOBI, etc.)\n- ğŸ“Š **Visual Progress** - Real-time display of download and conversion progress\n\n## ğŸ¯ Use as Claude Skill (Recommended)\n\n### Installation\n\n```bash\n# 1. Navigate to Claude Skills directory\ncd ~/.claude/skills  # Windows: %APPDATA%\\Claude\\skills\n\n# 2. Clone the repository\ngit clone https://github.com/zstmfhy/zlibrary-to-notebooklm.git zlib-to-notebooklm\n\n# 3. Complete initial login\ncd zlib-to-notebooklm\npython3 scripts/login.py\n```\n\n### Usage\n\nAfter installation, simply tell Claude Code:\n\n```text\nUse zlib-to-notebooklm skill to process this Z-Library link:\nhttps://zh.zlib.li/book/25314781/aa05a1/book-title\n```\n\nClaude will automatically:\n\n- Download the book (prioritizing PDF)\n- Create NotebookLM notebook\n- Upload the file\n- Return notebook ID\n- Suggest follow-up questions\n\n---\n\n## ğŸ› ï¸ Traditional Installation\n\n### 1. Install Dependencies\n\n```bash\n# Clone repository\ngit clone https://github.com/zstmfhy/zlibrary-to-notebooklm.git\ncd zlibrary-to-notebooklm\n\n# Install Python dependencies\npip install playwright ebooklib\n\n# Install Playwright browser\nplaywright install chromium\n```\n\n### 2. Login to Z-Library (One-time Only)\n\n```bash\npython3 scripts/login.py\n```\n\n**Steps:**\n1. Browser will automatically open and visit Z-Library\n2. Complete login in the browser\n3. Return to terminal and press **ENTER**\n4. Session saved!\n\n### 3. Download and Upload Books\n\n```bash\npython3 scripts/upload.py \"https://zh.zlib.li/book/...\"\n```\n\n**Automatically completes:**\n\n- âœ… Login using saved session\n- âœ… Download PDF (preserves formatting)\n- âœ… Fallback to EPUB â†’ Markdown\n- âœ… Smart chunking for large files (>350k words)\n- âœ… Create NotebookLM notebook\n- âœ… Upload content\n- âœ… Return notebook ID\n\n## ğŸ“– Usage Examples\n\n### Basic Usage\n\n```bash\n# Download single book\npython3 scripts/upload.py \"https://zh.zlib.li/book/12345/...\"\n```\n\n### Batch Processing\n\n```bash\n# Batch download multiple books\nfor url in \"url1\" \"url2\" \"url3\"; do\n    python3 scripts/upload.py \"$url\"\ndone\n```\n\n### Using NotebookLM\n\n```bash\n# After upload, use the notebook\nnotebooklm use <returned-notebook-id>\n\n# Start asking questions\nnotebooklm ask \"What are the core concepts of this book?\"\nnotebooklm ask \"Summarize Chapter 3\"\n```\n\n## ğŸ”„ Workflow\n\n```text\nZ-Library URL\n    â†“\n1. Launch browser (using saved session)\n    â†“\n2. Visit book page\n    â†“\n3. Smart format selection:\n   - Priority: PDF (preserves formatting)\n   - Fallback: EPUB (convert to Markdown)\n   - Other formats (auto-convert)\n    â†“\n4. Download to ~/Downloads\n    â†“\n5. Format processing:\n   - PDF â†’ Use directly\n   - EPUB â†’ Convert to Markdown\n   - Check file size â†’ Auto-chunk if >350k words\n    â†“\n6. Create NotebookLM notebook\n    â†“\n7. Upload content (chunked files uploaded individually)\n    â†“\n8. Return notebook ID âœ…\n```\n\n## ğŸ“ Project Structure\n\n```text\nzlibrary-to-notebooklm/\nâ”œâ”€â”€ SKILL.md              # Core Skill definition (required)\nâ”œâ”€â”€ README.md             # Project documentation\nâ”œâ”€â”€ README.zh-CN.md       # Chinese documentation\nâ”œâ”€â”€ LICENSE               # MIT License\nâ”œâ”€â”€ package.json          # npm config (for Claude Code skill)\nâ”œâ”€â”€ skill.yaml            # Skill configuration\nâ”œâ”€â”€ requirements.txt      # Python dependencies\nâ”œâ”€â”€ scripts/              # Executable scripts (official standard)\nâ”‚   â”œâ”€â”€ login.py         # Login script\nâ”‚   â”œâ”€â”€ upload.py        # Download + Upload script\nâ”‚   â””â”€â”€ convert_epub.py  # EPUB conversion tool\nâ”œâ”€â”€ docs/                 # Documentation\nâ”‚   â”œâ”€â”€ WORKFLOW.md      #",
      "default_branch": "main"
    },
    "fetched_at": "2026-02-01T03:45:11.009747"
  }
]