[
  {
    "basic_info": {
      "name": "nanochat",
      "full_name": "karpathy/nanochat",
      "owner": "karpathy",
      "description": "The best ChatGPT that $100 can buy.",
      "url": "https://github.com/karpathy/nanochat",
      "clone_url": "https://github.com/karpathy/nanochat.git",
      "ssh_url": "git@github.com:karpathy/nanochat.git",
      "homepage": "",
      "created_at": "2025-10-13T13:46:35Z",
      "updated_at": "2025-10-25T02:19:41Z",
      "pushed_at": "2025-10-24T15:06:08Z"
    },
    "stats": {
      "stars": 32167,
      "forks": 3489,
      "watchers": 32167,
      "open_issues": 90,
      "size": 191
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 322549,
        "HTML": 20192,
        "Rust": 16627,
        "Shell": 14077
      },
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "# nanochat\n\n![nanochat logo](dev/nanochat.png)\n\n> The best ChatGPT that $100 can buy.\n\nThis repo is a full-stack implementation of an LLM like ChatGPT in a single, clean, minimal, hackable, dependency-lite codebase. nanochat is designed to run on a single 8XH100 node via scripts like [speedrun.sh](speedrun.sh), that run the entire pipeline start to end. This includes tokenization, pretraining, finetuning, evaluation, inference, and web serving over a simple UI so that you can talk to your own LLM just like ChatGPT. nanochat will become the capstone project of the course LLM101n being developed by Eureka Labs.\n\n## Talk to it\n\nTo get a sense of the endpoint of this repo, you can currently find [nanochat d32](https://github.com/karpathy/nanochat/discussions/8) hosted on [nanochat.karpathy.ai](https://nanochat.karpathy.ai/). \"d32\" means that this model has 32 layers in the Transformer neural network. This model has 1.9 billion parameters, it was trained on 38 billion tokens by simply running the single script [run1000.sh](run1000.sh), and the total cost of training was ~$800 (about 33 hours training time on 8XH100 GPU node). While today this is enough to outperform GPT-2 of 2019, it falls dramatically short of moden Large Language Models like GPT-5. When talking to these micro models, you'll see that they make a lot of mistakes, they are a little bit naive and silly and they hallucinate a ton, a bit like children. It's kind of amusing. But what makes nanochat unique is that it is fully yours - fully configurable, tweakable, hackable, and trained by you from start to end. To train and talk to your own, we turn to...\n\n## Quick start\n\nThe fastest way to feel the magic is to run the speedrun script [speedrun.sh](speedrun.sh), which trains and inferences the $100 tier of nanochat. On an 8XH100 node at $24/hr, this gives a total run time of about 4 hours. Boot up a new 8XH100 GPU box from your favorite provider (e.g. I use and like [Lambda](https://lambda.ai/service/gpu-cloud)), and kick off the training script:\n\n```bash\nbash speedrun.sh\n```\n\nAlternatively, since the script runs for 4 hours, I like to launch it like this inside a new screen session `speedrun` (and also log output to `speedrun.log`):\n\n```bash\nscreen -L -Logfile speedrun.log -S speedrun bash speedrun.sh\n```\n\nSee the [screen cheatsheet](https://gist.github.com/jctosta/af918e1618682638aa82) if you are less familiar. You can watch it go inside the screen session, or detach with `Ctrl-a d` and `tail speedrun.log` to view progress. Now wait 4 hours. Once it's done, you can talk to your LLM via the ChatGPT-like web UI. Make sure again that your local uv virtual environment is active (run `source .venv/bin/activate`), and serve it:\n\n```bash\npython -m scripts.chat_web\n```\n\nAnd then visit the URL shown. Make sure to access it correctly, e.g. on Lambda use the public IP of the node you're on, followed by the port, so for example [http://209.20.xxx.xxx:8000/](http://209.20.xxx.xxx:8000/), etc. Then talk to your LLM as you'd normally talk to ChatGPT! Get it to write stories or poems. Ask it to tell you who you are to see a hallucination. Ask it why the sky is blue. Or why it's green. The speedrun is a 4e19 FLOPs capability model so it's a bit like talking to a kindergartener :).\n\n---\n\n<img width=\"2672\" height=\"1520\" alt=\"image\" src=\"https://github.com/user-attachments/assets/ed39ddf8-2370-437a-bedc-0f39781e76b5\" />\n\n---\n\nYou can also `cat report.md` file which appeared in the project directory and contains the \"report card\" of the run, i.e. a bunch of evaluations and metrics. At the very end, you'll see a summary table, for example:\n\n---\n\n- Characters: 333,989\n- Lines: 8,304\n- Files: 44\n- Tokens (approx): 83,497\n- Dependencies (uv.lock lines): 2,004\n\n| Metric          | BASE     | MID      | SFT      | RL       |\n|-----------------|----------|----------|----------|----------|\n| CORE            | 0.2219   | -        | -        | -        |\n| ARC-Challenge   | -        | 0.2875   | 0.2807   | -        |\n| ARC-Easy        | -        | 0.3561   | 0.3876   | -        |\n| GSM8K           | -        | 0.0250   | 0.0455   | 0.0758   |\n| HumanEval       | -        | 0.0671   | 0.0854   | -        |\n| MMLU            | -        | 0.3111   | 0.3151   | -        |\n| ChatCORE        | -        | 0.0730   | 0.0884   | -        |\n\nTotal wall clock time: 3h51m\n\n---\n\n(Your table might be missing the RL number by default). For a lot more information around the speedrun script and what to look for and expect, please refer to the walkthrough that I posted in Discussions of the repo: [\"Introducing nanochat: The best ChatGPT that $100 can buy\"](https://github.com/karpathy/nanochat/discussions/1).\n\n## Bigger models\n\nUnsurprisingly, $100 is not enough to train a highly performant ChatGPT clone. In fact, LLMs are famous for their multi-million dollar capex. For our purposes, I think there are two more scales of interest. First is the ~$300 tier d26 model (i.e. depth=26) that trains in ~12",
      "default_branch": "master"
    },
    "fetched_at": "2025-10-25T02:20:44.505819"
  },
  {
    "basic_info": {
      "name": "DeepSeek-OCR",
      "full_name": "deepseek-ai/DeepSeek-OCR",
      "owner": "deepseek-ai",
      "description": "Contexts Optical Compression",
      "url": "https://github.com/deepseek-ai/DeepSeek-OCR",
      "clone_url": "https://github.com/deepseek-ai/DeepSeek-OCR.git",
      "ssh_url": "git@github.com:deepseek-ai/DeepSeek-OCR.git",
      "homepage": null,
      "created_at": "2025-10-17T06:14:27Z",
      "updated_at": "2025-10-25T02:15:10Z",
      "pushed_at": "2025-10-23T04:54:11Z"
    },
    "stats": {
      "stars": 16805,
      "forks": 1029,
      "watchers": 16805,
      "open_issues": 125,
      "size": 7954
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 113538
      },
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "<!-- markdownlint-disable first-line-h1 -->\n<!-- markdownlint-disable html -->\n<!-- markdownlint-disable no-duplicate-header -->\n\n\n<div align=\"center\">\n  <img src=\"assets/logo.svg\" width=\"60%\" alt=\"DeepSeek AI\" />\n</div>\n\n\n<hr>\n<div align=\"center\">\n  <a href=\"https://www.deepseek.com/\" target=\"_blank\">\n    <img alt=\"Homepage\" src=\"assets/badge.svg\" />\n  </a>\n  <a href=\"https://huggingface.co/deepseek-ai/DeepSeek-OCR\" target=\"_blank\">\n    <img alt=\"Hugging Face\" src=\"https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-DeepSeek%20AI-ffc107?color=ffc107&logoColor=white\" />\n  </a>\n\n</div>\n\n<div align=\"center\">\n\n  <a href=\"https://discord.gg/Tc7c45Zzu5\" target=\"_blank\">\n    <img alt=\"Discord\" src=\"https://img.shields.io/badge/Discord-DeepSeek%20AI-7289da?logo=discord&logoColor=white&color=7289da\" />\n  </a>\n  <a href=\"https://twitter.com/deepseek_ai\" target=\"_blank\">\n    <img alt=\"Twitter Follow\" src=\"https://img.shields.io/badge/Twitter-deepseek_ai-white?logo=x&logoColor=white\" />\n  </a>\n\n</div>\n\n\n\n<p align=\"center\">\n  <a href=\"https://huggingface.co/deepseek-ai/DeepSeek-OCR\"><b>📥 Model Download</b></a> |\n  <a href=\"https://github.com/deepseek-ai/DeepSeek-OCR/blob/main/DeepSeek_OCR_paper.pdf\"><b>📄 Paper Link</b></a> |\n  <a href=\"https://arxiv.org/abs/2510.18234\"><b>📄 Arxiv Paper Link</b></a> |\n</p>\n\n<h2>\n<p align=\"center\">\n  <a href=\"\">DeepSeek-OCR: Contexts Optical Compression</a>\n</p>\n</h2>\n\n<p align=\"center\">\n<img src=\"assets/fig1.png\" style=\"width: 1000px\" align=center>\n</p>\n<p align=\"center\">\n<a href=\"\">Explore the boundaries of visual-text compression.</a>       \n</p>\n\n## Release\n- [2025/10/23]🚀🚀🚀 DeepSeek-OCR is now officially supported in upstream [vLLM](https://docs.vllm.ai/projects/recipes/en/latest/DeepSeek/DeepSeek-OCR.html#installing-vllm). Thanks to the [vLLM](https://github.com/vllm-project/vllm) team for their help.\n- [2025/10/20]🚀🚀🚀 We release DeepSeek-OCR, a model to investigate the role of vision encoders from an LLM-centric viewpoint.\n\n## Contents\n- [Install](#install)\n- [vLLM Inference](#vllm-inference)\n- [Transformers Inference](#transformers-inference)\n  \n\n\n\n\n## Install\n>Our environment is cuda11.8+torch2.6.0.\n1. Clone this repository and navigate to the DeepSeek-OCR folder\n```bash\ngit clone https://github.com/deepseek-ai/DeepSeek-OCR.git\n```\n2. Conda\n```Shell\nconda create -n deepseek-ocr python=3.12.9 -y\nconda activate deepseek-ocr\n```\n3. Packages\n\n- download the vllm-0.8.5 [whl](https://github.com/vllm-project/vllm/releases/tag/v0.8.5) \n```Shell\npip install torch==2.6.0 torchvision==0.21.0 torchaudio==2.6.0 --index-url https://download.pytorch.org/whl/cu118\npip install vllm-0.8.5+cu118-cp38-abi3-manylinux1_x86_64.whl\npip install -r requirements.txt\npip install flash-attn==2.7.3 --no-build-isolation\n```\n**Note:** if you want vLLM and transformers codes to run in the same environment, you don't need to worry about this installation error like: vllm 0.8.5+cu118 requires transformers>=4.51.1\n\n## vLLM-Inference\n- VLLM:\n>**Note:** change the INPUT_PATH/OUTPUT_PATH and other settings in the DeepSeek-OCR-master/DeepSeek-OCR-vllm/config.py\n```Shell\ncd DeepSeek-OCR-master/DeepSeek-OCR-vllm\n```\n1. image: streaming output\n```Shell\npython run_dpsk_ocr_image.py\n```\n2. pdf: concurrency ~2500tokens/s(an A100-40G)\n```Shell\npython run_dpsk_ocr_pdf.py\n```\n3. batch eval for benchmarks\n```Shell\npython run_dpsk_ocr_eval_batch.py\n```\n\n**[2025/10/23] The version of upstream [vLLM](https://docs.vllm.ai/projects/recipes/en/latest/DeepSeek/DeepSeek-OCR.html#installing-vllm):**\n\n```shell\nuv venv\nsource .venv/bin/activate\n# Until v0.11.1 release, you need to install vLLM from nightly build\nuv pip install -U vllm --pre --extra-index-url https://wheels.vllm.ai/nightly\n```\n\n```python\nfrom vllm import LLM, SamplingParams\nfrom vllm.model_executor.models.deepseek_ocr import NGramPerReqLogitsProcessor\nfrom PIL import Image\n\n# Create model instance\nllm = LLM(\n    model=\"deepseek-ai/DeepSeek-OCR\",\n    enable_prefix_caching=False,\n    mm_processor_cache_gb=0,\n    logits_processors=[NGramPerReqLogitsProcessor]\n)\n\n# Prepare batched input with your image file\nimage_1 = Image.open(\"path/to/your/image_1.png\").convert(\"RGB\")\nimage_2 = Image.open(\"path/to/your/image_2.png\").convert(\"RGB\")\nprompt = \"<image>\\nFree OCR.\"\n\nmodel_input = [\n    {\n        \"prompt\": prompt,\n        \"multi_modal_data\": {\"image\": image_1}\n    },\n    {\n        \"prompt\": prompt,\n        \"multi_modal_data\": {\"image\": image_2}\n    }\n]\n\nsampling_param = SamplingParams(\n            temperature=0.0,\n            max_tokens=8192,\n            # ngram logit processor args\n            extra_args=dict(\n                ngram_size=30,\n                window_size=90,\n                whitelist_token_ids={128821, 128822},  # whitelist: <td>, </td>\n            ),\n            skip_special_tokens=False,\n        )\n# Generate output\nmodel_outputs = llm.generate(model_input, sampling_param)\n\n# Print output\nfor output in model_outputs:\n    print(output.outputs[0].text)\n```\n## ",
      "default_branch": "main"
    },
    "fetched_at": "2025-10-25T02:20:45.774872"
  },
  {
    "basic_info": {
      "name": "TinyRecursiveModels",
      "full_name": "SamsungSAILMontreal/TinyRecursiveModels",
      "owner": "SamsungSAILMontreal",
      "description": null,
      "url": "https://github.com/SamsungSAILMontreal/TinyRecursiveModels",
      "clone_url": "https://github.com/SamsungSAILMontreal/TinyRecursiveModels.git",
      "ssh_url": "git@github.com:SamsungSAILMontreal/TinyRecursiveModels.git",
      "homepage": null,
      "created_at": "2025-10-07T13:24:28Z",
      "updated_at": "2025-10-25T01:42:17Z",
      "pushed_at": "2025-10-08T19:46:47Z"
    },
    "stats": {
      "stars": 5146,
      "forks": 682,
      "watchers": 5146,
      "open_issues": 24,
      "size": 1266
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 147529
      },
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "# Less is More: Recursive Reasoning with Tiny Networks\n\nThis is the codebase for the paper: \"Less is More: Recursive Reasoning with Tiny Networks\". TRM is a recursive reasoning approach that achieves amazing scores of 45% on ARC-AGI-1 and 8% on ARC-AGI-2 using a tiny 7M parameters neural network.\n\n[Paper](https://arxiv.org/abs/2510.04871)\n\n### Motivation\n\nTiny Recursion Model (TRM) is a recursive reasoning model that achieves amazing scores of 45% on ARC-AGI-1 and 8% on ARC-AGI-2 with a tiny 7M parameters neural network. The idea that one must rely on massive foundational models trained for millions of dollars by some big corporation in order to achieve success on hard tasks is a trap. Currently, there is too much focus on exploiting LLMs rather than devising and expanding new lines of direction. With recursive reasoning, it turns out that “less is more”: you don’t always need to crank up model size in order for a model to reason and solve hard problems. A tiny model pretrained from scratch, recursing on itself and updating its answers over time, can achieve a lot without breaking the bank.\n\nThis work came to be after I learned about the recent innovative Hierarchical Reasoning Model (HRM). I was amazed that an approach using small models could do so well on hard tasks like the ARC-AGI competition (reaching 40% accuracy when normally only Large Language Models could compete). But I kept thinking that it is too complicated, relying too much on biological arguments about the human brain, and that this recursive reasoning process could be greatly simplified and improved. Tiny Recursion Model (TRM) simplifies recursive reasoning to its core essence, which ultimately has nothing to do with the human brain, does not require any mathematical (fixed-point) theorem, nor any hierarchy.\n\n### How TRM works\n\n<p align=\"center\">\n  <img src=\"https://AlexiaJM.github.io/assets/images/TRM_fig.png\" alt=\"TRM\"  style=\"width: 30%;\">\n</p>\n\nTiny Recursion Model (TRM) recursively improves its predicted answer y with a tiny network. It starts with the embedded input question x and initial embedded answer y and latent z. For up to K improvements steps, it tries to improve its answer y. It does so by i) recursively updating n times its latent z given the question x, current answer y, and current latent z (recursive reasoning), and then ii) updating its answer y given the current answer y and current latent z. This recursive process allows the model to progressively improve its answer (potentially addressing any errors from its previous answer) in an extremely parameter-efficient manner while minimizing overfitting.\n\n### Requirements\n\n- Python 3.10 (or similar)\n- Cuda 12.6.0 (or similar)\n\n```bash\npip install --upgrade pip wheel setuptools\npip install --pre --upgrade torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu126 # install torch based on your cuda version\npip install -r requirements.txt # install requirements\npip install --no-cache-dir --no-build-isolation adam-atan2 \nwandb login YOUR-LOGIN # login if you want the logger to sync results to your Weights & Biases (https://wandb.ai/)\n```\n\n### Dataset Preparation\n\n```bash\n# ARC-AGI-1\npython -m dataset.build_arc_dataset \\\n  --input-file-prefix kaggle/combined/arc-agi \\\n  --output-dir data/arc1concept-aug-1000 \\\n  --subsets training evaluation concept \\\n  --test-set-name evaluation\n\n# ARC-AGI-2\npython -m dataset.build_arc_dataset \\\n  --input-file-prefix kaggle/combined/arc-agi \\\n  --output-dir data/arc2concept-aug-1000 \\\n  --subsets training2 evaluation2 concept \\\n  --test-set-name evaluation2\n\n## Note: You cannot train on both ARC-AGI-1 and ARC-AGI-2 and evaluate them both because ARC-AGI-2 training data contains some ARC-AGI-1 eval data\n\n# Sudoku-Extreme\npython dataset/build_sudoku_dataset.py --output-dir data/sudoku-extreme-1k-aug-1000  --subsample-size 1000 --num-aug 1000  # 1000 examples, 1000 augments\n\n# Maze-Hard\npython dataset/build_maze_dataset.py # 1000 examples, 8 augments\n```\n\n## Experiments\n\n### ARC-AGI-1 (assuming 4 H-100 GPUs):\n\n```bash\nrun_name=\"pretrain_att_arc1concept_4\"\ntorchrun --nproc-per-node 4 --rdzv_backend=c10d --rdzv_endpoint=localhost:0 --nnodes=1 pretrain.py \\\narch=trm \\\ndata_paths=\"[data/arc1concept-aug-1000]\" \\\narch.L_layers=2 \\\narch.H_cycles=3 arch.L_cycles=4 \\\n+run_name=${run_name} ema=True\n\n```\n\n*Runtime:* ~3 days\n\n### ARC-AGI-2 (assuming 4 H-100 GPUs):\n\n```bash\nrun_name=\"pretrain_att_arc2concept_4\"\ntorchrun --nproc-per-node 4 --rdzv_backend=c10d --rdzv_endpoint=localhost:0 --nnodes=1 pretrain.py \\\narch=trm \\\ndata_paths=\"[data/arc2concept-aug-1000]\" \\\narch.L_layers=2 \\\narch.H_cycles=3 arch.L_cycles=4 \\\n+run_name=${run_name} ema=True\n\n```\n\n*Runtime:* ~3 days\n\n### Sudoku-Extreme (assuming 1 L40S GPU):\n\n```bash\nrun_name=\"pretrain_mlp_t_sudoku\"\npython pretrain.py \\\narch=trm \\\ndata_paths=\"[data/sudoku-extreme-1k-aug-1000]\" \\\nevaluators=\"[]\" \\\nepochs=50000 eval_interval=5000 \\\nlr=1e-4 puzzle_emb_lr=1e-4 weight_decay=1.0 puzzle",
      "default_branch": "main"
    },
    "fetched_at": "2025-10-25T02:20:47.066296"
  },
  {
    "basic_info": {
      "name": "superpowers",
      "full_name": "obra/superpowers",
      "owner": "obra",
      "description": "Claude Code superpowers: core skills library",
      "url": "https://github.com/obra/superpowers",
      "clone_url": "https://github.com/obra/superpowers.git",
      "ssh_url": "git@github.com:obra/superpowers.git",
      "homepage": null,
      "created_at": "2025-10-09T19:45:18Z",
      "updated_at": "2025-10-25T02:07:34Z",
      "pushed_at": "2025-10-23T23:35:12Z"
    },
    "stats": {
      "stars": 4707,
      "forks": 295,
      "watchers": 4707,
      "open_issues": 19,
      "size": 296
    },
    "tech_info": {
      "language": "Shell",
      "languages": {
        "Shell": 6233,
        "TypeScript": 5054
      },
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "# Superpowers\n\nGive Claude Code superpowers with a comprehensive skills library of proven techniques, patterns, and workflows.\n\n## What You Get\n\n- **Testing Skills** - TDD, async testing, anti-patterns\n- **Debugging Skills** - Systematic debugging, root cause tracing, verification\n- **Collaboration Skills** - Brainstorming, planning, code review, parallel agents\n- **Development Skills** - Git worktrees, finishing branches, subagent workflows\n- **Meta Skills** - Creating, testing, and sharing skills\n\nPlus:\n- **Slash Commands** - `/superpowers:brainstorm`, `/superpowers:write-plan`, `/superpowers:execute-plan`\n- **Automatic Integration** - Skills activate automatically when relevant\n- **Consistent Workflows** - Systematic approaches to common engineering tasks\n\n## Learn More\n\nRead the introduction: [Superpowers for Claude Code](https://blog.fsck.com/2025/10/09/superpowers/)\n\n## Installation\n\n### Via Plugin Marketplace (Recommended)\n\n```bash\n# In Claude Code\n/plugin marketplace add obra/superpowers-marketplace\n/plugin install superpowers@superpowers-marketplace\n```\n\n### Verify Installation\n\n```bash\n# Check that commands appear\n/help\n\n# Should see:\n# /superpowers:brainstorm - Interactive design refinement\n# /superpowers:write-plan - Create implementation plan\n# /superpowers:execute-plan - Execute plan in batches\n```\n\n## Quick Start\n\n### Using Slash Commands\n\n**Brainstorm a design:**\n```\n/superpowers:brainstorm\n```\n\n**Create an implementation plan:**\n```\n/superpowers:write-plan\n```\n\n**Execute the plan:**\n```\n/superpowers:execute-plan\n```\n\n### Automatic Skill Activation\n\nSkills activate automatically when relevant. For example:\n- `test-driven-development` activates when implementing features\n- `systematic-debugging` activates when debugging issues\n- `verification-before-completion` activates before claiming work is done\n\n## What's Inside\n\n### Skills Library\n\n**Testing** (`skills/testing/`)\n- **test-driven-development** - RED-GREEN-REFACTOR cycle\n- **condition-based-waiting** - Async test patterns\n- **testing-anti-patterns** - Common pitfalls to avoid\n\n**Debugging** (`skills/debugging/`)\n- **systematic-debugging** - 4-phase root cause process\n- **root-cause-tracing** - Find the real problem\n- **verification-before-completion** - Ensure it's actually fixed\n- **defense-in-depth** - Multiple validation layers\n\n**Collaboration** (`skills/collaboration/`)\n- **brainstorming** - Socratic design refinement\n- **writing-plans** - Detailed implementation plans\n- **executing-plans** - Batch execution with checkpoints\n- **dispatching-parallel-agents** - Concurrent subagent workflows\n- **requesting-code-review** - Pre-review checklist\n- **receiving-code-review** - Responding to feedback\n- **using-git-worktrees** - Parallel development branches\n- **finishing-a-development-branch** - Merge/PR decision workflow\n- **subagent-driven-development** - Fast iteration with quality gates\n\n**Meta** (`skills/meta/`)\n- **writing-skills** - Create new skills following best practices\n- **sharing-skills** - Contribute skills back via branch and PR\n- **testing-skills-with-subagents** - Validate skill quality\n- **using-superpowers** - Introduction to the skills system\n\n### Commands\n\nAll commands are thin wrappers that activate the corresponding skill:\n\n- **brainstorm.md** - Activates the `brainstorming` skill\n- **write-plan.md** - Activates the `writing-plans` skill\n- **execute-plan.md** - Activates the `executing-plans` skill\n\n## How It Works\n\n1. **SessionStart Hook** - Loads the `using-superpowers` skill at session start\n2. **Skills System** - Uses Claude Code's first-party skills system\n3. **Automatic Discovery** - Claude finds and uses relevant skills for your task\n4. **Mandatory Workflows** - When a skill exists for your task, using it becomes required\n\n## Philosophy\n\n- **Test-Driven Development** - Write tests first, always\n- **Systematic over ad-hoc** - Process over guessing\n- **Complexity reduction** - Simplicity as primary goal\n- **Evidence over claims** - Verify before declaring success\n- **Domain over implementation** - Work at problem level, not solution level\n\n## Contributing\n\nSkills live directly in this repository. To contribute:\n\n1. Fork the repository\n2. Create a branch for your skill\n3. Follow the `writing-skills` skill for creating new skills\n4. Use the `testing-skills-with-subagents` skill to validate quality\n5. Submit a PR\n\nSee `skills/meta/writing-skills/SKILL.md` for the complete guide.\n\n## Updating\n\nSkills update automatically when you update the plugin:\n\n```bash\n/plugin update superpowers\n```\n\n## License\n\nMIT License - see LICENSE file for details\n\n## Support\n\n- **Issues**: https://github.com/obra/superpowers/issues\n- **Marketplace**: https://github.com/obra/superpowers-marketplace\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-10-25T02:20:48.362367"
  },
  {
    "basic_info": {
      "name": "OpenStock",
      "full_name": "Open-Dev-Society/OpenStock",
      "owner": "Open-Dev-Society",
      "description": "OpenStock is an open-source alternative to expensive market platforms. Track real-time prices, set personalized alerts, and explore detailed company insights — built openly, for everyone, forever free.",
      "url": "https://github.com/Open-Dev-Society/OpenStock",
      "clone_url": "https://github.com/Open-Dev-Society/OpenStock.git",
      "ssh_url": "git@github.com:Open-Dev-Society/OpenStock.git",
      "homepage": "https://openstock-ods.vercel.app",
      "created_at": "2025-09-28T18:16:32Z",
      "updated_at": "2025-10-25T01:55:54Z",
      "pushed_at": "2025-10-24T05:25:40Z"
    },
    "stats": {
      "stars": 4150,
      "forks": 483,
      "watchers": 4150,
      "open_issues": 6,
      "size": 2115
    },
    "tech_info": {
      "language": "TypeScript",
      "languages": {
        "TypeScript": 256175,
        "CSS": 16325,
        "JavaScript": 1509,
        "Dockerfile": 698
      },
      "license": "GNU Affero General Public License v3.0",
      "topics": [
        "coderabbit",
        "inngest",
        "nextjs",
        "shadcn-ui",
        "stock-market",
        "tailwindcss"
      ]
    },
    "content": {
      "readme": "<div align=\"center\">\n  Checkout new amazing projects also, <a href=\"github.com/open-dev-society/openreadme\" target=\"_blank\">OpenReadme </a> is live\n</div>  \n<div align=\"center\">\n  <br />\n  <a href=\"#\" target=\"_blank\">\n    <img src=\"./public/assets/images/dashboard.png\" alt=\"Project Banner\" />\n  </a>\n  © Open Dev Society. This project is licensed under AGPL-3.0; if you modify, redistribute, or deploy it (including as a web service), you must release your source code under the same license and credit the original authors.\n  <br />\n  <br/>\n\n  <div>\n    <img src=\"https://img.shields.io/badge/-Next.js-black?style=for-the-badge&logoColor=white&logo=next.js&color=000000\" alt=\"Next.js badge\" />\n    <img src=\"https://img.shields.io/badge/-TypeScript-black?style=for-the-badge&logoColor=white&logo=typescript&color=3178C6\"/>\n    <img src=\"https://img.shields.io/badge/-Tailwind%20CSS-black?style=for-the-badge&logoColor=white&logo=tailwindcss&color=38B2AC\"/>\n    <img src=\"https://img.shields.io/badge/-shadcn/ui-black?style=for-the-badge&logoColor=white&logo=shadcnui&color=000000\"/>\n    <img src=\"https://img.shields.io/badge/-Radix%20UI-black?style=for-the-badge&logoColor=white&logo=radixui&color=000000\"/>\n    <img src=\"https://img.shields.io/badge/-Better%20Auth-black?style=for-the-badge&logoColor=white&logo=betterauth&color=000000\"/>\n    <img src=\"https://img.shields.io/badge/-MongoDB-black?style=for-the-badge&logoColor=white&logo=mongodb&color=00A35C\"/>\n    <img src=\"https://img.shields.io/badge/-Inngest-black?style=for-the-badge&logoColor=white&logo=inngest&color=000000\"/>\n    <img src=\"https://img.shields.io/badge/-Nodemailer-black?style=for-the-badge&logoColor=white&logo=gmail&color=EA4335\"/>\n    <img src=\"https://img.shields.io/badge/-TradingView-black?style=for-the-badge&logoColor=white&logo=tradingview&color=2962FF\"/>\n    <img src=\"https://img.shields.io/badge/-Finnhub-black?style=for-the-badge&logoColor=white&color=30B27A\"/>\n    <img src=\"https://img.shields.io/badge/-CodeRabbit-black?style=for-the-badge&logoColor=white&logo=coderabbit&color=9146FF\"/>\n  </div>\n</div>\n\n# OpenStock\n\nOpenStock is an open-source alternative to expensive market platforms. Track real-time prices, set personalized alerts, and explore detailed company insights — built openly, for everyone, forever free.\n\nNote: OpenStock is community-built and not a brokerage. Market data may be delayed based on provider rules and your configuration. Nothing here is financial advice.\n\n## 📋 Table of Contents\n\n1. ✨ [Introduction](#introduction)\n2. 🌍 [Open Dev Society Manifesto](#manifesto)\n3. ⚙️ [Tech Stack](#tech-stack)\n4. 🔋 [Features](#features)\n5. 🤸 [Quick Start](#quick-start)\n6. 🐳 [Docker Setup](#docker-setup)\n7. 🔐 [Environment Variables](#environment-variables)\n8. 🧱 [Project Structure](#project-structure)\n9. 📡 [Data & Integrations](#data--integrations)\n10. 🧪 [Scripts & Tooling](#scripts--tooling)\n11. 🤝 [Contributing](#contributing)\n12. 🛡️ [Security](#security)\n13. 📜 [License](#license)\n14. 🙏 [Acknowledgements](#acknowledgements)\n\n## ✨ Introduction\n\nOpenStock is a modern stock market app powered by Next.js (App Router), shadcn/ui and Tailwind CSS, Better Auth for authentication, MongoDB for persistence, Finnhub for market data, and TradingView widgets for charts and market views.\n\n## 🌍 Open Dev Society Manifesto <a name=\"manifesto\"></a>\n\nWe live in a world where knowledge is hidden behind paywalls. Where tools are locked in subscriptions. Where information is twisted by bias. Where newcomers are told they’re not “good enough” to build.\n\nWe believe there’s a better way.\n\n- Our Belief: Technology should belong to everyone. Knowledge should be open, free, and accessible. Communities should welcome newcomers with trust, not gatekeeping.\n- Our Mission: Build free, open-source projects that make a real difference:\n    - Tools that professionals and students can use without barriers.\n    - Knowledge platforms where learning is free, forever.\n    - Communities where every beginner is guided, not judged.\n    - Resources that run on trust, not profit.\n- Our Promise: We will never lock knowledge. We will never charge for access. We will never trade trust for money. We run on transparency, donations, and the strength of our community.\n- Our Call: If you’ve ever felt you didn’t belong, struggled to find free resources, or wanted to build something meaningful — you belong here.\n\nBecause the future belongs to those who build it openly.\n\n## ⚙️ Tech Stack\n\nCore\n- Next.js 15 (App Router), React 19\n- TypeScript\n- Tailwind CSS v4 (via @tailwindcss/postcss)\n- shadcn/ui + Radix UI primitives\n- Lucide icons\n\nAuth & Data\n- Better Auth (email/password) with MongoDB adapter\n- MongoDB + Mongoose\n- Finnhub API for symbols, profiles, and market news\n- TradingView embeddable widgets\n\nAutomation & Comms\n- Inngest (events, cron, AI inference via Gemini)\n- Nodemailer (Gmail transport)\n- next-themes, cmdk (command palette), react-hook-form\n\nLanguage composition\n- TypeScript (~93.4%), C",
      "default_branch": "main"
    },
    "fetched_at": "2025-10-25T02:20:49.641337"
  },
  {
    "basic_info": {
      "name": "neutts-air",
      "full_name": "neuphonic/neutts-air",
      "owner": "neuphonic",
      "description": "On-device TTS model by Neuphonic",
      "url": "https://github.com/neuphonic/neutts-air",
      "clone_url": "https://github.com/neuphonic/neutts-air.git",
      "ssh_url": "git@github.com:neuphonic/neutts-air.git",
      "homepage": null,
      "created_at": "2025-10-02T12:48:55Z",
      "updated_at": "2025-10-25T00:23:26Z",
      "pushed_at": "2025-10-17T10:20:23Z"
    },
    "stats": {
      "stars": 3651,
      "forks": 344,
      "watchers": 3651,
      "open_issues": 32,
      "size": 1906
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 14928
      },
      "license": "Apache License 2.0",
      "topics": []
    },
    "content": {
      "readme": "# NeuTTS Air ☁️\n\nHuggingFace 🤗: [Model](https://huggingface.co/neuphonic/neutts-air), [Q8 GGUF](https://huggingface.co/neuphonic/neutts-air-q8-gguf), [Q4 GGUF](https://huggingface.co/neuphonic/neutts-air-q4-gguf) [Spaces](https://huggingface.co/spaces/neuphonic/neutts-air)\n\n[Demo Video](https://github.com/user-attachments/assets/020547bc-9e3e-440f-b016-ae61ca645184)\n\n_Created by [Neuphonic](http://neuphonic.com/) - building faster, smaller, on-device voice AI_\n\nState-of-the-art Voice AI has been locked behind web APIs for too long. NeuTTS Air is the world’s first super-realistic, on-device, TTS speech language model with instant voice cloning. Built off a 0.5B LLM backbone, NeuTTS Air brings natural-sounding speech, real-time performance, built-in security and speaker cloning to your local device - unlocking a new category of embedded voice agents, assistants, toys, and compliance-safe apps.\n\n## Key Features\n\n- 🗣Best-in-class realism for its size - produces natural, ultra-realistic voices that sound human\n- 📱Optimised for on-device deployment - provided in GGML format, ready to run on phones, laptops, or even Raspberry Pis\n- 👫Instant voice cloning - create your own speaker with as little as 3 seconds of audio\n- 🚄Simple LM + codec architecture built off a 0.5B backbone - the sweet spot between speed, size, and quality for real-world applications\n\n> [!CAUTION]\n> Websites like neutts.com are popping up and they're not affliated with Neuphonic, our github or this repo.\n>\n> We are on neuphonic.com only. Please be careful out there! 🙏\n\n## Model Details\n\nNeuTTS Air is built off Qwen 0.5B - a lightweight yet capable language model optimised for text understanding and generation - as well as a powerful combination of technologies designed for efficiency and quality:\n\n- **Supported Languages**: English\n- **Audio Codec**: [NeuCodec](https://huggingface.co/neuphonic/neucodec) - our 50hz neural audio codec that achieves exceptional audio quality at low bitrates using a single codebook\n- **Context Window**: 2048 tokens, enough for processing ~30 seconds of audio (including prompt duration)\n- **Format**: Available in GGML format for efficient on-device inference\n- **Responsibility**: Watermarked outputs\n- **Inference Speed**: Real-time generation on mid-range devices\n- **Power Consumption**: Optimised for mobile and embedded devices\n\n## Get Started\n\n> [!NOTE]\n> We have added a [streaming example](examples/basic_streaming_example.py) using the `llama-cpp-python` library as well as a [finetuning script](examples/finetune.py). For finetuning, please refer to the [finetune guide](TRAINING.md) for more details.\n\n1. **Clone Git Repo**\n\n   ```bash\n   git clone https://github.com/neuphonic/neutts-air.git\n   cd neutts-air\n   ```\n\n2. **Install `espeak` (required dependency)**\n\n   Please refer to the following link for instructions on how to install `espeak`:\n\n   https://github.com/espeak-ng/espeak-ng/blob/master/docs/guide.md\n\n   ```bash\n   # Mac OS\n   brew install espeak\n\n   # Ubuntu/Debian\n   sudo apt install espeak\n   ```\n\n   Mac users may need to put the following lines at the top of the neutts.py file.\n\n   ```python\n   from phonemizer.backend.espeak.wrapper import EspeakWrapper\n   _ESPEAK_LIBRARY = '/opt/homebrew/Cellar/espeak/1.48.04_1/lib/libespeak.1.1.48.dylib'  #use the Path to the library.\n   EspeakWrapper.set_library(_ESPEAK_LIBRARY)\n   ```\n\n   Windows users may need to run (see https://github.com/bootphon/phonemizer/issues/163)\n\n   ```pwsh\n   $env:PHONEMIZER_ESPEAK_LIBRARY = \"c:\\Program Files\\eSpeak NG\\libespeak-ng.dll\"\n   $env:PHONEMIZER_ESPEAK_PATH = \"c:\\Program Files\\eSpeak NG\"\n   setx PHONEMIZER_ESPEAK_LIBRARY \"c:\\Program Files\\eSpeak NG\\libespeak-ng.dll\"\n   setx PHONEMIZER_ESPEAK_PATH \"c:\\Program Files\\eSpeak NG\"\n   ```\n\n3. **Install Python dependencies**\n\n   The requirements file includes the dependencies needed to run the model with PyTorch.\n   When using an ONNX decoder or a GGML model, some dependencies (such as PyTorch) are no longer required.\n\n   The inference is compatible and tested on `python>=3.11`.\n\n   ```bash\n   pip install -r requirements.txt\n   ```\n\n4. **(Optional) Install Llama-cpp-python to use the `GGUF` models.**\n\n   ```bash\n   pip install llama-cpp-python\n   ```\n\n   To run llama-cpp with GPU suport (CUDA, MPS) support please refer to:\n   https://pypi.org/project/llama-cpp-python/\n\n5. **(Optional) Install onnxruntime to use the `.onnx` decoder.**\n   If you want to run the onnxdecoder\n   ```bash\n   pip install onnxruntime\n   ```\n\n## Running the Model\n\nRun the basic example script to synthesize speech:\n\n```bash\npython -m examples.basic_example \\\n  --input_text \"My name is Dave, and um, I'm from London\" \\\n  --ref_audio samples/dave.wav \\\n  --ref_text samples/dave.txt\n```\n\nTo specify a particular model repo for the backbone or codec, add the `--backbone` argument. Available backbones are listed in [NeuTTS-Air huggingface collection](https://huggingface.co/collections/neuphonic/neutts-air-68cc14b7033b4",
      "default_branch": "main"
    },
    "fetched_at": "2025-10-25T02:20:50.929631"
  },
  {
    "basic_info": {
      "name": "bdh",
      "full_name": "pathwaycom/bdh",
      "owner": "pathwaycom",
      "description": "Baby Dragon Hatchling (BDH) – Architecture and Code",
      "url": "https://github.com/pathwaycom/bdh",
      "clone_url": "https://github.com/pathwaycom/bdh.git",
      "ssh_url": "git@github.com:pathwaycom/bdh.git",
      "homepage": "",
      "created_at": "2025-09-30T12:05:01Z",
      "updated_at": "2025-10-25T02:12:55Z",
      "pushed_at": "2025-10-14T07:57:02Z"
    },
    "stats": {
      "stars": 3208,
      "forks": 128,
      "watchers": 3208,
      "open_issues": 2,
      "size": 1005
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 8721
      },
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "# Baby Dragon Hatchling\n\n## **Bridging the Gap Between Transformers and the Brain**\n\n**Baby Dragon Hatchling (BDH)** is a biologically inspired large language model architecture that connects principles of deep learning with the foundations of neuroscience. Developed by researchers at [Pathway](https://pathway.com), BDH provides a theoretical and practical framework for understanding the emergence of reasoning and generalization in artificial systems.\n\nThis repository contains the official implementation from the paper:\n> *A. Kosowski, P. Uznański, J. Chorowski, Z. Stamirowska, M. Bartoszkiewicz.*\n> [_The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain_](https://doi.org/10.48550/arXiv.2509.26507), arXiv (2025).\n\n\n## Overview\n\nBDH represents a **scale-free, locally interacting network of neurons** capable of intrinsic reasoning dynamics. BDH scales like a Transformer on performance benchmarks—yet retains full interpretability and theoretical grounding in the fine-grained dynamics of neuron interactions.\n\n**Key properties:**\n\n- **Scale-free network topology** mimicking biological connectivity\n- **Locally interacting neuron particles** with excitatory/inhibitory dynamics\n- **Hebbian working memory** based on synaptic plasticity, displaying monosemanticity\n- **GPU-friendly state-space formulation** for efficient implementation\n- **Interpretable activations** that are sparse and positive\n\nBDH formalizes a bridge between **neural computation and machine-based language understanding**. It shows how **macro reasoning behavior** in large AI models emerges from **micro-level neuron dynamics**, guided by principles of graph theory and local computation.\n\nEmpirically, BDH matches **GPT-2–scale Transformers** across language and translation tasks at equivalent parameter scales (10M–1B).\n\n\n***\n\n## Architecture\n\n<img src=\"figs/architecture.png\" width=\"600\"/>\n\n***\n\n## Relation to Transformers\n\n<img src=\"figs/vocab.png\" width=\"600\"/>\n\nBDH and the Transformer share attention-inspired computation; however, BDH’s graph-based architecture makes its attention **emerge naturally from neuron-level interactions**, reflecting attention as seen in biological systems.\n\n***\n\n## Scaling Laws\n\n<img src=\"figs/bdh_scaling.png\" width=\"600\"/>\n\nBDH follows **Transformer-like scaling laws**, maintaining parameter efficiency while achieving interpretability at any scale.\n\n***\n\n## Installation and Training\n\n```bash\n# install dependencies\npip install -r requirements.txt\n\n# train BDH on a toy dataset\npython train.py\n```\n\n<!--For visualization and interpretability analysis, explore the example notebooks in `notebooks/`.-->\n\n\n\n## Learn and Discuss\n\n- Watch the *SuperDataScience podcast* [▶️ *Dragon Hatchling: The Missing Link Between Transformers and the Brain*](https://www.youtube.com/watch?v=mfV44-mtg7c) (72 min.) featuring Adrian Kosowski in conversation with Jon Krohn, unpacking BDH’s neuron-level architecture and sparse reasoning dynamics.\n\n- Read about BDH in\n[*Forbes*](https://www.forbes.com/sites/victordey/2025/10/08/can-ai-learn-and-evolve-like-a-brain-pathways-bold-research-thinks-so/),\n[*Semafor*](https://www.semafor.com/article/10/01/2025/new-ai-research-claims-to-be-getting-closer-to-modeling-human-brain),\n[*The Turing Post*](https://www.turingpost.com/p/fod-121-300-million-to-start-a-big-promise-for-science#the-freshest-research-papers-catego),\n[*Quantum Zeitgeist*](https://quantumzeitgeist.com/palo-alto-ai-firm-pathway-unveils-post-transformer-architecture-for-autonomous-ai/),\n[*Golem*](https://www.golem.de/news/neue-ki-architektur-was-ist-baby-dragon-hatchling-2510-201047-2.html),\nand elsewhere in the media.\n\n- Discuss and share the BDH paper on:\n[*Hugging Face Papers*](https://huggingface.co/papers/2509.26507), \n[*Alphaxiv*](https://alphaxiv.org/abs/2509.26507),\nand [*EmergentMind*](https://emergentmind.com/papers/2509.26507).\n\n## Community Forks\n\n- [adamskrodzki/bdh](https://github.com/adamskrodzki/bdh): dynamic vocabulary, stateful attention\n- [mosure/burn_dragon_hatchling](https://github.com/mosure/burn_dragon_hatchling): Burn port\n- [severian42/bdh](https://github.com/severian42/bdh): MLX port\n- [Git-Faisal/bdh](https://github.com/Git-Faisal/bdh)\n- [GrahLnn/bdh](https://github.com/GrahLnn/bdh)\n\n## Acknowledgements\nWe thank Andrej Karpathy for the [nanoGPT](https://github.com/karpathy/nanoGPT/) code and the tiny Shapespeare dataset used in this demonstration.\n\nBDH research stands at the intersection of **AI architecture**, **biological learning models**, and **theoretical computer science**—an effort to map the *equations of reasoning* between artificial and biological intelligence.\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-10-25T02:20:52.240059"
  },
  {
    "basic_info": {
      "name": "jscamp",
      "full_name": "midudev/jscamp",
      "owner": "midudev",
      "description": "Contenido y ejercicios del JSCamp InfoJobs",
      "url": "https://github.com/midudev/jscamp",
      "clone_url": "https://github.com/midudev/jscamp.git",
      "ssh_url": "git@github.com:midudev/jscamp.git",
      "homepage": "https://jscamp.dev",
      "created_at": "2025-09-28T13:28:51Z",
      "updated_at": "2025-10-25T02:18:55Z",
      "pushed_at": "2025-10-22T17:58:32Z"
    },
    "stats": {
      "stars": 2788,
      "forks": 279,
      "watchers": 2788,
      "open_issues": 2,
      "size": 163
    },
    "tech_info": {
      "language": "CSS",
      "languages": {
        "CSS": 31194,
        "HTML": 26627,
        "JavaScript": 14626
      },
      "license": null,
      "topics": [
        "bootcamp"
      ]
    },
    "content": {
      "readme": "![CleanShot 2025-10-01 at 20 11 30@2x](https://github.com/user-attachments/assets/b6ef8402-d367-4a99-b939-8f11dedf91d7)\n\n# 🚀 JSCamp InfoJobs\n\nBienvenidos al bootcamp intensivo de JavaScript y desarrollo web full-stack diseñado para llevarte desde los fundamentos hasta las tecnologías más avanzadas del ecosistema JavaScript. Veremos HTML, CSS, JavaScript, TypeScript, Node.js, SQL, CI/CD y Docker.\n\n## 🎨 El Proyecto Práctico\n\n![CleanShot 2025-10-01 at 20 26 08@2x](https://github.com/user-attachments/assets/d9abec4d-ac41-4962-845c-93006bfe768b)\n\nA lo largo de este bootcamp, construiremos un proyecto completo **desde cero y paso a paso**, aplicando todos los conocimientos de cada módulo.\n\n👉 [Ver diseño del proyecto](https://stitch.withgoogle.com/projects/7508115667617706440)\n\nEste proyecto te permitirá consolidar todo lo aprendido y tener una aplicación real en tu portafolio.\n\n## 📺 La Plataforma\n\nEn **[JSCamp.dev](https://jscamp.dev)** encontrarás todos los videos y contenido del bootcamp para que puedas revisarlo cuando quieras. El registro es gratis.\nLos videos y materiales se irán subiendo **poco a poco** a medida que avancemos en el bootcamp.\n\n### ¿Tiene certificado?\n\nSí, existe un certificado opcional y muy limitado de pago que incluye:\n\n- 🎓 **Certificado Digital** - Certifica tus logros en el bootcamp\n- 📝 **Seguimiento de Ejercicios** - Revisaremos y corregiremos tus ejercicios\n- 💬 **Canal Exclusivo en Discord** - Comunidad premium y soporte directo\n- 🎥 **Directos Exclusivos** - Clases de repaso exclusivas con dudas y preguntas\n- 📄 **Revisión de tu CV** - Equipo de expertos revisan tu CV y te dan feedback\n- 🏢 **Workshop Presencial** - Entrada asegurada a los workshops de Barcelona y Madrid\n\n**Entra a [https://jscamp.dev](https://jscamp.dev), inicia sesión y consigue acceso.**\n\n## 📚 Contenido del Bootcamp\n\n- **00** - HTML & CSS\n- **01** - JavaScript\n- **02** - React\n- **03** - Estado Global y React Router\n- **04** - Node.js\n- **05** - TypeScript\n- **06** - SQL\n- **07** - CI/CD\n- **08** - Docker\n\n## 💻 Requisitos de Instalación\n\nAntes de comenzar, asegúrate de tener instalado el siguiente software:\n\n- **Navegador moderno** - Chrome, Firefox, Edge o Safari actualizado\n- **[Visual Studio Code](https://code.visualstudio.com/)** - Editor de código (recomendado)\n- **[Extensión Live Preview](https://marketplace.visualstudio.com/items?itemName=ms-vscode.live-server)** - Extensión para ver HTML/CSS\n- **[Node.js](https://nodejs.org/)** (versión 20 o superior) - Runtime de JavaScript\n- **[Git](https://git-scm.com/)** - Control de versiones\n- **[Docker](https://www.docker.com/)** - Para el módulo de Docker\n- **[Terminal Warp](https://midu.link/warp)** - Terminal con IA y Agentes\n\n## 👨‍💻 Instructor\n\nEste bootcamp es impartido por **midudev**, desarrollador y creador de contenido educativo con una gran comunidad en español.\n\n### 🌐 Redes Sociales\n\n- 🐦 **X**: [@midudev](https://twitter.com/midudev)\n- 📺 **YouTube**: [@midudev](https://youtube.com/@midudev)\n- 🎮 **Twitch**: [midudev](https://twitch.tv/midudev)\n- 📸 **Instagram**: [@midu.dev](https://instagram.com/midu.dev)\n- 💼 **LinkedIn**: [midudev](https://linkedin.com/in/midudev)\n- 🌍 **Web**: [midu.dev](https://midu.dev)\n\n## 🎯 Objetivos\n\nAl finalizar JSCAMP serás capaz de:\n\n- ✅ Construir aplicaciones web completas desde cero\n- ✅ Dominar el ecosistema de JavaScript moderno\n- ✅ Crear APIs REST con Node.js\n- ✅ Desarrollar interfaces con React\n- ✅ Implementar bases de datos SQL\n- ✅ Configurar pipelines de CI/CD\n- ✅ Containerizar aplicaciones con Docker\n- ✅ Aplicar TypeScript en proyectos reales\n\n## 🚀 Cómo Empezar\n\nCada módulo contiene ejercicios prácticos y proyectos reales. Navega a la carpeta correspondiente y sigue las instrucciones.\n\n```bash\n# Clona el repositorio\ngit clone git@github.com:midudev/jscamp.git\n\n# Navega al módulo que desees\ncd jscamp/00-html-css\n\n# ¡Comienza a aprender!\n```\n\n---\n\n⭐️ Si este contenido te resulta útil, no olvides dar una estrella al repositorio\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-10-25T02:20:53.503069"
  },
  {
    "basic_info": {
      "name": "openzl",
      "full_name": "facebook/openzl",
      "owner": "facebook",
      "description": "A novel data compression framework",
      "url": "https://github.com/facebook/openzl",
      "clone_url": "https://github.com/facebook/openzl.git",
      "ssh_url": "git@github.com:facebook/openzl.git",
      "homepage": "https://openzl.org",
      "created_at": "2025-09-30T18:30:07Z",
      "updated_at": "2025-10-25T02:01:43Z",
      "pushed_at": "2025-10-24T18:52:31Z"
    },
    "stats": {
      "stars": 2556,
      "forks": 100,
      "watchers": 2556,
      "open_issues": 47,
      "size": 15825
    },
    "tech_info": {
      "language": "C",
      "languages": {
        "C": 8871895,
        "C++": 5553960,
        "Python": 249928,
        "TypeScript": 122753,
        "Starlark": 116595,
        "CMake": 64550,
        "Makefile": 29737,
        "Assembly": 14169,
        "CSS": 10966,
        "PowerShell": 6170,
        "Shell": 5823,
        "Thrift": 4215,
        "Batchfile": 3464,
        "JavaScript": 1276,
        "HTML": 462
      },
      "license": "Other",
      "topics": []
    },
    "content": {
      "readme": "# OpenZL\n\nOpenZL delivers high compression ratios _while preserving high speed_, a level of performance that is out of reach for generic compressors. **Check out the [blog post](https://engineering.fb.com/2025/10/06/developer-tools/openzl-open-source-format-aware-compression-framework/) and [whitepaper](https://arxiv.org/abs/2510.03203) for a breakdown of how it works.**\n\nOpenZL takes a description of your data and builds from it a specialized compressor optimized for your specific format. [Learn how it works →](https://facebook.github.io/openzl/getting-started/introduction/)\n\nOpenZL consists of a core library and tools to generate specialized compressors —\nall compatible with a single universal decompressor.\nIt is designed for engineers that deal with large quantities of specialized datasets (like AI workloads for example) and require high speed for their processing pipelines.\n\nSee our [docs](https://facebook.github.io/openzl) for more information and our [quickstart guide](https://facebook.github.io/openzl/getting-started/quick-start) to get started with a guided tutorial.\n\n## Project Status\n\nThis project is under active development. The API, the compressed format, and the set of codecs and graphs included in OpenZL are all subject to (and will!) change as the project matures.\n\nHowever, we intend to maintain some stability guarantees in the face of that evolution. In particular, payloads compressed with any release-tagged version of the library will remain decompressible by new releases of the library for at least the next several years. And new releases of the library will be able to generate frames compatible with at least the previous release.\n\n(Commits on the `dev` branch offer no guarantees whatsoever. Use only release-tagged commits for any non-experimental deployments.)\n\nDespite the big scary warnings above, we consider the core to have reached production-readiness, and OpenZL is used extensively in production at Meta.\n\n## Building OpenZL\n\n### Prerequisites\nOpenZL requires a compiler that supports C11 and C++17. When building with `cmake`, `cmake 3.20.2` or newer is required. There is ongoing work to relax these restrictions. As that happens, this section will be updated.\n\n### Build with `make`\n\nThe OpenZL library and essential tools can be built using `make`:\n\n```sh\nmake\n```\n\n#### Build Options\n\nThe `Makefile` supports all standard build variables, such as `CC`, `CFLAGS`, `CPPFLAGS`, `LDFLAGS`, `LDLIBS`, etc.\n\nIt builds with multi-threading by default, auto-detecting the local number of cores, and can be overridden using standard `-j#` flag (ex: `make -j8`).\n\n#### Build Types\n\nBinary generation can be altered by explicitly requesting a build type:\n\nExample:\n```sh\nmake lib BUILD_TYPE=DEV\n```\n\nBuild types are documented in `make help`, and their exact flags are detailed with `make show-config`.\n\nUsual ones are:\n\n* `BUILD_TYPE=DEV`: debug build with asserts enabled and ASAN / UBSAN enabled\n* `BUILD_TYPE=OPT`: optimized build with asserts disabled (default)\n\n### Build with `cmake`\n\nOpenZL can be built using `cmake`. Basic usage is as follows:\n\n```sh\nmkdir build\ncd build\ncmake -DCMAKE_BUILD_TYPE=Release -DOPENZL_BUILD_TESTS=ON ..\nmake -j\nmake -j test\n```\n\nDetails on setting CMake variables is below.\n\n#### Build Modes\n\nBy default, we ship several different predefined build modes which can be set with the `OPENZL_BUILD_MODE` variable:\n\n* `none` (default): CMake default build mode controlled by `CMAKE_BUILD_TYPE`\n* `dev`: debug build with asserts enabled and ASAN / UBSAN enabled\n* `dev-nosan`: debug build with asserts enabled\n* `opt`: optimized build with asserts disabled\n* `opt-asan`: optimized build with asserts disabled and ASAN / UBSAN enabled\n* `dbgo`: optimized build with asserts enabled\n* `dbgo-asan`: optimized build with asserts enabled and ASAN / UBSAN enabled\n\n> [!CAUTION]\n> When switching between build modes, make sure to purge the CMake cache and re-configure the build. For instance,\n> `cmake --fresh -DOPENZL_BUILD_MODE=dev-nosan ..`\n\nFor ASAN / UBSAN, ensure that `libasan` and `libubsan` are installed on the machine.\n\n#### Editor Integration\n\nOpenZL ships with settings to configure VSCode to work with the CMake build system. To enable it install two extensions:\n\n1. `cmake-tools`\n2. `clangd` (or any other C++ language server that works with `compile_commands.json`)\n\n**Important:** For proper C++ language server support, you need to generate `compile_commands.json`:\n\nThe preferred method is to use the CMake Tools extension command \"`CMake: Configure`\".\n\nIf it doesn't work, or is too difficult to setup, you can use the manual setup:\n\n```bash\nmkdir -p cmakebuild\ncmake -B cmakebuild -DOPENZL_BUILD_TESTS=ON -DCMAKE_EXPORT_COMPILE_COMMANDS=ON .\ncp cmakebuild/compile_commands.json .\n```\n\n**When to regenerate:**\n\n* After cloning the repository (first-time setup)\n* When adding/removing source files\n* When modifying `CMakeLists.txt`\n\n#### CMake Variables\n\n* `CMAKE_C_COMPILER` = Set the C compiler for Op",
      "default_branch": "dev"
    },
    "fetched_at": "2025-10-25T02:20:54.795135"
  },
  {
    "basic_info": {
      "name": "HunyuanImage-3.0",
      "full_name": "Tencent-Hunyuan/HunyuanImage-3.0",
      "owner": "Tencent-Hunyuan",
      "description": "HunyuanImage-3.0: A Powerful Native Multimodal Model for Image Generation",
      "url": "https://github.com/Tencent-Hunyuan/HunyuanImage-3.0",
      "clone_url": "https://github.com/Tencent-Hunyuan/HunyuanImage-3.0.git",
      "ssh_url": "git@github.com:Tencent-Hunyuan/HunyuanImage-3.0.git",
      "homepage": "https://hunyuan.tencent.com/image",
      "created_at": "2025-09-27T07:18:47Z",
      "updated_at": "2025-10-25T00:50:04Z",
      "pushed_at": "2025-10-14T08:42:04Z"
    },
    "stats": {
      "stars": 2293,
      "forks": 96,
      "watchers": 2293,
      "open_issues": 28,
      "size": 34784
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 374880,
        "Shell": 806
      },
      "license": "Other",
      "topics": [
        "image-generation",
        "native-multimodal-model"
      ]
    },
    "content": {
      "readme": "[中文文档](./README_zh_CN.md)\n\n<div align=\"center\">\n\n<img src=\"./assets/logo.png\" alt=\"HunyuanImage-3.0 Logo\" width=\"600\">\n\n# 🎨 HunyuanImage-3.0: A Powerful Native Multimodal Model for Image Generation\n\n</div>\n\n\n<div align=\"center\">\n<img src=\"./assets/banner.png\" alt=\"HunyuanImage-3.0 Banner\" width=\"800\">\n\n</div>\n\n<div align=\"center\">\n  <a href=https://hunyuan.tencent.com/image target=\"_blank\"><img src=https://img.shields.io/badge/Official%20Site-333399.svg?logo=homepage height=22px></a>\n  <a href=https://huggingface.co/tencent/HunyuanImage-3.0 target=\"_blank\"><img src=https://img.shields.io/badge/%F0%9F%A4%97%20Models-d96902.svg height=22px></a>\n  <a href=https://github.com/Tencent-Hunyuan/HunyuanImage-3.0 target=\"_blank\"><img src= https://img.shields.io/badge/Page-bb8a2e.svg?logo=github height=22px></a>\n  <a href=https://arxiv.org/pdf/2509.23951 target=\"_blank\"><img src=https://img.shields.io/badge/Report-b5212f.svg?logo=arxiv height=22px></a>\n  <a href=https://x.com/TencentHunyuan target=\"_blank\"><img src=https://img.shields.io/badge/Hunyuan-black.svg?logo=x height=22px></a>\n  <a href=https://docs.qq.com/doc/DUVVadmhCdG9qRXBU target=\"_blank\"><img src=https://img.shields.io/badge/📚-PromptHandBook-blue.svg?logo=book height=22px></a>\n</div>\n\n\n<p align=\"center\">\n    👏 Join our <a href=\"./assets/WECHAT.md\" target=\"_blank\">WeChat</a> and <a href=\"https://discord.gg/ehjWMqF5wY\">Discord</a> | \n💻 <a href=\"https://hunyuan.tencent.com/modelSquare/home/play?modelId=289&from=/visual\">Official website(官网) Try our model!</a>&nbsp&nbsp\n</p>\n\n## 🔥🔥🔥 News\n- **September 28, 2025**: 📖 **HunyuanImage-3.0 Technical Report Released** - Comprehensive technical documentation now available\n- **September 28, 2025**: 🚀 **HunyuanImage-3.0 Open Source Release** - Inference code and model weights publicly available\n\n\n## 🧩 Community Contributions\n\nIf you develop/use HunyuanImage-3.0 in your projects, welcome to let us know.\n\n## 📑 Open-source Plan\n\n- HunyuanImage-3.0 (Image Generation Model)\n  - [x] Inference \n  - [x] HunyuanImage-3.0 Checkpoints\n  - [ ] HunyuanImage-3.0-Instruct Checkpoints (with reasoning)\n  - [ ] VLLM Support\n  - [ ] Distilled Checkpoints\n  - [ ] Image-to-Image Generation\n  - [ ] Multi-turn Interaction\n\n\n## 🗂️ Contents\n- [🔥🔥🔥 News](#-news)\n- [🧩 Community Contributions](#-community-contributions)\n- [📑 Open-source Plan](#-open-source-plan)\n- [📖 Introduction](#-introduction)\n- [✨ Key Features](#-key-features)\n- [🛠️ Dependencies and Installation](#-dependencies-and-installation)\n  - [💻 System Requirements](#-system-requirements)\n  - [📦 Environment Setup](#-environment-setup)\n  - [📥 Install Dependencies](#-install-dependencies)\n  - [Performance Optimizations](#performance-optimizations)\n- [🚀 Usage](#-usage)\n  - [🔥 Quick Start with Transformers](#-quick-start-with-transformers)\n  - [🏠 Local Installation & Usage](#-local-installation--usage)\n  - [🎨 Interactive Gradio Demo](#-interactive-gradio-demo)\n- [🧱 Models Cards](#-models-cards)\n- [📝 Prompt Guide](#-prompt-guide)\n  - [Manually Writing Prompts](#manually-writing-prompts)\n  - [System Prompt For Automatic Rewriting the Prompt](#system-prompt-for-automatic-rewriting-the-prompt)\n  - [Advanced Tips](#advanced-tips)\n  - [More Cases](#more-cases)\n- [📊 Evaluation](#-evaluation)\n- [📚 Citation](#-citation)\n- [🙏 Acknowledgements](#-acknowledgements)\n- [🌟🚀  Github Star History](#-github-star-history)\n\n---\n\n## 📖 Introduction\n\n**HunyuanImage-3.0** is a groundbreaking native multimodal model that unifies multimodal understanding and generation within an autoregressive framework. Our text-to-image module achieves performance **comparable to or surpassing** leading closed-source models.\n\n\n<div align=\"center\">\n  <img src=\"./assets/framework.png\" alt=\"HunyuanImage-3.0 Framework\" width=\"90%\">\n</div>\n\n## ✨ Key Features\n\n* 🧠 **Unified Multimodal Architecture:** Moving beyond the prevalent DiT-based architectures, HunyuanImage-3.0 employs a unified autoregressive framework. This design enables a more direct and integrated modeling of text and image modalities, leading to surprisingly effective and contextually rich image generation.\n\n* 🏆 **The Largest Image Generation MoE Model:** This is the largest open-source image generation Mixture of Experts (MoE) model to date. It features 64 experts and a total of 80 billion parameters, with 13 billion activated per token, significantly enhancing its capacity and performance.\n\n* 🎨 **Superior Image Generation Performance:** Through rigorous dataset curation and advanced reinforcement learning post-training, we've achieved an optimal balance between semantic accuracy and visual excellence. The model demonstrates exceptional prompt adherence while delivering photorealistic imagery with stunning aesthetic quality and fine-grained details.\n\n* 💭 **Intelligent World-Knowledge Reasoning:** The unified multimodal architecture endows HunyuanImage-3.0 with powerful reasoning capabilities. It leverages its extensive world knowledge to intelligently interpret user int",
      "default_branch": "main"
    },
    "fetched_at": "2025-10-25T02:20:56.076454"
  },
  {
    "basic_info": {
      "name": "The-Accidental-CTO",
      "full_name": "subhashchy/The-Accidental-CTO",
      "owner": "subhashchy",
      "description": "How I Scaled from Zero to a Million Store on Dukaan,  Without a CS Degree.  .. A System Design Handbook by  Subhash Choudhary ",
      "url": "https://github.com/subhashchy/The-Accidental-CTO",
      "clone_url": "https://github.com/subhashchy/The-Accidental-CTO.git",
      "ssh_url": "git@github.com:subhashchy/The-Accidental-CTO.git",
      "homepage": "",
      "created_at": "2025-09-26T09:07:20Z",
      "updated_at": "2025-10-24T20:03:31Z",
      "pushed_at": "2025-10-20T16:15:49Z"
    },
    "stats": {
      "stars": 2281,
      "forks": 172,
      "watchers": 2281,
      "open_issues": 8,
      "size": 11757
    },
    "tech_info": {
      "language": "TypeScript",
      "languages": {
        "TypeScript": 167424,
        "CSS": 8323,
        "HTML": 1233,
        "JavaScript": 846
      },
      "license": null,
      "topics": [
        "scaling",
        "system-design"
      ]
    },
    "content": {
      "readme": "\n# **The Accidental CTO**\n\n\n\n## **How I Scaled from Zero to a Million Stores on Dukaan, Without a CS \nI never set out to be a CTO. In fact, I didn’t even have a computer science degree. But somewhere between firefighting server crashes at 3 a.m. and obsessing over replication lag graphs, I found myself building systems that would eventually power over a **million online stores** at Dukaan.\n\nThis book, *The Accidental CTO*, is my behind-the-scenes account of that journey. It’s not a dry academic manual filled with abstract diagrams. Instead, it’s a story-driven handbook — one that mixes late-night startup battles with the **hard system design lessons** that only come from being in the trenches.\n\nFrom scaling a scrappy MVP to running massive distributed pipelines, I’ll take you through the challenges we faced and the decisions that made (or nearly broke) us.\n\n---\n\n### What You’ll Learn Inside\n\n* **Scaling applications**: How we went from thousands to millions of users without falling apart.\n* **Replication, sharding, caching, queues**: When to use them, when *not* to, and what tradeoffs they carry.\n* **Observability as survival**: Why metrics, logs, traces, SLAs, and SLOs aren’t optional — they’re lifelines.\n* **Resilience engineering**: Circuit breakers, retries, graceful degradation — designing for failure, not against it.\n* **The hidden costs of cloud**: Why at scale, your AWS bill can become your biggest investor, and when it makes sense to go self-hosted.\n* **The consistency/availability/latency triangle**: Why you can never fully win, and how to navigate the tradeoffs in real systems.\n\n---\n\n### Why I Wrote This Book\n\nI didn’t want to write another \"theory of distributed systems\" book. There are already plenty of those.\n\nWhat I wanted to share is the **practical side** of system design — the part you only learn when a real company, with real customers and real money at stake, is on fire. The part where you’re not solving toy interview questions but dealing with:\n\n* angry merchants refreshing dashboards,\n* Kafka pipelines silently choking on one bad partition,\n* a database replica 10 minutes behind and nobody knowing why.\n\nThis is the stuff no textbook teaches you.\n\n---\n\n### Who This Book Is For\n\nWhether you’re a **software engineer**, **architect**, or **startup founder**, I wrote this book to help you see distributed systems not as academic puzzles, but as **living, evolving machines** that you can actually build, operate, and grow.\n\nIf you’ve ever wondered *how real companies actually scale* — not in theory, but in practice — this is my candid, first-hand story.\n\nAnd maybe, just maybe, you’ll find a bit of yourself in *The Accidental CTO*.\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-10-25T02:20:57.367412"
  },
  {
    "basic_info": {
      "name": "beads",
      "full_name": "steveyegge/beads",
      "owner": "steveyegge",
      "description": "Beads - A memory upgrade for your coding agent",
      "url": "https://github.com/steveyegge/beads",
      "clone_url": "https://github.com/steveyegge/beads.git",
      "ssh_url": "git@github.com:steveyegge/beads.git",
      "homepage": "",
      "created_at": "2025-10-12T03:09:46Z",
      "updated_at": "2025-10-25T01:20:43Z",
      "pushed_at": "2025-10-25T00:06:26Z"
    },
    "stats": {
      "stars": 2066,
      "forks": 115,
      "watchers": 2066,
      "open_issues": 14,
      "size": 14784
    },
    "tech_info": {
      "language": "Go",
      "languages": {
        "Go": 1211828,
        "Python": 172584,
        "Shell": 17175,
        "PowerShell": 6787,
        "Nix": 1327
      },
      "license": "MIT License",
      "topics": [
        "agents",
        "claude-code",
        "coding"
      ]
    },
    "content": {
      "readme": "# bd - Beads Issue Tracker 🔗\n\n[![Go Version](https://img.shields.io/github/go-mod/go-version/steveyegge/beads)](https://go.dev/)\n[![Release](https://img.shields.io/github/v/release/steveyegge/beads)](https://github.com/steveyegge/beads/releases)\n[![CI](https://img.shields.io/github/actions/workflow/status/steveyegge/beads/ci.yml?branch=main&label=tests)](https://github.com/steveyegge/beads/actions/workflows/ci.yml)\n[![Go Report Card](https://goreportcard.com/badge/github.com/steveyegge/beads)](https://goreportcard.com/report/github.com/steveyegge/beads)\n[![License](https://img.shields.io/github/license/steveyegge/beads)](LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/beads-mcp)](https://pypi.org/project/beads-mcp/)\n\n**Give your coding agent a memory upgrade**\n\n> **⚠️ Alpha Status**: This project is in active development. The core features work well, but expect API changes before 1.0. Use for development/internal projects first.\n\nBeads is a lightweight memory system for coding agents, using a graph-based issue tracker. Four kinds of dependencies work to chain your issues together like beads, making them easy for agents to follow for long distances, and reliably perform complex task streams in the right order.\n\nDrop Beads into any project where you're using a coding agent, and you'll enjoy an instant upgrade in organization, focus, and your agent's ability to handle long-horizon tasks over multiple compaction sessions. Your agents will use issue tracking with proper epics, rather than creating a swamp of rotten half-implemented markdown plans.\n\nInstant start:\n\n```bash\ncurl -fsSL https://raw.githubusercontent.com/steveyegge/beads/main/scripts/install.sh | bash\n```\n\nThen tell your coding agent to start using the `bd` tool instead of markdown for all new work, somewhere in your `AGENTS.md` or `CLAUDE.md`. That's all there is to it!\n\nYou don't use Beads directly as a human. Your coding agent will file and manage issues on your behalf. They'll file things they notice automatically, and you can ask them at any time to add or update issues for you.\n\nBeads gives agents unprecedented long-term planning capability, solving their amnesia when dealing with complex nested plans. They can trivially query the ready work, orient themselves, and land on their feet as soon as they boot up.\n\nAgents using Beads will no longer silently pass over problems they notice due to lack of context space -- instead, they will automatically file issues for newly-discovered work as they go. No more lost work, ever.\n\nBeads issues are backed by git, but through a clever design it manages to act like a managed, centrally hosted SQL database shared by all of the agents working on a project (repo), even across machines.\n\nBeads even improves work auditability. The issue tracker has a sophisticated audit trail, which agents can use to reconstruct complex operations that may have spanned multiple sessions.\n\nAgents report that they enjoy working with Beads, and they will use it spontaneously for both recording new work and reasoning about your project in novel ways. Whether you are a human or an AI, Beads lets you have more fun and less stress with agentic coding.\n\n![AI Agent using Beads](https://raw.githubusercontent.com/steveyegge/beads/main/.github/images/agent-using-beads.jpg)\n\n## Features\n\n- ✨ **Zero setup** - `bd init` creates project-local database (and your agent will do it)\n- 🔗 **Dependency tracking** - Four dependency types (blocks, related, parent-child, discovered-from)\n- 📋 **Ready work detection** - Automatically finds issues with no open blockers\n- 🤖 **Agent-friendly** - `--json` flags for programmatic integration\n- 📦 **Git-versioned** - JSONL records stored in git, synced across machines\n- 🌍 **Distributed by design** - Agents on multiple machines share one logical database via git\n- 🏗️ **Extensible** - Add your own tables to the SQLite database\n- 🔍 **Multi-project isolation** - Each project gets its own database, auto-discovered by directory\n- 🌲 **Dependency trees** - Visualize full dependency graphs\n- 🎨 **Beautiful CLI** - Colored output for humans, JSON for bots\n- 💾 **Full audit trail** - Every change is logged\n- ⚡ **High performance** - Batch operations for bulk imports (1000 issues in ~950ms)\n- 🗜️ **Memory decay** - Semantic compaction gracefully reduces old closed issues\n\n## Installation\n\n**Quick install (all platforms):**\n```bash\ncurl -fsSL https://raw.githubusercontent.com/steveyegge/beads/main/scripts/install.sh | bash\n```\n\n**Homebrew (macOS/Linux):**\n```bash\nbrew tap steveyegge/beads\nbrew install bd\n```\n\n**Other platforms and methods:** See [INSTALLING.md](INSTALLING.md) for Windows, Arch Linux, and manual installation.\n\n**IDE Integration:** See [INSTALLING.md](INSTALLING.md) for Claude Code plugin and MCP server setup.\n\n## Quick Start\n\n### For Humans\n\nBeads is designed for **AI coding agents** to use on your behalf. Setup takes 30 seconds:\n\n```bash\n# 1. Initialize bd in your project\nbd init\n\n# 2. Tell your agent to configure",
      "default_branch": "main"
    },
    "fetched_at": "2025-10-25T02:20:58.636016"
  },
  {
    "basic_info": {
      "name": "dexter",
      "full_name": "virattt/dexter",
      "owner": "virattt",
      "description": "An autonomous agent for deep financial research",
      "url": "https://github.com/virattt/dexter",
      "clone_url": "https://github.com/virattt/dexter.git",
      "ssh_url": "git@github.com:virattt/dexter.git",
      "homepage": null,
      "created_at": "2025-10-14T21:02:00Z",
      "updated_at": "2025-10-25T02:12:46Z",
      "pushed_at": "2025-10-22T00:20:41Z"
    },
    "stats": {
      "stars": 2065,
      "forks": 233,
      "watchers": 2065,
      "open_issues": 7,
      "size": 27
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 57621,
        "JavaScript": 228
      },
      "license": null,
      "topics": []
    },
    "content": {
      "readme": "# Dexter 🤖\n\nDexter is an autonomous financial research agent that thinks, plans, and learns as it works. It performs analysis using task planning, self-reflection, and real-time market data. Think Claude Code, but built specifically for financial research.\n\n\n<img width=\"979\" height=\"651\" alt=\"Screenshot 2025-10-14 at 6 12 35 PM\" src=\"https://github.com/user-attachments/assets/5a2859d4-53cf-4638-998a-15cef3c98038\" />\n\n## Overview\n\nDexter takes complex financial questions and turns them into clear, step-by-step research plans. It runs those tasks using live market data, checks its own work, and refines the results until it has a confident, data-backed answer.  \n\nIt’s not just another chatbot.  It’s an agent that plans ahead, verifies its progress, and keeps iterating until the job is done.\n\n**Key Capabilities:**\n- **Intelligent Task Planning**: Automatically decomposes complex queries into structured research steps\n- **Autonomous Execution**: Selects and executes the right tools to gather financial data\n- **Self-Validation**: Checks its own work and iterates until tasks are complete\n- **Real-Time Financial Data**: Access to income statements, balance sheets, and cash flow statements\n- **Safety Features**: Built-in loop detection and step limits to prevent runaway execution\n\n[![Twitter Follow](https://img.shields.io/twitter/follow/virattt?style=social)](https://twitter.com/virattt)\n\n### Prerequisites\n\n- Python 3.10 or higher\n- [uv](https://github.com/astral-sh/uv) package manager\n- OpenAI API key (get [here](https://platform.openai.com/api-keys))\n- Financial Datasets API key (get [here](https://financialdatasets.ai))\n\n### Installation\n\n1. Clone the repository:\n```bash\ngit clone https://github.com/virattt/dexter.git\ncd dexter\n```\n\n2. Install dependencies with uv:\n```bash\nuv sync\n```\n\n3. Set up your environment variables:\n```bash\n# Copy the example environment file\ncp env.example .env\n\n# Edit .env and add your API keys\n# OPENAI_API_KEY=your-openai-api-key\n# FINANCIAL_DATASETS_API_KEY=your-financial-datasets-api-key\n```\n\n### Usage\n\nRun Dexter in interactive mode:\n```bash\nuv run dexter-agent\n```\n\n### Example Queries\n\nTry asking Dexter questions like:\n- \"What was Apple's revenue growth over the last 4 quarters?\"\n- \"Compare Microsoft and Google's operating margins for 2023\"\n- \"Analyze Tesla's cash flow trends over the past year\"\n- \"What is Amazon's debt-to-equity ratio based on recent financials?\"\n\nDexter will automatically:\n1. Break down your question into research tasks\n2. Fetch the necessary financial data\n3. Perform calculations and analysis\n4. Provide a comprehensive, data-rich answer\n\n## Architecture\n\nDexter uses a multi-agent architecture with specialized components:\n\n- **Planning Agent**: Analyzes queries and creates structured task lists\n- **Action Agent**: Selects appropriate tools and executes research steps\n- **Validation Agent**: Verifies task completion and data sufficiency\n- **Answer Agent**: Synthesizes findings into comprehensive responses\n\n## Project Structure\n\n```\ndexter/\n├── src/\n│   ├── dexter/\n│   │   ├── agent.py      # Main agent orchestration logic\n│   │   ├── model.py      # LLM interface\n│   │   ├── tools.py      # Financial data tools\n│   │   ├── prompts.py    # System prompts for each component\n│   │   ├── schemas.py    # Pydantic models\n│   │   ├── utils/        # Utility functions\n│   │   └── cli.py        # CLI entry point\n├── pyproject.toml\n└── uv.lock\n```\n\n## Configuration\n\nDexter supports configuration via the `Agent` class initialization:\n\n```python\nfrom dexter.agent import Agent\n\nagent = Agent(\n    max_steps=20,              # Global safety limit\n    max_steps_per_task=5       # Per-task iteration limit\n)\n```\n\n## How to Contribute\n\n1. Fork the repository\n2. Create a feature branch\n3. Commit your changes\n4. Push to the branch\n5. Create a Pull Request\n\n**Important**: Please keep your pull requests small and focused.  This will make it easier to review and merge.\n\n\n## License\n\nThis project is licensed under the MIT License.\n\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-10-25T02:20:59.916381"
  },
  {
    "basic_info": {
      "name": "DreamOmni2",
      "full_name": "dvlab-research/DreamOmni2",
      "owner": "dvlab-research",
      "description": "This project is the official implementation of 'DreamOmni2: Multimodal Instruction-based Editing and Generation''",
      "url": "https://github.com/dvlab-research/DreamOmni2",
      "clone_url": "https://github.com/dvlab-research/DreamOmni2.git",
      "ssh_url": "git@github.com:dvlab-research/DreamOmni2.git",
      "homepage": "",
      "created_at": "2025-09-28T05:20:51Z",
      "updated_at": "2025-10-25T02:04:05Z",
      "pushed_at": "2025-10-20T06:43:08Z"
    },
    "stats": {
      "stars": 2012,
      "forks": 181,
      "watchers": 2012,
      "open_issues": 18,
      "size": 15842
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 142712
      },
      "license": "Apache License 2.0",
      "topics": [
        "image-editing",
        "image-generation",
        "unified-generation-editing-model"
      ]
    },
    "content": {
      "readme": "# DreamOmni2: Multimodal Instruction-based Editing and Generation\n\n<p align=\"center\">\n    <a href=\"https://arxiv.org/html/2510.06679v1\">\n            <img alt=\"Build\" src=\"https://img.shields.io/badge/arXiv%20paper-2510.06679v1-b31b1b.svg\">\n    </a>\n    <a href=\"https://pbihao.github.io/projects/DreamOmni2/index.html\">\n        <img alt=\"Project Page\" src=\"https://img.shields.io/badge/Project-Page-blue\">\n    </a>\n    <a href=\"https://www.youtube.com/watch?v=8xpoiRK57uU\">\n        <img alt=\"Video Demo\" src=\"https://img.shields.io/badge/Video-Demo-red\">\n    </a>\n    <a href=\"https://huggingface.co/datasets/xiabs/DreamOmni2Bench\">\n        <img alt=\"Build\" src=\"https://img.shields.io/badge/DreamOmni2-Benchmark-green\">\n    </a>\n    <a href=\"https://huggingface.co/xiabs/DreamOmni2\">\n        <img alt=\"Build\" src=\"https://img.shields.io/badge/🤗-HF%20Model-yellow\">\n    </a>    \n    <a href=\"https://huggingface.co/spaces/wcy1122/DreamOmni2-Edit\">\n        <img alt=\"Build\" src=\"https://img.shields.io/badge/🤗-HF%20Editing%20Demo-yellow\">\n    </a>\n    <a href=\"https://huggingface.co/spaces/wcy1122/DreamOmni2-Gen\">\n        <img alt=\"Build\" src=\"https://img.shields.io/badge/🤗-HF%20Generation%20Demo-yellow\">\n    </a>\n    <a href=\"https://www.runninghub.ai/workflow/1980131298238959618\">\n        <img alt=\"Build\" src=\"https://img.shields.io/badge/ComfyUI-Runninghub-blue\">\n    </a>\n</p>\n\n## 🔥 News\n- 🔥**2025.10.10**: Release DreamOmni2 [editing demo](https://huggingface.co/spaces/wcy1122/DreamOmni2-Edit) and [generation demo](https://huggingface.co/spaces/wcy1122/DreamOmni2-Gen)\n- 🔥**2025.10.10**: Release DreamOmni2 [Benchmark](https://huggingface.co/datasets/xiabs/DreamOmni2Bench).\n- 🔥**2025.10.10**: Release DreamOmni2's [codes](https://github.com/dvlab-research/DreamOmni2) and [models](https://huggingface.co/xiabs/DreamOmni2).\n- 🔥**2025.10.09**: Release DreamOmni2 [tech report](https://arxiv.org/html/2510.06679v1).\n\n\n<p align=\"center\">\n  <img width=\"600\" src=\"imgs/gallery.png\">\n</p>\n\n\n<div align=\"center\">\n  <a href=\"https://cloud.video.taobao.com/vod/HxWB8i8sYkh0DdfvfByoMHqRtezNMCpWJdjzWTOCqdY.mp4\">\n    <img src=\"imgs/cover.png\" alt=\"Watch the video\" style=\"width: 600px;\">\n  </a>\n</div>\n\n\n## Introduction\n\n**(1) Multimodal Instruction-based Generation**\n\nFor traditional subject-driven generation based on concrete objects, DreamOmni2 achieves the best results among open-source models, showing superior identity and pose consistency. Additionally, DreamOmni2 can reference abstract attributes (such as material, texture, makeup, hairstyle, posture, design style, artistic style, etc.), even surpassing commercial models in this area.\n\n**(2) Multimodal Instruction-based Editing**\n\nBeyond traditional instruction-based editing models, DreamOmni2 supports multimodal instruction editing. In everyday editing tasks, there are often elements that are difficult to describe purely with language and require reference images. Our model addresses this need, supporting references to any concrete objects and abstract attributes, with performance comparable to commercial models.\n\n**(3) Unified Generation and Editing Model**\n\nBuilding upon these two new tasks, we introduce DreamOmni2, which is capable of multimodal instruction-based editing and generation under any concrete or abstract concept guidance. Overall, DreamOmni2 is a more intelligent and powerful open-sourced unified generation and editing model, offering enhanced capabilities across a wide range of tasks.\n\n## Editing and Generation Model?\nEditing and generation are distinct tasks. Editing requires strict consistency in preserving the non-edited areas of the source image, while generation only needs to retain the ID, IP, or attribution from the reference image as per the instructions, allowing the entire image to be regenerated with a focus on aesthetics. We’ve found that the instructions for generation and editing are often similar, so we’ve separated these two tasks to make it easier for users to choose the appropriate task type.\n\n## Quick Start\n\n### Requirements and Installation\n\nFirst, install the necessary dependencies:\n```bash\ngit clone https://github.com/dvlab-research/DreamOmni2\ncd ./DreamOmni2\npip install -r requirements.txt\n```\n\nNext, download the DreamOmni2 weights into the models folder.\n\n```bash\nhuggingface-cli download --resume-download --local-dir-use-symlinks False xiabs/DreamOmni2 --local-dir ./models\n```\n\n### Inference\n\nMultimodal Instriction-based Editing\n\n**Notably, for editing tasks, due to the format settings of the training data, we need to place the image to be edited in the first position.**\n\n```bash\npython3 /mnt/bn/unifygen/xiabin_dev/iclr2026/DreamOmni2/inference_edit.py \\\n    --input_img_path \"example_input/edit_tests/src.jpg\" \"example_input/edit_tests/ref.jpg\" \\\n    --input_instruction \"Make the woman from the second image stand on the road in the first image.\" \\\n    --output_path \"example_input/edit_tests/edit_res.png\"\n```\n\nMultimodal Instriction-based Generation\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-10-25T02:21:01.216793"
  },
  {
    "basic_info": {
      "name": "agentic-design-patterns-cn",
      "full_name": "ginobefun/agentic-design-patterns-cn",
      "owner": "ginobefun",
      "description": "《Agentic Design Patterns》中文翻译版",
      "url": "https://github.com/ginobefun/agentic-design-patterns-cn",
      "clone_url": "https://github.com/ginobefun/agentic-design-patterns-cn.git",
      "ssh_url": "git@github.com:ginobefun/agentic-design-patterns-cn.git",
      "homepage": null,
      "created_at": "2025-10-09T04:36:28Z",
      "updated_at": "2025-10-25T02:19:11Z",
      "pushed_at": "2025-10-24T06:14:52Z"
    },
    "stats": {
      "stars": 2009,
      "forks": 224,
      "watchers": 2009,
      "open_issues": 1,
      "size": 7635
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 100809
      },
      "license": null,
      "topics": []
    },
    "content": {
      "readme": "[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/ginobefun-agentic-design-patterns-cn-badge.png)](https://mseep.ai/app/ginobefun-agentic-design-patterns-cn)\n\n# Agentic Design Patterns | <mark>智能体设计模式</mark>\n\n## A Hands-On Guide to Building Intelligent Systems | <mark>构建智能系统的实践指南</mark>\n\n[![License: CC BY-NC 4.0](https://img.shields.io/badge/License-CC%20BY--NC%204.0-lightgrey.svg)](https://creativecommons.org/licenses/by-nc/4.0/)\n[![GitHub stars](https://img.shields.io/github/stars/ginobefun/agentic-design-patterns-cn)](https://github.com/ginobefun/agentic-design-patterns-cn/stargazers)\n[![GitHub forks](https://img.shields.io/github/forks/ginobefun/agentic-design-patterns-cn)](https://github.com/ginobefun/agentic-design-patterns-cn/network)\n\n**原书作者 (Author)**: [Antonio Gulli](https://www.linkedin.com/in/searchguy/)\n\n**原书链接 (Original Book)**: [Amazon](https://www.amazon.com/Agentic-Design-Patterns-Hands-Intelligent/dp/3032014018/)\n\n**原始文档链接 (Original Book Link)**: [Google Docs](https://docs.google.com/document/d/1rsaK53T3Lg5KoGwvf8ukOUvbELRtH-V0LnOIFDxBryE/preview?tab=t.0#heading=h.pxcur8v2qagu)\n\n---\n\n## 📖 项目简介 | Project Description\n\n本项目是对 Antonio Gulli 所著《Agentic Design Patterns: A Hands-On Guide to Building Intelligent Systems》的**中英文对照翻译**。该书是一部全面的技术指南，涵盖了现代人工智能系统中智能体 (Agent) 设计的核心概念和实践方法。\n\nThis project is a **bilingual Chinese-English translation** of \"Agentic Design Patterns: A Hands-On Guide to Building Intelligent Systems\" by Antonio Gulli. The book is a comprehensive technical guide covering core concepts and practical approaches to agent design in modern AI systems.\n\n---\n\n## 🎯 项目特色 | Key Features\n\n- 📚 **中英文对照** - 完整的双语对照翻译\n- 🎨 **高亮显示** - 中文内容使用黄色高亮，易于区分\n- 📝 **格式规范** - 严格遵循 Markdown 标准和翻译规范\n- 🔗 **代码链接** - 保留所有原书代码示例链接\n- ⚡ **持续更新** - 逐章翻译，持续更新进度\n\n---\n\n## 📋 翻译进度 | Translation Progress\n\n**<mark>总页数：424 页</mark>** | **Total: 424 Pages**\n\n### 前置内容 | Front Matter\n\n| 章节 | 概述 | 负责人 | AI 翻译 | 人工评审 | 交叉评审 |\n|------|------|--------|---------|----------|----------|\n| [献辞](01-Dedication.md) | 作者的献辞与致敬 | @ginobefun | ✅ | ✅ | ⏳ |\n| [致谢](02-Acknowledgment.md) | 致谢与感谢名单 | @ginobefun | ✅ | ✅ | ⏳ |\n| [序言](03-Foreword.md) | 本书的序言与背景介绍 | @ginobefun | ✅ | ✅ | ⏳ |\n| [思想领袖的洞见](04-Thought-Leader.md) | 权力与责任的深度思考 | @ginobefun | ✅ | ✅ | ⏳ |\n| [介绍](05-Introduction.md) | 全书引言与核心概念 | @ginobefun | ✅ | ✅ | ⏳ |\n| [什么是\"智能体\"？](06-What-Makes-Agent.md) | 定义 AI 系统的\"智能体\"特征 | @ginobefun | ✅ | ✅ | ⏳ |\n\n### 第一部分：核心设计模式 | Part One: Core Patterns (103 页)\n\n| 章节 | 设计模式概述 | 负责人 | AI 翻译 | 人工评审 | 交叉评审 |\n|------|-------------|--------|---------|----------|----------|\n| [第 1 章：提示链](07-Chapter-01-Prompt-Chaining.md) | 分而治之的任务分解模式，将复杂任务分解为处理流水线 | @ginobefun | ✅ | ✅ | ⏳ |\n| [第 2 章：路由](08-Chapter-02-Routing.md) | 智能决策与动态分发，根据情境选择最佳行动路径 | @ginobefun | ✅ | ✅ | ⏳ |\n| [第 3 章：并行化](09-Chapter-03-Parallelization.md) | 并发执行与性能提升，同时执行多个独立任务 | @ginobefun | ✅ | ✅ | ⏳ |\n| [第 4 章：反思](10-Chapter-04-Reflection.md) | 自我评估和迭代改进，通过反馈循环优化输出质量 | @ginobefun | ✅ | ✅ | ⏳ |\n| [第 5 章：工具使用](11-Chapter-05-Tool-Use.md) | 外部工具与 API 集成，扩展智能体能力边界 | @ginobefun | ✅ | ✅ | ⏳ |\n| [第 6 章：规划](12-Chapter-06-Planning.md) | 多步骤计划制定与执行，实现复杂目标分解 | @ginobefun | ✅ | ✅ | ⏳ |\n| [第 7 章：多智能体协作](13-Chapter-07-Multi-Agent-Collaboration.md) | 协同工作架构，多个智能体配合完成任务 | @ginobefun | ✅  | ❌ | ❌ |\n\n### 第二部分：高级设计模式 | Part Two: Advanced Patterns (61 页)\n\n| 章节 | 设计模式概述 | 负责人 | AI 翻译 | 人工评审 | 交叉评审 |\n|------|-------------|--------|---------|----------|----------|\n| [第 8 章：记忆管理](14-Chapter-08-Memory-Management.md) | 短期和长期记忆管理，维持上下文连续性 | @郑涛 | ✅ | ✅ | ❌ |\n| [第 9 章：学习与适应](15-Chapter-09-Learning-and-Adaptation.md) | 从经验中学习，持续优化智能体行为 | @陈诗中 | ⏳ | ❌ | ❌ |\n| [第 10 章：模型上下文协议](16-Chapter-10-Model-Context-Protocol.md) | 标准化交互协议，规范智能体通信方式 | @郑涛 | ⏳ | ❌ | ❌ |\n| [第 11 章：目标设定与监控](17-Chapter-11-Goal-Setting-and-Monitoring.md) | 动态目标管理，实时追踪任务进展 | [@李浪溪](https://github.com/seabornlee) | ✅ | ✅ | ⏳ |\n\n### 第三部分：集成设计模式 | Part Three: Integration Patterns (34 页)\n\n| 章节 | 设计模式概述 | 负责人 | AI 翻译 | 人工评审 | 交叉评审 |\n|------|-------------|--------|---------|----------|----------|\n| [第 12 章：异常处理与恢复](18-Chapter-12-Exception-Handling-and-Recovery.md) | 优雅错误处理，确保系统稳定性 | @EE | ❌ | ❌ | ❌ |\n| [第 13 章：人机协作](19-Chapter-13-Human-in-the-Loop.md) | 人机协作决策，融合人类智慧与 AI 能力 | @曾汉 | ✅ | ✅ | ⏳ |\n| [第 14 章：知识检索 (RAG)](20-Chapter-14-Knowledge-Retrieval.md) | 检索增强生成技术，结合外部知识库 | @EE | ✅ | ✅ | ⏳ |\n\n### 第四部分：生产设计模式 | Part Four: Production Patterns (114 页)\n\n| 章节 | 设计模式概述 | 负责人 | AI 翻译 | 人工评审 | 交叉评审 |\n|------|-------------|--------|---------|----------|----------|\n| [第 15 章：智能体间通信 (A2A)](21-Chapter-15-Inter-Agent-Communication.md) | 智能体通信协议，实现智能体间高效交互 | @朵朵肥 | ✅ | ❌ | ❌ |\n| [第 16 章：资源感知优化](22-Chapter-16-Resource-Aware-Optimization.md) | 资源优化管理，平衡性能与成本 | @IsaacZhaoo | ✅ | ✅ | ⏳ |\n| [第 17 章：推理技术](23-Chapter-17-Reasoning-Techniques.md) | 增强推理能力，提升决策质量 | @Diqing | ❌ | ❌ | ❌ |\n| [第 18 章：护栏/安全模式](24-Chapter-18-Guardrails-Safety-Patterns.md) | 安全保障机制，防止不当行为 | @IsaacZhaoo | ⏳ | ❌ | ❌ |\n| [第 19 章：评估与监控](25-Chapter-19-Evaluation-and-Monitoring.md) | 性能评估体系，量化智能体表现 | @朵朵肥 | ❌ | ❌ ",
      "default_branch": "main"
    },
    "fetched_at": "2025-10-25T02:21:02.519885"
  },
  {
    "basic_info": {
      "name": "sidekick.nvim",
      "full_name": "folke/sidekick.nvim",
      "owner": "folke",
      "description": "Your Neovim AI sidekick",
      "url": "https://github.com/folke/sidekick.nvim",
      "clone_url": "https://github.com/folke/sidekick.nvim.git",
      "ssh_url": "git@github.com:folke/sidekick.nvim.git",
      "homepage": "",
      "created_at": "2025-09-26T10:26:48Z",
      "updated_at": "2025-10-24T12:56:32Z",
      "pushed_at": "2025-10-24T08:06:14Z"
    },
    "stats": {
      "stars": 1686,
      "forks": 44,
      "watchers": 1686,
      "open_issues": 1,
      "size": 501
    },
    "tech_info": {
      "language": "Lua",
      "languages": {
        "Lua": 216764,
        "Shell": 127
      },
      "license": "Apache License 2.0",
      "topics": [
        "claude-code",
        "codex-cli",
        "copilot",
        "copilot-cli",
        "gemini-cli",
        "neovim",
        "neovim-plugin",
        "nvim",
        "nvim-plugin"
      ]
    },
    "content": {
      "readme": "# 🤖 `sidekick.nvim`\n\n**sidekick.nvim** is your Neovim AI sidekick that integrates Copilot LSP's\n\"Next Edit Suggestions\" with a built-in terminal for any AI CLI.\nReview and apply diffs, chat with AI assistants, and streamline your coding,\nwithout leaving your editor.\n\n<img width=\"2311\" height=\"1396\" alt=\"image\" src=\"https://github.com/user-attachments/assets/63a33610-9a8e-45e2-bbd0-b7e3a4fde621\" />\n\n## ✨ Features\n\n- **🤖 Next Edit Suggestions (NES) powered by Copilot LSP**\n  - 🪄 **Automatic Suggestions**: Fetches suggestions automatically when you pause typing or move the cursor.\n  - 🎨 **Rich Diffs**: Visualizes changes with inline and block-level diffs, featuring Treesitter-based syntax highlighting with granular diffing down to the word or character level.\n  - 🧭 **Hunk-by-Hunk Navigation**: Jump through edits to review them one by one before applying.\n  - 📊 **Statusline Integration**: Shows Copilot LSP's status, request progress, and preview text in your statusline.\n\n- **💬 Integrated AI CLI Terminal**\n  - 🚀 **Direct Access to AI CLIs**: Interact with your favorite AI command-line tools without leaving Neovim.\n  - 📦 **Pre-configured for Popular Tools**: Out-of-the-box support for Claude, Gemini, Grok, Codex, Copilot CLI, and more.\n  - ✨ **Context-Aware Prompts**: Automatically include file content, cursor position, and diagnostics in your prompts.\n  - 📝 **Prompt Library**: A library of pre-defined prompts for common tasks like explaining code, fixing issues, or writing tests.\n  - 🔄 **Session Persistence**: Keep your CLI sessions alive with `tmux` and `zellij` integration.\n  - 📂 **Automatic File Watching**: Automatically reloads files in Neovim when they are modified by AI tools.\n\n- **🔌 Extensible and Customizable**\n  - ⚙️ **Flexible Configuration**: Fine-tune every aspect of the plugin to your liking.\n  - 🧩 **Plugin-Friendly API**: A rich API for integrating with other plugins and building custom workflows.\n  - 🎨 **Customizable UI**: Change the appearance of diffs, signs, and more.\n\n## 📋 Requirements\n\n- **Neovim** `>= 0.11.2` or newer\n- The official [copilot-language-server](https://github.com/github/copilot-language-server-release) LSP server,\n  enabled with `vim.lsp.enable`. Can be installed in multiple ways:\n  1. install using `npm` or your OS's package manager\n  2. install with [mason-lspconfig.nvim](https://github.com/mason-org/mason-lspconfig.nvim)\n  3. [copilot.lua](https://github.com/zbirenbaum/copilot.lua) and [copilot.vim](https://github.com/github/copilot.vim)\n     both bundle the LSP Server in their plugin.\n- A working `lsp/copilot.lua` configuration.\n  - **TIP:** Included in [nvim-lspconfig](https://github.com/neovim/nvim-lspconfig)\n- [snacks.nvim](https://github.com/folke/snacks.nvim) for better prompt/tool selection **_(optional)_**\n- [nvim-treesitter-textobjects](https://github.com/nvim-treesitter/nvim-treesitter-textobjects) **_(`main` branch)_** for `{function}` and `{class}` context variables **_(optional)_**\n- AI cli tools, such as Codex, Claude, Copilot, Gemini, … **_(optional)_**\n  see the [🤖 AI CLI Integration](#-ai-cli-integration) section for details.\n- [lsof](https://man7.org/linux/man-pages/man8/lsof.8.html) and [ps](https://man7.org/linux/man-pages/man1/ps.1.html) are used\n  on Unix-like systems to detect running AI CLI tool sessions. **_(optional, but recommended)_**\n\n## 🚀 Quick Start\n\n1. **Install** the plugin with your package manager (see below)\n2. **Configure Copilot LSP** - must be enabled with `vim.lsp.enable`\n3. **Check health**: `:checkhealth sidekick`\n4. **Sign in to Copilot**: `:LspCopilotSignIn`\n5. **Try it out**:\n   - Type some code and pause - watch for Next Edit Suggestions appearing\n   - Press `<Tab>` to navigate through or apply suggestions\n   - Use `<leader>aa` to open AI CLI tools\n\n> [!NOTE]\n> **New to Next Edit Suggestions?** Unlike inline completions, NES suggests entire refactorings or multi-line changes anywhere in your file - think of it as Copilot's \"big picture\" suggestions.\n\n## 📦 Installation\n\nInstall with your favorite manager. With [lazy.nvim](https://github.com/folke/lazy.nvim):\n\n<!-- setup_base:start -->\n\n```lua\n{\n  \"folke/sidekick.nvim\",\n  opts = {\n    -- add any options here\n    cli = {\n      mux = {\n        backend = \"zellij\",\n        enabled = true,\n      },\n    },\n  },\n  keys = {\n    {\n      \"<tab>\",\n      function()\n        -- if there is a next edit, jump to it, otherwise apply it if any\n        if not require(\"sidekick\").nes_jump_or_apply() then\n          return \"<Tab>\" -- fallback to normal tab\n        end\n      end,\n      expr = true,\n      desc = \"Goto/Apply Next Edit Suggestion\",\n    },\n    {\n      \"<c-.>\",\n      function() require(\"sidekick.cli\").toggle() end,\n      desc = \"Sidekick Toggle\",\n      mode = { \"n\", \"t\", \"i\", \"x\" },\n    },\n    {\n      \"<leader>aa\",\n      function() require(\"sidekick.cli\").toggle() end,\n      desc = \"Sidekick Toggle CLI\",\n    },\n    {\n      \"<leader>as\",\n      function() require(\"sidekick.cli\").select() end,\n ",
      "default_branch": "main"
    },
    "fetched_at": "2025-10-25T02:21:03.830443"
  },
  {
    "basic_info": {
      "name": "Skill_Seekers",
      "full_name": "yusufkaraaslan/Skill_Seekers",
      "owner": "yusufkaraaslan",
      "description": "Single powerful tool to convert ANY documentation website into a Claude skill",
      "url": "https://github.com/yusufkaraaslan/Skill_Seekers",
      "clone_url": "https://github.com/yusufkaraaslan/Skill_Seekers.git",
      "ssh_url": "git@github.com:yusufkaraaslan/Skill_Seekers.git",
      "homepage": "",
      "created_at": "2025-10-17T14:43:48Z",
      "updated_at": "2025-10-25T02:12:54Z",
      "pushed_at": "2025-10-23T19:02:43Z"
    },
    "stats": {
      "stars": 1665,
      "forks": 151,
      "watchers": 1665,
      "open_issues": 132,
      "size": 524
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 367408,
        "Shell": 7440
      },
      "license": "MIT License",
      "topics": [
        "ai-tools",
        "automation",
        "claude-ai",
        "claude-skills",
        "documentation",
        "documentation-generator",
        "mcp",
        "mcp-server",
        "python",
        "web-scraping"
      ]
    },
    "content": {
      "readme": "[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/yusufkaraaslan-skill-seekers-badge.png)](https://mseep.ai/app/yusufkaraaslan-skill-seekers)\n\n# Skill Seeker\n\n[![Version](https://img.shields.io/badge/version-1.2.0-blue.svg)](https://github.com/yusufkaraaslan/Skill_Seekers/releases/tag/v1.2.0)\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)\n[![MCP Integration](https://img.shields.io/badge/MCP-Integrated-blue.svg)](https://modelcontextprotocol.io)\n[![Tested](https://img.shields.io/badge/Tests-142%20Passing-brightgreen.svg)](tests/)\n[![Project Board](https://img.shields.io/badge/Project-Board-purple.svg)](https://github.com/users/yusufkaraaslan/projects/2)\n\n**Automatically convert any documentation website into a Claude AI skill in minutes.**\n\n> 📋 **[View Development Roadmap & Tasks](https://github.com/users/yusufkaraaslan/projects/2)** - 134 tasks across 10 categories, pick any to contribute!\n\n## What is Skill Seeker?\n\nSkill Seeker is an automated tool that transforms any documentation website into a production-ready [Claude AI skill](https://claude.ai). Instead of manually reading and summarizing documentation, Skill Seeker:\n\n1. **Scrapes** documentation websites automatically\n2. **Organizes** content into categorized reference files\n3. **Enhances** with AI to extract best examples and key concepts\n4. **Packages** everything into an uploadable `.zip` file for Claude\n\n**Result:** Get comprehensive Claude skills for any framework, API, or tool in 20-40 minutes instead of hours of manual work.\n\n## Why Use This?\n\n- 🎯 **For Developers**: Quickly create Claude skills for your favorite frameworks (React, Vue, Django, etc.)\n- 🎮 **For Game Devs**: Generate skills for game engines (Godot, Unity documentation, etc.)\n- 🔧 **For Teams**: Create internal documentation skills for your company's APIs\n- 📚 **For Learners**: Build comprehensive reference skills for technologies you're learning\n\n## Key Features\n\n### 🌐 Documentation Scraping\n- ✅ **Universal Scraper** - Works with ANY documentation website\n- ✅ **Smart Categorization** - Automatically organizes content by topic\n- ✅ **Code Language Detection** - Recognizes Python, JavaScript, C++, GDScript, etc.\n- ✅ **8 Ready-to-Use Presets** - Godot, React, Vue, Django, FastAPI, and more\n\n### 📄 PDF Support (**v1.2.0**)\n- ✅ **Basic PDF Extraction** - Extract text, code, and images from PDF files\n- ✅ **OCR for Scanned PDFs** - Extract text from scanned documents\n- ✅ **Password-Protected PDFs** - Handle encrypted PDFs\n- ✅ **Table Extraction** - Extract complex tables from PDFs\n- ✅ **Parallel Processing** - 3x faster for large PDFs\n- ✅ **Intelligent Caching** - 50% faster on re-runs\n\n### 🤖 AI & Enhancement\n- ✅ **AI-Powered Enhancement** - Transforms basic templates into comprehensive guides\n- ✅ **No API Costs** - FREE local enhancement using Claude Code Max\n- ✅ **MCP Server for Claude Code** - Use directly from Claude Code with natural language\n\n### ⚡ Performance & Scale\n- ✅ **Large Documentation Support** - Handle 10K-40K+ page docs with intelligent splitting\n- ✅ **Router/Hub Skills** - Intelligent routing to specialized sub-skills\n- ✅ **Parallel Scraping** - Process multiple skills simultaneously\n- ✅ **Checkpoint/Resume** - Never lose progress on long scrapes\n- ✅ **Caching System** - Scrape once, rebuild instantly\n\n### ✅ Quality Assurance\n- ✅ **Fully Tested** - 142 tests with 100% pass rate\n\n## Quick Example\n\n### Option 1: Use from Claude Code (Recommended)\n\n```bash\n# One-time setup (5 minutes)\n./setup_mcp.sh\n\n# Then in Claude Code, just ask:\n\"Generate a React skill from https://react.dev/\"\n\"Scrape PDF at docs/manual.pdf and create skill\"\n```\n\n**Time:** Automated | **Quality:** Production-ready | **Cost:** Free\n\n### Option 2: Use CLI Directly (HTML Docs)\n\n```bash\n# Install dependencies (2 pip packages)\npip3 install requests beautifulsoup4\n\n# Generate a React skill in one command\npython3 cli/doc_scraper.py --config configs/react.json --enhance-local\n\n# Upload output/react.zip to Claude - Done!\n```\n\n**Time:** ~25 minutes | **Quality:** Production-ready | **Cost:** Free\n\n### Option 3: Use CLI for PDF Documentation\n\n```bash\n# Install PDF support\npip3 install PyMuPDF\n\n# Basic PDF extraction\npython3 cli/pdf_scraper.py --pdf docs/manual.pdf --name myskill\n\n# Advanced features\npython3 cli/pdf_scraper.py --pdf docs/manual.pdf --name myskill \\\n    --extract-tables \\        # Extract tables\n    --parallel \\              # Fast parallel processing\n    --workers 8               # Use 8 CPU cores\n\n# Scanned PDFs (requires: pip install pytesseract Pillow)\npython3 cli/pdf_scraper.py --pdf docs/scanned.pdf --name myskill --ocr\n\n# Password-protected PDFs\npython3 cli/pdf_scraper.py --pdf docs/encrypted.pdf --name myskill --password mypassword\n\n# Upload output/myskill.zip to Claude - Done!\n```\n\n**Time:** ~5-15 minutes (or 2-5 minut",
      "default_branch": "development"
    },
    "fetched_at": "2025-10-25T02:21:05.118363"
  },
  {
    "basic_info": {
      "name": "bentopdf",
      "full_name": "alam00000/bentopdf",
      "owner": "alam00000",
      "description": "A Privacy First PDF Toolkit",
      "url": "https://github.com/alam00000/bentopdf",
      "clone_url": "https://github.com/alam00000/bentopdf.git",
      "ssh_url": "git@github.com:alam00000/bentopdf.git",
      "homepage": "https://bentopdf.com/",
      "created_at": "2025-10-12T13:30:08Z",
      "updated_at": "2025-10-25T01:58:54Z",
      "pushed_at": "2025-10-24T21:27:40Z"
    },
    "stats": {
      "stars": 1644,
      "forks": 104,
      "watchers": 1644,
      "open_issues": 29,
      "size": 995
    },
    "tech_info": {
      "language": "TypeScript",
      "languages": {
        "TypeScript": 539900,
        "HTML": 99199,
        "CSS": 9983,
        "JavaScript": 4444,
        "Dockerfile": 469
      },
      "license": "Other",
      "topics": [
        "bentopdf",
        "hacktoberfest",
        "hacktoberfest-accepted",
        "javascript",
        "jpgtopdf",
        "pdf",
        "pdf-converter",
        "pdf-document",
        "pdf-document-processor",
        "pdf-generation",
        "pdf-viewer",
        "pdffiller",
        "privacy",
        "toolkit",
        "typescript"
      ]
    },
    "content": {
      "readme": "# BentoPDF\n\n**BentoPDF** is a powerful, privacy-first, client-side PDF toolkit that allows you to manipulate, edit, merge, and process PDF files directly in your browser. No server-side processing is required, ensuring your files remain secure and private.\n\n![Docker Pulls](https://img.shields.io/docker/pulls/bentopdf/bentopdf) [![Ko-fi](https://img.shields.io/badge/Buy%20me%20a%20Coffee-yellow?logo=kofi&style=flat-square)](https://ko-fi.com/alio0) ![GitHub Stars](https://img.shields.io/github/stars/alam00000/bentopdf?style=social)\n[![Sponsor me on GitHub](https://img.shields.io/badge/Sponsor-%E2%9D%A4-ff69b4)](https://github.com/sponsors/alam00000)\n\n## ⭐ Stargazers over time\n\n[![Star History Chart](https://api.star-history.com/svg?repos=alam00000/bentopdf&type=Date)](https://star-history.com/#alam00000/bentopdf&Date)\n\n---\n\n## ✨ Why BentoPDF?\n\n- **Privacy First**: All processing happens in your browser. Your files are never uploaded to a server, guaranteeing 100% privacy.\n- **No Limits**: Manipulate as many files as you want, as often you want. There are no restrictions or upload limits.\n- **High Performance**: Built with modern web technologies, BentoPDF is fast and efficient, handling even large PDF files with ease.\n- **Completely Free**: BentoPDF is a free and open-source tool for everyone.\n\n---\n\n## 🛠️ Features / Tools Supported\n\nBentoPDF offers a comprehensive suite of tools to handle all your PDF needs.\n\n### Organize & Manage PDFs\n\n| Tool Name                 | Description                                                                |\n| :------------------------ | :------------------------------------------------------------------------- |\n| **Merge PDFs**            | Combine multiple PDF files into one.                                       |\n| **Split PDFs**            | Extract specific pages or divide a document into smaller files.            |\n| **Organize Pages**        | Reorder, duplicate, or delete pages with a simple drag-and-drop interface. |\n| **Extract Pages**         | Save a specific range of pages as a new PDF.                               |\n| **Delete Pages**          | Remove unwanted pages from your document.                                  |\n| **Rotate PDF**            | Rotate individual or all pages in a document.                              |\n| **N-Up PDF**              | Combine multiple pages onto a single page.                                 |\n| **View PDF**              | A powerful, integrated PDF viewer.                                         |\n| **Alternate & Mix pages** | Merge pages by alternating pages from each PDF.                            |\n| **Posterize PDF**         | Split a PDF into multiple smaller pages for print.                         |\n\n### Edit & Modify PDFs\n\n| Tool Name              | Description                                                 |\n| :--------------------- | :---------------------------------------------------------- |\n| **PDF Editor**         | A comprehensive editor to modify your PDFs.                 |\n| **Add Page Numbers**   | Easily add page numbers with customizable formatting.       |\n| **Add Watermark**      | Add text or image watermarks to protect your documents.     |\n| **Header & Footer**    | Add customizable headers and footers.                       |\n| **Crop PDF**           | Crop specific pages or the entire document.                 |\n| **Invert Colors**      | Invert the colors of your PDF pages for better readability. |\n| **Change Background**  | Modify the background color of your PDF.                    |\n| **Change Text Color**  | Change the color of text content within the PDF.            |\n| **Fill Forms**         | Fill out PDF forms directly in your browser.                |\n| **Flatten PDF**        | Flatten form fields and annotations into static content.    |\n| **Remove Annotations** | Remove comments, highlights, and other annotations.         |\n| **Remove Blank Pages** | Auto detect and remove blank pages in a PDF.                |\n\n### Convert to PDF\n\n| Tool Name           | Description                                                     |\n| :------------------ | :-------------------------------------------------------------- |\n| **Image to PDF**    | Convert JPG, PNG, WebP, SVG, BMP, HEIC, and TIFF images to PDF. |\n| **Markdown to PDF** | Convert `.md` files into professional PDF documents.            |\n| **Text to PDF**     | Convert plain text files into a PDF.                            |\n\n### Convert from PDF\n\n| Tool Name            | Description                                                                    |\n| :------------------- | :----------------------------------------------------------------------------- |\n| **PDF to Image**     | Convert PDF pages to JPG, PNG, WebP, BMP, or TIFF formats.                     |\n| **PDF to Greyscale** | Convert a color PDF into a black-and-white version.                            |\n| **OCR PDF**          | Make scanned PDFs searchable and copyable ",
      "default_branch": "main"
    },
    "fetched_at": "2025-10-25T02:21:06.403289"
  },
  {
    "basic_info": {
      "name": "open-agent-builder",
      "full_name": "firecrawl/open-agent-builder",
      "owner": "firecrawl",
      "description": "🔥 Visual workflow builder for AI agents powered by Firecrawl - drag-and-drop web scraping pipelines with real-time execution",
      "url": "https://github.com/firecrawl/open-agent-builder",
      "clone_url": "https://github.com/firecrawl/open-agent-builder.git",
      "ssh_url": "git@github.com:firecrawl/open-agent-builder.git",
      "homepage": null,
      "created_at": "2025-10-16T15:34:46Z",
      "updated_at": "2025-10-25T01:22:15Z",
      "pushed_at": "2025-10-20T15:15:47Z"
    },
    "stats": {
      "stars": 1628,
      "forks": 260,
      "watchers": 1628,
      "open_issues": 6,
      "size": 1104
    },
    "tech_info": {
      "language": "TypeScript",
      "languages": {
        "TypeScript": 1583973,
        "CSS": 48176,
        "JavaScript": 4757
      },
      "license": null,
      "topics": []
    },
    "content": {
      "readme": "# Open Agent Builder\n\n<p align=\"center\">\n  <img src=\"https://media4.giphy.com/media/v1.Y2lkPTc5MGI3NjExcGNoY25xY2ptZTZtcDN6czBmdXJ2dnpkdWVjcXlqNXNhdjgyZXpkaiZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/tWtopK29eXAbvaDpi5/giphy.gif\" alt=\"Demo\" width=\"100%\" />\n</p>\n\n<div align=\"center\">\n\n**Build, test, and deploy AI agent workflows with a visual no-code interface**\n\n[![MIT License](https://img.shields.io/badge/License-MIT-green.svg)](https://choosealicense.com/licenses/mit/)\n[![Firecrawl](https://img.shields.io/badge/Powered%20by-Firecrawl-orange)](https://firecrawl.dev)\n\n[Documentation](#documentation) • [Examples](#example-workflows)\n\n</div>\n\n---\n\n## What is Open Agent Builder?\n\nOpen Agent Builder is a visual workflow builder for creating AI agent pipelines powered by [Firecrawl](https://firecrawl.dev). Design complex agent workflows with a drag-and-drop interface, then execute them with real-time streaming updates.\n\n**Perfect for:**\n- Web scraping and data extraction workflows\n- Multi-step AI agent pipelines\n- Automated research and content generation\n- Data transformation and analysis\n- Web automation with human-in-the-loop approvals\n\n> **Note:** This project is actively under development. Some features are still in progress and we welcome contributions and PRs!\n\n---\n\n## Key Features\n\n### Visual Workflow Builder\n- **Drag-and-drop interface** for building agent workflows\n- **Real-time execution** with streaming updates\n- **8 core node types**: Start, Agent, MCP Tools, Transform, If/Else, While Loop, User Approval, End\n- **Template library** with pre-built workflows\n- **MCP protocol support** for extensible tool integration\n\n### Powered by Firecrawl\n- **Native Firecrawl integration** for web scraping and searching\n\n### Enterprise Features\n- **LangGraph execution engine** for reliable state management\n- **Clerk authentication** for secure multi-user access\n- **Convex database** for persistent storage\n- **API endpoints** for programmatic execution\n- **Human-in-the-loop** approvals for sensitive operations\n\n---\n\n## Tech Stack\n\n| Technology | Purpose |\n|-----------|---------|\n| **[Firecrawl](https://firecrawl.dev)** | Web scraping API for converting websites into LLM-ready data |\n| **[Next.js 16 (canary)](https://nextjs.org/)** | React framework with App Router for frontend and API routes |\n| **[TypeScript](https://www.typescriptlang.org/)** | Type-safe development across the stack |\n| **[LangGraph](https://github.com/langchain-ai/langgraph)** | Workflow orchestration engine with state management, conditional routing, and human-in-the-loop support |\n| **[Convex](https://convex.dev)** | Real-time database with automatic reactivity for workflows, executions, and user data |\n| **[Clerk](https://clerk.com)** | Authentication and user management with JWT integration |\n| **[Tailwind CSS](https://tailwindcss.com/)** | Utility-first CSS framework for responsive UI |\n| **[React Flow](https://reactflow.dev/)** | Visual workflow builder canvas with drag-and-drop nodes |\n| **[Anthropic](https://www.anthropic.com/)** | Claude AI integration with native MCP support (Claude Haiku 4.5 & Sonnet 4.5) |\n| **[OpenAI](https://platform.openai.com/)** | gpt-5 integration (MCP support coming soon) |\n| **[Groq](https://groq.com/)** | Fast inference for open models (MCP support coming soon) |\n| **[E2B](https://e2b.dev)** | Sandboxed code execution for secure transform nodes |\n| **[Vercel](https://vercel.com)** | Deployment platform with edge functions |\n\n---\n\n## Prerequisites\n\nBefore you begin, you'll need:\n\n1. **Node.js 18+** installed on your machine\n2. **Firecrawl API key** (Required for web scraping) - [Get one here](https://firecrawl.dev)\n3. **Convex account** - [Sign up free](https://convex.dev)\n4. **Clerk account** - [Sign up free](https://clerk.com)\n\n> **Note:** LLM API keys can be added directly in the UI via Settings → API Keys after setup. For MCP tool support, Anthropic Claude (Haiku 4.5 or Sonnet 4.5) is currently recommended as the default option.\n\n---\n\n## Installation & Setup\n\n### 1. Clone the Repository\n\n```bash\ngit clone https://github.com/firecrawl/open-agent-builder.git\ncd open-agent-builder\nnpm install\n```\n\n### 2. Set Up Convex (Database)\n\nConvex handles all workflow and execution data persistence.\n\n```bash\n# Install Convex CLI globally\nnpm install -g convex\n\n# Initialize Convex project\nnpx convex dev\n```\n\nThis will:\n- Open your browser to create/link a Convex project\n- Generate a `NEXT_PUBLIC_CONVEX_URL` in your `.env.local`\n- Start the Convex development server\n\nKeep the Convex dev server running in a separate terminal.\n\n### 3. Set Up Clerk (Authentication)\n\nClerk provides secure user authentication and management.\n\n1. Go to [clerk.com](https://clerk.com) and create a new application\n2. In your Clerk dashboard:\n   - Go to **API Keys**\n   - Copy your keys\n3. Go to **JWT Templates** → **Convex**:\n   - Click \"Apply\"\n   - Copy the issuer URL\n\nAdd to your `.env.local`:\n\n```bash\n# Clerk Authentication\nNEXT_PUBLIC_CLERK_PUBLI",
      "default_branch": "main"
    },
    "fetched_at": "2025-10-25T02:21:07.722492"
  },
  {
    "basic_info": {
      "name": "surf",
      "full_name": "deta/surf",
      "owner": "deta",
      "description": "Personal AI Notebooks. Organize files & webpages and generate notes from them. Open source, local & open data, open model choice (incl. local).",
      "url": "https://github.com/deta/surf",
      "clone_url": "https://github.com/deta/surf.git",
      "ssh_url": "git@github.com:deta/surf.git",
      "homepage": "https://deta.surf",
      "created_at": "2025-10-20T15:09:57Z",
      "updated_at": "2025-10-25T02:18:34Z",
      "pushed_at": "2025-10-24T14:50:17Z"
    },
    "stats": {
      "stars": 1587,
      "forks": 95,
      "watchers": 1587,
      "open_issues": 12,
      "size": 274858
    },
    "tech_info": {
      "language": "TypeScript",
      "languages": {
        "TypeScript": 1876766,
        "Svelte": 1616300,
        "Rust": 675781,
        "JavaScript": 39662,
        "SCSS": 39395,
        "CSS": 18112,
        "HTML": 4089,
        "NSIS": 3077,
        "Handlebars": 1368,
        "Shell": 369
      },
      "license": "Apache License 2.0",
      "topics": [
        "claude",
        "deepseek",
        "gemma",
        "knowledge-base",
        "knowledge-management",
        "llm",
        "local",
        "local-llm",
        "ollama",
        "openai",
        "productivity",
        "rust",
        "svelte",
        "typescript"
      ]
    },
    "content": {
      "readme": "<div align=\"center\">\n  \n![splash](./docs/assets/repo-header.png)\n\n[**Website**](https://deta.surf) - [**Discord**](https://deta.surf/discord)\n\n</div>\n\n<br>\n\n# Deta Surf: Your AI Notebook\n\nDeta Surf is an AI notebook that brings all your files and the web directly into your stream of thought.\n\nIt’s meant for simultaneous research and thinking that minimizes the grunt work: manually searching, opening windows & tabs, scrolling, copying and pasting into a document editor.\n\nSurf is primarily built in Svelte, TypeScript and Rust, runs on MacOS, Windows & Linux, stores data locally in open formats, and is open source.\n\n![split](./docs/assets/split-note.webp)\n\n## Motivation\n\nMost applications are focused on a single task, or a single media type: notes, websites, or PDFs. Real thinking requires juggling media across sources to make connections and synthesize ideas. We want to help people think better, across all their media.\n\nSurf is built to be personal and open, in service of the user. This means local first data, open data formats, open source, and openness with respect to AI models. [Read more](https://deta.surf/motivation).\n\n## Installation\n\nCheckout the [GitHub releases](https://github.com/deta/surf/releases) for the latest stable version of Surf for MacOS, Windows and Linux.\n\nYou can also download Surf with some managed & additional features (e.g. AI) from the [Deta website](https://deta.surf). That version is subject to different terms.\n\nFor building from source and local development, see [CONTRIBUTING.md](CONTRIBUTING.md).\n\n## TL;DR - Things to try\n\n- _YouTube Notes_: visit a YouTube video and ask a question\n- _PDF Notes_: open a PDF and ask a question\n- _Create an applet_: use the \"app generation\" tool and ask for an app\n- _Notes that search the web_: use the \"web search\" tool and ask a question with \"search\" in it\n\n## Features\n\n### Multi-Media Library & Notebooks\n\n![notebooks](./docs/assets/readme/notebook-grid.png)\n\nStore almost any media in a private library on your computer, in an open and transparent format.\n\n- Support for local files, sites & links from the web (YouTube, Tweets & more), or create media directly in Surf.\n- Organize this library into Notebooks.\n- Open and use much of your library offline.\n- Use your library to power Surf’s AI features.\n\nSurf's library is built on a local storage engine called SFFS (Surf Flat File System), which stores data in open and transparent formats.\n\n[Details on the library](/docs/LIBRARY.md).\n\n### Smart Notes\n\n![smart-notes](./docs/assets/readme/smart-notes.png)\n\nExplore and think across your digital stuff without opening up a bunch of windows, clicking, scrolling and copying & pasting into your document (or chatbot).\n\n- `@-mention` and auto-generate from any tab, website or any resource in your [library](./docs/LIBRARY.md).\n- Trigger [web searches](./docs/SMART_NOTES.md#web-search) to do research, and bring the results back in your notes.\n- Integrated [citations](./docs/SMART_NOTES.md#citations) deeplinked to original sources, whether a section on a webpage, a timestamp in a video, or a page in a PDF.\n- Generate interactive applications without writing code using [Surflets](./docs/Surflets.md).\n- Paste in images, tables or data from other applications and have Surf understand and incorporate them.\n- Use rich formating, code blocks, to-do lists and more in your notes.\n\n[Read more](/docs/SMART_NOTES.md).\n\n### Tabs, Split View & Sidebar\n\n![split](./docs/assets/another-split.webp)\n\nSurf is built around tabs, split view and a sidebar for easy navigation.\n\n- Open local notes, files or web pages in tabs.\n- Split view allows you to view and interact with multiple resources side by side.\n- The sidebar provides quick access to your Notebooks & notes.\n\n### Surflets (App Generation)\n\n![surflets](./docs/assets/readme/surflets.png)\n\nSurf can code interactive applets to help you visualize, understand or explore concepts or data that are aided with code.\n\n[Read more](./docs/SURFLETS.md).\n\n### AI\n\n![models.png](./docs/assets/readme/models.png)\n\n[Surf’s notes](./docs/SMART_NOTES.md) and [Surflets](./docs/SURFLETS.md) are powered by large language models of your choice.\n\n- Bring your own key for popular models\n- Add a cloud model\n- Use Local Language Models\n\n[Read more](./docs/AI_MODELS.md).\n\n### Shortcuts\n\nFind the most common shortcuts [here](./docs/SHORTCUTS.md).\n\n## Security\n\n_To report a security concern, please see_ https://github.com/deta/surf/security/policy\n\n## Contributing\n\nSee [CONTRIBUTING.md](CONTRIBUTING.md) for details on contributing to the project and an overview of the codebase.\n\n## Code of Conduct\n\nSee [CODE_OF_CONDUCT.md](CODE_OF_CONDUCT.md) for details on our code of conduct.\n\n## License\n\nThe source code for this project is licensed under the Apache 2.0 license, with the following exceptions:\n\n1. Our patch for the @ghostery/adblocker-electron package is licensed under the Mozilla Public License 2.0 (MPL-2.0), consistent with the upstream project's licensing.",
      "default_branch": "main"
    },
    "fetched_at": "2025-10-25T02:21:09.137262"
  }
]