[
  {
    "basic_info": {
      "name": "planning-with-files",
      "full_name": "OthmanAdi/planning-with-files",
      "owner": "OthmanAdi",
      "description": "Claude Code skill implementing Manus-style persistent markdown planning â€” the workflow pattern behind the $2B acquisition.",
      "url": "https://github.com/OthmanAdi/planning-with-files",
      "clone_url": "https://github.com/OthmanAdi/planning-with-files.git",
      "ssh_url": "git@github.com:OthmanAdi/planning-with-files.git",
      "homepage": "https://www.aikux.ai",
      "created_at": "2026-01-03T07:37:28Z",
      "updated_at": "2026-01-25T03:06:21Z",
      "pushed_at": "2026-01-22T13:37:19Z"
    },
    "stats": {
      "stars": 11167,
      "forks": 1022,
      "watchers": 11167,
      "open_issues": 13,
      "size": 277
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 25314,
        "PowerShell": 14492,
        "Shell": 13656
      },
      "license": "MIT License",
      "topics": [
        "agent",
        "agent-skills",
        "agents",
        "antigravity",
        "claude",
        "claude-code",
        "claude-skills",
        "cursor",
        "factory-ai",
        "file-system",
        "file-system-access",
        "kilocode",
        "langchain",
        "langgraph",
        "manus",
        "manus-ai",
        "prompt-engineering",
        "reverse-engineering",
        "zod",
        "zod-validation"
      ]
    },
    "content": {
      "readme": "# Planning with Files\n\n> **Work like Manus** â€” the AI agent company Meta acquired for **$2 billion**.\n\n## Thank You\n\nTo everyone who starred, forked, and shared this skill â€” thank you. This project blew up in less than 24 hours, and the support from the community has been incredible.\n\nIf this skill helps you work smarter, that's all I wanted.\n\n---\n\nA Claude Code plugin that transforms your workflow to use persistent markdown files for planning, progress tracking, and knowledge storage â€” the exact pattern that made Manus worth billions.\n\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n[![Claude Code Plugin](https://img.shields.io/badge/Claude%20Code-Plugin-blue)](https://code.claude.com/docs/en/plugins)\n[![Claude Code Skill](https://img.shields.io/badge/Claude%20Code-Skill-green)](https://code.claude.com/docs/en/skills)\n[![Cursor Skills](https://img.shields.io/badge/Cursor-Skills-purple)](https://docs.cursor.com/context/skills)\n[![Kilocode Skills](https://img.shields.io/badge/Kilocode-Skills-orange)](https://kilo.ai/docs/agent-behavior/skills)\n[![Gemini CLI](https://img.shields.io/badge/Gemini%20CLI-Skills-4285F4)](https://geminicli.com/docs/cli/skills/)\n[![Version](https://img.shields.io/badge/version-2.7.1-brightgreen)](https://github.com/OthmanAdi/planning-with-files/releases)\n[![SkillCheck Validated](https://img.shields.io/badge/SkillCheck-Validated-4c1)](https://getskillcheck.com)\n\n## Quick Install\n\n```bash\n# Install the plugin\nclaude plugins install OthmanAdi/planning-with-files\n```\n\nThat's it! Now use `/planning-with-files:start` in Claude Code.\n\n**Alternative:** If you want `/planning-with-files` (without `:start`), copy skills to your local folder:\n\n```bash\n# Optional: Copy skills for /planning-with-files command\ncp -r ~/.claude/plugins/cache/planning-with-files/planning-with-files/*/skills/planning-with-files ~/.claude/skills/\n```\n\n**Windows (PowerShell):**\n```powershell\n# Install the plugin\nclaude plugins install OthmanAdi/planning-with-files\n\n# Optional: Copy skills for /planning-with-files command\nCopy-Item -Recurse -Path \"$env:USERPROFILE\\.claude\\plugins\\cache\\planning-with-files\\planning-with-files\\*\\skills\\planning-with-files\" -Destination \"$env:USERPROFILE\\.claude\\skills\\\"\n```\n\nSee [docs/installation.md](docs/installation.md) for all installation methods.\n\n## Supported IDEs\n\n| IDE | Status | Installation Guide | Format |\n|-----|--------|-------------------|--------|\n| Claude Code | âœ… Full Support | [Installation](docs/installation.md) | Plugin + SKILL.md |\n| Gemini CLI | âœ… Full Support | [Gemini Setup](docs/gemini.md) | Agent Skills |\n| Cursor | âœ… Full Support | [Cursor Setup](docs/cursor.md) | Skills |\n| Kilocode | âœ… Full Support | [Kilocode Setup](docs/kilocode.md) | Skills |\n| OpenCode | âœ… Full Support | [OpenCode Setup](docs/opencode.md) | Personal/Project Skill |\n| Codex | âœ… Full Support | [Codex Setup](docs/codex.md) | Personal Skill |\n| FactoryAI Droid | âœ… Full Support | [Factory Setup](docs/factory.md) | Workspace/Personal Skill |\n| Antigravity | âœ… Full Support | [Antigravity Setup](docs/antigravity.md) | Workspace/Personal Skill |\n\n> **Note:** If your IDE uses the legacy Rules system instead of Skills, see the [`legacy-rules-support`](https://github.com/OthmanAdi/planning-with-files/tree/legacy-rules-support) branch.\n\n## Documentation\n\n| Document | Description |\n|----------|-------------|\n| [Installation Guide](docs/installation.md) | All installation methods (plugin, manual, Cursor, Windows) |\n| [Quick Start](docs/quickstart.md) | 5-step guide to using the pattern |\n| [Workflow Diagram](docs/workflow.md) | Visual diagram of how files and hooks interact |\n| [Troubleshooting](docs/troubleshooting.md) | Common issues and solutions |\n| [Gemini CLI Setup](docs/gemini.md) | Google Gemini CLI integration guide |\n| [Cursor Setup](docs/cursor.md) | Cursor IDE-specific instructions |\n| [Windows Setup](docs/windows.md) | Windows-specific notes |\n| [Kilo Code Support](docs/kilocode.md) | Kilo Code integration guide |\n| [Codex Setup](docs/codex.md) | Codex IDE installation and usage |\n| [OpenCode Setup](docs/opencode.md) | OpenCode IDE installation, oh-my-opencode config |\n| [FactoryAI Droid Setup](docs/factory.md) | FactoryAI Droid integration guide |\n| [Antigravity Setup](docs/antigravity.md) | Antigravity IDE integration guide |\n\n## Versions\n\n| Version | Features | Install |\n|---------|----------|---------|\n| **v2.7.1** (current) | Dynamic Python detection fix | `claude plugins install OthmanAdi/planning-with-files` |\n| **v2.7.0** | Gemini CLI support | See [releases](https://github.com/OthmanAdi/planning-with-files/releases) |\n| **v2.6.0** | Start command (`/planning-with-files:start`), path resolution fix | See [releases](https://github.com/OthmanAdi/planning-with-files/releases) |\n| **v2.5.0** | Fixed autocomplete - SKILL.md matches Anthropic format | See [releases](https://github.com/OthmanAdi/planning-with-files/releases) |\n| *",
      "default_branch": "master"
    },
    "fetched_at": "2026-01-25T03:07:31.264003"
  },
  {
    "basic_info": {
      "name": "DeepTutor",
      "full_name": "HKUDS/DeepTutor",
      "owner": "HKUDS",
      "description": "\"DeepTutor: AI-Powered Personalized Learning Assistant\"",
      "url": "https://github.com/HKUDS/DeepTutor",
      "clone_url": "https://github.com/HKUDS/DeepTutor.git",
      "ssh_url": "git@github.com:HKUDS/DeepTutor.git",
      "homepage": "https://hkuds.github.io/DeepTutor",
      "created_at": "2025-12-28T15:35:54Z",
      "updated_at": "2026-01-25T02:36:29Z",
      "pushed_at": "2026-01-25T02:11:24Z"
    },
    "stats": {
      "stars": 9674,
      "forks": 1274,
      "watchers": 9674,
      "open_issues": 19,
      "size": 68174
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 1837880,
        "TypeScript": 981792,
        "Shell": 14539,
        "JavaScript": 12020,
        "Dockerfile": 11515,
        "CSS": 8674,
        "HTML": 3730
      },
      "license": "GNU Affero General Public License v3.0",
      "topics": [
        "ai-agents",
        "ai-tutor",
        "deepresearch",
        "idea-generation",
        "interactive-learning",
        "knowledge-graph",
        "large-language-models",
        "multi-agent-systems",
        "rag"
      ]
    },
    "content": {
      "readme": "<div align=\"center\">\n\n<img src=\"assets/logo-ver2.png\" alt=\"DeepTutor Logo\" width=\"150\" style=\"border-radius: 15px;\">\n\n# DeepTutor: AI-Powered Personalized Learning Assistant\n\n[![Python](https://img.shields.io/badge/Python-3.10%2B-3776AB?style=flat-square&logo=python&logoColor=white)](https://www.python.org/downloads/)\n[![FastAPI](https://img.shields.io/badge/FastAPI-0.100%2B-009688?style=flat-square&logo=fastapi&logoColor=white)](https://fastapi.tiangolo.com/)\n[![React](https://img.shields.io/badge/React-19-61DAFB?style=flat-square&logo=react&logoColor=black)](https://react.dev/)\n[![Next.js](https://img.shields.io/badge/Next.js-16-000000?style=flat-square&logo=next.js&logoColor=white)](https://nextjs.org/)\n[![TailwindCSS](https://img.shields.io/badge/Tailwind-3.4-06B6D4?style=flat-square&logo=tailwindcss&logoColor=white)](https://tailwindcss.com/)\n[![License](https://img.shields.io/badge/License-AGPL--3.0-blue?style=flat-square)](LICENSE)\n\n<p align=\"center\">\n  <a href=\"https://discord.gg/eRsjPgMU4t\"><img src=\"https://img.shields.io/badge/Discord-Join_Community-5865F2?style=for-the-badge&logo=discord&logoColor=white\" alt=\"Discord\"></a>\n  &nbsp;&nbsp;\n  <a href=\"./Communication.md\"><img src=\"https://img.shields.io/badge/Feishu-Join_Group-00D4AA?style=for-the-badge&logo=feishu&logoColor=white\" alt=\"Feishu\"></a>\n  &nbsp;&nbsp;\n  <a href=\"https://github.com/HKUDS/DeepTutor/issues/78\"><img src=\"https://img.shields.io/badge/WeChat-Join_Group-07C160?style=for-the-badge&logo=wechat&logoColor=white\" alt=\"WeChat\"></a>\n</p>\n\n\n\n[**Quick Start**](#quick-start) Â· [**Core Modules**](#core-modules) Â· [**FAQ**](#faq)\n\n[ğŸ‡¨ğŸ‡³ ä¸­æ–‡](assets/README/README_CN.md) Â· [ğŸ‡¯ğŸ‡µ æ—¥æœ¬èª](assets/README/README_JA.md) Â· [ğŸ‡ªğŸ‡¸ EspaÃ±ol](assets/README/README_ES.md) Â· [ğŸ‡«ğŸ‡· FranÃ§ais](assets/README/README_FR.md) Â· [ğŸ‡¸ğŸ‡¦ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©](assets/README/README_AR.md) Â· [ğŸ‡·ğŸ‡º Ğ ÑƒÑÑĞºĞ¸Ğ¹](assets/README/README_RU.md) Â· [ğŸ‡®ğŸ‡³ à¤¹à¤¿à¤¨à¥à¤¦à¥€](assets/README/README_HI.md) Â· [ğŸ‡µğŸ‡¹ PortuguÃªs](assets/README/README_PT.md)\n\n</div>\n\n<div align=\"center\">\n\nğŸ“š **Massive Document Knowledge Q&A** &nbsp;â€¢&nbsp; ğŸ¨ **Interactive Learning Visualization**<br>\nğŸ¯ **Knowledge Reinforcement** &nbsp;â€¢&nbsp; ğŸ” **Deep Research & Idea Generation**\n\n</div>\n\n---\n### ğŸ“° News\n\n> **[2026.1.1]** Happy New Year! Join our [Discord Community](https://discord.gg/eRsjPgMU4t), [Wechat Community](https://github.com/HKUDS/DeepTutor/issues/78), or [Discussions](https://github.com/HKUDS/DeepTutor/discussions) - shape the future of DeepTutor! ğŸ’¬\n\n> **[2025.12.30]** Visit our [Official Website](https://hkuds.github.io/DeepTutor/) for more details!\n\n> **[2025.12.29]** DeepTutor is now live! âœ¨\n\n### ğŸ“¦ Releases\n\n> **[2026.1.23]** Release [v0.6.0](https://github.com/HKUDS/DeepTutor/releases/tag/v0.6.0) - Frontend session persistence, full Chinese support, Docker deployment updates, and minor bug fixes -- Thanks for all the feedback!\n\n<details>\n<summary>History releases</summary>\n\n> **[2026.1.18]** Release [v0.5.2](https://github.com/HKUDS/DeepTutor/releases/tag/v0.5.1) - Enhance RAG pipeline with Docling support and improve CI/CD workflows with several minor bugs fixed -- Thanks to all the feedbacks!\n\n\n> **[2026.1.15]** Release [v0.5.0](https://github.com/HKUDS/DeepTutor/releases/tag/v0.5.0) - Unified LLM & Embedding services, RAG pipeline selection, and major enhancements to Home, History, QuestionGen & Settings modules -- Thanks to all the contributors!\n\n> **[2026.1.9]** Release [v0.4.1](https://github.com/HKUDS/DeepTutor/releases/tag/v0.4.1) with LLM Provider system overhaul, Question Generation robustness improvements, and codebase cleanup - Thanks to all the contributors!\n\n> **[2026.1.9]** Release [v0.4.0](https://github.com/HKUDS/DeepTutor/releases/tag/v0.4.0) with new code structure, multiple llm & embeddings support - Thanks to all the contributors!\n\n> **[2026.1.5]** [v0.3.0](https://github.com/HKUDS/DeepTutor/releases/tag/v0.3.0) - Unified PromptManager architecture, CI/CD automation & pre-built Docker images on GHCR\n\n> **[2026.1.2]** [v0.2.0](https://github.com/HKUDS/DeepTutor/releases/tag/v0.2.0) - Docker deployment, Next.js 16 & React 19 upgrade, WebSocket security & critical vulnerability fixes\n\n</details>\n\n---\n\n## Key Features of DeepTutor\n\n### ğŸ“š Massive Document Knowledge Q&A\nâ€¢ **Smart Knowledge Base**: Upload textbooks, research papers, technical manuals, and domain-specific documents. Build a comprehensive AI-powered knowledge repository for instant access.<br>\nâ€¢ **Multi-Agent Problem Solving**: Dual-loop reasoning architecture with RAG, web search, and code execution -- delivering step-by-step solutions with precise citations.\n\n### ğŸ¨ Interactive Learning Visualization\nâ€¢ **Knowledge Simplification & Explanations**: Transform complex concepts, knowledge, and algorithms into easy-to-understand visual aids, detailed step-by-step breakdowns, and engaging interactive demonstrations.<br>\nâ€¢ **Personalized Q&A**: Context-aware conversations that adapt to your learning progress, with interactive pages and session-based knowledge tracking",
      "default_branch": "main"
    },
    "fetched_at": "2026-01-25T03:07:32.446854"
  },
  {
    "basic_info": {
      "name": "maptoposter",
      "full_name": "originalankur/maptoposter",
      "owner": "originalankur",
      "description": "Transform your favorite cities into beautiful, minimalist designs. MapToPoster lets you create and export visually striking map posters with code.",
      "url": "https://github.com/originalankur/maptoposter",
      "clone_url": "https://github.com/originalankur/maptoposter.git",
      "ssh_url": "git@github.com:originalankur/maptoposter.git",
      "homepage": "",
      "created_at": "2026-01-08T12:29:35Z",
      "updated_at": "2026-01-25T02:53:51Z",
      "pushed_at": "2026-01-24T12:25:40Z"
    },
    "stats": {
      "stars": 7412,
      "forks": 675,
      "watchers": 7412,
      "open_issues": 30,
      "size": 288532
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 26177
      },
      "license": "MIT License",
      "topics": [
        "design",
        "maps",
        "matplotlib-pyplot",
        "openstreetmap",
        "osmnx",
        "poster",
        "python",
        "visualdesign"
      ]
    },
    "content": {
      "readme": "# City Map Poster Generator\n\nGenerate beautiful, minimalist map posters for any city in the world.\n\n<img src=\"posters/singapore_neon_cyberpunk_20260118_153328.png\" width=\"250\">\n<img src=\"posters/dubai_midnight_blue_20260118_140807.png\" width=\"250\">\n\n## Examples\n\n\n| Country      | City           | Theme           | Poster |\n|:------------:|:--------------:|:---------------:|:------:|\n| USA          | San Francisco  | sunset          | <img src=\"posters/san_francisco_sunset_20260118_144726.png\" width=\"250\"> |\n| Spain        | Barcelona      | warm_beige      | <img src=\"posters/barcelona_warm_beige_20260118_140048.png\" width=\"250\"> |\n| Italy        | Venice         | blueprint       | <img src=\"posters/venice_blueprint_20260118_140505.png\" width=\"250\"> |\n| Japan        | Tokyo          | japanese_ink    | <img src=\"posters/tokyo_japanese_ink_20260118_142446.png\" width=\"250\"> |\n| India        | Mumbai         | contrast_zones  | <img src=\"posters/mumbai_contrast_zones_20260118_145843.png\" width=\"250\"> |\n| Morocco      | Marrakech      | terracotta      | <img src=\"posters/marrakech_terracotta_20260118_143253.png\" width=\"250\"> |\n| Singapore    | Singapore      | neon_cyberpunk  | <img src=\"posters/singapore_neon_cyberpunk_20260118_153328.png\" width=\"250\"> |\n| Australia    | Melbourne      | forest          | <img src=\"posters/melbourne_forest_20260118_153446.png\" width=\"250\"> |\n| UAE          | Dubai          | midnight_blue   | <img src=\"posters/dubai_midnight_blue_20260118_140807.png\" width=\"250\"> |\n| USA          | Seattle        | emerald         | <img src=\"posters/seattle_emerald_20260124_162244.png\" width=\"250\"> |\n\n## Installation\n\n```bash\npip install -r requirements.txt\n```\n\n## Usage\n\n```bash\npython create_map_poster.py --city <city> --country <country> [options]\n```\n\n### Options\n\n| Option | Short | Description | Default |\n|--------|-------|-------------|---------|\n| `--city` | `-c` | City name | required |\n| `--country` | `-C` | Country name | required |\n| **OPTIONAL:** `--name` | | Override display name (city display on poster) | |\n| **OPTIONAL:** `--country-label` | | Override display country (country display on poster) | |\n| **OPTIONAL:** `--theme` | `-t` | Theme name | feature_based |\n| **OPTIONAL:** `--distance` | `-d` | Map radius in meters | 29000 |\n| **OPTIONAL:** `--list-themes` | | List all available themes | |\n| **OPTIONAL:** `--all-themes` | | Generate posters for all available themes | |\n| **OPTIONAL:** `--width` | `-W` | Image width in inches | 12 |\n| **OPTIONAL:** `--height` | `-H` | Image height in inches | 16 |\n\n### Resolution Guide (300 DPI)\n\nUse these values for `-W` and `-H` to target specific resolutions:\n\n| Target | Resolution (px) | Inches (-W / -H) |\n|--------|-----------------|------------------|\n| **Instagram Post** | 1080 x 1080 | 3.6 x 3.6 |\n| **Mobile Wallpaper** | 1080 x 1920 | 3.6 x 6.4 |\n| **HD Wallpaper** | 1920 x 1080 | 6.4 x 3.6 |\n| **4K Wallpaper** | 3840 x 2160 | 12.8 x 7.2 |\n| **A4 Print** | 2480 x 3508 | 8.3 x 11.7 |\n\n### Examples\n\n```bash\n# Iconic grid patterns\npython create_map_poster.py -c \"New York\" -C \"USA\" -t noir -d 12000           # Manhattan grid\npython create_map_poster.py -c \"Barcelona\" -C \"Spain\" -t warm_beige -d 8000   # Eixample district\n\n# Waterfront & canals\npython create_map_poster.py -c \"Venice\" -C \"Italy\" -t blueprint -d 4000       # Canal network\npython create_map_poster.py -c \"Amsterdam\" -C \"Netherlands\" -t ocean -d 6000  # Concentric canals\npython create_map_poster.py -c \"Dubai\" -C \"UAE\" -t midnight_blue -d 15000     # Palm & coastline\n\n# Radial patterns\npython create_map_poster.py -c \"Paris\" -C \"France\" -t pastel_dream -d 10000   # Haussmann boulevards\npython create_map_poster.py -c \"Moscow\" -C \"Russia\" -t noir -d 12000          # Ring roads\n\n# Organic old cities\npython create_map_poster.py -c \"Tokyo\" -C \"Japan\" -t japanese_ink -d 15000    # Dense organic streets\npython create_map_poster.py -c \"Marrakech\" -C \"Morocco\" -t terracotta -d 5000 # Medina maze\npython create_map_poster.py -c \"Rome\" -C \"Italy\" -t warm_beige -d 8000        # Ancient layout\n\n# Coastal cities\npython create_map_poster.py -c \"San Francisco\" -C \"USA\" -t sunset -d 10000    # Peninsula grid\npython create_map_poster.py -c \"Sydney\" -C \"Australia\" -t ocean -d 12000      # Harbor city\npython create_map_poster.py -c \"Mumbai\" -C \"India\" -t contrast_zones -d 18000 # Coastal peninsula\n\n# River cities\npython create_map_poster.py -c \"London\" -C \"UK\" -t noir -d 15000              # Thames curves\npython create_map_poster.py -c \"Budapest\" -C \"Hungary\" -t copper_patina -d 8000  # Danube split\n\n# List available themes\npython create_map_poster.py --list-themes\n\n# Generate posters for every theme\npython create_map_poster.py -c \"Tokyo\" -C \"Japan\" --all-themes\n```\n\n### Distance Guide\n\n| Distance | Best for |\n|----------|----------|\n| 4000-6000m | Small/dense cities (Venice, Amsterdam center) |\n| 8000-12000m | Medium cities, focused downtown (Paris, Barcelona) |\n| 15000-20000m | Large metros, f",
      "default_branch": "main"
    },
    "fetched_at": "2026-01-25T03:07:33.598358"
  },
  {
    "basic_info": {
      "name": "daily_stock_analysis",
      "full_name": "ZhuLinsen/daily_stock_analysis",
      "owner": "ZhuLinsen",
      "description": "LLMé©±åŠ¨çš„ A/H/ç¾è‚¡æ™ºèƒ½åˆ†æå™¨ï¼Œå¤šæ•°æ®æºè¡Œæƒ… + å®æ—¶æ–°é—» + Gemini å†³ç­–ä»ªè¡¨ç›˜ + å¤šæ¸ é“æ¨é€ï¼Œé›¶æˆæœ¬ï¼Œçº¯ç™½å«–ï¼Œå®šæ—¶è¿è¡Œ",
      "url": "https://github.com/ZhuLinsen/daily_stock_analysis",
      "clone_url": "https://github.com/ZhuLinsen/daily_stock_analysis.git",
      "ssh_url": "git@github.com:ZhuLinsen/daily_stock_analysis.git",
      "homepage": "",
      "created_at": "2026-01-10T06:43:20Z",
      "updated_at": "2026-01-25T03:04:49Z",
      "pushed_at": "2026-01-25T02:49:30Z"
    },
    "stats": {
      "stars": 5200,
      "forks": 5439,
      "watchers": 5200,
      "open_issues": 8,
      "size": 40439
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 632310,
        "Shell": 10010,
        "Dockerfile": 1377
      },
      "license": "MIT License",
      "topics": [
        "agent",
        "ai",
        "aigc",
        "gemini",
        "llm",
        "quant",
        "quantitative-trading",
        "rag",
        "stock"
      ]
    },
    "content": {
      "readme": "# ğŸ“ˆ è‚¡ç¥¨æ™ºèƒ½åˆ†æç³»ç»Ÿ\n\n[![GitHub stars](https://img.shields.io/github/stars/ZhuLinsen/daily_stock_analysis?style=social)](https://github.com/ZhuLinsen/daily_stock_analysis/stargazers)\n[![CI](https://github.com/ZhuLinsen/daily_stock_analysis/actions/workflows/ci.yml/badge.svg)](https://github.com/ZhuLinsen/daily_stock_analysis/actions/workflows/ci.yml)\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)\n[![GitHub Actions](https://img.shields.io/badge/GitHub%20Actions-Ready-2088FF?logo=github-actions&logoColor=white)](https://github.com/features/actions)\n\n> ğŸ¤– åŸºäº AI å¤§æ¨¡å‹çš„ Aè‚¡/æ¸¯è‚¡/ç¾è‚¡è‡ªé€‰è‚¡æ™ºèƒ½åˆ†æç³»ç»Ÿï¼Œæ¯æ—¥è‡ªåŠ¨åˆ†æå¹¶æ¨é€ã€Œå†³ç­–ä»ªè¡¨ç›˜ã€åˆ°ä¼ä¸šå¾®ä¿¡/é£ä¹¦/Telegram/é‚®ç®±\n\n[English](./README_EN.md) | ç®€ä½“ä¸­æ–‡\n\n![è¿è¡Œæ•ˆæœæ¼”ç¤º](./sources/all_2026-01-13_221547.gif)\n\n## âœ¨ åŠŸèƒ½ç‰¹æ€§\n\n### ğŸ¯ æ ¸å¿ƒåŠŸèƒ½\n- **AI å†³ç­–ä»ªè¡¨ç›˜** - ä¸€å¥è¯æ ¸å¿ƒç»“è®º + ç²¾ç¡®ä¹°å–ç‚¹ä½ + æ£€æŸ¥æ¸…å•\n- **å¤šç»´åº¦åˆ†æ** - æŠ€æœ¯é¢ + ç­¹ç åˆ†å¸ƒ + èˆ†æƒ…æƒ…æŠ¥ + å®æ—¶è¡Œæƒ…\n- **å¤§ç›˜å¤ç›˜** - æ¯æ—¥å¸‚åœºæ¦‚è§ˆã€æ¿å—æ¶¨è·Œã€åŒ—å‘èµ„é‡‘\n- **å¤šæ¸ é“æ¨é€** - æ”¯æŒä¼ä¸šå¾®ä¿¡ã€é£ä¹¦ã€Telegramã€é‚®ä»¶ï¼ˆè‡ªåŠ¨è¯†åˆ«ï¼‰\n- **é›¶æˆæœ¬éƒ¨ç½²** - GitHub Actions å…è´¹è¿è¡Œï¼Œæ— éœ€æœåŠ¡å™¨\n- **ğŸ’° ç™½å«– Gemini API** - Google AI Studio æä¾›å…è´¹é¢åº¦ï¼Œä¸ªäººä½¿ç”¨å®Œå…¨å¤Ÿç”¨\n- **ğŸ”„ å¤šæ¨¡å‹æ”¯æŒ** - æ”¯æŒ OpenAI å…¼å®¹ APIï¼ˆDeepSeekã€é€šä¹‰åƒé—®ç­‰ï¼‰ä½œä¸ºå¤‡é€‰\n\n### ğŸ“Š æ•°æ®æ¥æº\n- **è¡Œæƒ…æ•°æ®**: AkShareï¼ˆå…è´¹ï¼‰ã€Tushareã€Baostockã€YFinance\n- **æ–°é—»æœç´¢**: Tavilyã€SerpAPIã€Bocha\n- **AI åˆ†æ**: \n  - ä¸»åŠ›ï¼šGoogle Geminiï¼ˆgemini-3-flash-previewï¼‰â€”â€” [å…è´¹è·å–](https://aistudio.google.com/)\n  - å¤‡é€‰ï¼šåº”å¤§å®¶è¦æ±‚ï¼Œä¹Ÿæ”¯æŒäº†OpenAI å…¼å®¹ APIï¼ˆDeepSeekã€é€šä¹‰åƒé—®ã€Moonshot ç­‰ï¼‰\n\n### ğŸ›¡ï¸ äº¤æ˜“ç†å¿µå†…ç½®\n- âŒ **ä¸¥ç¦è¿½é«˜** - ä¹–ç¦»ç‡ > 5% è‡ªåŠ¨æ ‡è®°ã€Œå±é™©ã€\n- âœ… **è¶‹åŠ¿äº¤æ˜“** - MA5 > MA10 > MA20 å¤šå¤´æ’åˆ—\n- ğŸ“ **ç²¾ç¡®ç‚¹ä½** - ä¹°å…¥ä»·ã€æ­¢æŸä»·ã€ç›®æ ‡ä»·\n- ğŸ“‹ **æ£€æŸ¥æ¸…å•** - æ¯é¡¹æ¡ä»¶ç”¨ âœ…âš ï¸âŒ æ ‡è®°\n\n## ğŸš€ å¿«é€Ÿå¼€å§‹\n\n### æ–¹å¼ä¸€ï¼šGitHub Actionsï¼ˆæ¨èï¼Œé›¶æˆæœ¬ï¼‰\n\n**æ— éœ€æœåŠ¡å™¨ï¼Œæ¯å¤©è‡ªåŠ¨è¿è¡Œï¼**\n\n#### 1. Fork æœ¬ä»“åº“(é¡ºä¾¿ç‚¹ä¸‹â­å‘€)\n\nç‚¹å‡»å³ä¸Šè§’ `Fork` æŒ‰é’®\n\n#### 2. é…ç½® Secrets\n\nè¿›å…¥ä½  Fork çš„ä»“åº“ â†’ `Settings` â†’ `Secrets and variables` â†’ `Actions` â†’ `New repository secret`\n\n**AI æ¨¡å‹é…ç½®ï¼ˆäºŒé€‰ä¸€ï¼‰**\n\n| Secret åç§° | è¯´æ˜ | å¿…å¡« |\n|------------|------|:----:|\n| `GEMINI_API_KEY` | [Google AI Studio](https://aistudio.google.com/) è·å–å…è´¹ Key | âœ…* |\n| `OPENAI_API_KEY` | OpenAI å…¼å®¹ API Keyï¼ˆæ”¯æŒ DeepSeekã€é€šä¹‰åƒé—®ç­‰ï¼‰ | å¯é€‰ |\n| `OPENAI_BASE_URL` | OpenAI å…¼å®¹ API åœ°å€ï¼ˆå¦‚ `https://api.deepseek.com/v1`ï¼‰ | å¯é€‰ |\n| `OPENAI_MODEL` | æ¨¡å‹åç§°ï¼ˆå¦‚ `deepseek-chat`ï¼‰ | å¯é€‰ |\n\n> *æ³¨ï¼š`GEMINI_API_KEY` å’Œ `OPENAI_API_KEY` è‡³å°‘é…ç½®ä¸€ä¸ª\n\n**é€šçŸ¥æ¸ é“é…ç½®ï¼ˆå¯åŒæ—¶é…ç½®å¤šä¸ªï¼Œå…¨éƒ¨æ¨é€ï¼‰**\n\n| Secret åç§° | è¯´æ˜ | å¿…å¡« |\n|------------|------|:----:|\n| `WECHAT_WEBHOOK_URL` | ä¼ä¸šå¾®ä¿¡ Webhook URL | å¯é€‰ |\n| `FEISHU_WEBHOOK_URL` | é£ä¹¦ Webhook URL | å¯é€‰ |\n| `TELEGRAM_BOT_TOKEN` | Telegram Bot Tokenï¼ˆ@BotFather è·å–ï¼‰ | å¯é€‰ |\n| `TELEGRAM_CHAT_ID` | Telegram Chat ID | å¯é€‰ |\n| `EMAIL_SENDER` | å‘ä»¶äººé‚®ç®±ï¼ˆå¦‚ `xxx@qq.com`ï¼‰ | å¯é€‰ |\n| `EMAIL_PASSWORD` | é‚®ç®±æˆæƒç ï¼ˆéç™»å½•å¯†ç ï¼‰ | å¯é€‰ |\n| `EMAIL_RECEIVERS` | æ”¶ä»¶äººé‚®ç®±ï¼ˆå¤šä¸ªç”¨é€—å·åˆ†éš”ï¼Œç•™ç©ºåˆ™å‘ç»™è‡ªå·±ï¼‰ | å¯é€‰ |\n| `PUSHPLUS_TOKEN` | PushPlus Tokenï¼ˆ[è·å–åœ°å€](https://www.pushplus.plus)ï¼Œå›½å†…æ¨é€æœåŠ¡ï¼‰ | å¯é€‰ |\n| `CUSTOM_WEBHOOK_URLS` | è‡ªå®šä¹‰ Webhookï¼ˆæ”¯æŒé’‰é’‰ç­‰ï¼Œå¤šä¸ªç”¨é€—å·åˆ†éš”ï¼‰ | å¯é€‰ |\n| `CUSTOM_WEBHOOK_BEARER_TOKEN` | è‡ªå®šä¹‰ Webhook çš„ Bearer Tokenï¼ˆç”¨äºéœ€è¦è®¤è¯çš„ Webhookï¼‰ | å¯é€‰ |\n| `SINGLE_STOCK_NOTIFY` | å•è‚¡æ¨é€æ¨¡å¼ï¼šè®¾ä¸º `true` åˆ™æ¯åˆ†æå®Œä¸€åªè‚¡ç¥¨ç«‹å³æ¨é€ | å¯é€‰ |\n| `REPORT_TYPE` | æŠ¥å‘Šç±»å‹ï¼š`simple`(ç²¾ç®€) æˆ– `full`(å®Œæ•´)ï¼ŒDockerç¯å¢ƒæ¨èè®¾ä¸º `full` | å¯é€‰ |\n| `ANALYSIS_DELAY` | ä¸ªè‚¡åˆ†æå’Œå¤§ç›˜åˆ†æä¹‹é—´çš„å»¶è¿Ÿï¼ˆç§’ï¼‰ï¼Œé¿å…APIé™æµï¼Œå¦‚ `10` | å¯é€‰ |\n\n> *æ³¨ï¼šè‡³å°‘é…ç½®ä¸€ä¸ªæ¸ é“ï¼Œé…ç½®å¤šä¸ªåˆ™åŒæ—¶æ¨é€\n>\n> ğŸ“– æ›´å¤šé…ç½®ï¼ˆPushover æ‰‹æœºæ¨é€ã€é£ä¹¦äº‘æ–‡æ¡£ç­‰ï¼‰è¯·å‚è€ƒ [å®Œæ•´é…ç½®æŒ‡å—](docs/full-guide.md)\n\n**å…¶ä»–é…ç½®**\n\n| Secret åç§° | è¯´æ˜ | å¿…å¡« |\n|------------|------|:----:|\n| `STOCK_LIST` | è‡ªé€‰è‚¡ä»£ç ï¼Œå¦‚ `600519,hk00700,AAPL,TSLA` | âœ… |\n| `TAVILY_API_KEYS` | [Tavily](https://tavily.com/) æœç´¢ APIï¼ˆæ–°é—»æœç´¢ï¼‰ | æ¨è |\n| `BOCHA_API_KEYS` | [åšæŸ¥æœç´¢](https://open.bocha.cn/) Web Search APIï¼ˆä¸­æ–‡æœç´¢ä¼˜åŒ–ï¼Œæ”¯æŒAIæ‘˜è¦ï¼Œå¤šä¸ªkeyç”¨é€—å·åˆ†éš”ï¼‰ | å¯é€‰ |\n| `SERPAPI_API_KEYS` | [SerpAPI](https://serpapi.com/) å¤‡ç”¨æœç´¢ | å¯é€‰ |\n| `TUSHARE_TOKEN` | [Tushare Pro](https://tushare.pro/) Token | å¯é€‰ |\n\n#### 3. å¯ç”¨ Actions\n\nè¿›å…¥ `Actions` æ ‡ç­¾ â†’ ç‚¹å‡» `I understand my workflows, go ahead and enable them`\n\n#### 4. æ‰‹åŠ¨æµ‹è¯•\n\n`Actions` â†’ `æ¯æ—¥è‚¡ç¥¨åˆ†æ` â†’ `Run workflow` â†’ é€‰æ‹©æ¨¡å¼ â†’ `Run workflow`\n\n#### 5. å®Œæˆï¼\n\né»˜è®¤æ¯ä¸ªå·¥ä½œæ—¥ **18:00ï¼ˆåŒ—äº¬æ—¶é—´ï¼‰** è‡ªåŠ¨æ‰§è¡Œ\n\n### æ–¹å¼äºŒï¼šæœ¬åœ°è¿è¡Œ / Docker éƒ¨ç½²\n\n> ğŸ“– æœ¬åœ°è¿è¡Œã€Docker éƒ¨ç½²è¯¦ç»†æ­¥éª¤è¯·å‚è€ƒ [å®Œæ•´é…ç½®æŒ‡å—](docs/full-guide.md)\n\n## ğŸ“± æ¨é€æ•ˆæœ\n\n### å†³ç­–ä»ªè¡¨ç›˜\n```\nğŸ“Š 2026-01-10 å†³ç­–ä»ªè¡¨ç›˜\n3åªè‚¡ç¥¨ | ğŸŸ¢ä¹°å…¥:1 ğŸŸ¡è§‚æœ›:2 ğŸ”´å–å‡º:0\n\nğŸŸ¢ ä¹°å…¥ | è´µå·èŒ…å°(600519)\nğŸ“Œ ç¼©é‡å›è¸©MA5æ”¯æ’‘ï¼Œä¹–ç¦»ç‡1.2%å¤„äºæœ€ä½³ä¹°ç‚¹\nğŸ’° ç‹™å‡»: ä¹°å…¥1800 | æ­¢æŸ1750 | ç›®æ ‡1900\nâœ…å¤šå¤´æ’åˆ— âœ…ä¹–ç¦»å®‰å…¨ âœ…é‡èƒ½é…åˆ\n\nğŸŸ¡ è§‚æœ› | å®å¾·æ—¶ä»£(300750)\nğŸ“Œ ä¹–ç¦»ç‡7.8%è¶…è¿‡5%è­¦æˆ’çº¿ï¼Œä¸¥ç¦è¿½é«˜\nâš ï¸ ç­‰å¾…å›è°ƒè‡³MA5é™„è¿‘å†è€ƒè™‘\n\n---\nç”Ÿæˆæ—¶é—´: 18:00\n```\n\n### å¤§ç›˜å¤ç›˜\n\n![å¤§ç›˜å¤ç›˜æ¨é€æ•ˆæœ](./sources/dapan_2026-01-13_22-14-52.png)\n\n```\nğŸ¯ 2026-01-10 å¤§ç›˜å¤ç›˜\n\nğŸ“Š ä¸»è¦æŒ‡æ•°\n- ä¸Šè¯æŒ‡æ•°: 3250.12 (ğŸŸ¢+0.85%)\n- æ·±è¯æˆæŒ‡: 10521.36 (ğŸŸ¢+1.02%)\n- åˆ›ä¸šæ¿æŒ‡: 2156.78 (ğŸŸ¢+1.35%)\n\nğŸ“ˆ å¸‚åœºæ¦‚å†µ\nä¸Šæ¶¨: 3920 | ä¸‹è·Œ: 1349 | æ¶¨åœ: 155 | è·Œåœ: 3\n\nğŸ”¥ æ¿å—è¡¨ç°\né¢†æ¶¨: äº’è”ç½‘æœåŠ¡ã€æ–‡åŒ–ä¼ åª’ã€å°é‡‘å±\né¢†è·Œ: ä¿é™©ã€èˆªç©ºæœºåœºã€å…‰ä¼è®¾å¤‡\n```\n\n## âš™ï¸ é…ç½®è¯´æ˜\n\n> ğŸ“– å®Œæ•´ç¯å¢ƒå˜é‡ã€å®šæ—¶ä»»åŠ¡é…ç½®è¯·å‚è€ƒ [å®Œæ•´é…ç½®æŒ‡å—](docs/full-guide.md)\n\n## ğŸ–¥ï¸ æœ¬åœ° WebUIï¼ˆå¯é€‰ï¼‰\n\næœ¬åœ°è¿è¡Œæ—¶ï¼Œå¯å¯ç”¨ WebUI æ¥ç®¡ç†é…ç½®å’Œè§¦å‘åˆ†æã€‚\n\n### å¯åŠ¨æ–¹å¼\n\n| å‘½ä»¤ | è¯´æ˜ |\n|------|------|\n| `python main.py --webui` | å¯åŠ¨ WebUI + æ‰§è¡Œä¸€æ¬¡å®Œæ•´åˆ†æ |\n| `python main.py --webui-only` | ä»…å¯åŠ¨ WebUIï¼Œæ‰‹åŠ¨è§¦å‘åˆ†æ |\n\n- è®¿é—®åœ°å€ï¼š`http://127.0.0.1:8000`\n- è¯¦ç»†è¯´æ˜è¯·å‚è€ƒ [é…ç½®æŒ‡å— - WebUI](docs/full-guide.md#æœ¬åœ°-webui-ç®¡ç†ç•Œé¢)\n\n### åŠŸèƒ½ç‰¹æ€§\n\n- ğŸ“ **é…ç½®ç®¡ç†** - æŸ¥çœ‹/ä¿®æ”¹ `.env` é‡Œçš„è‡ªé€‰è‚¡åˆ—è¡¨\n- ğŸš€ **å¿«é€Ÿåˆ†æ** - é¡µé¢è¾“å…¥è‚¡ç¥¨ä»£ç ï¼Œä¸€é”®è§¦å‘åˆ†æ\n- ğŸ“Š **å®æ—¶è¿›åº¦** - åˆ†æä»»åŠ¡çŠ¶æ€å®æ—¶æ›´æ–°ï¼Œæ”¯æŒå¤šä»»åŠ¡å¹¶è¡Œ\n\n### API æ¥å£\n\n| æ¥å£ | æ–¹æ³• | è¯´æ˜ |\n|------|------|------|\n| `/` | GET | é…ç½®ç®¡ç†é¡µé¢ |\n| `/health` | GET | å¥åº·æ£€æŸ¥ |\n| `/analysis?code=xxx` | GET | è§¦å‘å•åªè‚¡ç¥¨å¼‚æ­¥åˆ†æ |\n| `",
      "default_branch": "main"
    },
    "fetched_at": "2026-01-25T03:07:34.766926"
  },
  {
    "basic_info": {
      "name": "Qwen3-TTS",
      "full_name": "QwenLM/Qwen3-TTS",
      "owner": "QwenLM",
      "description": "Qwen3-TTS is an open-source series of TTS models developed by the Qwen team at Alibaba Cloud, supporting stable, expressive, and streaming speech generation, free-form voice design, and vivid voice cloning.",
      "url": "https://github.com/QwenLM/Qwen3-TTS",
      "clone_url": "https://github.com/QwenLM/Qwen3-TTS.git",
      "ssh_url": "git@github.com:QwenLM/Qwen3-TTS.git",
      "homepage": null,
      "created_at": "2026-01-21T06:41:32Z",
      "updated_at": "2026-01-25T03:07:12Z",
      "pushed_at": "2026-01-24T13:44:40Z"
    },
    "stats": {
      "stars": 3586,
      "forks": 319,
      "watchers": 3586,
      "open_issues": 18,
      "size": 6940
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 402665
      },
      "license": "Apache License 2.0",
      "topics": []
    },
    "content": {
      "readme": "# Qwen3-TTS\n\n<br>\n\n<p align=\"center\">\n    <img src=\"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-TTS-Repo/qwen3_tts_logo.png\" width=\"400\"/>\n<p>\n\n<p align=\"center\">\n&nbsp&nbspğŸ¤— <a href=\"https://huggingface.co/collections/Qwen/qwen3-tts\">Hugging Face</a>&nbsp&nbsp | &nbsp&nbspğŸ¤– <a href=\"https://modelscope.cn/collections/Qwen/Qwen3-TTS\">ModelScope</a>&nbsp&nbsp | &nbsp&nbspğŸ“‘ <a href=\"https://qwen.ai/blog?id=qwen3tts-0115\">Blog</a>&nbsp&nbsp | &nbsp&nbspğŸ“‘ <a href=\"https://arxiv.org/abs/2601.15621\">Paper</a>&nbsp&nbsp\n<br>\nğŸ–¥ï¸ <a href=\"https://huggingface.co/spaces/Qwen/Qwen3-TTS\">Hugging Face Demo</a>&nbsp&nbsp | &nbsp&nbsp ğŸ–¥ï¸ <a href=\"https://modelscope.cn/studios/Qwen/Qwen3-TTS\">ModelScope Demo</a>&nbsp&nbsp | &nbsp&nbspğŸ’¬ <a href=\"https://github.com/QwenLM/Qwen/blob/main/assets/wechat.png\">WeChat (å¾®ä¿¡)</a>&nbsp&nbsp | &nbsp&nbspğŸ«¨ <a href=\"https://discord.gg/CV4E9rpNSD\">Discord</a>&nbsp&nbsp | &nbsp&nbspğŸ“‘ <a href=\"https://help.aliyun.com/zh/model-studio/qwen-tts-realtime\">API</a>\n\n</p>\n\nWe release **Qwen3-TTS**, a series of powerful speech generation capabilities developed by Qwen, offering comprehensive support for voice clone, voice design, ultra-high-quality human-like speech generation, and natural language-based voice control. It provides developers and users with the most extensive set of speech generation features available.\n\n\n## News\n* 2026.1.22: ğŸ‰ğŸ‰ğŸ‰ We have released [Qwen3-TTS](https://huggingface.co/collections/Qwen/qwen3-tts) series (0.6B/1.7B) based on Qwen3-TTS-Tokenizer-12Hz. Please check our [blog](https://qwen.ai/blog?id=qwen3tts-0115)!\n\n## Contents <!-- omit in toc -->\n\n- [Overview](#overview)\n  - [Introduction](#introduction)\n  - [Model Architecture](#model-architecture)\n  - [Released Models Description and Download](#released-models-description-and-download)\n- [Quickstart](#quickstart)\n  - [Environment Setup](#environment-setup)\n  - [Python Package Usage](#python-package-usage)\n    - [Custom Voice Generation](#custom-voice-generate)\n    - [Voice Design](#voice-design)\n    - [Voice Clone](#voice-clone)\n    - [Voice Design then Clone](#voice-design-then-clone)\n    - [Tokenizer Encode and Decode](#tokenizer-encode-and-decode)\n  - [Launch Local Web UI Demo](#launch-local-web-ui-demo)\n  - [DashScope API Usage](#dashscope-api-usage)\n- [vLLM Usage](#vllm-usage)\n- [Fine Tuning](#fine-tuning)\n- [Evaluation](#evaluation)\n- [Citation](#citation)\n\n## Overview\n### Introduction\n\n<p align=\"center\">\n    <img src=\"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-TTS-Repo/qwen3_tts_introduction.png\" width=\"90%\"/>\n<p>\n\nQwen3-TTS covers 10 major languages (Chinese, English, Japanese, Korean, German, French, Russian, Portuguese, Spanish, and Italian) as well as multiple dialectal voice profiles to meet global application needs. In addition, the models feature strong contextual understanding, enabling adaptive control of tone, speaking rate, and emotional expression based on instructions and text semantics, and they show markedly improved robustness to noisy input text. Key features:\n\n* **Powerful Speech Representation**: Powered by the self-developed Qwen3-TTS-Tokenizer-12Hz, it achieves efficient acoustic compression and high-dimensional semantic modeling of speech signals. It fully preserves paralinguistic information and acoustic environmental features, enabling high-speed, high-fidelity speech reconstruction through a lightweight non-DiT architecture.\n* **Universal End-to-End Architecture**: Utilizing a discrete multi-codebook LM architecture, it realizes full-information end-to-end speech modeling. This completely bypasses the information bottlenecks and cascading errors inherent in traditional LM+DiT schemes, significantly enhancing the modelâ€™s versatility, generation efficiency, and performance ceiling.\n* **Extreme Low-Latency Streaming Generation**: Based on the innovative Dual-Track hybrid streaming generation architecture, a single model supports both streaming and non-streaming generation. It can output the first audio packet immediately after a single character is input, with end-to-end synthesis latency as low as 97ms, meeting the rigorous demands of real-time interactive scenarios.\n* **Intelligent Text Understanding and Voice Control**: Supports speech generation driven by natural language instructions, allowing for flexible control over multi-dimensional acoustic attributes such as timbre, emotion, and prosody. By deeply integrating text semantic understanding, the model adaptively adjusts tone, rhythm, and emotional expression, achieving lifelike â€œwhat you imagine is what you hearâ€ output.\n\n\n### Model Architecture\n\n<p align=\"center\">\n    <img src=\"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-TTS-Repo/overview.png\" width=\"80%\"/>\n<p>\n\n### Released Models Description and Download\n\nBelow is an introduction and download information for the Qwen3-TTS models that have already been released. Other models mentioned in the technical report will be released in the near future. Please se",
      "default_branch": "main"
    },
    "fetched_at": "2026-01-25T03:07:35.931615"
  },
  {
    "basic_info": {
      "name": "Engram",
      "full_name": "deepseek-ai/Engram",
      "owner": "deepseek-ai",
      "description": "Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models",
      "url": "https://github.com/deepseek-ai/Engram",
      "clone_url": "https://github.com/deepseek-ai/Engram.git",
      "ssh_url": "git@github.com:deepseek-ai/Engram.git",
      "homepage": "",
      "created_at": "2026-01-12T05:26:50Z",
      "updated_at": "2026-01-25T01:50:11Z",
      "pushed_at": "2026-01-14T01:13:02Z"
    },
    "stats": {
      "stars": 3286,
      "forks": 222,
      "watchers": 3286,
      "open_issues": 12,
      "size": 2296
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 15017
      },
      "license": "Apache License 2.0",
      "topics": []
    },
    "content": {
      "readme": "<!-- markdownlint-disable first-line-h1 -->\n<!-- markdownlint-disable html -->\n<!-- markdownlint-disable no-duplicate-header -->\n\n<div align=\"center\">\n  <img src=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/logo.svg?raw=true\" width=\"60%\" alt=\"DeepSeek-V3\" />\n</div>\n<hr>\n<div align=\"center\" style=\"line-height: 1;\">\n  <a href=\"https://www.deepseek.com/\"><img alt=\"Homepage\"\n    src=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/badge.svg?raw=true\"/></a>\n  <a href=\"https://chat.deepseek.com/\"><img alt=\"Chat\"\n    src=\"https://img.shields.io/badge/ğŸ¤–%20Chat-DeepSeek%20V3-536af5?color=536af5&logoColor=white\"/></a>\n  <a href=\"https://huggingface.co/deepseek-ai\"><img alt=\"Hugging Face\"\n    src=\"https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-DeepSeek%20AI-ffc107?color=ffc107&logoColor=white\"/></a>\n  <br>\n  <a href=\"https://discord.gg/Tc7c45Zzu5\"><img alt=\"Discord\"\n    src=\"https://img.shields.io/badge/Discord-DeepSeek%20AI-7289da?logo=discord&logoColor=white&color=7289da\"/></a>\n  <a href=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/qr.jpeg?raw=true\"><img alt=\"Wechat\"\n    src=\"https://img.shields.io/badge/WeChat-DeepSeek%20AI-brightgreen?logo=wechat&logoColor=white\"/></a>\n  <a href=\"https://twitter.com/deepseek_ai\"><img alt=\"Twitter Follow\"\n    src=\"https://img.shields.io/badge/Twitter-deepseek_ai-white?logo=x&logoColor=white\"/></a>\n  <br>\n  <a href=\"LICENSE\" style=\"margin: 2px;\">\n    <img alt=\"License\" src=\"https://img.shields.io/badge/License-Apache 2.0-f5de53?&color=f5de53\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <br>\n</div>\n\n## 1. Introduction\n\nThis repository contains the official implementation for the paper: **[Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models](Engram_paper.pdf)**.\n\n> **Abstract:** While Mixture-of-Experts (MoE) scales capacity via conditional computation, Transformers lack a native primitive for knowledge lookup. To address this, we explore **conditional memory** as a complementary sparsity axis, instantiated via **Engram**, a module that modernizes classic $N$-gram embeddings for $\\mathcal{O}(1)$ lookup.\n\n**Key Contributions:**\n- **Sparsity Allocation:** We formulate the trade-off between neural computation (MoE) and static memory (Engram), identifying a U-shaped scaling law that guides optimal capacity allocation.\n- **Empirical Verification:** Under strict iso-parameter and iso-FLOPs constraints, the Engram-27B model demonstrates consistent improvements over MoE baselines across knowledge, reasoning, code and math domains.\n- **Mechanistic Analysis:** Our analysis suggests that Engram relieves early layers from static pattern reconstruction, potentially preserving effective depth for complex reasoning.\n- **System Efficiency:** The module employs deterministic addressing, enabling the offloading of massive embedding tables to host memory with minimal inference overhead.\n\n\n## 2. Architecture\n\nThe Engram module augments the backbone by retrieving static $N$-gram memory and fusing it with dynamic hidden states. The architecture is shown below ([drawio provided](drawio/Engram.drawio)):\n\n<p align=\"center\">\n  <img width=\"75%\" src=\"figures/arch.png\" alt=\"Engram Architecture\">\n</p>\n\n## 3. Evaluation\n\n### Scaling Law\n<p align=\"center\">\n  <img width=\"90%\" src=\"figures/scaling_law.png\" alt=\"Scaling Law\">\n</p>\n\n---\n\n### Large Scale Pre-training\n<p align=\"center\">\n  <img width=\"80%\" src=\"figures/27b_exp_results.png\" alt=\"Pre-training Results\">\n</p>\n\n---\n\n### Long-context Training\n<p align=\"center\">\n  <img width=\"80%\" src=\"figures/long_context_results.png\" alt=\"Long Context Results\">\n</p>\n\n\n## 4. Case Study of Engram\n<p align=\"center\">\n  <img width=\"80%\" src=\"figures/case.png\" alt=\"Long Context Results\">\n</p>\n\n## 5. Quick Start\n\nWe recommend using Python 3.8+ and PyTorch.\n```bash\npip install torch numpy transformers sympy\n```\nWe provide a standalone implementation to demonstrate the core logic of the Engram module:\n```bash\npython engram_demo_v1.py\n```\n\n> âš ï¸ **Note:** The provided code is a demonstration version intended to illustrate the data flow. It mocks standard components (like Attention/MoE/mHC) to focus on the Engram module. \n\n\n## 6. License\nThe use of Engram models is subject to [the Model License](LICENSE).\n\n## 7. Contact\n\nIf you have any questions, please raise an issue or contact us at [service@deepseek.com](mailto:service@deepseek.com).",
      "default_branch": "main"
    },
    "fetched_at": "2026-01-25T03:07:37.107414"
  },
  {
    "basic_info": {
      "name": "LTX-2",
      "full_name": "Lightricks/LTX-2",
      "owner": "Lightricks",
      "description": "Official Python inference and LoRA trainer package for the LTX-2 audioâ€“video generative model.",
      "url": "https://github.com/Lightricks/LTX-2",
      "clone_url": "https://github.com/Lightricks/LTX-2.git",
      "ssh_url": "git@github.com:Lightricks/LTX-2.git",
      "homepage": "https://ltx.io/model/ltx-2",
      "created_at": "2026-01-03T13:16:29Z",
      "updated_at": "2026-01-25T02:41:48Z",
      "pushed_at": "2026-01-15T19:40:05Z"
    },
    "stats": {
      "stars": 3078,
      "forks": 387,
      "watchers": 3078,
      "open_issues": 57,
      "size": 230
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 809295
      },
      "license": "Other",
      "topics": [
        "generative-ai",
        "ltx",
        "ltx-2"
      ]
    },
    "content": {
      "readme": "# LTX-2\n\n[![Website](https://img.shields.io/badge/Website-LTX-181717?logo=google-chrome)](https://ltx.io)\n[![Model](https://img.shields.io/badge/HuggingFace-Model-orange?logo=huggingface)](https://huggingface.co/Lightricks/LTX-2)\n[![Demo](https://img.shields.io/badge/Demo-Try%20Now-brightgreen?logo=vercel)](https://app.ltx.studio/ltx-2-playground/i2v)\n[![Paper](https://img.shields.io/badge/Paper-PDF-EC1C24?logo=adobeacrobatreader&logoColor=white)](https://arxiv.org/abs/2601.03233)\n[![Discord](https://img.shields.io/badge/Join-Discord-5865F2?logo=discord)](https://discord.gg/ltxplatform)\n\n**LTX-2** is the first DiT-based audio-video foundation model that contains all core capabilities of modern video generation in one model: synchronized audio and video, high fidelity, multiple performance modes, production-ready outputs, API access, and open access.\n\n<div align=\"center\">\n  <video src=\"https://github.com/user-attachments/assets/4414adc0-086c-43de-b367-9362eeb20228\" width=\"70%\" poster=\"\"> </video>\n</div>\n\n## ğŸš€ Quick Start\n\n```bash\n# Clone the repository\ngit clone https://github.com/Lightricks/LTX-2.git\ncd LTX-2\n\n# Set up the environment\nuv sync --frozen\nsource .venv/bin/activate\n```\n\n### Required Models\n\nDownload the following models from the [LTX-2 HuggingFace repository](https://huggingface.co/Lightricks/LTX-2):\n\n**LTX-2 Model Checkpoint** (choose and download one of the following)\n  * [`ltx-2-19b-dev-fp8.safetensors`](https://huggingface.co/Lightricks/LTX-2/blob/main/ltx-2-19b-dev-fp8.safetensors) - [Download](https://huggingface.co/Lightricks/LTX-2/resolve/main/ltx-2-19b-dev-fp8.safetensors)\n\n  * [`ltx-2-19b-dev.safetensors`](https://huggingface.co/Lightricks/LTX-2/blob/main/ltx-2-19b-dev.safetensors) - [Download](https://huggingface.co/Lightricks/LTX-2/resolve/main/ltx-2-19b-dev.safetensors)\n  * [`ltx-2-19b-distilled.safetensors`](https://huggingface.co/Lightricks/LTX-2/blob/main/ltx-2-19b-distilled.safetensors) - [Download](https://huggingface.co/Lightricks/LTX-2/resolve/main/ltx-2-19b-distilled.safetensors)\n  * [`ltx-2-19b-distilled-fp8.safetensors`](https://huggingface.co/Lightricks/LTX-2/blob/main/ltx-2-19b-distilled-fp8.safetensors) - [Download](https://huggingface.co/Lightricks/LTX-2/resolve/main/ltx-2-19b-distilled-fp8.safetensors)\n\n**Spatial Upscaler** - Required for current two-stage pipeline implementations in this repository\n  * [`ltx-2-spatial-upscaler-x2-1.0.safetensors`](https://huggingface.co/Lightricks/LTX-2/blob/main/ltx-2-spatial-upscaler-x2-1.0.safetensors) - [Download](https://huggingface.co/Lightricks/LTX-2/resolve/main/ltx-2-spatial-upscaler-x2-1.0.safetensors)\n\n**Temporal Upscaler** - Supported by the model and will be required for future pipeline implementations\n  * [`ltx-2-temporal-upscaler-x2-1.0.safetensors`](https://huggingface.co/Lightricks/LTX-2/blob/main/ltx-2-temporal-upscaler-x2-1.0.safetensors) - [Download](https://huggingface.co/Lightricks/LTX-2/resolve/main/ltx-2-temporal-upscaler-x2-1.0.safetensors)\n\n**Distilled LoRA** - Required for current two-stage pipeline implementations in this repository (except DistilledPipeline and ICLoraPipeline)\n  * [`ltx-2-19b-distilled-lora-384.safetensors`](https://huggingface.co/Lightricks/LTX-2/blob/main/ltx-2-19b-distilled-lora-384.safetensors) - [Download](https://huggingface.co/Lightricks/LTX-2/resolve/main/ltx-2-19b-distilled-lora-384.safetensors)\n\n**Gemma Text Encoder** (download all assets from the repository)\n  * [`Gemma 3`](https://huggingface.co/google/gemma-3-12b-it-qat-q4_0-unquantized/tree/main)\n\n**LoRAs**\n  * [`LTX-2-19b-IC-LoRA-Canny-Control`](https://huggingface.co/Lightricks/LTX-2-19b-IC-LoRA-Canny-Control) - [Download](https://huggingface.co/Lightricks/LTX-2-19b-IC-LoRA-Canny-Control/resolve/main/ltx-2-19b-ic-lora-canny-control.safetensors)\n  * [`LTX-2-19b-IC-LoRA-Depth-Control`](https://huggingface.co/Lightricks/LTX-2-19b-IC-LoRA-Depth-Control) - [Download](https://huggingface.co/Lightricks/LTX-2-19b-IC-LoRA-Depth-Control/resolve/main/ltx-2-19b-ic-lora-depth-control.safetensors)\n  * [`LTX-2-19b-IC-LoRA-Detailer`](https://huggingface.co/Lightricks/LTX-2-19b-IC-LoRA-Detailer) - [Download](https://huggingface.co/Lightricks/LTX-2-19b-IC-LoRA-Detailer/resolve/main/ltx-2-19b-ic-lora-detailer.safetensors)\n  * [`LTX-2-19b-IC-LoRA-Pose-Control`](https://huggingface.co/Lightricks/LTX-2-19b-IC-LoRA-Pose-Control) - [Download](https://huggingface.co/Lightricks/LTX-2-19b-IC-LoRA-Pose-Control/resolve/main/ltx-2-19b-ic-lora-pose-control.safetensors)\n  * [`LTX-2-19b-LoRA-Camera-Control-Dolly-In`](https://huggingface.co/Lightricks/LTX-2-19b-LoRA-Camera-Control-Dolly-In) - [Download](https://huggingface.co/Lightricks/LTX-2-19b-LoRA-Camera-Control-Dolly-In/resolve/main/ltx-2-19b-lora-camera-control-dolly-in.safetensors)\n  * [`LTX-2-19b-LoRA-Camera-Control-Dolly-Left`](https://huggingface.co/Lightricks/LTX-2-19b-LoRA-Camera-Control-Dolly-Left) - [Download](https://huggingface.co/Lightricks/LTX-2-19b-LoRA-Camera-Control-Dolly-Left/resolve/",
      "default_branch": "main"
    },
    "fetched_at": "2026-01-25T03:07:38.262781"
  },
  {
    "basic_info": {
      "name": "original_performance_takehome",
      "full_name": "anthropics/original_performance_takehome",
      "owner": "anthropics",
      "description": "Anthropic's original performance take-home, now open for you to try!",
      "url": "https://github.com/anthropics/original_performance_takehome",
      "clone_url": "https://github.com/anthropics/original_performance_takehome.git",
      "ssh_url": "git@github.com:anthropics/original_performance_takehome.git",
      "homepage": null,
      "created_at": "2026-01-19T19:16:04Z",
      "updated_at": "2026-01-25T03:06:13Z",
      "pushed_at": "2026-01-22T01:11:08Z"
    },
    "stats": {
      "stars": 2758,
      "forks": 551,
      "watchers": 2758,
      "open_issues": 10,
      "size": 8
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 57361,
        "HTML": 4758
      },
      "license": null,
      "topics": []
    },
    "content": {
      "readme": "# Anthropic's Original Performance Take-Home\n\nThis repo contains a version of Anthropic's original performance take-home, before Claude Opus 4.5 started doing better than humans given only 2 hours.\n\nThe original take-home was a 4-hour one that starts close to the contents of this repo, after Claude Opus 4 beat most humans at that, it was updated to a 2-hour one which started with code which achieved 18532 cycles (7.97x faster than this repo starts you). This repo is based on the newer take-home which has a few more instructions and comes with better debugging tools, but has the starter code reverted to the slowest baseline. After Claude Opus 4.5 we started using a different base for our time-limited take-homes.\n\nNow you can try to beat Claude Opus 4.5 given unlimited time!\n\n## Performance benchmarks \n\nMeasured in clock cycles from the simulated machine. All of these numbers are for models doing the 2 hour version which started at 18532 cycles:\n\n- **2164 cycles**: Claude Opus 4 after many hours in the test-time compute harness\n- **1790 cycles**: Claude Opus 4.5 in a casual Claude Code session, approximately matching the best human performance in 2 hours\n- **1579 cycles**: Claude Opus 4.5 after 2 hours in our test-time compute harness\n- **1548 cycles**: Claude Sonnet 4.5 after many more than 2 hours of test-time compute\n- **1487 cycles**: Claude Opus 4.5 after 11.5 hours in the harness\n- **1363 cycles**: Claude Opus 4.5 in an improved test time compute harness\n- **??? cycles**: Best human performance ever is substantially better than the above, but we won't say how much.\n\nWhile it's no longer a good time-limited test, you can still use this test to get us excited about hiring you! If you optimize below 1487 cycles, beating Claude Opus 4.5's best performance at launch, email us at performance-recruiting@anthropic.com with your code (and ideally a resume) so we can be appropriately impressed, especially if you get near the best solution we've seen. New model releases may change what threshold impresses us though, and no guarantees that we keep this readme updated with the latest on that.\n\nRun `python tests/submission_tests.py` to see which thresholds you pass.\n\n## Warning: LLMs can cheat\n\nNone of the solutions we received on the first day post-release below 1300 cycles were valid solutions. In each case, a language model modified the tests to make the problem easier.\n\nIf you use an AI agent, we recommend instructing it not to change the `tests/` folder and to use `tests/submission_tests.py` for verification.\n\nPlease run the following commands to validate your submission, and mention that you did so when submitting:\n```\n# This should be empty, the tests folder must be unchanged\ngit diff origin/main tests/\n# You should pass some of these tests and use the cycle count this prints\npython tests/submission_tests.py\n```\n\nAn example of this kind of hack is a model noticing that `problem.py` has multicore support, implementing multicore as an optimization, noticing there's no speedup and \"debugging\" that `N_CORES = 1` and \"fixing\" the core count so they get a speedup. Multicore is disabled intentionally in this version.\n",
      "default_branch": "main"
    },
    "fetched_at": "2026-01-25T03:07:39.409486"
  },
  {
    "basic_info": {
      "name": "personaplex",
      "full_name": "NVIDIA/personaplex",
      "owner": "NVIDIA",
      "description": "PersonaPlex code.",
      "url": "https://github.com/NVIDIA/personaplex",
      "clone_url": "https://github.com/NVIDIA/personaplex.git",
      "ssh_url": "git@github.com:NVIDIA/personaplex.git",
      "homepage": null,
      "created_at": "2026-01-05T19:10:35Z",
      "updated_at": "2026-01-25T03:05:42Z",
      "pushed_at": "2026-01-24T22:46:37Z"
    },
    "stats": {
      "stars": 2700,
      "forks": 327,
      "watchers": 2700,
      "open_issues": 15,
      "size": 1467
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 282630,
        "TypeScript": 91359,
        "CSS": 1981,
        "Dockerfile": 965,
        "HTML": 542,
        "JavaScript": 343
      },
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "# PersonaPlex: Voice and Role Control for Full Duplex Conversational Speech Models\n\n[![Weights](https://img.shields.io/badge/ğŸ¤—-Weights-yellow)](https://huggingface.co/nvidia/personaplex-7b-v1)\n[![Paper](https://img.shields.io/badge/ğŸ“„-Paper-blue)](https://research.nvidia.com/labs/adlr/files/personaplex/personaplex_preprint.pdf)\n[![Demo](https://img.shields.io/badge/ğŸ®-Demo-green)](https://research.nvidia.com/labs/adlr/personaplex/)\n[![Discord](https://img.shields.io/badge/Discord-Join-purple?logo=discord)](https://discord.gg/5jAXrrbwRb)\n\nPersonaPlex is a real-time, full-duplex speech-to-speech conversational model that enables persona control through text-based role prompts and audio-based voice conditioning. Trained on a combination of synthetic and real conversations, it produces natural, low-latency spoken interactions with a consistent persona. PersonaPlex is based on the [Moshi](https://arxiv.org/abs/2410.00037) architecture and weights.\n\n<p align=\"center\">\n  <img src=\"assets/architecture_diagram.png\" alt=\"PersonaPlex Model Architecture\">\n  <br>\n  <em>PersonaPlex Architecture</em>\n</p>\n\n## Usage\n\n### Prerequisites\n\nInstall the [Opus audio codec](https://github.com/xiph/opus) development library:\n```bash\n# Ubuntu/Debian\nsudo apt install libopus-dev\n\n# Fedora/RHEL\nsudo dnf install opus-devel\n\n# macOS\nbrew install opus\n```\n\n### Installation\n\nDownload this repository and install with:\n```bash\npip install moshi/.\n```\n\nExtra step for Blackwell based GPUs as suggested in (See https://github.com/NVIDIA/personaplex/issues/2):\n```bash\npip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu130\n```\n\n\n### Accept Model License\nLog in to your Huggingface account and accept the PersonaPlex model license [here](https://huggingface.co/nvidia/personaplex-7b-v1). <br>\nThen set up your Huggingface authentication:\n```bash\nexport HF_TOKEN=<YOUR_HUGGINGFACE_TOKEN>\n```\n\n### Launch Server\n\nLaunch server for live interaction (temporary SSL certs for https):\n```bash\nSSL_DIR=$(mktemp -d); python -m moshi.server --ssl \"$SSL_DIR\"\n```\n\n**CPU Offload:** If your GPU has insufficient memory, use the `--cpu-offload` flag to offload model layers to CPU. This requires the `accelerate` package (`pip install accelerate`):\n```bash\nSSL_DIR=$(mktemp -d); python -m moshi.server --ssl \"$SSL_DIR\" --cpu-offload\n```\n\nAccess the Web UI from a browser at `localhost:8998` if running locally, otherwise look for the access link printed by the script:\n```\nAccess the Web UI directly at https://11.54.401.33:8998\n```\n\n### Offline Evaluation\n\nFor offline evaluation use the offline script that streams in an input wav file and produces an output wav file from the captured output stream. The output file will be the same duration as the input file.\n\nAdd `--cpu-offload` to any command below if your GPU has insufficient memory (requires `accelerate` package). Or install cpu-only PyTorch for offline evaluation on pure CPU.\n\n**Assistant example:**\n```bash\nHF_TOKEN=<TOKEN> \\\npython -m moshi.offline \\\n  --voice-prompt \"NATF2.pt\" \\\n  --input-wav \"assets/test/input_assistant.wav\" \\\n  --seed 42424242 \\\n  --output-wav \"output.wav\" \\\n  --output-text \"output.json\"\n```\n\n**Service example:**\n```bash\nHF_TOKEN=<TOKEN> \\\npython -m moshi.offline \\\n  --voice-prompt \"NATM1.pt\" \\\n  --text-prompt \"$(cat assets/test/prompt_service.txt)\" \\\n  --input-wav \"assets/test/input_service.wav\" \\\n  --seed 42424242 \\\n  --output-wav \"output.wav\" \\\n  --output-text \"output.json\"\n```\n\n## Voices\n\nPersonaPlex supports a wide range of voices; we pre-package embeddings for voices that sound more natural and conversational (NAT) and others that are more varied (VAR). The fixed set of voices are labeled:\n```\nNatural(female): NATF0, NATF1, NATF2, NATF3\nNatural(male):   NATM0, NATM1, NATM2, NATM3\nVariety(female): VARF0, VARF1, VARF2, VARF3, VARF4\nVariety(male):   VARM0, VARM1, VARM2, VARM3, VARM4\n```\n\n## Prompting Guide\n\nThe model is trained on synthetic conversations for a fixed assistant role and varying customer service roles.\n\n### Assistant Role\n\nThe assistant role has the prompt:\n```\nYou are a wise and friendly teacher. Answer questions or provide advice in a clear and engaging way.\n```\n\nUse this prompt for the QA assistant focused \"User Interruption\" evaluation category in [FullDuplexBench](https://arxiv.org/abs/2503.04721).\n\n### Customer Service Roles\n\nThe customer service roles support a variety of prompts. Here are some examples for prompting style reference:\n```\nYou work for CitySan Services which is a waste management and your name is Ayelen Lucero. Information: Verify customer name Omar Torres. Current schedule: every other week. Upcoming pickup: April 12th. Compost bin service available for $8/month add-on.\n```\n```\nYou work for Jerusalem Shakshuka which is a restaurant and your name is Owen Foster. Information: There are two shakshuka options: Classic (poached eggs, $9.50) and Spicy (scrambled eggs with jalapenos, $10.25). Sides include warm pita ($2.50) and Israeli",
      "default_branch": "main"
    },
    "fetched_at": "2026-01-25T03:07:40.546518"
  },
  {
    "basic_info": {
      "name": "pocket-tts",
      "full_name": "kyutai-labs/pocket-tts",
      "owner": "kyutai-labs",
      "description": "A TTS that fits in your CPU (and pocket)",
      "url": "https://github.com/kyutai-labs/pocket-tts",
      "clone_url": "https://github.com/kyutai-labs/pocket-tts.git",
      "ssh_url": "git@github.com:kyutai-labs/pocket-tts.git",
      "homepage": null,
      "created_at": "2026-01-07T17:33:32Z",
      "updated_at": "2026-01-25T02:55:32Z",
      "pushed_at": "2026-01-22T11:04:32Z"
    },
    "stats": {
      "stars": 2441,
      "forks": 268,
      "watchers": 2441,
      "open_issues": 32,
      "size": 427
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 124141,
        "HTML": 14556,
        "Dockerfile": 312
      },
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "# Pocket TTS\n\n<img width=\"1446\" height=\"622\" alt=\"pocket-tts-logo-v2-transparent\" src=\"https://github.com/user-attachments/assets/637b5ed6-831f-4023-9b4c-741be21ab238\" />\n\nA lightweight text-to-speech (TTS) application designed to run efficiently on CPUs.\nForget about the hassle of using GPUs and web APIs serving TTS models. With Kyutai's Pocket TTS, generating audio is just a pip install and a function call away.\n\nSupports Python 3.10, 3.11, 3.12, 3.13 and 3.14. Requires PyTorch 2.5+. Does not require the gpu version of PyTorch.\n\n[ğŸ”Š Demo](https://kyutai.org/pocket-tts) | \n[ğŸ±â€ğŸ’»GitHub Repository](https://github.com/kyutai-labs/pocket-tts) | \n[ğŸ¤— Hugging Face Model Card](https://huggingface.co/kyutai/pocket-tts) | \n[âš™ï¸ Tech report](https://kyutai.org/blog/2026-01-13-pocket-tts) |\n[ğŸ“„ Paper](https://arxiv.org/abs/2509.06926) | \n[ğŸ“š Documentation](https://github.com/kyutai-labs/pocket-tts/tree/main/docs)\n\n\n## Main takeaways\n* Runs on CPU\n* Small model size, 100M parameters\n* Audio streaming\n* Low latency, ~200ms to get the first audio chunk\n* Faster than real-time, ~6x real-time on a CPU of MacBook Air M4\n* Uses only 2 CPU cores\n* Python API and CLI\n* Voice cloning\n* English only at the moment\n* Can handle infinitely long text inputs\n* [Can run on client-side in the browser](#in-browser-implementations)\n\n## Trying it from the website, without installing anything\n\nNavigate to the [Kyutai website](https://kyutai.org/pocket-tts) to try it out directly in your browser. You can input text, select different voices, and generate speech without any installation.\n\n## Trying it with the CLI\n\n### The `generate` command\nYou can use pocket-tts directly from the command line. We recommend using\n`uv` as it installs any dependencies on the fly in an isolated environment (uv installation instructions [here](https://docs.astral.sh/uv/getting-started/installation/#standalone-installer)).\nYou can also use `pip install pocket-tts` to install it manually.\n\nThis will generate a wav file `./tts_output.wav` saying the default text with the default voice, and display some speed statistics.\n```bash\nuvx pocket-tts generate\n# or if you installed it manually with pip:\npocket-tts generate\n```\nModify the voice with `--voice` and the text with `--text`. We provide a small catalog of voices.\n\nYou can take a look at [this page](https://huggingface.co/kyutai/tts-voices) which details the licenses\nfor each voice.\n\n* [alba](https://huggingface.co/kyutai/tts-voices/blob/main/alba-mackenna/casual.wav)\n* [marius](https://huggingface.co/kyutai/tts-voices/blob/main/voice-donations/Selfie.wav)\n* [javert](https://huggingface.co/kyutai/tts-voices/blob/main/voice-donations/Butter.wav)\n* [jean](https://huggingface.co/kyutai/tts-voices/blob/main/ears/p010/freeform_speech_01.wav)\n* [fantine](https://huggingface.co/kyutai/tts-voices/blob/main/vctk/p244_023.wav)\n* [cosette](https://huggingface.co/kyutai/tts-voices/blob/main/expresso/ex04-ex02_confused_001_channel1_499s.wav)\n* [eponine](https://huggingface.co/kyutai/tts-voices/blob/main/vctk/p262_023.wav)\n* [azelma](https://huggingface.co/kyutai/tts-voices/blob/main/vctk/p303_023.wav)\n\nThe `--voice` argument can also take a plain wav file as input for voice cloning.\nYou can use your own or check out our [voice repository](https://huggingface.co/kyutai/tts-voices).\n\nFeel free to check out the [generate documentation](https://github.com/kyutai-labs/pocket-tts/tree/main/docs/generate.md) for more details and examples.\nFor trying multiple voices and prompts quickly, prefer using the `serve` command.\n\n### The `serve` command\n\nYou can also run a local server to generate audio via HTTP requests.\n```bash\nuvx pocket-tts serve\n# or if you installed it manually with pip:\npocket-tts serve\n```\nNavigate to `http://localhost:8000` to try the web interface, it's faster than the command line as the model is kept in memory between requests.\n\nYou can check out the [serve documentation](https://github.com/kyutai-labs/pocket-tts/tree/main/docs/serve.md) for more details and examples.\n\n## Using it as a Python library\n\nYou can try out the Python library on Colab [here](https://colab.research.google.com/github/kyutai-labs/pocket-tts/blob/main/docs/pocket-tts-example.ipynb).\n\nInstall the package with\n```bash\npip install pocket-tts\n# or\nuv add pocket-tts\n```\n\nYou can use this package as a simple Python library to generate audio from text.\n```python\nfrom pocket_tts import TTSModel\nimport scipy.io.wavfile\n\ntts_model = TTSModel.load_model()\nvoice_state = tts_model.get_state_for_audio_prompt(\n    \"alba\"  # One of the pre-made voices, see above\n    # You can also use any voice file you have locally or from Hugging Face:\n    # \"./some_audio.wav\"\n    # or \"hf://kyutai/tts-voices/expresso/ex01-ex02_default_001_channel2_198s.wav\"\n)\naudio = tts_model.generate_audio(voice_state, \"Hello world, this is a test.\")\n# Audio is a 1D torch tensor containing PCM data.\nscipy.io.wavfile.write(\"output.wav\", tts_model.sample_rate, audio.numpy())\n```\n\nYou can have m",
      "default_branch": "main"
    },
    "fetched_at": "2026-01-25T03:07:41.689491"
  },
  {
    "basic_info": {
      "name": "antigravity-awesome-skills",
      "full_name": "sickn33/antigravity-awesome-skills",
      "owner": "sickn33",
      "description": "The Ultimate Collection of 200+ Agentic Skills for Claude Code/Antigravity/Cursor. Battle-tested, high-performance skills for AI agents including official skills from Anthropic and Vercel.",
      "url": "https://github.com/sickn33/antigravity-awesome-skills",
      "clone_url": "https://github.com/sickn33/antigravity-awesome-skills.git",
      "ssh_url": "git@github.com:sickn33/antigravity-awesome-skills.git",
      "homepage": "https://github.com/sickn33/antigravity-awesome-skills",
      "created_at": "2026-01-14T17:48:09Z",
      "updated_at": "2026-01-25T02:58:36Z",
      "pushed_at": "2026-01-24T20:08:01Z"
    },
    "stats": {
      "stars": 2390,
      "forks": 512,
      "watchers": 2390,
      "open_issues": 2,
      "size": 7784
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 1144997,
        "Shell": 262567,
        "JavaScript": 79990,
        "TypeScript": 38918,
        "HTML": 36813,
        "CSS": 6844
      },
      "license": "MIT License",
      "topics": [
        "agentic-skills",
        "ai-agents",
        "antigravity",
        "autonomous-coding",
        "claude-code",
        "mcp",
        "react-patterns",
        "security-auditing"
      ]
    },
    "content": {
      "readme": "# ğŸŒŒ Antigravity Awesome Skills: 244+ Agentic Skills for Claude Code, Gemini CLI, Cursor, Copilot & More\n\n> **The Ultimate Collection of 244+ Universal Agentic Skills for AI Coding Assistants â€” Claude Code, Gemini CLI, Codex CLI, Antigravity IDE, GitHub Copilot, Cursor, OpenCode**\n\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n[![Claude Code](https://img.shields.io/badge/Claude%20Code-Anthropic-purple)](https://claude.ai)\n[![Gemini CLI](https://img.shields.io/badge/Gemini%20CLI-Google-blue)](https://github.com/google-gemini/gemini-cli)\n[![Codex CLI](https://img.shields.io/badge/Codex%20CLI-OpenAI-green)](https://github.com/openai/codex)\n[![Cursor](https://img.shields.io/badge/Cursor-AI%20IDE-orange)](https://cursor.sh)\n[![Copilot](https://img.shields.io/badge/GitHub%20Copilot-VSCode-lightblue)](https://github.com/features/copilot)\n[![OpenCode](https://img.shields.io/badge/OpenCode-CLI-gray)](https://github.com/opencode-ai/opencode)\n[![Antigravity](https://img.shields.io/badge/Antigravity-DeepMind-red)](https://github.com/anthropics/antigravity)\n\n**Antigravity Awesome Skills** is a curated, battle-tested library of **243 high-performance agentic skills** designed to work seamlessly across all major AI coding assistants:\n\n- ğŸŸ£ **Claude Code** (Anthropic CLI)\n- ğŸ”µ **Gemini CLI** (Google DeepMind)\n- ğŸŸ¢ **Codex CLI** (OpenAI)\n- ğŸ”´ **Antigravity IDE** (Google DeepMind)\n- ğŸ©µ **GitHub Copilot** (VSCode Extension)\n- ğŸŸ  **Cursor** (AI-native IDE)\n- âšª **OpenCode** (Open-source CLI)\n\nThis repository provides essential skills to transform your AI assistant into a **full-stack digital agency**, including official capabilities from **Anthropic**, **OpenAI**, **Google**, **Supabase**, and **Vercel Labs**.\n\n## ğŸ“ Table of Contents\n\n- [ğŸš€ New Here? Start Here!](#-new-here-start-here)\n- [ğŸ”Œ Compatibility](#-compatibility)\n- [Features & Categories](#features--categories)\n- [Full Skill Registry](#full-skill-registry-155155)\n- [Installation](#installation)\n- [How to Contribute](#how-to-contribute)\n- [Credits & Sources](#credits--sources)\n- [License](#license)\n\n---\n\n## New Here? Start Here!\n\n**First time using this repository?** We've created beginner-friendly guides to help you get started:\n\n- **[GETTING_STARTED.md](GETTING_STARTED.md)** - Complete beginner's guide (5-minute read)\n- **[CONTRIBUTING.md](CONTRIBUTING.md)** - How to contribute (step-by-step)\n- **[SKILL_ANATOMY.md](docs/SKILL_ANATOMY.md)** - Understanding how skills work\n- **[VISUAL_GUIDE.md](docs/VISUAL_GUIDE.md)** - Visual guide with diagrams\n\n**Quick Start:**\n\n```bash\n# 1. Install skills\ngit clone https://github.com/sickn33/antigravity-awesome-skills.git .agent/skills\n\n# 2. Use a skill in your AI assistant\n@brainstorming help me design a todo app\n```\n\nThat's it! Your AI assistant now has 243 specialized skills. ğŸ‰\n\n**Additional Resources:**\n\n- ğŸ’¡ **[Real-World Examples](docs/EXAMPLES.md)** - See skills in action\n- â“ **[FAQ](FAQ.md)** - Common questions answered\n\n---\n\n## ğŸ”Œ Compatibility\n\nThese skills follow the universal **SKILL.md** format and work with any AI coding assistant that supports agentic skills:\n\n| Tool                | Type      | Compatibility | Installation Path                        |\n| ------------------- | --------- | ------------- | ---------------------------------------- |\n| **Claude Code**     | CLI       | âœ… Full       | `.claude/skills/` or `.agent/skills/`    |\n| **Gemini CLI**      | CLI       | âœ… Full       | `.gemini/skills/` or `.agent/skills/`    |\n| **Codex CLI**       | CLI       | âœ… Full       | `.codex/skills/` or `.agent/skills/`     |\n| **Antigravity IDE** | IDE       | âœ… Full       | `.agent/skills/`                         |\n| **Cursor**          | IDE       | âœ… Full       | `.cursor/skills/` or project root        |\n| **GitHub Copilot**  | Extension | âš ï¸ Partial    | Copy skill content to `.github/copilot/` |\n| **OpenCode**        | CLI       | âœ… Full       | `.opencode/skills/` or `.claude/skills/` |\n\n> [!TIP]\n> Most tools auto-discover skills in `.agent/skills/`. For maximum compatibility, clone to this directory.\n\n---\n\nWhether you are using **Gemini CLI**, **Claude Code**, **Codex CLI**, **Cursor**, **GitHub Copilot**, **Antigravity**, or **OpenCode**, these skills are designed to drop right in and supercharge your AI agent.\n\nThis repository aggregates the best capabilities from across the open-source community, transforming your AI assistant into a full-stack digital agency capable of Engineering, Design, Security, Marketing, and Autonomous Operations.\n\n## Features & Categories\n\nThe repository is organized into several key areas of expertise:\n\n| Category                    | Skills Count | Key Skills Included                                                                                                          |\n| :-------------------------- | :----------- | :----------------------------------------------------------------------------------------------------------------",
      "default_branch": "main"
    },
    "fetched_at": "2026-01-25T03:07:42.844074"
  },
  {
    "basic_info": {
      "name": "ucp",
      "full_name": "Universal-Commerce-Protocol/ucp",
      "owner": "Universal-Commerce-Protocol",
      "description": "Specification and documentation for the Universal Commerce Protocol (UCP)",
      "url": "https://github.com/Universal-Commerce-Protocol/ucp",
      "clone_url": "https://github.com/Universal-Commerce-Protocol/ucp.git",
      "ssh_url": "git@github.com:Universal-Commerce-Protocol/ucp.git",
      "homepage": "https://ucp.dev",
      "created_at": "2025-12-31T02:17:21Z",
      "updated_at": "2026-01-25T00:34:32Z",
      "pushed_at": "2026-01-23T22:48:51Z"
    },
    "stats": {
      "stars": 2129,
      "forks": 237,
      "watchers": 2129,
      "open_issues": 36,
      "size": 6957
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 82366,
        "TypeScript": 61601,
        "JavaScript": 5010,
        "Shell": 1934
      },
      "license": "Apache License 2.0",
      "topics": []
    },
    "content": {
      "readme": "<!--\n   Copyright 2026 UCP Authors\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n-->\n\n<!-- markdownlint-disable-file MD041 -->\n\n<p align=\"center\">\n  <h1 align=\"center\">Universal Commerce Protocol (UCP)</h1>\n</p>\n\n<p align=\"center\">\n  <b>An open standard enabling interoperability between various commerce\n   entities to facilitate seamless commerce integrations.</b>\n</p>\n\n<p align=\"center\">\n  <a href=\"https://ucp.dev\">Documentation</a> |\n  <a href=\"https://ucp.dev/specification/overview\">Specification</a> |\n  <a href=\"https://github.com/Universal-Commerce-Protocol/ucp/discussions\">Discussions</a>\n</p>\n\n## Overview\n\nThe Universal Commerce Protocol (UCP) addresses a fragmented commerce landscape\nby providing a standardized common language and functional primitives. It\nenables platforms (like AI agents and apps), businesses, Payment Service\nProviders (PSPs), and Credential Providers (CPs) to communicate effectively,\nensuring secure and consistent commerce experiences across the web.\n\nWith UCP, businesses can:\n\n* **Declare** supported capabilities to enable autonomous discovery by\n    platforms.\n* **Facilitate** secure checkout sessions, with or without human intervention.\n* **Offer** personalized shopping experiences through standardized data\n    exchange.\n\n## Why UCP?\n\nAs commerce becomes increasingly agentic and distributed, the ability for\ndifferent systems to interoperate without custom, one-off integrations is vital.\nUCP aims to:\n\n* **Standardize Interaction:** Provide a uniform way for platforms to interact\n    with businesses, regardless of the underlying backend.\n* **Modularize Commerce:** Breakdown commerce into distinct **Capabilities**\n    (e.g., Checkout, Order) and **Extensions** (e.g., Discounts,\n    Fulfillment), allowing for flexible implementation.\n* **Enable Agentic Commerce:** Designed from the ground up to support AI\n    agents acting on behalf of users to discover products, fill carts, and\n    complete purchases securely.\n* **Enhance Security:** Support for advanced security patterns like AP2\n    mandates and verifiable credentials.\n\n### Key Features\n\n* **Composable Architecture:** UCP defines **Capabilities** (such as\n    \"Checkout\" or \"Identity Linking\") that businesses implement to enable easy\n    integration. On top of that, specific **Extensions** can be added to enhance\n    the consumer experience without bloating the capability definitions.\n* **Dynamic Discovery:** Businesses declare their supported Capabilities in a\n    standardized profile, allowing platforms to autonomously discover and\n    configure themselves.\n* **Transport Agnostic:** The protocol is designed to work across various\n    transports. Businesses can offer Capabilities via REST APIs, MCP (Model\n    Context Protocol), or A2A, depending on their infrastructure.\n* **Built on Standards:** UCP leverages existing open standards for payments,\n    identity, and security wherever applicable, rather than reinventing the\n    wheel.\n* **Developer Friendly:** A comprehensive set of SDKs and libraries\n    facilitates rapid development and integration.\n\n## Key Capabilities\n\nThe initial release focuses on the essential primitives for transacting:\n\n* **Checkout:** Facilitates checkout sessions including cart management and\n    tax calculation, supporting flows with or without human intervention.\n* **Identity Linking:** Enables platforms to obtain authorization to perform\n    actions on a user's behalf via OAuth 2.0.\n* **Order:** Webhook-based updates for order lifecycle events (shipped,\n    delivered, returned).\n* **Payment Token Exchange:** Protocols for PSPs and Credential Providers to\n    securely exchange payment tokens and credentials.\n\n## Getting Started\n\n* ğŸ“š **Explore the Documentation:** Visit [ucp.dev](https://ucp.dev) for a\n    complete overview, the full protocol specification, tutorials, and guides.\n* ğŸ¬ **Review our\n    [samples](https://github.com/Universal-Commerce-Protocol/samples)** for\n    implementation examples.\n* ğŸ› ï¸ **Use our\n    [SDKs](https://github.com/orgs/Universal-Commerce-Protocol/repositories)**\n    to start building your own integrations.\n* ğŸ“ **Check conformance** with our [conformance tests](https://github.com/Universal-Commerce-Protocol/conformance).\n\n## Contributing\n\nWe welcome community contributions to enhance and evolve UCP.\n\n* **Questions & Discussions:** Join our [GitHub\n    Discussions](https://github.com/Universal-Commerce-Protocol/ucp/discussions).\n* **Issues & Feedback:** Report issues or suggest ",
      "default_branch": "main"
    },
    "fetched_at": "2026-01-25T03:07:43.999183"
  },
  {
    "basic_info": {
      "name": "SimpleMem",
      "full_name": "aiming-lab/SimpleMem",
      "owner": "aiming-lab",
      "description": "SimpleMem: Efficient Lifelong Memory for LLM Agents",
      "url": "https://github.com/aiming-lab/SimpleMem",
      "clone_url": "https://github.com/aiming-lab/SimpleMem.git",
      "ssh_url": "git@github.com:aiming-lab/SimpleMem.git",
      "homepage": "",
      "created_at": "2026-01-01T23:53:40Z",
      "updated_at": "2026-01-25T02:58:10Z",
      "pushed_at": "2026-01-24T13:45:29Z"
    },
    "stats": {
      "stars": 1961,
      "forks": 221,
      "watchers": 1961,
      "open_issues": 3,
      "size": 24804
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 600577,
        "CSS": 7190,
        "JavaScript": 7106,
        "HTML": 4395
      },
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "# SimpleMem: Efficient Lifelong Memory for LLM Agents\n\n<div align=\"center\">\n\n<p align=\"center\">\n  <img src=\"fig/Fig_icon.png\" width=\"70%\">\n</p>\n\n\n[![Project Page](https://img.shields.io/badge/ğŸ¬_INTERACTIVE_DEMO-Visit_Our_Website-FF6B6B?style=for-the-badge&labelColor=FF6B6B&color=4ECDC4&logoColor=white)](https://aiming-lab.github.io/SimpleMem-Page)\n\n[![Paper](https://img.shields.io/badge/ğŸ“„_Paper-arXiv-b31b1b?style=flat-square)](https://arxiv.org/abs/2601.02553)\n[![GitHub](https://img.shields.io/badge/GitHub-SimpleMem-181717?logo=github&style=flat-square)](https://github.com/aiming-lab/SimpleMem)\n[![License](https://img.shields.io/badge/License-MIT-green?style=flat-square)](LICENSE)\n\n</div>\n\n---\n\n## ğŸ”¥ News\n- **[01/20/2026]** **SimpleMem is now available on PyPI!** ğŸ“¦ You can now install SimpleMem directly via `pip install simplemem` for easier setup and integration. [View Package Usage Guide â†’](docs/PACKAGE_USAGE.md)\n- **[01/19/2026]** **Added Local Memory Storage for SimpleMem Skill!** ğŸ’¾ SimpleMem Skill now supports local memory storage, enabling seamless memory retention and management directly within Claude Skills.\n- **[01/18/2026]** **SimpleMem now supports Claude Skills!** ğŸš€ Use SimpleMem in claude.ai to remember long-term information and project history across conversations. Register at [mcp.simplemem.cloud](https://mcp.simplemem.cloud), add the domain to Claude's network whitelist, **configure with your token in the skill file**, and import the skill!\n- **[01/14/2026]** **SimpleMem MCP Server is now LIVE and Open Source!** ğŸ‰ Experience SimpleMem as a cloud-hosted memory service at [mcp.simplemem.cloud](https://mcp.simplemem.cloud). Easily integrate with your favorite chat platforms (LM Studio, Cherry Studio) and AI agents (Cursor, Claude Desktop) using the **Streamable HTTP** MCP protocol. The MCP implementation features production-ready optimizations including multi-tenant user isolation, faster response times, and enhanced security. [View MCP Documentation â†’](MCP/README.md)\n- **[01/08/2026]** We've set up a Discord server and WeChat group to make it easier to collaborate and exchange ideas on this project. Welcome to join the Group to share your thoughts, ask questions, or contribute your ideas! ğŸ”¥ Join our [Discord](https://discord.gg/KA2zC32M) and [WeChat Group](fig/wechat_logo2.jpg) Now!\n- **[01/05/2026]** SimpleMem paper was released on [arXiv](https://arxiv.org/abs/2601.02553)!\n\n---\n\n## ğŸ“‘ Table of Contents\n\n- [ğŸŒŸ Overview](#-overview)\n- [ğŸ¯ Key Contributions](#-key-contributions)\n- [ğŸš€ Performance Highlights](#-performance-highlights)\n- [ğŸ“¦ Installation](#-installation)\n- [âš¡ Quick Start](#-quick-start)\n- [ğŸ”Œ MCP Server](#-mcp-server)\n- [ğŸ“Š Evaluation](#-evaluation)\n- [ğŸ“ File Structure](#-file-structure)\n- [ğŸ“ Citation](#-citation)\n- [ğŸ“„ License](#-license)\n- [ğŸ™ Acknowledgments](#-acknowledgments)\n\n---\n\n## ğŸŒŸ Overview\n\n<div align=\"center\">\n<img src=\"fig/Fig_tradeoff.png\" alt=\"Performance vs Efficiency Trade-off\" width=\"900\"/>\n\n*SimpleMem achieves superior F1 score (43.24%) with minimal token cost (~550), occupying the ideal top-left position.*\n</div>\n\n\n**SimpleMem** addresses the fundamental challenge of **efficient long-term memory for LLM agents** through a three-stage pipeline grounded in **Semantic Lossless Compression**. Unlike existing systems that either passively accumulate redundant context or rely on expensive iterative reasoning loops, SimpleMem maximizes **information density** and **token utilization** through:\n\n<table>\n<tr>\n<td width=\"33%\" align=\"center\">\n\n### ğŸ” Stage 1\n**Semantic Structured Compression**\n\nEntropy-based filtering and de-linearization of dialogue into self-contained atomic facts\n\n</td>\n<td width=\"33%\" align=\"center\">\n\n### ğŸ—‚ï¸ Stage 2\n**Structured Indexing**\n\nAsynchronous evolution from fragmented atoms to higher-order molecular insights\n\n</td>\n<td width=\"33%\" align=\"center\">\n\n### ğŸ¯ Stage 3\n**Adaptive Retrieval**\n\nComplexity-aware pruning across semantic, lexical, and symbolic layers\n\n</td>\n</tr>\n</table>\n\n<img src=\"fig/Fig_framework.png\" alt=\"SimpleMem Framework\" width=\"900\"/>\n\n*The SimpleMem Architecture: A three-stage pipeline for efficient lifelong memory through semantic lossless compression*\n\n---\n\n### ğŸ† Performance Comparison\n\n<div align=\"center\">\n\n**Speed Comparison Demo**\n\n<video src=\"https://github.com/aiming-lab/SimpleMem/raw/main/fig/simplemem-new.mp4\" controls width=\"900\"></video>\n\n*SimpleMem vs. Baseline: Real-time speed comparison demonstration*\n\n</div>\n\n<div align=\"center\">\n\n**LoCoMo-10 Benchmark Results (GPT-4.1-mini)**\n\n| Model | â±ï¸ Construction Time | ğŸ” Retrieval Time | âš¡ Total Time | ğŸ¯ Average F1 |\n|:------|:--------------------:|:-----------------:|:-------------:|:-------------:|\n| A-Mem | 5140.5s | 796.7s | 5937.2s | 32.58% |\n| LightMem | 97.8s | 577.1s | 675.9s | 24.63% |\n| Mem0 | 1350.9s | 583.4s | 1934.3s | 34.20% |\n| **SimpleMem** â­ | **92.6s** | **388.3s** | **480.9s** | **43.24%** |\n\n</div>\n\n> **ğŸ’¡ Key Advantages:**\n> - ğŸ† **Highest F1 Score**: 43.24% (+",
      "default_branch": "main"
    },
    "fetched_at": "2026-01-25T03:07:45.140757"
  },
  {
    "basic_info": {
      "name": "HY-Motion-1.0",
      "full_name": "Tencent-Hunyuan/HY-Motion-1.0",
      "owner": "Tencent-Hunyuan",
      "description": "HY-Motion model for 3D character animation generation. ",
      "url": "https://github.com/Tencent-Hunyuan/HY-Motion-1.0",
      "clone_url": "https://github.com/Tencent-Hunyuan/HY-Motion-1.0.git",
      "ssh_url": "git@github.com:Tencent-Hunyuan/HY-Motion-1.0.git",
      "homepage": "https://hunyuan.tencent.com/motion",
      "created_at": "2025-12-29T11:09:18Z",
      "updated_at": "2026-01-25T01:16:36Z",
      "pushed_at": "2026-01-04T03:45:23Z"
    },
    "stats": {
      "stars": 1888,
      "forks": 141,
      "watchers": 1888,
      "open_issues": 9,
      "size": 20527
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 285533,
        "HTML": 40286
      },
      "license": "Other",
      "topics": []
    },
    "content": {
      "readme": "[ä¸­æ–‡é˜…è¯»](README_zh_cn.md)\n\n\n<p align=\"center\">\n  <img src=\"./assets/banner.png\" alt=\"Banner\" width=\"100%\">\n</p>\n\n<div align=\"center\">\n  <a href=\"https://hunyuan.tencent.com/motion\" target=\"_blank\">\n    <img src=\"https://img.shields.io/badge/Official%20Site-333399.svg?logo=homepage\" height=\"22px\" alt=\"Official Site\">\n  </a>\n  <a href=\"https://github.com/Tencent-Hunyuan/HY-Motion-1.0\" target=\"_blank\">\n    <img src=\"https://img.shields.io/badge/GitHub-Repo-181717?logo=github&logoColor=white\" height=\"22px\" alt=\"Github Repo\">\n  </a>\n  <a href=\"https://huggingface.co/spaces/tencent/HY-Motion-1.0\" target=\"_blank\">\n    <img src=\"https://img.shields.io/badge/%F0%9F%A4%97%20Demo-276cb4.svg\" height=\"22px\" alt=\"HuggingFace Space\">\n  </a>\n  <a href=\"https://huggingface.co/tencent/HY-Motion-1.0\" target=\"_blank\">\n    <img src=\"https://img.shields.io/badge/%F0%9F%A4%97%20Models-d96902.svg\" height=\"22px\" alt=\"HuggingFace Models\">\n  </a>\n  <a href=\"https://arxiv.org/pdf/2512.23464\" target=\"_blank\">\n    <img src=\"https://img.shields.io/badge/Report-b5212f.svg?logo=arxiv\" height=\"22px\" alt=\"ArXiv Report\">\n  </a>\n  <a href=\"https://x.com/TencentHunyuan\" target=\"_blank\">\n    <img src=\"https://img.shields.io/badge/Hunyuan-black.svg?logo=x\" height=\"22px\" alt=\"X (Twitter)\">\n  </a>\n</div>\n\n\n# HY-Motion 1.0: Scaling Flow Matching Models for 3D Motion Generation\n\n\n<p align=\"center\">\n  <img src=\"./assets/teaser.jpg\" alt=\"Teaser\" width=\"100%\">\n</p>\n\n\n## ğŸ”¥ News\n- **Dec 30, 2025**: ğŸ¤— We released the inference code and pretrained models of [HY-Motion 1.0](https://huggingface.co/tencent/HY-Motion-1.0). Please give it a try via our [HuggingFace Space](https://huggingface.co/spaces/tencent/HY-Motion-1.0) and our [Official Site](https://hunyuan.tencent.com/motion)!\n\n\n## **Introduction**\n\n**HY-Motion 1.0** is a series of text-to-3D human motion generation models based on Diffusion Transformer (DiT) and Flow Matching. It allows developers to generate skeleton-based 3D character animations from simple text prompts, which can be directly integrated into various 3D animation pipelines. This model series is the first to scale DiT-based text-to-motion models to the billion-parameter level, achieving significant improvements in instruction-following capabilities and motion quality over existing open-source models.\n\n### Key Features\n- **State-of-the-Art Performance**: Achieves state-of-the-art performance in both instruction-following capability and generated motion quality.\n\n- **Billion-Scale Models**: We are the first to successfully scale DiT-based models to the billion-parameter level for text-to-motion generation. This results in superior instruction understanding and following capabilities, outperforming comparable open-source models.\n\n- **Advanced Three-Stage Training**: Our models are trained using a comprehensive three-stage process:\n\n    - *Large-Scale Pre-training*: Trained on over 3,000 hours of diverse motion data to learn a broad motion prior.\n\n    - *High-Quality Fine-tuning*: Fine-tuned on 400 hours of curated, high-quality 3D motion data to enhance motion detail and smoothness.\n\n    - *Reinforcement Learning*: Utilizes Reinforcement Learning from human feedback and reward models to further refine instruction-following and motion naturalness.\n\n\n\n<p align=\"center\">\n  <img src=\"./assets/pipeline.png\" alt=\"System Overview\" width=\"100%\">\n</p>\n\n<p align=\"center\">\n  <img src=\"./assets/arch.png\" alt=\"Architecture\" width=\"100%\">\n</p>\n\n<p align=\"center\">\n  <img src=\"./assets/sotacomp.jpg\" alt=\"ComparisonSoTA\" width=\"100%\">\n</p>\n\n\n\n\n## ğŸ Model Zoo\n\n**HY-Motion 1.0 Series**\n\n| Model | Description | Date | Size | Huggingface | VRAM (min) |\n|:-------|:-------------|:------:|:------:|:-------------:|:-------------:|\n| **HY-Motion-1.0** | Standard Text2Motion Model | 2025-12-30 | 1.0B | [Download](https://huggingface.co/tencent/HY-Motion-1.0/tree/main/HY-Motion-1.0) | 26GB |\n| **HY-Motion-1.0-Lite** | Lightweight Text2Motion Model | 2025-12-30 | 0.46B | [Download](https://huggingface.co/tencent/HY-Motion-1.0/tree/main/HY-Motion-1.0-Lite) | 24GB |\n\n*Note*: To reduce GPU VRAM requirements, please use the following settings: `--num_seeds=1`, text prompt with less than 30 words, and motion length less than 5 seconds.  \n*Note*: This table does not includes GPU VRAM requirements for LLM-based prompt engineering feature. If you have sufficient VRAM to run HY-Motion-1.0 model but gradio fails with a VRAM-related error, Run the Gradio application with prompt engineering disabled by setting the environment variable like this: `DISABLE_PROMPT_ENGINEERING=True python3 gradio_app.py`\n\n## ğŸ¤— Get Started with HY-Motion 1.0\n\nHY-Motion 1.0 supports macOS, Windows, and Linux.\n\n\n- [Code Usage (CLI)](#code-usage-cli)\n- [Gradio App](#gradio-app)\n\n\n#### 1. Installation\n\nFirst, install PyTorch via the [official site](https://pytorch.org/). Then install the dependencies:\n\n```bash\ngit clone https://github.com/Tencent-Hunyuan/HY-Motion-1.0.git\ncd HY-Motion-1.0/\n# Make sure git-lf",
      "default_branch": "master"
    },
    "fetched_at": "2026-01-25T03:07:46.295321"
  },
  {
    "basic_info": {
      "name": "heartlib",
      "full_name": "HeartMuLa/heartlib",
      "owner": "HeartMuLa",
      "description": null,
      "url": "https://github.com/HeartMuLa/heartlib",
      "clone_url": "https://github.com/HeartMuLa/heartlib.git",
      "ssh_url": "git@github.com:HeartMuLa/heartlib.git",
      "homepage": null,
      "created_at": "2026-01-15T07:53:15Z",
      "updated_at": "2026-01-25T02:32:11Z",
      "pushed_at": "2026-01-23T11:40:47Z"
    },
    "stats": {
      "stars": 1870,
      "forks": 200,
      "watchers": 1870,
      "open_issues": 48,
      "size": 893
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 77582
      },
      "license": "Apache License 2.0",
      "topics": []
    },
    "content": {
      "readme": "<p align=\"center\">\n    <picture>\n        <source srcset=\"./assets/logo.png\" media=\"(prefers-color-scheme: dark)\">\n        <img src=\"./assets/logo.png\" width=\"30%\">\n    </picture>\n    \n</p>\n\n<p align=\"center\">\n    <a href=\"https://heartmula.github.io/\">Demo ğŸ¶</a> &nbsp;|&nbsp; ğŸ“‘ <a href=\"https://arxiv.org/pdf/2601.10547\">Paper</a>\n    <br>\n    <a href=\"https://huggingface.co/HeartMuLa/HeartMuLa-oss-3B\">HeartMuLa-oss-3B ğŸ¤—</a> &nbsp;|&nbsp; <a href=\"https://modelscope.cn/models/HeartMuLa/HeartMuLa-oss-3B\">HeartMuLa-oss-3B <picture>\n        <source srcset=\"./assets/badge.svg\" media=\"(prefers-color-scheme: dark)\">\n        <img src=\"./assets/badge.svg\" width=\"20px\">\n    </picture></a>\n    <br>\n    <a href=\"https://huggingface.co/HeartMuLa/HeartMuLa-RL-oss-3B-20260123\"> HeartMuLa-RL-oss-3B-20260123 ğŸ¤—</a> &nbsp;|&nbsp; <a href=\"https://modelscope.cn/models/HeartMuLa/HeartMuLa-RL-oss-3B-20260123\">HeartMuLa-RL-oss-3B-20260123 <picture>\n        <source srcset=\"./assets/badge.svg\" media=\"(prefers-color-scheme: dark)\">\n        <img src=\"./assets/badge.svg\" width=\"20px\">\n    </picture></a>\n    \n</p>\n\n---\n# HeartMuLa: A Family of Open Sourced Music Foundation Models\n\nHeartMuLa is a family of open sourced music foundation models including: \n1. HeartMuLa: a music language model that generates music conditioned on lyrics and tags with multilingual support including but not limited to English, Chinese, Japanese, Korean and Spanish.\n2. HeartCodec: a 12.5 hz music codec with high reconstruction fidelity;\n3. HeartTranscriptor: a whisper-based model specifically tuned for lyrics transcription; Check [this page](./examples/README.md) for its usage.\n4. HeartCLAP: an audioâ€“text alignment model that establishes a unified embedding space for music descriptions and cross-modal retrieval.\n---\n\n\nBelow shows the experiment result of our oss-3B version compared with other baselines.\n<p align=\"center\">\n    <picture>\n        <source srcset=\"./assets/exp.png\" media=\"(prefers-color-scheme: dark)\">\n        <img src=\"./assets/exp.png\" width=\"90%\">\n    </picture>\n    \n</p>\n\n---\n\n## ğŸ”¥ Highlight\n\nOur latest internal version of HeartMuLa-7B achieves **comparable performance with Suno** in terms of musicality, fidelity and controllability. If you are interested, welcome to reach us out via heartmula.ai@gmail.com\n\n## ğŸ“° News\nJoin on Discord! [<img alt=\"join discord\" src=\"https://img.shields.io/discord/842440537755353128?color=%237289da&logo=discord\"/>](https://discord.gg/BKXF5FgH)\n\n- ğŸš€ **23 Jan. 2026**\n\n    By leveraging Reinforcement Learning, we have continuously refined our model and are proud to officially release **HeartMuLa-RL-oss-3B-20260123**. This version is designed to achieve more precise control over styles and tags. Simultaneously, we are launching **HeartCodec-oss-20260123**, which optimizes audio decoding quality.\n\n- ğŸ«¶ **20 Jan. 2026** \n    \n    [Benji](https://github.com/benjiyaya) has created a wonderful [ComfyUI custom node](https://github.com/benjiyaya/HeartMuLa_ComfyUI) for HeartMuLa. Thanks Benji!\n- âš–ï¸ **20 Jan. 2026** \n\n    License update: We update the license of this repo and all related model weights to **Apache 2.0**.\n- ğŸš€ **14 Jan. 2026**  \n    The official release of **HeartTranscriptor-oss** and the first **HeartMuLa-oss-3B** version along with our **HeartCodec-oss**.\n\n---\n## ğŸ§­ TODOs\n\n- â³ Release scripts for inference acceleration and streaming inference. The current inference speed is around RTF $\\approx 1.0$.\n- â³ Support **reference audio conditioning**, **fine-grained controllable music generation**, **hot song generation**.\n- â³ Release the **HeartMuLa-oss-7B** version.\n- âœ… Release inference code and pretrained checkpoints of  \n  **HeartCodec-oss, HeartMuLa-oss-3B, and HeartTranscriptor-oss**.\n\n---\n\n## ğŸ› ï¸ Local Deployment\n\n### âš™ï¸ Environment Setup\n\nWe recommend using `python=3.10` for local deployment.\n\nClone this repo and install locally.\n\n```\ngit clone https://github.com/HeartMuLa/heartlib.git\ncd heartlib\npip install -e .\n```\n\nDownload our pretrained checkpoints from huggingface or modelscope using the following command:\n\n```\n# if you are using huggingface\nhf download --local-dir './ckpt' 'HeartMuLa/HeartMuLaGen'\n\n## To use version released on 20260123 (recommended)\nhf download --local-dir './ckpt/HeartMuLa-oss-3B' 'HeartMuLa/HeartMuLa-RL-oss-3B-20260123'\nhf download --local-dir './ckpt/HeartCodec-oss' HeartMuLa/HeartCodec-oss-20260123\n\n## To use oss-3B version\nhf download --local-dir './ckpt/HeartMuLa-oss-3B' 'HeartMuLa/HeartMuLa-oss-3B'\nhf download --local-dir './ckpt/HeartCodec-oss' 'HeartMuLa/HeartCodec-oss'\n\n# if you are using modelscope\nmodelscope download --model 'HeartMuLa/HeartMuLaGen' --local_dir './ckpt'\n\n## To use version released on 20260123 (recommended)\nmodelscope download --model 'HeartMuLa/HeartMuLa-RL-oss-3B-20260123' --local_dir './ckpt/HeartMuLa-oss-3B'\nmodelscope download --model 'HeartMuLa/HeartCodec-oss-20260123' --local_dir './ckpt/HeartCodec-oss'\n\n## To use oss-3B version\nmodelscope download --m",
      "default_branch": "main"
    },
    "fetched_at": "2026-01-25T03:07:47.434887"
  },
  {
    "basic_info": {
      "name": "skills",
      "full_name": "trailofbits/skills",
      "owner": "trailofbits",
      "description": "Trail of Bits Claude Code skills for security research, vulnerability detection, and audit workflows",
      "url": "https://github.com/trailofbits/skills",
      "clone_url": "https://github.com/trailofbits/skills.git",
      "ssh_url": "git@github.com:trailofbits/skills.git",
      "homepage": "",
      "created_at": "2026-01-14T18:23:21Z",
      "updated_at": "2026-01-25T02:43:14Z",
      "pushed_at": "2026-01-23T23:26:14Z"
    },
    "stats": {
      "stars": 1859,
      "forks": 143,
      "watchers": 1859,
      "open_issues": 12,
      "size": 506
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 301687,
        "Shell": 53365,
        "CodeQL": 12064,
        "C": 8714,
        "Swift": 5459,
        "C#": 5442,
        "Ruby": 5419,
        "Java": 5221,
        "JavaScript": 5033,
        "Kotlin": 5019,
        "TypeScript": 4645,
        "PHP": 3832,
        "Rust": 2608,
        "Go": 2213
      },
      "license": "Creative Commons Attribution Share Alike 4.0 International",
      "topics": [
        "agent-skills"
      ]
    },
    "content": {
      "readme": "# Trail of Bits Skills Marketplace\n\nA Claude Code plugin marketplace from Trail of Bits providing skills to enhance AI-assisted security analysis, testing, and development workflows.\n\n## Installation\n\n### Add the Marketplace\n\n```\n/plugin marketplace add trailofbits/skills\n```\n\n### Browse and Install Plugins\n\n```\n/plugin menu\n```\n\n### Local Development\n\nTo add the marketplace locally (e.g., for testing or development), navigate to the **parent directory** of this repository:\n\n```\ncd /path/to/parent  # e.g., if repo is at ~/projects/skills, be in ~/projects\n/plugins marketplace add ./skills\n```\n\n## Available Plugins\n\n### Smart Contract Security\n\n| Plugin | Description |\n|--------|-------------|\n| [building-secure-contracts](plugins/building-secure-contracts/) | Smart contract security toolkit with vulnerability scanners for 6 blockchains |\n| [entry-point-analyzer](plugins/entry-point-analyzer/) | Identify state-changing entry points in smart contracts for security auditing |\n\n### Code Auditing\n\n| Plugin | Description |\n|--------|-------------|\n| [audit-context-building](plugins/audit-context-building/) | Build deep architectural context through ultra-granular code analysis |\n| [burpsuite-project-parser](plugins/burpsuite-project-parser/) | Search and extract data from Burp Suite project files |\n| [differential-review](plugins/differential-review/) | Security-focused differential review of code changes with git history analysis |\n| [semgrep-rule-creator](plugins/semgrep-rule-creator/) | Create and refine Semgrep rules for custom vulnerability detection |\n| [semgrep-rule-variant-creator](plugins/semgrep-rule-variant-creator/) | Port existing Semgrep rules to new target languages with test-driven validation |\n| [sharp-edges](plugins/sharp-edges/) | Identify error-prone APIs, dangerous configurations, and footgun designs |\n| [static-analysis](plugins/static-analysis/) | Static analysis toolkit with CodeQL, Semgrep, and SARIF parsing |\n| [testing-handbook-skills](plugins/testing-handbook-skills/) | Skills from the [Testing Handbook](https://appsec.guide): fuzzers, static analysis, sanitizers, coverage |\n| [variant-analysis](plugins/variant-analysis/) | Find similar vulnerabilities across codebases using pattern-based analysis |\n\n### Verification\n\n| Plugin | Description |\n|--------|-------------|\n| [constant-time-analysis](plugins/constant-time-analysis/) | Detect compiler-induced timing side-channels in cryptographic code |\n| [property-based-testing](plugins/property-based-testing/) | Property-based testing guidance for multiple languages and smart contracts |\n| [spec-to-code-compliance](plugins/spec-to-code-compliance/) | Specification-to-code compliance checker for blockchain audits |\n\n### Audit Lifecycle\n\n| Plugin | Description |\n|--------|-------------|\n| [fix-review](plugins/fix-review/) | Verify fix commits address audit findings without introducing bugs |\n\n### Reverse Engineering\n\n| Plugin | Description |\n|--------|-------------|\n| [dwarf-expert](plugins/dwarf-expert/) | Interact with and understand the DWARF debugging format |\n\n### Mobile Security\n\n| Plugin | Description |\n|--------|-------------|\n| [firebase-apk-scanner](plugins/firebase-apk-scanner/) | Scan Android APKs for Firebase security misconfigurations |\n\n### Development\n\n| Plugin | Description |\n|--------|-------------|\n| [ask-questions-if-underspecified](plugins/ask-questions-if-underspecified/) | Clarify requirements before implementing |\n\n### Team Management\n\n| Plugin | Description |\n|--------|-------------|\n| [culture-index](plugins/culture-index/) | Interpret Culture Index survey results for individuals and teams |\n\n## Trophy Case\n\nBugs discovered using Trail of Bits Skills. Found something? [Let us know!](https://github.com/trailofbits/skills/issues/new?template=trophy-case.yml)\n\nWhen reporting bugs you've found, feel free to mention:\n> Found using [Trail of Bits Skills](https://github.com/trailofbits/skills)\n\n| Skill | Bug |\n|-------|-----|\n| constant-time-analysis | [Timing side-channel in ML-DSA signing](https://github.com/RustCrypto/signatures/pull/1144) |\n\n## Contributing\n\nWe welcome contributions! Please see [CLAUDE.md](CLAUDE.md) for skill authoring guidelines.\n\n## License\n\nThis work is licensed under a [Creative Commons Attribution-ShareAlike 4.0 International License](https://creativecommons.org/licenses/by-sa/4.0/).\n\n## About Trail of Bits\n\n[Trail of Bits](https://www.trailofbits.com/) is a security research and consulting firm.\n",
      "default_branch": "main"
    },
    "fetched_at": "2026-01-25T03:07:48.593482"
  },
  {
    "basic_info": {
      "name": "nanocode",
      "full_name": "1rgs/nanocode",
      "owner": "1rgs",
      "description": "Minimal Claude Code alternative. Single Python file, zero dependencies, ~250 lines.",
      "url": "https://github.com/1rgs/nanocode",
      "clone_url": "https://github.com/1rgs/nanocode.git",
      "ssh_url": "git@github.com:1rgs/nanocode.git",
      "homepage": null,
      "created_at": "2026-01-11T02:12:27Z",
      "updated_at": "2026-01-25T02:59:03Z",
      "pushed_at": "2026-01-14T05:59:51Z"
    },
    "stats": {
      "stars": 1694,
      "forks": 140,
      "watchers": 1694,
      "open_issues": 6,
      "size": 183
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 8445
      },
      "license": null,
      "topics": []
    },
    "content": {
      "readme": "# nanocode\n\nMinimal Claude Code alternative. Single Python file, zero dependencies, ~250 lines.\n\nBuilt using Claude Code, then used to build itself.\n\n![screenshot](screenshot.png)\n\n## Features\n\n- Full agentic loop with tool use\n- Tools: `read`, `write`, `edit`, `glob`, `grep`, `bash`\n- Conversation history\n- Colored terminal output\n\n## Usage\n\n```bash\nexport ANTHROPIC_API_KEY=\"your-key\"\npython nanocode.py\n```\n\n### OpenRouter\n\nUse [OpenRouter](https://openrouter.ai) to access any model:\n\n```bash\nexport OPENROUTER_API_KEY=\"your-key\"\npython nanocode.py\n```\n\nTo use a different model:\n\n```bash\nexport OPENROUTER_API_KEY=\"your-key\"\nexport MODEL=\"openai/gpt-5.2\"\npython nanocode.py\n```\n\n## Commands\n\n- `/c` - Clear conversation\n- `/q` or `exit` - Quit\n\n## Tools\n\n| Tool | Description |\n|------|-------------|\n| `read` | Read file with line numbers, offset/limit |\n| `write` | Write content to file |\n| `edit` | Replace string in file (must be unique) |\n| `glob` | Find files by pattern, sorted by mtime |\n| `grep` | Search files for regex |\n| `bash` | Run shell command |\n\n## Example\n\n```\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nâ¯ what files are here?\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\nâº Glob(**/*.py)\n  â¿  nanocode.py\n\nâº There's one Python file: nanocode.py\n```\n\n## License\n\nMIT\n",
      "default_branch": "master"
    },
    "fetched_at": "2026-01-25T03:07:49.743317"
  },
  {
    "basic_info": {
      "name": "notebooklm-py",
      "full_name": "teng-lin/notebooklm-py",
      "owner": "teng-lin",
      "description": "Unofficial Python API for Google NotebookLM",
      "url": "https://github.com/teng-lin/notebooklm-py",
      "clone_url": "https://github.com/teng-lin/notebooklm-py.git",
      "ssh_url": "git@github.com:teng-lin/notebooklm-py.git",
      "homepage": "https://github.com/teng-lin/notebooklm-py",
      "created_at": "2026-01-07T15:27:19Z",
      "updated_at": "2026-01-25T02:53:00Z",
      "pushed_at": "2026-01-24T20:18:12Z"
    },
    "stats": {
      "stars": 1485,
      "forks": 137,
      "watchers": 1485,
      "open_issues": 1,
      "size": 24750
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 1587466
      },
      "license": "MIT License",
      "topics": [
        "api",
        "claude",
        "notebookln",
        "python",
        "sdk",
        "skills"
      ]
    },
    "content": {
      "readme": "# notebooklm-py\n<p align=\"left\">\n  <img src=\"https://raw.githubusercontent.com/teng-lin/notebooklm-py/main/notebooklm-py.png\" alt=\"notebooklm-py logo\" width=\"128\">\n</p>\n\n**Comprehensive Python API for Google NotebookLM.** Full programmatic access to NotebookLM's featuresâ€”including capabilities the web UI doesn't exposeâ€”from Python or the command line.\n\n[![PyPI version](https://img.shields.io/pypi/v/notebooklm-py.svg)](https://pypi.org/project/notebooklm-py/)\n[![Python Version](https://img.shields.io/badge/python-3.10%20%7C%203.11%20%7C%203.12%20%7C%203.13%20%7C%203.14-blue)](https://pypi.org/project/notebooklm-py/)\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n[![Tests](https://github.com/teng-lin/notebooklm-py/actions/workflows/test.yml/badge.svg)](https://github.com/teng-lin/notebooklm-py/actions/workflows/test.yml)\n\n**Source & Development**: <https://github.com/teng-lin/notebooklm-py>\n\n> **âš ï¸ Unofficial Library - Use at Your Own Risk**\n>\n> This library uses **undocumented Google APIs** that can change without notice.\n>\n> - **Not affiliated with Google** - This is a community project\n> - **APIs may break** - Google can change internal endpoints anytime\n> - **Rate limits apply** - Heavy usage may be throttled\n>\n> Best for prototypes, research, and personal projects. See [Troubleshooting](docs/troubleshooting.md) for debugging tips.\n\n## What You Can Build\n\nğŸ¤– **AI Agent Tools** - Integrate NotebookLM into Claude Code or other LLM agents. Ships with [Claude Code skills](#agent-skills-claude-code) for natural language automation (`notebooklm skill install`), or build your own integrations with the async Python API.\n\nğŸ“š **Research Automation** - Bulk-import sources (URLs, PDFs, YouTube, Google Drive), run web/Drive research queries with auto-import, and extract insights programmatically. Build repeatable research pipelines.\n\nğŸ™ï¸ **Content Generation** - Generate Audio Overviews (podcasts), videos, slide decks, quizzes, flashcards, infographics, data tables, mind maps, and study guides. Full control over formats, styles, and output.\n\nğŸ“¥ **Downloads & Export** - Download all generated artifacts locally (MP3, MP4, PDF, PNG, CSV, JSON, Markdown). Export to Google Docs/Sheets. **Features the web UI doesn't offer**: batch downloads, quiz/flashcard export in multiple formats, mind map JSON extraction.\n\n## Three Ways to Use\n\n| Method | Best For |\n|--------|----------|\n| **Python API** | Application integration, async workflows, custom pipelines |\n| **CLI** | Shell scripts, quick tasks, CI/CD automation |\n| **Agent Skills** | Claude Code, LLM agents, natural language automation |\n\n## Features\n\n### Complete NotebookLM Coverage\n\n| Category | Capabilities |\n|----------|--------------|\n| **Notebooks** | Create, list, rename, delete |\n| **Sources** | URLs, YouTube, files (PDF, text, Markdown, Word, audio, video, images), Google Drive, pasted text; refresh, get guide/fulltext |\n| **Chat** | Questions, conversation history, custom personas |\n| **Research** | Web and Drive research agents (fast/deep modes) with auto-import |\n| **Sharing** | Public/private links, user permissions (viewer/editor), view level control |\n\n### Content Generation (All NotebookLM Studio Types)\n\n| Type | Options | Download Format |\n|------|---------|-----------------|\n| **Audio Overview** | 4 formats (deep-dive, brief, critique, debate), 3 lengths, 50+ languages | MP3/MP4 |\n| **Video Overview** | 2 formats, 9 visual styles (classic, whiteboard, kawaii, anime, etc.) | MP4 |\n| **Slide Deck** | Detailed or presenter format, adjustable length | PDF |\n| **Infographic** | 3 orientations, 3 detail levels | PNG |\n| **Quiz** | Configurable quantity and difficulty | JSON, Markdown, HTML |\n| **Flashcards** | Configurable quantity and difficulty | JSON, Markdown, HTML |\n| **Report** | Briefing doc, study guide, blog post, or custom prompt | Markdown |\n| **Data Table** | Custom structure via natural language | CSV |\n| **Mind Map** | Interactive hierarchical visualization | JSON |\n\n### Beyond the Web UI\n\nThese features are available via API/CLI but not exposed in NotebookLM's web interface:\n\n- **Batch downloads** - Download all artifacts of a type at once\n- **Quiz/Flashcard export** - Get structured JSON, Markdown, or HTML (web UI only shows interactive view)\n- **Mind map data extraction** - Export hierarchical JSON for visualization tools\n- **Data table CSV export** - Download structured tables as spreadsheets\n- **Source fulltext access** - Retrieve the indexed text content of any source\n- **Programmatic sharing** - Manage permissions without the UI\n\n## Installation\n\n```bash\n# Basic installation\npip install notebooklm-py\n\n# With browser login support (required for first-time setup)\npip install \"notebooklm-py[browser]\"\nplaywright install chromium\n```\n\n### Development Installation\n\nFor contributors or testing unreleased features:\n\n```bash\npip install git+https://github.com/teng-lin/notebooklm-py@main\n```\n\nâš ï¸ T",
      "default_branch": "main"
    },
    "fetched_at": "2026-01-25T03:07:50.906664"
  },
  {
    "basic_info": {
      "name": "AlphaGPT",
      "full_name": "imbue-bit/AlphaGPT",
      "owner": "imbue-bit",
      "description": "ä½¿ç”¨ç¬¦å·å›å½’åœ¨ä¸­å›½è‚¡å¸‚ä¸åŠ å¯†å¸‚åœºä¸Šè¿›è¡Œé«˜æ•ˆå› å­æŒ–æ˜ã€‚",
      "url": "https://github.com/imbue-bit/AlphaGPT",
      "clone_url": "https://github.com/imbue-bit/AlphaGPT.git",
      "ssh_url": "git@github.com:imbue-bit/AlphaGPT.git",
      "homepage": "",
      "created_at": "2026-01-08T02:05:21Z",
      "updated_at": "2026-01-25T03:03:47Z",
      "pushed_at": "2026-01-20T08:29:01Z"
    },
    "stats": {
      "stars": 1318,
      "forks": 2139,
      "watchers": 1318,
      "open_issues": 0,
      "size": 1995
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 109367
      },
      "license": "Apache License 2.0",
      "topics": [
        "deep-learning",
        "finance",
        "math",
        "quant",
        "quantitative-finance",
        "sr",
        "transformer",
        "uniswap-v4"
      ]
    },
    "content": {
      "readme": "# AlphaGPT\n\n> [!IMPORTANT]\n> GitHub çš„ issue å¹¶éè¢«è®¾è®¡ä¸ºæ‰“å¡å·¥å…·ï¼ä»…ç”¨äºã€Œæ‰“å¡ã€ã€Œç•™åã€çš„ issue å°†ä¼šè¢«ç›´æ¥åˆ é™¤ã€‚è‹¥æ‚¨éœ€è¦ç¤¾ç¾¤ï¼Œè¯·è€ƒè™‘QQç¾¤ç»„ 1076893473ã€‚\n\n> [!IMPORTANT]\n> è‹¥æ‚¨åœ¨åŠ å¯†å¸‚åœºè¿›è¡Œäº¤æ˜“ï¼Œå¦å¯å‚è€ƒ [Defense in Predatory Markets: A Differential Game Framework for AMM Liquidity via Uniswap V4 Hooks](https://github.com/imbue-bit/no_JIT) è¿›è¡Œåšå¸‚ã€‚ç¬”è€…æ‡’å¾—å‘ä¼šè®®æŠ•ç¨¿äº†ã€‚è‹¥æœ‰ç–‘é—®ï¼Œè¯·è”ç³» imbue2025@outlook.com. BTWï¼Œå¯¹è¯¥ä»“åº“ä»£ç è¿›è¡Œ live trading å‰ä½œé€‚å½“çš„ä¿®æ”¹å¯èƒ½ä¼šå‡ºç°æ„æƒ³ä¸åˆ°çš„ä¸šç»©ã€‚\n\n## What happenedï¼Ÿ\n\nç›®å‰åŒæ–¹å·²è¾¾æˆå’Œè§£ã€‚\n\n## å»ä¸­å¿ƒåŒ–\n\nç›®å‰åŒæ–¹å·²è¾¾æˆå’Œè§£ã€‚\n\n## è´£ä»»å…é™¤\n\nç›®å‰åŒæ–¹å·²è¾¾æˆå’Œè§£ã€‚\n\n## Abstract\n\nç›®å‰åŒæ–¹å·²è¾¾æˆå’Œè§£ã€‚\n\n## Motivation\n\nç›®å‰åŒæ–¹å·²è¾¾æˆå’Œè§£ã€‚\n\n## This was their money-making machine. Now it's your public library.\n\nç›®å‰åŒæ–¹å·²è¾¾æˆå’Œè§£ã€‚\n\n## OH! NO!\n\nç›®å‰åŒæ–¹å·²è¾¾æˆå’Œè§£ã€‚\n\n## Star History\n\n[![Star History Chart](https://api.star-history.com/svg?repos=imbue-bit/AlphaGPT&type=date&legend=top-left)](https://www.star-history.com/#imbue-bit/AlphaGPT&type=date&legend=top-left)\n",
      "default_branch": "main"
    },
    "fetched_at": "2026-01-25T03:07:52.045886"
  },
  {
    "basic_info": {
      "name": "IQuest-Coder-V1",
      "full_name": "IQuestLab/IQuest-Coder-V1",
      "owner": "IQuestLab",
      "description": null,
      "url": "https://github.com/IQuestLab/IQuest-Coder-V1",
      "clone_url": "https://github.com/IQuestLab/IQuest-Coder-V1.git",
      "ssh_url": "git@github.com:IQuestLab/IQuest-Coder-V1.git",
      "homepage": null,
      "created_at": "2025-12-31T06:42:57Z",
      "updated_at": "2026-01-24T15:35:49Z",
      "pushed_at": "2026-01-23T17:04:08Z"
    },
    "stats": {
      "stars": 1267,
      "forks": 88,
      "watchers": 1267,
      "open_issues": 16,
      "size": 31686
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 706343,
        "Shell": 47957
      },
      "license": "Other",
      "topics": []
    },
    "content": {
      "readme": "![Evaluation Results](./papers/iquest-coder-v1-logo.png)\n\n<p align=\"center\">\n  ğŸ“˜ <a href=\"https://iquestlab.github.io\">Blog</a >\n  &nbsp;â€¢&nbsp;\n  ğŸ“„ <a href=\"https://github.com/IQuestLab/IQuest-Coder-V1/blob/main/papers/IQuest_Coder_Technical_Report.pdf\">Technical Report</a >\n</p >\n\n# IQuest-Coder-V1 Model Family\n\n| Model | Link |\n|-------|------|\n| IQuest-Coder-V1-40B-Base-Stage1 | [ğŸ¤— Hugging Face](https://huggingface.co/IQuestLab/IQuest-Coder-V1-40B-Base-Stage1) |\n| IQuest-Coder-V1-40B-Base | [ğŸ¤— Hugging Face](https://huggingface.co/IQuestLab/IQuest-Coder-V1-40B-Base) |\n| IQuest-Coder-V1-40B-Instruct | [ğŸ¤— Hugging Face](https://huggingface.co/IQuestLab/IQuest-Coder-V1-40B-Instruct) |\n| IQuest-Coder-V1-40B-Loop-Instruct | [ğŸ¤— Hugging Face](https://huggingface.co/IQuestLab/IQuest-Coder-V1-40B-Loop-Instruct) |\n\n## Sampling Parameters:\nFor the IQuest-Coder-V1-Instruct: We suggest using Temperature=0.6, TopP=0.85, TopK=20.\n\n## IQuest-Coder-V1 Highlights\n\nIQuest-Coder-V1 is a new family of code large language models (LLMs) designed to advance autonomous software engineering and code intelligence. Built on the innovative code-flow multi-stage training paradigm, IQuest-Coder-V1 captures the dynamic evolution of software logic, delivering state-of-the-art performance across critical dimensions:\n\n- **State-of-the-Art Performance**: Achieves leading results on SWE-Bench Verified (76.2%), BigCodeBench (49.9%), LiveCodeBench v6 (81.1%), and other major coding benchmarks, surpassing competitive models across agentic software engineering, competitive programming, and complex tool use.\n- **Code-Flow Training Paradigm**: Moving beyond static code representations, our models learn from repository evolution patterns, commit transitions, and dynamic code transformations to understand real-world software development processes.\n- **Dual Specialization Paths**: Bifurcated post-training delivers two specialized variantsâ€”Thinking models (utilizing reasoning-driven RL for complex problem-solving) and Instruct models (optimized for general coding assistance and instruction-following).\n- **Efficient Architecture**: The IQuest-Coder-V1-Loop variant introduces a recurrent mechanism that optimizes the trade-off between model capacity and deployment footprint.\n- **Native Long Context**: All models natively support up to 128K tokens without requiring additional scaling techniques.\n\n## Model Overview\n\nThe IQuest-Coder-V1 series includes models ranging from 7B to 40B parameters, with both standard and Loop variants:\n\n| Model | Parameters | Layers | Hidden Size | Attention Heads (Q/KV) | Context Length |\n|-------|------------|--------|-------------|------------------------|----------------|\n| IQuest-Coder-V1-7B-Instruct | 7B | 14 | 5120 | 40/8 | 128K |\n| IQuest-Coder-V1-7B-Thinking | 7B | 14 | 5120 | 40/8 | 128K |\n| IQuest-Coder-V1-14B-Instruct | 14B | 28 | 5120 | 40/8 | 128K |\n| IQuest-Coder-V1-14B-Thinking | 14B | 28 | 5120 | 40/8 | 128K |\n| IQuest-Coder-V1-40B-Instruct | 40B | 80 | 5120 | 40/8 | 128K |\n| IQuest-Coder-V1-40B-Thinking | 40B | 80 | 5120 | 40/8 | 128K |\n| IQuest-Coder-V1-40B-Loop-Instruct | 40B | 80 (2 iterations) | 5120 | 40/8 | 128K |\n| IQuest-Coder-V1-40B-Loop-Thinking | 40B | 80 (2 iterations) | 5120 | 40/8 | 128K |\n\n**Architecture Features:**\n\n- Grouped Query Attention (GQA) for efficient inference\n- Native 128K context length support\n- Vocabulary size: 76,800 tokens\n- Loop variants use recurrent transformer design with shared parameters across two iterations\n\nFor more details, please refer to our Technical Report, GitHub.\n\n## Quickstart\n\nIQuest-Coder-V1 uses custom modeling code via Hugging Face's auto_map feature. We recommend using transformers>=4.52.4.\n\n### Basic Usage with Transformers\n\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_name = \"IQuest/IQuest-Coder-V1-40B-Instruct\"\n\n# Load the tokenizer and model\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype=\"auto\",\n    device_map=\"auto\"\n)\n\n# Prepare the input\nprompt = \"Write a Python function to calculate the Fibonacci sequence using dynamic programming.\"\nmessages = [\n    {\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n\n# Generate response\ngenerated_ids = model.generate(\n    **model_inputs,\n    max_new_tokens=8192\n)\ngenerated_ids = generated_ids[0][len(model_inputs.input_ids[0]):]\nresponse = tokenizer.decode(generated_ids, skip_special_tokens=True)\n\nprint(response)\n```\n\n### Using Thinking Models\n\nFor complex reasoning tasks, use the Thinking variant:\n\n```python\nmodel_name = \"IQuestLab/IQuest-Coder-V1-40B-Thinking\"\n\n# The Thinking model includes explicit reasoning traces\n# Use similar code as above, but expect longer, more detailed responses\n# with step-by-step problem decomposition\n```\n\n### Dep",
      "default_branch": "main"
    },
    "fetched_at": "2026-01-25T03:07:53.186302"
  }
]