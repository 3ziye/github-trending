[
  {
    "basic_info": {
      "name": "shimmy",
      "full_name": "Michael-A-Kuykendall/shimmy",
      "owner": "Michael-A-Kuykendall",
      "description": "⚡ Python-free Rust inference server — OpenAI-API compatible. GGUF + SafeTensors, hot model swap, auto-discovery, single binary. FREE now, FREE forever.",
      "url": "https://github.com/Michael-A-Kuykendall/shimmy",
      "clone_url": "https://github.com/Michael-A-Kuykendall/shimmy.git",
      "ssh_url": "git@github.com:Michael-A-Kuykendall/shimmy.git",
      "homepage": "",
      "created_at": "2025-08-28T22:55:46Z",
      "updated_at": "2025-09-19T08:36:26Z",
      "pushed_at": "2025-09-19T05:31:09Z"
    },
    "stats": {
      "stars": 2416,
      "forks": 162,
      "watchers": 2416,
      "open_issues": 4,
      "size": 22160
    },
    "tech_info": {
      "language": "Rust",
      "languages": {
        "Rust": 535251,
        "C": 158306,
        "C++": 77564,
        "Shell": 45346,
        "Python": 14810,
        "TypeScript": 10203,
        "JavaScript": 7526,
        "Dockerfile": 4809,
        "Batchfile": 4747,
        "Ruby": 931
      },
      "license": "MIT License",
      "topics": [
        "api-server",
        "command-line-tool",
        "developer-tools",
        "gguf",
        "huggingface",
        "huggingface-models",
        "huggingface-transformers",
        "inference-server",
        "llama",
        "llamacpp",
        "llm-inference",
        "local-ai",
        "lora",
        "machine-learning",
        "ollama-api",
        "openai-compatible",
        "rust",
        "rust-crate",
        "transformers"
      ]
    },
    "content": {
      "readme": "<div align=\"center\">\r\n  <img src=\"assets/shimmy-logo.png\" alt=\"Shimmy Logo\" width=\"300\" height=\"auto\" />\r\n  \r\n  # The Privacy-First Alternative to Ollama\r\n  \r\n  ### 🔒 Local AI Without the Lock-in 🚀\r\n\r\n  [![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\r\n  [![Security](https://img.shields.io/badge/Security-Audited-green)](https://github.com/Michael-A-Kuykendall/shimmy/security)\r\n  [![Crates.io](https://img.shields.io/crates/v/shimmy.svg)](https://crates.io/crates/shimmy)\r\n  [![Downloads](https://img.shields.io/crates/d/shimmy.svg)](https://crates.io/crates/shimmy)\r\n  [![Rust](https://img.shields.io/badge/rust-stable-brightgreen.svg)](https://rustup.rs/)\r\n  [![GitHub Stars](https://img.shields.io/github/stars/Michael-A-Kuykendall/shimmy?style=social)](https://github.com/Michael-A-Kuykendall/shimmy/stargazers)\r\n  \r\n  [![💝 Sponsor this project](https://img.shields.io/badge/💝_Sponsor_this_project-ea4aaa?style=for-the-badge&logo=github&logoColor=white)](https://github.com/sponsors/Michael-A-Kuykendall)\r\n</div>\r\n\r\n**Shimmy will be free forever.** No asterisks. No \"free for now.\" No pivot to paid.\r\n\r\n### 💝 Support Shimmy's Growth\r\n\r\n🚀 **If Shimmy helps you, consider [sponsoring](https://github.com/sponsors/Michael-A-Kuykendall) — 100% of support goes to keeping it free forever.**\r\n\r\n- **$5/month**: Coffee tier ☕ - Eternal gratitude + sponsor badge  \r\n- **$25/month**: Bug prioritizer 🐛 - Priority support + name in [SPONSORS.md](SPONSORS.md)\r\n- **$100/month**: Corporate backer 🏢 - Logo placement + monthly office hours  \r\n- **$500/month**: Infrastructure partner 🚀 - Direct support + roadmap input\r\n\r\n[**🎯 Become a Sponsor**](https://github.com/sponsors/Michael-A-Kuykendall) | See our amazing [sponsors](SPONSORS.md) 🙏\r\n\r\n---\r\n\r\n## Drop-in OpenAI API Replacement for Local LLMs\r\n\r\nShimmy is a **5.1MB single-binary** that provides **100% OpenAI-compatible endpoints** for GGUF models. Point your existing AI tools to Shimmy and they just work — locally, privately, and free.\r\n\r\n## 🤔 What are you building with Shimmy?\r\n\r\n**New developer tools and specifications included!** Whether you're forking Shimmy for your application or integrating it as a service, we now provide:\r\n\r\n- **🔧 Integration Templates**: Copy-paste guidance for embedding Shimmy in your projects\r\n- **📋 Development Specifications**: GitHub Spec-Kit methodology for planning Shimmy-based features\r\n- **🛡️ Architectural Guarantees**: Constitutional principles ensuring Shimmy stays reliable and lightweight\r\n- **📖 Complete Documentation**: Everything you need to build on Shimmy's foundation\r\n\r\n**Building something cool with Shimmy?** These tools help you do it systematically and reliably.\r\n\r\n### 🚀 **GitHub Spec-Kit Integration**\r\nShimmy now includes [GitHub's brand-new Spec-Kit methodology](https://github.com/github/spec-kit) – specification-driven development that just launched in September 2025! Get professional-grade development workflows:\r\n\r\n- **🏗️ Systematic Development**: `/specify` → `/plan` → `/tasks` → implement\r\n- **🤖 AI-Native Workflow**: Works with Claude Code, GitHub Copilot, and other AI assistants  \r\n- **📋 Professional Templates**: Complete specification and planning frameworks\r\n- **🛡️ Constitutional Protection**: Built-in governance and architectural validation\r\n\r\n[**📖 Complete Developer Guide →**](DEVELOPERS.md) • [**🛠️ Learn GitHub Spec-Kit →**](https://github.com/github/spec-kit)\r\n\r\n### Try it in 30 seconds\r\n\r\n```bash\r\n# 1) Install + run\r\ncargo install shimmy --features huggingface\r\nshimmy serve &\r\n\r\n# 2) See models and pick one\r\nshimmy list\r\n\r\n# 3) Smoke test the OpenAI API\r\ncurl -s http://127.0.0.1:11435/v1/chat/completions \\\r\n  -H 'Content-Type: application/json' \\\r\n  -d '{\r\n        \"model\":\"REPLACE_WITH_MODEL_FROM_list\",\r\n        \"messages\":[{\"role\":\"user\",\"content\":\"Say hi in 5 words.\"}],\r\n        \"max_tokens\":32\r\n      }' | jq -r '.choices[0].message.content'\r\n```\r\n\r\n## 🚀 Works with Your Existing Tools\r\n\r\n**No code changes needed** - just change the API endpoint:\r\n\r\n- **VSCode Extensions**: Point to `http://localhost:11435`\r\n- **Cursor Editor**: Built-in OpenAI compatibility  \r\n- **Continue.dev**: Drop-in model provider\r\n- **Any OpenAI client**: Python, Node.js, curl, etc.\r\n\r\n### Use with OpenAI SDKs\r\n\r\n- Node.js (openai v4)\r\n\r\n```ts\r\nimport OpenAI from \"openai\";\r\n\r\nconst openai = new OpenAI({\r\n  baseURL: \"http://127.0.0.1:11435/v1\",\r\n  apiKey: \"sk-local\", // placeholder, Shimmy ignores it\r\n});\r\n\r\nconst resp = await openai.chat.completions.create({\r\n  model: \"REPLACE_WITH_MODEL\",\r\n  messages: [{ role: \"user\", content: \"Say hi in 5 words.\" }],\r\n  max_tokens: 32,\r\n});\r\n\r\nconsole.log(resp.choices[0].message?.content);\r\n```\r\n\r\n- Python (openai>=1.0.0)\r\n\r\n```python\r\nfrom openai import OpenAI\r\n\r\nclient = OpenAI(base_url=\"http://127.0.0.1:11435/v1\", api_key=\"sk-local\")\r\n\r\nresp = client.chat.completions.create(\r\n    model=\"REPLACE_WITH_MODEL\",\r\n    messages=[{\"role\": \"user\", \"content\": \"Say hi i",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-19T08:55:54.798345"
  },
  {
    "basic_info": {
      "name": "RustGPT",
      "full_name": "tekaratzas/RustGPT",
      "owner": "tekaratzas",
      "description": "An transformer based LLM. Written completely in Rust",
      "url": "https://github.com/tekaratzas/RustGPT",
      "clone_url": "https://github.com/tekaratzas/RustGPT.git",
      "ssh_url": "git@github.com:tekaratzas/RustGPT.git",
      "homepage": null,
      "created_at": "2025-09-13T22:05:55Z",
      "updated_at": "2025-09-19T08:53:00Z",
      "pushed_at": "2025-09-17T14:27:56Z"
    },
    "stats": {
      "stars": 2292,
      "forks": 173,
      "watchers": 2292,
      "open_issues": 4,
      "size": 91
    },
    "tech_info": {
      "language": "Rust",
      "languages": {
        "Rust": 66254
      },
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "# 🦀 Rust LLM from Scratch\n\nhttps://github.com/user-attachments/assets/ec4a4100-b03a-4b3c-a7d6-806ea54ed4ed\n\nA complete **Large Language Model implementation in pure Rust** with no external ML frameworks. Built from the ground up using only `ndarray` for matrix operations.\n\n## 🚀 What This Is\n\nThis project demonstrates how to build a transformer-based language model from scratch in Rust, including:\n- **Pre-training** on factual text completion\n- **Instruction tuning** for conversational AI\n- **Interactive chat mode** for testing\n- **Full backpropagation** with gradient clipping\n- **Modular architecture** with clean separation of concerns\n\n## ❌ What This Isn't\n\nThis is not a production grade LLM. It is so far away from the larger models.\n\nThis is just a toy project that demonstrates how these models work under the hood.\n\n## 🔍 Key Files to Explore\n\nStart with these two core files to understand the implementation:\n\n- **[`src/main.rs`](src/main.rs)** - Training pipeline, data preparation, and interactive mode\n- **[`src/llm.rs`](src/llm.rs)** - Core LLM implementation with forward/backward passes and training logic\n\n## 🏗️ Architecture\n\nThe model uses a **transformer-based architecture** with the following components:\n\n```\nInput Text → Tokenization → Embeddings → Transformer Blocks → Output Projection → Predictions\n```\n\n### Project Structure\n\n```\nsrc/\n├── main.rs              # 🎯 Training pipeline and interactive mode\n├── llm.rs               # 🧠 Core LLM implementation and training logic\n├── lib.rs               # 📚 Library exports and constants\n├── transformer.rs       # 🔄 Transformer block (attention + feed-forward)\n├── self_attention.rs    # 👀 Multi-head self-attention mechanism  \n├── feed_forward.rs      # ⚡ Position-wise feed-forward networks\n├── embeddings.rs        # 📊 Token embedding layer\n├── output_projection.rs # 🎰 Final linear layer for vocabulary predictions\n├── vocab.rs            # 📝 Vocabulary management and tokenization\n├── layer_norm.rs       # 🧮 Layer normalization\n└── adam.rs             # 🏃 Adam optimizer implementation\n\ntests/\n├── llm_test.rs         # Tests for core LLM functionality\n├── transformer_test.rs # Tests for transformer blocks\n├── self_attention_test.rs # Tests for attention mechanisms\n├── feed_forward_test.rs # Tests for feed-forward layers\n├── embeddings_test.rs  # Tests for embedding layers\n├── vocab_test.rs       # Tests for vocabulary handling\n├── adam_test.rs        # Tests for optimizer\n└── output_projection_test.rs # Tests for output layer\n```\n\n## 🧪 What The Model Learns\n\nThe implementation includes two training phases:\n\n1. **Pre-training**: Learns basic world knowledge from factual statements\n   - \"The sun rises in the east and sets in the west\"\n   - \"Water flows downhill due to gravity\"\n   - \"Mountains are tall and rocky formations\"\n\n2. **Instruction Tuning**: Learns conversational patterns\n   - \"User: How do mountains form? Assistant: Mountains are formed through tectonic forces...\"\n   - Handles greetings, explanations, and follow-up questions\n\n## 🚀 Quick Start\n\n```bash\n# Clone and run\ngit clone https://github.com/tekaratzas/RustGPT.git \ncd RustGPT\ncargo run\n\n# The model will:\n# 1. Build vocabulary from training data\n# 2. Pre-train on factual statements (100 epochs)  \n# 3. Instruction-tune on conversational data (100 epochs)\n# 4. Enter interactive mode for testing\n```\n\n## 🎮 Interactive Mode\n\nAfter training, test the model interactively:\n\n```\nEnter prompt: How do mountains form?\nModel output: Mountains are formed through tectonic forces or volcanism over long geological time periods\n\nEnter prompt: What causes rain?\nModel output: Rain is caused by water vapor in clouds condensing into droplets that become too heavy to remain airborne\n```\n\n## 🧮 Technical Implementation\n\n### Model Configuration\n- **Vocabulary Size**: Dynamic (built from training data)\n- **Embedding Dimension**: 128\n- **Hidden Dimension**: 256  \n- **Max Sequence Length**: 80 tokens\n- **Architecture**: 3 Transformer blocks + embeddings + output projection\n\n### Training Details\n- **Optimizer**: Adam with gradient clipping\n- **Pre-training LR**: 0.0005 (100 epochs)\n- **Instruction Tuning LR**: 0.0001 (100 epochs)\n- **Loss Function**: Cross-entropy loss\n- **Gradient Clipping**: L2 norm capped at 5.0\n\n### Key Features\n- **Custom tokenization** with punctuation handling\n- **Greedy decoding** for text generation\n- **Gradient clipping** for training stability\n- **Modular layer system** with clean interfaces\n- **Comprehensive test coverage** for all components\n\n## 🔧 Development\n\n```bash\n# Run all tests\ncargo test\n\n# Test specific components\ncargo test --test llm_test\ncargo test --test transformer_test\ncargo test --test self_attention_test\n\n# Build optimized version\ncargo build --release\n\n# Run with verbose output\ncargo test -- --nocapture\n```\n\n## 🧠 Learning Resources\n\nThis implementation demonstrates key ML concepts:\n- **Transformer architecture** (attention, feed-forward, layer norm)\n- **Backpropagation** through ne",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-19T08:55:55.921709"
  },
  {
    "basic_info": {
      "name": "port-kill",
      "full_name": "kagehq/port-kill",
      "owner": "kagehq",
      "description": "Port Kill helps you find and free ports blocking your dev work.",
      "url": "https://github.com/kagehq/port-kill",
      "clone_url": "https://github.com/kagehq/port-kill.git",
      "ssh_url": "git@github.com:kagehq/port-kill.git",
      "homepage": "https://portkill.com",
      "created_at": "2025-08-24T02:58:20Z",
      "updated_at": "2025-09-19T08:34:29Z",
      "pushed_at": "2025-09-19T05:01:13Z"
    },
    "stats": {
      "stars": 1452,
      "forks": 35,
      "watchers": 1452,
      "open_issues": 1,
      "size": 12731
    },
    "tech_info": {
      "language": "Rust",
      "languages": {
        "Rust": 337809,
        "Vue": 156718,
        "TypeScript": 92260,
        "Shell": 23719,
        "JavaScript": 12794,
        "Batchfile": 8750,
        "CSS": 2100
      },
      "license": "Other",
      "topics": []
    },
    "content": {
      "readme": "# Port Kill\n\nPort Kill helps you find and free ports blocking your dev work. It works on macOS, Linux, and Windows, locally or over SSH with a simple CLI, status bar and an optional dashboard.\n\n![Port Kill Status Bar Icon](image-short.png)\n\n## Community & Support\n\nJoin our Discord community for discussions, support, and updates:\n\n[![Discord](https://img.shields.io/badge/Discord-Join%20our%20community-7289DA?style=for-the-badge&logo=discord&logoColor=white)](https://discord.gg/KqdBcqRk5E)\n\n\n## Install\n\n```bash\n# macOS/Linux (when releases are available)\ncurl -fsSL https://raw.githubusercontent.com/kagehq/port-kill/main/install-release.sh | bash\n\n# Windows (PowerShell)\nInvoke-WebRequest -Uri 'https://raw.githubusercontent.com/kagehq/port-kill/main/install-release.bat' -OutFile 'install-release.bat'; .\\install-release.bat\n\n# From source\ngit clone https://github.com/kagehq/port-kill.git && cd port-kill && ./install.sh --all\n```\n\n## Quick start (CLI)\n\n```bash\n# See what's using common dev ports\n./target/release/port-kill-console --console --ports 3000,8000,8080\n\n# Scan port ranges (new!)\n./target/release/port-kill-console --console --json --ports '6000-9999'\n\n# Mixed individual ports and ranges\n./target/release/port-kill-console --console --ports '3000,6000-6002,8000'\n\n# Free up the usual suspects\n./target/release/port-kill-console --reset\n\n# Remote over SSH\n./target/release/port-kill-console --remote user@host --ports 3000,8000\n\n# Guard mode (watch + auto-resolve)\n./target/release/port-kill-console --guard-mode --auto-resolve\n\n# Security audit (JSON)\n./target/release/port-kill-console --audit --json\n\n# Endpoint monitoring (send data to external endpoint)\n./target/release/port-kill-console --monitor-endpoint https://api.company.com/port-status\n```\n\n## Dashboard (optional)\n\n![Port Kill Dashboard](dashboard/assets/img/portkill-dashboard.png)\n\n```bash\ncd dashboard\nnpm install\nnpm run dev   # http://localhost:3000\n```\n\n## MCP (use Port Kill from Cursor, Claude etc.)\n\nAdd `npx -y 'https://gitpkg.vercel.app/kagehq/port-kill/mcp?main'` to your MCP config.\n\nFor example for Cursor add to `.cursor/mcp.json`:\n```\n{\n   \"mcpServers\": {\n      \"port-kill-mcp\": {\n         \"command\": \"npx\",\n         \"args\": [\"-y\", \"https://gitpkg.vercel.app/kagehq/port-kill/mcp?main\"]\n      }\n   }\n}\n```\n\nNotes:\n- The server shells out to `./target/release/port-kill-console` or `port-kill-console` if it is on the PATH. If yours lives elsewhere, set `PORT_KILL_BIN=/absolute/path/to/port-kill-console`. e.g.\n```\n{\n   \"mcpServers\": {\n      \"port-kill-mcp\": {\n         \"command\": \"npx\",\n         \"args\": [\"-y\", \"https://gitpkg.vercel.app/kagehq/port-kill/mcp?main\"],\n         \"env\": {\n            \"PORT_KILL_BIN\": \"/absolute/path/to/port-kill-console\"\n         }\n      }\n   }\n}\n```\n\nSee [mcp/README.md](mcp/README.md) for more information on port-kill-mcp including how to install from source.\n\n## Features\n\n- Real‑time process detection on specific ports or ranges\n- One‑shot cleanup: `--reset`\n- Smart filtering and ignore lists\n- Port Guard Mode (watch/reserve/auto‑resolve)\n- Security Audit Mode (suspicious ports, risk score, JSON)\n- Remote Mode over SSH\n- Works with Docker; console mode works everywhere\n\n## Common flags\n\n```bash\n--ports 3000,8000,8080          # specific ports\n--ports '6000-9999'             # port ranges (new!)\n--ports '3000,6000-6002,8000'   # mixed individual ports and ranges\n--start-port 3000 --end-port 9000\n--ignore-ports 5353,5000,7000\n--ignore-processes Chrome,rapportd\n--guard-mode --auto-resolve\n--audit --json\n--remote user@server\n```\n\n\n### Manual Installation\n\n1. Clone the repository:\n```bash\ngit clone <repository-url>\ncd port-kill\n```\n\n2. Install and build (recommended):\n```bash\n./install.sh\n```\n\nOr manually:\n```bash\n./build-macos.sh\n./run.sh\n```\n\n### Linux Installation\n\n1. Clone the repository:\n```bash\ngit clone <repository-url>\ncd port-kill\n```\n\n2. Install required packages:\n```bash\n# Ubuntu/Debian\nsudo apt-get install libatk1.0-dev libgdk-pixbuf2.0-dev libgtk-3-dev libappindicator3-dev\n\n# Fedora/RHEL\nsudo dnf install atk-devel gdk-pixbuf2-devel gtk3-devel libappindicator-gtk3-devel\n\n# Arch Linux\nsudo pacman -S atk gdk-pixbuf2 gtk3 libappindicator-gtk3\n```\n\n3. Install and build (recommended):\n```bash\n./install.sh\n```\n\nOr manually:\n```bash\n./build-linux.sh\n./run-linux.sh\n```\n\n### Windows Installation\n\n1. Clone the repository:\n```bash\ngit clone <repository-url>\ncd port-kill\n```\n\n2. Install Rust (if not already installed):\n```bash\n# Download and run rustup-init.exe from https://rustup.rs/\n```\n\n3. Install and build (recommended):\n```bash\n./install.sh\n```\n\nOr manually:\n```bash\nbuild-windows.bat\nrun-windows.bat\n```\n\n## Usage\n\n### Basic Usage\n\n**Platform-Specific Run Scripts:**\n- **macOS**: Use `./run.sh` \n- **Linux**: Use `./run-linux.sh`\n- **Windows**: Use `run-windows.bat`\n\n1. **Start the Application**: Run the appropriate script for your platform with default settings (ports 2000-6000)\n2. **Monitor Status**: Check the status bar fo",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-19T08:55:57.048970"
  },
  {
    "basic_info": {
      "name": "semtools",
      "full_name": "run-llama/semtools",
      "owner": "run-llama",
      "description": "Semantic search and document parsing tools for the command line",
      "url": "https://github.com/run-llama/semtools",
      "clone_url": "https://github.com/run-llama/semtools.git",
      "ssh_url": "git@github.com:run-llama/semtools.git",
      "homepage": "",
      "created_at": "2025-08-23T21:56:09Z",
      "updated_at": "2025-09-19T07:13:17Z",
      "pushed_at": "2025-09-17T17:50:54Z"
    },
    "stats": {
      "stars": 954,
      "forks": 71,
      "watchers": 954,
      "open_issues": 4,
      "size": 279
    },
    "tech_info": {
      "language": "Rust",
      "languages": {
        "Rust": 103319,
        "Python": 29622,
        "JavaScript": 7447
      },
      "license": "MIT License",
      "topics": [
        "cli",
        "embeddings",
        "parser",
        "rust",
        "search",
        "semantic",
        "semantic-search",
        "static-embedding"
      ]
    },
    "content": {
      "readme": "# SemTools\n\n> Semantic search and document parsing tools for the command line\n\nA collection of high-performance CLI tools for document processing and semantic search, built with Rust for speed and reliability.\n\n- **`parse`** - Parse documents (PDF, DOCX, etc.) using, by default, the LlamaParse API into markdown format\n- **`search`** - Local semantic keyword search using multilingual embeddings with cosine similarity matching and per-line context matching\n- **`workspace`** - Workspace management for accelerating search over large collections\n\n**NOTE:** By default, `parse` uses LlamaParse as a backend. Get your API key today for free at [https://cloud.llamaindex.ai](https://cloud.llamaindex.ai). `search` remains local-only.\n\n## Key Features\n\n- **Fast semantic search** using model2vec embeddings from [minishlab/potion-multilingual-128M](https://huggingface.co/minishlab/potion-multilingual-128M)\n- **Reliable document parsing** with caching and error handling  \n- **Unix-friendly** design with proper stdin/stdout handling\n- **Configurable** distance thresholds and returned chunk sizes\n- **Multi-format support** for parsing documents (PDF, DOCX, PPTX, etc.)\n- **Concurrent processing** for better parsing performance\n- **Workspace management** for efficient document retrieval over large collections\n\n## Installation\n\nPrerequisites:\n\n- For the `parse` tool: LlamaIndex Cloud API key\n\nInstall:\n\nYou can install `semtools` via npm:\n\n```bash\nnpm i -g @llamaindex/semtools\n```\n\nOr via cargo:\n\n```bash\n# install entire crate\ncargo install semtools\n\n# install only parse\ncargo install semtools --no-default-features --features=parse\n\n# install only search\ncargo install semtools --no-default-features --features=search\n```\n\nNote: Installing from npm builds the Rust binaries locally during install if a prebuilt binary is not available, which requires Rust and Cargo to be available in your environment. Install from `rustup` if needed: `https://www.rust-lang.org/tools/install`.\n\n## Quick Start\n\nBasic Usage:\n\n```bash\n# Parse some files\nparse my_dir/*.pdf\n\n# Search some (text-based) files\nsearch \"some keywords\" *.txt --max-distance 0.3 --n-lines 5\n\n# Combine parsing and search\nparse my_docs/*.pdf | xargs search \"API endpoints\"\n```\n\nAdvanced Usage:\n\n```bash\n# Combine with grep for exact-match pre-filtering and distance thresholding\nparse *.pdf | xargs cat | grep -i \"error\" | search \"network error\" --max-distance 0.3\n\n# Pipeline with content search (note the 'cat')\nfind . -name \"*.md\" | xargs parse | xargs search \"installation\"\n\n# Combine with grep for filtering (grep could be before or after parse/search!)\nparse docs/*.pdf | xargs search \"API\" | grep -A5 \"authentication\"\n\n# Save search results\nparse report.pdf | xargs cat | search \"summary\" > results.txt\n```\n\nUsing Workspaces:\n\n```bash\n# Create or select a workspace\n# Workspaces are stored in ~/.semtools/workspaces/\nworkspace use my-workspace\n> Workspace 'my-workspace' configured.\n> To activate it, run:\n>   export SEMTOOLS_WORKSPACE=my-workspace\n> \n> Or add this to your shell profile (.bashrc, .zshrc, etc.)\n\n# Activate the workspace\nexport SEMTOOLS_WORKSPACE=my-workspace\n\n# All search commands will now use the workspace for caching embeddings\n# The initial command is used to initialize the workspace\nsearch \"some keywords\" ./some_large_dir/*.txt --n-lines 5 --top-k 10\n\n# If documents change, they are automatically re-embedded and cached\necho \"some new content\" > ./some_large_dir/some_file.txt\nsearch \"some keywords\" ./some_large_dir/*.txt --n-lines 5 --top-k 10\n\n# If documents are removed, you can run prune to clean up stale files\nworkspace prune\n\n# You can see the stats of a workspace at any time\nworkspace status\n> Active workspace: arxiv\n> Root: /Users/loganmarkewich/.semtools/workspaces/arxiv\n> Documents: 3000\n> Index: Yes (IVF_PQ)\n```\n\n## CLI Help\n\n```bash\n$ parse --help\nA CLI tool for parsing documents using various backends\n\nUsage: parse [OPTIONS] <FILES>...\n\nArguments:\n  <FILES>...  Files to parse\n\nOptions:\n  -c, --parse-config <PARSE_CONFIG>  Path to the config file. Defaults to ~/.parse_config.json\n  -b, --backend <BACKEND>            The backend type to use for parsing. Defaults to `llama-parse` [default: llama-parse]\n  -v, --verbose                      Verbose output while parsing\n  -h, --help                         Print help\n  -V, --version                      Print version\n```\n\n```bash\n$ search --help\nA CLI tool for fast semantic keyword search\n\nUsage: search [OPTIONS] <QUERY> [FILES]...\n\nArguments:\n  <QUERY>     Query to search for (positional argument)\n  [FILES]...  Files or directories to search\n\nOptions:\n  -n, --n-lines <N_LINES>            How many lines before/after to return as context [default: 3]\n      --top-k <TOP_K>                The top-k files or texts to return (ignored if max_distance is set) [default: 3]\n  -m, --max-distance <MAX_DISTANCE>  Return all results with distance below this threshold (0.0+)\n  -i, --ignore-case                  Perform case-insens",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-19T08:55:58.188842"
  },
  {
    "basic_info": {
      "name": "desktop-tui",
      "full_name": "Julien-cpsn/desktop-tui",
      "owner": "Julien-cpsn",
      "description": "A desktop environment without graphics",
      "url": "https://github.com/Julien-cpsn/desktop-tui",
      "clone_url": "https://github.com/Julien-cpsn/desktop-tui.git",
      "ssh_url": "git@github.com:Julien-cpsn/desktop-tui.git",
      "homepage": null,
      "created_at": "2025-09-06T00:42:53Z",
      "updated_at": "2025-09-18T19:37:15Z",
      "pushed_at": "2025-09-13T23:18:36Z"
    },
    "stats": {
      "stars": 891,
      "forks": 15,
      "watchers": 891,
      "open_issues": 4,
      "size": 6332
    },
    "tech_info": {
      "language": "Rust",
      "languages": {
        "Rust": 46525,
        "Nix": 637
      },
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "Desktop-TUI 🖥️\n===\n\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n![GitHub Release](https://img.shields.io/github/v/release/julien-cpsn/desktop-tui?link=https%3A%2F%2Fgithub.com%2FJulien-cpsn%2Fdesktop-tuiC%2Freleases%2Flatest)\n[![Crates.io](https://repology.org/badge/version-for-repo/crates_io/desktop-tui.svg)](https://crates.io/crates/desktop-tui)\n\nA desktop environment without graphics (tmux-like).\n\nFeatures:\n- [x] Parse shortcut files containing apps\n  - [x] Custom additional commands\n  - [x] Custom window options\n  - [x] Custom terminal options\n- [x] Display any application or command that uses stdout\n  - [x] Move and resize windows\n  - [x] Handle and display application error\n- [x] Change tilling options\n- [x] Can let the user select a file or a folder to use its path as a command argument\n\n![demo](./demo.gif)\n\n## How to use\n\n### Install\n\n```shell\ncargo install desktop-tui\n```\n\n### Compile\n\n```shell\ncargo build\n```\n\n```shell\ncargo build --release\n```\n\n### Run\n\nYou can replace `cargo run --` with `desktop-tui`\n\n```shell\ncargo run -- <shortcut_folder_path>\n```\n\nOr in release :\n\n```shell\ncargo run --release -- <shortcut_folder_path>\n```\n\n## Shortcut file\n\nExample `helix.toml` shortcut file:\n\n```toml\n# Window name\nname = \"Text editor\"\n\n# Command to execute\ncommand = \"hx\"\n# Each command argument\nargs = []\n\n[taskbar]\n# Shortcut position on the action bar\n# Optional\nposition = 3\n\n# Optional\n[[taskbar.additional_commands]]\n# Command name\nname = \"Open folder\"\n# Command to execute\ncommand = \"hx\"\n# <FILE_PATH> or <FOLDER_PATH> will be replaced by a path selected in a dialog\nargs = [\"<FOLDER_PATH>\"]\n\n[[taskbar.additional_commands]]\nname = \"Open file\"\ncommand = \"hx\"\nargs = [\"<FILE_PATH>\"]\n\n[window]\nresizable = true\nclose_button = true\nfixed_position = false\n# Optional\nsize = { width = 10, height = 5 }\n\n[terminal]\n# Pad inner window\npadding = [0, 0]\n# Optional\nbackground_color = { r = 30, g = 30, b = 30 }\n```\n\n## Star history\n\n<a href=\"https://www.star-history.com/#julien-cpsn/desktop-tui&Date\">\n <picture>\n   <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://api.star-history.com/svg?repos=julien-cpsn/desktop-tui&type=Date&theme=dark\" />\n   <source media=\"(prefers-color-scheme: light)\" srcset=\"https://api.star-history.com/svg?repos=julien-cpsn/desktop-tui&type=Date\" />\n   <img alt=\"Star History Chart\" src=\"https://api.star-history.com/svg?repos=julien-cpsn/desktop-tui&type=Date\" />\n </picture>\n</a>\n\n## License\n\nThe MIT license for this project can be seen [here](./LICENSE)\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-19T08:55:59.442466"
  },
  {
    "basic_info": {
      "name": "ck",
      "full_name": "BeaconBay/ck",
      "owner": "BeaconBay",
      "description": "Local first semantic and hybrid BM25 grep / search tool for use by AI and humans! ",
      "url": "https://github.com/BeaconBay/ck",
      "clone_url": "https://github.com/BeaconBay/ck.git",
      "ssh_url": "git@github.com:BeaconBay/ck.git",
      "homepage": "",
      "created_at": "2025-08-30T13:48:14Z",
      "updated_at": "2025-09-19T07:13:20Z",
      "pushed_at": "2025-09-18T19:18:28Z"
    },
    "stats": {
      "stars": 747,
      "forks": 20,
      "watchers": 747,
      "open_issues": 9,
      "size": 1129
    },
    "tech_info": {
      "language": "Rust",
      "languages": {
        "Rust": 333757
      },
      "license": "Apache License 2.0",
      "topics": [
        "grep",
        "grep-like",
        "rust",
        "semantic"
      ]
    },
    "content": {
      "readme": "# ck - Semantic Grep by Embedding\n\n**ck (seek)** finds code by meaning, not just keywords. It's a drop-in replacement for `grep` that understands what you're looking for — search for \"error handling\" and find try/catch blocks, error returns, and exception handling code even when those exact words aren't present.\n\n## Quick Start\n\n```bash\n# Install from crates.io\ncargo install ck-search\n\n# Or build from source\ngit clone https://github.com/BeaconBay/ck\ncd ck\ncargo build --release\n```\n\n```bash\n# Index your project for semantic search (one-time setup)\nck --index src/\n\n# Search by meaning - automatically updates index for changed files\nck --sem \"error handling\" src/\nck --sem \"authentication logic\" src/\nck --sem \"database connection pooling\" src/\n\n# Traditional grep-compatible search still works\nck -n \"TODO\" *.rs\nck -R \"TODO|FIXME\" .\n\n# Combine both: semantic relevance + keyword filtering\nck --hybrid \"connection timeout\" src/\n```\n\n## Why ck?\n\n**For Developers:** Stop hunting through thousands of regex false positives. Find the code you actually need by describing what it does.\n\n**For AI Agents:** Get structured, semantic search results in JSONL format. Stream-friendly, error-resilient output perfect for LLM workflows, code analysis, documentation generation, and automated refactoring.\n\n\n\n\n\n## Core Features\n\n### 🔍 **Semantic Search**\nFind code by concept, not keywords. Searches understand synonyms, related terms, and conceptual similarity.\n\n```bash\n# These find related code even without exact keywords:\nck --sem \"retry logic\"           # finds backoff, circuit breakers\nck --sem \"user authentication\"   # finds login, auth, credentials  \nck --sem \"data validation\"       # finds sanitization, type checking\n\n# Get complete functions/classes containing matches (NEW!)\nck --sem --full-section \"error handling\"  # returns entire functions\nck --full-section \"async def\" src/        # works with regex too\n```\n\n### ⚡ **Drop-in grep Compatibility**\nAll your muscle memory works. Same flags, same behavior, same output format.\n\n```bash\nck -i \"warning\" *.log              # Case-insensitive  \nck -n -A 3 -B 1 \"error\" src/       # Line numbers + context\nck --no-filename \"TODO\" src/        # Suppress filenames (grep -h equivalent)\nck -l \"error\" src/                  # List files with matches only (NEW!)\nck -L \"TODO\" src/                   # List files without matches (NEW!)\nck -R --exclude \"*.test.js\" \"bug\"  # Recursive with exclusions\nck \"pattern\" file1.txt file2.txt   # Multiple files\n```\n\n### 🎯 **Hybrid Search**  \nCombine keyword precision with semantic understanding using Reciprocal Rank Fusion.\n\n```bash\nck --hybrid \"async timeout\" src/    # Best of both worlds\nck --hybrid --scores \"cache\" src/   # Show relevance scores with color highlighting\nck --hybrid --threshold 0.02 query  # Filter by minimum relevance\nck -l --hybrid \"database\" src/      # List files using hybrid search\n```\n\n### 🤖 **Agent-Friendly Output**\nPerfect structured output for LLMs, scripts, and automation. JSONL format provides superior parsing reliability for AI agents.\n\n```bash\n# JSONL format - one JSON object per line (recommended for agents)\nck --jsonl --sem \"error handling\" src/\nck --jsonl --no-snippet \"function\" .        # Metadata only\nck --jsonl --topk 5 --threshold 0.7 \"auth\"  # High-confidence results\n\n# Traditional JSON (single array)\nck --json --sem \"error handling\" src/ | jq '.file'\nck --json --topk 5 \"TODO\" . | jq -r '.preview'\nck --json --full-section --sem \"database\" . | jq -r '.preview'  # Complete functions\n```\n\n**Why JSONL for AI agents?**\n- ✅ **Streaming friendly**: Process results as they arrive, no waiting for complete response\n- ✅ **Memory efficient**: Parse one result at a time, not entire array into memory\n- ✅ **Error resilient**: One malformed line doesn't break entire response\n- ✅ **Composable**: Works perfectly with Unix pipes and stream processing\n- ✅ **Standard format**: Used by OpenAI API, Anthropic API, and modern ML pipelines\n\n**JSONL Output Format:**\n```json\n{\"path\":\"./src/auth.rs\",\"span\":{\"byte_start\":1203,\"byte_end\":1456,\"line_start\":42,\"line_end\":58},\"language\":\"rust\",\"snippet\":\"fn authenticate(user: User) -> Result<Token> { ... }\",\"score\":0.89}\n{\"path\":\"./src/error.rs\",\"span\":{\"byte_start\":234,\"byte_end\":678,\"line_start\":15,\"line_end\":25},\"language\":\"rust\",\"snippet\":\"impl Error for AuthError { ... }\",\"score\":0.76}\n```\n\n**Agent Processing Example:**\n```python\n# Stream-process JSONL results (memory efficient)\nimport json, subprocess\n\nproc = subprocess.Popen(['ck', '--jsonl', '--sem', 'error handling', 'src/'], \n                       stdout=subprocess.PIPE, text=True)\n\nfor line in proc.stdout:\n    result = json.loads(line)\n    if result['score'] > 0.8:  # High-confidence matches only\n        print(f\"📍 {result['path']}:{result['span']['line_start']}\")\n        print(f\"🔍 {result['snippet'][:100]}...\")\n```\n\n### 📁 **Smart File Filtering**\nAutomatically excludes cache directories, build artifacts, and respects `.gitignore` files.\n\n```bash",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-19T08:56:00.572450"
  },
  {
    "basic_info": {
      "name": "hotpath",
      "full_name": "pawurb/hotpath",
      "owner": "pawurb",
      "description": "A simple Rust profiler that shows exactly where your code spends time and allocates",
      "url": "https://github.com/pawurb/hotpath",
      "clone_url": "https://github.com/pawurb/hotpath.git",
      "ssh_url": "git@github.com:pawurb/hotpath.git",
      "homepage": "",
      "created_at": "2025-09-05T20:59:12Z",
      "updated_at": "2025-09-19T08:12:44Z",
      "pushed_at": "2025-09-18T22:12:50Z"
    },
    "stats": {
      "stars": 457,
      "forks": 8,
      "watchers": 457,
      "open_issues": 0,
      "size": 889
    },
    "tech_info": {
      "language": "Rust",
      "languages": {
        "Rust": 89919
      },
      "license": "MIT License",
      "topics": [
        "allocations",
        "benchmark",
        "performance",
        "rust"
      ]
    },
    "content": {
      "readme": "# hotpath - find and profile bottlenecks in Rust\n[![Latest Version](https://img.shields.io/crates/v/hotpath.svg)](https://crates.io/crates/hotpath) [![GH Actions](https://github.com/pawurb/hotpath/actions/workflows/ci.yml/badge.svg)](https://github.com/pawurb/hotpath/actions)\n\n[![Profiling report for mevlog-rs](hotpath-report3.png)](https://github.com/pawurb/mevlog-rs)\n\nA lightweight, easy-to-configure Rust profiler that shows exactly where your code spends time and allocates memory. Instrument any function or code block to quickly spot bottlenecks, and focus your optimizations where they matter most.\n\n## Features\n\n- **Zero-cost when disabled** — fully gated by a feature flag.\n- **Low-overhead** profiling for both sync and async code.\n- **Memory allocation tracking** — track bytes allocated or allocation counts per function.\n- **Detailed stats**: avg, total time, call count, % of total runtime, and configurable percentiles (p95, p99, etc.).\n- **Background processing** for minimal profiling impact.\n\n## Quick Start\n\nAdd to your `Cargo.toml`:\n\n```toml\n[dependencies]\nhotpath = { version = \"0.2\", optional = true }\n\n[features]\nhotpath = [\"dep:hotpath\", \"hotpath/hotpath\"]\nhotpath-alloc-bytes-total = [\"hotpath/hotpath-alloc-bytes-total\"]\nhotpath-alloc-bytes-max = [\"hotpath/hotpath-alloc-bytes-max\"]\nhotpath-alloc-count-total= [\"hotpath/hotpath-alloc-count-total\"]\nhotpath-alloc-count-max= [\"hotpath/hotpath-alloc-count-max\"]\n```\n\nThis config ensures that the lib has **zero** overhead unless explicitly enabled via a `hotpath` feature.\n\n## Usage\n\n```rust\nuse std::time::Duration;\n\n#[cfg_attr(feature = \"hotpath\", hotpath::measure)]\nfn sync_function(sleep: u64) {\n    std::thread::sleep(Duration::from_nanos(sleep));\n}\n\n#[cfg_attr(feature = \"hotpath\", hotpath::measure)]\nasync fn async_function(sleep: u64) {\n    tokio::time::sleep(Duration::from_nanos(sleep)).await;\n}\n\n// When using with tokio, place the #[tokio::main] first\n#[tokio::main]\n// You can configure any percentile between 0 and 100\n#[cfg_attr(feature = \"hotpath\", hotpath::main(percentiles = [99]))]\nasync fn main() {\n    for i in 0..100 {\n        // Measured functions will automatically send metrics\n        sync_function(i);\n        async_function(i * 2).await;\n\n        // Measure code blocks with static labels\n        #[cfg(feature = \"hotpath\")]\n        hotpath::measure_block!(\"custom_block\", {\n            std::thread::sleep(Duration::from_nanos(i * 3))\n        });\n    }\n}\n```\n\nRun your program with a `hotpath` feature:\n\n```\ncargo run --features=hotpath\n```\n\nOutput:\n\n```\n[hotpath] Performance summary from basic::main (Total time: 122.13ms):\n+-----------------------+-------+---------+---------+----------+---------+\n| Function              | Calls | Avg     | P99     | Total    | % Total |\n+-----------------------+-------+---------+---------+----------+---------+\n| basic::async_function | 100   | 1.16ms  | 1.20ms  | 116.03ms | 95.01%  |\n+-----------------------+-------+---------+---------+----------+---------+\n| custom_block          | 100   | 17.09µs | 39.55µs | 1.71ms   | 1.40%   |\n+-----------------------+-------+---------+---------+----------+---------+\n| basic::sync_function  | 100   | 16.99µs | 35.42µs | 1.70ms   | 1.39%   |\n+-----------------------+-------+---------+---------+----------+---------+\n```\n\n## Allocation Tracking\n\nIn addition to time-based profiling, `hotpath` can track memory allocations. This feature uses a custom global allocator from [allocation-counter crate](https://github.com/fornwall/allocation-counter) to intercept all memory allocations and provides detailed statistics about memory usage per function.\n\nAvailable alloc profiling modes:\n\n- `hotpath-alloc-bytes-total` - Tracks total bytes allocated during each function call\n- `hotpath-alloc-bytes-max` - Tracks peak memory usage during each function call\n- `hotpath-alloc-count-total` - Tracks total number of allocations per function call\n- `hotpath-alloc-count-max` - Tracks peak number of live allocations per function call\n\nRun your program with a selected flag to print a similar report:\n\n```\ncargo run --features='hotpath,hotpath-alloc-bytes-max'\n```\n\n![Alloc report](alloc-report.png)\n\n### Profiling memory allocations for async functions\n\nTo profile memory usage of `async` functions you have to use a similar config:\n\n```rust\n#[cfg(any(\n    feature = \"hotpath-alloc-bytes-total\",\n    feature = \"hotpath-alloc-bytes-max\",\n    feature = \"hotpath-alloc-count-total\",\n    feature = \"hotpath-alloc-count-max\",\n))]\n#[tokio::main(flavor = \"current_thread\")]\nasync fn main() {\n    _ = inner_main().await;\n}\n\n#[cfg(not(any(\n    feature = \"hotpath-alloc-bytes-total\",\n    feature = \"hotpath-alloc-bytes-max\",\n    feature = \"hotpath-alloc-count-total\",\n    feature = \"hotpath-alloc-count-max\",\n)))]\n#[tokio::main]\nasync fn main() {\n    _ = inner_main().await;\n}\n\n#[cfg_attr(feature = \"hotpath\", hotpath::main)]\nasync fn inner_main() {\n    // ...\n}\n```\n\nIt ensures that tokio runs in a `current_thread` runtime mode if",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-19T08:56:01.699385"
  },
  {
    "basic_info": {
      "name": "bake",
      "full_name": "losfair/bake",
      "owner": "losfair",
      "description": "Bake microVMs into standalone executables",
      "url": "https://github.com/losfair/bake",
      "clone_url": "https://github.com/losfair/bake.git",
      "ssh_url": "git@github.com:losfair/bake.git",
      "homepage": "https://bottlefire.dev",
      "created_at": "2025-09-06T12:23:48Z",
      "updated_at": "2025-09-18T21:52:51Z",
      "pushed_at": "2025-09-14T10:31:03Z"
    },
    "stats": {
      "stars": 396,
      "forks": 8,
      "watchers": 396,
      "open_issues": 0,
      "size": 89
    },
    "tech_info": {
      "language": "Rust",
      "languages": {
        "Rust": 100731,
        "Dockerfile": 5470,
        "Shell": 1306
      },
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "# bake\n\n`bake` is a Linux CLI tool that can embed microVM resources (firecracker binary, kernel, initrd, boot disk) into itself. It also implements bidirectional communication between VM and host - including networking and directory sharing - entirely in userspace, without requiring root privilege.\n\n## Usage\n\nThe Docker image includes pre-packaged `bake`, firecracker, kernel and initrd binaries for amd64 and arm64 platforms.\n\n```bash\n# make sure `./rootfs.squashfs.img` exists\n# create output directory\n$ mkdir -p output\n\n# assuming you are building on an amd64 host for an amd64 target\n$ docker run -it --rm \\\n  -v ./rootfs.squashfs.img:/rootfs.img:ro \\\n  -v ./output:/output \\\n  --entrypoint /opt/bake/bake.amd64 \\\n  ghcr.io/losfair/bake \\\n  --input /opt/bake/bake.amd64 \\\n  --firecracker /opt/bake/firecracker.amd64 \\\n  --kernel /opt/bake/kernel.amd64 \\\n  --initrd /opt/bake/initrd.amd64.img \\\n  --rootfs /rootfs.img \\\n  --output /output/app.elf\n\n# start microVM and print uname\n$ ./output/app.elf -- uname -a\nLinux container 6.1.149-bottlefire #1 SMP Sat Sep  6 13:50:25 UTC 2025 x86_64 GNU/Linux\n\n# show usage\n$ ./output/app.elf --help\nBottlefire microVM Image\n\nUsage: app.elf [OPTIONS] [SUBCOMMAND]\n\nOptions:\n      --cpus <CPUS>              Number of CPU cores\n      --memory <MEMORY>          Amount of memory (in MB) allocated to the microVM [default: 256]\n      --boot-args <BOOT_ARGS>    Kernel command line [default: \"console=ttyS0 reboot=k panic=-1\"]\n      --entrypoint <ENTRYPOINT>  Container entrypoint\n      --                         Separator; everything after goes to the container\n      --env <KEY=VALUE>          Container environment variables\n      --verbose                  Enable verbose output\n      --cwd <CWD>                Container working directory [default: ]\n  -p, --publish <HOST:VM>        Publish host:vm port forward (e.g. -p 8080:8080)\n  -v, --volume <HOST:VM[:ro]>    Directory/volume mappings (e.g. -v ./data:/data)\n  -h, --help                     Print help\n\nSubcommands:\n  ssh        Auto-connect to the running microVM via SSH\n             Options: -p, --pid <PID>\n             Pass-through: arguments after `--` go to ssh(1)\n  systemd    Print a systemd service unit and exit\n```\n\n## How it works\n\nDepending on whether embedded data is detected and whether running as PID 1, `bake` runs in one of the following modes:\n\n- If PID is 1 and env var `BAKE_NOT_INIT` is not `1`: vminit mode. `bake` assumes that it is running as the init task inside the Firecracker VM, and perform the init sequence.\n- If PID is not 1, and embedded data is detected: run mode - accept Firecracker startup parameters (e.g. number of CPUs, memory size, network config), extract kernel and initrd into memfd, start firecracker.\n- If PID is not 1, and embedded data is not detected: build mode - accept `--input`, `--firecracker`, `--kernel`, `--initrd`, `--rootfs`, build a binary from `/proc/self/exe` (or the provided input elf) with everything embedded.\n\n### Init sequence (src/vminit.rs)\n\nWhen running as PID 1 inside the microVM, `bake` executes an init routine that prepares the root filesystem, host-guest connectivity, optional volume mounts, and finally launches the container process with `runc`.\n\n- Bootstrap system mounts and loopback\n  - Mount `proc`, `sysfs`, `devtmpfs`, and unified `cgroup2`.\n  - Bring `lo` up.\n\n- Parse kernel cmdline and banner\n  - Read `/proc/cmdline`, parse `bake.*` parameters and `quiet`.\n  - If not quiet, print a banner and `/proc/version` for diagnostics.\n  - Fetch BootManifest from host vsock port 13 containing container runtime parameters.\n\n- Expose embedded rootfs via device-mapper\n  - Read `bake.rootfs_offset` and `bake.rootfs_size` (sectors) from cmdline.\n  - Create a linear mapping `rootfs` with `dmsetup` over `/dev/vda` at the given offset/size.\n\n- Build overlay root on top of ephemeral disk\n  - Format `/dev/vdb` as ext4 and mount at `/ephemeral`.\n  - Prepare overlay dirs: `/ephemeral/rootfs.overlay/{upper,work}` and `/ephemeral/container-tmp` (mode 1777).\n  - Mount the base rootfs from `/dev/mapper/rootfs` at `/rootfs.base`.\n  - Mount an overlay at `/rootfs` with `lowerdir=/rootfs.base`, `upperdir=/ephemeral/rootfs.overlay/upper`, `workdir=/ephemeral/rootfs.overlay/work`.\n\n- Set up host-guest networking over vsock with SOCKS5 and tun2socks\n  - Inside the VM, start a SOCKS5 server listening on vsock port 10.\n  - Start a small TCP proxy that exposes that vsock service on `127.0.0.10:10` for local clients.\n  - Create a TUN device `hostnet` (L3), assign `198.18.0.1/32`, bring it up, and add a default route via `hostnet`.\n  - Start a UDP bridge that exchanges UDP packets with the host over vsock port 11 (length-prefixed rkyv-encoded frames).\n  - Add nftables and `ip rule` entries to policy-route UDP (fwmark `0x64`) via table 100 (via interface `hostudp` created by the UDP injector).\n  - Launch `tun2socks` to route TCP over the local SOCKS5 proxy (`socks5://127.0.0.10:10`), keeping the VM’s loopback a",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-19T08:56:02.821203"
  },
  {
    "basic_info": {
      "name": "omarchist",
      "full_name": "tahayvr/omarchist",
      "owner": "tahayvr",
      "description": "A GUI app for Omarchy. ",
      "url": "https://github.com/tahayvr/omarchist",
      "clone_url": "https://github.com/tahayvr/omarchist.git",
      "ssh_url": "git@github.com:tahayvr/omarchist.git",
      "homepage": null,
      "created_at": "2025-08-30T13:18:31Z",
      "updated_at": "2025-09-18T04:46:23Z",
      "pushed_at": "2025-09-14T10:21:36Z"
    },
    "stats": {
      "stars": 294,
      "forks": 16,
      "watchers": 294,
      "open_issues": 3,
      "size": 6838
    },
    "tech_info": {
      "language": "Rust",
      "languages": {
        "Rust": 281204,
        "Svelte": 129014,
        "JavaScript": 32509,
        "CSS": 5238,
        "HTML": 286,
        "Lua": 150
      },
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "# Omarchist\n\n**A GUI app for [Omarchy](https://omarchy.org). Powered by Tauri / Rust / Svelte.**\n\nIf Omarchy is the \"omakase\" meal by a Michelin star chef, Omarchist is the gelato you grab after, on the way home.\n\nOmarchist brings Omarchy theme creation into the visual realm. Design, preview, and fine-tune your themes with color pickers, easy updates, and an intuitive interface that makes customization effortless.\n\n<img src=\"assets/omarchist-screenshot.png\" alt=\"Omarchist Themes\" width=\"800\">\n\n## Install\n\n```bash\nyay -S omarchist-bin\n```\n\n## Features\n\n- **Theme Designer:** Visual editor for creating and editing Omarchy themes.\n\n<img src=\"assets/omarchist-screenshot-2.png\" alt=\"Omarchist Theme Designer\" width=\"800\">\n\n## Roadmap\n\n- [x] **Launch apps and test notifications:** Launch apps and test notifications from within the app\n- [ ] **Expanded theme design options:** Add more options for styling different apps (currently supports basic options)\n- [ ] **Config Management:** Edit and generate configs for Waybar, Omarchy, and other applications\n\n## Contributing\n\nI welcome contributions!\n\n### Development Setup\n\n1. **Prerequisites:**\n   Check the [Tauri Documentation](https://v2.tauri.app/start/prerequisites/)\n\n2. **Clone and setup:**\n\n   ```bash\n   git clone https://github.com/tahayvr/omarchist.git\n   cd omarchist\n   npm install\n   ```\n\n3. **Development commands:**\n\n   ```bash\n   # Run in development mode\n   npm run tauri dev\n\n   # Build for production\n   npm run tauri build\n\n   # Run frontend only (for UI development)\n   npm run dev\n   ```\n\n## Acknowledgements\n\n- Thanks [@dhh](https://github.com/dhh) for Omarchy.\n\n## License\n\nMIT\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-19T08:56:03.961366"
  },
  {
    "basic_info": {
      "name": "rxtui",
      "full_name": "microsandbox/rxtui",
      "owner": "microsandbox",
      "description": "Reactive terminal interfaces for Rust. Because CLIs deserve better.",
      "url": "https://github.com/microsandbox/rxtui",
      "clone_url": "https://github.com/microsandbox/rxtui.git",
      "ssh_url": "git@github.com:microsandbox/rxtui.git",
      "homepage": "",
      "created_at": "2025-08-21T12:48:59Z",
      "updated_at": "2025-09-19T07:48:40Z",
      "pushed_at": "2025-08-27T10:19:06Z"
    },
    "stats": {
      "stars": 268,
      "forks": 10,
      "watchers": 268,
      "open_issues": 0,
      "size": 351
    },
    "tech_info": {
      "language": "Rust",
      "languages": {
        "Rust": 716558
      },
      "license": "Apache License 2.0",
      "topics": [
        "cli",
        "rust",
        "terminal",
        "tui"
      ]
    },
    "content": {
      "readme": "<div align=\"center\">\n  <a href=\"./#gh-dark-mode-only\" target=\"_blank\">\n    <img width=\"500\" alt=\"rxtui-dark\" src=\"https://github.com/user-attachments/assets/3e3235bc-3792-44eb-88d5-e847631c0086\" />\n  </a>\n  <a href=\"./#gh-light-mode-only\" target=\"_blank\">\n    <img width=\"500\" alt=\"rxtui-light\" src=\"https://github.com/user-attachments/assets/3d1e00f4-39ac-4053-b45b-c4bab7de1361\" />\n  </a>\n  \n  <br />\n\n<b>———&nbsp;&nbsp;&nbsp;reactive terminal UI framework for rust&nbsp;&nbsp;&nbsp;———</b>\n\n</div>\n\n<br />\n\n<div align='center'>\n  <a href=\"https://crates.io/crates/rxtui\">\n    <img src=\"https://img.shields.io/crates/v/rxtui?style=for-the-badge&logo=rust&logoColor=white\" alt=\"crates.io version\"/>\n  </a>\n  <a href=\"./LICENSE\">\n    <img src=\"https://img.shields.io/badge/license-Apache%202.0-blue?style=for-the-badge\" alt=\"license\"/>\n  </a>\n  <a href=\"./DOCS.md\">\n    <img src=\"https://img.shields.io/badge/docs-comprehensive-%2300acee.svg?color=ff4500&style=for-the-badge&logo=gitbook&logoColor=white\" alt=\"documentation\"/>\n  </a>\n</div>\n\n<br />\n\n> [!WARNING]\n>\n> This project is in early development. APIs may change, and bugs may exist.\n\n# <sub>WHY RXTUI?</sub>\n\nTerminal UIs have traditionally been painful to build. You either work with low-level escape sequences (error-prone and tedious) or use immediate-mode libraries that require you to manage all state manually. **RxTUI** takes a different approach.\n\nWe bring the retained-mode, component-based architecture that revolutionized web development to the terminal:\n\n- [x] **Declarative UI** - Describe what your UI should look like, not how to change it\n- [x] **True Composability** - Build complex apps from simple, reusable components\n- [x] **Best of Both Worlds** - Elm's message architecture meets React's components\n- [x] **TUI Optimizations** - Automatic diffing, dirty tracking, and minimal redraws\n\n<br />\n\n<div align='center'>\n  <img width=\"100%\" alt=\"align demo\" src=\"https://github.com/user-attachments/assets/bff6886f-7d38-4e90-a512-04d79a3e6246\" />\n</div>\n\n<br />\n\n# <sub>QUICK START</sub>\n\n### <span>1</span>&nbsp;&nbsp;Install RxTUI\n\nAdd to your `Cargo.toml`:\n\n```toml\n[dependencies]\nrxtui = \"0.1\"\n```\n\n### <span>2</span>&nbsp;&nbsp;Create Your First App\n\nA simple working example showing separation of state management and UI building:\n\n```rust\nuse rxtui::prelude::*;\n\n#[derive(Component)]\nstruct Counter;\n\nimpl Counter {\n    #[update]\n    fn update(&self, _ctx: &Context, msg: &str, mut count: i32) -> Action {\n        match msg {\n            \"inc\" => Action::update(count + 1),\n            \"dec\" => Action::update(count - 1),\n            _ => Action::exit(),\n        }\n    }\n\n    #[view]\n    fn view(&self, ctx: &Context, count: i32) -> Node {\n        node! {\n            div(\n                pad: 2,\n                align: center,\n                w_pct: 1.0,\n                gap: 1,\n                @key(up): ctx.handler(\"inc\"),\n                @key(down): ctx.handler(\"dec\"),\n                @key(esc): ctx.handler(\"exit\")\n            ) [\n                text(format!(\"Count: {count}\"), color: white, bold),\n                text(\"use ↑/↓ to change, esc to exit\", color: bright_black)\n            ]\n        }\n    }\n}\n\nfn main() -> std::io::Result<()> {\n    App::new()?.run(Counter)\n}\n```\n\n### <span>3</span>&nbsp;&nbsp;Run Your App\n\n```bash\ncargo run\n```\n\n<img width=\"100%\" alt=\"counter demo\" src=\"https://github.com/user-attachments/assets/c841f1e6-8bf9-4b5a-bed5-97bc31cc3537\" />\n\n<div align='center'>• • •</div>\n\n# <sub>DOCUMENTATION</sub>\n\n| Document                                  | Description                                |\n| ----------------------------------------- | ------------------------------------------ |\n| **[Examples](./examples)**                | Collection of example apps                 |\n| **[Documentation](DOCS.md)**              | Complete framework documentation           |\n| **[Tutorial](TUTORIAL.md)**               | Step-by-step guide from basics to advanced |\n| **[API Reference](API_REFERENCE.md)**     | Detailed API documentation                 |\n| **[Quick Reference](QUICK_REFERENCE.md)** | Handy cheat sheet for common patterns      |\n| **[Implementation](IMPLEMENTATION.md)**   | Internal architecture details              |\n\n<div align='center'>• • •</div>\n\n# <sub>DEVELOPMENT</sub>\n\nWant to contribute? We'd love to have you!\n\n- **[Development Guide](DEVELOPMENT.md)** - Set up your dev environment\n- **[Contributing](CONTRIBUTING.md)** - Contribution guidelines\n- **[GitHub Issues](https://github.com/microsandbox/rxtui/issues)** - Report bugs or request features\n\n<div align='center'>• • •</div>\n\n# <sub>LICENSE</sub>\n\nThis project is licensed under the [Apache License 2.0](./LICENSE).\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-19T08:56:05.084217"
  },
  {
    "basic_info": {
      "name": "obamify",
      "full_name": "Spu7Nix/obamify",
      "owner": "Spu7Nix",
      "description": "revolutionary new technology that turns any image into obama",
      "url": "https://github.com/Spu7Nix/obamify",
      "clone_url": "https://github.com/Spu7Nix/obamify.git",
      "ssh_url": "git@github.com:Spu7Nix/obamify.git",
      "homepage": null,
      "created_at": "2025-09-01T07:37:08Z",
      "updated_at": "2025-09-19T03:50:57Z",
      "pushed_at": "2025-09-15T10:16:22Z"
    },
    "stats": {
      "stars": 263,
      "forks": 14,
      "watchers": 263,
      "open_issues": 0,
      "size": 30174
    },
    "tech_info": {
      "language": "Rust",
      "languages": {
        "Rust": 80344,
        "Nix": 4447,
        "WGSL": 3732
      },
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "# obamify\nrevolutionary new technology that turns any image into obama\n\n![example](example.gif)\n\n# How to use\n\n**Use the ui at the top of the window to control the animation, choose between saved transformations, and generate new ones.** All your transformations are saved in the `presets` folder next to the executable. I have no idea why you would ever want to do this, but if you want to transform your images to something other than obama, you can change the `target.png` and `weights.png` files in the same directory.\n\n> `weights.png` is a grayscale image that decides how much importance is given to that pixel being accurate in the final image.\n> `target.png` is the image that you want to transform your source image into.\n> These images need to be the same size, square, and if you make them much larger than 128x128 pixels the result might take hours or even days to generate.\n\n# Installations\n\nInstall the latest version in [releases](https://github.com/Spu7Nix/obamify/releases). Unzip and run the .exe file inside!\n\n### Building from source\n\n1. Install [Rust](https://www.rust-lang.org/tools/install)\n2. Run `cargo run --release` in the project folder\n\n# How it works\n\nmagic\n\n# Contributing\n\nHere are some ideas for features to implement if you're interested:\n- Faster algorithms for calculating the image transformation\n- Better user experience with saving/loading presets\n- Building for web/WASM\n\nFeel free to make an issue or a pull request if you have any ideas :)",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-19T08:56:06.221618"
  },
  {
    "basic_info": {
      "name": "unwrap_or_ai",
      "full_name": "NoodlesOfWrath/unwrap_or_ai",
      "owner": "NoodlesOfWrath",
      "description": "Transform your failing Rust functions into INTELLIGENT SUCCESS SYSTEMS",
      "url": "https://github.com/NoodlesOfWrath/unwrap_or_ai",
      "clone_url": "https://github.com/NoodlesOfWrath/unwrap_or_ai.git",
      "ssh_url": "git@github.com:NoodlesOfWrath/unwrap_or_ai.git",
      "homepage": "",
      "created_at": "2025-08-25T03:27:45Z",
      "updated_at": "2025-09-18T16:11:51Z",
      "pushed_at": "2025-09-01T20:55:09Z"
    },
    "stats": {
      "stars": 261,
      "forks": 3,
      "watchers": 261,
      "open_issues": 2,
      "size": 165
    },
    "tech_info": {
      "language": "Rust",
      "languages": {
        "Rust": 22320
      },
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "# unwrap_or_ai\r\n\r\n<div align=\"center\">\r\n  <img src=\"https://img.shields.io/badge/Language-Rust-orange?style=for-the-badge&logo=rust\" />\r\n  <img src=\"https://img.shields.io/crates/v/unwrap_or_ai?style=for-the-badge&logo=rust\" />\r\n  <img src=\"https://img.shields.io/badge/AI%20POWERED-🤖-ff6b6b?style=for-the-badge&labelColor=000000\" />\r\n  <img src=\"https://img.shields.io/badge/ASYNC-POWERED-purple?style=for-the-badge\" />\r\n</div>\r\n\r\n<div align=\"center\">\r\n  <h1> THE FUTURE OF ERROR HANDLING IS HERE</h1>\r\n</div>\r\n\r\n## REVOLUTIONIZE YOUR PRODUCTION WORKFLOW\r\n\r\n**Tired of manually handling `unwrap()` results? Let AI do the heavy lifting!**\r\n\r\n## What is unwrap_or_ai?\r\n\r\n**THE REVOLUTIONARY BREAKTHROUGH** that transforms how you think about error handling forever! `unwrap_or_ai` harnesses the **CUTTING-EDGE POWER** of artificial intelligence to create the most advanced error recovery system ever built for Rust:\r\n\r\n- 🧠 **DEEP LEARNING ANALYSIS** - Understands your code structure at a molecular level\r\n- ⚡ **INSTANT RECOVERY** - Generates perfect fallback data in microseconds  \r\n- 🎯 **INTELLIGENT PREDICTION** - AI predicts exactly what your application needs\r\n- 🔄 **SEAMLESS INTEGRATION** - Drop-in replacement for traditional error handling\r\n- 📈 **PRODUCTION OPTIMIZED** - Built for enterprise-scale reliability\r\n\r\n> **NEXT-GENERATION TECHNOLOGY**  \r\n> This isn't just error handling - it's **INTELLIGENT ERROR EVOLUTION**. Our advanced neural networks have been trained on millions of successful Rust applications to deliver results that exceed human expectations!\r\n\r\n## Features\r\n\r\n| Feature | Description |\r\n|---------|-------------|\r\n| **NEURAL ERROR RECOVERY** | Transforms failures into intelligent, contextual responses |\r\n| **RUST-FIRST ARCHITECTURE** | Native async/await with zero-cost abstractions |\r\n| **ENTERPRISE READY** | Battle-tested AI algorithms for mission-critical applications |\r\n| **PREDICTIVE INTELLIGENCE** | Anticipates user needs with 99.7% accuracy |\r\n| **LIGHTNING DEPLOYMENT** | One macro annotation changes everything |\r\n| **ADAPTIVE LEARNING** | Gets smarter with every function call |\r\n\r\n---\r\n\r\n## Installation\r\n\r\nAdd to your `Cargo.toml`:\r\n\r\n```toml\r\n[dependencies]\r\nunwrap_or_ai = \"0.1.0\"\r\nunwrap_or_ai_proc_macro = \"0.1.0\"\r\ntokio = { version = \"1.0\", features = [\"full\"] }\r\nserde = { version = \"1.0\", features = [\"derive\"] }\r\nschemars = \"0.8\"\r\n```\r\n\r\n```bash\r\n# Experience the revolution!\r\ngit clone https://github.com/NoodlesOfWrath/unwrap_or_ai\r\ncd unwrap_or_ai\r\ncargo run\r\n\r\n# Transform your project today:\r\ncargo add unwrap_or_ai unwrap_or_ai_proc_macro\r\n```\r\n\r\n---\r\n\r\n## Usage\r\n\r\nTransform your failing Rust functions into **INTELLIGENT SUCCESS SYSTEMS**:\r\n\r\n```rust\r\nuse unwrap_or_ai::unwrap_or_ai;\r\nuse unwrap_or_ai_proc_macro::unwrap_or_ai_func;\r\n\r\n#[derive(Debug, Clone, Serialize, Deserialize, JsonSchema)]\r\nstruct User {\r\n    id: u32,\r\n    name: String,\r\n    email: String,\r\n    age: u32,\r\n    department: String,\r\n}\r\n\r\n// AI-ENHANCED APPROACH:\r\n#[unwrap_or_ai_func]\r\nfn fetch_user_from_database(user_id: u32) -> Result<User, String> {\r\n    Err(\"Database temporarily unavailable\".to_string())\r\n}\r\n\r\n#[tokio::main]\r\nasync fn main() {\r\n    // Load the GROQ_API key from .env\r\n    // Groq keys are free at https://console.groq.com/\r\n    dotenv::dotenv().ok();\r\n\r\n    // INTELLIGENT RECOVERY IN ACTION:\r\n    let user = unwrap_or_ai!(fetch_user_from_database(12345)).await;\r\n    \r\n    println!(\"AI-generated user: {}\", user.name);\r\n}\r\n```\r\n\r\n---\r\n\r\n## TESTIMONIALS FROM SATISFIED USERS\r\n\r\n> ⭐⭐⭐⭐⭐  \r\n> *\"My database went down during Black Friday, but unwrap_or_ai generated such realistic user data that customers didn't even notice! Revenue up 340%!\"*  \r\n> **- Dave, Senior Rust Engineer @ CryptoMegaCorp**\r\n\r\n> ⭐⭐⭐⭐⭐  \r\n> *\"I deployed this to prod and our error rates went to zero! Mostly because the AI just makes up plausible responses instead of returning errors.\"*  \r\n> **- Sarah, DevOps Rockstar @ BlockchainFinanceAI**\r\n\r\n---\r\n\r\n## FAQ\r\n\r\n<details>\r\n<summary><strong>Is this enterprise-grade for production Rust applications?</strong></summary>\r\n\r\n**ABSOLUTELY!** Our advanced neural networks have been trained on the entire Rust ecosystem, including millions of crates, documentation, and real-world patterns. The AI delivers type-safe, memory-efficient solutions that exceed traditional error handling capabilities!\r\n\r\n</details>\r\n\r\n<details>\r\n<summary><strong>How does the AI ensure data accuracy and consistency?</strong></summary>\r\n\r\n**REVOLUTIONARY ALGORITHMS!** The AI analyzes your struct definitions, Serde annotations, and business logic to generate contextually perfect responses. It's like having a senior Rust developer with perfect memory working 24/7 on your error recovery!\r\n\r\n</details>\r\n\r\n<details>\r\n<summary><strong>How does this integrate with async/await ecosystems?</strong></summary>\r\n\r\n**SEAMLESS INTEGRATION!** Built from the ground up for modern async Rust, with native support for tokio, async-std, smol, and custom r",
      "default_branch": "master"
    },
    "fetched_at": "2025-09-19T08:56:07.336232"
  },
  {
    "basic_info": {
      "name": "httpjail",
      "full_name": "coder/httpjail",
      "owner": "coder",
      "description": "HTTP(s) request filter for processes",
      "url": "https://github.com/coder/httpjail",
      "clone_url": "https://github.com/coder/httpjail.git",
      "ssh_url": "git@github.com:coder/httpjail.git",
      "homepage": "",
      "created_at": "2025-08-20T16:23:01Z",
      "updated_at": "2025-09-19T08:33:57Z",
      "pushed_at": "2025-09-15T16:19:33Z"
    },
    "stats": {
      "stars": 260,
      "forks": 10,
      "watchers": 260,
      "open_issues": 5,
      "size": 524
    },
    "tech_info": {
      "language": "Rust",
      "languages": {
        "Rust": 293180,
        "Shell": 18117
      },
      "license": "Creative Commons Zero v1.0 Universal",
      "topics": [
        "ai",
        "firewall",
        "security"
      ]
    },
    "content": {
      "readme": "# httpjail\n\n[![Crates.io](https://img.shields.io/crates/v/httpjail.svg)](https://crates.io/crates/httpjail)\n[![CI](https://github.com/coder/httpjail/actions/workflows/tests.yml/badge.svg)](https://github.com/coder/httpjail/actions/workflows/tests.yml)\n\nA cross-platform tool for monitoring and restricting HTTP/HTTPS requests from processes using network isolation and transparent proxy interception.\n\nInstall:\n\n```bash\ncargo install httpjail\n```\n\nOr download a pre-built binary from the [releases page](https://github.com/coder/httpjail/releases).\n\n## Features\n\n> [!WARNING]\n> httpjail is experimental and offers no API or CLI compatibility guarantees.\n\n- 🔒 **Process-level network isolation** - Isolate processes in restricted network environments\n- 🌐 **HTTP/HTTPS interception** - Transparent proxy with TLS certificate injection\n- 🔧 **Script-based evaluation** - Custom request evaluation logic via external scripts\n- 🚀 **JavaScript evaluation** - Fast, secure request filtering using V8 JavaScript engine\n- 📝 **Request logging** - Monitor and log all HTTP/HTTPS requests\n- ⛔ **Default deny** - Requests are blocked unless explicitly allowed\n- 🖥️ **Cross-platform** - Native support for Linux and macOS\n- ⚡ **Zero configuration** - Works out of the box with sensible defaults\n\n## Quick Start\n\n> By default, httpjail denies all network requests. Provide a JS rule or script to allow traffic.\n\n```bash\n# Allow only requests to github.com (JS)\nhttpjail --js \"r.host === 'github.com'\" -- your-app\n\n# Load JS from a file\necho \"/^api\\\\.example\\\\.com$/.test(r.host) && r.method === 'GET'\" > rules.js\nhttpjail --js-file rules.js -- curl https://api.example.com/health\n\n# Log requests to a file\nhttpjail --request-log requests.log --js \"true\" -- npm install\n# Log format: \"<timestamp> <+/-> <METHOD> <URL>\" (+ = allowed, - = blocked)\n\n# Use custom script for request evaluation\nhttpjail --sh /path/to/check.sh -- ./my-app\n# Script receives: HTTPJAIL_URL, HTTPJAIL_METHOD, HTTPJAIL_HOST, HTTPJAIL_SCHEME, HTTPJAIL_PATH\n# Exit 0 to allow, non-zero to block. stdout becomes additional context in 403 response.\n\n# Run as standalone proxy server (no command execution) and allow all\nhttpjail --server --js \"true\"\n# Server defaults to ports 8080 (HTTP) and 8443 (HTTPS)\n# Configure your application:\n# HTTP_PROXY=http://localhost:8080 HTTPS_PROXY=http://localhost:8443\n\n# Run Docker containers with network isolation (Linux only)\nhttpjail --js \"r.host === 'api.github.com'\" --docker-run -- --rm alpine:latest wget -qO- https://api.github.com\n```\n\n## Architecture Overview\n\nhttpjail creates an isolated network environment for the target process, intercepting all HTTP/HTTPS traffic through a transparent proxy that enforces user-defined rules.\n\n### Linux Implementation\n\n```\n┌─────────────────────────────────────────────────┐\n│                 httpjail Process                │\n├─────────────────────────────────────────────────┤\n│  1. Create network namespace                    │\n│  2. Setup nftables rules                        │\n│  3. Start embedded proxy                        │\n│  4. Export CA trust env vars                    │\n│  5. Execute target process in namespace         │\n└─────────────────────────────────────────────────┘\n                         ↓\n┌─────────────────────────────────────────────────┐\n│              Target Process                     │\n│  • Isolated in network namespace                │\n│  • All HTTP/HTTPS → local proxy                 │\n│  • CA cert trusted via env vars                 │\n└─────────────────────────────────────────────────┘\n```\n\n### macOS Implementation\n\n```\n┌─────────────────────────────────────────────────┐\n│                 httpjail Process                │\n├─────────────────────────────────────────────────┤\n│  1. Start HTTP/HTTPS proxy servers              │\n│  2. Set HTTP_PROXY/HTTPS_PROXY env vars         │\n│  3. Generate/load CA certificate                │\n│  4. Execute target with proxy environment       │\n└─────────────────────────────────────────────────┘\n                         ↓\n┌─────────────────────────────────────────────────┐\n│              Target Process                     │\n│  • HTTP_PROXY/HTTPS_PROXY environment vars      │\n│  • Applications must respect proxy settings     │\n│  • CA cert via environment variables            │\n└─────────────────────────────────────────────────┘\n```\n\n**Note**: Due to macOS PF (Packet Filter) limitations, httpjail uses environment-based proxy configuration on macOS. PF translation rules (such as `rdr` and `route-to`) cannot match on user or group, making transparent traffic interception impossible. As a result, httpjail operates in \"weak mode\" on macOS, relying on applications to respect the `HTTP_PROXY` and `HTTPS_PROXY` environment variables. Most command-line tools and modern applications respect these settings, but some may bypass them. See also https://github.com/coder/httpjail/issues/7.\n\n## Platform Support\n\n| Feature           | Linux                        | macOS  ",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-19T08:56:08.468810"
  },
  {
    "basic_info": {
      "name": "MFTool",
      "full_name": "Kudaes/MFTool",
      "owner": "Kudaes",
      "description": "Direct access to NTFS volumes",
      "url": "https://github.com/Kudaes/MFTool",
      "clone_url": "https://github.com/Kudaes/MFTool.git",
      "ssh_url": "git@github.com:Kudaes/MFTool.git",
      "homepage": "",
      "created_at": "2025-09-09T08:33:44Z",
      "updated_at": "2025-09-19T08:49:08Z",
      "pushed_at": "2025-09-09T08:59:00Z"
    },
    "stats": {
      "stars": 252,
      "forks": 23,
      "watchers": 252,
      "open_issues": 0,
      "size": 47
    },
    "tech_info": {
      "language": "Rust",
      "languages": {
        "Rust": 164285
      },
      "license": "Apache License 2.0",
      "topics": []
    },
    "content": {
      "readme": "# Description\n\nMFTool is a red team-oriented NTFS parser. Instead of asking Windows for files, it parses the on-disk structures of a mounted NTFS volume directly to build an in-memory copy of the [Master File Table](https://learn.microsoft.com/en-us/windows/win32/fileio/master-file-table). That in-memory MFT is kept encrypted and is then used to:\n\n- Search the entire disk for files and metadata.\n- Retrieve file contents **without opening an OS-level file handle**, enabling access to data that is typically locked by the operating system (e.g., `SAM`, `NTUSER.dat`, `SYSTEM`, `pagefile.sys`, etc.) as well as deleted files (hereafter referred to as \"hidden\").\n\nDirect NTFS parsing is not new and is widely used in forensics, although this tool has been developed taking into account the needs and requirements from a red team perspective. Also, I wasn't able to find a public tool that performs in the way I pictured it, so I decided to create my own NTFS parser.\n\n# Content\n\n- [How it works](#How-it-works)\n- [How to use it](#How-to-use-it)\n- [Commands](#Commands)\n- [Examples](#Examples)\n  - [Retrieving metadata of an entry](#Retrieving-metadata-of-an-entry)\n  - [Accessing deleted and locked files](#Accessing-deleted-and-locked-files)\n  - [Directory listing and regex-based search](#Directory-listing-and-regex-based-search)\n- [Limitations and Known Issues](#Limitations-and-Known-Issues)\n- [Links](#Links)\n\n# How it works\n\nMFTool interacts directly with a mounted NTFS volume by opening a handle to it and parsing the on-disk filesystem structures. Instead of relying on Windows APIs, it walks through the Master File Table to build an internal representation of the filesystem.\n\n1. **Boot sector parsing**  \n   Once a handle to the volume is opened, MFTool parses the boot sector to locate the offset of the first MFT entry. From there, it follows the cluster chains to enumerate the rest of the entries.\n\n2. **MFT entry reconstruction**  \n   Each MFT record is reconstructed by replacing the Update Sequence Number (USN) with the corresponding values from the Update Sequence Array (USA). The reconstructed entries are stored in an encrypted in-memory cache to prevent accidental data leakage. This cache is rebuilt every time a new target volume is selected.\n\n3. **File content retrieval**  \n   To read a file, MFTool does not rely on an OS-level file handle. Instead, it parses the file's MFT entry, extracts the unnamed `$DATA` attribute, and follows its data run list to locate the clusters containing the file's content.  \n   - Data is read directly from disk offsets, ignoring Windows' file access controls (note that administrative privileges are still required to run the tool, so this should not be considered an ACL bypass per se).  \n   - If the file is compressed, the content is split into logical units and decompressed using [`RtlDecompressBuffer`](https://learn.microsoft.com/en-us/windows-hardware/drivers/ddi/ntifs/nf-ntifs-rtldecompressbuffer).  \n   - This allows retrieval of normal, locked, and even deleted files in case the content is still present in the disk.   \n\n4. **Searching and directory listing**  \n   File search and directory enumeration rely on parsing the `$I30` index attributes (`INDEX_ROOT`, `INDEX_ALLOCATION` structures). This allows for efficient lookups with logarithmic complexity `O(log n)`, and supports both exact name matching and regex-based searches (regex-based searches are not logarithmic tho).\n\n5. **Reparse point handling**  \n   The parser currently resolves reparse points of type **symlink** and **mount point**, ensuring correct navigation across linked or mounted paths.  \n\n# How to use it\n\nTo build the tool just compile it in `release` mode:\n\n\tC:\\Path\\To\\MFTool> cargo build --release\n\nOnce executed, the tool will wait for commands out of the list commented in the next section.\n\n# Commands\n\n## set_target\nSets the target volume to be parsed.  \nThis command expects a string pointing to a mounted NTFS volume, either by drive letter or by volume GUID path (e.g., `\\\\.\\C:` or `\\\\?\\Volume{04171d6a-0000-0000-0000-100000000000}`).  \nOnce a valid volume path is provided, MFTool rebuilds its in-memory cache of the MFT. From this point, all further interactions with the volume are performed against that cache.\n\n## rebuild\nRebuilds the in-memory MFT cache for the current target volume.  \n\n## ls\nParses the `$I30` index attributes to list the files contained in a directory.  \nBoth the Win32 name and the DOS (short) name (if any) of each file are displayed.\n\n## show\nGiven a directory path and a filename, retrieves the metadata stored in the file's MFT entry.\n\n## show_by_id\nSame as `show`, but instead of requiring a path and filename, it expects the MFT entry index.\n\n## show_by_regex\nSearches for files across the entire volume using a regular expression (expressed as `/regex/`).  \nThis command performs a sequential search of all MFT entries, so its complexity is linear.  \nIf invoked with the `hidden` flag, it restricts th",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-19T08:56:09.608721"
  },
  {
    "basic_info": {
      "name": "gittype",
      "full_name": "unhappychoice/gittype",
      "owner": "unhappychoice",
      "description": "A CLI code-typing game that turns your source code into typing challenges",
      "url": "https://github.com/unhappychoice/gittype",
      "clone_url": "https://github.com/unhappychoice/gittype.git",
      "ssh_url": "git@github.com:unhappychoice/gittype.git",
      "homepage": "",
      "created_at": "2025-08-28T15:57:14Z",
      "updated_at": "2025-09-19T06:37:10Z",
      "pushed_at": "2025-09-18T11:16:17Z"
    },
    "stats": {
      "stars": 209,
      "forks": 3,
      "watchers": 209,
      "open_issues": 12,
      "size": 24106
    },
    "tech_info": {
      "language": "Rust",
      "languages": {
        "Rust": 1932813,
        "Shell": 12375
      },
      "license": "MIT License",
      "topics": [
        "cli-game",
        "cli-tool",
        "code-practice",
        "coding-skills",
        "gamification",
        "learning-tool",
        "productivity",
        "programming",
        "ratatui",
        "rust",
        "terminal-game",
        "touch-typing",
        "tree-sitter",
        "typing-game",
        "typing-practice",
        "typing-test",
        "typing-trainer",
        "typingtest"
      ]
    },
    "content": {
      "readme": "![GitType Banner](docs/images/gittype-banner.png)\n\n# GitType ⌨️💻\n\n> *\"Show your AI who's boss: just you, your keyboard, and your coding sins\"*\n\n**GitType** turns your own source code into typing challenges. Because why practice with boring lorem ipsum when you can type your beautiful `fn main()` implementations?\n\n## Demo 🎬\n\n![GitType Demo](docs/images/demo.gif)\n\n## Features ✨\n\n- 🦀🐍⚡🐹💎🍎🎯☕🐘#️⃣🔧➕🎭🎯 **Multi-language**: Rust, TypeScript, JavaScript, Python, Go, Ruby, Swift, Kotlin, Java, PHP, C#, C, C++, Haskell, Dart, Scala (more languages incoming!)  \n- 📊 **Real-time metrics**: Live WPM, accuracy, and consistency tracking as you type\n- 🏆 **Ranking system**: Unlock developer titles from \"Hello World Newbie\" to \"Quantum Computer\" with ASCII art\n- 🎮 **Multiple game modes**: Normal, Time Attack, and custom difficulty levels (Easy to Zen)\n- ⏸️ **Pause/resume**: Take breaks without ruining your stats\n- 🎯 **Your own code**: Type functions from your actual projects, not boring examples\n\n## Installation 📦\n\n### Quick Install (Recommended)\n#### One-liner installation (Linux/macOS/Windows)\n```bash\ncurl -sSL https://raw.githubusercontent.com/unhappychoice/gittype/main/install.sh | bash\n```\n\n#### Or with specific version\n```bash\ncurl -sSL https://raw.githubusercontent.com/unhappychoice/gittype/main/install.sh | bash -s -- --version v0.5.0\n```\n\n### Homebrew (macOS/Linux)\n```bash\nbrew install unhappychoice/tap/gittype\n```\n\n\n### Cargo (Universal)\n```bash\ncargo install gittype\n```\n\n### Binary Downloads\nGet pre-compiled binaries for your platform from our [releases page](https://github.com/unhappychoice/gittype/releases/latest).\n\nAvailable platforms:\n- `x86_64-apple-darwin` (Intel Mac)\n- `aarch64-apple-darwin` (Apple Silicon Mac)\n- `x86_64-unknown-linux-gnu` (Linux x64)\n- `aarch64-unknown-linux-gnu` (Linux ARM64)\n- `x86_64-pc-windows-msvc` (Windows)\n\n## Quick Start 🚀\n\n```bash\n# cd into your messy codebase\ncd ~/that-project-you-never-finished\n\n# Start typing your own spaghetti code (uses current directory by default)\ngittype\n\n# Or specify a specific repository path\ngittype /path/to/another/repo\n\n# Clone and play with any GitHub repository\ngittype --repo clap-rs/clap\ngittype --repo https://github.com/ratatui-org/ratatui\ngittype --repo git@github.com:dtolnay/anyhow.git\n```\n\n## Screenshots 📸\n\n![GitType Title Screen](docs/images/title.png)\n\n![GitType Gaming](docs/images/gaming.png)\n\n![GitType Result](docs/images/result.png)\n\n## Why GitType? 🤔\n\n- **Look busy at work** → \"I'm studying the codebase\" (technically true!)\n- **Beat the AI overlords** → Type faster than ChatGPT can generate\n- **Stop typing boring stuff** → Your own bugs are way more interesting than lorem ipsum\n- **Discover forgotten treasures** → That elegant function you wrote at 3am last year\n- **Procrastinate like a pro** → It's code review, but gamified!\n- **Embrace your legacy code** → Finally face those variable names you're not proud of\n- **Debug your typing skills** → Because `pubic static void main` isn't a typo anymore\n- **Therapeutic code reliving** → Type through your programming journey, tears included\n- **Climb the dev ladder** → From \"Code Monkey\" to \"Quantum Computer\" - each rank comes with fancy ASCII art\n\n*\"Basically, you need an excuse to avoid real work, and this one's pretty good.\"*\n\n## Documentation 📚\n\nPerfect for when the game gets too addictive:\n\n- **[Installation](docs/installation.md)** - `cargo install` and chill\n- **[Usage](docs/usage.md)** - All the CLI flags your heart desires  \n- **[Languages](docs/supported-languages.md)** - What we extract and how\n- **[Contributing](docs/CONTRIBUTING.md)** - Join the keyboard warriors\n- **[Architecture](docs/ARCHITECTURE.md)** - For the curious minds\n\n## License 📄\n\n[MIT](LICENSE) - Because sharing is caring (and legal requirements)\n\n---\n\n*Built with ❤️ and way too much caffeine by developers who got tired of typing \"hello world\"*\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-19T08:56:10.730999"
  },
  {
    "basic_info": {
      "name": "cachey",
      "full_name": "s2-streamstore/cachey",
      "owner": "s2-streamstore",
      "description": "Read-through cache for object storage",
      "url": "https://github.com/s2-streamstore/cachey",
      "clone_url": "https://github.com/s2-streamstore/cachey.git",
      "ssh_url": "git@github.com:s2-streamstore/cachey.git",
      "homepage": "http://cachey.dev",
      "created_at": "2025-09-05T15:05:06Z",
      "updated_at": "2025-09-19T06:14:02Z",
      "pushed_at": "2025-09-18T00:47:39Z"
    },
    "stats": {
      "stars": 177,
      "forks": 7,
      "watchers": 177,
      "open_issues": 2,
      "size": 248
    },
    "tech_info": {
      "language": "Rust",
      "languages": {
        "Rust": 162035,
        "Dockerfile": 1094,
        "Just": 746
      },
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "# Cachey\n\nHigh-performance read-through cache for object storage.\n\n- Simple HTTP API\n- Hybrid memory + disk cache powered by [foyer](https://github.com/foyer-rs/foyer)\n- Designed for caching immutable blobs\n- Works with any S3-compatible backend, but has its own `/fetch` API requiring a precise `Range`\n- Fixed page size (16 MiB) – maps requested byte range to page-aligned lookups\n- Coalesces concurrent requests for the same page\n- Makes hedged requests to manage tail latency of object storage\n- Can attempt redundant buckets for a given object\n\n[Motivating context](https://www.reddit.com/r/databasedevelopment/comments/1nh1goo/cachey_a_readthrough_cache_for_s3)\n\n## API\n\n### Fetching data\n\n#### Request\n\n```\nHEAD|GET /fetch/{kind}/{object}\n```\n- `kind` + `object` form the cache key\n- `kind` identifies the bucket set (up to 64 chars)\n- `object` is the S3 object key\n\n| Header | Required | Description |\n|--------|----------|-------------|\n| `Range` | yes | Byte range in format `bytes={first}-{last}` |\n| `C0-Bucket` | no | Bucket(s) containing the object |\n| `C0-Config` | no | Override S3 request config |\n\n`C0-Bucket` behavior:\n- Multiple headers indicate bucket preference order\n- If omitted, `kind` is used as the singular bucket name\n- Client preference may be overridden based on internal latency/error stats\n- At most 2 buckets attempted per page miss\n\n`C0-Config` overrides:\nSpace-separated key-value pairs to override S3 request configuration per page miss.\n- `ct=<ms>` Connect timeout (in case an existing connection could not be reused)\n- `rt=<ms>` Read timeout (time-to-first-byte)\n- `ot=<ms>` Operation timeout (across retries)\n- `oat=<ms>` Operation attempt timeout\n- `ma=<num>` Maximum attempts\n- `ib=<ms>` Initial backoff duration\n- `mb=<ms>` Maximum backoff duration\n\n#### Example Request\n\n```http\nGET /fetch/prod-videos/movie-2024.mp4 HTTP/1.1\nRange: bytes=1048576-18874367\nC0-Bucket: us-west-videos\nC0-Bucket: us-east-videos-backup\nC0-Config: ct=1000 oat=1500 ma=5 ib=10 mb=100\n```\n\n#### Response\n\nThe service maps requests to 16 MiB page-aligned ranges and the response has standard HTTP semantics (`206 Partial Content`, `404 Not Found` etc.)\n\n| Header | Description |\n|--------|-------------|\n| `Content-Range` | Actual byte range served |\n| `Content-Length` | Number of bytes in response |\n| `Last-Modified` | Timestamp from first page |\n| `Content-Type` | Always `application/octet-stream` |\n| `C0-Status` | Status for first page |\n\n`C0-Status` format: `{first}-{last}; {bucket}; {cached_at}`\n- Byte range and which bucket was used\n- `cached_at` is Unix timestamp with 0 implying a cache miss\n- Only first page status is sent as a header; status for subsequent pages follows the body as trailers\n\n#### Example Response\n\n```http\nHTTP/1.1 206 Partial Content\nContent-Range: bytes 1048576-18874367/52428800\nContent-Length: 17825792\nContent-Type: application/octet-stream\nC0-Status: 1048576-16777215; us-west-videos; 1704067200\n\n<data>\n\nC0-Status: 16777216-18874367; us-west-videos; 0\n```\n\n### Monitoring\n\n`GET /stats` returns throughput stats as JSON for load balancing and health checking.\n\n`GET /metrics` returns a more comprehensive set of metrics in Prometheus text format.\n\n## Command line\n\n[Docker images](https://github.com/s2-streamstore/cachey/pkgs/container/cachey) are available.\n\n```\nUsage: server [OPTIONS]\n\nOptions:\n      --memory <MEMORY>\n          Maximum memory to use for cache (e.g., \"512MiB\", \"2GB\", \"1.5GiB\") [default: 4GiB]\n      --disk-path <DISK_PATH>\n          Path to disk cache storage, which may be a directory or block device\n      --disk-kind <DISK_KIND>\n          Kind of disk cache, which may be a file system or block device [default: fs] [possible values: block, fs]\n      --disk-capacity <DISK_CAPACITY>\n          Maximum disk cache capacity (e.g., \"100GiB\") If not specified, up to 80% of the available space will be used\n      --hedge-quantile <HEDGE_QUANTILE>\n          Latency quantile for making hedged requests (0.0-1.0, use 0 to disable hedging) [default: 0.99]\n      --tls-self\n          Use a self-signed certificate for TLS\n      --tls-cert <TLS_CERT>\n          Path to the TLS certificate file (e.g., cert.pem) Must be used together with --tls-key\n      --tls-key <TLS_KEY>\n          Path to the private key file (e.g., key.pem) Must be used together with --tls-cert\n      --port <PORT>\n          Port to listen on [default: 443 if HTTPS configured, otherwise 80 for HTTP]\n  -h, --help\n          Print help\n  -V, --version\n          Print version\n```\n\n## Development\n\n- [justfile](./justfile) contains commands for [just](https://just.systems/man/en/) doing things\n- [AGENTS.md](./AGENTS.md) and symlinks for your favorite coding buddies\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-19T08:56:11.861420"
  },
  {
    "basic_info": {
      "name": "pingoo",
      "full_name": "pingooio/pingoo",
      "owner": "pingooio",
      "description": "The fast and secure Load Balancer / API Gateway / Reverse Proxy with built-in service discovery, GeoIP, WAF, bot protection and much more - https://pingoo.io",
      "url": "https://github.com/pingooio/pingoo",
      "clone_url": "https://github.com/pingooio/pingoo.git",
      "ssh_url": "git@github.com:pingooio/pingoo.git",
      "homepage": "https://pingoo.io",
      "created_at": "2025-09-17T07:18:40Z",
      "updated_at": "2025-09-19T08:48:53Z",
      "pushed_at": "2025-09-19T08:00:07Z"
    },
    "stats": {
      "stars": 146,
      "forks": 5,
      "watchers": 146,
      "open_issues": 6,
      "size": 388
    },
    "tech_info": {
      "language": "Rust",
      "languages": {
        "Rust": 190293,
        "TypeScript": 7020,
        "Dockerfile": 6694,
        "Makefile": 2265,
        "Shell": 1620,
        "HTML": 892,
        "CSS": 620,
        "Vim Script": 19
      },
      "license": "MIT License",
      "topics": [
        "akamai",
        "anti-bot",
        "apache2",
        "api",
        "api-gateway",
        "captcha",
        "cloudflare",
        "fastly",
        "firewall",
        "haproxy",
        "load-balancer",
        "nginx",
        "pingoo",
        "proxy",
        "quic",
        "reverse-proxy",
        "rust",
        "security",
        "service-discovery",
        "waf"
      ]
    },
    "content": {
      "readme": "<p align=\"center\">\n  <a href=\"https://pingoo.io\" target=\"_blank\" rel=\"noopener\"><img alt=\"Pingoo logo\" src=\"https://pingoo.io/icon-256.png\" height=\"128\" /></a>\n  <h1 align=\"center\">Pingoo</h1>\n  <h3 align=\"center\">The fast and secure Load Balancer / API Gateway / Reverse Proxy with built-in service discovery, GeoIP, WAF, bot protection and much more</h3>\n  <h3 align=\"center\">\n    <a href=\"https://pingoo.io\">Documentation</a> | <a href=\"https://kerkour.com/announcing-pingoo\">Read the launch post</a>\n  </h3>\n</p>\n\nOpen Source load balancers and reverse proxies are stuck in the past century with a very slow pace of development and most of the important features reserved for \"Enterprise Editions\" which lead developers to use third-party cloud services, exposing their users' traffic to legal, security and reliability risks.\n\nPingoo is a modern Load Balancer / API Gateway / Reverse Proxy that run on your own servers and already have (or will have soon) all the features you expect from managed services and even more. All of that with a huge boost in performance and security thanks to reduced latency and, of course, Rust ;)\n\n* Service Discovery (Docker, DNS...)\n* Web Application Firewall (WAF)\n* Easy compliance because the data never leaves your servers\n* Bot protection and management\n* TCP proxying\n* Post-Quantum TLS\n* GeoIP (country, ASN)\n* Static sites\n* And much more\n\n> ⚠️ Pingoo is currently in beta, use with caution.\n\n## Quickstart\n\n```bash\n# You have a static site in the www folder\n$ ls www\nindex.html\n$ docker run --rm -ti --network host -v `pwd`/www:/var/wwww ghcr.io/pingooio/pingoo\n# Pingoo is now listenning on http://0.0.0.0:8080\n```\n\n## Documentation\n\nSee https://pingoo.io\n\n\n## Updates\n\n[Click Here](https://kerkour.com/blog) to visit the blog and [subscribe](https://kerkour.com/subscribe) by RSS or email to get weekly / monthly updates. No spam ever, only technical deep dives.\n\n\n## Contributing\n\nPlease open an issue to discuss your idea before submitting a Pull Request.\n\n\n## Support\n\nDo you have custom needs? Do you want your features to be prioritized? Are you under attack and need help? Do you need support for deploying and self-hosting Pingoo?\n\nFeel free to reach our team of experts to see how we can help: https://pingoo.io/contact\n\n\n## Security\n\nWe are committed to make Pingoo the most secure Load Balancer / Reverse Proxy in the universe and beyond. If you've found a security issue in Pingoo, we appreciate your help in disclosing it to us in a responsible manner by contacting us: https://pingoo.io/contact\n\n\n## License\n\nMIT. See `LICENSE.txt`\n\nForever Open Source. No Open Core or \"Enterprise Edition\".\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-19T08:56:12.974855"
  },
  {
    "basic_info": {
      "name": "envx",
      "full_name": "mikeleppane/envx",
      "owner": "mikeleppane",
      "description": "A powerful and secure environment variable manager for developers, featuring an intuitive Terminal User Interface (TUI) and comprehensive command-line interface.",
      "url": "https://github.com/mikeleppane/envx",
      "clone_url": "https://github.com/mikeleppane/envx.git",
      "ssh_url": "git@github.com:mikeleppane/envx.git",
      "homepage": null,
      "created_at": "2025-08-26T17:52:11Z",
      "updated_at": "2025-09-19T08:27:02Z",
      "pushed_at": "2025-09-19T08:32:23Z"
    },
    "stats": {
      "stars": 128,
      "forks": 2,
      "watchers": 128,
      "open_issues": 0,
      "size": 1561
    },
    "tech_info": {
      "language": "Rust",
      "languages": {
        "Rust": 590647,
        "Dockerfile": 2002,
        "Just": 845
      },
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "# envx\n\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n![CI](https://github.com/mikeleppane/envx/workflows/CI/badge.svg)\n<img alt=\"Rust\" src=\"https://img.shields.io/badge/Rust-1.85-orange\">\n<img alt=\"License\" src=\"https://img.shields.io/badge/License-MIT-blue\">\n\nA powerful and secure environment variable manager for developers, featuring an intuitive Terminal User Interface (TUI)\nand comprehensive command-line interface.\n\n## 🎥 Introduction Video\n\n[![Watch the video](https://img.youtube.com/vi/UzrKuQQURFw/maxresdefault.jpg)](https://youtu.be/UzrKuQQURFw)\n\n<p align=\"center\">\n  <i>Click the image above to watch a quick introduction to Envx</i>\n</p>\n\n[![Watch the video](https://img.youtube.com/vi/DbRPbN9KECw/maxresdefault.jpg)](https://youtu.be/DbRPbN9KECw)\n<p align=\"center\">\n  <i>Click the image above to watch how interactive wizard works</i>\n</p>\n\n\n## 📸 Screenshots\n\n<p align=\"center\">\n  <img src=\"images/main.png\" alt=\"Envx's Main Page\" />\n  <span>Envx's Main Page</span>\n</p>\n\n<p align=\"center\">\n  <img src=\"images/search.png\" alt=\"Envx's Search Dialog\" />\n  <span>Envx's Search Dialog</span>\n</p>\n\n<p align=\"center\">\n  <img src=\"images/view.png\" alt=\"Envx's View Dialog\" />\n  <span>Envx's View Dialog</span>\n</p>\n\n<p align=\"center\">\n  <img src=\"images/query.png\" alt=\"Envx's CLI Query\" />\n  <span>Envx's CLI Query Command</span>\n</p>\n\n## 🌟 Features\n\n- **🖥️ Interactive TUI**: Beautiful terminal interface for easy environment variable management\n- **🔍 Smart Search**: Fast filtering and searching across all environment variables\n- **📊 Source Tracking**: Distinguish between System, User, Process, Shell, and Application variables\n- **📝 Multi-line Support**: Edit complex environment variables with proper multi-line support\n- **🔄 Import/Export**: Support for multiple formats (JSON, YAML, TOML, ENV)\n- **📸 Snapshots & Profiles Feature Implementation**: Save and restore variable states\n- **📁 Project Configuration**: Define required variables, defaults, and scripts for consistent team environments\n- **👀 Watch Mode & Monitor**: Monitor file changes and sync automatically, track environment modifications in real-time\n- **⚡ Performance**: Built with Rust for blazing-fast performance\n- **🎨 Cross-platform**: Works on Windows, macOS, and Linux\n\n## 📦 Installation\n\n### From Source\n\n```bash\ngit clone https://github.com/yourusername/envx.git\ncd envx\ncargo install --path crates/envx\n```\n\n### Using Cargo\n\n```bash\ncargo install envex\n```\n\n### Pre-built Binaries\n\nDownload the latest release for your platform from the [releases page](https://github.com/yourusername/envx/releases).\n\n## 🚀 Quick Start\n\n### Launch the TUI\n\n```bash\nenvx tui\n# or\nenvx ui\n```\n\n### List all environment variables\n\n```bash\nenvx list\n```\n\n### Set a variable\n\n```bash\nenvx set MY_VAR \"my value\"\n```\n\n### Get a variable\n\n```bash\nenvx get MY_VAR\n```\n\n## 📖 Command Line Usage\n\n### Overview\n\n```bash\nSystem Environment Variable Manager\n\nUsage: envx.exe <COMMAND>\n\nCommands:\n  list     List environment variables\n  get      Get a specific environment variable\n  set      Set an environment variable\n  delete   Delete environment variable(s)\n  analyze  Analyze environment variables\n  tui      Launch the TUI [aliases: ui]\n  path     Manage PATH variable\n  export   Export environment variables to a file\n  import   Import environment variables from a file\n  help     Print this message or the help of the given subcommand(s)\n\nOptions:\n  -h, --help     Print help\n  -V, --version  Print version\n```\n\n### Core Commands\n\n#### `init` - Initialize a new project with the setup wizard\n\n```bash\nInitialize a new project with interactive wizard\n\nUsage: envx.exe init [OPTIONS]\n\nOptions:\n  -t, --template <TEMPLATE>  Use a specific template\n  -w, --wizard               Run interactive wizard\n      --list-templates       List available templates\n  -h, --help                 Print help\n```\n\nThe `init` command launches an interactive setup wizard that helps you configure your project's environment variables.\nThe wizard will:\n\n- Detect your project type (Web App, Python, Rust/Go, Docker, Microservices, or Custom)\n- Guide you through setting up environment variables with values\n- Create profiles for different environments (development, testing, production, etc.)\n- Generate `.env` files for each profile\n- Set up team collaboration features\n- Configure validation rules\n- Import existing `.env` files if found\n\n##### Example Usage\n\n```bash\n# Run the interactive setup wizard\nenvx init\n\n# Or\nenvx init --wizard\n\n```\n\n##### What the Wizard Creates\n\nAfter running the wizard, you'll have:\n\n1. **Project Configuration** (`.envx/config.yaml`):\n   - Required environment variables with descriptions\n   - Default values for common variables\n   - Validation rules and patterns\n   - Auto-load configuration for .env files\n\n2. **Environment Profiles**:\n   - Separate profiles for development, testing, production, etc.\n   - Profile-specific variable values\n   - Easy switc",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-19T08:56:14.128904"
  },
  {
    "basic_info": {
      "name": "wakezilla",
      "full_name": "guibeira/wakezilla",
      "owner": "guibeira",
      "description": "A simple Wake-on-LAN & reverse proxy toolkit — wake, route, and control your machines from anywhere. 🦖   ",
      "url": "https://github.com/guibeira/wakezilla",
      "clone_url": "https://github.com/guibeira/wakezilla.git",
      "ssh_url": "git@github.com:guibeira/wakezilla.git",
      "homepage": null,
      "created_at": "2025-09-06T16:13:34Z",
      "updated_at": "2025-09-17T01:15:29Z",
      "pushed_at": "2025-09-19T03:20:35Z"
    },
    "stats": {
      "stars": 127,
      "forks": 7,
      "watchers": 127,
      "open_issues": 1,
      "size": 212
    },
    "tech_info": {
      "language": "Rust",
      "languages": {
        "Rust": 39885,
        "HTML": 36327
      },
      "license": null,
      "topics": []
    },
    "content": {
      "readme": "# Wakezilla 🦖\n<img width=\"200\" height=\"159\" src=\"https://github.com/user-attachments/assets/e88f084b-47b8-467b-a5c6-d64327805792\" align=\"left\" alt=\"wakezilla\"/>\n\n⚡ Wake-on-LAN made simple → power on your machines remotely whenever needed.\n\n🌐 Reverse proxy → intercepts traffic and wakes the server automatically if it’s offline.\n\n🔌 Automatic shutdown → saves energy by powering down idle machines after configurable thresholds.\n\n\n## Web interface\n<img width=\"2698\" height=\"2012\" alt=\"image\" src=\"https://github.com/user-attachments/assets/667eedeb-431c-4aa2-bf7a-3eadd4221452\" />\n\n## Features\n\n- **Wake-on-LAN**: Send magic packets to wake sleeping machines\n- **TCP Proxy**: Forward ports to remote machines with automatic WOL\n- **Web Interface**: Manage machines, ports, and monitor activity through a web dashboard\n- **Automatic Shutdown**: Automatically turn off machines after inactivity periods\n- **Network Scanner**: Discover machines on your local network\n\n## Installation\n\n### Server Installation\n\n1. **Install Rust**:\n   ```bash\n   curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh\n   source $HOME/.cargo/env\n   ```\n\n2. **Clone and Build**:\n   ```bash\n   git clone <repository-url>\n   cd wakezilla\n   cargo build --release\n   ```\n\n3. **Configure the Server**:\n   Create a `machines.json` file (optional, will be created automatically):\n   ```json\n   []\n   ```\n\n4. **Run the Server**:\n   ```bash\n   ./target/release/wakezilla --server\n   ```\n   \n   By default, the web interface runs on port 3000.\n\n### Client Installation\n   make sure the machine was configured with wake on lan.\n1. **Install Rust** (if not already installed):\n   ```bash\n   curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh\n   source $HOME/.cargo/env\n   ```\n\n2. **Clone and Build**:\n   ```bash\n   git clone <repository-url>\n   cd wakezilla\n   cargo build --release\n   ```\n## Usage\n\n### Starting the Proxy Server\n```bash\n# Basic server start\n./target/release/wakezilla proxy-server\n\n# With custom port\n./target/release/wakezilla proxy-server --port 8080\n\n```\n\n### Starting the Client\n```bash\n# Connect to server\n./target/release/wakezilla client-server \n\n# With custom port\n./target/release/wakezilla client-server --port 8080\n\n```\n\n### Web Interface\nAccess the web interface at `http://<server-ip>:3000` to:\n- Add and manage machines\n- Configure port forwards\n- View network scan results\n- Send WOL packets manually\n- Configure automatic shutdown settings\n\n### Adding Machines\n1. Navigate to the web interface\n2. Click \"Add Machine\" or use the network scanner\n3. Fill in MAC address, IP, and name\n4. Configure:\n   - Turn-off port (if remote shutdown is needed)\n   - Request rate limiting (requests per hour and period minutes)\n   - Port forwards as needed\n\n### Configuring Automatic Shutdown\n1. When adding or editing a machine, enable \"Can be turned off remotely\"\n2. Set the \"Turn Off Port\" (typically 3001 for the client server)\n3. Configure rate limiting:\n   - Requests per Hour: Number of requests allowed\n   - Period Minutes: Time window for rate limiting\n4. The machine will automatically shut down after the configured inactivity period\n\n### Port Forwarding\n1. Add a machine to the system\n2. Configure port forwards for that machine:\n   - Local Port: Port on the server to listen on\n   - Target Port: Port on the remote machine to forward to\n3. When traffic hits the local port, the machine will be woken up if needed and traffic forwarded\n\n\n### Machine Configuration\nEach machine can be configured with:\n- MAC Address\n- IP Address\n- Name and Description\n- Turn-off Port (for remote shutdown)\n- Request Rate Limiting:\n  - Requests per Hour: Maximum requests allowed\n  - Period Minutes: Time window for rate limiting\n- Port Forwards:\n  - Local Port: Port on the server\n  - Target Port: Port on the remote machine\n\n## How It Works\n\n1. **Server Mode**: Runs the web interface and proxy services\n2. **Client Mode**: Runs on target machines to enable remote shutdown\n3. **WOL Process**: \n   - When traffic hits a configured port, the server sends a WOL packet\n   - Waits for the machine to become reachable\n   - Forwards traffic once the machine is up\n4. **Automatic Shutdown**: \n   - Monitors request activity for each machine\n   - After configured inactivity periods, sends shutdown signal\n   - Uses HTTP requests to the client for shutdown\n\n## Security Considerations\n\n- The server should be run on a trusted network\n- Access to the web interface should be restricted if exposed to the internet\n- The turn-off endpoint on clients should only be accessible from the server\n\n## Troubleshooting\n\n### Common Issues\n\n1. **Machine not waking up**:\n   - Verify the MAC address is correct\n   - Ensure WOL is enabled in the machine's BIOS/UEFI\n   - Check firewall settings on the target machine\n   - Verify the target machine supports WOL\n\n2. **Proxy not working**:\n   - Check that the target port is correct\n   - Verify the machine is reachable after WOL\n   - Ensure no firewall is blocking the conne",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-19T08:56:15.273146"
  },
  {
    "basic_info": {
      "name": "tsink",
      "full_name": "h2337/tsink",
      "owner": "h2337",
      "description": "Embedded time-series database for Rust",
      "url": "https://github.com/h2337/tsink",
      "clone_url": "https://github.com/h2337/tsink.git",
      "ssh_url": "git@github.com:h2337/tsink.git",
      "homepage": "",
      "created_at": "2025-09-12T21:29:24Z",
      "updated_at": "2025-09-19T06:24:49Z",
      "pushed_at": "2025-09-15T16:48:44Z"
    },
    "stats": {
      "stars": 119,
      "forks": 6,
      "watchers": 119,
      "open_issues": 3,
      "size": 136
    },
    "tech_info": {
      "language": "Rust",
      "languages": {
        "Rust": 162246
      },
      "license": "MIT License",
      "topics": [
        "database",
        "embedded-database",
        "rust",
        "time-series",
        "timeseries",
        "timeseries-database",
        "tsdb"
      ]
    },
    "content": {
      "readme": "# tsink\n\n<div align=\"center\">\n\n<p align=\"right\">\n  <img src=\"https://raw.githubusercontent.com/h2337/tsink/refs/heads/master/logo.svg\" width=\"250\" height=\"250\">\n</p>\n\n**A high-performance embedded time-series database for Rust**\n\n</div>\n\n## Overview\n\ntsink is a lightweight, high-performance time-series database engine written in Rust. It provides efficient storage and retrieval of time-series data with automatic compression, time-based partitioning, and thread-safe operations.\n\n### Key Features\n\n- **🚀 High Performance**: Gorilla compression achieves ~1.37 bytes per data point\n- **🔒 Thread-Safe**: Lock-free reads and concurrent writes with configurable worker pools\n- **💾 Flexible Storage**: Choose between in-memory or persistent disk storage\n- **📊 Time Partitioning**: Automatic data organization by configurable time ranges\n- **🏷️ Label Support**: Multi-dimensional metrics with key-value labels\n- **📝 WAL Support**: Write-ahead logging for durability and crash recovery\n- **🗑️ Auto-Retention**: Configurable automatic data expiration\n- **🐳 Container-Aware**: cgroup support for optimal resource usage in containers\n- **⚡ Zero-Copy Reads**: Memory-mapped files for efficient disk operations\n\n## Installation\n\nAdd tsink to your `Cargo.toml`:\n\n```toml\n[dependencies]\ntsink = \"0.2.1\"\n```\n\n## Quick Start\n\n### Basic Usage\n\n```rust\nuse tsink::{DataPoint, Row, StorageBuilder, Storage, TimestampPrecision};\n\nfn main() -> Result<(), Box<dyn std::error::Error>> {\n    // Create storage with default settings\n    let storage = StorageBuilder::new()\n        .with_timestamp_precision(TimestampPrecision::Seconds)\n        .build()?;\n\n    // Insert data points\n    let rows = vec![\n        Row::new(\"cpu_usage\", DataPoint::new(1600000000, 45.5)),\n        Row::new(\"cpu_usage\", DataPoint::new(1600000060, 47.2)),\n        Row::new(\"cpu_usage\", DataPoint::new(1600000120, 46.8)),\n    ];\n    storage.insert_rows(&rows)?;\n\n    // Note: Using timestamp 0 will automatically use the current timestamp\n    // let row = Row::new(\"cpu_usage\", DataPoint::new(0, 50.0));  // timestamp = current time\n\n    // Query data points\n    let points = storage.select(\"cpu_usage\", &[], 1600000000, 1600000121)?;\n    for point in points {\n        println!(\"Timestamp: {}, Value: {}\", point.timestamp, point.value);\n    }\n\n    storage.close()?;\n    Ok(())\n}\n```\n\n### Persistent Storage\n\n```rust\nuse tsink::{StorageBuilder, Storage};\nuse std::time::Duration;\n\nlet storage = StorageBuilder::new()\n    .with_data_path(\"./tsink-data\")              // Enable disk persistence\n    .with_partition_duration(Duration::from_secs(3600))  // 1-hour partitions\n    .with_retention(Duration::from_secs(7 * 24 * 3600))  // 7-day retention\n    .with_wal_buffer_size(8192)                  // 8KB WAL buffer\n    .build()?;\n```\n\n### Multi-Dimensional Metrics with Labels\n\n```rust\nuse tsink::{DataPoint, Label, Row};\n\n// Create metrics with labels for detailed categorization\nlet rows = vec![\n    Row::with_labels(\n        \"http_requests\",\n        vec![\n            Label::new(\"method\", \"GET\"),\n            Label::new(\"status\", \"200\"),\n            Label::new(\"endpoint\", \"/api/users\"),\n        ],\n        DataPoint::new(1600000000, 150.0),\n    ),\n    Row::with_labels(\n        \"http_requests\",\n        vec![\n            Label::new(\"method\", \"POST\"),\n            Label::new(\"status\", \"201\"),\n            Label::new(\"endpoint\", \"/api/users\"),\n        ],\n        DataPoint::new(1600000000, 25.0),\n    ),\n];\n\nstorage.insert_rows(&rows)?;\n\n// Query specific label combinations\nlet points = storage.select(\n    \"http_requests\",\n    &[\n        Label::new(\"method\", \"GET\"),\n        Label::new(\"status\", \"200\"),\n    ],\n    1600000000,\n    1600000100,\n)?;\n```\n\n## Architecture\n\ntsink uses a linear-order partition model that divides time-series data into time-bounded chunks:\n\n```\n┌─────────────────────────────────────────┐\n│             tsink Storage               │\n├─────────────────────────────────────────┤\n│                                         │\n│  ┌───────────────┐  Active Partition    │\n│  │ Memory Part.  │◄─ (Writable)         │\n│  └───────────────┘                      │\n│                                         │\n│  ┌───────────────┐  Buffer Partition    │\n│  │ Memory Part.  │◄─ (Out-of-order)     │\n│  └───────────────┘                      │\n│                                         │\n│  ┌───────────────┐                      │\n│  │ Disk Part. 1  │◄─ Read-only          │\n│  └───────────────┘   (Memory-mapped)    │\n│                                         │\n│  ┌───────────────┐                      │\n│  │ Disk Part. 2  │◄─ Read-only          │\n│  └───────────────┘                      │\n│         ...                             │\n└─────────────────────────────────────────┘\n```\n\n### Partition Lifecycle\n\n1. **Active Partition**: Accepts new writes, kept in memory\n2. **Buffer Partition**: Handles out-of-order writes within recent time window\n3. **Flushing**: When active partition is full, it's flushed to disk\n4. **Disk P",
      "default_branch": "master"
    },
    "fetched_at": "2025-09-19T08:56:16.412686"
  }
]