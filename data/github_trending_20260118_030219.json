[
  {
    "basic_info": {
      "name": "planning-with-files",
      "full_name": "OthmanAdi/planning-with-files",
      "owner": "OthmanAdi",
      "description": "Claude Code skill implementing Manus-style persistent markdown planning â€” the workflow pattern behind the $2B acquisition.",
      "url": "https://github.com/OthmanAdi/planning-with-files",
      "clone_url": "https://github.com/OthmanAdi/planning-with-files.git",
      "ssh_url": "git@github.com:OthmanAdi/planning-with-files.git",
      "homepage": "",
      "created_at": "2026-01-03T07:37:28Z",
      "updated_at": "2026-01-18T02:58:24Z",
      "pushed_at": "2026-01-17T23:35:28Z"
    },
    "stats": {
      "stars": 9683,
      "forks": 856,
      "watchers": 9683,
      "open_issues": 7,
      "size": 260
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 25314,
        "PowerShell": 10869,
        "Shell": 10242
      },
      "license": "MIT License",
      "topics": [
        "agent",
        "agent-skills",
        "agents",
        "claude",
        "claude-code",
        "claude-skills",
        "file-system",
        "file-system-access",
        "langchain",
        "langgraph",
        "manus",
        "manus-ai",
        "prompt-engineering",
        "reverse-engineering",
        "zod",
        "zod-validation"
      ]
    },
    "content": {
      "readme": "# Planning with Files\n\n> **Work like Manus** â€” the AI agent company Meta acquired for **$2 billion**.\n\n## Thank You\n\nTo everyone who starred, forked, and shared this skill â€” thank you. This project blew up in less than 24 hours, and the support from the community has been incredible.\n\nIf this skill helps you work smarter, that's all I wanted.\n\n---\n\nA Claude Code plugin that transforms your workflow to use persistent markdown files for planning, progress tracking, and knowledge storage â€” the exact pattern that made Manus worth billions.\n\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n[![Claude Code Plugin](https://img.shields.io/badge/Claude%20Code-Plugin-blue)](https://code.claude.com/docs/en/plugins)\n[![Claude Code Skill](https://img.shields.io/badge/Claude%20Code-Skill-green)](https://code.claude.com/docs/en/skills)\n[![Cursor Rules](https://img.shields.io/badge/Cursor-Rules-purple)](https://docs.cursor.com/context/rules-for-ai)\n[![Version](https://img.shields.io/badge/version-2.3.0-brightgreen)](https://github.com/OthmanAdi/planning-with-files/releases)\n\n## Quick Install\n\n```bash\n/plugin marketplace add OthmanAdi/planning-with-files\n/plugin install planning-with-files@planning-with-files\n```\n\nSee [docs/installation.md](docs/installation.md) for all installation methods.\n\n## Supported IDEs\n\n| IDE | Status | Installation Guide | Format |\n|-----|--------|-------------------|--------|\n| Claude Code | âœ… Full Support | [Installation](docs/installation.md) | Plugin + SKILL.md |\n| Cursor | âœ… Full Support | [Cursor Setup](docs/cursor.md) | Rules |\n| Kilocode | âœ… Full Support | [Kilocode Setup](docs/kilocode.md) | Rules |\n| OpenCode | âœ… Full Support | [OpenCode Setup](docs/opencode.md) | Personal/Project Skill |\n| Codex | âœ… Full Support | [Codex Setup](docs/codex.md) | Personal Skill |\n\n## Documentation\n\n| Document | Description |\n|----------|-------------|\n| [Installation Guide](docs/installation.md) | All installation methods (plugin, manual, Cursor, Windows) |\n| [Quick Start](docs/quickstart.md) | 5-step guide to using the pattern |\n| [Workflow Diagram](docs/workflow.md) | Visual diagram of how files and hooks interact |\n| [Troubleshooting](docs/troubleshooting.md) | Common issues and solutions |\n| [Cursor Setup](docs/cursor.md) | Cursor IDE-specific instructions |\n| [Windows Setup](docs/windows.md) | Windows-specific notes |\n| [Kilo Code Support](docs/kilocode.md) | Kilo Code integration guide |\n| [Codex Setup](docs/codex.md) | Codex IDE installation and usage |\n| [OpenCode Setup](docs/opencode.md) | OpenCode IDE installation, oh-my-opencode config |\n\n## Versions\n\n| Version | Features | Install |\n|---------|----------|---------|\n| **v2.3.0** (current) | Codex & OpenCode IDE support | `/plugin install planning-with-files@planning-with-files` |\n| **v2.2.2** | Restored skill activation language | See [releases](https://github.com/OthmanAdi/planning-with-files/releases) |\n| **v2.2.1** | Session recovery after /clear, enhanced PreToolUse hook | See [releases](https://github.com/OthmanAdi/planning-with-files/releases) |\n| **v2.2.0** | Kilo Code IDE support, Windows PowerShell support, OS-aware hooks | See [releases](https://github.com/OthmanAdi/planning-with-files/releases) |\n| **v2.1.2** | Fix template cache issue (Issue #18) | See [releases](https://github.com/OthmanAdi/planning-with-files/releases) |\n| **v2.1.0** | Claude Code v2.1 compatible, PostToolUse hook, user-invocable | See [releases](https://github.com/OthmanAdi/planning-with-files/releases) |\n| **v2.0.x** | Hooks, templates, scripts | See [releases](https://github.com/OthmanAdi/planning-with-files/releases) |\n| **v1.0.0** (legacy) | Core 3-file pattern | `git clone -b legacy` |\n\nSee [CHANGELOG.md](CHANGELOG.md) for details.\n\n## Why This Skill?\n\nOn December 29, 2025, [Meta acquired Manus for $2 billion](https://techcrunch.com/2025/12/29/meta-just-bought-manus-an-ai-startup-everyone-has-been-talking-about/). In just 8 months, Manus went from launch to $100M+ revenue. Their secret? **Context engineering**.\n\n> \"Markdown is my 'working memory' on disk. Since I process information iteratively and my active context has limits, Markdown files serve as scratch pads for notes, checkpoints for progress, building blocks for final deliverables.\"\n> â€” Manus AI\n\n## The Problem\n\nClaude Code (and most AI agents) suffer from:\n\n- **Volatile memory** â€” TodoWrite tool disappears on context reset\n- **Goal drift** â€” After 50+ tool calls, original goals get forgotten\n- **Hidden errors** â€” Failures aren't tracked, so the same mistakes repeat\n- **Context stuffing** â€” Everything crammed into context instead of stored\n\n## The Solution: 3-File Pattern\n\nFor every complex task, create THREE files:\n\n```\ntask_plan.md      â†’ Track phases and progress\nfindings.md       â†’ Store research and findings\nprogress.md       â†’ Session log and test results\n```\n\n### The Core Principle\n\n```\nContext Window = RAM (volatile, limited)\nFilesystem",
      "default_branch": "master"
    },
    "fetched_at": "2026-01-18T03:02:20.251046"
  },
  {
    "basic_info": {
      "name": "DeepTutor",
      "full_name": "HKUDS/DeepTutor",
      "owner": "HKUDS",
      "description": "\"DeepTutor: AI-Powered Personalized Learning Assistant\"",
      "url": "https://github.com/HKUDS/DeepTutor",
      "clone_url": "https://github.com/HKUDS/DeepTutor.git",
      "ssh_url": "git@github.com:HKUDS/DeepTutor.git",
      "homepage": "https://hkuds.github.io/DeepTutor",
      "created_at": "2025-12-28T15:35:54Z",
      "updated_at": "2026-01-18T02:21:12Z",
      "pushed_at": "2026-01-18T02:08:22Z"
    },
    "stats": {
      "stars": 8945,
      "forks": 1170,
      "watchers": 8945,
      "open_issues": 13,
      "size": 68152
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 1716221,
        "TypeScript": 926945,
        "Shell": 13838,
        "Dockerfile": 11515,
        "CSS": 8674,
        "JavaScript": 2572
      },
      "license": "GNU Affero General Public License v3.0",
      "topics": [
        "ai-agents",
        "ai-tutor",
        "deepresearch",
        "idea-generation",
        "interactive-learning",
        "knowledge-graph",
        "large-language-models",
        "multi-agent-systems",
        "rag"
      ]
    },
    "content": {
      "readme": "<div align=\"center\">\n\n<img src=\"assets/logo-ver2.png\" alt=\"DeepTutor Logo\" width=\"150\" style=\"border-radius: 15px;\">\n\n# DeepTutor: AI-Powered Personalized Learning Assistant\n\n[![Python](https://img.shields.io/badge/Python-3.10%2B-3776AB?style=flat-square&logo=python&logoColor=white)](https://www.python.org/downloads/)\n[![FastAPI](https://img.shields.io/badge/FastAPI-0.100%2B-009688?style=flat-square&logo=fastapi&logoColor=white)](https://fastapi.tiangolo.com/)\n[![React](https://img.shields.io/badge/React-19-61DAFB?style=flat-square&logo=react&logoColor=black)](https://react.dev/)\n[![Next.js](https://img.shields.io/badge/Next.js-16-000000?style=flat-square&logo=next.js&logoColor=white)](https://nextjs.org/)\n[![TailwindCSS](https://img.shields.io/badge/Tailwind-3.4-06B6D4?style=flat-square&logo=tailwindcss&logoColor=white)](https://tailwindcss.com/)\n[![License](https://img.shields.io/badge/License-AGPL--3.0-blue?style=flat-square)](LICENSE)\n\n<p align=\"center\">\n  <a href=\"https://discord.gg/eRsjPgMU4t\"><img src=\"https://img.shields.io/badge/Discord-Join_Community-5865F2?style=for-the-badge&logo=discord&logoColor=white\" alt=\"Discord\"></a>\n  &nbsp;&nbsp;\n  <a href=\"./Communication.md\"><img src=\"https://img.shields.io/badge/Feishu-Join_Group-00D4AA?style=for-the-badge&logo=feishu&logoColor=white\" alt=\"Feishu\"></a>\n  &nbsp;&nbsp;\n  <a href=\"https://github.com/HKUDS/DeepTutor/issues/78\"><img src=\"https://img.shields.io/badge/WeChat-Join_Group-07C160?style=for-the-badge&logo=wechat&logoColor=white\" alt=\"WeChat\"></a>\n</p>\n\n\n\n[**Quick Start**](#quick-start) Â· [**Core Modules**](#core-modules) Â· [**FAQ**](#faq)\n\n[ğŸ‡¨ğŸ‡³ ä¸­æ–‡](assets/README/README_CN.md) Â· [ğŸ‡¯ğŸ‡µ æ—¥æœ¬èª](assets/README/README_JA.md) Â· [ğŸ‡ªğŸ‡¸ EspaÃ±ol](assets/README/README_ES.md) Â· [ğŸ‡«ğŸ‡· FranÃ§ais](assets/README/README_FR.md) Â· [ğŸ‡¸ğŸ‡¦ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©](assets/README/README_AR.md) Â· [ğŸ‡·ğŸ‡º Ğ ÑƒÑÑĞºĞ¸Ğ¹](assets/README/README_RU.md) Â· [ğŸ‡®ğŸ‡³ à¤¹à¤¿à¤¨à¥à¤¦à¥€](assets/README/README_HI.md) Â· [ğŸ‡µğŸ‡¹ PortuguÃªs](assets/README/README_PT.md)\n\n</div>\n\n<div align=\"center\">\n\nğŸ“š **Massive Document Knowledge Q&A** &nbsp;â€¢&nbsp; ğŸ¨ **Interactive Learning Visualization**<br>\nğŸ¯ **Knowledge Reinforcement** &nbsp;â€¢&nbsp; ğŸ” **Deep Research & Idea Generation**\n\n</div>\n\n---\n### ğŸ“° News\n\n> **[2026.1.15]** DeepTutor [v0.5.0](https://github.com/HKUDS/DeepTutor/releases/tag/v0.5.0) is out! Fixed multiple environment configuration and stability issues. We recommend everyone to pull the latest version! ğŸ‰\n\n> **[2026.1.1]** Happy New Year! Join our [Discord Community](https://discord.gg/zpP9cssj), [Wechat Community](https://github.com/HKUDS/DeepTutor/issues/78), or [Discussions](https://github.com/HKUDS/DeepTutor/discussions) - shape the future of DeepTutor! ğŸ’¬\n\n> **[2025.12.30]** Visit our [Official Website](https://hkuds.github.io/DeepTutor/) for more details!\n\n> **[2025.12.29]** DeepTutor is now live! âœ¨\n\n### ğŸ“¦ Releases\n\n> **[2026.1.15]** Release [v0.5.0](https://github.com/HKUDS/DeepTutor/releases/tag/v0.5.0) - Unified LLM & Embedding services, RAG pipeline selection, and major enhancements to Home, History, QuestionGen & Settings modules -- Thanks to all the contributors!\n<details>\n<summary>History releases</summary>\n\n> **[2026.1.9]** Release [v0.4.1](https://github.com/HKUDS/DeepTutor/releases/tag/v0.4.1) with LLM Provider system overhaul, Question Generation robustness improvements, and codebase cleanup - Thanks to all the contributors!\n\n> **[2026.1.9]** Release [v0.4.0](https://github.com/HKUDS/DeepTutor/releases/tag/v0.4.0) with new code structure, multiple llm & embeddings support - Thanks to all the contributors!\n\n> **[2026.1.5]** [v0.3.0](https://github.com/HKUDS/DeepTutor/releases/tag/v0.3.0) - Unified PromptManager architecture, CI/CD automation & pre-built Docker images on GHCR\n\n> **[2026.1.2]** [v0.2.0](https://github.com/HKUDS/DeepTutor/releases/tag/v0.2.0) - Docker deployment, Next.js 16 & React 19 upgrade, WebSocket security & critical vulnerability fixes\n\n</details>\n\n---\n\n## Key Features of DeepTutor\n\n### ğŸ“š Massive Document Knowledge Q&A\nâ€¢ **Smart Knowledge Base**: Upload textbooks, research papers, technical manuals, and domain-specific documents. Build a comprehensive AI-powered knowledge repository for instant access.<br>\nâ€¢ **Multi-Agent Problem Solving**: Dual-loop reasoning architecture with RAG, web search, and code execution -- delivering step-by-step solutions with precise citations.\n\n### ğŸ¨ Interactive Learning Visualization\nâ€¢ **Knowledge Simplification & Explanations**: Transform complex concepts, knowledge, and algorithms into easy-to-understand visual aids, detailed step-by-step breakdowns, and engaging interactive demonstrations.<br>\nâ€¢ **Personalized Q&A**: Context-aware conversations that adapt to your learning progress, with interactive pages and session-based knowledge tracking.\n\n### ğŸ¯ Knowledge Reinforcement with Practice Exercise Generator\nâ€¢ **Intelligent Exercise Creation**: Generate targeted quizzes, practice problems, and customized assessments tailored to your current knowledge level and specific learning",
      "default_branch": "main"
    },
    "fetched_at": "2026-01-18T03:02:21.413993"
  },
  {
    "basic_info": {
      "name": "Agent-Skills-for-Context-Engineering",
      "full_name": "muratcankoylan/Agent-Skills-for-Context-Engineering",
      "owner": "muratcankoylan",
      "description": "A comprehensive collection of Agent Skills for context engineering, multi-agent architectures, and production agent systems. Use when building, optimizing, or debugging agent systems that require effective context management.",
      "url": "https://github.com/muratcankoylan/Agent-Skills-for-Context-Engineering",
      "clone_url": "https://github.com/muratcankoylan/Agent-Skills-for-Context-Engineering.git",
      "ssh_url": "git@github.com:muratcankoylan/Agent-Skills-for-Context-Engineering.git",
      "homepage": null,
      "created_at": "2025-12-21T02:43:42Z",
      "updated_at": "2026-01-18T02:48:25Z",
      "pushed_at": "2026-01-13T17:30:43Z"
    },
    "stats": {
      "stars": 7308,
      "forks": 575,
      "watchers": 7308,
      "open_issues": 5,
      "size": 3607
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 171752
      },
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "# Agent Skills for Context Engineering\n\nA comprehensive, open collection of Agent Skills focused on context engineering principles for building production-grade AI agent systems. These skills teach the art and science of curating context to maximize agent effectiveness across any agent platform.\n\n## What is Context Engineering?\n\nContext engineering is the discipline of managing the language model's context window. Unlike prompt engineering, which focuses on crafting effective instructions, context engineering addresses the holistic curation of all information that enters the model's limited attention budget: system prompts, tool definitions, retrieved documents, message history, and tool outputs.\n\nThe fundamental challenge is that context windows are constrained not by raw token capacity but by attention mechanics. As context length increases, models exhibit predictable degradation patterns: the \"lost-in-the-middle\" phenomenon, U-shaped attention curves, and attention scarcity. Effective context engineering means finding the smallest possible set of high-signal tokens that maximize the likelihood of desired outcomes.\n\n## Skills Overview\n\n### Foundational Skills\n\nThese skills establish the foundational understanding required for all subsequent context engineering work.\n\n| Skill | Description |\n|-------|-------------|\n| [context-fundamentals](skills/context-fundamentals/) | Understand what context is, why it matters, and the anatomy of context in agent systems |\n| [context-degradation](skills/context-degradation/) | Recognize patterns of context failure: lost-in-middle, poisoning, distraction, and clash |\n| [context-compression](skills/context-compression/) | Design and evaluate compression strategies for long-running sessions |\n\n### Architectural Skills\n\nThese skills cover the patterns and structures for building effective agent systems.\n\n| Skill | Description |\n|-------|-------------|\n| [multi-agent-patterns](skills/multi-agent-patterns/) | Master orchestrator, peer-to-peer, and hierarchical multi-agent architectures |\n| [memory-systems](skills/memory-systems/) | Design short-term, long-term, and graph-based memory architectures |\n| [tool-design](skills/tool-design/) | Build tools that agents can use effectively |\n| [filesystem-context](skills/filesystem-context/) | Use filesystems for dynamic context discovery, tool output offloading, and plan persistence |\n| [hosted-agents](skills/hosted-agents/) | **NEW** Build background coding agents with sandboxed VMs, pre-built images, multiplayer support, and multi-client interfaces |\n\n### Operational Skills\n\nThese skills address the ongoing operation and optimization of agent systems.\n\n| Skill | Description |\n|-------|-------------|\n| [context-optimization](skills/context-optimization/) | Apply compaction, masking, and caching strategies |\n| [evaluation](skills/evaluation/) | Build evaluation frameworks for agent systems |\n| [advanced-evaluation](skills/advanced-evaluation/) | Master LLM-as-a-Judge techniques: direct scoring, pairwise comparison, rubric generation, and bias mitigation |\n\n### Development Methodology\n\nThese skills cover the meta-level practices for building LLM-powered projects.\n\n| Skill | Description |\n|-------|-------------|\n| [project-development](skills/project-development/) | Design and build LLM projects from ideation through deployment, including task-model fit analysis, pipeline architecture, and structured output design |\n\n### Cognitive Architecture Skills\n\nThese skills cover formal cognitive modeling for rational agent systems.\n\n| Skill | Description |\n|-------|-------------|\n| [bdi-mental-states](skills/bdi-mental-states/) | **NEW** Transform external RDF context into agent mental states (beliefs, desires, intentions) using formal BDI ontology patterns for deliberative reasoning and explainability |\n\n## Design Philosophy\n\n### Progressive Disclosure\n\nEach skill is structured for efficient context use. At startup, agents load only skill names and descriptions. Full content loads only when a skill is activated for relevant tasks.\n\n### Platform Agnosticism\n\nThese skills focus on transferable principles rather than vendor-specific implementations. The patterns work across Claude Code, Cursor, and any agent platform that supports skills or allows custom instructions.\n\n### Conceptual Foundation with Practical Examples\n\nScripts and examples demonstrate concepts using Python pseudocode that works across environments without requiring specific dependency installations.\n\n## Usage\n\n### Usage with Claude Code\n\nThis repository is a **Claude Code Plugin Marketplace** containing context engineering skills that Claude automatically discovers and activates based on your task context.\n\n### Installation\n\n**Step 1: Add the Marketplace**\n\nRun this command in Claude Code to register this repository as a plugin source:\n\n```\n/plugin marketplace add muratcankoylan/Agent-Skills-for-Context-Engineering\n```\n\n**Step 2: Browse and Install**\n\nOption A - Browse available ",
      "default_branch": "main"
    },
    "fetched_at": "2026-01-18T03:02:22.542255"
  },
  {
    "basic_info": {
      "name": "Continuous-Claude-v3",
      "full_name": "parcadei/Continuous-Claude-v3",
      "owner": "parcadei",
      "description": "Context management for Claude Code. Hooks maintain state via ledgers and handoffs. MCP execution without context pollution. Agent orchestration with isolated context windows.",
      "url": "https://github.com/parcadei/Continuous-Claude-v3",
      "clone_url": "https://github.com/parcadei/Continuous-Claude-v3.git",
      "ssh_url": "git@github.com:parcadei/Continuous-Claude-v3.git",
      "homepage": "",
      "created_at": "2025-12-23T00:12:49Z",
      "updated_at": "2026-01-18T02:22:18Z",
      "pushed_at": "2026-01-15T11:42:39Z"
    },
    "stats": {
      "stars": 3279,
      "forks": 237,
      "watchers": 3279,
      "open_issues": 11,
      "size": 1479
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 2040952,
        "TypeScript": 722358,
        "Lean": 271555,
        "Shell": 93778,
        "JavaScript": 70247
      },
      "license": "MIT License",
      "topics": [
        "agents",
        "claude-code",
        "claude-code-cli",
        "claude-code-hooks",
        "claude-code-mcp",
        "claude-code-skills",
        "claude-code-subagents",
        "claude-skills",
        "mcp"
      ]
    },
    "content": {
      "readme": "# Continuous Claude\n\n> A persistent, learning, multi-agent development environment built on Claude Code\n\n[![License: MIT](https://img.shields.io/badge/License-MIT-blue.svg)](LICENSE)\n[![Claude Code](https://img.shields.io/badge/Claude-Code-orange.svg)](https://claude.ai/code)\n[![Skills](https://img.shields.io/badge/Skills-109-green.svg)](#skills-system)\n[![Agents](https://img.shields.io/badge/Agents-32-purple.svg)](#agents-system)\n[![Hooks](https://img.shields.io/badge/Hooks-30-blue.svg)](#hooks-system)\n\n**Continuous Claude** transforms Claude Code into a continuously learning system that maintains context across sessions, orchestrates specialized agents, and eliminates wasting tokens through intelligent code analysis.\n\n## Table of Contents\n\n- [Why Continuous Claude?](#why-continuous-claude)\n- [Design Principles](#design-principles)\n- [How to Talk to Claude](#how-to-talk-to-claude)\n- [Quick Start](#quick-start)\n- [Architecture](#architecture)\n- [Core Systems](#core-systems)\n  - [Skills (109)](#skills-system)\n  - [Agents (32)](#agents-system)\n  - [Hooks (30)](#hooks-system)\n  - [TLDR Code Analysis](#tldr-code-analysis)\n  - [Memory System](#memory-system)\n  - [Continuity System](#continuity-system)\n  - [Math System](#math-system)\n- [Workflows](#workflows)\n- [Installation](#installation)\n- [Updating](#updating)\n- [Configuration](#configuration)\n- [Contributing](#contributing)\n- [License](#license)\n\n---\n\n## Why Continuous Claude?\n\nClaude Code has a **compaction problem**: when context fills up, the system compacts your conversation, losing nuanced understanding and decisions made during the session.\n\n**Continuous Claude solves this with:**\n\n| Problem | Solution |\n|---------|----------|\n| Context loss on compaction | YAML handoffs - more token-efficient transfer |\n| Starting fresh each session | Memory system recalls + daemon auto-extracts learnings |\n| Reading entire files burns tokens | 5-layer code analysis + semantic index |\n| Complex tasks need coordination | Meta-skills orchestrate agent workflows |\n| Repeating workflows manually | 109 skills with natural language triggers |\n\n**The mantra: Compound, don't compact.** Extract learnings automatically, then start fresh with full context.\n\n### Why \"Continuous\"? Why \"Compounding\"?\n\nThe name is a pun. **Continuous** because Claude maintains state across sessions. **Compounding** because each session makes the system smarterâ€”learnings accumulate like compound interest.\n\n---\n\n## Design Principles\n\nAn agent is five things: **Prompt + Tools + Context + Memory + Model**.\n\n| Component | What We Optimize |\n|-----------|------------------|\n| **Prompt** | Skills inject relevant context; hooks add system reminders |\n| **Tools** | TLDR reduces tokens; agents parallelize work |\n| **Context** | Not just *what* Claude knows, but *how* it's provided |\n| **Memory** | Daemon extracts learnings; recall surfaces them |\n| **Model** | Becomes swappable when the other four are solid |\n\n### Anti-Complexity\n\nWe resist plugin sprawl. Every MCP, subscription, and tool you add promises improvement but risks breaking context, tools, or prompts through clashes.\n\n**Our approach:**\n- **Time, not money** â€” No required paid services. Perplexity and NIA are optional, high-value-per-token.\n- **Learn, don't accumulate** â€” A system that learns handles edge cases better than one that collects plugins.\n- **Shift-left validation** â€” Hooks run pyright/ruff after edits, catching errors before tests.\n\nThe failure modes of complex systems are structurally invisible until they happen. A learning, context-efficient system doesn't prevent all failuresâ€”but it recovers and improves.\n\n---\n\n## How to Talk to Claude\n\n**You don't need to memorize slash commands.** Just describe what you want naturally.\n\n### The Skill Activation System\n\nWhen you send a message, a hook injects context that tells **Claude** which skills and agents are relevant. Claude infers from a rule-based system and decides which tools to use.\n\n```\n> \"Fix the login bug in auth.py\"\n\nğŸ¯ SKILL ACTIVATION CHECK\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n\nâš ï¸ CRITICAL SKILLS (REQUIRED):\n  â†’ create_handoff\n\nğŸ“š RECOMMENDED SKILLS:\n  â†’ fix\n  â†’ debug\n\nğŸ¤– RECOMMENDED AGENTS (token-efficient):\n  â†’ debug-agent\n  â†’ scout\n\nACTION: Use Skill tool BEFORE responding\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n```\n\n### Priority Levels\n\n| Level | Meaning |\n|-------|---------|\n| âš ï¸ **CRITICAL** | Must use (e.g., handoffs before ending session) |\n| ğŸ“š **RECOMMENDED** | Should use (e.g., workflow skills) |\n| ğŸ’¡ **SUGGESTED** | Consider using (e.g., optimization tools) |\n| ğŸ“Œ **OPTIONAL** | Nice to have (e.g., documentation helpers) |\n\n### Natural Language Examples\n\n| What You Say | What Activates |\n|--------------|----------------|\n| \"Fix the broken login\" | `/fix` workflow â†’ debug-agent, scout |\n| \"Build a user dashboard\" | `/build` workflow â†’ plan-agent, kraken |\n| \"I want to understand this codebase\" | `/explore` + scout agent |\n| \"What could go wrong with this plan?\" | `/premort",
      "default_branch": "main"
    },
    "fetched_at": "2026-01-18T03:02:23.676472"
  },
  {
    "basic_info": {
      "name": "Engram",
      "full_name": "deepseek-ai/Engram",
      "owner": "deepseek-ai",
      "description": "Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models",
      "url": "https://github.com/deepseek-ai/Engram",
      "clone_url": "https://github.com/deepseek-ai/Engram.git",
      "ssh_url": "git@github.com:deepseek-ai/Engram.git",
      "homepage": "",
      "created_at": "2026-01-12T05:26:50Z",
      "updated_at": "2026-01-18T02:52:07Z",
      "pushed_at": "2026-01-14T01:13:02Z"
    },
    "stats": {
      "stars": 2734,
      "forks": 154,
      "watchers": 2734,
      "open_issues": 8,
      "size": 2296
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 15017
      },
      "license": "Apache License 2.0",
      "topics": []
    },
    "content": {
      "readme": "<!-- markdownlint-disable first-line-h1 -->\n<!-- markdownlint-disable html -->\n<!-- markdownlint-disable no-duplicate-header -->\n\n<div align=\"center\">\n  <img src=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/logo.svg?raw=true\" width=\"60%\" alt=\"DeepSeek-V3\" />\n</div>\n<hr>\n<div align=\"center\" style=\"line-height: 1;\">\n  <a href=\"https://www.deepseek.com/\"><img alt=\"Homepage\"\n    src=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/badge.svg?raw=true\"/></a>\n  <a href=\"https://chat.deepseek.com/\"><img alt=\"Chat\"\n    src=\"https://img.shields.io/badge/ğŸ¤–%20Chat-DeepSeek%20V3-536af5?color=536af5&logoColor=white\"/></a>\n  <a href=\"https://huggingface.co/deepseek-ai\"><img alt=\"Hugging Face\"\n    src=\"https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-DeepSeek%20AI-ffc107?color=ffc107&logoColor=white\"/></a>\n  <br>\n  <a href=\"https://discord.gg/Tc7c45Zzu5\"><img alt=\"Discord\"\n    src=\"https://img.shields.io/badge/Discord-DeepSeek%20AI-7289da?logo=discord&logoColor=white&color=7289da\"/></a>\n  <a href=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/qr.jpeg?raw=true\"><img alt=\"Wechat\"\n    src=\"https://img.shields.io/badge/WeChat-DeepSeek%20AI-brightgreen?logo=wechat&logoColor=white\"/></a>\n  <a href=\"https://twitter.com/deepseek_ai\"><img alt=\"Twitter Follow\"\n    src=\"https://img.shields.io/badge/Twitter-deepseek_ai-white?logo=x&logoColor=white\"/></a>\n  <br>\n  <a href=\"LICENSE\" style=\"margin: 2px;\">\n    <img alt=\"License\" src=\"https://img.shields.io/badge/License-Apache 2.0-f5de53?&color=f5de53\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <br>\n</div>\n\n## 1. Introduction\n\nThis repository contains the official implementation for the paper: **[Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models](Engram_paper.pdf)**.\n\n> **Abstract:** While Mixture-of-Experts (MoE) scales capacity via conditional computation, Transformers lack a native primitive for knowledge lookup. To address this, we explore **conditional memory** as a complementary sparsity axis, instantiated via **Engram**, a module that modernizes classic $N$-gram embeddings for $\\mathcal{O}(1)$ lookup.\n\n**Key Contributions:**\n- **Sparsity Allocation:** We formulate the trade-off between neural computation (MoE) and static memory (Engram), identifying a U-shaped scaling law that guides optimal capacity allocation.\n- **Empirical Verification:** Under strict iso-parameter and iso-FLOPs constraints, the Engram-27B model demonstrates consistent improvements over MoE baselines across knowledge, reasoning, code and math domains.\n- **Mechanistic Analysis:** Our analysis suggests that Engram relieves early layers from static pattern reconstruction, potentially preserving effective depth for complex reasoning.\n- **System Efficiency:** The module employs deterministic addressing, enabling the offloading of massive embedding tables to host memory with minimal inference overhead.\n\n\n## 2. Architecture\n\nThe Engram module augments the backbone by retrieving static $N$-gram memory and fusing it with dynamic hidden states. The architecture is shown below ([drawio provided](drawio/Engram.drawio)):\n\n<p align=\"center\">\n  <img width=\"75%\" src=\"figures/arch.png\" alt=\"Engram Architecture\">\n</p>\n\n## 3. Evaluation\n\n### Scaling Law\n<p align=\"center\">\n  <img width=\"90%\" src=\"figures/scaling_law.png\" alt=\"Scaling Law\">\n</p>\n\n---\n\n### Large Scale Pre-training\n<p align=\"center\">\n  <img width=\"80%\" src=\"figures/27b_exp_results.png\" alt=\"Pre-training Results\">\n</p>\n\n---\n\n### Long-context Training\n<p align=\"center\">\n  <img width=\"80%\" src=\"figures/long_context_results.png\" alt=\"Long Context Results\">\n</p>\n\n\n## 4. Case Study of Engram\n<p align=\"center\">\n  <img width=\"80%\" src=\"figures/case.png\" alt=\"Long Context Results\">\n</p>\n\n## 5. Quick Start\n\nWe recommend using Python 3.8+ and PyTorch.\n```bash\npip install torch numpy transformers sympy\n```\nWe provide a standalone implementation to demonstrate the core logic of the Engram module:\n```bash\npython engram_demo_v1.py\n```\n\n> âš ï¸ **Note:** The provided code is a demonstration version intended to illustrate the data flow. It mocks standard components (like Attention/MoE/mHC) to focus on the Engram module. \n\n\n## 6. License\nThe use of Engram models is subject to [the Model License](LICENSE).\n\n## 7. Contact\n\nIf you have any questions, please raise an issue or contact us at [service@deepseek.com](mailto:service@deepseek.com).",
      "default_branch": "main"
    },
    "fetched_at": "2026-01-18T03:02:24.837332"
  },
  {
    "basic_info": {
      "name": "LTX-2",
      "full_name": "Lightricks/LTX-2",
      "owner": "Lightricks",
      "description": "Official Python inference and LoRA trainer package for the LTX-2 audioâ€“video generative model.",
      "url": "https://github.com/Lightricks/LTX-2",
      "clone_url": "https://github.com/Lightricks/LTX-2.git",
      "ssh_url": "git@github.com:Lightricks/LTX-2.git",
      "homepage": "https://ltx.io/model/ltx-2",
      "created_at": "2026-01-03T13:16:29Z",
      "updated_at": "2026-01-18T01:26:07Z",
      "pushed_at": "2026-01-15T19:40:05Z"
    },
    "stats": {
      "stars": 2657,
      "forks": 308,
      "watchers": 2657,
      "open_issues": 41,
      "size": 230
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 809295
      },
      "license": "Other",
      "topics": [
        "generative-ai",
        "ltx",
        "ltx-2"
      ]
    },
    "content": {
      "readme": "# LTX-2\n\n[![Website](https://img.shields.io/badge/Website-LTX-181717?logo=google-chrome)](https://ltx.io)\n[![Model](https://img.shields.io/badge/HuggingFace-Model-orange?logo=huggingface)](https://huggingface.co/Lightricks/LTX-2)\n[![Demo](https://img.shields.io/badge/Demo-Try%20Now-brightgreen?logo=vercel)](https://app.ltx.studio/ltx-2-playground/i2v)\n[![Paper](https://img.shields.io/badge/Paper-PDF-EC1C24?logo=adobeacrobatreader&logoColor=white)](https://arxiv.org/abs/2601.03233)\n[![Discord](https://img.shields.io/badge/Join-Discord-5865F2?logo=discord)](https://discord.gg/ltxplatform)\n\n**LTX-2** is the first DiT-based audio-video foundation model that contains all core capabilities of modern video generation in one model: synchronized audio and video, high fidelity, multiple performance modes, production-ready outputs, API access, and open access.\n\n<div align=\"center\">\n  <video src=\"https://github.com/user-attachments/assets/4414adc0-086c-43de-b367-9362eeb20228\" width=\"70%\" poster=\"\"> </video>\n</div>\n\n## ğŸš€ Quick Start\n\n```bash\n# Clone the repository\ngit clone https://github.com/Lightricks/LTX-2.git\ncd LTX-2\n\n# Set up the environment\nuv sync --frozen\nsource .venv/bin/activate\n```\n\n### Required Models\n\nDownload the following models from the [LTX-2 HuggingFace repository](https://huggingface.co/Lightricks/LTX-2):\n\n**LTX-2 Model Checkpoint** (choose and download one of the following)\n  * [`ltx-2-19b-dev-fp8.safetensors`](https://huggingface.co/Lightricks/LTX-2/blob/main/ltx-2-19b-dev-fp8.safetensors) - [Download](https://huggingface.co/Lightricks/LTX-2/resolve/main/ltx-2-19b-dev-fp8.safetensors)\n\n  * [`ltx-2-19b-dev.safetensors`](https://huggingface.co/Lightricks/LTX-2/blob/main/ltx-2-19b-dev.safetensors) - [Download](https://huggingface.co/Lightricks/LTX-2/resolve/main/ltx-2-19b-dev.safetensors)\n  * [`ltx-2-19b-distilled.safetensors`](https://huggingface.co/Lightricks/LTX-2/blob/main/ltx-2-19b-distilled.safetensors) - [Download](https://huggingface.co/Lightricks/LTX-2/resolve/main/ltx-2-19b-distilled.safetensors)\n  * [`ltx-2-19b-distilled-fp8.safetensors`](https://huggingface.co/Lightricks/LTX-2/blob/main/ltx-2-19b-distilled-fp8.safetensors) - [Download](https://huggingface.co/Lightricks/LTX-2/resolve/main/ltx-2-19b-distilled-fp8.safetensors)\n\n**Spatial Upscaler** - Required for current two-stage pipeline implementations in this repository\n  * [`ltx-2-spatial-upscaler-x2-1.0.safetensors`](https://huggingface.co/Lightricks/LTX-2/blob/main/ltx-2-spatial-upscaler-x2-1.0.safetensors) - [Download](https://huggingface.co/Lightricks/LTX-2/resolve/main/ltx-2-spatial-upscaler-x2-1.0.safetensors)\n\n**Temporal Upscaler** - Supported by the model and will be required for future pipeline implementations\n  * [`ltx-2-temporal-upscaler-x2-1.0.safetensors`](https://huggingface.co/Lightricks/LTX-2/blob/main/ltx-2-temporal-upscaler-x2-1.0.safetensors) - [Download](https://huggingface.co/Lightricks/LTX-2/resolve/main/ltx-2-temporal-upscaler-x2-1.0.safetensors)\n\n**Distilled LoRA** - Required for current two-stage pipeline implementations in this repository (except DistilledPipeline and ICLoraPipeline)\n  * [`ltx-2-19b-distilled-lora-384.safetensors`](https://huggingface.co/Lightricks/LTX-2/blob/main/ltx-2-19b-distilled-lora-384.safetensors) - [Download](https://huggingface.co/Lightricks/LTX-2/resolve/main/ltx-2-19b-distilled-lora-384.safetensors)\n\n**Gemma Text Encoder** (download all assets from the repository)\n  * [`Gemma 3`](https://huggingface.co/google/gemma-3-12b-it-qat-q4_0-unquantized/tree/main)\n\n**LoRAs**\n  * [`LTX-2-19b-IC-LoRA-Canny-Control`](https://huggingface.co/Lightricks/LTX-2-19b-IC-LoRA-Canny-Control) - [Download](https://huggingface.co/Lightricks/LTX-2-19b-IC-LoRA-Canny-Control/resolve/main/ltx-2-19b-ic-lora-canny-control.safetensors)\n  * [`LTX-2-19b-IC-LoRA-Depth-Control`](https://huggingface.co/Lightricks/LTX-2-19b-IC-LoRA-Depth-Control) - [Download](https://huggingface.co/Lightricks/LTX-2-19b-IC-LoRA-Depth-Control/resolve/main/ltx-2-19b-ic-lora-depth-control.safetensors)\n  * [`LTX-2-19b-IC-LoRA-Detailer`](https://huggingface.co/Lightricks/LTX-2-19b-IC-LoRA-Detailer) - [Download](https://huggingface.co/Lightricks/LTX-2-19b-IC-LoRA-Detailer/resolve/main/ltx-2-19b-ic-lora-detailer.safetensors)\n  * [`LTX-2-19b-IC-LoRA-Pose-Control`](https://huggingface.co/Lightricks/LTX-2-19b-IC-LoRA-Pose-Control) - [Download](https://huggingface.co/Lightricks/LTX-2-19b-IC-LoRA-Pose-Control/resolve/main/ltx-2-19b-ic-lora-pose-control.safetensors)\n  * [`LTX-2-19b-LoRA-Camera-Control-Dolly-In`](https://huggingface.co/Lightricks/LTX-2-19b-LoRA-Camera-Control-Dolly-In) - [Download](https://huggingface.co/Lightricks/LTX-2-19b-LoRA-Camera-Control-Dolly-In/resolve/main/ltx-2-19b-lora-camera-control-dolly-in.safetensors)\n  * [`LTX-2-19b-LoRA-Camera-Control-Dolly-Left`](https://huggingface.co/Lightricks/LTX-2-19b-LoRA-Camera-Control-Dolly-Left) - [Download](https://huggingface.co/Lightricks/LTX-2-19b-LoRA-Camera-Control-Dolly-Left/resolve/",
      "default_branch": "main"
    },
    "fetched_at": "2026-01-18T03:02:26.028311"
  },
  {
    "basic_info": {
      "name": "daily_stock_analysis",
      "full_name": "ZhuLinsen/daily_stock_analysis",
      "owner": "ZhuLinsen",
      "description": "LLMé©±åŠ¨çš„ A/Hè‚¡æ™ºèƒ½åˆ†æå™¨ï¼Œå¤šæ•°æ®æºè¡Œæƒ… + å®æ—¶æ–°é—» + Gemini å†³ç­–ä»ªè¡¨ç›˜ + å¤šæ¸ é“æ¨é€ï¼Œé›¶æˆæœ¬ï¼Œçº¯ç™½å«–ï¼Œå®šæ—¶è¿è¡Œ",
      "url": "https://github.com/ZhuLinsen/daily_stock_analysis",
      "clone_url": "https://github.com/ZhuLinsen/daily_stock_analysis.git",
      "ssh_url": "git@github.com:ZhuLinsen/daily_stock_analysis.git",
      "homepage": "",
      "created_at": "2026-01-10T06:43:20Z",
      "updated_at": "2026-01-18T03:00:43Z",
      "pushed_at": "2026-01-17T12:34:36Z"
    },
    "stats": {
      "stars": 2481,
      "forks": 2406,
      "watchers": 2481,
      "open_issues": 7,
      "size": 38652
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 407413,
        "Dockerfile": 1217
      },
      "license": "MIT License",
      "topics": [
        "agent",
        "ai",
        "aigc",
        "gemini",
        "llm",
        "quant",
        "quantitative-trading",
        "rag",
        "stock"
      ]
    },
    "content": {
      "readme": "# ğŸ“ˆ Aè‚¡æ™ºèƒ½åˆ†æç³»ç»Ÿ\n\n[![GitHub stars](https://img.shields.io/github/stars/ZhuLinsen/daily_stock_analysis?style=social)](https://github.com/ZhuLinsen/daily_stock_analysis/stargazers)\n[![CI](https://github.com/ZhuLinsen/daily_stock_analysis/actions/workflows/ci.yml/badge.svg)](https://github.com/ZhuLinsen/daily_stock_analysis/actions/workflows/ci.yml)\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)\n[![GitHub Actions](https://img.shields.io/badge/GitHub%20Actions-Ready-2088FF?logo=github-actions&logoColor=white)](https://github.com/features/actions)\n\n> ğŸ¤– åŸºäº AI å¤§æ¨¡å‹çš„ A/H è‚¡è‡ªé€‰è‚¡æ™ºèƒ½åˆ†æç³»ç»Ÿï¼Œæ¯æ—¥è‡ªåŠ¨åˆ†æå¹¶æ¨é€ã€Œå†³ç­–ä»ªè¡¨ç›˜ã€åˆ°ä¼ä¸šå¾®ä¿¡/é£ä¹¦/Telegram/é‚®ç®±\n\n![è¿è¡Œæ•ˆæœæ¼”ç¤º](./sources/all_2026-01-13_221547.gif)\n\n## âœ¨ åŠŸèƒ½ç‰¹æ€§\n\n### ğŸ¯ æ ¸å¿ƒåŠŸèƒ½\n- **AI å†³ç­–ä»ªè¡¨ç›˜** - ä¸€å¥è¯æ ¸å¿ƒç»“è®º + ç²¾ç¡®ä¹°å–ç‚¹ä½ + æ£€æŸ¥æ¸…å•\n- **å¤šç»´åº¦åˆ†æ** - æŠ€æœ¯é¢ + ç­¹ç åˆ†å¸ƒ + èˆ†æƒ…æƒ…æŠ¥ + å®æ—¶è¡Œæƒ…\n- **å¤§ç›˜å¤ç›˜** - æ¯æ—¥å¸‚åœºæ¦‚è§ˆã€æ¿å—æ¶¨è·Œã€åŒ—å‘èµ„é‡‘\n- **å¤šæ¸ é“æ¨é€** - æ”¯æŒä¼ä¸šå¾®ä¿¡ã€é£ä¹¦ã€Telegramã€é‚®ä»¶ï¼ˆè‡ªåŠ¨è¯†åˆ«ï¼‰\n- **é›¶æˆæœ¬éƒ¨ç½²** - GitHub Actions å…è´¹è¿è¡Œï¼Œæ— éœ€æœåŠ¡å™¨\n- **ğŸ’° ç™½å«– Gemini API** - Google AI Studio æä¾›å…è´¹é¢åº¦ï¼Œä¸ªäººä½¿ç”¨å®Œå…¨å¤Ÿç”¨\n- **ğŸ”„ å¤šæ¨¡å‹æ”¯æŒ** - æ”¯æŒ OpenAI å…¼å®¹ APIï¼ˆDeepSeekã€é€šä¹‰åƒé—®ç­‰ï¼‰ä½œä¸ºå¤‡é€‰\n\n### ğŸ“Š æ•°æ®æ¥æº\n- **è¡Œæƒ…æ•°æ®**: AkShareï¼ˆå…è´¹ï¼‰ã€Tushareã€Baostockã€YFinance\n- **æ–°é—»æœç´¢**: Tavilyã€SerpAPIã€Bocha\n- **AI åˆ†æ**: \n  - ä¸»åŠ›ï¼šGoogle Geminiï¼ˆgemini-3-flash-previewï¼‰â€”â€” [å…è´¹è·å–](https://aistudio.google.com/)\n  - å¤‡é€‰ï¼šåº”å¤§å®¶è¦æ±‚ï¼Œä¹Ÿæ”¯æŒäº†OpenAI å…¼å®¹ APIï¼ˆDeepSeekã€é€šä¹‰åƒé—®ã€Moonshot ç­‰ï¼‰\n\n### ğŸ›¡ï¸ äº¤æ˜“ç†å¿µå†…ç½®\n- âŒ **ä¸¥ç¦è¿½é«˜** - ä¹–ç¦»ç‡ > 5% è‡ªåŠ¨æ ‡è®°ã€Œå±é™©ã€\n- âœ… **è¶‹åŠ¿äº¤æ˜“** - MA5 > MA10 > MA20 å¤šå¤´æ’åˆ—\n- ğŸ“ **ç²¾ç¡®ç‚¹ä½** - ä¹°å…¥ä»·ã€æ­¢æŸä»·ã€ç›®æ ‡ä»·\n- ğŸ“‹ **æ£€æŸ¥æ¸…å•** - æ¯é¡¹æ¡ä»¶ç”¨ âœ…âš ï¸âŒ æ ‡è®°\n\n## ğŸš€ å¿«é€Ÿå¼€å§‹\n\n### æ–¹å¼ä¸€ï¼šGitHub Actionsï¼ˆæ¨èï¼Œé›¶æˆæœ¬ï¼‰\n\n**æ— éœ€æœåŠ¡å™¨ï¼Œæ¯å¤©è‡ªåŠ¨è¿è¡Œï¼**\n\n#### 1. Fork æœ¬ä»“åº“\n\nç‚¹å‡»å³ä¸Šè§’ `Fork` æŒ‰é’®\n\n#### 2. é…ç½® Secrets\n\nè¿›å…¥ä½  Fork çš„ä»“åº“ â†’ `Settings` â†’ `Secrets and variables` â†’ `Actions` â†’ `New repository secret`\n\n**AI æ¨¡å‹é…ç½®ï¼ˆäºŒé€‰ä¸€ï¼‰**\n\n| Secret åç§° | è¯´æ˜ | å¿…å¡« |\n|------------|------|:----:|\n| `GEMINI_API_KEY` | [Google AI Studio](https://aistudio.google.com/) è·å–å…è´¹ Key | âœ…* |\n| `OPENAI_API_KEY` | OpenAI å…¼å®¹ API Keyï¼ˆæ”¯æŒ DeepSeekã€é€šä¹‰åƒé—®ç­‰ï¼‰ | å¯é€‰ |\n| `OPENAI_BASE_URL` | OpenAI å…¼å®¹ API åœ°å€ï¼ˆå¦‚ `https://api.deepseek.com/v1`ï¼‰ | å¯é€‰ |\n| `OPENAI_MODEL` | æ¨¡å‹åç§°ï¼ˆå¦‚ `deepseek-chat`ï¼‰ | å¯é€‰ |\n\n> *æ³¨ï¼š`GEMINI_API_KEY` å’Œ `OPENAI_API_KEY` è‡³å°‘é…ç½®ä¸€ä¸ª\n\n**é€šçŸ¥æ¸ é“é…ç½®ï¼ˆå¯åŒæ—¶é…ç½®å¤šä¸ªï¼Œå…¨éƒ¨æ¨é€ï¼‰**\n\n| Secret åç§° | è¯´æ˜ | å¿…å¡« |\n|------------|------|:----:|\n| `WECHAT_WEBHOOK_URL` | ä¼ä¸šå¾®ä¿¡ Webhook URL | å¯é€‰ |\n| `FEISHU_WEBHOOK_URL` | é£ä¹¦ Webhook URL | å¯é€‰ |\n| `TELEGRAM_BOT_TOKEN` | Telegram Bot Tokenï¼ˆ@BotFather è·å–ï¼‰ | å¯é€‰ |\n| `TELEGRAM_CHAT_ID` | Telegram Chat ID | å¯é€‰ |\n| `EMAIL_SENDER` | å‘ä»¶äººé‚®ç®±ï¼ˆå¦‚ `xxx@qq.com`ï¼‰ | å¯é€‰ |\n| `EMAIL_PASSWORD` | é‚®ç®±æˆæƒç ï¼ˆéç™»å½•å¯†ç ï¼‰ | å¯é€‰ |\n| `EMAIL_RECEIVERS` | æ”¶ä»¶äººé‚®ç®±ï¼ˆå¤šä¸ªç”¨é€—å·åˆ†éš”ï¼Œç•™ç©ºåˆ™å‘ç»™è‡ªå·±ï¼‰ | å¯é€‰ |\n| `CUSTOM_WEBHOOK_URLS` | è‡ªå®šä¹‰ Webhookï¼ˆæ”¯æŒé’‰é’‰ç­‰ï¼Œå¤šä¸ªç”¨é€—å·åˆ†éš”ï¼‰ | å¯é€‰ |\n| `CUSTOM_WEBHOOK_BEARER_TOKEN` | è‡ªå®šä¹‰ Webhook çš„ Bearer Tokenï¼ˆç”¨äºéœ€è¦è®¤è¯çš„ Webhookï¼‰ | å¯é€‰ |\n| `SINGLE_STOCK_NOTIFY` | å•è‚¡æ¨é€æ¨¡å¼ï¼šè®¾ä¸º `true` åˆ™æ¯åˆ†æå®Œä¸€åªè‚¡ç¥¨ç«‹å³æ¨é€ | å¯é€‰ |\n\n> *æ³¨ï¼šè‡³å°‘é…ç½®ä¸€ä¸ªæ¸ é“ï¼Œé…ç½®å¤šä¸ªåˆ™åŒæ—¶æ¨é€\n>\n> ğŸ“– æ›´å¤šé…ç½®ï¼ˆPushover æ‰‹æœºæ¨é€ã€é£ä¹¦äº‘æ–‡æ¡£ç­‰ï¼‰è¯·å‚è€ƒ [å®Œæ•´é…ç½®æŒ‡å—](docs/full-guide.md)\n\n**å…¶ä»–é…ç½®**\n\n| Secret åç§° | è¯´æ˜ | å¿…å¡« |\n|------------|------|:----:|\n| `STOCK_LIST` | è‡ªé€‰è‚¡ä»£ç ï¼Œå¦‚ `600519,300750,002594` | âœ… |\n| `TAVILY_API_KEYS` | [Tavily](https://tavily.com/) æœç´¢ APIï¼ˆæ–°é—»æœç´¢ï¼‰ | æ¨è |\n| `BOCHA_API_KEYS` | [åšæŸ¥æœç´¢](https://open.bocha.cn/) Web Search APIï¼ˆä¸­æ–‡æœç´¢ä¼˜åŒ–ï¼Œæ”¯æŒAIæ‘˜è¦ï¼Œå¤šä¸ªkeyç”¨é€—å·åˆ†éš”ï¼‰ | å¯é€‰ |\n| `SERPAPI_API_KEYS` | [SerpAPI](https://serpapi.com/) å¤‡ç”¨æœç´¢ | å¯é€‰ |\n| `TUSHARE_TOKEN` | [Tushare Pro](https://tushare.pro/) Token | å¯é€‰ |\n\n#### 3. å¯ç”¨ Actions\n\nè¿›å…¥ `Actions` æ ‡ç­¾ â†’ ç‚¹å‡» `I understand my workflows, go ahead and enable them`\n\n#### 4. æ‰‹åŠ¨æµ‹è¯•\n\n`Actions` â†’ `æ¯æ—¥è‚¡ç¥¨åˆ†æ` â†’ `Run workflow` â†’ é€‰æ‹©æ¨¡å¼ â†’ `Run workflow`\n\n#### 5. å®Œæˆï¼\n\né»˜è®¤æ¯ä¸ªå·¥ä½œæ—¥ **18:00ï¼ˆåŒ—äº¬æ—¶é—´ï¼‰** è‡ªåŠ¨æ‰§è¡Œ\n\n### æ–¹å¼äºŒï¼šæœ¬åœ°è¿è¡Œ / Docker éƒ¨ç½²\n\n> ğŸ“– æœ¬åœ°è¿è¡Œã€Docker éƒ¨ç½²è¯¦ç»†æ­¥éª¤è¯·å‚è€ƒ [å®Œæ•´é…ç½®æŒ‡å—](docs/full-guide.md)\n\n## ğŸ“± æ¨é€æ•ˆæœ\n\n### å†³ç­–ä»ªè¡¨ç›˜\n```\nğŸ“Š 2026-01-10 å†³ç­–ä»ªè¡¨ç›˜\n3åªè‚¡ç¥¨ | ğŸŸ¢ä¹°å…¥:1 ğŸŸ¡è§‚æœ›:2 ğŸ”´å–å‡º:0\n\nğŸŸ¢ ä¹°å…¥ | è´µå·èŒ…å°(600519)\nğŸ“Œ ç¼©é‡å›è¸©MA5æ”¯æ’‘ï¼Œä¹–ç¦»ç‡1.2%å¤„äºæœ€ä½³ä¹°ç‚¹\nğŸ’° ç‹™å‡»: ä¹°å…¥1800 | æ­¢æŸ1750 | ç›®æ ‡1900\nâœ…å¤šå¤´æ’åˆ— âœ…ä¹–ç¦»å®‰å…¨ âœ…é‡èƒ½é…åˆ\n\nğŸŸ¡ è§‚æœ› | å®å¾·æ—¶ä»£(300750)\nğŸ“Œ ä¹–ç¦»ç‡7.8%è¶…è¿‡5%è­¦æˆ’çº¿ï¼Œä¸¥ç¦è¿½é«˜\nâš ï¸ ç­‰å¾…å›è°ƒè‡³MA5é™„è¿‘å†è€ƒè™‘\n\n---\nç”Ÿæˆæ—¶é—´: 18:00\n```\n\n### å¤§ç›˜å¤ç›˜\n\n![å¤§ç›˜å¤ç›˜æ¨é€æ•ˆæœ](./sources/dapan_2026-01-13_22-14-52.png)\n\n```\nğŸ¯ 2026-01-10 å¤§ç›˜å¤ç›˜\n\nğŸ“Š ä¸»è¦æŒ‡æ•°\n- ä¸Šè¯æŒ‡æ•°: 3250.12 (ğŸŸ¢+0.85%)\n- æ·±è¯æˆæŒ‡: 10521.36 (ğŸŸ¢+1.02%)\n- åˆ›ä¸šæ¿æŒ‡: 2156.78 (ğŸŸ¢+1.35%)\n\nğŸ“ˆ å¸‚åœºæ¦‚å†µ\nä¸Šæ¶¨: 3920 | ä¸‹è·Œ: 1349 | æ¶¨åœ: 155 | è·Œåœ: 3\n\nğŸ”¥ æ¿å—è¡¨ç°\né¢†æ¶¨: äº’è”ç½‘æœåŠ¡ã€æ–‡åŒ–ä¼ åª’ã€å°é‡‘å±\né¢†è·Œ: ä¿é™©ã€èˆªç©ºæœºåœºã€å…‰ä¼è®¾å¤‡\n```\n\n## âš™ï¸ é…ç½®è¯´æ˜\n\n> ğŸ“– å®Œæ•´ç¯å¢ƒå˜é‡ã€å®šæ—¶ä»»åŠ¡é…ç½®è¯·å‚è€ƒ [å®Œæ•´é…ç½®æŒ‡å—](docs/full-guide.md)\n## ğŸ“ é¡¹ç›®ç»“æ„\n\n```\ndaily_stock_analysis/\nâ”œâ”€â”€ main.py              # ä¸»ç¨‹åºå…¥å£\nâ”œâ”€â”€ analyzer.py          # AI åˆ†æå™¨ï¼ˆGeminiï¼‰\nâ”œâ”€â”€ market_analyzer.py   # å¤§ç›˜å¤ç›˜åˆ†æ\nâ”œâ”€â”€ search_service.py    # æ–°é—»æœç´¢æœåŠ¡\nâ”œâ”€â”€ notification.py      # æ¶ˆæ¯æ¨é€\nâ”œâ”€â”€ scheduler.py         # å®šæ—¶ä»»åŠ¡\nâ”œâ”€â”€ storage.py           # æ•°æ®å­˜å‚¨\nâ”œâ”€â”€ config.py            # é…ç½®ç®¡ç†\nâ”œâ”€â”€ data_provider/       # æ•°æ®æºé€‚é…å™¨\nâ”‚   â”œâ”€â”€ akshare_fetcher.py\nâ”‚   â”œâ”€â”€ tushare_fetcher.py\nâ”‚   â”œâ”€â”€ baostock_fetcher.py\nâ”‚   â””â”€â”€ yfinance_fetcher.py\nâ”œâ”€â”€ .github/workflows/   # GitHub Actions\nâ”œâ”€â”€ Dockerfile           # Docker é•œåƒ\nâ””â”€â”€ docker-compose.yml   # Docker ç¼–æ’\n```\n\n## ğŸ—ºï¸ Roadmap\n\n> ğŸ“¢ ä»¥ä¸‹åŠŸèƒ½å°†è§†åç»­æƒ…å†µé€æ­¥å®Œæˆï¼Œå¦‚æœä½ æœ‰å¥½çš„æƒ³æ³•æˆ–å»ºè®®ï¼Œæ¬¢è¿ [æäº¤ Issue](https://github.com/ZhuLinsen/daily_stock_analysis/issues) è®¨è®ºï¼\n\n### ğŸ”” é€šçŸ¥æ¸ é“æ‰©å±•\n- [x] ä¼ä¸šå¾®ä¿¡æœºå™¨äºº\n- [x] é£ä¹¦æœºå™¨äºº\n- [x] Telegram Bot\n- [x] é‚®ä»¶é€šçŸ¥ï¼ˆSMTPï¼‰\n- [x] è‡ªå®šä¹‰ Webhookï¼ˆæ”¯æŒé’‰é’‰ã€Discordã€Slackã€Bark ç­‰ï¼‰\n",
      "default_branch": "main"
    },
    "fetched_at": "2026-01-18T03:02:27.153233"
  },
  {
    "basic_info": {
      "name": "ucp",
      "full_name": "Universal-Commerce-Protocol/ucp",
      "owner": "Universal-Commerce-Protocol",
      "description": "Specification and documentation for the Universal Commerce Protocol (UCP)",
      "url": "https://github.com/Universal-Commerce-Protocol/ucp",
      "clone_url": "https://github.com/Universal-Commerce-Protocol/ucp.git",
      "ssh_url": "git@github.com:Universal-Commerce-Protocol/ucp.git",
      "homepage": "https://ucp.dev",
      "created_at": "2025-12-31T02:17:21Z",
      "updated_at": "2026-01-18T02:54:07Z",
      "pushed_at": "2026-01-17T06:10:26Z"
    },
    "stats": {
      "stars": 1925,
      "forks": 195,
      "watchers": 1925,
      "open_issues": 31,
      "size": 5386
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 76520,
        "TypeScript": 49627,
        "JavaScript": 5010,
        "Shell": 1988
      },
      "license": "Apache License 2.0",
      "topics": []
    },
    "content": {
      "readme": "<!--\n   Copyright 2026 UCP Authors\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n-->\n\n<p align=\"center\">\n  <h1 align=\"center\">Universal Commerce Protocol (UCP)</h1>\n</p>\n\n<p align=\"center\">\n  <b>An open standard enabling interoperability between various commerce\n   entities to facilitate seamless commerce integrations.</b>\n</p>\n\n<p align=\"center\">\n  <a href=\"https://ucp.dev\">Documentation</a> |\n  <a href=\"https://ucp.dev/specification/overview\">Specification</a> |\n  <a href=\"https://github.com/Universal-Commerce-Protocol/ucp/discussions\">Discussions</a>\n</p>\n\n## Overview\n\nThe Universal Commerce Protocol (UCP) addresses a fragmented commerce landscape\nby providing a standardized common language and functional primitives. It\nenables platforms (like AI agents and apps), businesses, Payment Service\nProviders (PSPs), and Credential Providers (CPs) to communicate effectively,\nensuring secure and consistent commerce experiences across the web.\n\nWith UCP, businesses can:\n\n*   **Declare** supported capabilities to enable autonomous discovery by\n    platforms.\n*   **Facilitate** secure checkout sessions, with or without human intervention.\n*   **Offer** personalized shopping experiences through standardized data\n    exchange.\n\n## Why UCP?\n\nAs commerce becomes increasingly agentic and distributed, the ability for\ndifferent systems to interoperate without custom, one-off integrations is vital.\nUCP aims to:\n\n*   **Standardize Interaction:** Provide a uniform way for platforms to interact\n    with businesses, regardless of the underlying backend.\n*   **Modularize Commerce:** Breakdown commerce into distinct **Capabilities**\n    (e.g., Checkout, Order) and **Extensions** (e.g., Discounts,\n    Fulfillment), allowing for flexible implementation.\n*   **Enable Agentic Commerce:** Designed from the ground up to support AI\n    agents acting on behalf of users to discover products, fill carts, and\n    complete purchases securely.\n*   **Enhance Security:** Support for advanced security patterns like AP2\n    mandates and verifiable credentials.\n\n### Key Features\n\n*   **Composable Architecture:** UCP defines **Capabilities** (such as\n    \"Checkout\" or \"Identity Linking\") that businesses implement to enable easy\n    integration. On top of that, specific **Extensions** can be added to enhance\n    the consumer experience without bloating the capability definitions.\n*   **Dynamic Discovery:** Businesses declare their supported Capabilities in a\n    standardized profile, allowing platforms to autonomously discover and\n    configure themselves.\n*   **Transport Agnostic:** The protocol is designed to work across various\n    transports. Businesses can offer Capabilities via REST APIs, MCP (Model\n    Context Protocol), or A2A, depending on their infrastructure.\n*   **Built on Standards:** UCP leverages existing open standards for payments,\n    identity, and security wherever applicable, rather than reinventing the\n    wheel.\n*   **Developer Friendly:** A comprehensive set of SDKs and libraries\n    facilitates rapid development and integration.\n\n## Key Capabilities\n\nThe initial release focuses on the essential primitives for transacting:\n\n*   **Checkout:** Facilitates checkout sessions including cart management and\n    tax calculation, supporting flows with or without human intervention.\n*   **Identity Linking:** Enables platforms to obtain authorization to perform\n    actions on a user's behalf via OAuth 2.0.\n*   **Order:** Webhook-based updates for order lifecycle events (shipped,\n    delivered, returned).\n*   **Payment Token Exchange:** Protocols for PSPs and Credential Providers to\n    securely exchange payment tokens and credentials.\n\n## Getting Started\n\n*   ğŸ“š **Explore the Documentation:** Visit [ucp.dev](https://ucp.dev) for a\n    complete overview, the full protocol specification, tutorials, and guides.\n*   ğŸ¬ **Review our\n    [samples](https://github.com/Universal-Commerce-Protocol/samples)** for\n    implementation examples.\n*   ğŸ› ï¸ **Use our\n    [SDKs](https://github.com/orgs/Universal-Commerce-Protocol/repositories)**\n    to start building your own integrations.\n*   ğŸ“ **Check conformance** with our [conformance tests](https://github.com/Universal-Commerce-Protocol/conformance).\n\n## Contributing\n\nWe welcome community contributions to enhance and evolve UCP.\n\n*   **Questions & Discussions:** Join our [GitHub\n    Discussions](https://github.com/Universal-Commerce-Protocol/ucp/discussions).\n*   **Issues & Feedback:** Report issues or sugges",
      "default_branch": "main"
    },
    "fetched_at": "2026-01-18T03:02:28.289970"
  },
  {
    "basic_info": {
      "name": "HY-Motion-1.0",
      "full_name": "Tencent-Hunyuan/HY-Motion-1.0",
      "owner": "Tencent-Hunyuan",
      "description": "HY-Motion model for 3D character animation generation. ",
      "url": "https://github.com/Tencent-Hunyuan/HY-Motion-1.0",
      "clone_url": "https://github.com/Tencent-Hunyuan/HY-Motion-1.0.git",
      "ssh_url": "git@github.com:Tencent-Hunyuan/HY-Motion-1.0.git",
      "homepage": "https://hunyuan.tencent.com/motion",
      "created_at": "2025-12-29T11:09:18Z",
      "updated_at": "2026-01-18T02:46:46Z",
      "pushed_at": "2026-01-04T03:45:23Z"
    },
    "stats": {
      "stars": 1801,
      "forks": 130,
      "watchers": 1801,
      "open_issues": 13,
      "size": 20527
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 285533,
        "HTML": 40286
      },
      "license": "Other",
      "topics": []
    },
    "content": {
      "readme": "[ä¸­æ–‡é˜…è¯»](README_zh_cn.md)\n\n\n<p align=\"center\">\n  <img src=\"./assets/banner.png\" alt=\"Banner\" width=\"100%\">\n</p>\n\n<div align=\"center\">\n  <a href=\"https://hunyuan.tencent.com/motion\" target=\"_blank\">\n    <img src=\"https://img.shields.io/badge/Official%20Site-333399.svg?logo=homepage\" height=\"22px\" alt=\"Official Site\">\n  </a>\n  <a href=\"https://github.com/Tencent-Hunyuan/HY-Motion-1.0\" target=\"_blank\">\n    <img src=\"https://img.shields.io/badge/GitHub-Repo-181717?logo=github&logoColor=white\" height=\"22px\" alt=\"Github Repo\">\n  </a>\n  <a href=\"https://huggingface.co/spaces/tencent/HY-Motion-1.0\" target=\"_blank\">\n    <img src=\"https://img.shields.io/badge/%F0%9F%A4%97%20Demo-276cb4.svg\" height=\"22px\" alt=\"HuggingFace Space\">\n  </a>\n  <a href=\"https://huggingface.co/tencent/HY-Motion-1.0\" target=\"_blank\">\n    <img src=\"https://img.shields.io/badge/%F0%9F%A4%97%20Models-d96902.svg\" height=\"22px\" alt=\"HuggingFace Models\">\n  </a>\n  <a href=\"https://arxiv.org/pdf/2512.23464\" target=\"_blank\">\n    <img src=\"https://img.shields.io/badge/Report-b5212f.svg?logo=arxiv\" height=\"22px\" alt=\"ArXiv Report\">\n  </a>\n  <a href=\"https://x.com/TencentHunyuan\" target=\"_blank\">\n    <img src=\"https://img.shields.io/badge/Hunyuan-black.svg?logo=x\" height=\"22px\" alt=\"X (Twitter)\">\n  </a>\n</div>\n\n\n# HY-Motion 1.0: Scaling Flow Matching Models for 3D Motion Generation\n\n\n<p align=\"center\">\n  <img src=\"./assets/teaser.jpg\" alt=\"Teaser\" width=\"100%\">\n</p>\n\n\n## ğŸ”¥ News\n- **Dec 30, 2025**: ğŸ¤— We released the inference code and pretrained models of [HY-Motion 1.0](https://huggingface.co/tencent/HY-Motion-1.0). Please give it a try via our [HuggingFace Space](https://huggingface.co/spaces/tencent/HY-Motion-1.0) and our [Official Site](https://hunyuan.tencent.com/motion)!\n\n\n## **Introduction**\n\n**HY-Motion 1.0** is a series of text-to-3D human motion generation models based on Diffusion Transformer (DiT) and Flow Matching. It allows developers to generate skeleton-based 3D character animations from simple text prompts, which can be directly integrated into various 3D animation pipelines. This model series is the first to scale DiT-based text-to-motion models to the billion-parameter level, achieving significant improvements in instruction-following capabilities and motion quality over existing open-source models.\n\n### Key Features\n- **State-of-the-Art Performance**: Achieves state-of-the-art performance in both instruction-following capability and generated motion quality.\n\n- **Billion-Scale Models**: We are the first to successfully scale DiT-based models to the billion-parameter level for text-to-motion generation. This results in superior instruction understanding and following capabilities, outperforming comparable open-source models.\n\n- **Advanced Three-Stage Training**: Our models are trained using a comprehensive three-stage process:\n\n    - *Large-Scale Pre-training*: Trained on over 3,000 hours of diverse motion data to learn a broad motion prior.\n\n    - *High-Quality Fine-tuning*: Fine-tuned on 400 hours of curated, high-quality 3D motion data to enhance motion detail and smoothness.\n\n    - *Reinforcement Learning*: Utilizes Reinforcement Learning from human feedback and reward models to further refine instruction-following and motion naturalness.\n\n\n\n<p align=\"center\">\n  <img src=\"./assets/pipeline.png\" alt=\"System Overview\" width=\"100%\">\n</p>\n\n<p align=\"center\">\n  <img src=\"./assets/arch.png\" alt=\"Architecture\" width=\"100%\">\n</p>\n\n<p align=\"center\">\n  <img src=\"./assets/sotacomp.jpg\" alt=\"ComparisonSoTA\" width=\"100%\">\n</p>\n\n\n\n\n## ğŸ Model Zoo\n\n**HY-Motion 1.0 Series**\n\n| Model | Description | Date | Size | Huggingface | VRAM (min) |\n|:-------|:-------------|:------:|:------:|:-------------:|:-------------:|\n| **HY-Motion-1.0** | Standard Text2Motion Model | 2025-12-30 | 1.0B | [Download](https://huggingface.co/tencent/HY-Motion-1.0/tree/main/HY-Motion-1.0) | 26GB |\n| **HY-Motion-1.0-Lite** | Lightweight Text2Motion Model | 2025-12-30 | 0.46B | [Download](https://huggingface.co/tencent/HY-Motion-1.0/tree/main/HY-Motion-1.0-Lite) | 24GB |\n\n*Note*: To reduce GPU VRAM requirements, please use the following settings: `--num_seeds=1`, text prompt with less than 30 words, and motion length less than 5 seconds.  \n*Note*: This table does not includes GPU VRAM requirements for LLM-based prompt engineering feature. If you have sufficient VRAM to run HY-Motion-1.0 model but gradio fails with a VRAM-related error, Run the Gradio application with prompt engineering disabled by setting the environment variable like this: `DISABLE_PROMPT_ENGINEERING=True python3 gradio_app.py`\n\n## ğŸ¤— Get Started with HY-Motion 1.0\n\nHY-Motion 1.0 supports macOS, Windows, and Linux.\n\n\n- [Code Usage (CLI)](#code-usage-cli)\n- [Gradio App](#gradio-app)\n\n\n#### 1. Installation\n\nFirst, install PyTorch via the [official site](https://pytorch.org/). Then install the dependencies:\n\n```bash\ngit clone https://github.com/Tencent-Hunyuan/HY-Motion-1.0.git\ncd HY-Motion-1.0/\n# Make sure git-lf",
      "default_branch": "master"
    },
    "fetched_at": "2026-01-18T03:02:29.430094"
  },
  {
    "basic_info": {
      "name": "pocket-tts",
      "full_name": "kyutai-labs/pocket-tts",
      "owner": "kyutai-labs",
      "description": "A TTS that fits in your CPU (and pocket)",
      "url": "https://github.com/kyutai-labs/pocket-tts",
      "clone_url": "https://github.com/kyutai-labs/pocket-tts.git",
      "ssh_url": "git@github.com:kyutai-labs/pocket-tts.git",
      "homepage": null,
      "created_at": "2026-01-07T17:33:32Z",
      "updated_at": "2026-01-18T02:59:44Z",
      "pushed_at": "2026-01-17T14:55:57Z"
    },
    "stats": {
      "stars": 1546,
      "forks": 171,
      "watchers": 1546,
      "open_issues": 27,
      "size": 419
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 122631,
        "HTML": 14508,
        "Dockerfile": 312
      },
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "# Pocket TTS\n\n<img width=\"1446\" height=\"622\" alt=\"pocket-tts-logo-v2-transparent\" src=\"https://github.com/user-attachments/assets/637b5ed6-831f-4023-9b4c-741be21ab238\" />\n\nA lightweight text-to-speech (TTS) application designed to run efficiently on CPUs.\nForget about the hassle of using GPUs and web APIs serving TTS models. With Kyutai's Pocket TTS, generating audio is just a pip install and a function call away.\n\nSupports Python 3.10, 3.11, 3.12, 3.13 and 3.14. Requires PyTorch 2.5+. Does not require the gpu version of PyTorch.\n\n[ğŸ”Š Demo](https://kyutai.org/tts) | \n[ğŸ±â€ğŸ’»GitHub Repository](https://github.com/kyutai-labs/pocket-tts) | \n[ğŸ¤— Hugging Face Model Card](https://huggingface.co/kyutai/pocket-tts) | \n[ğŸ“„ Paper](https://arxiv.org/abs/2509.06926) | \n[ğŸ“š Documentation](https://github.com/kyutai-labs/pocket-tts/tree/main/docs)\n\n\n## Main takeaways\n* Runs on CPU\n* Small model size, 100M parameters\n* Audio streaming\n* Low latency, ~200ms to get the first audio chunk\n* Faster than real-time, ~6x real-time on a CPU of MacBook Air M4\n* Uses only 2 CPU cores\n* Python API and CLI\n* Voice cloning\n* English only at the moment\n* Can handle infinitely long text inputs\n\n## Trying it from the website, without installing anything\n\nNavigate to the [Kyutai website](https://kyutai.org/tts) to try it out directly in your browser. You can input text, select different voices, and generate speech without any installation.\n\n## Trying it with the CLI\n\n### The `generate` command\nYou can use pocket-tts directly from the command line. We recommend using\n`uv` as it installs any dependencies on the fly in an isolated environment (uv installation instructions [here](https://docs.astral.sh/uv/getting-started/installation/#standalone-installer)).\nYou can also use `pip install pocket-tts` to install it manually.\n\nThis will generate a wav file `./tts_output.wav` saying the default text with the default voice, and display some speed statistics.\n```bash\nuvx pocket-tts generate\n# or if you installed it manually with pip:\npocket-tts generate\n```\nModify the voice with `--voice` and the text with `--text`. We provide a small catalog of voices.\n\nYou can take a look at [this page](https://huggingface.co/kyutai/tts-voices) which details the licenses\nfor each voice.\n\n* [alba](https://huggingface.co/kyutai/tts-voices/blob/main/alba-mackenna/casual.wav)\n* [marius](https://huggingface.co/kyutai/tts-voices/blob/main/voice-donations/Selfie.wav)\n* [javert](https://huggingface.co/kyutai/tts-voices/blob/main/voice-donations/Butter.wav)\n* [jean](https://huggingface.co/kyutai/tts-voices/blob/main/ears/p010/freeform_speech_01.wav)\n* [fantine](https://huggingface.co/kyutai/tts-voices/blob/main/vctk/p244_023.wav)\n* [cosette](https://huggingface.co/kyutai/tts-voices/blob/main/expresso/ex04-ex02_confused_001_channel1_499s.wav)\n* [eponine](https://huggingface.co/kyutai/tts-voices/blob/main/vctk/p262_023.wav)\n* [azelma](https://huggingface.co/kyutai/tts-voices/blob/main/vctk/p303_023.wav)\n\nThe `--voice` argument can also take a plain wav file as input for voice cloning.\nFeel free to check out the [generate documentation](https://github.com/kyutai-labs/pocket-tts/tree/main/docs/generate.md) for more details and examples.\nFor trying multiple voices and prompts quickly, prefer using the `serve` command.\n\n### The `serve` command\n\nYou can also run a local server to generate audio via HTTP requests.\n```bash\nuvx pocket-tts serve\n# or if you installed it manually with pip:\npocket-tts serve\n```\nNavigate to `http://localhost:8000` to try the web interface, it's faster than the command line as the model is kept in memory between requests.\n\nYou can check out the [serve documentation](https://github.com/kyutai-labs/pocket-tts/tree/main/docs/serve.md) for more details and examples.\n\n## Using it as a Python library\n\nInstall the package with\n```bash\npip install pocket-tts\n# or\nuv add pocket-tts\n```\n\nYou can use this package as a simple Python library to generate audio from text.\n```python\nfrom pocket_tts import TTSModel\nimport scipy.io.wavfile\n\ntts_model = TTSModel.load_model()\nvoice_state = tts_model.get_state_for_audio_prompt(\n    \"hf://kyutai/tts-voices/alba-mackenna/casual.wav\"\n)\naudio = tts_model.generate_audio(voice_state, \"Hello world, this is a test.\")\n# Audio is a 1D torch tensor containing PCM data.\nscipy.io.wavfile.write(\"output.wav\", tts_model.sample_rate, audio.numpy())\n```\n\nYou can have multiple voice states around if \nyou have multiple voices you want to use. `load_model()` \nand `get_state_for_audio_prompt()` are relatively slow operations,\nso we recommend to keep the model and voice states in memory if you can.\n\nYou can check out the [Python API documentation](https://github.com/kyutai-labs/pocket-tts/tree/main/docs/python-api.md) for more details and examples.\n\n## Unsupported features\n\nAt the moment, we do not support (but would love pull requests adding):\n- [Running the TTS inside a web browser (WebAssembly)](https://github.com/kyutai-labs/pocket-tts/issues/1)\n- [A ",
      "default_branch": "main"
    },
    "fetched_at": "2026-01-18T03:02:30.547289"
  },
  {
    "basic_info": {
      "name": "nanocode",
      "full_name": "1rgs/nanocode",
      "owner": "1rgs",
      "description": "Minimal Claude Code alternative. Single Python file, zero dependencies, ~250 lines.",
      "url": "https://github.com/1rgs/nanocode",
      "clone_url": "https://github.com/1rgs/nanocode.git",
      "ssh_url": "git@github.com:1rgs/nanocode.git",
      "homepage": null,
      "created_at": "2026-01-11T02:12:27Z",
      "updated_at": "2026-01-18T02:56:03Z",
      "pushed_at": "2026-01-14T05:59:51Z"
    },
    "stats": {
      "stars": 1442,
      "forks": 114,
      "watchers": 1442,
      "open_issues": 5,
      "size": 183
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 8445
      },
      "license": null,
      "topics": []
    },
    "content": {
      "readme": "# nanocode\n\nMinimal Claude Code alternative. Single Python file, zero dependencies, ~250 lines.\n\nBuilt using Claude Code, then used to build itself.\n\n![screenshot](screenshot.png)\n\n## Features\n\n- Full agentic loop with tool use\n- Tools: `read`, `write`, `edit`, `glob`, `grep`, `bash`\n- Conversation history\n- Colored terminal output\n\n## Usage\n\n```bash\nexport ANTHROPIC_API_KEY=\"your-key\"\npython nanocode.py\n```\n\n### OpenRouter\n\nUse [OpenRouter](https://openrouter.ai) to access any model:\n\n```bash\nexport OPENROUTER_API_KEY=\"your-key\"\npython nanocode.py\n```\n\nTo use a different model:\n\n```bash\nexport OPENROUTER_API_KEY=\"your-key\"\nexport MODEL=\"openai/gpt-5.2\"\npython nanocode.py\n```\n\n## Commands\n\n- `/c` - Clear conversation\n- `/q` or `exit` - Quit\n\n## Tools\n\n| Tool | Description |\n|------|-------------|\n| `read` | Read file with line numbers, offset/limit |\n| `write` | Write content to file |\n| `edit` | Replace string in file (must be unique) |\n| `glob` | Find files by pattern, sorted by mtime |\n| `grep` | Search files for regex |\n| `bash` | Run shell command |\n\n## Example\n\n```\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nâ¯ what files are here?\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\nâº Glob(**/*.py)\n  â¿  nanocode.py\n\nâº There's one Python file: nanocode.py\n```\n\n## License\n\nMIT\n",
      "default_branch": "master"
    },
    "fetched_at": "2026-01-18T03:02:31.678483"
  },
  {
    "basic_info": {
      "name": "rlm",
      "full_name": "alexzhang13/rlm",
      "owner": "alexzhang13",
      "description": "General plug-and-play inference library for Recursive Language Models (RLMs), supporting various sandboxes.",
      "url": "https://github.com/alexzhang13/rlm",
      "clone_url": "https://github.com/alexzhang13/rlm.git",
      "ssh_url": "git@github.com:alexzhang13/rlm.git",
      "homepage": "https://arxiv.org/abs/2512.24601v1",
      "created_at": "2025-12-20T23:12:43Z",
      "updated_at": "2026-01-18T02:49:45Z",
      "pushed_at": "2026-01-15T23:23:43Z"
    },
    "stats": {
      "stars": 1275,
      "forks": 222,
      "watchers": 1275,
      "open_issues": 26,
      "size": 2811
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 242595,
        "Makefile": 1613
      },
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "\n---\n\n<h1 align=\"center\" style=\"font-size:2.8em\">\n<span>Recursive Language Models (<span style=\"color:orange\">RLM</span>s)</span>\n</h1>\n\n<p align=\"center\" style=\"font-size:1.3em\">\n  <a href=\"https://arxiv.org/abs/2512.24601\">Full Paper</a> â€¢\n  <a href=\"https://alexzhang13.github.io/blog/2025/rlm/\">Blogpost</a> â€¢\n  <a href=\"https://alexzhang13.github.io/rlm/\">Documentation</a> â€¢\n  <a href=\"https://github.com/alexzhang13/rlm-minimal\">RLM Minimal</a>\n</p>\n\n<p align=\"center\">\n  <a href=\"https://github.com/alexzhang13/rlm/actions/workflows/style.yml\">\n    <img src=\"https://github.com/alexzhang13/rlm/actions/workflows/style.yml/badge.svg\" alt=\"Style\" />\n  </a>\n  <a href=\"https://github.com/alexzhang13/rlm/actions/workflows/test.yml\">\n    <img src=\"https://github.com/alexzhang13/rlm/actions/workflows/test.yml/badge.svg\" alt=\"Test\" />\n  </a>\n</p>\n\n<p align=\"center\">\n  <a href=\"https://arxiv.org/abs/2512.24601\">\n    <img src=\"media/paper_preview.png\" alt=\"Paper Preview\" width=\"300\"/>\n  </a>\n</p>\n\n## Overview\nRecursive Language Models (RLMs) are a task-agnostic inference paradigm for language models (LMs) to handle near-infinite length contexts by enabling the LM to *programmatically* examine, decompose, and recursively call itself over its input. RLMs replace the canonical `llm.completion(prompt, model)` call with a `rlm.completion(prompt, model)` call. RLMs offload the context as a variable in a REPL environment that the LM can interact with and launch sub-LM calls inside of.\n\nThis repository provides an extensible inference engine for using RLMs around standard API-based and local LLMs. The initial experiments and idea were proposed in a [blogpost](https://alexzhang13.github.io/blog/2025/rlm/) in 2025, with expanded results in an [arXiv preprint](https://arxiv.org/abs/2512.24601).\n\n> [!NOTE]\n> This repository contains inference code for RLMs with support for various sandbox environments. Open-source contributions are welcome. This repository is maintained by the authors of the paper from the MIT OASYS lab.\n\n<!-- ## Installation\n```\npip install rlm\n```\nTo install the latest from `main`:\n```\npip install git+https://github.com/alexzhang13/rlm.git\n```\n``` -->\n\n## Quick Setup\nSet up the dependencies with `uv` (or your virtual environment of choice):\n```bash\ncurl -LsSf https://astral.sh/uv/install.sh | sh\nuv init && uv venv --python 3.12  # change version as needed\nuv pip install -e .\n```\n\nThis project includes a `Makefile` to simplify common tasks.\n\n- `make install`: Install base dependencies.\n- `make check`: Run linter, formatter, and tests.\n\nTo run a quick test, the following will run an RLM query with the OpenAI client using your environment variable `OPENAI_API_KEY` (feel free to change this). This will generate console output as well as a log which you can use with the visualizer to explore the trajectories.\n```bash\nmake quickstart\n```\n\nThe default RLM client uses a REPL environment that runs on the host process through Python `exec` calls. It uses the same virtual environment as the host process (i.e. it will have access to the same dependencies), but with some limitations in its available global modules. As an example, we can call RLM completions using GPT-5-nano:\n```python\nfrom rlm import RLM\n\nrlm = RLM(\n    backend=\"openai\",\n    backend_kwargs={\"model_name\": \"gpt-5-nano\"},\n    verbose=True,  # For printing to console with rich, disabled by default.\n)\n\nprint(rlm.completion(\"Print me the first 100 powers of two, each on a newline.\").response)\n```\n\n## REPL Environments\nWe support two types of REPL environments -- isolated, and non-isolated. Non-isolated environments (default) run code execution on the same machine as the RLM (e.g. through `exec`), which is pretty reasonable for some local low-risk tasks, like simple benchmarking, but can be problematic if the prompts or tool calls can interact with malicious users. Fully isolated environments used Cloud-based sandboxes (e.g. Prime Sandboxes, [Modal Sandboxes](https://modal.com/docs/guide/sandboxes)) to run code generated by the RLM, ensuring completely isolation from the host process. Environments can be added, but we natively support the following: `local` (default), `modal`, `prime`.\n\n```python\nrlm = RLM(\n    environment=\"...\", # \"local\", \"docker\", \"modal\", \"prime\"\n    environment_kwargs={...},\n)\n```\n\n### Local Environments\nThe default `local` environment `LocalREPL` runs in the same process as the RLM itself, with specified global and local namespaces for minimal security. Using this REPL is generally safe, but should not be used for production settings. It also shares the same virtual environment (e.g. Conda or uv) as the host process.\n\n#### Docker <img src=\"https://github.com/docker.png\" alt=\"Docker\" height=\"20\" style=\"vertical-align: middle;\"/> (*requires [Docker installed](https://docs.docker.com/desktop/setup/install/)*)\nWe also support a Docker-based environment called `DockerREPL` that launches the REPL environment as a Docker image. By default, we use ",
      "default_branch": "main"
    },
    "fetched_at": "2026-01-18T03:02:32.798216"
  },
  {
    "basic_info": {
      "name": "skills",
      "full_name": "trailofbits/skills",
      "owner": "trailofbits",
      "description": "Trail of Bits Claude Code skills for security research, vulnerability detection, and audit workflows",
      "url": "https://github.com/trailofbits/skills",
      "clone_url": "https://github.com/trailofbits/skills.git",
      "ssh_url": "git@github.com:trailofbits/skills.git",
      "homepage": "",
      "created_at": "2026-01-14T18:23:21Z",
      "updated_at": "2026-01-18T02:57:09Z",
      "pushed_at": "2026-01-16T20:43:39Z"
    },
    "stats": {
      "stars": 1264,
      "forks": 96,
      "watchers": 1264,
      "open_issues": 3,
      "size": 672
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 301687,
        "CodeQL": 12064,
        "C": 8714,
        "Swift": 5459,
        "C#": 5442,
        "Ruby": 5419,
        "Java": 5221,
        "JavaScript": 5033,
        "Kotlin": 5019,
        "TypeScript": 4645,
        "PHP": 3832,
        "Shell": 3035,
        "Rust": 2608,
        "Go": 2213
      },
      "license": "Creative Commons Attribution Share Alike 4.0 International",
      "topics": [
        "agent-skills"
      ]
    },
    "content": {
      "readme": "# Trail of Bits Skills Marketplace\n\nA Claude Code plugin marketplace from Trail of Bits providing skills to enhance AI-assisted security analysis, testing, and development workflows.\n\n## Installation\n\n### Add the Marketplace\n\n```\n/plugin marketplace add trailofbits/skills\n```\n\n### Browse and Install Plugins\n\n```\n/plugin menu\n```\n\n### Local Development\n\nTo add the marketplace locally (e.g., for testing or development), navigate to the **parent directory** of this repository:\n\n```\ncd /path/to/parent  # e.g., if repo is at ~/projects/skills, be in ~/projects\n/plugins marketplace add ./skills\n```\n\n## Available Plugins\n\n### Smart Contract Security\n\n| Plugin | Description |\n|--------|-------------|\n| [building-secure-contracts](plugins/building-secure-contracts/) | Smart contract security toolkit with vulnerability scanners for 6 blockchains |\n| [entry-point-analyzer](plugins/entry-point-analyzer/) | Identify state-changing entry points in smart contracts for security auditing |\n\n### Code Auditing\n\n| Plugin | Description |\n|--------|-------------|\n| [audit-context-building](plugins/audit-context-building/) | Build deep architectural context through ultra-granular code analysis |\n| [burpsuite-project-parser](plugins/burpsuite-project-parser/) | Search and extract data from Burp Suite project files |\n| [differential-review](plugins/differential-review/) | Security-focused differential review of code changes with git history analysis |\n| [semgrep-rule-creator](plugins/semgrep-rule-creator/) | Create and refine Semgrep rules for custom vulnerability detection |\n| [sharp-edges](plugins/sharp-edges/) | Identify error-prone APIs, dangerous configurations, and footgun designs |\n| [static-analysis](plugins/static-analysis/) | Static analysis toolkit with CodeQL, Semgrep, and SARIF parsing |\n| [testing-handbook-skills](plugins/testing-handbook-skills/) | Skills from the [Testing Handbook](https://appsec.guide): fuzzers, static analysis, sanitizers, coverage |\n| [variant-analysis](plugins/variant-analysis/) | Find similar vulnerabilities across codebases using pattern-based analysis |\n\n### Verification\n\n| Plugin | Description |\n|--------|-------------|\n| [constant-time-analysis](plugins/constant-time-analysis/) | Detect compiler-induced timing side-channels in cryptographic code |\n| [property-based-testing](plugins/property-based-testing/) | Property-based testing guidance for multiple languages and smart contracts |\n| [spec-to-code-compliance](plugins/spec-to-code-compliance/) | Specification-to-code compliance checker for blockchain audits |\n\n### Audit Lifecycle\n\n| Plugin | Description |\n|--------|-------------|\n| [fix-review](plugins/fix-review/) | Verify fix commits address audit findings without introducing bugs |\n\n### Reverse Engineering\n\n| Plugin | Description |\n|--------|-------------|\n| [dwarf-expert](plugins/dwarf-expert/) | Interact with and understand the DWARF debugging format |\n\n### Development\n\n| Plugin | Description |\n|--------|-------------|\n| [ask-questions-if-underspecified](plugins/ask-questions-if-underspecified/) | Clarify requirements before implementing |\n\n### Team Management\n\n| Plugin | Description |\n|--------|-------------|\n| [culture-index](plugins/culture-index/) | Interpret Culture Index survey results for individuals and teams |\n\n## Trophy Case\n\nBugs discovered using Trail of Bits Skills. Found something? [Let us know!](https://github.com/trailofbits/skills/issues/new?template=trophy-case.yml)\n\nWhen reporting bugs you've found, feel free to mention:\n> Found using [Trail of Bits Skills](https://github.com/trailofbits/skills)\n\n| Skill | Bug |\n|-------|-----|\n| constant-time-analysis | [Timing side-channel in ECDSA verification](https://github.com/RustCrypto/signatures/pull/1144) |\n\n## Contributing\n\nWe welcome contributions! Please see [CLAUDE.md](CLAUDE.md) for skill authoring guidelines.\n\n## License\n\nThis work is licensed under a [Creative Commons Attribution-ShareAlike 4.0 International License](https://creativecommons.org/licenses/by-sa/4.0/).\n\n## About Trail of Bits\n\n[Trail of Bits](https://www.trailofbits.com/) is a security research and consulting firm.\n",
      "default_branch": "main"
    },
    "fetched_at": "2026-01-18T03:02:33.935608"
  },
  {
    "basic_info": {
      "name": "SimpleMem",
      "full_name": "aiming-lab/SimpleMem",
      "owner": "aiming-lab",
      "description": "SimpleMem: Efficient Lifelong Memory for LLM Agents",
      "url": "https://github.com/aiming-lab/SimpleMem",
      "clone_url": "https://github.com/aiming-lab/SimpleMem.git",
      "ssh_url": "git@github.com:aiming-lab/SimpleMem.git",
      "homepage": "",
      "created_at": "2026-01-01T23:53:40Z",
      "updated_at": "2026-01-18T02:27:54Z",
      "pushed_at": "2026-01-17T22:14:03Z"
    },
    "stats": {
      "stars": 1240,
      "forks": 136,
      "watchers": 1240,
      "open_issues": 4,
      "size": 24798
    },
    "tech_info": {
      "language": "Python",
      "languages": {},
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "# SimpleMem: Efficient Lifelong Memory for LLM Agents\n\n<div align=\"center\">\n\n<p align=\"center\">\n  <img src=\"fig/Fig_icon.png\" width=\"70%\">\n</p>\n\n\n[![Project Page](https://img.shields.io/badge/ğŸ¬_INTERACTIVE_DEMO-Visit_Our_Website-FF6B6B?style=for-the-badge&labelColor=FF6B6B&color=4ECDC4&logoColor=white)](https://aiming-lab.github.io/SimpleMem-Page)\n\n[![Paper](https://img.shields.io/badge/ğŸ“„_Paper-arXiv-b31b1b?style=flat-square)](https://arxiv.org/abs/2601.02553)\n[![GitHub](https://img.shields.io/badge/GitHub-SimpleMem-181717?logo=github&style=flat-square)](https://github.com/aiming-lab/SimpleMem)\n[![License](https://img.shields.io/badge/License-MIT-green?style=flat-square)](LICENSE)\n\n</div>\n\n---\n\n## ğŸ”¥ News\n- **[01/18/2026]** **SimpleMem now supports Claude Skills!** ğŸš€ Use SimpleMem in claude.ai to remember long-term information and project history across conversations. Register at [mcp.simplemem.cloud](https://mcp.simplemem.cloud), add the domain to Claude's network whitelist, **configure with your token in the skill file**, and import the skill!\n- **[01/14/2026]** **SimpleMem MCP Server is now LIVE and Open Source!** ğŸ‰ Experience SimpleMem as a cloud-hosted memory service at [mcp.simplemem.cloud](https://mcp.simplemem.cloud). Easily integrate with your favorite chat platforms (LM Studio, Cherry Studio) and AI agents (Cursor, Claude Desktop) using the **Streamable HTTP** MCP protocol. The MCP implementation features production-ready optimizations including multi-tenant user isolation, faster response times, and enhanced security. [View MCP Documentation â†’](MCP/README.md)\n- **[01/08/2026]** We've set up a Discord server and WeChat group to make it easier to collaborate and exchange ideas on this project. Welcome to join the Group to share your thoughts, ask questions, or contribute your ideas! ğŸ”¥ Join our [Discord](https://discord.gg/KA2zC32M) and [WeChat Group](fig/wechat_logo2.jpg) Now!\n- **[01/05/2026]** SimpleMem paper was released on [arXiv](https://arxiv.org/abs/2601.02553)!\n\n---\n\n## ğŸ“‘ Table of Contents\n\n- [ğŸŒŸ Overview](#-overview)\n- [ğŸ¯ Key Contributions](#-key-contributions)\n- [ğŸš€ Performance Highlights](#-performance-highlights)\n- [ğŸ“¦ Installation](#-installation)\n- [âš¡ Quick Start](#-quick-start)\n- [ğŸ”Œ MCP Server](#-mcp-server)\n- [ğŸ“Š Evaluation](#-evaluation)\n- [ğŸ“ File Structure](#-file-structure)\n- [ğŸ“ Citation](#-citation)\n- [ğŸ“„ License](#-license)\n- [ğŸ™ Acknowledgments](#-acknowledgments)\n\n---\n\n## ğŸŒŸ Overview\n\n<div align=\"center\">\n<img src=\"fig/Fig_tradeoff.png\" alt=\"Performance vs Efficiency Trade-off\" width=\"900\"/>\n\n*SimpleMem achieves superior F1 score (43.24%) with minimal token cost (~550), occupying the ideal top-left position.*\n</div>\n\n\n**SimpleMem** addresses the fundamental challenge of **efficient long-term memory for LLM agents** through a three-stage pipeline grounded in **Semantic Lossless Compression**. Unlike existing systems that either passively accumulate redundant context or rely on expensive iterative reasoning loops, SimpleMem maximizes **information density** and **token utilization** through:\n\n<table>\n<tr>\n<td width=\"33%\" align=\"center\">\n\n### ğŸ” Stage 1\n**Semantic Structured Compression**\n\nEntropy-based filtering and de-linearization of dialogue into self-contained atomic facts\n\n</td>\n<td width=\"33%\" align=\"center\">\n\n### ğŸ—‚ï¸ Stage 2\n**Structured Indexing**\n\nAsynchronous evolution from fragmented atoms to higher-order molecular insights\n\n</td>\n<td width=\"33%\" align=\"center\">\n\n### ğŸ¯ Stage 3\n**Adaptive Retrieval**\n\nComplexity-aware pruning across semantic, lexical, and symbolic layers\n\n</td>\n</tr>\n</table>\n\n<img src=\"fig/Fig_framework.png\" alt=\"SimpleMem Framework\" width=\"900\"/>\n\n*The SimpleMem Architecture: A three-stage pipeline for efficient lifelong memory through semantic lossless compression*\n\n---\n\n### ğŸ† Performance Comparison\n\n<div align=\"center\">\n\n**Speed Comparison Demo**\n\n<video src=\"https://github.com/aiming-lab/SimpleMem/raw/main/fig/simplemem-new.mp4\" controls width=\"900\"></video>\n\n*SimpleMem vs. Baseline: Real-time speed comparison demonstration*\n\n</div>\n\n<div align=\"center\">\n\n**LoCoMo-10 Benchmark Results (GPT-4.1-mini)**\n\n| Model | â±ï¸ Construction Time | ğŸ” Retrieval Time | âš¡ Total Time | ğŸ¯ Average F1 |\n|:------|:--------------------:|:-----------------:|:-------------:|:-------------:|\n| A-Mem | 5140.5s | 796.7s | 5937.2s | 32.58% |\n| LightMem | 97.8s | 577.1s | 675.9s | 24.63% |\n| Mem0 | 1350.9s | 583.4s | 1934.3s | 34.20% |\n| **SimpleMem** â­ | **92.6s** | **388.3s** | **480.9s** | **43.24%** |\n\n</div>\n\n> **ğŸ’¡ Key Advantages:**\n> - ğŸ† **Highest F1 Score**: 43.24% (+26.4% vs. Mem0, +75.6% vs. LightMem)\n> - âš¡ **Fastest Retrieval**: 388.3s (32.7% faster than LightMem, 51.3% faster than Mem0)\n> - ğŸš€ **Fastest End-to-End**: 480.9s total processing time (12.5Ã— faster than A-Mem)\n\n---\n\n## ğŸ¯ Key Contributions\n\n### 1ï¸âƒ£ Semantic Lossless Compression Pipeline\n\nSimpleMem transforms raw, ambiguous dialogue streams into **atomic entries** â€” self-contained facts with resolved coreferences and",
      "default_branch": "main"
    },
    "fetched_at": "2026-01-18T03:02:35.037203"
  },
  {
    "basic_info": {
      "name": "IQuest-Coder-V1",
      "full_name": "IQuestLab/IQuest-Coder-V1",
      "owner": "IQuestLab",
      "description": null,
      "url": "https://github.com/IQuestLab/IQuest-Coder-V1",
      "clone_url": "https://github.com/IQuestLab/IQuest-Coder-V1.git",
      "ssh_url": "git@github.com:IQuestLab/IQuest-Coder-V1.git",
      "homepage": null,
      "created_at": "2025-12-31T06:42:57Z",
      "updated_at": "2026-01-17T20:49:29Z",
      "pushed_at": "2026-01-11T15:14:56Z"
    },
    "stats": {
      "stars": 1231,
      "forks": 87,
      "watchers": 1231,
      "open_issues": 15,
      "size": 30765
    },
    "tech_info": {
      "language": "Python",
      "languages": {},
      "license": "Other",
      "topics": []
    },
    "content": {
      "readme": "",
      "default_branch": "main"
    },
    "fetched_at": "2026-01-18T03:02:36.074224"
  },
  {
    "basic_info": {
      "name": "notebooklm-py",
      "full_name": "teng-lin/notebooklm-py",
      "owner": "teng-lin",
      "description": "Unofficial Python library for automating Google NotebookLM",
      "url": "https://github.com/teng-lin/notebooklm-py",
      "clone_url": "https://github.com/teng-lin/notebooklm-py.git",
      "ssh_url": "git@github.com:teng-lin/notebooklm-py.git",
      "homepage": "https://github.com/teng-lin/notebooklm-py",
      "created_at": "2026-01-07T15:27:19Z",
      "updated_at": "2026-01-18T03:01:11Z",
      "pushed_at": "2026-01-18T01:39:04Z"
    },
    "stats": {
      "stars": 1182,
      "forks": 95,
      "watchers": 1182,
      "open_issues": 2,
      "size": 4617
    },
    "tech_info": {
      "language": "Python",
      "languages": {},
      "license": "MIT License",
      "topics": [
        "api",
        "claude",
        "notebookln",
        "python",
        "sdk",
        "skills"
      ]
    },
    "content": {
      "readme": "",
      "default_branch": "main"
    },
    "fetched_at": "2026-01-18T03:02:37.110761"
  },
  {
    "basic_info": {
      "name": "claude-workflow-v2",
      "full_name": "CloudAI-X/claude-workflow-v2",
      "owner": "CloudAI-X",
      "description": "Universal Claude Code workflow plugin with agents, skills, hooks, and commands",
      "url": "https://github.com/CloudAI-X/claude-workflow-v2",
      "clone_url": "https://github.com/CloudAI-X/claude-workflow-v2.git",
      "ssh_url": "git@github.com:CloudAI-X/claude-workflow-v2.git",
      "homepage": null,
      "created_at": "2026-01-01T02:20:41Z",
      "updated_at": "2026-01-18T01:48:43Z",
      "pushed_at": "2026-01-16T23:30:21Z"
    },
    "stats": {
      "stars": 1172,
      "forks": 175,
      "watchers": 1172,
      "open_issues": 0,
      "size": 188
    },
    "tech_info": {
      "language": "Python",
      "languages": {},
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "",
      "default_branch": "main"
    },
    "fetched_at": "2026-01-18T03:02:38.148077"
  },
  {
    "basic_info": {
      "name": "autocoder",
      "full_name": "leonvanzyl/autocoder",
      "owner": "leonvanzyl",
      "description": null,
      "url": "https://github.com/leonvanzyl/autocoder",
      "clone_url": "https://github.com/leonvanzyl/autocoder.git",
      "ssh_url": "git@github.com:leonvanzyl/autocoder.git",
      "homepage": null,
      "created_at": "2025-12-30T17:01:32Z",
      "updated_at": "2026-01-17T21:29:04Z",
      "pushed_at": "2026-01-17T13:25:17Z"
    },
    "stats": {
      "stars": 1168,
      "forks": 265,
      "watchers": 1168,
      "open_issues": 52,
      "size": 417
    },
    "tech_info": {
      "language": "Python",
      "languages": {},
      "license": "Other",
      "topics": []
    },
    "content": {
      "readme": "",
      "default_branch": "master"
    },
    "fetched_at": "2026-01-18T03:02:39.185564"
  },
  {
    "basic_info": {
      "name": "smtp-tunnel-proxy",
      "full_name": "x011/smtp-tunnel-proxy",
      "owner": "x011",
      "description": "A high-speed covert tunnel that disguises TCP traffic as SMTP email communication to bypass Deep Packet Inspection (DPI) firewalls.",
      "url": "https://github.com/x011/smtp-tunnel-proxy",
      "clone_url": "https://github.com/x011/smtp-tunnel-proxy.git",
      "ssh_url": "git@github.com:x011/smtp-tunnel-proxy.git",
      "homepage": "",
      "created_at": "2026-01-06T19:38:32Z",
      "updated_at": "2026-01-17T23:15:21Z",
      "pushed_at": "2026-01-07T08:24:27Z"
    },
    "stats": {
      "stars": 1050,
      "forks": 94,
      "watchers": 1050,
      "open_issues": 1,
      "size": 277
    },
    "tech_info": {
      "language": "Python",
      "languages": {},
      "license": "GNU General Public License v3.0",
      "topics": [
        "censorship-circumvention",
        "censorship-resistance",
        "covert-tunnel",
        "deep-packet-inspection",
        "encryption",
        "evasion",
        "network-security",
        "proxy",
        "python",
        "smtp",
        "socks5",
        "socks5-proxy",
        "socks5-server",
        "tls",
        "tunnel"
      ]
    },
    "content": {
      "readme": "",
      "default_branch": "main"
    },
    "fetched_at": "2026-01-18T03:02:40.222329"
  },
  {
    "basic_info": {
      "name": "tally",
      "full_name": "davidfowl/tally",
      "owner": "davidfowl",
      "description": "Let agents classify your bank transactions.",
      "url": "https://github.com/davidfowl/tally",
      "clone_url": "https://github.com/davidfowl/tally.git",
      "ssh_url": "git@github.com:davidfowl/tally.git",
      "homepage": "http://tallyai.money",
      "created_at": "2025-12-26T18:43:19Z",
      "updated_at": "2026-01-17T22:47:06Z",
      "pushed_at": "2026-01-17T17:20:55Z"
    },
    "stats": {
      "stars": 979,
      "forks": 70,
      "watchers": 979,
      "open_issues": 10,
      "size": 3701
    },
    "tech_info": {
      "language": "Python",
      "languages": {},
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "",
      "default_branch": "main"
    },
    "fetched_at": "2026-01-18T03:02:41.258735"
  }
]