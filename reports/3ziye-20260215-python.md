## ğŸ“ çƒ­ç‚¹é¡¹ç›®-20260215-python

<div style="background-color: #f5f5f5; padding: 15px; border-radius: 8px; margin: 20px 0;">
- ğŸ¤– æœ¬æŠ¥å‘ŠåŸºäº GitHub API è‡ªåŠ¨ç”Ÿæˆ
- ğŸ“… æ•°æ®æ›´æ–°æ—¶é—´: 2026-02-15 11:38:07 (ä¸Šæµ·æ—¶åŒº)
- ğŸŒŸ æ˜Ÿæ ‡æ•°ç­‰ç»Ÿè®¡ä¿¡æ¯ä¸ºç”Ÿæˆæ—¶çš„å®æ—¶æ•°æ®
- ğŸ“š é¡¹ç›®ä¿¡æ¯æ¥æºäºå„é¡¹ç›®çš„ README æ–‡æ¡£
- ğŸ’¡ çƒ­åº¦æŒ‡æ•°è®¡ç®—æ–¹å¼: æ˜Ÿæ ‡æ•° + Forkæ•° Ã— 0.5
- æœ¬æŠ¥å‘Šç”± ä¸‰å­å¶å¼€æº github-trendingé¡¹ç›®åˆ†æå·¥å…·è‡ªåŠ¨ç”Ÿæˆ
</div>

## ğŸ”— ç›¸å…³é“¾æ¥

- [GitHub API æ–‡æ¡£](https://docs.github.com/en/rest)
- [é¡¹ç›®æ•°æ®è·å–å™¨æºç ](https://github.com/3ziye/github-trending)

---

# çƒ­ç‚¹é¡¹ç›®-20260215-python

<div align="center">
ğŸ“Š <strong>ç”Ÿæˆæ—¶é—´</strong>: 2026å¹´02æœˆ15æ—¥ 11:38  â€¢  
ğŸ¯ <strong>é¡¹ç›®æ•°é‡</strong>: 20 ä¸ª  â€¢  
â±ï¸ <strong>çƒ­åº¦æ—¶é—´</strong>: æœˆæ¦œ  â€¢  
ğŸ”¥ <strong>æ•°æ®æ¥æº</strong>: GitHub API
</div>

---

## ğŸš€ çƒ­é—¨é¡¹ç›®è¯¦æƒ…

### 1. nanobot

**ğŸ”¥ çƒ­åº¦æŒ‡æ•°**: 20,351

**é¡¹ç›®åç§°**: [HKUDS/nanobot](https://github.com/HKUDS/nanobot)
**é¡¹ç›®æè¿°**: "ğŸˆ nanobot: The Ultra-Lightweight OpenClaw"
**ä¸»è¦è¯­è¨€**: Python
**è®¸å¯è¯**: MIT License
**â­ æ˜Ÿæ ‡æ•°**: 18,958
**ğŸ´ Fork æ•°**: 2,786
**ğŸ‘€ å…³æ³¨æ•°**: 18,958
**ğŸ› å¼€æ”¾é—®é¢˜**: 416
**æœ€åæ›´æ–°**: 2026-02-15 (0åˆ†é’Ÿå‰)
**ğŸ“– é¡¹ç›®æ–‡æ¡£**: [README](3ziye-20260215-python/1-nanobot-Readme.md)

**ğŸ  é¡¹ç›®ä¸»é¡µ**: æ— 

**ğŸ“‚ å…‹éš†åœ°å€**: `https://github.com/HKUDS/nanobot.git`

**ğŸ’» æŠ€æœ¯æ ˆ**: **ä¸»è¦è¯­è¨€**: Python

#### ğŸ¯ æ ¸å¿ƒç‰¹æ€§
**2026-02-12** ğŸ§  Redesigned memory system â€” Less code, more reliable. Join the [discussion](https://github.com/HKUDS/nanobot/discussions/566) about it!
**2026-02-10** ğŸ‰ Released v0.1.3.post6 with improvements! Check the updates [notes](https://github.com/HKUDS/nanobot/releases/tag/v0.1.3.post6) and our [roadmap](https://github.com/HKUDS/nanobot/discussions/431).
**2026-02-09** ğŸ’¬ Added Slack, Email, and QQ support â€” nanobot now supports multiple chat platforms!
**2026-02-08** ğŸ”§ Refactored Providersâ€”adding a new LLM provider now takes just 2 simple steps! Check [here](#providers).
**2026-02-07** ğŸš€ Released v0.1.3.post5 with Qwen support & several key improvements! Check [here](https://github.com/HKUDS/nanobot/releases/tag/v0.1.3.post5) for details.
**2026-02-06** âœ¨ Added Moonshot/Kimi provider, Discord integration, and enhanced security hardening!
**2026-02-05** âœ¨ Added Feishu channel, DeepSeek provider, and enhanced scheduled tasks support!

#### ğŸ¨ é€‚ç”¨åœºæ™¯
- æ•°æ®ç§‘å­¦ä¸åˆ†æ
- æœºå™¨å­¦ä¹ é¡¹ç›®
- Web åº”ç”¨å¼€å‘

---

### 2. Qwen3-TTS

**ğŸ”¥ çƒ­åº¦æŒ‡æ•°**: 8,172

**é¡¹ç›®åç§°**: [QwenLM/Qwen3-TTS](https://github.com/QwenLM/Qwen3-TTS)
**é¡¹ç›®æè¿°**: Qwen3-TTS is an open-source series of TTS models developed by the Qwen team at Alibaba Cloud, supporting stable, express...
**ä¸»è¦è¯­è¨€**: Python
**è®¸å¯è¯**: Apache License 2.0
**â­ æ˜Ÿæ ‡æ•°**: 7,691
**ğŸ´ Fork æ•°**: 963
**ğŸ‘€ å…³æ³¨æ•°**: 7,691
**ğŸ› å¼€æ”¾é—®é¢˜**: 62
**æœ€åæ›´æ–°**: 2026-02-15 (4åˆ†é’Ÿå‰)
**ğŸ“– é¡¹ç›®æ–‡æ¡£**: [README](3ziye-20260215-python/2-Qwen3-TTS-Readme.md)

**ğŸ  é¡¹ç›®ä¸»é¡µ**: æ— 

**ğŸ“‚ å…‹éš†åœ°å€**: `https://github.com/QwenLM/Qwen3-TTS.git`

**ğŸ’» æŠ€æœ¯æ ˆ**: **ä¸»è¦è¯­è¨€**: Python

#### ğŸ¯ æ ¸å¿ƒç‰¹æ€§
- 2026.1.22: ğŸ‰ğŸ‰ğŸ‰ We have released [Qwen3-TTS](https://huggingface.co/collections/Qwen/qwen3-tts) series (0.6B/1.7B) based on Qwen3-TTS-Tokenizer-12Hz. Please check our [blog](https://qwen.ai/blog?id=qwen3tts-0115)!
- [Overview](#overview)
- [Quickstart](#quickstart)
- [vLLM Usage](#vllm-usage)
- [Fine Tuning](#fine-tuning)
- [Evaluation](#evaluation)
- [Citation](#citation)

#### ğŸ¨ é€‚ç”¨åœºæ™¯
- æ•°æ®ç§‘å­¦ä¸åˆ†æ
- æœºå™¨å­¦ä¹ é¡¹ç›®
- Web åº”ç”¨å¼€å‘

---

### 3. knowledge-work-plugins

**ğŸ”¥ çƒ­åº¦æŒ‡æ•°**: 7,681

**é¡¹ç›®åç§°**: [anthropics/knowledge-work-plugins](https://github.com/anthropics/knowledge-work-plugins)
**é¡¹ç›®æè¿°**: Open source repository of plugins primarily intended for knowledge workers to use in Claude Cowork
**ä¸»è¦è¯­è¨€**: Python
**è®¸å¯è¯**: Apache License 2.0
**â­ æ˜Ÿæ ‡æ•°**: 7,323
**ğŸ´ Fork æ•°**: 717
**ğŸ‘€ å…³æ³¨æ•°**: 7,323
**ğŸ› å¼€æ”¾é—®é¢˜**: 41
**æœ€åæ›´æ–°**: 2026-02-15 (1å°æ—¶å‰)
**ğŸ“– é¡¹ç›®æ–‡æ¡£**: [README](3ziye-20260215-python/3-knowledge-work-plugins-Readme.md)

**ğŸ  é¡¹ç›®ä¸»é¡µ**: æ— 

**ğŸ“‚ å…‹éš†åœ°å€**: `https://github.com/anthropics/knowledge-work-plugins.git`

**ğŸ’» æŠ€æœ¯æ ˆ**: **ä¸»è¦è¯­è¨€**: Python

#### ğŸ¨ é€‚ç”¨åœºæ™¯
- æ•°æ®ç§‘å­¦ä¸åˆ†æ
- æœºå™¨å­¦ä¹ é¡¹ç›®
- Web åº”ç”¨å¼€å‘

---

### 4. original_performance_takehome

**ğŸ”¥ çƒ­åº¦æŒ‡æ•°**: 3,827

**é¡¹ç›®åç§°**: [anthropics/original_performance_takehome](https://github.com/anthropics/original_performance_takehome)
**é¡¹ç›®æè¿°**: Anthropic's original performance take-home, now open for you to try!
**ä¸»è¦è¯­è¨€**: Python
**è®¸å¯è¯**: None
**â­ æ˜Ÿæ ‡æ•°**: 3,448
**ğŸ´ Fork æ•°**: 758
**ğŸ‘€ å…³æ³¨æ•°**: 3,448
**ğŸ› å¼€æ”¾é—®é¢˜**: 13
**æœ€åæ›´æ–°**: 2026-02-15 (2å°æ—¶å‰)
**ğŸ“– é¡¹ç›®æ–‡æ¡£**: [README](3ziye-20260215-python/4-original_performance_takehome-Readme.md)

**ğŸ  é¡¹ç›®ä¸»é¡µ**: æ— 

**ğŸ“‚ å…‹éš†åœ°å€**: `https://github.com/anthropics/original_performance_takehome.git`

**ğŸ’» æŠ€æœ¯æ ˆ**: **ä¸»è¦è¯­è¨€**: Python

#### ğŸ¯ æ ¸å¿ƒç‰¹æ€§
**2164 cycles**: Claude Opus 4 after many hours in the test-time compute harness
**1790 cycles**: Claude Opus 4.5 in a casual Claude Code session, approximately matching the best human performance in 2 hours
**1579 cycles**: Claude Opus 4.5 after 2 hours in our test-time compute harness
**1548 cycles**: Claude Sonnet 4.5 after many more than 2 hours of test-time compute
**1487 cycles**: Claude Opus 4.5 after 11.5 hours in the harness
**1363 cycles**: Claude Opus 4.5 in an improved test time compute harness
**??? cycles**: Best human performance ever is substantially better than the above, but we won't say how much.

#### ğŸ¨ é€‚ç”¨åœºæ™¯
- æ•°æ®ç§‘å­¦ä¸åˆ†æ
- æœºå™¨å­¦ä¹ é¡¹ç›®
- Web åº”ç”¨å¼€å‘

---

### 5. lingbot-world

**ğŸ”¥ çƒ­åº¦æŒ‡æ•°**: 2,971

**é¡¹ç›®åç§°**: [Robbyant/lingbot-world](https://github.com/Robbyant/lingbot-world)
**é¡¹ç›®æè¿°**: Advancing Open-source World Models
**ä¸»è¦è¯­è¨€**: Python
**è®¸å¯è¯**: Apache License 2.0
**â­ æ˜Ÿæ ‡æ•°**: 2,857
**ğŸ´ Fork æ•°**: 229
**ğŸ‘€ å…³æ³¨æ•°**: 2,857
**ğŸ› å¼€æ”¾é—®é¢˜**: 18
**æœ€åæ›´æ–°**: 2026-02-15 (26åˆ†é’Ÿå‰)
**ğŸ“– é¡¹ç›®æ–‡æ¡£**: [README](3ziye-20260215-python/5-lingbot-world-Readme.md)

**ğŸ  é¡¹ç›®ä¸»é¡µ**: https://technology.robbyant.com/lingbot-world

**ğŸ“‚ å…‹éš†åœ°å€**: `https://github.com/Robbyant/lingbot-world.git`

**ğŸ’» æŠ€æœ¯æ ˆ**: **ä¸»è¦è¯­è¨€**: Python

#### ğŸ·ï¸ é¡¹ç›®æ ‡ç­¾
- `aigc`
- `image-to-video`
- `lingbot-world`
- `video-generation`
- `world-models`

#### ğŸ¯ æ ¸å¿ƒç‰¹æ€§
**High-Fidelity & Diverse Environments**: It maintains high fidelity and robust dynamics in a broad spectrum of environments, including realism, scientific contexts, cartoon styles, and beyond.
**Long-Term Memory & Consistency**: It enables a minute-level horizon while preserving contextual consistency over time, which is also known as long-term memory.
- Jan 29, 2026: ğŸ‰ We release the technical report, code, and models for LingBot-World.
- It maintains high fidelity and robust dynamics in a broad spectrum of environments, including realism, scientific contexts, cartoon styles, and beyond.
- It enables a minute-level horizon while preserving contextual consistency over time, which is also known as **long-term memory**.

#### ğŸ¨ é€‚ç”¨åœºæ™¯
- æ•°æ®ç§‘å­¦ä¸åˆ†æ
- æœºå™¨å­¦ä¹ é¡¹ç›®
- Web åº”ç”¨å¼€å‘

---

### 6. last30days-skill

**ğŸ”¥ çƒ­åº¦æŒ‡æ•°**: 2,764

**é¡¹ç›®åç§°**: [mvanhorn/last30days-skill](https://github.com/mvanhorn/last30days-skill)
**é¡¹ç›®æè¿°**: Claude Code skill that researches any topic across Reddit + X from the last 30 days, then writes copy-paste-ready prompt...
**ä¸»è¦è¯­è¨€**: Python
**è®¸å¯è¯**: None
**â­ æ˜Ÿæ ‡æ•°**: 2,612
**ğŸ´ Fork æ•°**: 304
**ğŸ‘€ å…³æ³¨æ•°**: 2,612
**ğŸ› å¼€æ”¾é—®é¢˜**: 9
**æœ€åæ›´æ–°**: 2026-02-15 (6åˆ†é’Ÿå‰)
**ğŸ“– é¡¹ç›®æ–‡æ¡£**: [README](3ziye-20260215-python/6-last30days-skill-Readme.md)

**ğŸ  é¡¹ç›®ä¸»é¡µ**: æ— 

**ğŸ“‚ å…‹éš†åœ°å€**: `https://github.com/mvanhorn/last30days-skill.git`

**ğŸ’» æŠ€æœ¯æ ˆ**: **ä¸»è¦è¯­è¨€**: Python

#### ğŸ·ï¸ é¡¹ç›®æ ‡ç­¾
- `ai-prompts`
- `claude`
- `claude-code`
- `reddit`
- `twitter`

#### ğŸ¯ æ ¸å¿ƒç‰¹æ€§
- 3. Separate subjects into distinct objects â€” Multi-character s

#### ğŸ¨ é€‚ç”¨åœºæ™¯
- æ•°æ®ç§‘å­¦ä¸åˆ†æ
- æœºå™¨å­¦ä¹ é¡¹ç›®
- Web åº”ç”¨å¼€å‘

---

### 7. quip-protocol

**ğŸ”¥ çƒ­åº¦æŒ‡æ•°**: 2,512

**é¡¹ç›®åç§°**: [QuipNetwork/quip-protocol](https://github.com/QuipNetwork/quip-protocol)
**é¡¹ç›®æè¿°**: experimental quip protocol network node
**ä¸»è¦è¯­è¨€**: Python
**è®¸å¯è¯**: GNU Affero General Public License v3.0
**â­ æ˜Ÿæ ‡æ•°**: 2,487
**ğŸ´ Fork æ•°**: 50
**ğŸ‘€ å…³æ³¨æ•°**: 2,487
**ğŸ› å¼€æ”¾é—®é¢˜**: 1
**æœ€åæ›´æ–°**: 2026-02-15 (50åˆ†é’Ÿå‰)
**ğŸ“– é¡¹ç›®æ–‡æ¡£**: [README](3ziye-20260215-python/7-quip-protocol-Readme.md)

**ğŸ  é¡¹ç›®ä¸»é¡µ**: æ— 

**ğŸ“‚ å…‹éš†åœ°å€**: `https://github.com/QuipNetwork/quip-protocol.git`

**ğŸ’» æŠ€æœ¯æ ˆ**: **ä¸»è¦è¯­è¨€**: Python

#### ğŸ¯ æ ¸å¿ƒç‰¹æ€§
**Quantum Annealing PoW**: Using Ising model optimization as the mining puzzle
**Competitive Mining**: Multiple miners (QPU and SA) compete to mine blocks
**Multi-Miner Support**: Configure any number of QPU and SA miners
**Dynamic Difficulty**: Inverted difficulty mechanism that prevents miner monopolization
**Streak Rewards**: Consecutive wins increase block rewards
**Solution Diversity**: Requires multiple diverse solutions to prevent trivial mining
**Individual Miner Tracking**: Each miner has unique ID and performance stats
**Quantum PoW only** - No transactions, accounts, or other typical blockchain features

#### ğŸ¨ é€‚ç”¨åœºæ™¯
- æ•°æ®ç§‘å­¦ä¸åˆ†æ
- æœºå™¨å­¦ä¹ é¡¹ç›®
- Web åº”ç”¨å¼€å‘

---

### 8. DeepSeek-OCR-2

**ğŸ”¥ çƒ­åº¦æŒ‡æ•°**: 2,333

**é¡¹ç›®åç§°**: [deepseek-ai/DeepSeek-OCR-2](https://github.com/deepseek-ai/DeepSeek-OCR-2)
**é¡¹ç›®æè¿°**: Visual Causal Flow
**ä¸»è¦è¯­è¨€**: Python
**è®¸å¯è¯**: Apache License 2.0
**â­ æ˜Ÿæ ‡æ•°**: 2,247
**ğŸ´ Fork æ•°**: 172
**ğŸ‘€ å…³æ³¨æ•°**: 2,247
**ğŸ› å¼€æ”¾é—®é¢˜**: 44
**æœ€åæ›´æ–°**: 2026-02-14 (12å°æ—¶å‰)
**ğŸ“– é¡¹ç›®æ–‡æ¡£**: [README](3ziye-20260215-python/8-DeepSeek-OCR-2-Readme.md)

**ğŸ  é¡¹ç›®ä¸»é¡µ**: æ— 

**ğŸ“‚ å…‹éš†åœ°å€**: `https://github.com/deepseek-ai/DeepSeek-OCR-2.git`

**ğŸ’» æŠ€æœ¯æ ˆ**: **ä¸»è¦è¯­è¨€**: Python

#### ğŸ¯ æ ¸å¿ƒç‰¹æ€§
- [Install](#install)
- [vLLM Inference](#vllm-inference)
- [Transformers Inference](#transformers-inference)
- download the vllm-0.8.5 [whl](https://github.com/vllm-project/vllm/releases/tag/v0.8.5)
- VLLM:

#### ğŸ¨ é€‚ç”¨åœºæ™¯
- æ•°æ®ç§‘å­¦ä¸åˆ†æ
- æœºå™¨å­¦ä¹ é¡¹ç›®
- Web åº”ç”¨å¼€å‘

---

### 9. Trellis

**ğŸ”¥ çƒ­åº¦æŒ‡æ•°**: 2,259

**é¡¹ç›®åç§°**: [mindfold-ai/Trellis](https://github.com/mindfold-ai/Trellis)
**é¡¹ç›®æè¿°**: All-in-one AI framework & toolkit for Claude Code & Cursor
**ä¸»è¦è¯­è¨€**: Python
**è®¸å¯è¯**: GNU Affero General Public License v3.0
**â­ æ˜Ÿæ ‡æ•°**: 2,205
**ğŸ´ Fork æ•°**: 108
**ğŸ‘€ å…³æ³¨æ•°**: 2,205
**ğŸ› å¼€æ”¾é—®é¢˜**: 0
**æœ€åæ›´æ–°**: 2026-02-15 (1å°æ—¶å‰)
**ğŸ“– é¡¹ç›®æ–‡æ¡£**: [README](3ziye-20260215-python/9-Trellis-Readme.md)

**ğŸ  é¡¹ç›®ä¸»é¡µ**: https://trytrellis.app

**ğŸ“‚ å…‹éš†åœ°å€**: `https://github.com/mindfold-ai/Trellis.git`

**ğŸ’» æŠ€æœ¯æ ˆ**: **ä¸»è¦è¯­è¨€**: Python

#### ğŸ·ï¸ é¡¹ç›®æ ‡ç­¾
- `ai-agent`
- `ai-coding`
- `claude-code`
- `cli`
- `cursor`
- `developer-tools`
- `typescript`
- `workflow`

#### ğŸ¨ é€‚ç”¨åœºæ™¯
- æ•°æ®ç§‘å­¦ä¸åˆ†æ
- æœºå™¨å­¦ä¹ é¡¹ç›®
- Web åº”ç”¨å¼€å‘
- å‘½ä»¤è¡Œå·¥å…·å¼€å‘

---

### 10. dash

**ğŸ”¥ çƒ­åº¦æŒ‡æ•°**: 1,727

**é¡¹ç›®åç§°**: [agno-agi/dash](https://github.com/agno-agi/dash)
**é¡¹ç›®æè¿°**: Self-learning data agent that grounds its answers in 6 layers of context. Inspired by OpenAI's in-house implementation.
**ä¸»è¦è¯­è¨€**: Python
**è®¸å¯è¯**: Apache License 2.0
**â­ æ˜Ÿæ ‡æ•°**: 1,638
**ğŸ´ Fork æ•°**: 179
**ğŸ‘€ å…³æ³¨æ•°**: 1,638
**ğŸ› å¼€æ”¾é—®é¢˜**: 5
**æœ€åæ›´æ–°**: 2026-02-15 (28åˆ†é’Ÿå‰)
**ğŸ“– é¡¹ç›®æ–‡æ¡£**: [README](3ziye-20260215-python/10-dash-Readme.md)

**ğŸ  é¡¹ç›®ä¸»é¡µ**: æ— 

**ğŸ“‚ å…‹éš†åœ°å€**: `https://github.com/agno-agi/dash.git`

**ğŸ’» æŠ€æœ¯æ ˆ**: **ä¸»è¦è¯­è¨€**: Python

#### ğŸ¯ æ ¸å¿ƒç‰¹æ€§
- Who won the most F1 World Championships?
- How many races has Lewis Hamilton won?
- Compare Ferrari vs Mercedes points 2015-2020
**Schemas lack meaning.**
**Types are misleading.**
**Tribal knowledge is missing.**
**No way to learn from mistakes.**
**Results generally lack interpretation.**

#### ğŸ¨ é€‚ç”¨åœºæ™¯
- æ•°æ®ç§‘å­¦ä¸åˆ†æ
- æœºå™¨å­¦ä¹ é¡¹ç›®
- Web åº”ç”¨å¼€å‘

---

### 11. GLM-OCR

**ğŸ”¥ çƒ­åº¦æŒ‡æ•°**: 1,608

**é¡¹ç›®åç§°**: [zai-org/GLM-OCR](https://github.com/zai-org/GLM-OCR)
**é¡¹ç›®æè¿°**: GLM-OCR: Accurate Ã—  Fast Ã— Comprehensive
**ä¸»è¦è¯­è¨€**: Python
**è®¸å¯è¯**: Apache License 2.0
**â­ æ˜Ÿæ ‡æ•°**: 1,559
**ğŸ´ Fork æ•°**: 99
**ğŸ‘€ å…³æ³¨æ•°**: 1,559
**ğŸ› å¼€æ”¾é—®é¢˜**: 31
**æœ€åæ›´æ–°**: 2026-02-15 (1å°æ—¶å‰)
**ğŸ“– é¡¹ç›®æ–‡æ¡£**: [README](3ziye-20260215-python/11-GLM-OCR-Readme.md)

**ğŸ  é¡¹ç›®ä¸»é¡µ**: æ— 

**ğŸ“‚ å…‹éš†åœ°å€**: `https://github.com/zai-org/GLM-OCR.git`

**ğŸ’» æŠ€æœ¯æ ˆ**: **ä¸»è¦è¯­è¨€**: Python

#### ğŸ·ï¸ é¡¹ç›®æ ‡ç­¾
- `glm`
- `image2text`
- `ocr`

#### ğŸ¯ æ ¸å¿ƒç‰¹æ€§
**Optimized for Real-World Scenarios**: Designed and optimized for practical business use cases, maintaining robust performance on complex tables, code-heavy documents, seals, and other challenging real-world layouts.
**Efficient Inference**: With only 0.9B parameters, GLM-OCR supports deployment via vLLM, SGLang, and Ollama, significantly reducing inference latency and compute cost, making it ideal for high-concurrency services and edge deployments.
**Easy to Use**: Fully open-sourced and equipped with a comprehensive [SDK](https://github.com/zai-org/GLM-OCR) and inference toolchain, offering simple installation, one-line invocation, and smooth integration into existing production pipelines.
**[Coming Soon]** GLM-OCR Technical Report
**[2026.2.12]** Fine-tuning tutorial based on LLaMA-Factory is now available. See: [GLM-OCR Fine-tuning Guide](examples/finetune/README.md)

#### ğŸ¨ é€‚ç”¨åœºæ™¯
- æ•°æ®ç§‘å­¦ä¸åˆ†æ
- æœºå™¨å­¦ä¹ é¡¹ç›®
- Web åº”ç”¨å¼€å‘

---

### 12. Qwen3-ASR

**ğŸ”¥ çƒ­åº¦æŒ‡æ•°**: 1,576

**é¡¹ç›®åç§°**: [QwenLM/Qwen3-ASR](https://github.com/QwenLM/Qwen3-ASR)
**é¡¹ç›®æè¿°**: Qwen3-ASR is an open-source series of ASR models developed by the Qwen team at Alibaba Cloud, supporting stable multilin...
**ä¸»è¦è¯­è¨€**: Python
**è®¸å¯è¯**: Apache License 2.0
**â­ æ˜Ÿæ ‡æ•°**: 1,514
**ğŸ´ Fork æ•°**: 124
**ğŸ‘€ å…³æ³¨æ•°**: 1,514
**ğŸ› å¼€æ”¾é—®é¢˜**: 31
**æœ€åæ›´æ–°**: 2026-02-15 (23åˆ†é’Ÿå‰)
**ğŸ“– é¡¹ç›®æ–‡æ¡£**: [README](3ziye-20260215-python/12-Qwen3-ASR-Readme.md)

**ğŸ  é¡¹ç›®ä¸»é¡µ**: æ— 

**ğŸ“‚ å…‹éš†åœ°å€**: `https://github.com/QwenLM/Qwen3-ASR.git`

**ğŸ’» æŠ€æœ¯æ ˆ**: **ä¸»è¦è¯­è¨€**: Python

#### ğŸ¯ æ ¸å¿ƒç‰¹æ€§
- 2026.1.29: ğŸ‰ğŸ‰ğŸ‰ We have released the [Qwen3-ASR](https://huggingface.co/collections/Qwen/qwen3-asr) series (0.6B/1.7B) and the Qwen3-ForcedAligner-0.6B model. Please check out our [blog](https://qwen.ai/blog?id=qwen3asr)!
- [Overview](#overview)
- [Quickstart](#quickstart)
- [Launch Local Web UI Demo](#launch-local-web-ui-demo)
- [Deployment with vLLM](#deployment-with-vllm)
- [Fine Tuning](#fine-tuning)
- [Docker](#docker)
- [Evaluation](#evaluation)

#### ğŸ¨ é€‚ç”¨åœºæ™¯
- æ•°æ®ç§‘å­¦ä¸åˆ†æ
- æœºå™¨å­¦ä¹ é¡¹ç›®
- Web åº”ç”¨å¼€å‘

---

### 13. Edit-Banana

**ğŸ”¥ çƒ­åº¦æŒ‡æ•°**: 1,518

**é¡¹ç›®åç§°**: [BIT-DataLab/Edit-Banana](https://github.com/BIT-DataLab/Edit-Banana)
**é¡¹ç›®æè¿°**: Edit Banana: A framework for converting statistical formats into editable.
**ä¸»è¦è¯­è¨€**: Python
**è®¸å¯è¯**: None
**â­ æ˜Ÿæ ‡æ•°**: 1,487
**ğŸ´ Fork æ•°**: 62
**ğŸ‘€ å…³æ³¨æ•°**: 1,487
**ğŸ› å¼€æ”¾é—®é¢˜**: 5
**æœ€åæ›´æ–°**: 2026-02-15 (12åˆ†é’Ÿå‰)
**ğŸ“– é¡¹ç›®æ–‡æ¡£**: [README](3ziye-20260215-python/13-Edit-Banana-Readme.md)

**ğŸ  é¡¹ç›®ä¸»é¡µ**: https://editbanana.anxin6.cn/

**ğŸ“‚ å…‹éš†åœ°å€**: `https://github.com/BIT-DataLab/Edit-Banana.git`

**ğŸ’» æŠ€æœ¯æ ˆ**: **ä¸»è¦è¯­è¨€**: Python

#### ğŸ·ï¸ é¡¹ç›®æ ‡ç­¾
- `ai`
- `data`
- `figure`
- `llm`
- `nanobanana`
- `open-source`
- `python`
- `pythonprogramming`

#### ğŸ¨ é€‚ç”¨åœºæ™¯
- æ•°æ®ç§‘å­¦ä¸åˆ†æ
- æœºå™¨å­¦ä¹ é¡¹ç›®
- Web åº”ç”¨å¼€å‘

---

### 14. Youtube-clipper-skill

**ğŸ”¥ çƒ­åº¦æŒ‡æ•°**: 1,454

**é¡¹ç›®åç§°**: [op7418/Youtube-clipper-skill](https://github.com/op7418/Youtube-clipper-skill)
**é¡¹ç›®æè¿°**: æš‚æ— æè¿°
**ä¸»è¦è¯­è¨€**: Python
**è®¸å¯è¯**: MIT License
**â­ æ˜Ÿæ ‡æ•°**: 1,346
**ğŸ´ Fork æ•°**: 216
**ğŸ‘€ å…³æ³¨æ•°**: 1,346
**ğŸ› å¼€æ”¾é—®é¢˜**: 7
**æœ€åæ›´æ–°**: 2026-02-15 (1å°æ—¶å‰)
**ğŸ“– é¡¹ç›®æ–‡æ¡£**: [README](3ziye-20260215-python/14-Youtube-clipper-skill-Readme.md)

**ğŸ  é¡¹ç›®ä¸»é¡µ**: æ— 

**ğŸ“‚ å…‹éš†åœ°å€**: `https://github.com/op7418/Youtube-clipper-skill.git`

**ğŸ’» æŠ€æœ¯æ ˆ**: **ä¸»è¦è¯­è¨€**: Python

#### ğŸ¯ æ ¸å¿ƒç‰¹æ€§
- **AI Semantic Analysis** - Generate fine-grained chapters (2-5 minutes each) by understanding video content, not just mechanical time splitting
- **Precise Clipping** - Use FFmpeg to extract video segments with frame-accurate timing
- **Bilingual Subtitles** - Batch translate subtitles to Chinese/English with 95% API call reduction
- **Subtitle Burning** - Hardcode bilingual subtitles into videos with customizable styling
- **Content Summarization** - Auto-generate social media content (Xiaohongshu, Douyin, WeChat)
---

#### ğŸ¨ é€‚ç”¨åœºæ™¯
- æ•°æ®ç§‘å­¦ä¸åˆ†æ
- æœºå™¨å­¦ä¹ é¡¹ç›®
- Web åº”ç”¨å¼€å‘

---

### 15. ComfyUI-Qwen-TTS

**ğŸ”¥ çƒ­åº¦æŒ‡æ•°**: 1,083

**é¡¹ç›®åç§°**: [flybirdxx/ComfyUI-Qwen-TTS](https://github.com/flybirdxx/ComfyUI-Qwen-TTS)
**é¡¹ç›®æè¿°**: A Simple Implementation of Qwen3-TTS's ComfyUI
**ä¸»è¦è¯­è¨€**: Python
**è®¸å¯è¯**: None
**â­ æ˜Ÿæ ‡æ•°**: 1,032
**ğŸ´ Fork æ•°**: 103
**ğŸ‘€ å…³æ³¨æ•°**: 1,032
**ğŸ› å¼€æ”¾é—®é¢˜**: 38
**æœ€åæ›´æ–°**: 2026-02-15 (2å°æ—¶å‰)
**ğŸ“– é¡¹ç›®æ–‡æ¡£**: [README](3ziye-20260215-python/15-ComfyUI-Qwen-TTS-Readme.md)

**ğŸ  é¡¹ç›®ä¸»é¡µ**: æ— 

**ğŸ“‚ å…‹éš†åœ°å€**: `https://github.com/flybirdxx/ComfyUI-Qwen-TTS.git`

**ğŸ’» æŠ€æœ¯æ ˆ**: **ä¸»è¦è¯­è¨€**: Python

#### ğŸ¯ æ ¸å¿ƒç‰¹æ€§
**2026-02-04**: Feature Update: Added Global Pause Control (`QwenTTSConfigNode`) and `extra_model_paths.yaml` support ([update.md](doc/update.md))
**2026-01-29**: Feature Update: Support for loading custom fine-tuned models & speakers ([update.md](doc/update.md))
**2026-01-27**: UI Optimization: Sleek LoadSpeaker UI; fixed PyTorch 2.6+ compatibility ([update.md](doc/update.md))
**2026-01-26**: Functional Update: New voice persistence system (SaveVoice / LoadSpeaker) ([update.md](doc/update.md))
**2026-01-24**: Added attention mechanism selection & model memory management features ([update.md](doc/update.md))
**2026-01-24**: Added generation parameters (top_p, top_k, temperature, repetition_penalty) to all TTS nodes ([update.md](doc/update.md))
**2026-01-23**: Dependency compatibility & Mac (MPS) support, New nodes: VoiceClonePromptNode, DialogueInferenceNode ([update.md](doc/update.md))
**Qwen3-TTS Multi-Role Multi-Round Dialogue Generation Workflow**:

#### ğŸ¨ é€‚ç”¨åœºæ™¯
- æ•°æ®ç§‘å­¦ä¸åˆ†æ
- æœºå™¨å­¦ä¹ é¡¹ç›®
- Web åº”ç”¨å¼€å‘

---

### 16. Khazix-Skills

**ğŸ”¥ çƒ­åº¦æŒ‡æ•°**: 939

**é¡¹ç›®åç§°**: [KKKKhazix/Khazix-Skills](https://github.com/KKKKhazix/Khazix-Skills)
**é¡¹ç›®æè¿°**: æš‚æ— æè¿°
**ä¸»è¦è¯­è¨€**: Python
**è®¸å¯è¯**: None
**â­ æ˜Ÿæ ‡æ•°**: 873
**ğŸ´ Fork æ•°**: 133
**ğŸ‘€ å…³æ³¨æ•°**: 873
**ğŸ› å¼€æ”¾é—®é¢˜**: 5
**æœ€åæ›´æ–°**: 2026-02-14 (14å°æ—¶å‰)
**ğŸ“– é¡¹ç›®æ–‡æ¡£**: [README](3ziye-20260215-python/16-Khazix-Skills-Readme.md)

**ğŸ  é¡¹ç›®ä¸»é¡µ**: æ— 

**ğŸ“‚ å…‹éš†åœ°å€**: `https://github.com/KKKKhazix/Khazix-Skills.git`

**ğŸ’» æŠ€æœ¯æ ˆ**: **ä¸»è¦è¯­è¨€**: Python

#### ğŸ¯ æ ¸å¿ƒç‰¹æ€§
- Fetches repository metadata (README, latest commit hash)
- Creates standardized skill directory structure
- Generates `SKILL.md` with extended frontmatter for lifecycle management
- Creates wrapper scripts for tool invocation

#### ğŸ¨ é€‚ç”¨åœºæ™¯
- æ•°æ®ç§‘å­¦ä¸åˆ†æ
- æœºå™¨å­¦ä¹ é¡¹ç›®
- Web åº”ç”¨å¼€å‘

---

### 17. lingbot-depth

**ğŸ”¥ çƒ­åº¦æŒ‡æ•°**: 895

**é¡¹ç›®åç§°**: [Robbyant/lingbot-depth](https://github.com/Robbyant/lingbot-depth)
**é¡¹ç›®æè¿°**: Masked Depth Modeling for Spatial Perception
**ä¸»è¦è¯­è¨€**: Python
**è®¸å¯è¯**: Apache License 2.0
**â­ æ˜Ÿæ ‡æ•°**: 865
**ğŸ´ Fork æ•°**: 61
**ğŸ‘€ å…³æ³¨æ•°**: 865
**ğŸ› å¼€æ”¾é—®é¢˜**: 7
**æœ€åæ›´æ–°**: 2026-02-15 (7å°æ—¶å‰)
**ğŸ“– é¡¹ç›®æ–‡æ¡£**: [README](3ziye-20260215-python/17-lingbot-depth-Readme.md)

**ğŸ  é¡¹ç›®ä¸»é¡µ**: https://technology.robbyant.com/lingbot-depth

**ğŸ“‚ å…‹éš†åœ°å€**: `https://github.com/Robbyant/lingbot-depth.git`

**ğŸ’» æŠ€æœ¯æ ˆ**: **ä¸»è¦è¯­è¨€**: Python

#### ğŸ·ï¸ é¡¹ç›®æ ‡ç­¾
- `depth`
- `depth-camera`
- `masked-image-modeling`

#### ğŸ¯ æ ¸å¿ƒç‰¹æ€§
**Depth Completion & Refinement**: Fills missing regions with metric accuracy and improved quality
**Scene Reconstruction**: High-fidelity indoor mapping with a strong depth prior
**4D Point Tracking**: Accurate dynamic tracking in metric space for robot learning
**Dexterous Manipulation**: Robust grasping with precise geometric understanding
**[2026.02.15]** Upload LingBot-Depth-v0.5 which fixes the bug in previous version.
- The curated 3

#### ğŸ¨ é€‚ç”¨åœºæ™¯
- æ•°æ®ç§‘å­¦ä¸åˆ†æ
- æœºå™¨å­¦ä¹ é¡¹ç›®
- Web åº”ç”¨å¼€å‘

---

### 18. lingbot-vla

**ğŸ”¥ çƒ­åº¦æŒ‡æ•°**: 833

**é¡¹ç›®åç§°**: [Robbyant/lingbot-vla](https://github.com/Robbyant/lingbot-vla)
**é¡¹ç›®æè¿°**: A Pragmatic VLA Foundation Model
**ä¸»è¦è¯­è¨€**: Python
**è®¸å¯è¯**: Apache License 2.0
**â­ æ˜Ÿæ ‡æ•°**: 803
**ğŸ´ Fork æ•°**: 60
**ğŸ‘€ å…³æ³¨æ•°**: 803
**ğŸ› å¼€æ”¾é—®é¢˜**: 16
**æœ€åæ›´æ–°**: 2026-02-15 (10å°æ—¶å‰)
**ğŸ“– é¡¹ç›®æ–‡æ¡£**: [README](3ziye-20260215-python/18-lingbot-vla-Readme.md)

**ğŸ  é¡¹ç›®ä¸»é¡µ**: æ— 

**ğŸ“‚ å…‹éš†åœ°å€**: `https://github.com/Robbyant/lingbot-vla.git`

**ğŸ’» æŠ€æœ¯æ ˆ**: **ä¸»è¦è¯­è¨€**: Python

#### ğŸ¯ æ ¸å¿ƒç‰¹æ€§
**Large-scale Pre-training Data**: 20,000 hours of real-world
**Strong Performance**: Achieve clear superiority over competitors on simulation and real-world benchmarks.
**Training Efficiency**: Represent a 1.5 âˆ¼ 2.8Ã— (depending on the relied VLM base model) speedup over existing VLA-oriented codebases.
**[2026-01-27]** LingBot-VLA Technical Report is available on Arxiv.
**[2026-01-27]** Weights and code released!
**Pretrained Checkpoints for Post-Training with and without depth**

#### ğŸ¨ é€‚ç”¨åœºæ™¯
- æ•°æ®ç§‘å­¦ä¸åˆ†æ
- æœºå™¨å­¦ä¹ é¡¹ç›®
- Web åº”ç”¨å¼€å‘

---

### 19. skill-scanner

**ğŸ”¥ çƒ­åº¦æŒ‡æ•°**: 830

**é¡¹ç›®åç§°**: [cisco-ai-defense/skill-scanner](https://github.com/cisco-ai-defense/skill-scanner)
**é¡¹ç›®æè¿°**: Security Scanner for Agent Skills
**ä¸»è¦è¯­è¨€**: Python
**è®¸å¯è¯**: Other
**â­ æ˜Ÿæ ‡æ•°**: 785
**ğŸ´ Fork æ•°**: 91
**ğŸ‘€ å…³æ³¨æ•°**: 785
**ğŸ› å¼€æ”¾é—®é¢˜**: 6
**æœ€åæ›´æ–°**: 2026-02-15 (4å°æ—¶å‰)
**ğŸ“– é¡¹ç›®æ–‡æ¡£**: [README](3ziye-20260215-python/19-skill-scanner-Readme.md)

**ğŸ  é¡¹ç›®ä¸»é¡µ**: https://blogs.cisco.com/ai/personal-ai-agents-like-moltbot-are-a-security-nightmare

**ğŸ“‚ å…‹éš†åœ°å€**: `https://github.com/cisco-ai-defense/skill-scanner.git`

**ğŸ’» æŠ€æœ¯æ ˆ**: **ä¸»è¦è¯­è¨€**: Python

#### ğŸ·ï¸ é¡¹ç›®æ ‡ç­¾
- `agent`
- `agent-skills`
- `security`

#### ğŸ¯ æ ¸å¿ƒç‰¹æ€§
- **Multi-Engine Detection** - Static analysis, behavioral dataflow, LLM semantic analysis, and cloud-based scanning
- **False Positive Filtering** - Meta-analyzer significantly reduces noise while preserving detection capability
- **CI/CD Ready** - SARIF output for GitHub Code Scanning, exit codes for build failures
- **Extensible** - Plugin architecture for custom analyzers
**[Join the Cisco AI Discord](https://discord.com/invite/nKWtDcXxtx)** to discuss, share feedback, or connect with the team.
---

#### ğŸ¨ é€‚ç”¨åœºæ™¯
- æ•°æ®ç§‘å­¦ä¸åˆ†æ
- æœºå™¨å­¦ä¹ é¡¹ç›®
- Web åº”ç”¨å¼€å‘

---

### 20. LuxTTS

**ğŸ”¥ çƒ­åº¦æŒ‡æ•°**: 813

**é¡¹ç›®åç§°**: [ysharma3501/LuxTTS](https://github.com/ysharma3501/LuxTTS)
**é¡¹ç›®æè¿°**: A high-quality rapid TTS voice cloning model that reaches speeds of 150x realtime.
**ä¸»è¦è¯­è¨€**: Python
**è®¸å¯è¯**: Apache License 2.0
**â­ æ˜Ÿæ ‡æ•°**: 762
**ğŸ´ Fork æ•°**: 102
**ğŸ‘€ å…³æ³¨æ•°**: 762
**ğŸ› å¼€æ”¾é—®é¢˜**: 14
**æœ€åæ›´æ–°**: 2026-02-15 (15åˆ†é’Ÿå‰)
**ğŸ“– é¡¹ç›®æ–‡æ¡£**: [README](3ziye-20260215-python/20-LuxTTS-Readme.md)

**ğŸ  é¡¹ç›®ä¸»é¡µ**: æ— 

**ğŸ“‚ å…‹éš†åœ°å€**: `https://github.com/ysharma3501/LuxTTS.git`

**ğŸ’» æŠ€æœ¯æ ˆ**: **ä¸»è¦è¯­è¨€**: Python

#### ğŸ¯ æ ¸å¿ƒç‰¹æ€§
- Voice cloning: SOTA voice cloning on par with models 10x larger.
- Clarity: Clear 48khz speech generation unlike most TTS models which are limited to 24khz.
- Speed: Reaches speeds of 150x realtime on a single GPU and faster then realtime on CPU's as well.
- Efficiency: Fits within 1gb vram meaning it can fit in any local gpu.

#### ğŸ¨ é€‚ç”¨åœºæ™¯
- æ•°æ®ç§‘å­¦ä¸åˆ†æ
- æœºå™¨å­¦ä¹ é¡¹ç›®
- Web åº”ç”¨å¼€å‘
- APIæœåŠ¡å¼€å‘

---