[
  {
    "basic_info": {
      "name": "GuitarPedal",
      "full_name": "torvalds/GuitarPedal",
      "owner": "torvalds",
      "description": null,
      "url": "https://github.com/torvalds/GuitarPedal",
      "clone_url": "https://github.com/torvalds/GuitarPedal.git",
      "ssh_url": "git@github.com:torvalds/GuitarPedal.git",
      "homepage": null,
      "created_at": "2025-09-17T01:01:29Z",
      "updated_at": "2025-09-17T18:16:37Z",
      "pushed_at": "2025-09-17T01:02:41Z"
    },
    "stats": {
      "stars": 283,
      "forks": 4,
      "watchers": 283,
      "open_issues": 5,
      "size": 322
    },
    "tech_info": {
      "language": null,
      "languages": {},
      "license": null,
      "topics": []
    },
    "content": {
      "readme": "## Random guitar pedal board design\n\n### Background\n\nThis is a personal toy project that has gone through several phases, but\nthe common theme has been that it makes absolutely no sense outside of\nthe very specific niche of \"Linus is trying to learn random things about\nelectronics\".\n\nSo keep that in mind: there is very little point to any of this to\nanybody else.  Don't expect some great useful guitar pedal experience.\n\nI call it my \"LEGO for adults\" hobby, because this got started when I\nwanted to extend my traditional after-Christmas activity (which was\nreceiving and building _actual_ LEGO kits, which has been a thing for me\nsince I was a wee tyke) with something else.\n\nSo for Christmas 2024, I got a new soldering iron and randomly started\ndoing guitar pedal kits.  And so over the next month or two, I built at\nleast two dozen kits, and had to literally look for victims to give them\naway to because I had no use for them myself.\n\n> [!NOTE]\n> Of all the kits I built, the ones I enjoyed the most were the Aion FX\n> ones, and if you are looking for a kit build of traditional analog\n> guitar pedals, I can heartily recommend them.\n>\n> The documentation, the customer service, the components, and the\n> enclosures were all top notch. See [\"Aion FX\"](https://aionfx.com/)\n\nAnyway, after building a lot of these traditional analog guitar pedal\nkits I decided I really wanted to actually understand what they did,\nbecause I really had very little experience with any analog circuits.\n\nWhile I've done some very limited electronics most of my life, almost\nall of it has been related to computers, so it's been either digital\nlogic or power supplies for them.\n\nAlso, I was looking for a different kind of soldering experience where\nthere was less snipping of legs of through-hole components.  I actually\nlike soldering SMT components, but that doesn't tend to be what those\nguitar pedal kits do.\n\nI had done some very limited PCB design with kicad a few years ago, so I\ndecided to just start learning more about analog circuits.  And then it\nkind of grew from that.\n\n### Electrical design\n\nThis is the \"fourth generation\" of my guitar pedal design journey, and\nis a new repository because the goal of the learning experience has\nevolved.\n\nWhat started out being about the analog circuits (and the power rails:\nthose were always a big thing) got to the point where I realized I\nreally want to do a mixed signal design: understanding what the circuits\ndo is one thing, re-creating some analog design from the 70s when you\ndon't actually care about the sound is another thing entirely.\n\nAlso, on the actual analog signal side, I started out using op-amps, but\nas I was attempting to learn how things actually worked, I had switched\nover to a \"discrete components only\" model, and this continues that\ntrend (except for the whole digital side, of course).\n\n> [!NOTE]\n> To me \"discrete components\" does include more optimized packages:\n> things like dual diodes or matched transistors, but not more complex\n> circuits like a op-amp (or a 555 timer or D Flip-flop or other classic\n> logic IC)\n\nAlso, because I don't typically *listen* to the end result, but look at\nit with a signal generator and an oscilloscope, I've grown to detest\npower supply noise.\n\nNot knowing what I was doing, quite a lot of my circuits have been very\nnoisy indeed, and have coupled in noise from the power supply into the\nsignal chain, and you can really see that on an oscilloscope even when\nit's not always audible.\n\nEven in op-amp designs, where the op-amp itself has a very high PSRR and\nisn't mixing power supply noise into the signal, my biasing circuits\nwere often not great, and so the op-amp would see not just the signal\nbut the power supply noise coming in through the DC biasing.\n\nAnd every time I tried a dual power rail (so that I could just keep the\nsignal ground-referenced), the noise from the switching ended up just\nalways noticeable, and the extra complexity was annoying when a lot of\neffects then didn't have any real use for the dual rail.\n\nFiltering obviously helps, but this is just a long-winded explanation\nfor why I ended up really appreciating the \"bias to ground\" JFET model\nfor the signal input side, and the common drain follower in particular.\n\nThat works with a single JFET (the MMBF5103 worked well for me), but my\nfavorite design so far is a dual-JFET LS844 with the second matched JFET\nused as a current sink.  It has basically infinite input impedance (and\ncould be DC coupled, although I do the coupling capacitor with resistor\nto ground) and gives a good output signal somewhere roughly in the\nmiddle of the single-supply 9V rail.\n\nSee [https://www.linearsystems.com/_files/ugd/7e8069_52b1022fbded45fab609459acb337629.pdf](LS844 Application note)\n\nWhy do I mention this in particular? Mainly because it's a great example\nof how completely *insane* my designs are.  That LS844 is used as a\nvoltage follower with a noticeable DC offset, and that single dual-JFET\nSOT-23-6 component is m",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-17T18:17:01.419473"
  },
  {
    "basic_info": {
      "name": "Asus-ROG-Aml-Deep-Dive",
      "full_name": "Zephkek/Asus-ROG-Aml-Deep-Dive",
      "owner": "Zephkek",
      "description": "A deep dive into the ACPI.sys DPC latency problems on Asus ROG laptops",
      "url": "https://github.com/Zephkek/Asus-ROG-Aml-Deep-Dive",
      "clone_url": "https://github.com/Zephkek/Asus-ROG-Aml-Deep-Dive.git",
      "ssh_url": "git@github.com:Zephkek/Asus-ROG-Aml-Deep-Dive.git",
      "homepage": null,
      "created_at": "2025-09-16T22:03:00Z",
      "updated_at": "2025-09-17T18:14:52Z",
      "pushed_at": "2025-09-17T13:51:47Z"
    },
    "stats": {
      "stars": 276,
      "forks": 8,
      "watchers": 276,
      "open_issues": 3,
      "size": 1487
    },
    "tech_info": {
      "language": "ASL",
      "languages": {
        "ASL": 8072935
      },
      "license": null,
      "topics": []
    },
    "content": {
      "readme": "# The ASUS Gaming Laptop ACPI Firmware Bug: A Deep Technical Investigation\r\n\r\n## If You're Here, You Know The Pain\r\n\r\nYou own a high-end ASUS ROG laptop perhaps a Strix, Scar, or Zephyrus. It's specifications are impressive: an RTX 30/40 series GPU, a top-tier Intel processor, and plenty of RAM. Yet, it stutters during basic tasks like watching a YouTube video, audio crackles and pops on Discord calls, the mouse cursor freezes for a split second, just long enough to be infuriating.\r\n\r\nYou've likely tried all the conventional fixes:\r\n- Updating every driver imaginable, multiple times.\r\n- Performing a \"clean\" reinstallation of Windows.\r\n- Disabling every conceivable power-saving option.\r\n- Manually tweaking processor interrupt affinities.\r\n- Following convoluted multi-step guides from Reddit threads.\r\n- Even installing Linux, only to find the problem persists.\r\n\r\nIf none of that worked, it's because the issue isn't with the operating system or a driver. The problem is far deeper, embedded in the machine's firmware, the BIOS.\r\n\r\n## Initial Symptoms and Measurement\r\n\r\n### The Pattern Emerges\r\n\r\nThe first tool in any performance investigator's toolkit for these symptoms is LatencyMon. It acts as a canary in the coal mine for system-wide latency issues. On an affected ASUS Zephyrus M16, the results are immediate and damning:\r\n\r\n```\r\nCONCLUSION\r\nYour system appears to be having trouble handling real-time audio and other tasks. \r\nYou are likely to experience buffer underruns appearing as drop outs, clicks or pops.\r\n\r\nHIGHEST MEASURED INTERRUPT TO PROCESS LATENCY\r\nHighest measured interrupt to process latency (Œºs):   65,816.60\r\nAverage measured interrupt to process latency (Œºs):   23.29\r\n\r\nHIGHEST REPORTED ISR ROUTINE EXECUTION TIME\r\nHighest ISR routine execution time (Œºs):              536.80\r\nDriver with highest ISR routine execution time:       ACPI.sys\r\n\r\nHIGHEST REPORTED DPC ROUTINE EXECUTION TIME  \r\nHighest DPC routine execution time (Œºs):              5,998.83\r\nDriver with highest DPC routine execution time:       ACPI.sys\r\n```\r\n\r\nThe data clearly implicates `ACPI.sys`. However, the per-CPU data reveals a more specific pattern:\r\n\r\n```\r\nCPU 0 Interrupt cycle time (s):                       208.470124\r\nCPU 0 ISR highest execution time (Œºs):                536.804674\r\nCPU 0 DPC highest execution time (Œºs):                5,998.834725\r\nCPU 0 DPC total execution time (s):                   90.558238\r\n```\r\n\r\nCPU 0 is taking the brunt of the impact, spending over 90 seconds processing interrupts while other cores remain largely unaffected. This isn't a failure of load balancing; it's a process locked to a single core.\r\n\r\nA similar test on a Scar 15 from 2022 shows the exact same culprit: high DPC latency originating from `ACPI.sys`.\r\n\r\n<img width=\"974\" height=\"511\" alt=\"latencymon\" src=\"https://github.com/user-attachments/assets/fdf6f26a-dda8-4561-82c7-349fc8c298ab\" />\r\n\r\nIt's easy to blame a Windows driver, but `ACPI.sys` is not a typical driver. It primarily functions as an interpreter for ACPI Machine Language (AML), the code provided by the laptop's firmware (BIOS). If `ACPI.sys` is slow, it's because the firmware is feeding it inefficient or flawed AML code to execute. These slowdowns are often triggered by General Purpose Events (GPEs) and traffic from the Embedded Controller (EC). To find the true source, we must dig deeper.\r\n\r\n## Capturing the Problem in More Detail: ETW Tracing\r\n\r\n### Setting Up Advanced ACPI Tracing\r\n\r\nTo understand what `ACPI.sys` is doing during these latency spikes, we can use Event Tracing for Windows (ETW) to capture detailed logs from the ACPI providers.\r\n\r\n```powershell\r\n# Find the relevant ACPI ETW providers\r\nlogman query providers | findstr /i acpi\r\n# This returns two key providers:\r\n# Microsoft-Windows-Kernel-Acpi {C514638F-7723-485B-BCFC-96565D735D4A}\r\n# Microsoft-ACPI-Provider {DAB01D4D-2D48-477D-B1C3-DAAD0CE6F06B}\r\n\r\n# Start a comprehensive trace session\r\nlogman start ACPITrace -p {DAB01D4D-2D48-477D-B1C3-DAAD0CE6F06B} 0xFFFFFFFF 5 -o C:\\Temp\\acpi.etl -ets\r\nlogman update ACPITrace -p {C514638F-7723-485B-BCFC-96565D735D4A} 0xFFFFFFFF 5 -ets\r\n\r\n# Then once we're done we can stop the trace and check the etl file and save the data in csv format aswell.\r\nlogman stop ACPITrace -ets\r\ntracerpt C:\\Temp\\acpi.etl -o C:\\Temp\\acpi_events.csv -of CSV\r\n```\r\n\r\n### An Unexpected Discovery\r\n\r\nAnalyzing the resulting trace file in the Windows Performance Analyzer reveals a crucial insight. The spikes aren't random; they are periodic, occurring like clockwork every 30 to 60 seconds.\r\n\r\n<img width=\"1673\" height=\"516\" alt=\"61c7abb1-d7aa-4b69-9a88-22cca7352f00\" src=\"https://github.com/user-attachments/assets/2aac7320-3e06-4025-841c-86129f9d5b62\" />\r\n\r\nRandom interruptions often suggest hardware faults or thermal throttling. A perfectly repeating pattern points to a systemic issue, a timer or a scheduled event baked into the system's logic.\r\n\r\nThe raw event data confirms this pattern:\r\n```csv\r\nClock-",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-17T18:17:02.634921"
  },
  {
    "basic_info": {
      "name": "provenance-action",
      "full_name": "danielroe/provenance-action",
      "owner": "danielroe",
      "description": "GitHub Action that detects dependency provenance downgrades from lockfile changes (npm/pnpm/yarn).",
      "url": "https://github.com/danielroe/provenance-action",
      "clone_url": "https://github.com/danielroe/provenance-action.git",
      "ssh_url": "git@github.com:danielroe/provenance-action.git",
      "homepage": "",
      "created_at": "2025-09-16T11:08:14Z",
      "updated_at": "2025-09-17T17:54:44Z",
      "pushed_at": "2025-09-17T15:20:37Z"
    },
    "stats": {
      "stars": 214,
      "forks": 3,
      "watchers": 214,
      "open_issues": 1,
      "size": 108
    },
    "tech_info": {
      "language": "TypeScript",
      "languages": {
        "TypeScript": 57180
      },
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "# `danielroe/provenance-action`\n\nDetect and fail CI when dependencies in your lockfile lose npm provenance or trusted publisher status.\n\n> [!WARNING]\n> This action is under active development and is only one tool to assist in securing your dependencies.\n\n## ‚ú® Features\n- supports `pnpm-lock.yaml`, `package-lock.json`, `yarn.lock` (v1 and v2+), `bun.lock`\n- handles transitives by comparing resolved versions\n- inline GitHub annotations at the lockfile line\n- JSON output and optional hard‚Äëfail (default: on)\n- pure TypeScript, Node 24+\n\nüëâ See it in action: [danielroe/provenance-action-test](https://github.com/danielroe/provenance-action-test)\n\n## üöÄ Quick start\n```yaml\nname: ci\non:\n  pull_request:\n    branches:\n      - main\npermissions:\n  contents: read\njobs:\n  check-provenance:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n        with:\n          fetch-depth: 0\n      - name: Check provenance downgrades\n        uses: danielroe/provenance-action@main\n        id: check\n        with:\n          fail-on-provenance-change: true # optional, default: false\n        #   lockfile: pnpm-lock.yaml      # optional\n        #   base-ref: origin/main         # optional, default: origin/main\n        #   fail-on-downgrade: true       # optional, default: true\n      - name: Print result\n        run: \"echo 'Downgraded: ${{ steps.check.outputs.downgraded }}'\"\n```\n\n## üîß Inputs\n- `lockfile` (optional): Path to the lockfile. Auto-detected if omitted.\n- `workspace-path` (optional): Path to workspace root. Default: `.`\n- `base-ref` (optional): Git ref to compare against. Default: `origin/main`.\n- `fail-on-downgrade` (optional): Controls failure behavior. Accepts `true`, `false`, `any`, or `only-provenance-loss`. Default: `true` (which is the same as `any`).\n- `fail-on-provenance-change` (optional): When `true`, fail on provenance repository/branch changes. Default: `false`.\n\n## üì§ Outputs\n- `downgraded`: JSON array of `{ name, from, to, downgradeType }` for detected downgrades. `downgradeType` is `provenance` or `trusted_publisher`.\n- `changed`: JSON array of provenance change events `{ name, from, to, type, previousRepository?, newRepository?, previousBranch?, newBranch? }`.\n\n## üß† How it works\n1. Diffs your lockfile against the base ref and collects changed resolved versions (including transitives).\n2. Checks npm provenance via the attestations API for each `name@version`.\n3. Falls back to version metadata for `dist.attestations`.\n4. Emits file+line annotations in the lockfile.\n5. If provenance exists for both the previous and new version, extracts GitHub `owner/repo` and branch from attestations and warns when they differ (repo changed or branch changed).\n\n## üîí Why this matters\nTrusted publishing links a package back to its source repo and build workflow, providing strong provenance guarantees. It helps ensure the package you install corresponds to audited source and CI.\n\nHowever, maintainers can still be phished or coerced into publishing without trusted publishing enabled, or switching to a non‚Äëtrusted path. In those cases, packages may still carry attestations, but the chain back to the trusted publisher can be weakened.\n\nThis action:\n- Detects when a dependency update loses npm provenance (no attestations) or loses trusted publisher (attestations but no trusted publisher marker), and\n- Fails CI by default (configurable), before that change lands in your main branch.\n\nThis is a stopgap until package managers enforce stronger policies natively. Until then, it offers a lightweight guardrail in CI.\n\n## ‚ö†Ô∏è Notes\n- Runs on Node 24+ and executes the TypeScript entrypoint directly.\n- `bun.lockb` is not supported. (You can generate a `bun.lock` with `bun install --save-text-lockfile`.)\n- Repository and branch change detection is best‚Äëeffort; attestation shapes vary and some packages omit repo/ref details.\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-17T18:17:03.865020"
  },
  {
    "basic_info": {
      "name": "VoxCPM",
      "full_name": "OpenBMB/VoxCPM",
      "owner": "OpenBMB",
      "description": "VoxCPM: Tokenizer-Free TTS for Context-Aware Speech Generation and True-to-Life Voice Cloning",
      "url": "https://github.com/OpenBMB/VoxCPM",
      "clone_url": "https://github.com/OpenBMB/VoxCPM.git",
      "ssh_url": "git@github.com:OpenBMB/VoxCPM.git",
      "homepage": "",
      "created_at": "2025-09-16T03:41:49Z",
      "updated_at": "2025-09-17T18:13:53Z",
      "pushed_at": "2025-09-17T17:21:51Z"
    },
    "stats": {
      "stars": 208,
      "forks": 18,
      "watchers": 208,
      "open_issues": 3,
      "size": 1507
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 107143
      },
      "license": "Apache License 2.0",
      "topics": []
    },
    "content": {
      "readme": "## üéôÔ∏è VoxCPM: Tokenizer-Free TTS for Context-Aware Speech Generation and True-to-Life Voice Cloning\n\n\n[![Project Page](https://img.shields.io/badge/Project%20Page-GitHub-blue)](https://github.com/OpenBMB/VoxCPM/) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-OpenBMB-yellow)](https://huggingface.co/openbmb/VoxCPM-0.5B) [![ModelScope](https://img.shields.io/badge/ModelScope-OpenBMB-purple)](https://modelscope.cn/models/OpenBMB/VoxCPM-0.5B)  [![Live Playground](https://img.shields.io/badge/Live%20PlayGround-Demo-orange)](https://huggingface.co/spaces/OpenBMB/VoxCPM-Demo) [![Samples](https://img.shields.io/badge/Page-Samples-red)](https://openbmb.github.io/VoxCPM-demopage)\n\n\n\n<div align=\"center\">\n  <img src=\"assets/voxcpm_logo.png\" alt=\"VoxCPM Logo\" width=\"40%\">\n</div>\n\n<div align=\"center\">\n\nüëã Contact us on [WeChat](assets/wechat.png)\n\n</div>\n\n## News \n* [2025.09.16] üî• üî• üî•  We Open Source the VoxCPM-0.5B [weights](https://huggingface.co/openbmb/VoxCPM-0.5B)!\n* [2025.09.16] üéâ üéâ üéâ  We Provide the [Gradio PlayGround](https://huggingface.co/spaces/OpenBMB/VoxCPM-Demo) for VoxCPM-0.5B, try it now! \n\n## Overview\n\nVoxCPM is a novel tokenizer-free Text-to-Speech (TTS) system that redefines realism in speech synthesis. By modeling speech in a continuous space, it overcomes the limitations of discrete tokenization and enables two flagship capabilities: context-aware speech generation and true-to-life zero-shot voice cloning.\n\nUnlike mainstream approaches that convert speech to discrete tokens, VoxCPM uses an end-to-end diffusion autoregressive architecture that directly generates continuous speech representations from text. Built on [MiniCPM-4](https://huggingface.co/openbmb/MiniCPM4-0.5B) backbone, it achieves implicit semantic-acoustic decoupling through hierachical language modeling and FSQ constraints, greatly enhancing both expressiveness and generation stability.\n\n<div align=\"center\">\n  <img src=\"assets/voxcpm_model.png\" alt=\"VoxCPM Model Architecture\" width=\"90%\">\n</div>\n\n\n###  üöÄ Key Features\n- **Context-Aware, Expressive Speech Generation** - VoxCPM comprehends text to infer and generate appropriate prosody, delivering speech with remarkable expressiveness and natural flow. It spontaneously adapts speaking style based on content, producing highly fitting vocal expression trained on a massive 1.8 million-hour bilingual corpus.\n- **True-to-Life Voice Cloning** - With only a short reference audio clip, VoxCPM performs accurate zero-shot voice cloning, capturing not only the speaker‚Äôs timbre but also fine-grained characteristics such as accent, emotional tone, rhythm, and pacing to create a faithful and natural replica.\n- **High-Efficiency Synthesis** - VoxCPM supports streaming synthesis with a Real-Time Factor (RTF) as low as 0.17 on a consumer-grade NVIDIA RTX 4090 GPU, making it possible for real-time applications.\n\n\n\n\n\n##  Quick Start\n\n### üîß Install from PyPI\n``` sh\npip install voxcpm\n```\n### 1.  Model Download (Optional)\nBy default, when you first run the script, the model will be downloaded automatically, but you can also download the model in advance.\n- Download VoxCPM-0.5B\n    ```\n    from huggingface_hub import snapshot_download\n    snapshot_download(\"openbmb/VoxCPM-0.5B\",local_files_only=local_files_only)\n    ```\n- Download ZipEnhancer and SenseVoice-Small. We use ZipEnhancer to enhance speech prompts and SenseVoice-Small for speech prompt ASR in the web demo.\n    ```\n    from modelscope import snapshot_download\n    snapshot_download('iic/speech_zipenhancer_ans_multiloss_16k_base')\n    snapshot_download('iic/SenseVoiceSmall')\n    ```\n\n### 2. Basic Usage\n```python\nimport soundfile as sf\nfrom voxcpm import VoxCPM\n\nmodel = VoxCPM.from_pretrained(\"openbmb/VoxCPM-0.5B\")\n\nwav = model.generate(\n    text=\"VoxCPM is an innovative end-to-end TTS model from ModelBest, designed to generate highly expressive speech.\",\n    prompt_wav_path=None,      # optional: path to a prompt speech for voice cloning\n    prompt_text=None,          # optional: reference text\n    cfg_value=2.0,             # LM guidance on LocDiT, higher for better adherence to the prompt, but maybe worse\n    inference_timesteps=10,   # LocDiT inference timesteps, higher for better result, lower for fast speed\n    normalize=True,           # enable external TN tool\n    denoise=True,             # enable external Denoise tool\n    retry_badcase=True,        # enable retrying mode for some bad cases (unstoppable)\n    retry_badcase_max_times=3,  # maximum retrying times\n    retry_badcase_ratio_threshold=6.0, # maximum length restriction for bad case detection (simple but effective), it could be adjusted for slow pace speech\n)\n\nsf.write(\"output.wav\", wav, 16000)\nprint(\"saved: output.wav\")\n```\n\n### 3. CLI Usage\n\nAfter installation, the entry point is `voxcpm` (or use `python -m voxcpm.cli`).\n\n```bash\n# 1) Direct synthesis (single text)\nvoxcpm --text \"VoxCPM is an innovative end-to-end TTS model from ModelBest, designed to generat",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-17T18:17:05.106212"
  },
  {
    "basic_info": {
      "name": "Qwen3-ASR-Toolkit",
      "full_name": "QwenLM/Qwen3-ASR-Toolkit",
      "owner": "QwenLM",
      "description": "Official Python toolkit for the Qwen3-ASR API. Parallel high‚Äëthroughput calls, robust long‚Äëaudio transcription, multi‚Äësample‚Äërate support.",
      "url": "https://github.com/QwenLM/Qwen3-ASR-Toolkit",
      "clone_url": "https://github.com/QwenLM/Qwen3-ASR-Toolkit.git",
      "ssh_url": "git@github.com:QwenLM/Qwen3-ASR-Toolkit.git",
      "homepage": "",
      "created_at": "2025-09-16T09:03:49Z",
      "updated_at": "2025-09-17T18:14:23Z",
      "pushed_at": "2025-09-17T07:22:44Z"
    },
    "stats": {
      "stars": 143,
      "forks": 6,
      "watchers": 143,
      "open_issues": 0,
      "size": 10
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 11717
      },
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "# Qwen3-ASR-Toolkit\n\n[![PyPI version](https://badge.fury.io/py/qwen3-asr-toolkit.svg)](https://badge.fury.io/py/qwen3-asr-toolkit)\n[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://www.python.org/downloads/)\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n\nAn advanced, high-performance Python command-line toolkit for using the **Qwen-ASR API** (formerly Qwen3-ASR-Flash). This implementation overcomes the API's 3-minute audio length limitation by intelligently splitting long audio/video files and processing them in parallel, enabling rapid transcription of hours-long content.\n\n## üöÄ Key Features\n\n-   **Break the 3-Minute Limit**: Seamlessly transcribe audio and video files of any length by bypassing the official API's duration constraint.\n-   **Smart Audio Splitting**: Utilizes **Voice Activity Detection (VAD)** to split audio into meaningful chunks at natural silent pauses. This ensures that words and sentences are not awkwardly cut off.\n-   **High-Speed Parallel Processing**: Leverages multi-threading to send audio chunks to the Qwen-ASR API concurrently, dramatically reducing the total transcription time for long files.\n-   **Automatic Audio Resampling**: Automatically converts audio from any sample rate and channel count to the 16kHz mono format required by the Qwen-ASR API. You can use any audio file without worrying about pre-processing.\n-   **Universal Media Support**: Supports virtually any audio and video format (e.g., `.mp4`, `.mov`, `.mkv`, `.mp3`, `.wav`, `.m4a`) thanks to its reliance on FFmpeg.\n-   **Simple & Easy to Use**: A straightforward command-line interface allows you to get started with just a single command.\n\n## ‚öôÔ∏è How It Works\n\nThis tool follows a robust pipeline to deliver fast and accurate transcriptions for long-form media:\n\n1.  **Media Loading**: The script first loads your local audio or video file.\n2.  **VAD-based Chunking**: It analyzes the audio stream using Voice Activity Detection (VAD) to identify silent segments.\n3.  **Intelligent Splitting**: The audio is then split into smaller chunks based on the detected silences. Each chunk is kept under the 3-minute API limit, preventing mid-sentence cuts.\n4.  **Parallel API Calls**: A thread pool is initiated to upload and process these chunks concurrently using the DashScope Qwen-ASR API.\n5.  **Result Aggregation**: The transcribed text segments from all chunks are collected, re-ordered, and saved.\n\n## üèÅ Getting Started\n\nFollow these steps to set up and run the project on your local machine.\n\n### Prerequisites\n\n-   Python 3.8 or higher.\n-   **FFmpeg**: The script requires FFmpeg to be installed on your system to handle media files.\n    -   **Ubuntu/Debian**: `sudo apt update && sudo apt install ffmpeg`\n    -   **macOS**: `brew install ffmpeg`\n    -   **Windows**: Download from the [official FFmpeg website](https://ffmpeg.org/download.html) and add it to your system's PATH.\n-   **DashScope API Key**: You need an API key from Alibaba Cloud's DashScope.\n    -   You can obtain one from the [DashScope Console](https://dashscope.console.aliyun.com/apiKey). If you are calling the API services of Tongyi Qwen for the first time, you can follow the tutorial on [this website](https://help.aliyun.com/zh/model-studio/first-api-call-to-qwen) to create your own API Key.\n    -   For better security and convenience, it is **highly recommended** to set your API key as an environment variable named `DASHSCOPE_API_KEY`. The script will automatically use it, and you won't need to pass the `--api-key` argument in the command.\n\n        **On Linux/macOS:**\n        ```bash\n        export DASHSCOPE_API_KEY=\"your_api_key_here\"\n        ```\n        *(To make this permanent, add the line to your `~/.bashrc`, `~/.zshrc`, or `~/.profile` file.)*\n\n        **On Windows (Command Prompt):**\n        ```cmd\n        set DASHSCOPE_API_KEY=\"your_api_key_here\"\n        ```\n\n        **On Windows (PowerShell):**\n        ```powershell\n        $env:DASHSCOPE_API_KEY=\"your_api_key_here\"\n        ```\n        *(For a permanent setting on Windows, search for \"Edit the system environment variables\" in the Start Menu and add `DASHSCOPE_API_KEY` to your user variables.)*\n\n### Installation\n\nWe recommend installing the tool directly from PyPI for the simplest setup.\n\n#### Option 1: Install from PyPI (Recommended)\n\nSimply run the following command in your terminal. This will install the package and make the `qwen3-asr` command available system-wide.\n\n```bash\npip install qwen3-asr-toolkit\n```\n\n#### Option 2: Install from Source\n\nIf you want to install the latest development version or contribute to the project, you can install from the source code.\n\n1.  Clone the repository:\n    ```bash\n    git clone https://github.com/QwenLM/Qwen3-ASR-Toolkit.git\n    cd Qwen3-ASR-Toolkit\n    ```\n\n2.  Install the package:\n    ```bash\n    pip install .\n    ```\n\n## üìñ Usage\n\nOnce installed, you can use the `qwen3-asr` command dire",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-17T18:17:06.327492"
  },
  {
    "basic_info": {
      "name": "LLaVA-OneVision-1.5",
      "full_name": "EvolvingLMMs-Lab/LLaVA-OneVision-1.5",
      "owner": "EvolvingLMMs-Lab",
      "description": null,
      "url": "https://github.com/EvolvingLMMs-Lab/LLaVA-OneVision-1.5",
      "clone_url": "https://github.com/EvolvingLMMs-Lab/LLaVA-OneVision-1.5.git",
      "ssh_url": "git@github.com:EvolvingLMMs-Lab/LLaVA-OneVision-1.5.git",
      "homepage": null,
      "created_at": "2025-09-16T14:05:47Z",
      "updated_at": "2025-09-17T17:52:23Z",
      "pushed_at": "2025-09-17T17:02:14Z"
    },
    "stats": {
      "stars": 136,
      "forks": 7,
      "watchers": 136,
      "open_issues": 2,
      "size": 2671
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 7579110,
        "Jupyter Notebook": 521664,
        "Shell": 177763,
        "C++": 38717,
        "Cuda": 16941,
        "C": 2951,
        "Dockerfile": 2631,
        "HTML": 2625,
        "Makefile": 313
      },
      "license": null,
      "topics": []
    },
    "content": {
      "readme": "# LLaVA-OneVision-1.5: Fully Open Framework for Democratized Multimodal Training\n\n\n[ü§ó Mid-Training-Data (Uploading!)](https://huggingface.co/datasets/lmms-lab/LLaVA-One-Vision-1.5-Mid-Training-85M) | \n[ü§ó Insturct-Data (Uploading!)](https://huggingface.co/datasets/lmms-lab/LLaVA-One-Vision-1.5-Insturct-26M) \n\n**LLaVA-OneVision1.5** introduces a novel family of **fully open-source** Large Multimodal Models (LMMs) that achieves **state-of-the-art performance**  with substantially **lower cost** through training on **native resolution** images.\n\n1. **Superior Performance**\nA family of fully open-source large multimodal models demonstrating **superior performance** across multiple multimodal benchmarks, **outperforming Qwen2.5-VL** in most evaluation tasks.\n\n2. **High-Quality Data at Scale**\nMeticulously curated **pre-training and SFT data** with rigorous filtering and quality control, achieving **superior data efficiency** with only **64B tokens**.\n- Concept-balanced, highly diverse, high-quality caption data\n- Comprehensive instruction fine-tuning data covering a wide range of tasks\n\n3. **Ultra-Efficient Training Framework**\nComplete end-to-end training framework designed for maximum efficiency:\n- **$16K total budget** for full model training\n- **45% HFU efficiency** on A100 GPUs ($0.6 per GPU/Hour)\n- Built on **MegatronLM** with support for **MoE**, **FP8**, and **long sequence parallelization**\n- Optimized codebase for cost-effective scaling\n\n4. **Fully Open Framework** for community access and reproducibility:\n- ‚úÖ High-quality pre-training & SFT data\n- ‚úÖ Complete training framework & code\n- ‚úÖ Training recipes & configurations\n- ‚úÖ Base & instruct model checkpoints\n- ‚úÖ Comprehensive training logs & metrics\n\n\n## Model\n\n| Model                  | #Vision Param | #Language Param | #Total Param | HF Link                                                                      |\n|------------------------|---------------|-----------------|--------------|------------------------------------------------------------------------------|\n| LLaVA-OV-1.5-4B-Instruct      | 0.3B          | 4.4B            | 4.7B         | [ü§ó link]()                |\n| LLaVA-OV-1.5-8B-Instruct      | 0.3B          | 8.2B            | 8.5B         | [ü§ó link](https://huggingface.co/lmms-lab/LLaVA-OneVision-1.5-8B-Instruct) |\n\n\n## Dataset\n\n![Dataset Visualization](asset/dataset.jpg)\n\n\n| Description | Link |\n|-------------|------|\n| Mid-training data for LLaVA-OneVision-1.5 | [ü§ó Download (Uploading!)](https://huggingface.co/datasets/lmms-lab/LLaVA-One-Vision-1.5-Mid-Training-85M) |\n| SFT data for LLaVA-OneVision-1.5 | [ü§ó Download (Uploading!)](https://huggingface.co/datasets/lmms-lab/LLaVA-OneVision-1.5-Insturct-Data) |\n\n\n## Evaluation Results\n\n\nAll evaluations were conducted using lmms_eval.\n\n|                                  | **LLaVA-OV-1.5-8B** | **Qwen2.5 VL 7B** | **LLaVA-OV-1.5-4B** | **Qwen2.5 VL 3B** |\n|:----------------------------------|:---------------:|:-------------:|:---------------:|:-------------:|\n| MMMU (Validation)                 |    **55.44**    |     51.33     |    **51.44**    |     46.44     |\n| MMMU-Pro (Standard)               |    **37.40**    |     36.30     |    **33.24**    |     31.10     |\n| MMMU-Pro (Vision)                 |      25.15      |   **32.83**   |    **23.53**    |     21.27     |\n| MMBench (English; Test)           |    **84.14**    |     83.40     |    **82.29**    |     77.97     |\n| MMBench (Chinese; Test)           |      81.00      |   **81.61**   |    **76.73**    |     74.55     |\n| MME-RealWorld (English)           |    **62.31**    |     57.33     |    **57.16**    |     51.60     |\n| MME-RealWorld (Chinese)           |    **56.11**    |     51.50     |      21.38      |   **45.38**   |\n| AI2D (With Mask)                  |    **84.16**    |     82.58     |    **84.62**    |     78.56     |\n| AI2D (Without Mask)               |    **94.11**    |     93.36     |    **92.84**    |     90.74     |\n| CV-Bench                          |    **80.82**    |     79.95     |    **74.00**    |     71.53     |\n| VL-RewardBench                    |      45.90      |   **49.65**   |    **45.90**    |     42.06     |\n| V*                                |    **78.01**    |     76.96     |      66.49      |   **69.63**   |\n| PixmoCount                        |      62.19      |   **63.33**   |    **59.17**    |     50.85     |\n| CountBench                        |    **88.19**    |     86.35     |    **77.80**    |     72.51     |\n| ChartQA                           |    **86.48**    |     84.08     |    **85.11**    |     83.36     |\n| CharXiv (Direct Questions)        |    **74.10**    |     69.80     |    **70.70**    |     58.20     |\n| DocVQA (Test)                     |    **95.00**    |     94.93     |    **93.48**    |     92.67     |\n| InfoVQA (Test)                    |      78.42      |   **81.67**   |    **75.27**    |     75.63     |\n| WeMath                            |    **33.62**    |     33.",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-17T18:17:07.547475"
  },
  {
    "basic_info": {
      "name": "vibe-coding-playbook",
      "full_name": "RiyaParikh0112/vibe-coding-playbook",
      "owner": "RiyaParikh0112",
      "description": null,
      "url": "https://github.com/RiyaParikh0112/vibe-coding-playbook",
      "clone_url": "https://github.com/RiyaParikh0112/vibe-coding-playbook.git",
      "ssh_url": "git@github.com:RiyaParikh0112/vibe-coding-playbook.git",
      "homepage": null,
      "created_at": "2025-09-16T13:58:41Z",
      "updated_at": "2025-09-17T17:55:11Z",
      "pushed_at": "2025-09-16T17:58:27Z"
    },
    "stats": {
      "stars": 111,
      "forks": 51,
      "watchers": 111,
      "open_issues": 1,
      "size": 50
    },
    "tech_info": {
      "language": null,
      "languages": {},
      "license": null,
      "topics": []
    },
    "content": {
      "readme": "# vibe-coding-playbook",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-17T18:17:08.763305"
  },
  {
    "basic_info": {
      "name": "VibeVoice-finetuning",
      "full_name": "voicepowered-ai/VibeVoice-finetuning",
      "owner": "voicepowered-ai",
      "description": "Unofficial WIP LoRa Finetuning repository for VibeVoice",
      "url": "https://github.com/voicepowered-ai/VibeVoice-finetuning",
      "clone_url": "https://github.com/voicepowered-ai/VibeVoice-finetuning.git",
      "ssh_url": "git@github.com:voicepowered-ai/VibeVoice-finetuning.git",
      "homepage": null,
      "created_at": "2025-09-16T10:57:57Z",
      "updated_at": "2025-09-17T15:27:47Z",
      "pushed_at": "2025-09-16T16:31:47Z"
    },
    "stats": {
      "stars": 110,
      "forks": 25,
      "watchers": 110,
      "open_issues": 0,
      "size": 189
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 312954
      },
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "\r\n  \r\n\r\n# Unofficial WIP Finetuning repo for VibeVoice\r\n\r\n  \r\n\r\n# Hardware requirements\r\n\r\n  \r\n\r\nTo train a VibeVoice 1.5B LoRa, a machine with at least 16gb VRAM is recommended.\r\n\r\nTo train a VibeVoice 7B LoRa, a machine with at least 48gb VRAM is recommended.\r\n\r\nKeep in mind longer audios increase VRAM requirements\r\n\r\n  \r\n\r\n# Installation\r\n\r\nIt is recommended to install this in a fresh environment. Specifically, the Dockerized environment `runpod/pytorch:2.8.0-py3.11-cuda12.8.1-cudnn-devel-ubuntu22.04` has been tested to work.\r\n\r\n  \r\n\r\nTransformers version 4.51.3 is known to work, while other versions have errors related to Qwen2 architecture.\r\n\r\n  \r\n```\r\ngit clone https://github.com/voicepowered-ai/VibeVoice-finetuning\r\n\r\npip install -e .\r\n\r\npip uninstall -y transformers && pip install transformers==4.51.3\r\n\r\n(OPTIONAL) wandb login\r\n\r\n(OPTIONAL) export HF_HOME=/workspace/hf_models\r\n```\r\n\r\n  \r\n\r\n# Usage\r\n\r\n  \r\n\r\n## VibeVoice 1.5B / 7B (LoRA) fine-tuning\r\n\r\n  \r\n\r\n  \r\n\r\nWe put some code together for training VibeVoice (7B) with LoRA. This uses the vendored VibeVoice model/processor and trains with a dual loss: masked CE on text tokens plus diffusion MSE on acoustic latents.\r\n\r\n  \r\n\r\n  \r\n\r\nRequirements:\r\n\r\n  \r\n\r\n- Download a compatible VibeVoice 7B or 1.5b checkpoint (config + weights) and its processor files (preprocessor_config.json) or run straight from HF model.\r\n\r\n- A 24khz audio dataset with audio files (target audio), text prompts (transcriptions) and optionally voice prompts (reference audio)\r\n\r\n  \r\n\r\n  \r\n  \r\n\r\n### Training with Hugging Face Dataset\r\n\r\n  \r\n```\r\npython -m src.finetune_vibevoice_lora \\\r\n\r\n--model_name_or_path aoi-ot/VibeVoice-Large \\\r\n\r\n--processor_name_or_path src/vibevoice/processor \\\r\n\r\n--dataset_name your/dataset \\\r\n\r\n--text_column_name text \\\r\n\r\n--audio_column_name audio \\\r\n\r\n--voice_prompts_column_name voice_prompts \\\r\n\r\n--output_dir outputTrain3 \\\r\n\r\n--per_device_train_batch_size 8 \\\r\n\r\n--gradient_accumulation_steps 16 \\\r\n\r\n--learning_rate 2.5e-5 \\\r\n\r\n--num_train_epochs 5 \\\r\n\r\n--logging_steps 10 \\\r\n\r\n--save_steps 100 \\\r\n\r\n--eval_steps 100 \\\r\n\r\n--report_to wandb \\\r\n\r\n--remove_unused_columns False \\\r\n\r\n--bf16 True \\\r\n\r\n--do_train \\\r\n\r\n--gradient_clipping \\\r\n\r\n--gradient_checkpointing False \\\r\n\r\n--ddpm_batch_mul 4 \\\r\n\r\n--diffusion_loss_weight 1.4 \\\r\n\r\n--train_diffusion_head True \\\r\n\r\n--ce_loss_weight 0.04 \\\r\n\r\n--voice_prompt_drop_rate 0.2 \\\r\n\r\n--lora_target_modules q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj \\\r\n\r\n--lr_scheduler_type cosine \\\r\n\r\n--warmup_ratio 0.03 \\\r\n\r\n--max_grad_norm 0.8\r\n```\r\n  \r\n\r\n----------\r\n\r\n  \r\n\r\n### Training with Local JSONL Dataset\r\n\r\n  \r\n```\r\npython -m src.finetune_vibevoice_lora \\\r\n\r\n--model_name_or_path aoi-ot/VibeVoice-Large \\\r\n\r\n--processor_name_or_path src/vibevoice/processor \\\r\n\r\n--train_jsonl prompts.jsonl \\\r\n\r\n--text_column_name text \\\r\n\r\n--audio_column_name audio \\\r\n\r\n--output_dir outputTrain3 \\\r\n\r\n--per_device_train_batch_size 8 \\\r\n\r\n--gradient_accumulation_steps 16 \\\r\n\r\n--learning_rate 2.5e-5 \\\r\n\r\n--num_train_epochs 5 \\\r\n\r\n--logging_steps 10 \\\r\n\r\n--save_steps 100 \\\r\n\r\n--report_to wandb \\\r\n\r\n--remove_unused_columns False \\\r\n\r\n--bf16 True \\\r\n\r\n--do_train \\\r\n\r\n--gradient_clipping \\\r\n\r\n--gradient_checkpointing False \\\r\n\r\n--ddpm_batch_mul 4 \\\r\n\r\n--diffusion_loss_weight 1.4 \\\r\n\r\n--train_diffusion_head True \\\r\n\r\n--ce_loss_weight 0.04 \\\r\n\r\n--voice_prompt_drop_rate 0.2 \\\r\n\r\n--lora_target_modules q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj \\\r\n\r\n--lr_scheduler_type cosine \\\r\n\r\n--warmup_ratio 0.03 \\\r\n\r\n--max_grad_norm 0.8\r\n```\r\n\r\n\r\n### JSONL format:\r\n\r\n  \r\n\r\nYou can provide an optional `voice_prompts` key. If it is omitted, a voice prompt will be automatically generated from the target audio.\r\n\r\n  \r\n\r\n**Example without a pre-defined voice prompt (will be auto-generated):**\r\n\r\n`{\"text\": \"Speaker 0: Speaker0 transcription.\", \"audio\": \"/workspace/wavs/segment_000000.wav\"}`\r\n\r\n  \r\n\r\n**Example with a pre-defined voice prompt:**\r\n\r\n`{\"text\": \"Speaker 0: Speaker0 transcription.\", \"audio\": \"/workspace/wavs/segment_000000.wav\", \"voice_prompts\": \"/path/to/a/different/prompt.wav\"}`\r\n\r\n  \r\n\r\n**Example with multiple speakers and voice prompts:**\r\n\r\n`{\"text\": \"Speaker 0: How is the project coming along?\\nSpeaker 1: It's going well, we should be finished by Friday.\", \"audio\": \"/data/conversations/convo_01.wav\", \"voice_prompts\": [\"/data/prompts/alice_voice_prompt.wav\", \"/data/prompts/bob_voice_prompt.wav\"]}`\r\n\r\n  \r\n  \r\n  \r\n\r\n# Notes:\r\n\r\n  \r\n\r\n- Audio is assumed to be 24 kHz; input audio will be loaded/resampled to 24 kHz.\r\n\r\n  \r\n\r\n- If you pass raw NumPy arrays or torch Tensors as audio (without sampling rate metadata), the collator assumes they are already 24 kHz. To trigger resampling, provide dicts like {\"array\": <np.ndarray>, \"sampling_rate\": <int>} or file paths.\r\n\r\n  \r\n\r\n- Tokenizers (acoustic/semantic) are frozen by default. LoRA is applied to the LLM (Qwen) and optionally to the diffusion head.\r\n\r\n  \r\n\r\n- The collator builds interleaved seq",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-17T18:17:10.001336"
  },
  {
    "basic_info": {
      "name": "aisdk-prompt-optimizer",
      "full_name": "Scale3-Labs/aisdk-prompt-optimizer",
      "owner": "Scale3-Labs",
      "description": "A tool kit for generating high quality prompts for AISDK using DSPy GEPA optimizer",
      "url": "https://github.com/Scale3-Labs/aisdk-prompt-optimizer",
      "clone_url": "https://github.com/Scale3-Labs/aisdk-prompt-optimizer.git",
      "ssh_url": "git@github.com:Scale3-Labs/aisdk-prompt-optimizer.git",
      "homepage": null,
      "created_at": "2025-09-16T17:54:46Z",
      "updated_at": "2025-09-17T18:15:39Z",
      "pushed_at": "2025-09-17T15:57:09Z"
    },
    "stats": {
      "stars": 90,
      "forks": 5,
      "watchers": 90,
      "open_issues": 0,
      "size": 109
    },
    "tech_info": {
      "language": "TypeScript",
      "languages": {
        "TypeScript": 90031,
        "Python": 8751,
        "CSS": 4475,
        "JavaScript": 605
      },
      "license": "Other",
      "topics": []
    },
    "content": {
      "readme": "# AISDK Prompt Optimizer\n\n[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)\n[![Open Source](https://badges.frapsoft.com/os/v1/open-source.svg?v=103)](https://opensource.org/)\n\nTransform your AI interactions with intelligent prompt optimization. Teach your AI, collect ideal samples, and generate optimized prompts using the powerful AISDK Prompt Optimizer.\n\n**Fully Open Source** - Built by the team that created [Langtrace AI](https://langtrace.ai) and [Zest](https://heyzest.ai)\n\n## What is GEPA?\n\n**GEPA** (Genetic-Pareto) is a reflective optimizer that adaptively evolves textual components (such as prompts) of AI systems. Unlike traditional optimization methods that only use scalar scores, GEPA leverages rich textual feedback to guide the optimization process, allowing it to propose high-performing prompts in very few rollouts.\n\nKey features of GEPA:\n- **Reflective Prompt Mutation**: Uses LLMs to reflect on execution traces and propose new instructions\n- **Rich Textual Feedback**: Leverages any textual feedback beyond just scalar rewards\n- **Pareto-based Selection**: Maintains a frontier of candidates that excel in different scenarios\n\nLearn more: [GEPA Documentation](https://dspy.ai/api/optimizers/GEPA/)\n\n## How It Works\n\n1. **Start Conversation**: Begin chatting with the AI and teach it desired behaviors through examples\n2. **Mark Examples**: Save ideal conversation samples that represent perfect responses\n3. **Run Optimization**: Let AISDK Prompt Optimizer analyze patterns and generate optimized prompts\n4. **Deploy Results**: Use the optimized prompts in your applications\n\n## Features\n\n- **Teach Your AI**: Guide your AI through interactive conversations and demonstrate the ideal responses you want to achieve\n- **Collect Ideal Samples**: Gather high-quality conversation examples that represent perfect AI behavior for your use case\n- **AISDK Prompt Optimizer**: Leverage advanced optimization algorithms to automatically generate and refine prompt candidates\n\n## Quick Start\n\n### Prerequisites\n- Node.js (18+ recommended)\n- `uv` package manager for Python\n- OpenAI API key\n- AI Gateway API key\n\n### Environment Setup\n\nBefore running the application, you need to set up your environment variables:\n\n1. Copy the example environment file:\n   ```bash\n   cp .env.example .env\n   ```\n\n2. Edit the `.env` file and add your API keys:\n   ```bash\n   # Required: OpenAI API key for AI model access\n   OPENAI_API_KEY=your_actual_openai_api_key_here\n   \n   # Required: AI Gateway API key for prompt optimization\n   AI_GATEWAY_API_KEY=your_actual_ai_gateway_api_key_here\n   ```\n\n**Important**: Never commit your actual API keys to version control. The `.env` file is already included in `.gitignore`.\n\n### Installation & Setup\n\n```bash\n# Clone the repository\ngit clone https://github.com/Scale3-Labs/aisdk-prompt-optimizer\ncd aisdk-prompt-optimizer\n\n# Install dependencies\nnpm install\n\n# Start both services (recommended)\nnpm run dev:all\n```\n\n### Alternative: Start Services Separately\n\n```bash\n# Terminal 1: Start the Python optimizer\ncd python_optimizer\nuv run app.py\n\n# Terminal 2: Start the web app\nnpm run dev\n```\n\nThe web app will be available at `http://localhost:3000` and the Python optimizer at `http://localhost:8000`. Both services need to be running for the optimization features to work.\n\n## Available Scripts\n\n- `npm run dev` - Start the Next.js development server\n- `npm run dev:py` - Start the Python optimizer server\n- `npm run dev:all` - Start both services concurrently\n\n## Architecture\n\n### Python Optimizer (dspy.GEPA)\n\nThe repository includes a lightweight Flask server exposing the `dspy.GEPA` optimizer, managed with `uv`. The Next.js `/api/optimize` route calls this server and writes optimization artifacts to:\n- `data/prompt.md` - Generated optimized prompts\n- `data/complete-optimization.json` - Complete optimization results and metadata\n\n### Web Application\n\nBuilt with Next.js and shadcn/ui components, the web interface provides:\n- Interactive chat interface for teaching the AI\n- Sample collection and management\n- Optimization trigger and results visualization\n- Modern, responsive UI with dark/light mode support\n\n## Technology Stack\n\n- **Frontend**: Next.js, TypeScript, Tailwind CSS, shadcn/ui\n- **Backend**: Python Flask server with dspy.GEPA optimizer\n- **Package Management**: npm (frontend), uv (Python)\n\n## Learn More\n\n- [DSPy Documentation](https://dspy.ai/)\n- [GEPA Optimizer API](https://dspy.ai/api/optimizers/GEPA/)\n- [Next.js Documentation](https://nextjs.org/docs)\n\n## Deployment\n\nThe easiest way to deploy your Next.js app is to use the [Vercel Platform](https://vercel.com/new?utm_medium=default-template&filter=next.js&utm_source=create-next-app&utm_campaign=create-next-app-readme) from the creators of Next.js.\n\nFor the Python optimizer, you'll need to deploy it to a Python-compatible hosting service and update the API endpoints accordingly.\n\nCheck out the [N",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-17T18:17:11.327823"
  },
  {
    "basic_info": {
      "name": "free-sqlite",
      "full_name": "fjb040911/free-sqlite",
      "owner": "fjb040911",
      "description": "Free SQLite for VSCode.Support writing SQL statements",
      "url": "https://github.com/fjb040911/free-sqlite",
      "clone_url": "https://github.com/fjb040911/free-sqlite.git",
      "ssh_url": "git@github.com:fjb040911/free-sqlite.git",
      "homepage": null,
      "created_at": "2025-09-16T06:20:19Z",
      "updated_at": "2025-09-17T18:12:42Z",
      "pushed_at": "2025-09-17T13:43:45Z"
    },
    "stats": {
      "stars": 83,
      "forks": 0,
      "watchers": 83,
      "open_issues": 0,
      "size": 10040
    },
    "tech_info": {
      "language": "TypeScript",
      "languages": {
        "TypeScript": 34158,
        "JavaScript": 16831
      },
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "# Free sqlite\n\nVSCode extension to explore and query SQLite databases.\nopen-source and free.\n\n## ‚ú® Features\n - üìÉ Open any SQLite file - Simply click on .sqlite or .db files to open them instantly\n - üõ° Table Explorer - Browse all tables in your database from an integrated sidebar\n - üåà Data Visualization - View table data in a clean, modern interface that adapts to your VS Code theme\n - üñ• SQL statement editor - SQL statement editor, Automatically complete SQL keywords, table names, and fields\n - üì¶ Query result export - The query results can be exported as Excel or CSV\n - ‚ù§Ô∏è Favorites - Collect some of your most commonly used SQL statements\n\n## How to use\n\n### Install\nVSCode extension install!\n\n\n[Install free sqlite](https://marketplace.visualstudio.com/items?itemName=free-sqlite.free-sqlite)\n\n### Open database\nNow! Browse all tables in your database in the right panel\n\n\n![open](https://github.com/fjb040911/free-sqlite/blob/main/doc/open.gif?raw=true)\n\n### Multiple files\n![multiple](https://github.com/fjb040911/free-sqlite/blob/main/doc/multi.gif?raw=true)\n\n### SQL Editor\nAutomatically complete SQL keywords, table names, and fields\n\n\n![editor](https://github.com/fjb040911/free-sqlite/blob/main/doc/select.gif?raw=true)\n\n### SQL Query error output\nIf there is an error when running SQL, you can see the error log through the following channels\n\n\nTerminal>output>free sqlite\n\n\n![output](https://github.com/fjb040911/free-sqlite/blob/main/doc/oupput.png?raw=true)\n\n### Favorite\nQuickly execute or view your frequently used SQL\n\n\n![favorite](https://github.com/fjb040911/free-sqlite/blob/main/doc/favoites.gif?raw=true)\n\n### Export\n![export](https://github.com/fjb040911/free-sqlite/blob/main/doc/expot.gif?raw=true)\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-17T18:17:12.574470"
  },
  {
    "basic_info": {
      "name": "pingoo",
      "full_name": "pingooio/pingoo",
      "owner": "pingooio",
      "description": "The fast and secure Load Balancer / API Gateway / Reverse Proxy with built-in service discovery, GeoIP, WAF, bot protection and much more - https://pingoo.io",
      "url": "https://github.com/pingooio/pingoo",
      "clone_url": "https://github.com/pingooio/pingoo.git",
      "ssh_url": "git@github.com:pingooio/pingoo.git",
      "homepage": "https://pingoo.io",
      "created_at": "2025-09-17T07:18:40Z",
      "updated_at": "2025-09-17T18:05:16Z",
      "pushed_at": "2025-09-17T17:25:29Z"
    },
    "stats": {
      "stars": 67,
      "forks": 3,
      "watchers": 67,
      "open_issues": 2,
      "size": 260
    },
    "tech_info": {
      "language": "Rust",
      "languages": {
        "Rust": 179208,
        "TypeScript": 7653,
        "Dockerfile": 6484,
        "Makefile": 2253,
        "Shell": 1620,
        "HTML": 892,
        "CSS": 620,
        "Vim Script": 19
      },
      "license": "MIT License",
      "topics": [
        "akamai",
        "anti-bot",
        "apache2",
        "api",
        "api-gateway",
        "captcha",
        "cloudflare",
        "fastly",
        "firewall",
        "haproxy",
        "load-balancer",
        "nginx",
        "pingoo",
        "proxy",
        "quic",
        "reverse-proxy",
        "rust",
        "security",
        "service-discovery",
        "waf"
      ]
    },
    "content": {
      "readme": "<p align=\"center\">\n  <a href=\"https://pingoo.io\" target=\"_blank\" rel=\"noopener\"><img alt=\"Pingoo logo\" src=\"https://pingoo.io/icon-256.png\" height=\"128\" /></a>\n  <h1 align=\"center\">Pingoo</h1>\n  <h3 align=\"center\">The fast and secure Load Balancer / API Gateway / Reverse Proxy with built-in service discovery, GeoIP, WAF, bot protection and much more</h3>\n  <h3 align=\"center\">\n    <a href=\"https://pingoo.io\">Documentation</a> | <a href=\"https://kerkour.com/announcing-pingoo\">Read the launch post</a>\n  </h3>\n</p>\n\nOpen Source load balancers and reverse proxies are stuck in the past century with a very slow pace of development and most of the important features reserved for \"Enterprise Editions\" which lead developers to use third-party cloud services, exposing their users' traffic to legal, security and reliability risks.\n\nPingoo is a modern Load Balancer / API Gateway / Reverse Proxy that run on your own servers and already have (or will have soon) all the features you expect from managed services and even more. All of that with a huge boost in performance and security thanks to reduced latency and, of course, Rust ;)\n\n* Service Discovery (Docker, DNS...)\n* Web Application Firewall (WAF)\n* Easy compliance because the data never leaves your servers\n* Bot protection and management\n* TCP proxying\n* Post-Quantum TLS\n* GeoIP (country, ASN)\n* Static sites\n* And much more\n\n> ‚ö†Ô∏è Pingoo is currently in beta, use with caution.\n\n## Quickstart\n\n```bash\n# You have a static site in the www folder\n$ ls www\nindex.html\n$ docker run --rm -ti --network host -v `pwd`/www:/wwww ghcr.io/pingooio/pingoo\n# Pingoo is now listenning on http://0.0.0.0:8080\n```\n\n## Documentation\n\nSee https://pingoo.io\n\n\n## Updates\n\nSubscribe to the blog by RSS or email to get weekly / monthly updates: [https://kerkour.com/subscribe](https://kerkour.com/subscribe). No spam ever, only technical deep dives.\n\n\n## Contributing\n\nPlease open an issue to discuss your idea before submitting a Pull Request.\n\n\n## Support\n\nDo you have custom needs? Do you want your features to be prioritized? Are you under attack and need help? Do you need support for deploying and self-hosting Pingoo?\n\nFeel free to reach our team of experts to see how we can help: https://pingoo.io/contact\n\n\n## Security\n\nWe are committed to make Pingoo the most secure Load Balancer / Reverse Proxy in the universe and beyond. If you've found a security issue in Pingoo, we appreciate your help in disclosing it to us in a responsible manner by contacting us: https://pingoo.io/contact\n\n\n## License\n\nMIT. See `LICENSE.txt`\n\nForever Open Source. No Open Core or \"Enterprise Edition\".\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-17T18:17:13.791611"
  },
  {
    "basic_info": {
      "name": "uuidv47",
      "full_name": "stateless-me/uuidv47",
      "owner": "stateless-me",
      "description": "‚ö° UUIDv47 = v4 privacy + v7 performance",
      "url": "https://github.com/stateless-me/uuidv47",
      "clone_url": "https://github.com/stateless-me/uuidv47.git",
      "ssh_url": "git@github.com:stateless-me/uuidv47.git",
      "homepage": null,
      "created_at": "2025-09-16T14:22:55Z",
      "updated_at": "2025-09-17T18:15:59Z",
      "pushed_at": "2025-09-17T13:45:15Z"
    },
    "stats": {
      "stars": 67,
      "forks": 1,
      "watchers": 67,
      "open_issues": 1,
      "size": 16
    },
    "tech_info": {
      "language": "C",
      "languages": {
        "C": 12102,
        "Makefile": 2337
      },
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "UUIDv47 - UUIDv7-in / UUIDv4-out (SipHash-masked timestamp)\n==================================================================\n\nuuidv47 lets you store sortable UUIDv7 in your database while emitting a\nUUIDv4-looking fa√ßade at your API boundary. It does this by XOR-masking\nonly the UUIDv7 timestamp field with a keyed SipHash-2-4 stream tied to\nthe UUID‚Äôs own random bits.\n\n- Header-only C (C89) ¬∑ zero deps\n- Deterministic, invertible mapping (exact round-trip)\n- RFC-compatible version/variant bits (v7 in DB, v4 on the wire)\n- Key-recovery resistant (SipHash-2-4, 128-bit key)\n- Full tests provided\n\n------------------------------------------------------------------\n\nTable of contents\n-----------------\n- Why\n- Quick start\n- Public API\n- Specification\n  - UUIDv7 bit layout\n  - Fa√ßade mapping (v7 ‚Üî v4)\n  - SipHash message derived from random\n  - Invertibility\n  - Collision analysis\n- Security model\n- Build, test, coverage\n- Integration tips\n- Performance notes\n- FAQ\n- License\n\n------------------------------------------------------------------\n\nWhy\n---\n- DB-friendly: UUIDv7 is time-ordered ‚Üí better index locality & pagination.\n- Externally neutral: The fa√ßade hides timing patterns and looks like v4 to clients/systems.\n- Secret safety: Uses a PRF (SipHash-2-4). Non-crypto hashes are not suitable when the key must not leak.\n\n------------------------------------------------------------------\n\nQuick start\n-----------\n```c\n#include <stdio.h>\n#include \"uuidv47.h\"\n\nint main(void){\n  const char* s = \"00000000-0000-7000-8000-000000000000\";\n  uuid128_t v7;\n  if (!uuid_parse(s, &v7)) return 1;\n  uuidv47_key_t key = { .k0 = 0x0123456789abcdefULL, .k1 = 0xfedcba9876543210ULL };\n  uuid128_t facade = uuidv47_encode_v4facade(v7, key);\n  uuid128_t back = uuidv47_decode_v4facade(facade, key);\n\n  char a[37], b[37], c[37];\n  uuid_format(&v7, a);\n  uuid_format(&facade, b);\n  uuid_format(&back, c);\n  printf(\"v7 (DB) : %s\\n\", a);\n  printf(\"v4 (API): %s\\n\", b);\n  printf(\"back    : %s\\n\", c);\n}\n```\n\nBuild & run with the provided Makefile:\n  make test\n  make coverage\n  sudo make install\n\n------------------------------------------------------------------\n\nPublic API\n----------\n\n```c\ntypedef struct { uint8_t  b[16]; } uuid128_t;\ntypedef struct { uint64_t k0, k1; } uuidv47_key_t;\n\nuuid128_t uuidv47_encode_v4facade(uuid128_t v7, uuidv47_key_t key);\nuuid128_t uuidv47_decode_v4facade(uuid128_t v4_facade, uuidv47_key_t key);\nint  uuid_version(const uuid128_t* u);\nvoid set_version(uuid128_t* u, int ver);\nvoid set_variant_rfc4122(uuid128_t* u);\nbool uuid_parse (const char* str, uuid128_t* out);\nvoid uuid_format(const uuid128_t* u, char out[37]);\n```\n\n------------------------------------------------------------------\n\nSpecification\n-------------\nUUIDv7 bit layout:\n- ts_ms_be: 48-bit big-endian timestamp\n- ver:      high nibble of byte 6 = 0x7 (v7) or 0x4 (fa√ßade)\n- rand_a:   12 random bits\n- var:      RFC variant (0b10)\n- rand_b:   62 random bits\n\nFa√ßade mapping:\n- Encode: ts48 ^ mask48(R), set version=4\n- Decode: encTS ^ mask48(R), set version=7\n- Random bits unchanged\n\nSipHash input: 10 bytes from random field:\n  msg[0] = (byte6 & 0x0F)\n  msg[1] = byte7\n  msg[2] = (byte8 & 0x3F)\n  msg[3..9] = bytes9..15\n\nInvertibility: XOR mask is reversible with known key.\n\nCollision analysis: Injective mapping. Only risk is duplicate randoms per ms.\n\n------------------------------------------------------------------\n\nSecurity model\n--------------\n- Goal: Secret key unrecoverable even with chosen inputs.\n- Achieved: SipHash-2-4 is a keyed PRF.\n- Keys: 128-bit. Derive via HKDF.\n- Rotation: store small key ID outside UUID.\n\n------------------------------------------------------------------\n\nBuild, test, coverage\n---------------------\n```\nmake test\nmake coverage\nmake debug\nsudo make install\n```\n\n------------------------------------------------------------------\n\nIntegration tips\n----------------\n- Do encode/decode at API boundary.\n- For Postgres, write tiny C extension.\n- For sharding, hash v4 fa√ßade with xxh3 or SipHash.\n\n------------------------------------------------------------------\n\nPerformance\n-----------\nSipHash-2-4 on 10-byte message is extremely fast. No allocations.\n\n------------------------------------------------------------------\n\nFAQ\n---\nQ: Why not xxHash with a secret?\nA: Not a PRF; secret can leak. Use SipHash.\n\nQ: Is fa√ßade indistinguishable from v4?\nA: Yes, variable bits uniform, version/variant set to v4.\n\n------------------------------------------------------------------\n\nLicense\n-------\nMIT, Copyright (c) 2025 Stateless Limited\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-17T18:17:15.021116"
  },
  {
    "basic_info": {
      "name": "TradingView-PCapp",
      "full_name": "darklight797t6/TradingView-PCapp",
      "owner": "darklight797t6",
      "description": null,
      "url": "https://github.com/darklight797t6/TradingView-PCapp",
      "clone_url": "https://github.com/darklight797t6/TradingView-PCapp.git",
      "ssh_url": "git@github.com:darklight797t6/TradingView-PCapp.git",
      "homepage": null,
      "created_at": "2025-09-17T12:35:15Z",
      "updated_at": "2025-09-17T16:07:44Z",
      "pushed_at": "2025-09-17T12:48:01Z"
    },
    "stats": {
      "stars": 56,
      "forks": 0,
      "watchers": 56,
      "open_issues": 0,
      "size": 1463
    },
    "tech_info": {
      "language": null,
      "languages": {},
      "license": null,
      "topics": []
    },
    "content": {
      "readme": "\n# TradingView App on pc\n\nAccess realtime market prices and indicator values from TradingView!\n\n‚ñåFeatures\n\n- [x] Access to TradingView's enhanced data feeds\n- [x] Efficiently backtest strategies and explore different settings\n- [x] Retrieve drawings from your charts\n- [x] Compatible with invite-only indicators\n- [x] Supports a large number of simultaneous indicators (subject to rate limits)\n- [x] Realtime data streaming\n- [x] Access TradingView's technical analysis data\n- [x] Replay mode + Simulated Replay mode (for free plans)\n- [x] Retrieve historical data for specific date ranges\n- [ ] TradingView socket server compatibility layer (alpha)\n- [ ] Interact with public chats\n- [ ] Access Screener top values\n- [ ] Get Hotlists\n- [ ] Get Calendar\n- IF YOU WANT A FEATURE, ASK ME!\n\n‚ñåPotential Uses\n\n- Algorithmic trading\n- Custom Discord alerts\n- Advanced backtesting\n- Machine learning research\n- Free replay mode on certain timeframes, subject to limitations\n\n___\n\n‚ñåInstallation\n\nStable version:\n\nnpm i @mathieuc/tradingview\n\nLast version:\n\nnpm i github:Mathieu2301/TradingView-API\n\n‚ñåExamples\n\nYou can find all the examples and snippets in ./examples folder.\n\n‚ñåBefore opening an issue\n\nPlease look at examples and previously resolved issues before opening a new one. I can't help everyone (especially for questions that are not library related but JavaScript related). Thank you for your understanding.\n___\n\n```\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-17T18:17:16.245861"
  },
  {
    "basic_info": {
      "name": "grok2api",
      "full_name": "VeroFess/grok2api",
      "owner": "VeroFess",
      "description": "rewrite grok2api",
      "url": "https://github.com/VeroFess/grok2api",
      "clone_url": "https://github.com/VeroFess/grok2api.git",
      "ssh_url": "git@github.com:VeroFess/grok2api.git",
      "homepage": null,
      "created_at": "2025-09-16T12:48:33Z",
      "updated_at": "2025-09-17T17:39:03Z",
      "pushed_at": "2025-09-17T07:19:37Z"
    },
    "stats": {
      "stars": 47,
      "forks": 20,
      "watchers": 47,
      "open_issues": 0,
      "size": 53
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 160146,
        "HTML": 33516,
        "Dockerfile": 306
      },
      "license": null,
      "topics": []
    },
    "content": {
      "readme": "# Grok API Gateway\n\n## ‰∏éÂéüÁâàÂ∑ÆÂºÇ\n\nÊú¨ fork ÁâàÊú¨Áõ∏ËæÉ‰∫éÂéüÁâàÂ¢ûÂä†‰∫Ü‰ª•‰∏ãÂäüËÉΩÔºö\n\n0. **Âü∫Êú¨ÂÖ®ÈÉ®ÈáçÂÜô‰∫Ü...**\n1. **Ëá™Âä®Ëé∑Âèñ x-statsig-id** - ‰ΩøÁî® Playwright Ëá™Âä®Ëé∑ÂèñÂπ∂ÁÆ°ÁêÜËÆ§ËØÅÂ§¥\n2. **ÊµÅÊ®°ÂºèÊ†áÁ≠æËøáÊª§** - Ëá™Âä®ÁßªÈô§ÂìçÂ∫î‰∏≠ÁöÑ `<xaiArtifact` Á≠âÊ†áÁ≠æ\n3. **Â¢ûÂº∫ÁªüËÆ°ÂäüËÉΩ** - ÊîπËøõÁöÑ‰ª§Áâå‰ΩøÁî®ÁªüËÆ°ÂíåÁõëÊéß\n4. **Grok4ÊîØÊåÅ** - ÂèçÊ≠£ÊàëËÉΩÁî®.jpg\n\n## ÁéØÂ¢ÉÂèòÈáèÈÖçÁΩÆ\n\n### ÂøÖÈúÄÈÖçÁΩÆ\n\n| ÁéØÂ¢ÉÂèòÈáè | ÊèèËø∞ | ÈªòËÆ§ÂÄº | Á§∫‰æã |\n|---------|------|--------|------|\n| `API_KEY` | API ËÆøÈóÆÂØÜÈí• | `sk-123456` | `sk-your-api-key` |\n| `SSO` | Grok SSO ‰ª§ÁâåÔºàÊôÆÈÄöÔºâ | - | `token1,token2,token3` |\n| `SSO_SUPER` | Grok SSO ‰ª§ÁâåÔºàË∂ÖÁ∫ßÔºâ | - | `super_token1,super_token2` |\n\n### ÂèØÈÄâÈÖçÁΩÆ\n\n| ÁéØÂ¢ÉÂèòÈáè | ÊèèËø∞ | ÈªòËÆ§ÂÄº | ÊúâÊïàÂÄº | Á§∫‰æã |\n|---------|------|--------|--------|------|\n| `IS_CUSTOM_SSO` | ÂÖÅËÆ∏Âä®ÊÄÅ SSO ‰ª§Áâå | `false` | `true/false` | `true` |\n| `IS_TEMP_CONVERSATION` | ‰∏¥Êó∂ÂØπËØùÊ®°Âºè | `true` | `true/false` | `false` |\n| `SHOW_THINKING` | ÊòæÁ§∫Êé®ÁêÜËøáÁ®ã | `false` | `true/false` | `true` |\n| `SHOW_SEARCH_RESULTS` | ÊòæÁ§∫ÊêúÁ¥¢ÁªìÊûú | `true` | `true/false` | `false` |\n| `IS_SUPER_GROK` | ÂêØÁî®Ë∂ÖÁ∫ß Grok ÂäüËÉΩ | `false` | `true/false` | `true` |\n| `MANAGER_SWITCH` | ÂêØÁî® Web ÁÆ°ÁêÜÁïåÈù¢ | - | `true/false` | `true` |\n| `ADMINPASSWORD` | ÁÆ°ÁêÜÁïåÈù¢ÂØÜÁ†Å | - | ‰ªªÊÑèÂ≠óÁ¨¶‰∏≤ | `admin123` |\n| `PORT` | ÊúçÂä°Á´ØÂè£ | `5200` | Êï∞Â≠ó | `8080` |\n| `PROXY` | ‰ª£ÁêÜÊúçÂä°Âô® | - | HTTP/SOCKS5 URL | `http://127.0.0.1:1080` |\n| `CF_CLEARANCE` | Cloudflare ‰ª§Áâå | - | CF ‰ª§ÁâåÂ≠óÁ¨¶‰∏≤ | `cf_clearance_token` |\n| `DISABLE_DYNAMIC_HEADERS` | Á¶ÅÁî®Âä®ÊÄÅÂ§¥ÈÉ®Ëé∑ÂèñÔºàÁ¶ÅÁî® Playwright Ëá™Âä®Ëé∑Âèñ x-statsig-idÔºâ | `false` | `true/false` | `true` |\n| `FILTERED_TAGS` | ËøáÊª§Ê†áÁ≠æÂàóË°® | `xaiartifact,xai:tool_usage_card,grok:render,details,summary` | ÈÄóÂè∑ÂàÜÈöî | `tag1,tag2,tag3` |\n| `TAG_CONFIG` | ËøáÊª§Ê†áÁ≠æÈÖçÁΩÆ | `{\"xaiartifact\":{\"behavior\":\"preserve_content\"},\"xai:tool_usage_card\":{\"behavior\":\"remove_all\"},\"grok:render\":{\"behavior\":\"remove_all\"},\"details\":{\"behavior\":\"preserve_content\"},\"summary\":{\"behavior\":\"preserve_content\"}}` | json | `{\"xaiartifact\":{\"behavior\":\"preserve_content\"},\"xai:tool_usage_card\":{\"behavior\":\"remove_all\"},\"grok:render\":{\"behavior\":\"remove_all\"},\"details\":{\"behavior\":\"preserve_content\"},\"summary\":{\"behavior\":\"preserve_content\"}}` |\n| `CONTENT_TYPE_MAPPINGS` | ËøáÊª§Ê†áÁ≠æÈáçÂÜôÈÖçÁΩÆ | Â§™Èïø‰∫Ü,ÁúãÊ∫êÁ†Å | json | {\"text/plain\":{\"stag\":\"```\",\"etag\":\"```\"},\"text/python\":{\"stag\":\"```python\\n\",\"etag\":\"\\n```\"}} |\n\n### Ê†áÁ≠æËøáÊª§ÈÖçÁΩÆ\n\nÊ∑ªÂä†‰∫ÜÈ´òÁ∫ßÊ†áÁ≠æËøáÊª§ÂäüËÉΩÔºåÂèØÂú®ÊµÅÂºèÂìçÂ∫î‰∏≠Ëá™Âä®Â§ÑÁêÜÁâπÂÆöÁöÑ XML/HTML Ê†áÁ≠æ„ÄÇ\n\nÊ≥®ÊÑèÈÖçÁΩÆÈîôËØØ‰ºöÁõ¥Êé•Á†¥ÂùèËæìÂá∫!!!\n\n#### FILTERED_TAGS\n\n**ÊèèËø∞**ÔºöÊ†áÁ≠æËøáÊª§ÂàóË°®, ÂΩìÈÅáÂà∞‰∏çÂú®ÂàóË°®‰∏≠ÁöÑÊ†áÁ≠æÊó∂‰ºöÁ´ãÂç≥ÊîæÂºÉÂêéÁª≠ÈáçÂÜô\n\n**Ê†ºÂºè**ÔºöÈÄóÂè∑ÂàÜÈöîÁöÑÊ†áÁ≠æÂêçÁß∞ÔºåÂ∞èÂÜô\n\n**ÈªòËÆ§ÂÄº**Ôºö`xaiartifact,xai:tool_usage_card,grok:render,details,summary`\n\n**Á§∫‰æã**Ôºö\n```bash\nFILTERED_TAGS=xaiartifact,grok:render,grok:thinking\n```\n\n#### TAG_CONFIG\n\n**ÊèèËø∞**ÔºöÈ´òÁ∫ßÊ†áÁ≠æË°å‰∏∫ÈÖçÁΩÆÔºåÊîØÊåÅ‰∏∫‰∏çÂêåÊ†áÁ≠æËÆæÁΩÆ‰∏çÂêåÁöÑÂ§ÑÁêÜÁ≠ñÁï•„ÄÇ\n\n**Ê†ºÂºè**ÔºöJSON ÂØπË±°ÔºåÈîÆ‰∏∫Ê†áÁ≠æÂêçÁß∞ÔºàÂ∞èÂÜôÔºâÔºåÂÄº‰∏∫ÈÖçÁΩÆÂØπË±°\n\n**ÈÖçÁΩÆÈÄâÈ°π**Ôºö\n- `behavior`: Ê†áÁ≠æË°å‰∏∫\n  - `\"preserve_content\"`: ‰øùÁïôÂÜÖÂÆπÔºåÊ∑ªÂä†Ê†ºÂºèÂåñÊ†áËÆ∞\n  - `\"remove_all\"`: ÂÆåÂÖ®ÁßªÈô§Ê†áÁ≠æÂíåÂÜÖÂÆπ\n\n**ÈªòËÆ§ÂÄº**ÔºöÂü∫‰∫é FILTERED_TAGS Ëá™Âä®ÁîüÊàê\n\n**Á§∫‰æã**Ôºö\n```json\n{\n  \"xaiartifact\": {\"behavior\": \"preserve_content\"},\n  \"xai:tool_usage_card\": {\"behavior\": \"remove_all\"},\n  \"grok:render\": {\"behavior\": \"remove_all\"},\n  \"details\": {\"behavior\": \"preserve_content\"},\n  \"summary\": {\"behavior\": \"preserve_content\"}\n}\n```\n\n**Âú® docker-compose.yml ‰∏≠ÈÖçÁΩÆ**Ôºö\n```yaml\nenvironment:\n  TAG_CONFIG: '{\"xaiartifact\":{\"behavior\":\"preserve_content\"},\"xai:tool_usage_card\":{\"behavior\":\"remove_all\"},\"grok:render\":{\"behavior\":\"remove_all\"},\"details\":{\"behavior\":\"preserve_content\"},\"summary\":{\"behavior\":\"preserve_content\"}}'\n```\n\n#### CONTENT_TYPE_MAPPINGS\n\n**ÊèèËø∞**ÔºöÂÜÖÂÆπÁ±ªÂûãÊò†Â∞ÑÈÖçÁΩÆÔºåÂÆö‰πâ‰∏çÂêå contentType ÁöÑÊ†ºÂºèÂåñÊ†áËÆ∞„ÄÇ\n\n**Ê†ºÂºè**ÔºöJSON ÂØπË±°ÔºåÈîÆ‰∏∫ MIME Á±ªÂûãÔºåÂÄº‰∏∫ÂåÖÂê´ stagÔºàÂºÄÂßãÊ†áËÆ∞ÔºâÂíå etagÔºàÁªìÊùüÊ†áËÆ∞ÔºâÁöÑÂØπË±°\n\n**ÈªòËÆ§Êò†Â∞Ñ**Ôºö\n```json\n{\n  \"text/plain\": {\"stag\": \"```\", \"etag\": \"```\"},\n  \"text/markdown\": {\"stag\": \"\", \"etag\": \"\"},\n  \"application/json\": {\"stag\": \"```json\\n\", \"etag\": \"\\n```\"}\n}\n```\n\n**Á§∫‰æãÈÖçÁΩÆ**Ôºö\n```yaml\nenvironment:\n  CONTENT_TYPE_MAPPINGS: '{\"text/plain\":{\"stag\":\"```\",\"etag\":\"```\"},\"text/python\":{\"stag\":\"```python\\n\",\"etag\":\"\\n```\"}}'\n```\n\n**Â∑•‰ΩúÂéüÁêÜ**Ôºö\n1. ÂΩìÈÅáÂà∞ `preserve_content` Ë°å‰∏∫ÁöÑÊ†áÁ≠æÊó∂Ôºå‰ºöÊü•ÊâæÊ†áÁ≠æÁöÑ `contentType` Â±ûÊÄß\n2. Ê†πÊçÆ `contentType` Âú®Êò†Â∞ÑË°®‰∏≠Êü•ÊâæÂØπÂ∫îÁöÑÊ†ºÂºèÂåñÊ†áËÆ∞\n3. Áî® `stag` + ÂÜÖÂÆπ + `etag` ÊõøÊç¢ÂéüÂßãÊ†áÁ≠æÂíåÂØπÂ∫îÁöÑÂ∞ÅÈó≠Ê†áÁ≠æ\n\n\n## Âø´ÈÄüÂºÄÂßã\n\n### ‰ΩøÁî® Docker Hub ÈïúÂÉè\n\nÁé∞Âú®ÂèØ‰ª•Áõ¥Êé•‰ªé Docker Hub ÊãâÂèñÈ¢ÑÊûÑÂª∫ÁöÑÈïúÂÉèÔºö\n\n```bash\n# ÊãâÂèñÈïúÂÉè\ndocker pull verofess/grok2api\n\n# ËøêË°åÂÆπÂô®\ndocker run -d \\\n  --name grok2api \\\n  -p 5200:5200 \\\n  -e API_KEY=sk-your-api-key \\\n  -e SSO=your-sso-token \\\n  verofess/grok2api\n\n# ÊàñËÄÖ‰ΩøÁî® docker-compose\ndocker-compose up -d\n```\n\n### Docker Compose Á§∫‰æã\n\n```yaml\nservices:\n  grok2api:\n    image: verofess/grok2api\n    container_name: grok2api\n    ports:\n      - \"5200:5200\"\n    environment:\n      - API_KEY=sk-your-api-key\n      - SSO=your-sso-token\n      - IS_TEMP_CONVERSATION=true\n      - SHOW_THINKING=false\n    restart: unless-stopped\n```",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-17T18:17:17.478831"
  },
  {
    "basic_info": {
      "name": "InternVLA-M1",
      "full_name": "InternRobotics/InternVLA-M1",
      "owner": "InternRobotics",
      "description": null,
      "url": "https://github.com/InternRobotics/InternVLA-M1",
      "clone_url": "https://github.com/InternRobotics/InternVLA-M1.git",
      "ssh_url": "git@github.com:InternRobotics/InternVLA-M1.git",
      "homepage": null,
      "created_at": "2025-09-16T12:47:28Z",
      "updated_at": "2025-09-17T18:11:53Z",
      "pushed_at": "2025-09-17T18:11:49Z"
    },
    "stats": {
      "stars": 45,
      "forks": 0,
      "watchers": 45,
      "open_issues": 0,
      "size": 14254
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 587378,
        "Shell": 10751,
        "Makefile": 661
      },
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "# InternVLA-M1\n\n**InternVLA-M1** is an open-source, end-to-end **vision‚Äìlanguage‚Äìaction (VLA) framework** for building and researching generalist robot policies.\n\nhttps://github.com/user-attachments/assets/e83ae046-a503-46a8-95e4-ef381919b7f8\n\n[![Paper](https://img.shields.io/badge/Paper-arXiv-red.svg)](https://github.com/InternRobotics/InternVLA-M1/blob/InternVLA-M1/assets/InternVLA_M1.pdf) [![Website](https://img.shields.io/badge/Website-GitHub%20Pages-blue.svg)](https://internrobotics.github.io/internvla-m1.github.io) [![Demo](https://img.shields.io/badge/Demo-YouTube-red.svg)](https://youtu.be/n129VDqJCk4) [![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)\n\n![](assets/teaser.png)\n\n## üî• Key Features\n\n1. **Modular & Extensible**  \n   All core components (model architecture, training data, training strategies, evaluation pipeline) are fully decoupled, enabling independent development, debugging, and extension of each module.\n\n\n2. **Dual-System and Dual-Supervision**\n   InternVLA-M1 integrates both a language head and an action head under a unified framework, enabling collaborative training with dual supervision. \n\n3. **Efficient Training & Fast Convergence**\n   Learns spatial and visual priors from large-scale multimodal pretraining and transfers them via spatial prompt fine-tuning. Achieves strong performance (e.g., SOTA-level convergence on  in \\~2.5 epochs without separate action pretraining). \n\n## üéØ Target Audience\n\n1. Users who want to leverage open-source VLMs (e.g., Qwen2.5-VL) for robot control.\n2. Teams co-training action datasets jointly with multimodal (vision‚Äìlanguage) data.\n3. Researchers exploring alternative VLA architectures and training strategies.\n\n## üìä Experimental Results\n|             | WindowX | Google Robot(VA) | Google Robot(VM) | LIBERO |\n|-------------|---------|------------------|------------------|--------|\n| pi0         | 27.1    | 54.8             | 58.8             | 94.2   |\n| gr00t       | 61.9    | 44.5             | 35.2             | 93.9   |\n| InternVLA-M1 |**71.7** |**76.0**          |**80.7**          |**95.9**|\n|             |         |                  |                  |        |\n\n\n\n\n\n\n# üöÄ Quick Start\n\n## üõ† Environment Setup\n\n```bash\n# Clone the repo\ngit clone https://github.com/InternRobotics/InternVLA-M1\n\n# Create conda environment\nconda create -n internvla-m1 python=3.10 -y\nconda activate internvla-m1\n\n# Install requirements\npip install -r requirements.txt\n\n# Install FlashAttention2\npip install flash-attn --no-build-isolation\n\n# Install InternVLA-M1\npip install -e .\n```\n\n## üìò Examples\n\nWe provide several end-to-end examples for reference:\n\n* **Reproduce InternVLA-M1 in simplerEnv**\n  [Example](/examples/simplerEnv/setup.md)\n\n* **Training/Deployment on real robots**\n  [Example](/examples/real_robot/setup.md)\n\n* **Extending InternVLA-M1**\n  [Example](examples/extending_m1/README.md)\n\n## üìà Model Zoo\nWe will release a series of pretrained models and checkpoints to facilitate reproduction and downstream use.\n\n- Full list and download links: assets/MODEL_ZOO.md\n\n‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è **Note on Hugging Face availability**  \nDue to Hugging Face Hub storage restrictions, some model weights are not yet uploaded there.  \nThis is a temporary limitation and not related to the training or upload scripts. We are in contact with Hugging Face to unlock more storage.  \n\nStatus: rolling release. If you need early access or encounter broken links, please open an issue.\n\n# üó∫Ô∏è Roadmap\n\n* [ ] Release model weights (Stay tuned, coming soon)\n* [ ] Add multi-task mixed training examples\n* [ ] Release real-robot demo\n* [ ] Unify evaluation scripts and metrics\n\n# ü§ù Contributing\n\nWe welcome contributions via Pull Requests or Issues.\nPlease include detailed logs and reproduction steps when reporting bugs.\n\n# üìú Citation\n\nIf you find this useful in your research, please consider citing:\n\n```bibtex\n@misc{internvla2024,\n  title  = {InternVLA-M1: Latent Spatial Grounding for Instruction-Following Robotic Manipulation},\n  author = {InternVLA-M1 Contributors},\n  year   = {2025},\n  booktitle={arXiv},\n}\n```\n\n# üì¨ Contact\n\n* Issues: Submit via GitHub Issues with detailed logs and steps\n\n# üôè Acknowledgements\n\nWe thank the open-source community for their inspiring work. This project builds upon and is inspired by the following projects (alphabetical order):\n- [IPEC-COMMUNITY](https://huggingface.co/IPEC-COMMUNITY): Curated OXE / LIBERO style multi-task datasets and formatting examples.\n- [Isaac-GR00T](https://github.com/NVIDIA/Isaac-GR00T): Standardized action data loader (GR00T-LeRobot).\n- [Qwen2.5-VL](https://github.com/QwenLM/Qwen2.5-VL/blob/main/qwen-vl-finetune/README.md): Multimodal input/output format, data loader, and pretrained VLM backbone.\n- [CogACT](https://github.com/microsoft/CogACT/tree/main/action_model): Reference for a DiT-style action head design.\n- [llavavla](https://github.com/JinhuiYE/llavavla): Baseline code structure and engineering design references.\n- [GenManip Simulati",
      "default_branch": "InternVLA-M1"
    },
    "fetched_at": "2025-09-17T18:17:18.710398"
  },
  {
    "basic_info": {
      "name": "autogen-financial-analysis",
      "full_name": "liangdabiao/autogen-financial-analysis",
      "owner": "liangdabiao",
      "description": "‰∏Ä‰∏™Âü∫‰∫éÂæÆËΩØAutoGenÊ°ÜÊû∂ÁöÑ‰ºÅ‰∏öÁ∫ßÈáëËûçÂàÜÊûêÁ≥ªÁªüÔºå‰ΩøÁî®Â§öAgentÊû∂ÊûÑÊèê‰æõÂÖ®Èù¢ÁöÑË¥¢Âä°ÂàÜÊûê„ÄÅÈ£éÈô©ËØÑ‰º∞ÂíåÈáèÂåñÊäïËµÑÂàÜÊûêÂäüËÉΩ„ÄÇ Ê†∏ÂøÉÂäüËÉΩ: Â§öÊ∫êÊï∞ÊçÆÊî∂ÈõÜ : Êï¥ÂêàYahoo Finance„ÄÅAlpha Vantage„ÄÅQuandlÁ≠âÂ§ö‰∏™ÈáëËûçÊï∞ÊçÆÊ∫ê, Êô∫ËÉΩË¥¢Âä°ÂàÜÊûê : Âü∫‰∫éAutoGenÁöÑÂ§öAgentÂçè‰ΩúÂàÜÊûê, È£éÈô©ËØÑ‰º∞ : VaRËÆ°ÁÆó„ÄÅÂéãÂäõÊµãËØï„ÄÅËíôÁâπÂç°Ê¥õÊ®°Êãü, ÈáèÂåñÂàÜÊûê : Âõ†Â≠êÊ®°Âûã„ÄÅÊäïËµÑÁªÑÂêà‰ºòÂåñ„ÄÅÊú∫Âô®Â≠¶‰π†È¢ÑÊµã, ÂÆûÊó∂ÁõëÊéß : Á≥ªÁªüÊÄßËÉΩÁõëÊéßÂíåÂëäË≠¶, Êï∞ÊçÆÂèØËßÜÂåñ : ‰∫§‰∫íÂºèÂõæË°®ÂíåÊä•ÂëäÁîüÊàê,",
      "url": "https://github.com/liangdabiao/autogen-financial-analysis",
      "clone_url": "https://github.com/liangdabiao/autogen-financial-analysis.git",
      "ssh_url": "git@github.com:liangdabiao/autogen-financial-analysis.git",
      "homepage": null,
      "created_at": "2025-09-16T08:42:33Z",
      "updated_at": "2025-09-17T16:51:26Z",
      "pushed_at": "2025-09-17T00:24:38Z"
    },
    "stats": {
      "stars": 43,
      "forks": 12,
      "watchers": 43,
      "open_issues": 0,
      "size": 349
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 873205,
        "HTML": 52025,
        "PLpgSQL": 7113,
        "Dockerfile": 917
      },
      "license": null,
      "topics": []
    },
    "content": {
      "readme": "# AutoGen Financial Analysis System\n\n‰∏Ä‰∏™Âü∫‰∫éÂæÆËΩØAutoGenÊ°ÜÊû∂ÁöÑ‰ºÅ‰∏öÁ∫ßÈáëËûçÂàÜÊûêÁ≥ªÁªüÔºå‰ΩøÁî®Â§öAgentÊû∂ÊûÑÊèê‰æõÂÖ®Èù¢ÁöÑË¥¢Âä°ÂàÜÊûê„ÄÅÈ£éÈô©ËØÑ‰º∞ÂíåÈáèÂåñÊäïËµÑÂàÜÊûêÂäüËÉΩ„ÄÇ\n\n## üöÄ ÂäüËÉΩÁâπÊÄß\n\n### üîç Ê†∏ÂøÉÂäüËÉΩ\n- **Â§öÊ∫êÊï∞ÊçÆÊî∂ÈõÜ**: Êï¥ÂêàYahoo Finance„ÄÅAlpha VantageÁ≠âÂ§ö‰∏™ÈáëËûçÊï∞ÊçÆÊ∫ê\n- **Êô∫ËÉΩË¥¢Âä°ÂàÜÊûê**: Âü∫‰∫éAutoGenÁöÑÂ§öAgentÂçè‰ΩúÂàÜÊûê\n- **È£éÈô©ËØÑ‰º∞**: VaRËÆ°ÁÆó„ÄÅÂéãÂäõÊµãËØï„ÄÅËíôÁâπÂç°Ê¥õÊ®°Êãü\n- **ÈáèÂåñÂàÜÊûê**: Âõ†Â≠êÊ®°Âûã„ÄÅÊäïËµÑÁªÑÂêà‰ºòÂåñ„ÄÅÁ≠ñÁï•ÂõûÊµã„ÄÅÊú∫Âô®Â≠¶‰π†È¢ÑÊµã\n- **ÂÆûÊó∂ÁõëÊéß**: Á≥ªÁªüÊÄßËÉΩÁõëÊéßÂíåÂëäË≠¶\n- **Êï∞ÊçÆÂèØËßÜÂåñ**: ‰∫§‰∫íÂºèÂõæË°®ÂíåÊä•ÂëäÁîüÊàê\n\n### üèóÔ∏è ÊäÄÊúØÊû∂ÊûÑ\n- **ÂæÆÊúçÂä°Êû∂ÊûÑ**: Ê®°ÂùóÂåñËÆæËÆ°ÔºåÊîØÊåÅÊ∞¥Âπ≥Êâ©Â±ï\n- **ÂºÇÊ≠•Â§ÑÁêÜ**: È´òÊÄßËÉΩÂºÇÊ≠•‰ªªÂä°Â§ÑÁêÜ\n- **ÁºìÂ≠òÁ≥ªÁªü**: Â§öÁ∫ßÁºìÂ≠òÁ≠ñÁï•ÔºåÊèêÂçáÂìçÂ∫îÈÄüÂ∫¶\n- **ÂÆâÂÖ®ÊÄß**: ÂÆåÊï¥ÁöÑË∫´‰ªΩËÆ§ËØÅ„ÄÅÊéàÊùÉÂíåÂä†ÂØÜ\n- **ÁõëÊéßÂëäË≠¶**: Prometheus + GrafanaÁõëÊéß‰ΩìÁ≥ª\n- **ÂÆπÂô®Âåñ**: DockerÂíåKubernetesÈÉ®ÁΩ≤ÊîØÊåÅ\n\n## üì¶ ÂÆâË£ÖÊåáÂçó\n\n### Á≥ªÁªüË¶ÅÊ±Ç\n- Python 3.8+\n- Redis 6.0+\n- PostgreSQL 12+\n- Docker (ÂèØÈÄâ)\n\n### Âø´ÈÄüÂÆâË£Ö\n\n1. **ÂÖãÈöÜÈ°πÁõÆ**\n```bash\ngit clone https://github.com/your-username/autogen-financial-analysis.git\ncd autogen-financial-analysis\n```\n\n2. **ÂàõÂª∫ËôöÊãüÁéØÂ¢É**\n```bash\npython -m venv venv\nsource venv/bin/activate  # Linux/Mac\n# Êàñ\nvenv\\Scripts\\activate     # Windows\n```\n\n3. **ÂÆâË£Ö‰æùËµñ**\n```bash\npip install -r requirements.txt\n```\n\n4. **ÈÖçÁΩÆÁéØÂ¢ÉÂèòÈáè**\n```bash\ncp .env.example .env\n# ÁºñËæë .env Êñá‰ª∂ÔºåÊ∑ªÂä†ÂøÖË¶ÅÁöÑAPIÂØÜÈí•\n```\n\n### DockerÈÉ®ÁΩ≤\n\n```bash\n# ÊûÑÂª∫Âπ∂ÂêØÂä®ÊâÄÊúâÊúçÂä°\ndocker-compose up -d\n\n# Êü•ÁúãÊúçÂä°Áä∂ÊÄÅ\ndocker-compose ps\n\n# Êü•ÁúãÊó•Âøó\ndocker-compose logs -f\n```\n\n## üõ†Ô∏è ‰ΩøÁî®ÊñπÊ≥ï\n\n### ÂëΩ‰ª§Ë°åÁïåÈù¢\n\n#### Âü∫Êú¨Áî®Ê≥ï\n```bash\n# ÂàÜÊûêÂçï‰∏™ÂÖ¨Âè∏\npython -m src.main analyze AAPL\n\n# ÂàÜÊûêÊäïËµÑÁªÑÂêà\npython -m src.main portfolio AAPL MSFT GOOG\n\n# ‰∫§‰∫íÊ®°Âºè\npython -m src.main interactive\n```\n\n#### È´òÁ∫ßÈÄâÈ°π\n```bash\n# ÊåáÂÆöÂàÜÊûêÁ±ªÂûã\npython -m src.main analyze AAPL --type comprehensive\n\n# ÂØºÂá∫Êä•Âëä\npython -m src.main analyze AAPL --format html,pdf\n\n# Ëá™ÂÆö‰πâÈÖçÁΩÆ\npython -m src.main analyze AAPL --config custom_config.yaml\n```\n\n#### ÈáèÂåñÂàÜÊûêÈÄâÈ°π\n```bash\n# ÂØπÂçï‰∏™ËÇ°Á•®ËøõË°åÈáèÂåñÂàÜÊûê\npython -m src.main quant AAPL\n\n# ‰ΩøÁî®ÁâπÂÆöÂõ†Â≠êËøõË°åÂàÜÊûê\npython -m src.main quant AAPL --factors momentum value growth\n\n# ‰ΩøÁî®ÁâπÂÆöÂõ†Â≠êÊ®°Âûã\npython -m src.main quant AAPL --method carhart\n\n# ÂØºÂá∫ÈáèÂåñÂàÜÊûêÊä•Âëä\npython -m src.main quant AAPL --format html,pdf,json\n```\n\n#### Á≠ñÁï•ÂõûÊµãÈÄâÈ°π\n```bash\n# ËøêË°åÂä®ÈáèÁ≠ñÁï•ÂõûÊµã\npython -m src.main backtest --strategy momentum --start-date 2020-01-01 --end-date 2023-01-01\n\n# ËÆæÁΩÆÂõûÊµãÂèÇÊï∞\npython -m src.main backtest --strategy momentum --start-date 2020-01-01 --end-date 2023-01-01 --initial-capital 100000 --commission 0.001\n\n# ÂØºÂá∫ÂõûÊµãÊä•Âëä\npython -m src.main backtest --strategy momentum --start-date 2020-01-01 --end-date 2023-01-01 --format html,pdf\n```\n\n#### Á≠ñÁï•‰ºòÂåñÈÄâÈ°π\n```bash\n# ‰ºòÂåñÁ≠ñÁï•ÂèÇÊï∞\npython -m src.main optimize --strategy momentum --param window=5,10,15,20\n\n# ËÆæÁΩÆ‰ºòÂåñÊó∂Èó¥ËåÉÂõ¥\npython -m src.main optimize --strategy momentum --param window=5,10,15,20 --start-date 2020-01-01 --end-date 2023-01-01\n```\n\n#### ÊäïËµÑÁªÑÂêà‰ºòÂåñÈÄâÈ°π\n```bash\n# ‰ΩøÁî®ÂùáÂÄº-ÊñπÂ∑Æ‰ºòÂåñÊñπÊ≥ï\npython -m src.main optimize-portfolio --symbols AAPL MSFT GOOG --method mean_variance\n\n# ‰ΩøÁî®È£éÈô©Âπ≥‰ª∑‰ºòÂåñÊñπÊ≥ï\npython -m src.main optimize-portfolio --symbols AAPL MSFT GOOG --method risk_parity\n\n# ËÆæÁΩÆÈ£éÈô©ÂéåÊÅ∂Á≥ªÊï∞\npython -m src.main optimize-portfolio --symbols AAPL MSFT GOOG --method mean_variance --risk-aversion 1.5\n```\n\n### WebÁïåÈù¢\n\nÂêØÂä®WebÊúçÂä°Ôºö\n```bash\npython -m src.api.app\n```\n\nËÆøÈóÆ `http://localhost:8000` ‰ΩøÁî®WebÁïåÈù¢„ÄÇ\n\n### APIÊé•Âè£\n\n#### ÂàõÂª∫ÂàÜÊûê‰ªªÂä°\n```bash\ncurl -X POST \"http://localhost:8000/api/v1/analysis\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"symbols\": [\"AAPL\", \"MSFT\"],\n    \"analysis_type\": \"comprehensive\",\n    \"export_formats\": [\"html\", \"pdf\"]\n  }'\n```\n\n#### Êü•Áúã‰ªªÂä°Áä∂ÊÄÅ\n```bash\ncurl -X GET \"http://localhost:8000/api/v1/analysis/{task_id}\"\n```\n\n#### WebSocketÂÆûÊó∂Êõ¥Êñ∞\n```javascript\nconst ws = new WebSocket('ws://localhost:8000/ws');\nws.onmessage = function(event) {\n    const data = JSON.parse(event.data);\n    console.log('‰ªªÂä°Êõ¥Êñ∞:', data);\n};\n```\n\n## üìä ÂàÜÊûêÊä•Âëä\n\n### Ë¥¢Âä°ÂàÜÊûêÊä•Âëä\n- **ÁõàÂà©ËÉΩÂäõÂàÜÊûê**: ROE„ÄÅROA„ÄÅÊØõÂà©Áéá„ÄÅÂáÄÂà©Áéá\n- **ÂÅøÂÄ∫ËÉΩÂäõÂàÜÊûê**: ËµÑ‰∫ßË¥üÂÄ∫Áéá„ÄÅÊµÅÂä®ÊØîÁéá„ÄÅÈÄüÂä®ÊØîÁéá\n- **ËøêËê•ÊïàÁéáÂàÜÊûê**: ÊÄªËµÑ‰∫ßÂë®ËΩ¨Áéá„ÄÅÂ≠òË¥ßÂë®ËΩ¨Áéá\n- **ÊàêÈïøÊÄßÂàÜÊûê**: Êî∂ÂÖ•Â¢ûÈïøÁéá„ÄÅÂà©Ê∂¶Â¢ûÈïøÁéá\n- **ÊùúÈÇ¶ÂàÜÊûê**: ROEÂàÜËß£‰∏∫ÂáÄÂà©Ê∂¶Áéá„ÄÅËµÑ‰∫ßÂë®ËΩ¨ÁéáÂíåÊùÉÁõä‰πòÊï∞\n\n### È£éÈô©ËØÑ‰º∞Êä•Âëä\n- **Â∏ÇÂú∫È£éÈô©**: VaR„ÄÅCVaR„ÄÅBetaÁ≥ªÊï∞\n- **‰ø°Áî®È£éÈô©**: Z-Score„ÄÅAltmanÊ®°Âûã\n- **ÊµÅÂä®ÊÄßÈ£éÈô©**: ÊµÅÂä®ÊÄßË¶ÜÁõñÁéá„ÄÅÂáÄÁ®≥ÂÆöËµÑÈáëÁéá\n- **Êìç‰ΩúÈ£éÈô©**: ÂéÜÂè≤Ê®°Êãü„ÄÅËíôÁâπÂç°Ê¥õÊ®°Êãü\n- **ÂéãÂäõÊµãËØï**: ÊûÅÁ´ØÂ∏ÇÂú∫ÊÉÖÊôØÂàÜÊûê\n\n### ÈáèÂåñÂàÜÊûêÊä•Âëä\n- **Âõ†Â≠êÂàÜÊûê**: Â§öÂõ†Â≠êÊö¥Èú≤„ÄÅÂõ†Â≠êÊî∂ÁõäÁéá„ÄÅ‰ø°ÊÅØÁ≥ªÊï∞\n- **ÊäïËµÑÁªÑÂêà‰ºòÂåñ**: ÊúâÊïàÂâçÊ≤ø„ÄÅÈ£éÈô©Âπ≥‰ª∑„ÄÅÊúÄÂ§ßÂàÜÊï£Âåñ\n- **Á≠ñÁï•ÂõûÊµã**: Á¥ØËÆ°Êî∂Áõä„ÄÅÊúÄÂ§ßÂõûÊí§„ÄÅÂ§èÊôÆÊØîÁéá\n- **È£éÈô©Ë¥°ÁåÆÂàÜÊûê**: ÂêÑËµÑ‰∫ßÂØπÁªÑÂêàÈ£éÈô©ÁöÑË¥°ÁåÆÂ∫¶\n- **Áª©ÊïàÂΩíÂõ†**: Êî∂ÁõäÊù•Ê∫êÂàÜËß£\n\n### ÊäïËµÑÁªÑÂêàÂàÜÊûêÊä•Âëä\n- **ÊúâÊïàÂâçÊ≤ø**: È£éÈô©Êî∂ÁõäÊúÄ‰ºòÂåñÁªÑÂêà\n- **Â§èÊôÆÊØîÁéá**: È£éÈô©Ë∞ÉÊï¥ÂêéÊî∂Áõä\n- **ÊúÄÂ§ßÂõûÊí§**: ÂéÜÂè≤ÊúÄÂ§ßÊçüÂ§±\n- **Áõ∏ÂÖ≥Á≥ªÊï∞**: ËµÑ‰∫ßÈó¥Áõ∏ÂÖ≥ÊÄßÂàÜÊûê\n- **È£éÈô©Âπ≥‰ª∑**: È£éÈô©Ë¥°ÁåÆÂ∫¶‰ºòÂåñ\n\n## üîß ÈÖçÁΩÆËØ¥Êòé\n\n### ‰∏ªË¶ÅÈÖçÁΩÆÊñá‰ª∂\n\n#### config.yaml\n```yaml\n# AutoGenÈÖçÁΩÆ\nautogen:\n  gpt_model: \"gpt-4\"\n  temperature: 0.7\n  max_tokens: 4000\n\n# Êï∞ÊçÆÊ∫êÈÖçÁΩÆ\ndata_sources:\n  yahoo_finance:\n    timeout: 30\n    retry_count: 3\n  alpha_vantage:\n    api_key: \"${ALPHA_VANTAGE_API_KEY}\"\n    calls_per_minute: 5\n```\n\n#### ÁéØÂ¢ÉÂèòÈáè\n```bash\n# APIÂØÜÈí•\nYAHOO_FINANCE_API_KEY=your_key_here\nALPHA_VANTAGE_API_KEY=your_key_here\n\n# Êï∞ÊçÆÂ∫ìÈÖçÁΩÆ\nDATABASE_URL=postgresql://user:password@localhost:5432/autogen_financial\nREDIS_URL=redis://localhost:6379/0\n\n# ÂÆâÂÖ®ÈÖçÁΩÆ\nSECRET_KEY=your_secret_key_here\nJWT_SECRET=your_jwt_secret_here\n```\n\n## üß™ ÊµãËØï\n\n### ËøêË°åÊµãËØï\n```bash\n# ËøêË°åÊâÄÊúâÊµãËØï\npytest\n\n# ËøêË°åÁâπÂÆöÊ®°ÂùóÊµãËØï\npytest tests/test_data.py\n\n# ËøêË°åAPIÊµãËØï\npytest tests/test_api.py\n\n# ÁîüÊàêË¶ÜÁõñÁéáÊä•Âëä\npytest --cov=src --cov-report=html\n```\n\n### ÊµãËØïË¶ÜÁõñÁéá\n- Êï∞ÊçÆÊî∂ÈõÜÊ®°Âùó: 95%\n- Ë¥¢Âä°ÂàÜÊûêÊ®°Âùó: 92%\n- È£éÈô©ÂàÜÊûêÊ®°Âùó: 90%\n- APIÊé•Âè£: 88%\n- Êï¥‰ΩìË¶ÜÁõñÁéá: 93%\n\n## üìà ÊÄßËÉΩÁõëÊéß\n\n### Á≥ªÁªüÊåáÊ†á\n- CPU‰ΩøÁî®Áéá\n- ÂÜÖÂ≠ò‰ΩøÁî®Èáè\n- Á£ÅÁõòI/O\n- ÁΩëÁªúÂêûÂêêÈáè\n- Êï∞ÊçÆÂ∫ìËøûÊé•Êï∞\n- RedisÂëΩ‰∏≠Áéá\n\n### ‰∏öÂä°ÊåáÊ†á\n- Êï∞ÊçÆÊî∂ÈõÜÊàêÂäüÁéá\n- ÂàÜÊûê‰ªªÂä°ÊâßË°åÊó∂Èó¥\n- APIÂìçÂ∫îÊó∂Èó¥\n- ÈîôËØØÁéá\n- Áî®Êà∑Ê¥ªË∑ÉÂ∫¶\n\n### ËÆøÈóÆÁõëÊéßÁïåÈù¢\n```bash\n# Grafana‰ª™Ë°®Êùø\nhttp://localhost:3000\n\n# PrometheusÊü•ËØ¢ÁïåÈù¢\nhttp://localhost:9090\n```\n\n## üîí ÂÆâ",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-17T18:17:19.969706"
  },
  {
    "basic_info": {
      "name": "CRT_Python_AI_A",
      "full_name": "gilshan-s/CRT_Python_AI_A",
      "owner": "gilshan-s",
      "description": "Coding",
      "url": "https://github.com/gilshan-s/CRT_Python_AI_A",
      "clone_url": "https://github.com/gilshan-s/CRT_Python_AI_A.git",
      "ssh_url": "git@github.com:gilshan-s/CRT_Python_AI_A.git",
      "homepage": null,
      "created_at": "2025-09-16T04:28:17Z",
      "updated_at": "2025-09-17T05:01:25Z",
      "pushed_at": "2025-09-16T05:24:03Z"
    },
    "stats": {
      "stars": 39,
      "forks": 37,
      "watchers": 39,
      "open_issues": 0,
      "size": 2
    },
    "tech_info": {
      "language": null,
      "languages": {},
      "license": null,
      "topics": []
    },
    "content": {
      "readme": "# CRT_Python_AI_A\nCoding\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-17T18:17:21.193835"
  },
  {
    "basic_info": {
      "name": "ECE_F_CRT_PYTHON",
      "full_name": "gilshan-s/ECE_F_CRT_PYTHON",
      "owner": "gilshan-s",
      "description": "CODING",
      "url": "https://github.com/gilshan-s/ECE_F_CRT_PYTHON",
      "clone_url": "https://github.com/gilshan-s/ECE_F_CRT_PYTHON.git",
      "ssh_url": "git@github.com:gilshan-s/ECE_F_CRT_PYTHON.git",
      "homepage": null,
      "created_at": "2025-09-17T04:49:52Z",
      "updated_at": "2025-09-17T05:02:14Z",
      "pushed_at": "2025-09-17T04:49:52Z"
    },
    "stats": {
      "stars": 38,
      "forks": 41,
      "watchers": 38,
      "open_issues": 0,
      "size": 0
    },
    "tech_info": {
      "language": null,
      "languages": {},
      "license": null,
      "topics": []
    },
    "content": {
      "readme": "# ECE_F_CRT_PYTHON\nCODING\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-17T18:17:22.413686"
  },
  {
    "basic_info": {
      "name": "comfydeploy",
      "full_name": "comfy-deploy/comfydeploy",
      "owner": "comfy-deploy",
      "description": "ComfyDeployed",
      "url": "https://github.com/comfy-deploy/comfydeploy",
      "clone_url": "https://github.com/comfy-deploy/comfydeploy.git",
      "ssh_url": "git@github.com:comfy-deploy/comfydeploy.git",
      "homepage": null,
      "created_at": "2025-09-17T02:53:50Z",
      "updated_at": "2025-09-17T17:45:51Z",
      "pushed_at": "2025-09-17T15:03:11Z"
    },
    "stats": {
      "stars": 36,
      "forks": 7,
      "watchers": 36,
      "open_issues": 0,
      "size": 323
    },
    "tech_info": {
      "language": null,
      "languages": {},
      "license": "GNU General Public License v3.0",
      "topics": []
    },
    "content": {
      "readme": "# ComfyDeploy\n\nRe: Open-sourcing ComfyDeploy\n\nTL;DR: We are open-sourcing ComfyDeploy again, with full platform backend + frontend.\n\n[Discord](https://discord.gg/qtHUaVNRVM) | [Website](https://www.comfydeploy.com)\n\n---\n\n### How we got here?\n\nIn late 2023, I started ComfyDeploy as an open source project while I was working at my previous company. We had a problem deploying ComfyUI to our production server due to how complicated it is to get ComfyUI into a serverless environment.\n\nLittle did I know I was going to meet my co-founder Nick and embark on a journey I never dreamt of.\n\nI posted on Twitter about this little project that I was working on as an indie hacker, and it blew up overnight. I woke up to 100k impressions on the post. I put up my cal link and people started scheduling calls. I got to talk to a bunch of people across the globe, with people reaching out to help or potentially use ComfyDeploy.\n\nNick was one of the early contributors, and later we decided to apply to a bunch of accelerators together. It was early February, just a couple of weeks after we met. We applied to YC‚Äîwe were unsure if we would even get in, but we still decided to quit our jobs, and the day after we quit, we got in.\n\nIt was insane to even think that we would have had a chance.\n\n### Life Changing Decision\n\nWe immediately said yes.\n\nWe got into YC with ComfyDeploy around 2.5K MRR.\n\nAround the same timeframe, ComfyOrg was introduced, Stability collapsed, and Flux just came out.\n\nWe kept building ComfyDeploy, for months, and kept things going.\n\nThe company was growing, but very slowly. We realized the biggest issue is that we are still really early, and it takes time for businesses and enterprises to really adopt such a niche tool. And we are not ComfyOrg.\n\n### Dynamic Changes\n\nMeanwhile, closed source models dropped, and a bunch of workflows that we knew became no longer useful.\n\nComing from a game developer background, I saw huge potential with ComfyUI at first, but I never would have imagined that one giant model could do exactly what you put into words, and you still need workflows to fine-tune and control the exact outputs. And ComfyDeploy did make it possible for teams to experiment with this.\n\nWe were just unsure about the future of the company. While we kept getting more and more business inquiries about ComfyUI, this is what really kept us going before giving up. It's a mixed signal.\n\nMeanwhile, my friends bootstrapped their mobile app to $500k a month. I was really stressed‚Äîwere we going to live up to the aspirations that we set out, both personally and for the company?\n\n### Problems\n\nI feel immensely grateful for the customers that we were able to help, and this is something I feel forever privileged to experience‚Äîworking with the best creative teams across the globe and brands that I look up to.\n\nBut we were stuck in the middle. First, we are not ComfyOrg; second, closed source models were doing things way better and slowly eating up the market.\n\nAnd it's inevitable that ComfyOrg will have some sort of cloud solution. And we do appreciate all the work ComfyOrg has put into making ComfyUI great again. We realized we would be competing in some ways especially in the cloud space, and we could never grow out of this, which constrained the company's growth at the same time.\n\nAs of today, ComfyDeploy is doing $29k MRR, and our last 30 days' revenue was $50k processed. Which is the highest we have ever got, but also the most depressing day I have ever had.\n\n### So what now?\n\nWe have been working for months on our pay-as-you-go tier‚Äîno longer requiring you to talk to us, just pay for the cloud resources you use. And also, going back to our roots: open-sourcing the ENTIRE CLOUD PLATFORM while continuing to support existing customers.\n\nOur current customers and creative teams are the people who kept us going for the last year, and I will be forever grateful. But this might just not be the field for us, and we respect Comfy and do not want to get in the way. And I think it's going to be really good for everyone.\n\n#### What does this mean for current customers: \n\nThe service will stay online and supported until the last standing user!\n\n#### What does this mean for future customers: \nYou can either go with the pay-as-you-go tier to get started, or self-host the entire platform. If ComfyOrg eventually builds something like ComfyDeploy, the official solution will be recommended by us!\n\n#### What does this mean for ComfyDeploy team: \n\nWe will continue supporting existing customers. At the same time we will start exploring new ideas. And this does not mean the end to us. But rather a fresh start. Stay tuned for what's coming next.\n\n#### What I will be doing for the next couple days\n1. Documentation and tutorials to set up ComfyDeploy\n2. Explore new problems that could resonate with us.\n\n### Credits\n\nA list of credits that got us here (doesn't go in order)\nComfy, Nick, Karrix, Jeff, Edgar, Ecjojo, Brad, Aashay and Semil, Jon, Choco",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-17T18:17:23.619104"
  },
  {
    "basic_info": {
      "name": "C3_CRT_Python",
      "full_name": "gilshan-s/C3_CRT_Python",
      "owner": "gilshan-s",
      "description": "Coding",
      "url": "https://github.com/gilshan-s/C3_CRT_Python",
      "clone_url": "https://github.com/gilshan-s/C3_CRT_Python.git",
      "ssh_url": "git@github.com:gilshan-s/C3_CRT_Python.git",
      "homepage": null,
      "created_at": "2025-09-16T09:17:49Z",
      "updated_at": "2025-09-16T09:46:04Z",
      "pushed_at": "2025-09-16T09:41:18Z"
    },
    "stats": {
      "stars": 30,
      "forks": 29,
      "watchers": 30,
      "open_issues": 0,
      "size": 2
    },
    "tech_info": {
      "language": null,
      "languages": {},
      "license": null,
      "topics": []
    },
    "content": {
      "readme": "# C3_CRT_Python\nCoding\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-17T18:17:24.863841"
  }
]