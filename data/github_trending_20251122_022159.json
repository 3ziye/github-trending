[
  {
    "basic_info": {
      "name": "Valdi",
      "full_name": "Snapchat/Valdi",
      "owner": "Snapchat",
      "description": "Valdi is a cross-platform UI framework that delivers native performance without sacrificing developer velocity.",
      "url": "https://github.com/Snapchat/Valdi",
      "clone_url": "https://github.com/Snapchat/Valdi.git",
      "ssh_url": "git@github.com:Snapchat/Valdi.git",
      "homepage": "https://discord.gg/uJyNEeYX2U",
      "created_at": "2025-11-06T17:33:28Z",
      "updated_at": "2025-11-22T02:21:00Z",
      "pushed_at": "2025-11-22T00:19:51Z"
    },
    "stats": {
      "stars": 12149,
      "forks": 410,
      "watchers": 12149,
      "open_issues": 43,
      "size": 82063
    },
    "tech_info": {
      "language": "C++",
      "languages": {
        "C++": 8517177,
        "TypeScript": 4865203,
        "JavaScript": 2334883,
        "Swift": 1834904,
        "C": 1666128,
        "Kotlin": 1178292,
        "Objective-C": 1002546,
        "Objective-C++": 563220,
        "Starlark": 418252,
        "Java": 43300,
        "Shell": 35915,
        "Smarty": 6857,
        "HTML": 4271,
        "Python": 3905,
        "Pug": 645,
        "Makefile": 426,
        "Dockerfile": 235,
        "Linker Script": 171,
        "CSS": 140,
        "Go": 86,
        "SCSS": 20
      },
      "license": "Other",
      "topics": [
        "android",
        "cross-platform",
        "ios",
        "typescript",
        "valdi"
      ]
    },
    "content": {
      "readme": "# Valdi\n\n[![License: MIT](https://img.shields.io/badge/License-MIT-blue.svg)](./LICENSE.md)\n[![Platforms](https://img.shields.io/badge/platform-iOS%20%7C%20Android%20%7C%20macOS-lightgrey)](./docs/INSTALL.md)\n[![Status](https://img.shields.io/badge/status-beta-yellow)]()\n[![Discord](https://img.shields.io/discord/1285677307163574322?color=7289da&label=Discord&logo=discord&logoColor=white)](https://discord.gg/uJyNEeYX2U)\n[![TypeScript](https://img.shields.io/badge/TypeScript-5.x-blue?logo=typescript)](https://www.typescriptlang.org/)\n[![Documentation](https://img.shields.io/badge/docs-available-brightgreen)](./docs/README.md)\n[![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg)](./CONTRIBUTING.md)\n\n> [!NOTE]\n> **Beta Status:** Valdi has been widely used in Snap's production apps for the last 8 years. We're calling this a beta because our tools and documentation need more battle testing in the open source world. Valdi will exit beta when we're happy with the developer experience.\n\n**Valdi is a cross-platform UI framework that delivers native performance without sacrificing developer velocity.** Write your UI once in declarative TypeScript, and it compiles directly to native views on iOS, Android, and macOS‚Äîno web views, no JavaScript bridges. \n\n## Quick Example\n\nA basic Valdi component:\n\n```tsx\nimport { Component } from 'valdi_core/src/Component';\n\nclass HelloWorld extends Component {\n  onRender() {\n    const message = 'Hello World! üëª';\n    <view backgroundColor='#FFFC00' padding={30}>\n      <label color='black' value={message} />\n    </view>;\n  }\n}\n```\n\n<p align=\"center\">\n  <img src=\"./docs/docs/assets/start-about/IMG_1445.jpg\" width=\"400\" alt=\"Hello World example running on iOS\" />\n</p>\n\n## Quick Start\n\n**Prerequisites:** Xcode (macOS only) - everything else is automatic!\n\n```bash\n# Install Valdi CLI\nnpm install -g @snap/valdi\n\n# One-command setup (installs all dependencies)\nvaldi dev_setup\n\n# Create your first project\nmkdir my_project && cd my_project\nvaldi bootstrap\nvaldi install ios  # or android\n```\n\n> [!TIP]\n> **Editor Extensions:** For the best development experience, install the [Valdi VSCode/Cursor extensions](./docs/INSTALL.md#vscodecursor-setup-optional-but-recommended) for syntax highlighting, debugging, and device logs during hot reload.\n\n## Quick Links\n\n- [Getting Started Guide](./docs/INSTALL.md)\n- [Documentation](./docs/README.md)\n- [Codelabs](./docs/docs/start-code-lab.md)\n- [API Reference](./docs/api/api-quick-reference.md)\n- [Frequently Asked Questions](./docs/docs/faq.md)\n- [Component Library](https://github.com/Snapchat/Valdi_Widgets)\n\n## Why Choose Valdi?\n\nValdi is a cross-platform UI framework designed to solve the fundamental problem of cross-platform development: velocity vs. runtime performance. For 8 years, it has powered a large portion of Snap's production apps.\n\n### True Native Performance\n\nUnlike frameworks that rely on web views or JavaScript bridges, Valdi compiles declaratively rendered TypeScript components into platform-native views. Valdi also includes several other performance advantages:\n\n- **[Automatic view recycling](./docs/docs/performance-view-recycling.md)** - Global view pooling system reuses native views across all screens, dramatically reducing inflation latency\n- **Optimized component rendering** - Components re-render independently without triggering parent re-renders, enabling fast incremental updates\n- **Optimized layout engine** - C++ layout engine runs on the main thread with minimal marshalling overhead\n- **Viewport-aware rendering** - Only visible views are inflated, making infinite scrolling performant by default\n\nLearn more in our [Performance Optimization Guide](./docs/docs/performance-optimization.md).\n\n### Developer Experience Built for Speed\n\nValdi eliminates the traditional compile-test-debug cycle that slows native development:\n\n- **Instant hot reload** - See changes in milliseconds on iOS, Android, or desktop without recompiling\n- **[Full VSCode debugging](./docs/docs/workflow-hermes-debugger.md)** - Set breakpoints, inspect variables, profile performance, and capture heap dumps directly in VSCode\n- **Familiar syntax** - TSX components with TypeScript for type safety\n\n### Flexible Adoption Model\n\nValdi integrates easily into existing apps - start small and scale as needed:\n\n- **[Embed Valdi in native](./docs/docs/native-bindings.md)** - Drop Valdi components into existing UIKit or Android view hierarchies\n- **[Embed native in Valdi](./docs/docs/native-customviews.md)** - Use platform-specific views within Valdi layouts via `<custom-view>`\n- **[Polyglot modules](./docs/docs/native-polyglot.md)** - Write performance-critical code in C++, Swift, Kotlin, or Objective-C with type-safe bindings to TypeScript\n- **[Full-stack architecture](./docs/docs/advanced-full-stack.md)** - Build entire features in Valdi with worker threads for background processing, eliminating platform-specific bridge code\n\n### Deep Native Integration\n\nValdi ",
      "default_branch": "main"
    },
    "fetched_at": "2025-11-22T02:21:59.928978"
  },
  {
    "basic_info": {
      "name": "AI-Trader",
      "full_name": "HKUDS/AI-Trader",
      "owner": "HKUDS",
      "description": "\"AI-Trader: Can AI Beat the Market?\" Live Trading Bench: https://ai4trade.ai",
      "url": "https://github.com/HKUDS/AI-Trader",
      "clone_url": "https://github.com/HKUDS/AI-Trader.git",
      "ssh_url": "git@github.com:HKUDS/AI-Trader.git",
      "homepage": "https://ai4trade.ai",
      "created_at": "2025-10-23T12:45:00Z",
      "updated_at": "2025-11-22T02:17:32Z",
      "pushed_at": "2025-11-20T11:22:28Z"
    },
    "stats": {
      "stars": 9564,
      "forks": 1480,
      "watchers": 9564,
      "open_issues": 37,
      "size": 16338
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 415459,
        "Shell": 6419
      },
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "\n\n<div align=\"center\">\n  <picture>\n      <img src=\"./assets/AI-Trader-log.png\" width=\"20%\" style=\"border: none; box-shadow: none;\">\n  </picture>\n</div >\n\n<div align=\"center\">\n\n# üöÄ AI-Trader: Can AI Beat the Market?\n\n[![Python](https://img.shields.io/badge/Python-3.10+-blue.svg)](https://python.org)\n[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)\n[![GitHub stars](https://img.shields.io/github/stars/HKUDS/AI-Trader?style=social)](https://github.com/HKUDS/AI-Trader)\n[![Feishu](https://img.shields.io/badge/üí¨Feishu-Group-blue?style=flat)](./Communication.md) \n[![WeChat](https://img.shields.io/badge/WeChat-Group-green?style=flat&logo=wechat)](./Communication.md)\n\n**AI agents battle for supremacy in NASDAQ 100, SSE 50, and cryptocurrency markets. Zero human input. Pure competition.**\n\n## üèÜ Current Championship Leaderboard üèÜ \n[*Click Here: AI Live Trading*](https://ai4trade.ai)\n\n</div>\n\n---\n## Friends of AI-Trader: Other Interesting Projects\n- [TradeTrap](https://github.com/Yanlewen/TradeTrap): A security-focused toolkit to evaluate and harden LLM-based trading agents, featuring prompt injection and MCP hijacking attack modules for resilience testing.\n\n- [RockAlpha](https://rockalpha.rockflow.ai/): The investment arena launched by RockFlow. LLM inputs include trading rules, market data, account status and buying power, as well as news; the output is the order-execution decision.\n\n- [TwinMarket](https://github.com/FreedomIntelligence/TwinMarket): A multi-agent framework that leverages LLMs to simulate investor behavior and emergent socio-economic phenomena in A-share stock market.\n---\n## üéâ Weekly Update\n\n### üìà Market Expansion\n- ‚úÖ **A-Share Market Support** - Extended our trading capabilities to include Chinese A-share markets, expanding our global market coverage.\n- ‚úÖ **Cryptocurrency Market Support** - Added support for trading major cryptocurrencies including Bitcoin, Ethereum, and 8 other leading digital assets.\n\n### ‚è∞ Enhanced Trading Capabilities\n- ‚úÖ **Hourly Trading Support** - We've upgraded from daily to hourly trading intervals, enabling more precise and responsive market participation with granular timing control.\n\n### üé® User Experience Improvements\n- ‚úÖ **Live Trading Dashboard** - Introduced real-time visualization of all agent trading activities: https://ai4trade.ai.\n\n- ‚úÖ **Agent Reasoning Display** - Implemented complete transparency into AI decision-making processes, featuring detailed reasoning chains that show how each trading decision is formed.\n\n- ‚úÖ **Interactive Leaderboard** - Launched a dynamic performance ranking system with live updates, allowing users to track and compare agent performance in real-time.\n\n- ‚è∞ **Important Notice** - To maintain a well-managed repository, we no longer upload runtime data to the repo, as it would make it very bloated. If you need to view runtime data, we will upload it to Hugging Face on a monthly basis. You can view real-time runtime data here: https://ai4trade.ai.\n---\n\n## **How to use this dataset**\n\nIt's simple! \n\nYou just need to submit a PR that includes at least: `./agent/{your_strategy}.py` (you can inherit from Basemodel to create your strategy!), `./configs/{yourconfig}`, and instructions on how to run your strategy. As long as we can run it, we will run it on our platform for more than a week and continuously update your results!\n\n---\n\n<div align=\"center\">\n\n[üöÄ Quick Start](#-quick-start) ‚Ä¢ [üìà Performance Analysis](#-performance-analysis) ‚Ä¢ [üõ†Ô∏è Configuration Guide](#-configuration-guide) ‚Ä¢ [‰∏≠ÊñáÊñáÊ°£](README_CN.md)\n\n</div>\n\n\n## üåü Project Introduction\n\n> **AI-Trader enables five distinct AI models, each employing unique investment strategies, to compete autonomously in the same market and determine which can generate the highest profits in NASDAQ 100, SSE 50, or cryptocurrency trading!**\n\n### üéØ Core Features\n\n- ü§ñ **Fully Autonomous Decision-Making**: AI agents perform 100% independent analysis, decision-making, and execution without human intervention\n- üõ†Ô∏è **Pure Tool-Driven Architecture**: Built on MCP toolchain, enabling AI to complete all trading operations through standardized tool calls\n- üèÜ **Multi-Model Competition Arena**: Deploy multiple AI models (GPT, Claude, Qwen, etc.) for competitive trading\n- üìä **Real-Time Performance Analytics**: Comprehensive trading records, position monitoring, and profit/loss analysis\n- üîç **Intelligent Market Intelligence**: Integrated Jina search for real-time market news and financial reports\n- ‚ö° **MCP Toolchain Integration**: Modular tool ecosystem based on Model Context Protocol\n- üîå **Extensible Strategy Framework**: Support for third-party strategies and custom AI agent integration\n- ‚è∞ **Historical Replay Capability**: Time-period replay functionality with automatic future information filtering\n\n---\n\n### üéÆ Trading Environment\nEach AI model starts with $10,000, 100,000¬•, or 50,000 USDT to trade NASDAQ 100 stocks, SSE 50 stocks, or major cryptocurrencies in a controlled environment with real mar",
      "default_branch": "main"
    },
    "fetched_at": "2025-11-22T02:22:01.217850"
  },
  {
    "basic_info": {
      "name": "nofx",
      "full_name": "NoFxAiOS/nofx",
      "owner": "NoFxAiOS",
      "description": "NOFX: Defining the Next-Generation AI Trading Operating System. A multi-exchange Al trading platform(Binance/Hyperliquid/Aster) with multi-Ai competition(deepseek/qwen/claude)self-evolution, and real-time dashboard",
      "url": "https://github.com/NoFxAiOS/nofx",
      "clone_url": "https://github.com/NoFxAiOS/nofx.git",
      "ssh_url": "git@github.com:NoFxAiOS/nofx.git",
      "homepage": "https://x.com/nofx_official",
      "created_at": "2025-10-28T07:17:53Z",
      "updated_at": "2025-11-22T01:59:20Z",
      "pushed_at": "2025-11-20T23:12:41Z"
    },
    "stats": {
      "stars": 7939,
      "forks": 2059,
      "watchers": 7939,
      "open_issues": 334,
      "size": 44190
    },
    "tech_info": {
      "language": "Go",
      "languages": {
        "Go": 872856,
        "TypeScript": 762973,
        "Shell": 74460,
        "CSS": 13770,
        "Makefile": 4089,
        "JavaScript": 2932,
        "HTML": 1062
      },
      "license": "GNU Affero General Public License v3.0",
      "topics": [
        "agentic-ai",
        "agentictrading",
        "ai",
        "ai-trading",
        "aitradingos",
        "nof1ai",
        "trading"
      ]
    },
    "content": {
      "readme": "# ü§ñ NOFX - Agentic Trading OS\n\n[![Go Version](https://img.shields.io/badge/Go-1.21+-00ADD8?style=flat&logo=go)](https://golang.org/)\n[![React](https://img.shields.io/badge/React-18+-61DAFB?style=flat&logo=react)](https://reactjs.org/)\n[![TypeScript](https://img.shields.io/badge/TypeScript-5.0+-3178C6?style=flat&logo=typescript)](https://www.typescriptlang.org/)\n[![License](https://img.shields.io/badge/License-AGPL--3.0-blue.svg)](LICENSE)\n[![Backed by Amber.ac](https://img.shields.io/badge/Backed%20by-Amber.ac-orange.svg)](https://amber.ac)\n\n**Languages:** [English](README.md) | [‰∏≠Êñá](docs/i18n/zh-CN/README.md) | [–£–∫—Ä–∞—ó–Ω—Å—å–∫–∞](docs/i18n/uk/README.md) | [–†—É—Å—Å–∫–∏–π](docs/i18n/ru/README.md) | [Êó•Êú¨Ë™û](docs/i18n/ja/README.md)\n\n**üìö Documentation:** [Docs Home](docs/README.md) | [Getting Started](docs/getting-started/README.md) | [Prompt Writing Guide](docs/prompt-guide.md) ([‰∏≠Êñá](docs/prompt-guide.zh-CN.md)) | [Changelog](CHANGELOG.md) | [Contributing](CONTRIBUTING.md) | [Security](SECURITY.md)\n\n---\n\n## üìë Table of Contents\n\n- [üöÄ Universal AI Trading Operating System](#-universal-ai-trading-operating-system)\n- [üë• Developer Community](#-developer-community)\n- [üÜï What's New](#-whats-new-latest-update)\n- [üì∏ Screenshots](#-screenshots)\n- [‚ú® Current Implementation](#-current-implementation---crypto-markets)\n- [üîÆ Roadmap](#-roadmap---universal-market-expansion)\n- [üèóÔ∏è Technical Architecture](#Ô∏è-technical-architecture)\n- [üí∞ Register Binance Account](#-register-binance-account-save-on-fees)\n- [üî∑ Register Hyperliquid Account](#-using-hyperliquid-exchange)\n- [üî∂ Register Aster DEX Account](#-using-aster-dex-exchange)\n- [üöÄ Quick Start](#-quick-start)\n- [üìñ AI Decision Flow](#-ai-decision-flow)\n- [üß† AI Self-Learning](#-ai-self-learning-example)\n- [üìä Web Interface Features](#-web-interface-features)\n- [üéõÔ∏è API Endpoints](#Ô∏è-api-endpoints)\n- [‚ö†Ô∏è Important Risk Warnings](#Ô∏è-important-risk-warnings)\n- [üõ†Ô∏è Common Issues](#Ô∏è-common-issues)\n- [üìà Performance Tips](#-performance-optimization-tips)\n- [üîÑ Changelog](#-changelog)\n- [üìÑ License](#-license)\n- [ü§ù Contributing](#-contributing)\n\n---\n\n## üöÄ Universal AI Trading Operating System\n\n**NOFX** is a **universal Agentic Trading OS** built on a unified architecture. We've successfully closed the loop in crypto markets: **\"Multi-Agent Decision ‚Üí Unified Risk Control ‚Üí Low-Latency Execution ‚Üí Live/Paper Account Backtesting\"**, and are now expanding this same technology stack to **stocks, futures, options, forex, and all financial markets**.\n\n### üéØ Core Features\n\n- **Universal Data & Backtesting Layer**: Cross-market, cross-timeframe, cross-exchange unified representation and factor library, accumulating transferable \"strategy memory\"\n- **Multi-Agent Self-Play & Self-Evolution**: Strategies automatically compete and select the best, continuously iterating based on account-level PnL and risk constraints\n- **Integrated Execution & Risk Control**: Low-latency routing, slippage/risk control sandbox, account-level limits, one-click market switching\n\n### üè¢ Backed by [Amber.ac](https://amber.ac)\n\n### üë• Core Team\n\n- **Tinkle** - [@Web3Tinkle](https://x.com/Web3Tinkle)\n\n### üíº Seed Funding Round Open\n\nWe are currently raising our **seed round**.\n\n**For investment inquiries**, please DM **Tinkle** via Twitter.\n\n---\n\n> ‚ö†Ô∏è **Risk Warning**: This system is experimental. AI auto-trading carries significant risks. Strongly recommended for learning/research purposes or testing with small amounts only!\n\n## üë• Developer Community\n\nJoin our Telegram developer community to discuss, share ideas, and get support:\n\n**üí¨ [NOFX Developer Community](https://t.me/nofx_dev_community)**\n\n---\n\n## üÜï What's New (Latest Update)\n\n### üöÄ Multi-Exchange Support!\n\nNOFX now supports **three major exchanges**: Binance, Hyperliquid, and Aster DEX!\n\n#### **Hyperliquid Exchange**\n\nA high-performance decentralized perpetual futures exchange!\n\n**Key Features:**\n- ‚úÖ Full trading support (long/short, leverage, stop-loss/take-profit)\n- ‚úÖ Automatic precision handling (order size & price)\n- ‚úÖ Unified trader interface (seamless exchange switching)\n- ‚úÖ Support for both mainnet and testnet\n- ‚úÖ No API keys needed - just your Ethereum private key\n\n**New Workflow:**\n1. **Configure AI Models**: Add your DeepSeek/Qwen API keys through the web interface\n2. **Configure Exchanges**: Set up Binance/Hyperliquid API credentials\n3. **Create Traders**: Combine any AI model with any exchange to create custom traders\n4. **Monitor & Control**: Start/stop traders and monitor performance in real-time\n\n**Why This Update?**\n- üéØ **User-Friendly**: No more editing JSON files or server restarts\n- üîß **Flexible**: Mix and match different AI models with different exchanges\n- üìä **Scalable**: Create unlimited trader combinations\n- üîí **Secure**: Database storage with proper data management\n\nSee [Quick Start](#-quick-start) for the new setup process!\n\n#### **Aster DEX Exchange** (NEW! v2.0.2)\n\nA Binance-compatible decentralized perpetual futures exchange!\n\n**Key Features:**\n- ‚úÖ Binance-",
      "default_branch": "dev"
    },
    "fetched_at": "2025-11-22T02:22:02.574758"
  },
  {
    "basic_info": {
      "name": "claude-code-infrastructure-showcase",
      "full_name": "diet103/claude-code-infrastructure-showcase",
      "owner": "diet103",
      "description": "Examples of my Claude Code infrastructure with skill auto-activation, hooks, and agents",
      "url": "https://github.com/diet103/claude-code-infrastructure-showcase",
      "clone_url": "https://github.com/diet103/claude-code-infrastructure-showcase.git",
      "ssh_url": "git@github.com:diet103/claude-code-infrastructure-showcase.git",
      "homepage": null,
      "created_at": "2025-10-30T03:12:16Z",
      "updated_at": "2025-11-22T01:37:12Z",
      "pushed_at": "2025-10-31T01:41:31Z"
    },
    "stats": {
      "stars": 6937,
      "forks": 905,
      "watchers": 6937,
      "open_issues": 13,
      "size": 214
    },
    "tech_info": {
      "language": "Shell",
      "languages": {
        "Shell": 19297,
        "JavaScript": 12798
      },
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "# Claude Code Infrastructure Showcase\n\n**A curated reference library of production-tested Claude Code infrastructure.**\n\nBorn from 6 months of real-world use managing a complex TypeScript microservices project, this showcase provides the patterns and systems that solved the \"skills don't activate automatically\" problem and scaled Claude Code for enterprise development.\n\n> **This is NOT a working application** - it's a reference library. Copy what you need into your own projects.\n\n---\n\n## What's Inside\n\n**Production-tested infrastructure for:**\n- ‚úÖ **Auto-activating skills** via hooks\n- ‚úÖ **Modular skill pattern** (500-line rule with progressive disclosure)\n- ‚úÖ **Specialized agents** for complex tasks\n- ‚úÖ **Dev docs system** that survives context resets\n- ‚úÖ **Comprehensive examples** using generic blog domain\n\n**Time investment to build:** 6 months of iteration\n**Time to integrate into your project:** 15-30 minutes\n\n---\n\n## Quick Start - Pick Your Path\n\n### ü§ñ Using Claude Code to Integrate?\n\n**Claude:** Read [`CLAUDE_INTEGRATION_GUIDE.md`](CLAUDE_INTEGRATION_GUIDE.md) for step-by-step integration instructions tailored for AI-assisted setup.\n\n### üéØ I want skill auto-activation\n\n**The breakthrough feature:** Skills that actually activate when you need them.\n\n**What you need:**\n1. The skill-activation hooks (2 files)\n2. A skill or two relevant to your work\n3. 15 minutes\n\n**üëâ [Setup Guide: .claude/hooks/README.md](.claude/hooks/README.md)**\n\n### üìö I want to add ONE skill\n\nBrowse the [skills catalog](.claude/skills/) and copy what you need.\n\n**Available:**\n- **backend-dev-guidelines** - Node.js/Express/TypeScript patterns\n- **frontend-dev-guidelines** - React/TypeScript/MUI v7 patterns\n- **skill-developer** - Meta-skill for creating skills\n- **route-tester** - Test authenticated API routes\n- **error-tracking** - Sentry integration patterns\n\n**üëâ [Skills Guide: .claude/skills/README.md](.claude/skills/README.md)**\n\n### ü§ñ I want specialized agents\n\n10 production-tested agents for complex tasks:\n- Code architecture review\n- Refactoring assistance\n- Documentation generation\n- Error debugging\n- And more...\n\n**üëâ [Agents Guide: .claude/agents/README.md](.claude/agents/README.md)**\n\n---\n\n## What Makes This Different?\n\n### The Auto-Activation Breakthrough\n\n**Problem:** Claude Code skills just sit there. You have to remember to use them.\n\n**Solution:** UserPromptSubmit hook that:\n- Analyzes your prompts\n- Checks file context\n- Automatically suggests relevant skills\n- Works via `skill-rules.json` configuration\n\n**Result:** Skills activate when you need them, not when you remember them.\n\n### Production-Tested Patterns\n\nThese aren't theoretical examples - they're extracted from:\n- ‚úÖ 6 microservices in production\n- ‚úÖ 50,000+ lines of TypeScript\n- ‚úÖ React frontend with complex data grids\n- ‚úÖ Sophisticated workflow engine\n- ‚úÖ 6 months of daily Claude Code use\n\nThe patterns work because they solved real problems.\n\n### Modular Skills (500-Line Rule)\n\nLarge skills hit context limits. The solution:\n\n```\nskill-name/\n  SKILL.md                  # <500 lines, high-level guide\n  resources/\n    topic-1.md              # <500 lines each\n    topic-2.md\n    topic-3.md\n```\n\n**Progressive disclosure:** Claude loads main skill first, loads resources only when needed.\n\n---\n\n## Repository Structure\n\n```\n.claude/\n‚îú‚îÄ‚îÄ skills/                 # 5 production skills\n‚îÇ   ‚îú‚îÄ‚îÄ backend-dev-guidelines/  (12 resource files)\n‚îÇ   ‚îú‚îÄ‚îÄ frontend-dev-guidelines/ (11 resource files)\n‚îÇ   ‚îú‚îÄ‚îÄ skill-developer/         (7 resource files)\n‚îÇ   ‚îú‚îÄ‚îÄ route-tester/\n‚îÇ   ‚îú‚îÄ‚îÄ error-tracking/\n‚îÇ   ‚îî‚îÄ‚îÄ skill-rules.json    # Skill activation configuration\n‚îú‚îÄ‚îÄ hooks/                  # 6 hooks for automation\n‚îÇ   ‚îú‚îÄ‚îÄ skill-activation-prompt.*  (ESSENTIAL)\n‚îÇ   ‚îú‚îÄ‚îÄ post-tool-use-tracker.sh   (ESSENTIAL)\n‚îÇ   ‚îú‚îÄ‚îÄ tsc-check.sh        (optional, needs customization)\n‚îÇ   ‚îî‚îÄ‚îÄ trigger-build-resolver.sh  (optional)\n‚îú‚îÄ‚îÄ agents/                 # 10 specialized agents\n‚îÇ   ‚îú‚îÄ‚îÄ code-architecture-reviewer.md\n‚îÇ   ‚îú‚îÄ‚îÄ refactor-planner.md\n‚îÇ   ‚îú‚îÄ‚îÄ frontend-error-fixer.md\n‚îÇ   ‚îî‚îÄ‚îÄ ... 7 more\n‚îî‚îÄ‚îÄ commands/               # 3 slash commands\n    ‚îú‚îÄ‚îÄ dev-docs.md\n    ‚îî‚îÄ‚îÄ ...\n\ndev/\n‚îî‚îÄ‚îÄ active/                 # Dev docs pattern examples\n    ‚îî‚îÄ‚îÄ public-infrastructure-repo/\n```\n\n---\n\n## Component Catalog\n\n### üé® Skills (5)\n\n| Skill | Lines | Purpose | Best For |\n|-------|-------|---------|----------|\n| [**skill-developer**](.claude/skills/skill-developer/) | 426 | Creating and managing skills | Meta-development |\n| [**backend-dev-guidelines**](.claude/skills/backend-dev-guidelines/) | 304 | Express/Prisma/Sentry patterns | Backend APIs |\n| [**frontend-dev-guidelines**](.claude/skills/frontend-dev-guidelines/) | 398 | React/MUI v7/TypeScript | React frontends |\n| [**route-tester**](.claude/skills/route-tester/) | 389 | Testing authenticated routes | API testing |\n| [**error-tracking**](.claude/skills/error-tracking/) | ~250 | Sentry integration | Error monitoring |\n\n**All skills follow the modular pattern** - main",
      "default_branch": "main"
    },
    "fetched_at": "2025-11-22T02:22:03.824416"
  },
  {
    "basic_info": {
      "name": "espectre",
      "full_name": "francescopace/espectre",
      "owner": "francescopace",
      "description": "üõú ESPectre üëª  - Motion detection system based on Wi-Fi spectre analysis (CSI), with Home Assistant integration.",
      "url": "https://github.com/francescopace/espectre",
      "clone_url": "https://github.com/francescopace/espectre.git",
      "ssh_url": "git@github.com:francescopace/espectre.git",
      "homepage": "",
      "created_at": "2025-10-26T10:41:51Z",
      "updated_at": "2025-11-22T01:49:18Z",
      "pushed_at": "2025-11-22T01:56:03Z"
    },
    "stats": {
      "stars": 3186,
      "forks": 210,
      "watchers": 3186,
      "open_issues": 4,
      "size": 10293
    },
    "tech_info": {
      "language": "C",
      "languages": {
        "C": 2425358,
        "HTML": 96181,
        "Python": 53881,
        "C++": 42172,
        "Shell": 16686,
        "CMake": 2084
      },
      "license": "GNU General Public License v3.0",
      "topics": [
        "csi",
        "diy",
        "esp-32",
        "espectre",
        "home-assistant",
        "motion-detection",
        "wifi",
        "wifi-sensing"
      ]
    },
    "content": {
      "readme": "[![License](https://img.shields.io/badge/license-GPLv3-blue.svg)](https://github.com/francescopace/espectre/blob/main/LICENSE)\n[![C](https://img.shields.io/badge/C-ESP--IDF-orange.svg)](https://github.com/espressif/esp-idf)\n[![Platform](https://img.shields.io/badge/platform-ESP32--S3%20%7C%20ESP32--C6-red.svg)](https://www.espressif.com/en/products/socs)\n[![Status](https://img.shields.io/badge/status-experimental-orange.svg)](https://github.com/francescopace/espectre)\n[![Changelog](https://img.shields.io/badge/changelog-v1.2.0-blue.svg)](https://github.com/francescopace/espectre/blob/main/CHANGELOG.md)\n\n# üõú ESPectre üëª\n\n**Motion detection system based on Wi-Fi spectre analysis (CSI), with Home Assistant integration.**\n\n**üì∞ Featured Article**: Read the complete story behind ESPectre on Medium **[üáÆüáπ Italian](https://medium.com/@francesco.pace/come-ho-trasformato-il-mio-wi-fi-in-un-sensore-di-movimento-40053fd83128?source=friends_link&sk=46d9cfa026790ae807ecc291ac5eac67&utm_source=github&utm_medium=readme&utm_campaign=espectre)**, **[üá¨üáß English](https://medium.com/@francesco.pace/how-i-turned-my-wi-fi-into-a-motion-sensor-61a631a9b4ec?sk=c7f79130d78b0545fce4a228a6a79af3&utm_source=github&utm_medium=readme&utm_campaign=espectre)**\n\n**‚ö†Ô∏è Disclaimer**: This is an experimental project for educational and research purposes. The author assumes no responsibility for misuse or damage resulting from the use of this system. Use responsibly and in compliance with applicable laws.\n\n---\n\n## üìë Table of Contents\n\n- [In 3 Points](#-in-3-points)\n- [Mathematical Approach](#-mathematical-approach)\n- [What You Need](#-what-you-need)\n- [Quick Start](#-quick-start)\n- [How It Works](#-how-it-works-simple-version)\n- [What You Can Do With It](#-what-you-can-do-with-it)\n- [Sensor Placement Guide](#-where-to-place-the-sensor)\n- [System Architecture](#Ô∏è-system-architecture)\n- [FAQ](#-faq-for-beginners)\n- [Security and Privacy](#-security-and-privacy)\n- [Technical Deep Dive](#-technical-deep-dive)\n- [Future Evolutions](#-future-evolutions-ai-approach)\n- [References](#-references)\n- [Changelog](#-changelog)\n- [License](#-license)\n- [Author](#-author)\n\n---\n\n## üéØ In 3 Points\n\n1. **What it does**: Detects movement using Wi-Fi (no cameras, no microphones)\n2. **What you need**: A ~‚Ç¨10 device (ESP32-S3 or ESP32-C6) + Home Assistant or MQTT server\n3. **Setup time**: 30-45 minutes (first time, including ESP-IDF setup)\n\n---\n\n## üî¨ Mathematical Approach\n\n**This project currently does NOT use Machine Learning models.** Instead, it employs a **mathematical approach** that extracts **10 features** from CSI (Channel State Information) data using statistical and signal processing techniques.\n\n### Key Points\n\n- ‚úÖ **No ML training required**: Works out-of-the-box with mathematical algorithms\n- ‚úÖ **10 extracted features**: Statistical, spatial, and temporal features\n- ‚úÖ **Real-time processing**: Low latency detection on ESP32 hardware (S3/C6)\n- ‚úÖ **Foundation for ML**: These features can serve as the basis for collecting labeled datasets to train ML models for advanced tasks (people counting, activity recognition, gesture detection)\n\nThe mathematical approach provides excellent movement detection without the complexity of ML model training, while the extracted features offer a solid foundation for future ML-based enhancements.\n\n---\n\n## üõí What You Need\n\n### Hardware\n\n- ‚úÖ **2.4GHz Wi-Fi Router** - the one you already have at home works fine\n- ‚úÖ **ESP32-S3 or ESP32-C6** - Available on Amazon, AliExpress, or electronics stores\n\nüìñ See [ESP32-PLATFORM-SUPPORT.md](ESP32-PLATFORM-SUPPORT.md) for detailed platform comparison and recommendations\n\n![3 x ESP32-S3 DevKit bundle with external antennas](images/home_lab.jpg)\n*ESP32-S3 DevKit with external antennas*\n\n### Software (All Free)\n\n- ‚úÖ **MQTT Broker** (required for operation):\n  - **Home Assistant** with built-in MQTT broker (on Raspberry Pi, PC, NAS, or cloud)\n  - OR standalone **Mosquitto** MQTT server (can run on any device, including Raspberry Pi)\n- ‚úÖ **ESP-IDF v6.1** (development framework for building firmware)\n\n### Required Skills\n\n- ‚úÖ **Basic command line knowledge** required for building and flashing firmware\n- ‚ùå **NO** router configuration needed\n\n---\n\n## üöÄ Quick Start\n\n**Setup time**: ~30-45 minutes (first time)  \n**Difficulty**: Intermediate (requires ESP-IDF setup)\n\n1. **Setup & Installation**: Follow the complete guide in [SETUP.md](SETUP.md)\n2. **Calibration & Tuning**: Optimize for your environment with [CALIBRATION.md](CALIBRATION.md)\n\n### Web-Based Monitor\n\nESPectre includes a web-based monitoring interface (`espectre-monitor.html`) for real-time visualization and configuration without command line tools.\n\n![Web Monitor Interface](images/web_monitor_chart.png)\n*Real-time CSI monitoring and configuration interface*\n\n---\n\n## üìñ How It Works (Simple Version)\n\nWhen someone moves in a room, they \"disturb\" the Wi-Fi waves traveling between the router and the sensor. It's like when you move your hand in fr",
      "default_branch": "main"
    },
    "fetched_at": "2025-11-22T02:22:05.115563"
  },
  {
    "basic_info": {
      "name": "Depth-Anything-3",
      "full_name": "ByteDance-Seed/Depth-Anything-3",
      "owner": "ByteDance-Seed",
      "description": "Depth Anything 3",
      "url": "https://github.com/ByteDance-Seed/Depth-Anything-3",
      "clone_url": "https://github.com/ByteDance-Seed/Depth-Anything-3.git",
      "ssh_url": "git@github.com:ByteDance-Seed/Depth-Anything-3.git",
      "homepage": "https://depth-anything-3.github.io/",
      "created_at": "2025-11-12T08:44:03Z",
      "updated_at": "2025-11-22T01:52:00Z",
      "pushed_at": "2025-11-20T18:04:24Z"
    },
    "stats": {
      "stars": 2518,
      "forks": 168,
      "watchers": 2518,
      "open_issues": 55,
      "size": 23036
    },
    "tech_info": {
      "language": "Jupyter Notebook",
      "languages": {
        "Jupyter Notebook": 650520,
        "Python": 618306
      },
      "license": "Apache License 2.0",
      "topics": []
    },
    "content": {
      "readme": "<div align=\"center\">\n<h1 style=\"border-bottom: none; margin-bottom: 0px \">Depth Anything 3: Recovering the Visual Space from Any Views</h1>\n<!-- <h2 style=\"border-top: none; margin-top: 3px;\">Recovering the Visual Space from Any Views</h2> -->\n\n\n[**Haotong Lin**](https://haotongl.github.io/)<sup>&ast;</sup> ¬∑ [**Sili Chen**](https://github.com/SiliChen321)<sup>&ast;</sup> ¬∑ [**Jun Hao Liew**](https://liewjunhao.github.io/)<sup>&ast;</sup> ¬∑ [**Donny Y. Chen**](https://donydchen.github.io)<sup>&ast;</sup> ¬∑ [**Zhenyu Li**](https://zhyever.github.io/) ¬∑ [**Guang Shi**](https://scholar.google.com/citations?user=MjXxWbUAAAAJ&hl=en) ¬∑ [**Jiashi Feng**](https://scholar.google.com.sg/citations?user=Q8iay0gAAAAJ&hl=en)\n<br>\n[**Bingyi Kang**](https://bingykang.github.io/)<sup>&ast;&dagger;</sup>\n\n&dagger;project lead&emsp;&ast;Equal Contribution\n\n<a href=\"https://arxiv.org/abs/2511.10647\"><img src='https://img.shields.io/badge/arXiv-Depth Anything 3-red' alt='Paper PDF'></a>\n<a href='https://depth-anything-3.github.io'><img src='https://img.shields.io/badge/Project_Page-Depth Anything 3-green' alt='Project Page'></a>\n<a href='https://huggingface.co/spaces/depth-anything/Depth-Anything-3'><img src='https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Demo-blue'></a>\n<!-- <a href='https://huggingface.co/datasets/depth-anything/VGB'><img src='https://img.shields.io/badge/Benchmark-VisGeo-yellow' alt='Benchmark'></a> -->\n<!-- <a href='https://huggingface.co/datasets/depth-anything/data'><img src='https://img.shields.io/badge/Benchmark-xxx-yellow' alt='Data'></a> -->\n\n</div>\n\nThis work presents **Depth Anything 3 (DA3)**, a model that predicts spatially consistent geometry from\narbitrary visual inputs, with or without known camera poses.\nIn pursuit of minimal modeling, DA3 yields two key insights:\n- üíé A **single plain transformer** (e.g., vanilla DINO encoder) is sufficient as a backbone without architectural specialization,\n- ‚ú® A singular **depth-ray representation** obviates the need for complex multi-task learning.\n\nüèÜ DA3 significantly outperforms\n[DA2](https://github.com/DepthAnything/Depth-Anything-V2) for monocular depth estimation,\nand [VGGT](https://github.com/facebookresearch/vggt) for multi-view depth estimation and pose estimation.\nAll models are trained exclusively on **public academic datasets**.\n\n<!-- <p align=\"center\">\n  <img src=\"assets/images/da3_teaser.png\" alt=\"Depth Anything 3\" width=\"100%\">\n</p> -->\n<p align=\"center\">\n  <img src=\"assets/images/demo320-2.gif\" alt=\"Depth Anything 3 - Left\" width=\"70%\">\n</p>\n<p align=\"center\">\n  <img src=\"assets/images/da3_radar.png\" alt=\"Depth Anything 3\" width=\"100%\">\n</p>\n\n\n## üì∞ News\n- **2025-11-14:** üéâ Paper, project page, code and models are all released.\n\n## ‚ú® Highlights\n\n### üèÜ Model Zoo\nWe release three series of models, each tailored for specific use cases in visual geometry.\n\n- üåü **DA3 Main Series** (`DA3-Giant`, `DA3-Large`, `DA3-Base`, `DA3-Small`) These are our flagship foundation models, trained with a unified depth-ray representation. By varying the input configuration, a single model can perform a wide range of tasks:\n  + üåä **Monocular Depth Estimation**: Predicts a depth map from a single RGB image.\n  + üåä **Multi-View Depth Estimation**: Generates consistent depth maps from multiple images for high-quality fusion.\n  + üéØ **Pose-Conditioned Depth Estimation**: Achieves superior depth consistency when camera poses are provided as input.\n  + üì∑ **Camera Pose Estimation**:  Estimates camera extrinsics and intrinsics from one or more images.\n  + üü° **3D Gaussian Estimation**: Directly predicts 3D Gaussians, enabling high-fidelity novel view synthesis.\n\n- üìê **DA3 Metric Series** (`DA3Metric-Large`) A specialized model fine-tuned for metric depth estimation in monocular settings, ideal for applications requiring real-world scale.\n\n- üîç **DA3 Monocular Series** (`DA3Mono-Large`). A dedicated model for high-quality relative monocular depth estimation. Unlike disparity-based models (e.g.,  [Depth Anything 2](https://github.com/DepthAnything/Depth-Anything-V2)), it directly predicts depth, resulting in superior geometric accuracy.\n\nüîó Leveraging these available models, we developed a **nested series** (`DA3Nested-Giant-Large`). This series combines a any-view giant model with a metric model to reconstruct visual geometry at a real-world metric scale.\n\n### üõ†Ô∏è Codebase Features\nOur repository is designed to be a powerful and user-friendly toolkit for both practical application and future research.\n- üé® **Interactive Web UI & Gallery**: Visualize model outputs and compare results with an easy-to-use Gradio-based web interface.\n- ‚ö° **Flexible Command-Line Interface (CLI)**: Powerful and scriptable CLI for batch processing and integration into custom workflows.\n- üíæ **Multiple Export Formats**: Save your results in various formats, including `glb`, `npz`, depth images, `ply`, 3DGS videos, etc, to seamlessly connect with other tools.\n- üîß **Extensible and Modular Design**:",
      "default_branch": "main"
    },
    "fetched_at": "2025-11-22T02:22:06.399680"
  },
  {
    "basic_info": {
      "name": "react-native-godot",
      "full_name": "borndotcom/react-native-godot",
      "owner": "borndotcom",
      "description": "React Native Godot - Embed Godot Engine in React Native apps",
      "url": "https://github.com/borndotcom/react-native-godot",
      "clone_url": "https://github.com/borndotcom/react-native-godot.git",
      "ssh_url": "git@github.com:borndotcom/react-native-godot.git",
      "homepage": "",
      "created_at": "2025-11-01T10:54:51Z",
      "updated_at": "2025-11-21T20:00:36Z",
      "pushed_at": "2025-11-07T13:56:23Z"
    },
    "stats": {
      "stars": 2419,
      "forks": 100,
      "watchers": 2419,
      "open_issues": 7,
      "size": 43456
    },
    "tech_info": {
      "language": "C++",
      "languages": {
        "C++": 96704,
        "Objective-C++": 44758,
        "TypeScript": 31195,
        "Java": 29411,
        "JavaScript": 13042,
        "C": 6699,
        "Ruby": 4547,
        "Objective-C": 4216,
        "Python": 4120,
        "Shell": 3800,
        "GDScript": 3456,
        "Kotlin": 2881,
        "CMake": 2267,
        "Swift": 1232
      },
      "license": "MIT License",
      "topics": [
        "android",
        "godot",
        "godot-engine",
        "ios",
        "react-native"
      ]
    },
    "content": {
      "readme": "![Cover-21](https://github.com/user-attachments/assets/770e4972-84f7-433e-87db-6391601256ba)\nBorn React Native Godot\n-----------------------\n\nReact Native Godot allows embedding the Godot Engine into React Native applications.\n\nBorn React Native Godot was created by [Born](https://born.com) and developed by [Migeran](https://migeran.com), in close collaboration between the two teams.\n\n# Main Features\n\n* Supports Android and iOS, built on [LibGodot](https://github.com/migeran/libgodot).\n* Stable implementation serving millions of users in [Born](https://born.com)'s applications.\n* Supports starting, stopping and restarting the Godot Engine. [(docs)](#initialize-the-godot-instance)\n* When restarting, the engine can be reconfigured, so a different Godot app may be loaded each time. [(docs)](#stop-the-godot-instance)\n* It is also possible to pause and resume the running Godot instance. [(docs)](#pause-the-godot-instance)\n* Godot is running on a separate thread, so it does not affect the main thread of the application nor the React Native JavaScript thread. [(docs)](#threading-and-javascript-in-react-native)\n* The Godot main window and any subwindows created by the Godot app may be embedded into the React Native application either on the same screen, or on separate screens (see [example app](example/)).\n* The whole Godot API is accessible from TypeScript / JavaScript. It is possible to instantiate objects, call methods, get and set properties, attach JS functions to signals, provide JS functions as callables to Godot methods ... etc. [(docs)](#godot-api-usage)\n\n<p align=\"center\">\n  <video src=\"https://github.com/user-attachments/assets/33266f05-d733-4c1d-ab49-edaaf426e3e1\" width=\"600\" controls></video>\n</p>\n\n# Getting Started with the Example App\n\nThe [example app](example/) shows the main features of React Native Godot in action.\n\n## Install Prerequisites\n\nDuring development we use [ASDF](https://asdf-vm.com/) to manage most external dependencies required for React Native development, like Node, Java, Gradle or Ruby. If you also use ASDF, just run:\n\n```sh\nasdf install\n```\n    \nThis will make sure that all the dependencies are the same like in our environment. Otherwise you may also install React Native prerequisites using any other method.\n\n## Export the Godot app\n\nRun the following scripts for either platform you plan to test (or both):\n\n```sh\ncd example\n./export_godot_GodotTest.sh android\n./export_godot_GodotTest.sh ios\n./export_godot_GodotTest2.sh android\n./export_godot_GodotTest2.sh ios\n```\n\nThe script is configured to look for Godot in the standard system wide installation folder on macOS. If your Godot is installed elsewhere, or you are on Linux, just point the `GODOT_EDITOR`\nenvironment variable to your Godot editor prior to running the above scripts:\n\n```sh\nexport GODOT_EDITOR=/path/to/godot_editor\n```\n\n## Configure and download LibGodot\n\n```sh\ncd example\nyarn\nyarn download-prebuilt\n```\n\nThese commands will resolve all the React Native and other dependencies from npm. The second one will download the prebuilt LibGodot release from GitHub.\n\n## Run on the iOS Simulator\n\n```sh\ncd example/ios\nbundle install\nbundle exec pod install\ncd ..\nyarn ios\n```\n\n## Run on the Android Emulator\n\n```sh\ncd example\nyarn android\n```\n\n## Use your native IDEs\n\nYou may use Xcode and Android Studio the same way as with any other project. Just open:\n\n* ``ios/GodotTest.xcworkspace`` from Xcode\n* ``android`` from Android Studio\n\n> [!note]\n> If you are using ASDF to manage your Java and Node dependencies, you should start Android Studio from under the `react-native-godot` (or `example`) folder, so it can find these tools. For example on macOS:\n\n```sh\ncd example\nopen -a \"Android Studio\"\n```\n\n## Convenience script for dependency management\n\nThere is an `update_deps.sh` script included in the example app's folder. It will execute all the setup commands for both iOS and Android in one step, so you may start your work immediately.\n\n```sh\ncd example\n./update_deps.sh\nyarn ios # or yarn android\n```\n\n# Your first React Native Godot App\n\nBorn React Native Godot is distributed on npm.\n\nJust follow these steps to add it to your React Native application:\n\n## Update `package.json`\n\n```sh\nyarn add @borndotcom/react-native-godot\n```\n\n## Download the prebuilt LibGodot packages\n\nThe LibGodot packages used by React Native Godot are not distributed on npm. Instead, they are downloaded separately by issuing the following command:\n\n```sh\nyarn download-prebuilt\n```\n\nThis way React Native Godot can be updated independently from LibGodot, and also local, customized builds of LibGodot are supported.\n\n## Import React Native Godot in your App code\n\n```typescript\nimport { RTNGodot, RTNGodotView, runOnGodotThread } from \"@borndotcom/react-native-godot\";\n```\n\n## Add the Godot View to your view, e.g.\n\n```tsx\nconst App = () => {\n  return (\n    <View>\n      <RTNGodotView style={...}/>\n    </View>\n  );\n};\n```\n\nIf no `windowName` property is specified, that view is fo",
      "default_branch": "master"
    },
    "fetched_at": "2025-11-22T02:22:07.701480"
  },
  {
    "basic_info": {
      "name": "misaka26",
      "full_name": "straight-tamago/misaka26",
      "owner": "straight-tamago",
      "description": "iOS /iPadOS 16.0 - 26.1, An ultimate customization tool, uilitizing the bug that makes TrollRestore possible. ",
      "url": "https://github.com/straight-tamago/misaka26",
      "clone_url": "https://github.com/straight-tamago/misaka26.git",
      "ssh_url": "git@github.com:straight-tamago/misaka26.git",
      "homepage": "",
      "created_at": "2025-11-16T14:48:46Z",
      "updated_at": "2025-11-22T02:08:34Z",
      "pushed_at": "2025-11-19T13:53:41Z"
    },
    "stats": {
      "stars": 2166,
      "forks": 85,
      "watchers": 2166,
      "open_issues": 203,
      "size": 21
    },
    "tech_info": {
      "language": null,
      "languages": {},
      "license": null,
      "topics": []
    },
    "content": {
      "readme": "# misaka26\nAn ultimate customization tool, uilitizing the bug that makes [TrollRestore](https://github.com/JJTech0130/TrollRestore) possible.\n### iOS /iPadOS 16.0 - 26.1 & 26.2 beta 1\n**Supported iOS 16.0 ~ 26.1 & 26.2 beta 1**\n\n> [!NOTE]\n> **Use this tool at your own risk. There is a chance you may bootloop, so create a backup before using.**\n\nDevelopers: [@34306](https://github.com/34306) [@straight-tamago](https://github.com/straight-tamago)\n\n<a href=\"https://github.com/straight-tamago/misaka26/releases/latest\"><img src=\"https://img.shields.io/github/v/release/straight-tamago/misaka26?color=d774d5\" /></a>\n<a href=\"https://github.com/straight-tamago/misaka26/releases\"><img src=\"https://img.shields.io/github/downloads/straight-tamago/misaka26/total?color=d774d5\" /></a>\n\n## Join Discord Support üçâ \n‚Ä¢ Misaka Support ‚ú® **(Sever 1)**:   \n<a href='https://discord.gg/KSExeZVAGX'><img align='center' alt='Discord' src='https://img.shields.io/discord/1156843198799421490?color=36309d&label=DISCORD&logo=discord&logoColor=white&style=for-the-badge'></a>  \n‚Ä¢ Misaka Support ‚ú® **(Sever 2)**:  \n<a href='https://discord.gg/mVrPxY3X6W'><img align='center' alt='Discord' src='https://img.shields.io/discord/1074625970029477919?color=36309d&label=DISCORD&logo=discord&logoColor=white&style=for-the-badge'></a>   \n\n### Installation:\nTo fix permission issues on macOS, run:\n```bash\nxattr -c /path/to/misaka26.app\n```\n\n## How to Use\n\nDownload lastest misaka26 on Release tab, place it to `/Applications`\nWhen open it, go to System Settings > Privacy and Security > Security > Allow running this Application > Open anyway\n\n1. Generate your MobileGestalt by using this [Shortcut](https://routinehub.co/shortcut/23246/#/login)\n2. AirDrop or send it to macOS\n3. Select it in misaka26 application\n4. Enable features\n5. Apply\n6. Profit?\n\n## Supported Features\n- **TrollPad (MultiTasking)** (NEW, require macOS)\n(iOS 18.0+) (need respring, read below)\n- **Enable PWM (iOS 26.0+)**\n- **Enable Security Research Device (SRD) mode (iOS 26.0+)**\n- **Allow Install M chip/Pro chip games on AppStore (eg: RE4) (iOS 26.0+)**\n\n\n- **TrollStore Installer (iOS 15.2 ~ 16.7RC (20H18) & 17.0)**\n- **Dynamic Island** (iOS 16.0+)\n- **Charge Limit** (iOS 17.0+)\n- **Boot Chime** (iOS 17.0+)\n- **Stage Manager** (iOS 16.0+)\n- **Shutter Sound** (iOS 16.0+)  \nPlease do not use camera silence for the purpose of voyeurism. For photographing pets, etc.\n- **Always-on Display (AoD)** (iOS 18.0+)\n- **Apple Pencil** (iOS 18.0+)\n- **Action Button** (iOS 17.0+)\n- **Internal Storage** (iOS 17.0+)\n- **Clock UI** (iOS 18.0+)\n- **SOS Collision** (iOS 18.0+)\n- **TapToWake** (iPhone SE 2/3, iOS 18.0+)\n- **Apple Intelligence** (iOS 18.1 Beta, ALL DEVICES ON 18.1)\n- **Landscape FaceID** (iOS 17.0+)\n- **Old Photo UI** (iOS 18.0+)\n- **iPad Apps Support** (iOS 16.0+)\n- **Developer Mode & Metal HUD** (iOS 16.0+)\n- **CameraControl** (18.0+)\n- **AoD Vibrancy** (may affect others tweak, iOS 18.0+)\n- **Sleep apnea** (iOS 18.0+)\n- **Find My Friend (KH/A devices)**\n\n**Some upcoming features:** ‚ù§Ô∏è\n- **Disable call record greetings**\n- **Allow modify custom path**\n\n\n**You will need to do a respring to take effect, download this [ipa](https://github.com/34306/mdc0/releases/download/1.0/respringapp.ipa) to respring**\n\n## icon\n\n## Credit\n- [Duy Tran](https://github.com/khanhduytran0) for the exploit writeup\n- [hanakim3945](https://github.com/hanakim3945)\n- pengubow for those new flags\n\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-11-22T02:22:08.952034"
  },
  {
    "basic_info": {
      "name": "omnilingual-asr",
      "full_name": "facebookresearch/omnilingual-asr",
      "owner": "facebookresearch",
      "description": "Omnilingual ASR Open-Source Multilingual SpeechRecognition for 1600+ Languages",
      "url": "https://github.com/facebookresearch/omnilingual-asr",
      "clone_url": "https://github.com/facebookresearch/omnilingual-asr.git",
      "ssh_url": "git@github.com:facebookresearch/omnilingual-asr.git",
      "homepage": null,
      "created_at": "2025-11-06T22:38:00Z",
      "updated_at": "2025-11-22T01:22:04Z",
      "pushed_at": "2025-11-19T18:07:58Z"
    },
    "stats": {
      "stars": 2157,
      "forks": 177,
      "watchers": 2157,
      "open_issues": 15,
      "size": 1026
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 295799
      },
      "license": "Other",
      "topics": []
    },
    "content": {
      "readme": "<div align=\"center\">\n  <img src=\"./omniASR_header.jpg\" alt=\"Header image with a collage of on-the-ground photos from the transcription gathering efforts in Pakistan and Liberia.\" width=\"100%\" />\n  <p><i>Photographs captured during corpus creation efforts in Pakistan and Liberia.</i></p>\n</div>\n\n# Omnilingual ASR: Open-Source Multilingual Speech Recognition for 1600+ Languages\n\nOmnilingual ASR is an open-source speech recognition system supporting over 1,600 languages ‚Äî including hundreds never previously covered by any ASR technology. Designed for broad accessibility, it enables new languages to be added with just a few paired examples without requiring specialized expertise or large datasets. By combining scalable zero-shot learning with a flexible model family, Omnilingual ASR aims to make speech technology more inclusive and adaptable for communities and researchers worldwide.\n\n* [Huggingface Demo](https://huggingface.co/spaces/facebook/omniasr-transcriptions)\n* [Huggingface Dataset](https://huggingface.co/datasets/facebook/omnilingual-asr-corpus)\n* [Paper](https://ai.meta.com/research/publications/omnilingual-asr-open-source-multilingual-speech-recognition-for-1600-languages/)\n* [Blogpost](http://ai.meta.com/blog/omnilingual-asr-advancing-automatic-speech-recognition)\n\n<div align=\"center\">\n  <img src=\"./result_table.png\" alt=\"Performance results table\" width=\"100%\" />\n  <p><i>Our 7B-LLM-ASR system achieves state-of-the-art performance across 1,600+ languages, with character error rates (CER) below 10 for 78% of those languages.</i></p>\n</div>\n\n\n## Documentation\n\n### Quick Start\n- **[Installation & Basic Usage](#installation)** - Setup and first transcription\n- **[Inference Pipeline](src/omnilingual_asr/models/inference/README.md)** - Comprehensive transcription guide with batch processing, language conditioning, and context examples\n- **[Supported Languages](#supported-languages)** - View the complete list of 1600+ supported languages\n\n\n### Models & Architecture\n- **[Model Specifications](#model-architectures)** - Available models, parameters, and memory requirements\n- **[Architecture Overview](src/omnilingual_asr/models/README.md)** - Technical details on W2V, CTC, and LLM model families\n- **[Asset Management](src/omnilingual_asr/cards/README.md)** - Configuration system for models, tokenizers, and datasets\n\n### Training & Data Pipeline\n- **[Data Preparation](workflows/dataprep/README.md)** - End-to-end guide for multilingual dataset preparation, HuggingFace integration, and parquet processing\n- **[Training Recipes](workflows/recipes/wav2vec2/asr/README.md)** - Pre-configured workflows for CTC and LLM model training\n\n---\n\n## Installation\n\nThe models were developed using [fairseq2](https://github.com/facebookresearch/fairseq2), a research-focused sequence modeling toolkit. While we provide a **reference** inference pipeline that works across platforms, audio support requires [libsndfile](https://github.com/facebookresearch/fairseq2?tab=readme-ov-file#system-dependencies) (Mac: `brew install libsndfile`; Windows may need an additional [setup](https://github.com/facebookresearch/fairseq2?tab=readme-ov-file#installing-on-windows)).\n\n```bash\n# using pip\npip install omnilingual-asr\n\n# using uv\nuv add omnilingual-asr\n```\n\n## Inference\n\n```python\nfrom omnilingual_asr.models.inference.pipeline import ASRInferencePipeline\n\npipeline = ASRInferencePipeline(model_card=\"omniASR_LLM_7B\")\n\naudio_files = [\"/path/to/eng_audio1.flac\", \"/path/to/deu_audio2.wav\"]\nlang = [\"eng_Latn\", \"deu_Latn\"]\ntranscriptions = pipeline.transcribe(audio_files, lang=lang, batch_size=2)\n```\n\nMore details on running specific models can be found in the [src/omnilingual_asr/models/inference](/src/omnilingual_asr/models/inference/README.md) directory.\n\n> **‚ö†Ô∏è Important:** Currently only audio files shorter than 40 seconds are accepted for inference. We plan to add support for transcribing unlimited-length audio files shortly.\n\n### Supported Languages\n\nTo view the full list of 1600+ supported languages, you can access the language list [programmatically](/src/omnilingual_asr/models/wav2vec2_llama/lang_ids.py):\n\n```python\nfrom omnilingual_asr.models.wav2vec2_llama.lang_ids import supported_langs\n\n# Print all supported languages\nprint(f\"Total supported languages: {len(supported_langs)}\")\nprint(supported_langs)\n\n# Check if a specific language is supported\nif \"eng_Latn\" in supported_langs:\n    print(\"English (Latin script) is supported!\")\n```\n\nLanguages follow the format `{language_code}_{script}`, for example `eng_Latn` - English (Latin script), `cmn_Hans` - Mandarin Chinese (Simplified), ...\n\n### Using the HuggingFace Dataset ü§ó\n\nWe provide a large-scale multilingual speech dataset on HuggingFace under CC-BY-4.0 License: [`facebook/omnilingual-asr-corpus`](https://huggingface.co/datasets/facebook/omnilingual-asr-corpus).\nThis dataset can be directly used with our inference pipeline for evaluation or testing:\n\n```bash\npip install \"omnilingual-asr[dat",
      "default_branch": "main"
    },
    "fetched_at": "2025-11-22T02:22:10.215714"
  },
  {
    "basic_info": {
      "name": "deepseek-ocr.rs",
      "full_name": "TimmyOVO/deepseek-ocr.rs",
      "owner": "TimmyOVO",
      "description": "Rust multi‚Äëbackend OCR/VLM engine (DeepSeek‚ÄëOCR, PaddleOCR‚ÄëVL, DotsOCR) with DSQ quantization and an OpenAI‚Äëcompatible server & CLI ‚Äì run locally without Python.",
      "url": "https://github.com/TimmyOVO/deepseek-ocr.rs",
      "clone_url": "https://github.com/TimmyOVO/deepseek-ocr.rs.git",
      "ssh_url": "git@github.com:TimmyOVO/deepseek-ocr.rs.git",
      "homepage": "",
      "created_at": "2025-10-25T13:42:10Z",
      "updated_at": "2025-11-21T21:13:37Z",
      "pushed_at": "2025-11-17T17:00:58Z"
    },
    "stats": {
      "stars": 1942,
      "forks": 148,
      "watchers": 1942,
      "open_issues": 8,
      "size": 1295
    },
    "tech_info": {
      "language": "Rust",
      "languages": {
        "Rust": 941644,
        "Python": 62963,
        "Dockerfile": 1057
      },
      "license": "Apache License 2.0",
      "topics": [
        "candle",
        "ocr",
        "ocr-recognition",
        "openai",
        "rust"
      ]
    },
    "content": {
      "readme": "# deepseek-ocr.rs üöÄ\n\nRust implementation of the DeepSeek-OCR inference stack with a fast CLI and an OpenAI-compatible HTTP server. The workspace packages multiple OCR backends, prompt tooling, and a serving layer so you can build document understanding pipelines that run locally on CPU, Apple Metal, or (alpha) NVIDIA CUDA GPUs.\n\n> ‰∏≠ÊñáÊñáÊ°£ËØ∑Áúã [README_CN.md](README_CN.md)„ÄÇ  \n\n> Want ready-made binaries? Latest macOS (Metal-enabled) and Windows bundles live in the [build-binaries workflow artifacts](https://github.com/TimmyOVO/deepseek-ocr.rs/actions/workflows/build-binaries.yml). Grab them from the newest green run.\n\n## Choosing a Model üî¨\n\n| Model | Memory footprint* | Best on | When to pick it |\n| --- | --- | --- | --- |\n| **DeepSeek‚ÄëOCR** | **‚âà6.3GB** FP16 weights, **‚âà13GB** RAM/VRAM with cache & activations (512-token budget) | Apple Silicon + Metal (FP16), high-VRAM NVIDIA GPUs, 32GB+ RAM desktops | Highest accuracy, SAM+CLIP global/local context, MoE DeepSeek‚ÄëV2 decoder (3B params, ~570M active per token). Use when latency is secondary to quality. |\n| **PaddleOCR‚ÄëVL** | **‚âà4.7GB** FP16 weights, **‚âà9GB** RAM/VRAM with cache & activations | 16GB laptops, CPU-only boxes, mid-range GPUs | Dense 0.9B Ernie decoder with SigLIP vision tower. Faster startup, lower memory, great for batch jobs or lightweight deployments. |\n| **DotsOCR** | **‚âà9GB** FP16 weights, but expect **30‚Äì50GB** RAM/VRAM for high-res docs due to huge vision tokens | Apple Silicon + Metal BF16, ‚â•24GB CUDA cards, or 64GB RAM CPU workstations | Unified VLM (DotsVision + Qwen2) that nails layout, reading order, grounding, and multilingual math if you can tolerate the latency and memory bill. |\n\n\\*Measured from the default FP16 safetensors. Runtime footprint varies with sequence length.\n\nGuidance:\n\n- **Need maximum fidelity, multi-region reasoning, or already have 16‚Äì24GB VRAM?** Use **DeepSeek‚ÄëOCR**. The hybrid SAM+CLIP tower plus DeepSeek‚ÄëV2 MoE decoder handles complex layouts best, but expect higher memory/latency.\n- **Deploying to CPU-only nodes, 16GB laptops, or latency-sensitive services?** Choose **PaddleOCR‚ÄëVL**. Its dense Ernie decoder (18 layers, hidden 1024) activates fewer parameters per token and keeps memory under 10GB while staying close in quality on most docs.\n- **Chasing reading-order accuracy, layout grounding, or multi-page multilingual PDFs on roomy hardware?** Pick **DotsOCR** with BF16 on Metal/CUDA. Prefill runs around 40‚Äì50 tok/s on M-series GPUs but can fall to ~12 tok/s on CPU because of the heavy vision tower.\n\n## Why Rust? üí°\n\nThe original DeepSeek-OCR ships as a Python + Transformers stack‚Äîpowerful, but hefty to deploy and awkward to embed. Rewriting the pipeline in Rust gives us:\n\n- Smaller deployable artifacts with zero Python runtime or conda baggage.\n- Memory-safe, thread-friendly infrastructure that blends into native Rust backends.\n- Unified tooling (CLI + server) running on Candle + Rocket without the Python GIL overhead.\n- Drop-in compatibility with OpenAI-style clients while tuned for single-turn OCR prompts.\n\n## Technical Stack ‚öôÔ∏è\n\n- **Candle** for tensor compute, with Metal and CUDA backends and FlashAttention support.\n- **Rocket** + async streaming for OpenAI-compatible `/v1/responses` and `/v1/chat/completions`.\n- **tokenizers** (upstream DeepSeek release) wrapped by `crates/assets` for deterministic caching via Hugging Face and ModelScope mirrors.\n- **Pure Rust vision/prompt pipeline** shared by CLI and server to avoid duplicated logic.\n\n## Advantages over the Python Release ü•∑\n\n- Faster cold-start on Apple Silicon, lower RSS, and native binary distribution.\n- Deterministic dual-source (Hugging Face + ModelScope) asset download + verification built into the workspace.\n- Automatic single-turn chat compaction so OCR outputs stay stable even when clients send history.\n- Ready-to-use OpenAI compatibility for tools like Open WebUI without adapters.\n\n## Highlights ‚ú®\n\n- **One repo, two entrypoints** ‚Äì a batteries-included CLI for batch jobs and a Rocket-based server that speaks `/v1/responses` and `/v1/chat/completions`.\n- **Works out of the box** ‚Äì pulls model weights, configs, and tokenizer from whichever of Hugging Face or ModelScope responds fastest on first run.\n- **Optimised for Apple Silicon** ‚Äì optional Metal backend with FP16 execution for real-time OCR on laptops.\n- **CUDA (alpha)** ‚Äì experimental support via `--features cuda` + `--device cuda --dtype f16`; expect rough edges while we finish kernel coverage.\n- **Intel MKL (preview)** ‚Äì faster BLAS on x86 via `--features mkl` (install Intel oneMKL beforehand).\n- **OpenAI client compatibility** ‚Äì drop-in replacement for popular SDKs; the server automatically collapses chat history to the latest user turn for OCR-friendly prompts.\n\n## Model Matrix üì¶\n\nThe workspace exposes three base model IDs plus DSQ-quantized variants for DeepSeek‚ÄëOCR, PaddleOCR‚ÄëVL, and DotsOCR:\n\n| Model ID | Base Model | Precision | Suggested Use Case |\n| --- | --- | --- | --- |\n| `deepsee",
      "default_branch": "master"
    },
    "fetched_at": "2025-11-22T02:22:11.529657"
  },
  {
    "basic_info": {
      "name": "smart-excalidraw-next",
      "full_name": "liujuntao123/smart-excalidraw-next",
      "owner": "liujuntao123",
      "description": "A smart, powerful, and beautiful excalidraw drawing tool.Draw Professional Charts with Natural Language",
      "url": "https://github.com/liujuntao123/smart-excalidraw-next",
      "clone_url": "https://github.com/liujuntao123/smart-excalidraw-next.git",
      "ssh_url": "git@github.com:liujuntao123/smart-excalidraw-next.git",
      "homepage": "https://smart-excalidraw.aizhi.site",
      "created_at": "2025-10-30T02:12:40Z",
      "updated_at": "2025-11-22T01:02:19Z",
      "pushed_at": "2025-11-20T07:23:20Z"
    },
    "stats": {
      "stars": 1872,
      "forks": 237,
      "watchers": 1872,
      "open_issues": 19,
      "size": 326
    },
    "tech_info": {
      "language": "JavaScript",
      "languages": {
        "JavaScript": 185429,
        "CSS": 471
      },
      "license": null,
      "topics": [
        "ai",
        "chart",
        "excalidraw"
      ]
    },
    "content": {
      "readme": "# Smart Excalidraw\n> **Áî®Ëá™ÁÑ∂ËØ≠Ë®ÄÔºåÁªòÂà∂‰∏ì‰∏öÂõæË°®**\n\n## Âú®Á∫øÁΩëÁ´ô\n**ÔºàÊé®ËçêÔºÅÔºâüöÄÂÖ®Èù¢ÂçáÁ∫ßÔºåÊõ¥Âº∫Â§ßÂ•ΩÁî®ÁöÑÁªòÂõæÂ∑•ÂÖ∑ https://smart-draw.aizhi.site/**\n\n\n\nüöÄüöÄüöÄ ÈôêÊó∂Á¶èÂà©\n\nÊ∑ªÂä†Â∫ïÈÉ®‰ΩúËÄÖÂæÆ‰ø°ËøõÁæ§ÂèØÈ¢ÜÂèñÂÖçË¥πclaude-4.5-sonnet key\n\n## English Version\nRead the English version: [README_EN.md](README_EN.md)\n\n## ÊïàÊûúÈ¢ÑËßà\nÊìç‰ΩúÁïåÈù¢\n<img width=\"2330\" height=\"1255\" alt=\"PixPin_2025-10-31_17-14-27\" src=\"https://github.com/user-attachments/assets/5319ad5c-c507-42e0-b67a-e9dfb2d7ecfa\" />\nÊäÄÊúØÊû∂ÊûÑÂõæ\n<img width=\"1920\" height=\"1134\" alt=\"Untitled-2025-11-03-1105\" src=\"https://github.com/user-attachments/assets/d2e01c4e-d300-4c20-bd98-d056e4f02102\" />\n‰ø°ÊÅØÂõæ\n<img width=\"2183\" height=\"828\" alt=\"Untitled-2025-11-03-1054\" src=\"https://github.com/user-attachments/assets/0e46e8da-fe64-40a9-911b-f6c0e5589bae\" />\n\n\n\n## ‚ú® Ê†∏ÂøÉÁâπÊÄß\n\n### üéØ AI È©±Âä®ÔºåÊïàÊûúÂá∫‰ºó\nÈÄöËøáÂÖàËøõÁöÑÂ§ßËØ≠Ë®ÄÊ®°ÂûãÁêÜËß£‰Ω†ÁöÑÈúÄÊ±ÇÔºåÁîüÊàêÁªìÊûÑÊ∏ÖÊô∞„ÄÅÂ∏ÉÂ±ÄÂêàÁêÜÁöÑ‰∏ì‰∏öÁ∫ßÂõæË°®„ÄÇ\n\n### üîó Áã¨ÂàõËøûÊé•ÁÆóÊ≥ï\nÈááÁî®Áã¨ÂàõÁöÑÊô∫ËÉΩÁÆ≠Â§¥‰ºòÂåñÁÆóÊ≥ïÔºåËá™Âä®ËÆ°ÁÆóÊúÄ‰Ω≥ËøûÊé•ÁÇπÔºåÁ°Æ‰øùÂõæË°®‰∫ïÁÑ∂ÊúâÂ∫è„ÄÅÈÄªËæëÊ∏ÖÊô∞ÔºåÂëäÂà´Ê∑∑‰π±ÁöÑÁ∫øÊù°‰∫§Âèâ„ÄÇ\n\n### üìä ‰∏∞ÂØåÂõæË°®Á±ªÂûã\nÊîØÊåÅ 20+ ÁßçÂõæË°®Á±ªÂûãÔºåÂåÖÊã¨ÊµÅÁ®ãÂõæ„ÄÅÊû∂ÊûÑÂõæ„ÄÅÊó∂Â∫èÂõæ„ÄÅER Âõæ„ÄÅÊÄùÁª¥ÂØºÂõæÁ≠â„ÄÇ‰πüÂèØ‰ª•ËÆ©AIÊ†πÊçÆ‰Ω†ÁöÑÊèèËø∞Ëá™Âä®ÈÄâÊã©ÊúÄÂêàÈÄÇÁöÑÂõæË°®Á±ªÂûã„ÄÇ\n\n### üé® ÂÆåÁæé Excalidraw ÈõÜÊàê\nÁîüÊàêÁöÑÂõæË°®ÂÆåÂÖ®Âü∫‰∫é Excalidraw Ê†ºÂºèÔºåÂèØ‰ª•Âú®ÁîªÂ∏É‰∏äËá™Áî±ÁºñËæë„ÄÅË∞ÉÊï¥Ê†∑Âºè„ÄÅÊ∑ªÂä†ÁªÜËäÇÔºåÂÆûÁé∞ AI ÁîüÊàê‰∏éÊâãÂä®Á≤æ‰øÆÁöÑÂÆåÁæéÁªìÂêà„ÄÇ\n\n### ‚ö° ÂºÄÁÆ±Âç≥Áî®\nÂè™ÈúÄÈÖçÁΩÆ‰∏Ä‰∏™ AI API ÂØÜÈí•Âç≥ÂèØÂºÄÂßã‰ΩøÁî®ÔºåÊó†ÈúÄÂ§çÊùÇÁöÑÁéØÂ¢ÉÊê≠Âª∫„ÄÇÊâÄÊúâÈÖçÁΩÆ‰øùÂ≠òÂú®Êú¨Âú∞ÊµèËßàÂô®ÔºåÈöêÁßÅÂÆâÂÖ®Êúâ‰øùÈöú„ÄÇ\n\n\n\n## üöÄ Âø´ÈÄüÂºÄÂßã\n\n### ÊñπÂºè‰∏ÄÔºö‰ΩøÁî®ËÆøÈóÆÂØÜÁ†Å\n\nÂ¶ÇÊûúÊúçÂä°Âô®ÁÆ°ÁêÜÂëòÂ∑≤ÈÖçÁΩÆËÆøÈóÆÂØÜÁ†ÅÔºå‰Ω†ÂèØ‰ª•Áõ¥Êé•‰ΩøÁî®ÊúçÂä°Âô®Á´ØÁöÑ LLM ÈÖçÁΩÆÔºåÊó†ÈúÄËá™Â∑±Êèê‰æõ API KeyÔºö\n\n1. ÁÇπÂáªÂè≥‰∏äËßíÁöÑ **\"ËÆøÈóÆÂØÜÁ†Å\"** ÊåâÈíÆ\n2. ËæìÂÖ•ÁÆ°ÁêÜÂëòÊèê‰æõÁöÑËÆøÈóÆÂØÜÁ†Å\n3. ÁÇπÂáª **\"È™åËØÅÂØÜÁ†Å\"** ÊµãËØïËøûÊé•\n4. ÂãæÈÄâ **\"ÂêØÁî®ËÆøÈóÆÂØÜÁ†Å\"** Âπ∂‰øùÂ≠ò\n\nÂêØÁî®ÂêéÔºåÂ∫îÁî®Â∞Ü‰ºòÂÖà‰ΩøÁî®ÊúçÂä°Âô®Á´ØÈÖçÁΩÆÔºå‰Ω†Êó†ÈúÄÈÖçÁΩÆËá™Â∑±ÁöÑ API Key Âç≥ÂèØÂºÄÂßãÂàõ‰ΩúÔºÅ\n\n### ÊñπÂºè‰∫åÔºöÈÖçÁΩÆËá™Â∑±ÁöÑ AI\n\n1. ÁÇπÂáªÂè≥‰∏äËßíÁöÑ **\"ÈÖçÁΩÆ LLM\"** ÊåâÈíÆ\n2. ÈÄâÊã©Êèê‰æõÂïÜÁ±ªÂûãÔºàOpenAI Êàñ AnthropicÔºâ\n3. Â°´ÂÖ•‰Ω†ÁöÑ API Key\n4. ÈÄâÊã©Ê®°ÂûãÔºà**Êé®Ëçê‰ΩøÁî® claude-sonnet-4.5**ÔºåÊïàÊûúÊúÄ‰Ω≥Ôºâ\n5. ‰øùÂ≠òÈÖçÁΩÆ\n\nÂ∞±Ëøô‰πàÁÆÄÂçïÔºÅÁé∞Âú®‰Ω†ÂèØ‰ª•ÂºÄÂßãÂàõ‰Ωú‰∫Ü„ÄÇ\n\n### Á¨¨‰∫åÊ≠•ÔºöÂàõÂª∫ÂõæË°®\n\nÂú®ËæìÂÖ•Ê°Ü‰∏≠Áî®Ëá™ÁÑ∂ËØ≠Ë®ÄÊèèËø∞‰Ω†ÁöÑÈúÄÊ±ÇÔºå‰æãÂ¶ÇÔºö\n- \"Áîª‰∏Ä‰∏™Áî®Êà∑ÁôªÂΩïÁöÑÊµÅÁ®ãÂõæ\"\n- \"ÂàõÂª∫‰∏Ä‰∏™ÂæÆÊúçÂä°Êû∂ÊûÑÂõæÔºåÂåÖÂê´ÁΩëÂÖ≥„ÄÅËÆ§ËØÅÊúçÂä°Âíå‰∏öÂä°ÊúçÂä°\"\n- \"ËÆæËÆ°‰∏Ä‰∏™ÁîµÂïÜÁ≥ªÁªüÁöÑÊï∞ÊçÆÂ∫ì ER Âõæ\"\n\nAI ‰ºöËá™Âä®ÁîüÊàêÂõæË°®Ôºå‰Ω†ÂèØ‰ª•Âú®ÁîªÂ∏É‰∏äÁõ¥Êé•ÁºñËæëÂíåË∞ÉÊï¥„ÄÇ\n\n## üíª Êú¨Âú∞ÈÉ®ÁΩ≤\n\nÂ¶ÇÊûú‰Ω†ÊÉ≥Âú®Êú¨Âú∞ËøêË°åÈ°πÁõÆÔºö\n\n```bash\n# ÂÖãÈöÜÈ°πÁõÆ\ngit clone <your-repo-url>\ncd smart-excalidraw-next\n\n# ÂÆâË£Ö‰æùËµñ\npnpm install\n\n# ÂêØÂä®ÂºÄÂèëÊúçÂä°Âô®\npnpm dev\n```\n\nËÆøÈóÆ http://localhost:3000 Âç≥ÂèØ‰ΩøÁî®„ÄÇ\n\n### ÈÖçÁΩÆÊúçÂä°Âô®Á´Ø LLMÔºàÂèØÈÄâÔºâ\n\nÂ¶ÇÊûú‰Ω†ÊÉ≥‰∏∫Áî®Êà∑Êèê‰æõÁªü‰∏ÄÁöÑ LLM ÈÖçÁΩÆÔºåÈÅøÂÖç‰ªñ‰ª¨Ëá™Â∑±Áî≥ËØ∑ API KeyÔºåÂèØ‰ª•ÈÖçÁΩÆÊúçÂä°Âô®Á´ØËÆøÈóÆÂØÜÁ†ÅÂäüËÉΩÔºö\n\n1. Â§çÂà∂ÁéØÂ¢ÉÂèòÈáèÁ§∫‰æãÊñá‰ª∂Ôºö\n```bash\ncp .env.example \n```\n\n2. Âú® `.env` ‰∏≠ÈÖçÁΩÆ‰ª•‰∏ãÂèòÈáèÔºö\n```bash\n# ËÆøÈóÆÂØÜÁ†ÅÔºàÁî®Êà∑ÈúÄË¶ÅËæìÂÖ•Ê≠§ÂØÜÁ†ÅÊâçËÉΩ‰ΩøÁî®ÊúçÂä°Âô®Á´Ø LLMÔºâ\nACCESS_PASSWORD=your-secure-password\n\n# LLM Êèê‰æõÂïÜÁ±ªÂûãÔºàopenai Êàñ anthropicÔºâ\nSERVER_LLM_TYPE=anthropic\n\n# API Âü∫Á°Ä URL\nSERVER_LLM_BASE_URL=https://api.anthropic.com/v1\n\n# API ÂØÜÈí•\nSERVER_LLM_API_KEY=sk-ant-your-key-here\n\n# Ê®°ÂûãÂêçÁß∞\nSERVER_LLM_MODEL=claude-sonnet-4-5-20250929\n```\n\n3. ÈáçÂêØÂºÄÂèëÊúçÂä°Âô®ÔºåÁî®Êà∑Âç≥ÂèØÈÄöËøáËÆøÈóÆÂØÜÁ†Å‰ΩøÁî®ÊúçÂä°Âô®Á´ØÈÖçÁΩÆÁöÑ LLM„ÄÇ\n\n**‰ºòÂäøÔºö**\n- Áî®Êà∑Êó†ÈúÄËá™Â∑±Áî≥ËØ∑ÂíåÈÖçÁΩÆ API Key\n- Áªü‰∏ÄÁÆ°ÁêÜ API ‰ΩøÁî®ÂíåÊàêÊú¨\n- ÈÄÇÂêàÂõ¢ÈòüÊàñÁªÑÁªáÂÜÖÈÉ®‰ΩøÁî®\n- Êèê‰æõÂÖçË¥π‰ΩìÈ™åÁªôÁî®Êà∑\n\n## ‚ùì Â∏∏ËßÅÈóÆÈ¢ò\n\n**Q: Êé®Ëçê‰ΩøÁî®Âì™‰∏™ AI Ê®°ÂûãÔºü**\nA: Âº∫ÁÉàÊé®Ëçê‰ΩøÁî® **claude-sonnet-4.5**ÔºåÂÆÉÂú®ÁêÜËß£ÈúÄÊ±ÇÂíåÁîüÊàêÂõæË°®ÊñπÈù¢Ë°®Áé∞ÊúÄ‰Ω≥„ÄÇ\n\n**Q: Êï∞ÊçÆÂÆâÂÖ®ÂêóÔºü**\nA: ÊâÄÊúâÈÖçÁΩÆ‰ø°ÊÅØ‰ªÖ‰øùÂ≠òÂú®‰Ω†ÁöÑÊµèËßàÂô®Êú¨Âú∞Ôºå‰∏ç‰ºö‰∏ä‰º†Âà∞‰ªª‰ΩïÊúçÂä°Âô®„ÄÇ\n\n**Q: ÊîØÊåÅÂì™‰∫õÂõæË°®Á±ªÂûãÔºü**\nA: ÊîØÊåÅÊµÅÁ®ãÂõæ„ÄÅÊû∂ÊûÑÂõæ„ÄÅÊó∂Â∫èÂõæ„ÄÅER Âõæ„ÄÅÊÄùÁª¥ÂØºÂõæ„ÄÅÁΩëÁªúÊãìÊâëÂõæÁ≠â 20+ ÁßçÁ±ªÂûãÔºåAI ‰ºöËá™Âä®ÈÄâÊã©ÊúÄÂêàÈÄÇÁöÑÁ±ªÂûã„ÄÇ\n\n**Q: ÁîüÊàêÁöÑÂõæË°®ÂèØ‰ª•‰øÆÊîπÂêóÔºü**\nA: ÂΩìÁÑ∂ÂèØ‰ª•ÔºÅÁîüÊàêÂêéÂèØ‰ª•Âú® Excalidraw ÁîªÂ∏É‰∏äËá™Áî±ÁºñËæëÔºåÂåÖÊã¨Ë∞ÉÊï¥‰ΩçÁΩÆ„ÄÅ‰øÆÊîπÊ†∑Âºè„ÄÅÊ∑ªÂä†ÂÖÉÁ¥†Á≠â„ÄÇ\n\n**Q: ‰ªÄ‰πàÊòØËÆøÈóÆÂØÜÁ†ÅÂäüËÉΩÔºü**\nA: ËÆøÈóÆÂØÜÁ†ÅÂäüËÉΩÂÖÅËÆ∏ÊúçÂä°Âô®ÁÆ°ÁêÜÂëòÈÖçÁΩÆÁªü‰∏ÄÁöÑ LLMÔºåÁî®Êà∑Âè™ÈúÄËæìÂÖ•ÂØÜÁ†ÅÂç≥ÂèØ‰ΩøÁî®ÔºåÊó†ÈúÄËá™Â∑±Áî≥ËØ∑ API Key„ÄÇÂêØÁî®ËÆøÈóÆÂØÜÁ†ÅÂêéÔºåÂ∞Ü‰ºòÂÖà‰ΩøÁî®ÊúçÂä°Âô®Á´ØÈÖçÁΩÆÔºåÂøΩÁï•Êú¨Âú∞ÈÖçÁΩÆ„ÄÇ\n\n**Q: ËÆøÈóÆÂØÜÁ†ÅÂíåÊú¨Âú∞ÈÖçÁΩÆÁöÑ‰ºòÂÖàÁ∫ßÊòØ‰ªÄ‰πàÔºü**\nA: Â¶ÇÊûúÂêØÁî®‰∫ÜËÆøÈóÆÂØÜÁ†ÅÔºåÁ≥ªÁªüÂ∞Ü‰ºòÂÖà‰ΩøÁî®ÊúçÂä°Âô®Á´ØÁöÑ LLM ÈÖçÁΩÆ„ÄÇÂè™ÊúâÂú®Êú™ÂêØÁî®ËÆøÈóÆÂØÜÁ†ÅÊó∂ÔºåÊâç‰ºö‰ΩøÁî®Êú¨Âú∞ÈÖçÁΩÆÁöÑ API Key„ÄÇ\n\n## üõ†Ô∏è ÊäÄÊúØÊ†à\n\nNext.js 16 ¬∑ React 19 ¬∑ Excalidraw ¬∑ Tailwind CSS 4 ¬∑ Monaco Editor\n\n## üìÑ ËÆ∏ÂèØËØÅ\n\nMIT License\n\n## ËÅîÁ≥ª‰ΩúËÄÖ\nÂæÆ‰ø°Âè∑Ôºö liujuntaoljt\n\n<img width=\"200\"  alt=\"ÂæÆ‰ø°ÂõæÁâá_20251103110224_44_85\" src=\"https://github.com/user-attachments/assets/6d8c4da2-af27-4213-b929-0d47fa51e9b5\" />\n\n## üíñ ËµûÂä©\n\nÊÑüË∞¢‰ª•‰∏ãËµûÂä©ËÄÖÂØπÊú¨È°πÁõÆÁöÑÊîØÊåÅÔºö\n\n<!-- ËµûÂä©ËÄÖÂêçÂçï -->\n- API‰∏≠ËΩ¨Á´ôÔºö[AI ÁΩëÂÖ≥ÔΩúÊèí‰ª∂‰∏ñÁïå](https://ai-router.plugins-world.cn)\n\nÂ¶ÇÊûúËøô‰∏™È°πÁõÆÂØπ‰Ω†ÊúâÂ∏ÆÂä©ÔºåÊ¨¢ËøéÈÄöËøá‰ª•‰∏ãÊñπÂºèÊîØÊåÅÔºö\n- ‚≠ê ÁªôÈ°πÁõÆÁÇπ‰∏™ Star\n- üí¨ ÂàÜ‰∫´ÁªôÊõ¥Â§öÈúÄË¶ÅÁöÑ‰∫∫\n- üí∞ Êàê‰∏∫ËµûÂä©ËÄÖÔºàËÅîÁ≥ª‰ΩúËÄÖÂæÆ‰ø°Ôºâ\n\n## ÂèãÊÉÖÈìæÊé•\n\n- https://github.com/ZhangQL2824/auto-drawio.git\n\n---\n\n## Star History\n\n[![Star History Chart](https://api.star-history.com/svg?repos=liujuntao123/smart-excalidraw-next&type=date&legend=top-left)](https://www.star-history.com/#liujuntao123/smart-excalidraw-next&type=date&legend=top-left)\n\n**Áî®Ëá™ÁÑ∂ËØ≠Ë®ÄÔºåÁªòÂà∂‰∏ì‰∏öÂõæË°®** - ËÆ©ÂèØËßÜÂåñÂàõ‰ΩúÂõûÂΩíÁÆÄÂçï\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-11-22T02:22:12.781577"
  },
  {
    "basic_info": {
      "name": "MiniMax-M2",
      "full_name": "MiniMax-AI/MiniMax-M2",
      "owner": "MiniMax-AI",
      "description": "MiniMax-M2, a model built for Max coding & agentic workflows.",
      "url": "https://github.com/MiniMax-AI/MiniMax-M2",
      "clone_url": "https://github.com/MiniMax-AI/MiniMax-M2.git",
      "ssh_url": "git@github.com:MiniMax-AI/MiniMax-M2.git",
      "homepage": "https://www.minimax.io/",
      "created_at": "2025-10-25T21:28:29Z",
      "updated_at": "2025-11-22T01:47:15Z",
      "pushed_at": "2025-11-13T08:12:36Z"
    },
    "stats": {
      "stars": 1797,
      "forks": 137,
      "watchers": 1797,
      "open_issues": 25,
      "size": 1220
    },
    "tech_info": {
      "language": null,
      "languages": {},
      "license": "Other",
      "topics": [
        "large-language-models",
        "llm"
      ]
    },
    "content": {
      "readme": "<div align=\"center\">\n  <picture>\n    <source srcset=\"figures/MiniMaxLogo-Dark.png\" media=\"(prefers-color-scheme: dark)\">\n      <img src=\"figures/MiniMaxLogo-Light.png\" width=\"60%\" alt=\"MiniMax\">\n    </source>\n  </picture>\n</div>\n<hr>\n\n<div align=\"center\" style=\"line-height: 1.4; font-size:16px; margin-top: 30px;\">\n  Join Our \n  <a href=\"https://github.com/MiniMax-AI/MiniMax-AI.github.io/blob/main/images/wechat-qrcode.jpeg\" target=\"_blank\" style=\"font-size:17px; margin: 2px;\">\n    üí¨ WeChat\n  </a> | \n  <a href=\"https://discord.com/invite/hvvt8hAye6\" target=\"_blank\" style=\"font-size:17px; margin: 2px;\">\n    üß© Discord\n  </a> \n  community.\n</div>\n<div align=\"center\" style=\"line-height: 1.2; font-size:16px;\">\n  <a href=\"https://agent.minimax.io/\" target=\"_blank\" style=\"display: inline-block; margin: 4px;\">\n    MiniMax Agent\n  </a> | \n  <a href=\"https://platform.minimax.io/docs/guides/text-generation\" target=\"_blank\" style=\"display: inline-block; margin: 4px;\">\n    ‚ö°Ô∏è API (Now Free for a limited time!)\n  </a> | \n  <a href=\"https://github.com/MiniMax-AI/MiniMax-MCP\" style=\"display: inline-block; margin: 4px;\">\n    MCP\n  </a> |\n  <a href=\"https://www.minimax.io\" target=\"_blank\" style=\"display: inline-block; margin: 4px;\">\n    MiniMax Website\n  </a> \n</div>\n<div align=\"center\" style=\"lline-height: 1.2; font-size:16px; margin-bottom: 30px;\">\n  <a href=\"https://huggingface.co/MiniMaxAI\" target=\"_blank\" style=\"margin: 2px;\">\n    ü§ó Hugging Face \n  </a> | \n  <a href=\"https://github.com/MiniMax-AI/MiniMax-M2\" target=\"_blank\" style=\"margin: 2px;\">\n    üêô GitHub\n  </a> | \n  <a href=\"https://www.modelscope.cn/organization/MiniMax\" target=\"_blank\" style=\"margin: 2px;\">\n    ü§ñÔ∏è ModelScope\n  </a> | \n  <a href=\"https://github.com/MiniMax-AI/MiniMax-M2/blob/main/LICENSE\" style=\"margin: 2px;\">\n    üìÑ License: MIT\n  </a>\n</div>\n\n# Meet MiniMax-M2\n\nToday, we release and open source MiniMax-M2, a **Mini** model built for **Max** coding & agentic workflows.\n\n**MiniMax-M2** redefines efficiency for agents. It's a compact, fast, and cost-effective MoE model (230 billion total parameters with 10 billion active parameters) built for elite performance in coding and agentic tasks, all while maintaining powerful general intelligence. With just 10 billion activated parameters, MiniMax-M2 provides the sophisticated, end-to-end tool use performance expected from today's leading models, but in a streamlined form factor that makes deployment and scaling easier than ever.\n\n<p align=\"center\">\n  <img width=\"100%\" src=\"figures/Bench.png\">\n</p>\n\n---\n\n## Highlights\n\n**Superior Intelligence**. According to benchmarks from Artificial Analysis, MiniMax-M2 demonstrates highly competitive general intelligence across mathematics, science, instruction following, coding, and agentic tool use. **Its composite score ranks #1 among open-source models globally**.\n\n**Advanced Coding**. Engineered for end-to-end developer workflows, MiniMax-M2 excels at multi-file edits, coding-run-fix loops, and test-validated repairs. Strong performance on Terminal-Bench and (Multi-)SWE-Bench‚Äìstyle tasks demonstrates practical effectiveness in terminals, IDEs, and CI across languages.\n\n**Agent Performance**. MiniMax-M2 plans and executes complex, long-horizon toolchains across shell, browser, retrieval, and code runners. In BrowseComp-style evaluations, it consistently locates hard-to-surface sources, maintains evidence traceable, and gracefully recovers from flaky steps.\n\n**Efficient Design**. With 10 billion activated parameters (230 billion in total), MiniMax-M2 delivers lower latency, lower cost, and higher throughput for interactive agents and batched sampling‚Äîperfectly aligned with the shift toward highly deployable models that still shine on coding and agentic tasks.\n\n---\n\n## Coding & Agentic Benchmarks\n\nThese comprehensive evaluations test real-world end-to-end coding and agentic tool use: editing real repos, executing commands, browsing the web, and delivering functional solutions. Performance on this suite correlates with day-to-day developer experience in terminals, IDEs, and CI.\n\n| **Benchmark** | **MiniMax-M2** | **Claude Sonnet 4** | **Claude Sonnet 4.5** | **Gemini 2.5 Pro** | **GPT-5 (thinking)** | **GLM-4.6** | **Kimi K2 0905** | **DeepSeek-V3.2** |\n|-----------|------------|-----------------|-------------------|-----------------|------------------|---------|---------------|----------------|\n| **SWE-bench Verified** | 69.4 | 72.7 * | 77.2 * | 63.8 * | 74.9 * | 68 * | 69.2 * | 67.8 * |\n| **Multi-SWE-Bench** | 36.2 | 35.7 * | 44.3 | / | / | 30 | 33.5 | 30.6 |\n| **SWE-bench Multilingual** | 56.5 | 56.9 * | 68 | / | / | 53.8 | 55.9 * | 57.9 * |\n| **Terminal-Bench** | 46.3 | 36.4 * | 50 * | 25.3 * | 43.8 * | 40.5 * | 44.5 * | 37.7 * |\n| **ArtifactsBench** | 66.8 | 57.3* | 61.5 | 57.7* | 73* | 59.8 | 54.2 | 55.8 |\n| **BrowseComp** | 44 | 12.2 | 19.6 | 9.9 | 54.9* | 45.1* | 14.1 | 40.1* |\n| **BrowseComp-zh** | 48.5 | 29.1 | 40.8 | 32.2 | 65 | 49.5 | 28.8 | 47.9* |\n| **GAIA (",
      "default_branch": "main"
    },
    "fetched_at": "2025-11-22T02:22:14.083049"
  },
  {
    "basic_info": {
      "name": "reader3",
      "full_name": "karpathy/reader3",
      "owner": "karpathy",
      "description": "Quick illustration of how one can easily read books together with LLMs. It's great and I highly recommend it.",
      "url": "https://github.com/karpathy/reader3",
      "clone_url": "https://github.com/karpathy/reader3.git",
      "ssh_url": "git@github.com:karpathy/reader3.git",
      "homepage": null,
      "created_at": "2025-11-18T02:37:00Z",
      "updated_at": "2025-11-22T02:04:48Z",
      "pushed_at": "2025-11-18T02:37:51Z"
    },
    "stats": {
      "stars": 1726,
      "forks": 179,
      "watchers": 1726,
      "open_issues": 8,
      "size": 271
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 13925,
        "HTML": 8921
      },
      "license": null,
      "topics": []
    },
    "content": {
      "readme": "# reader 3\n\n![reader3](reader3.png)\n\nA lightweight, self-hosted EPUB reader that lets you read through EPUB books one chapter at a time. This makes it very easy to copy paste the contents of a chapter to an LLM, to read along. Basically - get epub books (e.g. [Project Gutenberg](https://www.gutenberg.org/) has many), open them up in this reader, copy paste text around to your favorite LLM, and read together and along.\n\nThis project was 90% vibe coded just to illustrate how one can very easily [read books together with LLMs](https://x.com/karpathy/status/1990577951671509438). I'm not going to support it in any way, it's provided here as is for other people's inspiration and I don't intend to improve it. Code is ephemeral now and libraries are over, ask your LLM to change it in whatever way you like.\n\n## Usage\n\nThe project uses [uv](https://docs.astral.sh/uv/). So for example, download [Dracula EPUB3](https://www.gutenberg.org/ebooks/345) to this directory as `dracula.epub`, then:\n\n```bash\nuv run reader3.py dracula.epub\n```\n\nThis creates the directory `dracula_data`, which registers the book to your local library. We can then run the server:\n\n```bash\nuv run server.py\n```\n\nAnd visit [localhost:8123](http://localhost:8123/) to see your current Library. You can easily add more books, or delete them from your library by deleting the folder. It's not supposed to be complicated or complex.\n\n## License\n\nMIT",
      "default_branch": "master"
    },
    "fetched_at": "2025-11-22T02:22:15.362023"
  },
  {
    "basic_info": {
      "name": "ai-agents-from-scratch",
      "full_name": "pguso/ai-agents-from-scratch",
      "owner": "pguso",
      "description": "Demystify AI agents by building them yourself. Local LLMs, no black boxes, real understanding of function calling, memory, and ReAct patterns.",
      "url": "https://github.com/pguso/ai-agents-from-scratch",
      "clone_url": "https://github.com/pguso/ai-agents-from-scratch.git",
      "ssh_url": "git@github.com:pguso/ai-agents-from-scratch.git",
      "homepage": "",
      "created_at": "2025-10-23T19:39:04Z",
      "updated_at": "2025-11-21T19:27:58Z",
      "pushed_at": "2025-11-21T19:27:55Z"
    },
    "stats": {
      "stars": 1472,
      "forks": 170,
      "watchers": 1472,
      "open_issues": 12,
      "size": 369
    },
    "tech_info": {
      "language": "JavaScript",
      "languages": {
        "JavaScript": 497479
      },
      "license": "MIT License",
      "topics": [
        "ai-agents",
        "educational",
        "function-calling",
        "llm",
        "llm-agent",
        "node-llama-cpp",
        "react-agent",
        "tutorial"
      ]
    },
    "content": {
      "readme": "# AI Agents From Scratch\n\nLearn to build AI agents locally without frameworks. Understand what happens under the hood before using production frameworks.\n\n## Purpose\n\nThis repository teaches you to build AI agents from first principles using **local LLMs** and **node-llama-cpp**. By working through these examples, you'll understand:\n\n- How LLMs work at a fundamental level\n- What agents really are (LLM + tools + patterns)\n- How different agent architectures function\n- Why frameworks make certain design choices\n\n**Philosophy**: Learn by building. Understand deeply, then use frameworks wisely.\n\n## Next Phase: Build LangChain & LangGraph Concepts From Scratch\n\n> After mastering the fundamentals, the next stage of this project walks you through **re-implementing the core parts of LangChain and LangGraph** in plain JavaScript using local models.\n> This is **not** about building a new framework, it‚Äôs about understanding *how frameworks work*.  \n\n## Phase 1: Agent Fundamentals - From LLMs to ReAct\n\n### Prerequisites\n- Node.js 18+\n- At least 8GB RAM (16GB recommended)\n- Download models and place in `./models/` folder, details in [DOWNLOAD.md](DOWNLOAD.md)\n\n### Installation\n```bash\nnpm install\n```\n\n### Run Examples\n```bash\nnode intro/intro.js\nnode simple-agent/simple-agent.js\nnode react-agent/react-agent.js\n```\n\n## Learning Path\n\nFollow these examples in order to build understanding progressively:\n\n### 1. **Introduction** - Basic LLM Interaction\n`intro/` | [Code](examples/01_intro/intro.js) | [Code Explanation](examples/01_intro/CODE.md) | [Concepts](examples/01_intro/CONCEPT.md)\n\n**What you'll learn:**\n- Loading and running a local LLM\n- Basic prompt/response cycle\n\n**Key concepts**: Model loading, context, inference pipeline, token generation\n\n---\n\n### 2. (Optional) **OpenAI Intro** - Using Proprietary Models\n`openai-intro/` | [Code](examples/02_openai-intro/openai-intro.js) | [Code Explanation](examples/02_openai-intro/CODE.md) | [Concepts](examples/02_openai-intro/CONCEPT.md)\n\n**What you'll learn:**\n- How to call hosted LLMs (like GPT-4)\n- Temperature Control\n- Token Usage\n\n**Key concepts**: Inference endpoints, network latency, cost vs control, data privacy, vendor dependence\n\n---\n\n### 3. **Translation** - System Prompts & Specialization\n`translation/` | [Code](examples/03_translation/translation.js) | [Code Explanation](examples/03_translation/CODE.md) | [Concepts](examples/03_translation/CONCEPT.md)\n\n**What you'll learn:**\n- Using system prompts to specialize agents\n- Output format control\n- Role-based behavior\n- Chat wrappers for different models\n\n**Key concepts**: System prompts, agent specialization, behavioral constraints, prompt engineering\n\n---\n\n### 4. **Think** - Reasoning & Problem Solving\n`think/` | [Code](examples/04_think/think.js) | [Code Explanation](examples/04_think/CODE.md) | [Concepts](examples/04_think/CONCEPT.md)\n\n**What you'll learn:**\n- Configuring LLMs for logical reasoning\n- Complex quantitative problems\n- Limitations of pure LLM reasoning\n- When to use external tools\n\n**Key concepts**: Reasoning agents, problem decomposition, cognitive tasks, reasoning limitations\n\n---\n\n### 5. **Batch** - Parallel Processing\n`batch/` | [Code](examples/05_batch/batch.js) | [Code Explanation](examples/05_batch/CODE.md) | [Concepts](examples/05_batch/CONCEPT.md)\n\n**What you'll learn:**\n- Processing multiple requests concurrently\n- Context sequences for parallelism\n- GPU batch processing\n- Performance optimization\n\n**Key concepts**: Parallel execution, sequences, batch size, throughput optimization\n\n---\n\n### 6. **Coding** - Streaming & Response Control\n`coding/` | [Code](examples/06_coding/coding.js) | [Code Explanation](examples/06_coding/CODE.md) | [Concepts](examples/06_coding/CONCEPT.md)\n\n**What you'll learn:**\n- Real-time streaming responses\n- Token limits and budget management\n- Progressive output display\n- User experience optimization\n\n**Key concepts**: Streaming, token-by-token generation, response control, real-time feedback\n\n---\n\n### 7. **Simple Agent** - Function Calling (Tools)\n`simple-agent/` | [Code](examples/07_simple-agent/simple-agent.js) | [Code Explanation](examples/07_simple-agent/CODE.md) | [Concepts](examples/07_simple-agent/CONCEPT.md)\n\n**What you'll learn:**\n- Function calling / tool use fundamentals\n- Defining tools the LLM can use\n- JSON Schema for parameters\n- How LLMs decide when to use tools\n\n**Key concepts**: Function calling, tool definitions, agent decision making, action-taking\n\n**This is where text generation becomes agency!**\n\n---\n\n### 8. **Simple Agent with Memory** - Persistent State\n`simple-agent-with-memory/` | [Code](examples/08_simple-agent-with-memory/simple-agent-with-memory.js) | [Code Explanation](examples/08_simple-agent-with-memory/CODE.md) | [Concepts](examples/08_simple-agent-with-memory/CONCEPT.md)\n\n**What you'll learn:**\n- Persisting information across sessions\n- Long-term memory management\n- Facts and preferences storage\n- Memory retrieval strategies\n\n**Ke",
      "default_branch": "main"
    },
    "fetched_at": "2025-11-22T02:22:16.644037"
  },
  {
    "basic_info": {
      "name": "NoLongerEvil-Thermostat",
      "full_name": "codykociemba/NoLongerEvil-Thermostat",
      "owner": "codykociemba",
      "description": "Breathe fresh life into your bricked Nest, now with 100% less evil!",
      "url": "https://github.com/codykociemba/NoLongerEvil-Thermostat",
      "clone_url": "https://github.com/codykociemba/NoLongerEvil-Thermostat.git",
      "ssh_url": "git@github.com:codykociemba/NoLongerEvil-Thermostat.git",
      "homepage": "https://nolongerevil.com",
      "created_at": "2025-11-02T03:30:37Z",
      "updated_at": "2025-11-22T01:41:18Z",
      "pushed_at": "2025-11-22T02:16:37Z"
    },
    "stats": {
      "stars": 1272,
      "forks": 69,
      "watchers": 1272,
      "open_issues": 24,
      "size": 31779
    },
    "tech_info": {
      "language": "C",
      "languages": {
        "C": 26428,
        "Shell": 9532,
        "Makefile": 478,
        "Nix": 114
      },
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "# Nest Thermostat Firmware Setup\n\n[![Buy Me A Coffee](https://img.shields.io/badge/Buy%20Me%20A%20Coffee-donate-yellow.svg)](https://buymeacoffee.com/codykociemba)\n\n> **‚ö†Ô∏è WARNING: EXPERIMENTAL SOFTWARE**\n>\n> This project is currently in the **experimental/testing phase**. Do NOT use this firmware on any thermostat that is critical for your heating or cooling needs. Flashing this firmware may brick your device or cause unexpected behavior. Only proceed if you have a backup thermostat or can afford to have your device non-functional during testing.\n\n## A Note from the Developer\n\nThis project has blown up way more than I ever expected! I want to be transparent: this was thrown together in a couple of days, so it's still very new and a work in progress. Thank you all so much for your support and enthusiasm!\n\n**Support the Project:**\nDeveloping and maintaining open-source projects takes a lot of time (and late nights). If this project helped you reclaim your device or you just want to support the effort to keep this hardware usable, consider buying me a coffee. It keeps the code flowing and the development going!\n\n<a href=\"https://buymeacoffee.com/codykociemba\" target=\"_blank\"><img src=\"https://cdn.buymeacoffee.com/buttons/v2/default-yellow.png\" alt=\"Buy Me A Coffee\" style=\"height: 60px !important;width: 217px !important;\" ></a>\n\n**Self-Hosted Open Source Option Available:** A self-hosted open source solution has been posted on the [`open-source-prototype`](https://github.com/codykociemba/NoLongerEvil-Thermostat/tree/open-source-prototype) branch. Check out the [discussion here](https://github.com/codykociemba/NoLongerEvil-Thermostat/discussions/34) for more details.\n\n**Hardware Alternative:** If you're interested in the hardware side of things, check out [https://sett.homes](https://sett.homes) for a drop-in PCB replacement option.\n\n**Important:** The README instructions below are for the hosted version only. If you're adventurous, feel free to dive into the self-hosted branch. Otherwise, you may want to wait until it's been fully fleshed out if you don't want to deal with bugs.\n\n---\n\nThis directory contains the tools and firmware needed to flash custom firmware to Nest Thermostat devices using the OMAP DFU (Device Firmware Update) interface.\n\n## Prerequesites\n\nYou will need to have, ideally, a linux computer available. MacOS can also be used but some people are having difficulties. Windows should be able to be used with MingW or CygWin but YMMV.\n\n‚ö†Ô∏è This firmware is only for Nest Generation 1 and 2. On the back plate you should see a bubble level, which should be green. If it's blue, that's gen 3 and not supported yet.\n\n## Overview\n\nThis firmware loader uses the OMAP bootloader interface to flash custom bootloader and kernel images to Nest Thermostat devices. The device must be put into DFU mode to accept new firmware.\n\n**Important:** After flashing this firmware, your device will no longer contact Nest/Google servers. It will operate independently and connect to the NoLongerEvil platform instead, giving you complete control over your thermostat.\n\n## How it Works\n\nThe custom firmware flashes the device with modified bootloader and kernel components that redirect all network traffic from the original Nest/Google servers to a server we specify. This server hosts a reverse-engineered replica of their API, allowing the thermostat to function independently while giving you complete control over your device data and settings.\n\nBy intercepting the communication layer, the thermostat believes it's communicating with the official Nest infrastructure, but instead connects to the NoLongerEvil platform. This approach ensures full compatibility with the device's existing software while breaking free from Google's cloud dependency.\n\n## Quick Start\n\n### 1. Clone the Repository\n\n```bash\ngit clone --recurse-submodules https://github.com/codykociemba/NoLongerEvil-Thermostat.git\ncd NoLongerEvil-Thermostat\n```\n\n### 2. Install Prerequisites\n\nBefore building, you'll need to install some required packages:\n\n#### Linux (Debian/Ubuntu)\n\n```bash\nsudo apt-get update\nsudo apt-get install build-essential libusb-1.0-0-dev gcc pkg-config\n```\n\n#### macOS\n\nFirst, install Xcode Command Line Tools:\n\n```bash\nxcode-select --install\n```\n\nThen install libusb using Homebrew (the build script will attempt to install this automatically if missing):\n\n```bash\n# Install Homebrew if you don't have it\n/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\n\n# Install libusb\nbrew install libusb pkg-config\n```\n\n### 3. Build the omap_loader tool\n\n```bash\nchmod +x build.sh\n./build.sh\n```\n\nThe build script will automatically detect your operating system (Linux, macOS, or Windows) and build the appropriate binary.\n\n### 4. Start the firmware installer\n\n**IMPORTANT: You must start the installer script BEFORE rebooting the device.**\n\n#### Linux\n\n```bash\nchmod +x install.sh\n./install.sh\n```\n\n#### macOS\n\n```bash\nchmod +x insta",
      "default_branch": "main"
    },
    "fetched_at": "2025-11-22T02:22:17.922340"
  },
  {
    "basic_info": {
      "name": "Emu3.5",
      "full_name": "baaivision/Emu3.5",
      "owner": "baaivision",
      "description": "Native Multimodal Models are World Learners",
      "url": "https://github.com/baaivision/Emu3.5",
      "clone_url": "https://github.com/baaivision/Emu3.5.git",
      "ssh_url": "git@github.com:baaivision/Emu3.5.git",
      "homepage": "",
      "created_at": "2025-10-29T13:40:19Z",
      "updated_at": "2025-11-21T15:37:43Z",
      "pushed_at": "2025-11-19T08:25:09Z"
    },
    "stats": {
      "stars": 1267,
      "forks": 44,
      "watchers": 1267,
      "open_issues": 23,
      "size": 23845
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 269009
      },
      "license": "Apache License 2.0",
      "topics": []
    },
    "content": {
      "readme": "<div align='center'>\n<h1>Emu3.5: Native Multimodal Models are World Learners</h1>\n\nEmu3.5 Team, BAAI\n\n[Project Page](https://emu.world/) | [ü§óHF Models](https://huggingface.co/collections/BAAI/emu35) | [Paper](https://arxiv.org/pdf/2510.26583)\n</div>\n\n\n<div align='center'>\n<img src=\"./assets/arch.png\" class=\"interpolation-image\" alt=\"arch.\" height=\"100%\" width=\"100%\" />\n</div>\n\n\n<div align='center'>\n<img src=\"./assets/co.png\" class=\"interpolation-image\" alt=\"arch.\" height=\"90%\" width=\"90%\" />\n</div>\n\n\n|  üîπ | **Core Concept**                         | **Description**                                                                                                                            |\n| :-: | :--------------------------------------- | :----------------------------------------------------------------------------------------------------------------------------------------- |\n|  üß† | **Unified World Modeling**               | Predicts the **next state jointly across vision and language**, enabling coherent **world modeling** and **generation**.              |\n|  üß© | **End-to-End Pretraining**               | Trained with a **unified next-token prediction** objective over **interleaved vision‚Äìlanguage sequences**.                                 |\n|  üìö | **Over 10T+ Multimodal Tokens**               | Pre-trained on **over 10 trillion interleaved tokens** from **video frames** and **transcripts**, capturing **spatiotemporal structure**.       |\n|  üîÑ | **Native Multimodal I/O**                | Processes and generates **interleaved visual‚Äìtext sequences** without **modality adapters** or **task-specific heads**.                    |\n|  üéØ | **RL Post-Training**                     | Large-scale **reinforcement learning** enhances **reasoning**, **compositionality**, and **generation quality**.                           |\n|  ‚ö°  | **Discrete Diffusion Adaptation (DiDA)** | Converts **sequential decoding ‚Üí bidirectional parallel prediction**, achieving **‚âà20√ó faster inference without performance loss**.      |\n| üñºÔ∏è | **Versatile Generation**                 | Excels in **long-horizon vision‚Äìlanguage generation**, **any-to-image (X2I)** synthesis, and **text-rich image creation**.                 |\n|  üåê | **Generalizable World Modeling**         | Enables **spatiotemporally consistent world exploration**, and **open-world embodied manipulation** across diverse scenarios.          |\n|  üèÜ | **Performance Benchmark**                | Matches **Gemini 2.5 Flash Image (Nano Banana)** on **image generation/editing**, and **outperforms** on **interleaved generation tasks**. |\n\n\n\n## üî• News\n\n- **2025-11-19 ¬∑ üöÄ vLLM Offline Inference Released** ‚Äî Meet `inference_vllm.py` with a new cond/uncond batch scheduler, delivering **4‚Äì5√ó faster end-to-end generation** on vLLM 0.11.0 across Emu3.5 tasks. Jump to [#Run Inference with vLLM](#run-inference-with-vllm) for setup guidance and see PR [#47](https://github.com/baaivision/Emu3.5/pull/47) for full details.\n- **2025-11-17 ¬∑ üéõÔ∏è Gradio Demo (Transformers Backend)** ‚Äî Introduced `gradio_demo_image.py` and `gradio_demo_interleave.py` presets for the standard Transformers runtime, providing turnkey T2I/X2I and interleaved generation experiences with streaming output. Try the commands in [#Gradio Demo](#3-gradio-demo) to launch both UIs locally.\n\n## Table of Contents\n\n1. [Model & Weights](#1-model--weights)\n2. [Quick Start](#2-quick-start)\n3. [Gradio Demo](#3-gradio-demo)\n4. [Schedule](#4-schedule)\n5. [Citation](#5-citation)\n\n## 1. Model & Weights\n\n| Model name               | HF Weight |\n| ------------------------ | --------- |\n| Emu3.5               | [ü§ó HF link](https://huggingface.co/BAAI/Emu3.5/tree/main) |\n| Emu3.5-Image                | [ü§ó HF link](https://huggingface.co/BAAI/Emu3.5-Image/tree/main) |\n| Emu3.5-VisionTokenizer     | [ü§ó HF link](https://huggingface.co/BAAI/Emu3.5-VisionTokenizer/tree/main) |\n\n\n*Note:*  \n- **Emu3.5** supports general-purpose multimodal predictions, including interleaved image-text generation and single-image generation (T2I/X2I) tasks.\n- **Emu3.5-Image** is a model focused on T2I/X2I tasks for best performance on these scenarios.\n- Both models are pure next-token predictors without DiDA acceleration (each image may take several minutes to generate).  \n- ‚ö° **Stay tuned for DiDA-accelerated weights.**\n\n> üí° **Usage tip:**  \n> For **interleaved image-text generation**, use **Emu3.5**.  \n> For **single-image generation** (T2I and X2I), use **Emu3.5-Image** for the best quality.\n\n\n\n## 2. Quick Start\n\n### Environment Setup\n\n```bash\n# Requires Python 3.12 or higher.\ngit clone https://github.com/baaivision/Emu3.5\ncd Emu3.5\npip install -r requirements/transformers.txt\npip install flash_attn==2.8.3 --no-build-isolation\n```\n### Configuration\n\nEdit `configs/config.py` to set:\n\n- Paths: `model_path`, `vq_path`\n- Task template: `task_type in {t2i, x2i, howto, story, explore, vla}`\n- Input image: `use_image` (True to provide reference images, controls <|IMAGE|> tok",
      "default_branch": "main"
    },
    "fetched_at": "2025-11-22T02:22:19.195390"
  },
  {
    "basic_info": {
      "name": "pg_lake",
      "full_name": "Snowflake-Labs/pg_lake",
      "owner": "Snowflake-Labs",
      "description": "pg_lake: Postgres with Iceberg and data lake access",
      "url": "https://github.com/Snowflake-Labs/pg_lake",
      "clone_url": "https://github.com/Snowflake-Labs/pg_lake.git",
      "ssh_url": "git@github.com:Snowflake-Labs/pg_lake.git",
      "homepage": "https://github.com/Snowflake-Labs/pg_lake/blob/main/docs/README.md",
      "created_at": "2025-11-04T10:38:17Z",
      "updated_at": "2025-11-21T18:09:05Z",
      "pushed_at": "2025-11-22T00:10:24Z"
    },
    "stats": {
      "stars": 1250,
      "forks": 51,
      "watchers": 1250,
      "open_issues": 33,
      "size": 4689
    },
    "tech_info": {
      "language": "C",
      "languages": {
        "C": 3077797,
        "Python": 2385893,
        "C++": 161930,
        "PLpgSQL": 44929,
        "Makefile": 32433,
        "Dockerfile": 11557,
        "Shell": 10371,
        "CMake": 2392,
        "Emacs Lisp": 730
      },
      "license": "Apache License 2.0",
      "topics": []
    },
    "content": {
      "readme": "# pg_lake: Postgres for Iceberg and Data lakes\n\n`pg_lake` integrates Iceberg and data lake files into Postgres. With the `pg_lake` extensions, you can use Postgres as a stand-alone lakehouse system that supports transactions and fast queries on Iceberg tables, and can directly work with raw data files in object stores like S3.\n\nAt a high level, `pg_lake` lets you:\n\n- **Create and modify [Iceberg](https://iceberg.apache.org/)** tables directly from PostgreSQL, with full transactional guarantees and query them from other engines\n- **Query and import data files in object storage** in [Parquet](https://parquet.apache.org/), CSV, JSON, and Iceberg format\n- **Export query results back to object storage** in Parquet, CSV, or JSON formats using COPY commands\n- **Read geospatial formats** supported by GDAL, such as GeoJSON and Shapefiles\n- **Use the built-in [map type](./pg_map/README.md)** for semi-structured or key‚Äìvalue data  \n- **Combine heap, Iceberg, and external Parquet/CSV/JSON** files in the same SQL queries and modifications ‚Äî all with full transactional guarantees and no SQL limitations  \n- **Infer table columns and types** from external data sources such as Iceberg, Parquet, JSON, and CSV files\n- **Leverage DuckDB‚Äôs query engine** underneath for fast execution without leaving Postgres  \n\n## Setting up `pg_lake`\n\nThere are two ways to set up `pg_lake`:  \n- **Using Docker**, for an easy, ready-to-run test environment.  \n- **Building from source**, for a manual setup or development use.  \n\nBoth approaches include the PostgreSQL extensions, the `pgduck_server` application and setting up S3-compatible storage.\n\n### Using Docker\n\nFollow the [Docker README](./docker/README.md) to set up and run `pg_lake` with Docker.\n\n\n### Building from source\n\nOnce you‚Äôve [built and installed the required components](./docs/building-from-source.md), you can initialize `pg_lake` inside Postgres.\n\n#### Creating the extensions\n\nCreate all required extensions at once using `CASCADE`:\n\n```sql\nCREATE EXTENSION pg_lake CASCADE;\nNOTICE:  installing required extension \"pg_lake_table\"\nNOTICE:  installing required extension \"pg_lake_engine\"\nNOTICE:  installing required extension \"pg_extension_base\"\nNOTICE:  installing required extension \"pg_lake_iceberg\"\nNOTICE:  installing required extension \"pg_lake_copy\"\nCREATE EXTENSION\n```\n\n#### Running `pgduck_server`\n\n`pgduck_server` is a standalone process that implements the Postgres wire-protocol (locally), and underneath uses `DuckDB` to execute queries.\n\nWhen you run `pgduck_server` it starts listening to port `5332` on unix domain socket:\n```\npgduck_server\nLOG pgduck_server is listening on unix_socket_directory: /tmp with port 5332, max_clients allowed 10000\n```\n\nAs `pgduck_server` implements Postgres wire protocol, you can access it via `psql` on port `5332` and host `/tmp` and run commands via DuckDB. \n\nFor example, you can get the DuckDB version:\n\n```sql\npsql -p 5332 -h /tmp\n\nselect version() as duckdb_version; \nduckdb_version \n---------------- \nv1.3.2 (1 row)\n```\n\nYou can also provide some additional settings while starting the server, to see all:\n```\npgduck_server --help\n```\n\nThere are some important settings that should be adjusted, especially on production systems:\n\n\n- `--memory_limit`: Optionally specify the maximum memory of pgduck_server similar to DuckDB's memory_limit, the default is 80 percent of the system memory\n- `--init_file_path <path>`: Execute all statements in this file on start-up\n- `--cache_dir`: Specify the directory to use to cache remote files (from S3)\n\nNote that if you want to make adjustments to duckdb settings, you can use the `--init_file_path` approach OR you can\nconnect to the running pgduck_server and make changes. For example:\n\n```\n$ psql -h /tmp -p 5332\npsql (17.5, server 16.4.DuckPG)\nType \"help\" for help.\n\npostgres=> set global threads = 16;\nSET\n```\n\nThe connection above is to the pgduck_server on its port (default 5332), NOT to the postgres/pg_lake server. \n\n\n#### Connecting `pg_lake` to s3 (or compatible)\n\n`pgduck_server` relies on the DuckDB [secrets manager](https://duckdb.org/docs/stable/configuration/secrets_manager) for credentials and it follows the credentials chain by default for AWS and GCP. Make sure your cloud credentials are configured properly ‚Äî for example, by setting them in ~/.aws/credentials.  \n\nOnce you set up the credential chain, you should set the `pg_lake_iceberg.default_location_prefix`. This is the location where Iceberg tables are stored:\n\n```sql\nSET pg_lake_iceberg.default_location_prefix TO 's3://testbucketpglake';\n```\n\nYou can also set the credentials on `pgduck_server` for [local development with `minio`](docs/building-from-source.md#running-s3-compatible-service-minio-locally).\n\n## Using pg_lake\n\n### Create an Iceberg table\n\nYou can create Iceberg tables by adding `USING iceberg` to your `CREATE TABLE` statements.\n\n```sql\nCREATE TABLE iceberg_test USING iceberg \n      AS SELECT \n            i as key, 'val_'|| i  as val\n    ",
      "default_branch": "main"
    },
    "fetched_at": "2025-11-22T02:22:20.531520"
  },
  {
    "basic_info": {
      "name": "xiaomi-miloco",
      "full_name": "XiaoMi/xiaomi-miloco",
      "owner": "XiaoMi",
      "description": "Xiaomi Miloco",
      "url": "https://github.com/XiaoMi/xiaomi-miloco",
      "clone_url": "https://github.com/XiaoMi/xiaomi-miloco.git",
      "ssh_url": "git@github.com:XiaoMi/xiaomi-miloco.git",
      "homepage": null,
      "created_at": "2025-11-06T13:01:59Z",
      "updated_at": "2025-11-22T01:10:10Z",
      "pushed_at": "2025-11-21T12:56:59Z"
    },
    "stats": {
      "stars": 1240,
      "forks": 70,
      "watchers": 1240,
      "open_issues": 43,
      "size": 13400
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 1054483,
        "JavaScript": 506375,
        "Shell": 86782,
        "C++": 83167,
        "Less": 71704,
        "HTML": 53752,
        "CSS": 9737,
        "Dockerfile": 5505,
        "C": 2948,
        "CMake": 1813
      },
      "license": "Other",
      "topics": []
    },
    "content": {
      "readme": "# Xiaomi Miloco\n\n**Xiaomi Local Copilot** is a future exploration solution for smart homes. Using Xiaomi Home cameras as the source of visual information and a self-developed LLM as its core, it connects all IoT devices throughout the house. Based on the development paradigm of LLM, it enables users to define various family needs and rules in natural language, achieving broader and more creative smart device integration.\n\n<div align=\"center\">\n\nEnglish | [ÁÆÄ‰Ωì‰∏≠Êñá](README_zh_Hans.md)\n\n</div>\n\n## News\n\n- [2025-11] Xiaomi Miloco Framework Open Source\n\n## Key Features\n\n1. New Interaction Paradigm: Based on the development paradigm of LLM, rule-setting and complex device command control can be completed through natural language interaction.\n2. New Use for Visual Data: Using camera data streams as a source of perceptual information, the LLM is used to analyze various home scene events contained in the visual data to respond to user queries.\n3. On-Device LLM: The home scene tasks are split into two stages: planning and visual understanding. It provides Xiaomi's self-developed on-device model to realize on-device video understanding and ensure family privacy and security.\n4. Xiaomi Home Ecosystem: It connects with the Xiaomi Home ecosystem, supports the retrieval and execution of Mi Home devices and scenes, and supports sending customized content for Xiao Home notifications.\n\n    <img src=\"assets/images/ai_center.jpg\" width=\"60%\" />\n\n## Quick Start\n\n### System Requirements\n\n- **Hardware Requirements**\n```Plain Text\nCPU: x64 architecture\nGraphics Card: NVIDIA 30 series and above, 8GB VRAM minimum (recommended 12GB and above)\nStorage: Recommended 16GB or more available space (for local model storage)\n```\n\n- **Software Requirements**\n```Plain Text\nOperating System:\n  - Linux: x64 architecture, recommended Ubuntu 22.04 and above LTS versions\n  - Windows: x64 architecture, recommended Windows 10 and above, requires WSL2 support\n  - macOS: Not currently supported\nDocker: Version 20.10 and above, requires docker compose support\nNVIDIA Driver: NVIDIA driver with CUDA support\nNVIDIA Container Toolkit: For Docker GPU support\n```\n\n### Install\n\n> **Note**: Please ensure your system meets the above hardware and software requirements. Windows systems need to enter the WSL environment.\n\n**Install with Docker**  \nOne-click installation via command line\n```bash\nbash -c \"$(wget -qO- https://xiaomi-miloco.cnbj1.mi-fds.com/xiaomi-miloco/install.sh)\"\n```\nOr download the source code first, then execute the one-click installation script:\n```bash\ngit clone https://github.com/XiaoMi/xiaomi-miloco.git\n\nbash scripts/install.sh\n```\nFor detailed installation steps, please refer to the [Docker Deployment Documentation](docs/environment-setup.md).\n\n**Install with source code**  \nFor source code installation steps, please refer to the [Development Guide](docs/development/developer-setup.md).\n\n## Usage Documentation\n\nPlease refer to the [Usage Documentation](docs/usage/README.md).\n\n## Contributing\n\nPlease refer to the [Contributing Guide](CONTRIBUTING.md).\n\n## License\n\nFor license details, please see [LICENSE.md](LICENSE.md).\n\n**Important Notice**: This project is limited to non-commercial use only. Without written authorization from Xiaomi Corporation, this project may not be used for developing applications, web services, or other forms of software.\n\n## Security Issues\n\nIf you discover potential security issues in this project, or believe you may have found a security issue, please notify the [Miloco Team](xiaomi-miloco@xiaomi.com) via our vulnerability reporting email. Please do not create public GitHub Issues.\n\n## Contact Us\n\n### Issue Reporting\n\nFor issue reporting, please participate through the following methods:\n- Submit a [GitHub Issue](https://github.com/XiaoMi/xiaomi-miloco/issues/new/)\n\n### Technical Discussion\n\n- GitHub [Discussions](https://github.com/XiaoMi/xiaomi-miloco/discussions/)\n- Project Discussion Group (WeChat):\n\n  <img src=\"assets/images/miloco_wechat_6.jpg\" width=\"30%\" />  <img src=\"assets/images/miloco_wechat_group_12.jpeg\" width=\"30%\" />\n\n\n### Join Us\n\nThe **Xiaomi Miloco** team is hiring. Send your resume to `xiaomi-miloco@xiaomi.com`, and it will be delivered directly to the project lead.\n\n## Acknowledgments\n\nThank you to the original team members who worked hard for MilocoÔºözhaoy„ÄÅyangyongjie„ÄÅxx„ÄÅChangyu„ÄÅyyk„ÄÅjunhui„ÄÅÈÉ≠ÂÖ¥ÂÆù„ÄÅ47„ÄÅafei„ÄÇ\n\nYour passion and talent are the fundamental driving force behind Miloco's continuous innovation and progress.\n\nSpecial thanks to:\n- The [llama.cpp](https://github.com/ggml-org/llama.cpp) open source project for providing inference backend capabilities\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-11-22T02:22:21.806759"
  },
  {
    "basic_info": {
      "name": "crypto-trading-open",
      "full_name": "cryptocj520/crypto-trading-open",
      "owner": "cryptocj520",
      "description": "crypto-trading-open",
      "url": "https://github.com/cryptocj520/crypto-trading-open",
      "clone_url": "https://github.com/cryptocj520/crypto-trading-open.git",
      "ssh_url": "git@github.com:cryptocj520/crypto-trading-open.git",
      "homepage": null,
      "created_at": "2025-11-11T12:00:02Z",
      "updated_at": "2025-11-21T23:41:01Z",
      "pushed_at": "2025-11-11T12:03:28Z"
    },
    "stats": {
      "stars": 1228,
      "forks": 673,
      "watchers": 1228,
      "open_issues": 12,
      "size": 997
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 2825921,
        "Shell": 37998
      },
      "license": null,
      "topics": []
    },
    "content": {
      "readme": "# Â§ö‰∫§ÊòìÊâÄÁ≠ñÁï•Ëá™Âä®ÂåñÁ≥ªÁªü\n\n**Multi-Exchange Strategy Automation System**\n\n## üéØ È°πÁõÆÁÆÄ‰ªã\n\nËøôÊòØ‰∏Ä‰∏™‰ºÅ‰∏öÁ∫ßÁöÑÂ§ö‰∫§ÊòìÊâÄÂä†ÂØÜË¥ßÂ∏ÅËá™Âä®Âåñ‰∫§ÊòìÁ≥ªÁªüÔºåÊèê‰æõÈ´òÊÄßËÉΩ„ÄÅÈ´òÂèØÈù†ÊÄßÁöÑÁΩëÊ†º‰∫§Êòì„ÄÅÂà∑Èáè‰∫§Êòì„ÄÅÂ•óÂà©ÁõëÊéßÂíåÂ∏ÇÂú∫ÁõëÊéßÂäüËÉΩ„ÄÇÁ≥ªÁªüÈááÁî®‰∏•Ê†ºÁöÑÂàÜÂ±ÇÊû∂ÊûÑËÆæËÆ°ÔºåÊîØÊåÅ Hyperliquid„ÄÅBackpack„ÄÅLighter„ÄÅBinance„ÄÅOKX„ÄÅEdgeX Á≠âÂ§ö‰∏™‰∫§ÊòìÊâÄÁöÑÂÆåÊï¥ÈÄÇÈÖç„ÄÇ\n\n## üèóÔ∏è Ê†∏ÂøÉÁ≥ªÁªüÊû∂ÊûÑ\n\n### Á≥ªÁªüÁªÑ‰ª∂\n\n```\nÂ§ö‰∫§ÊòìÊâÄÁ≠ñÁï•Ëá™Âä®ÂåñÁ≥ªÁªü\n‚îú‚îÄ‚îÄ üìä ÁΩëÊ†º‰∫§ÊòìÁ≥ªÁªü (Grid Trading)\n‚îÇ   ‚îú‚îÄ‚îÄ ÊôÆÈÄöÁΩëÊ†º              # Âõ∫ÂÆö‰ª∑Ê†ºÂå∫Èó¥ÁΩëÊ†º\n‚îÇ   ‚îú‚îÄ‚îÄ È©¨‰∏ÅÁΩëÊ†º              # È©¨‰∏ÅÊ†ºÂ∞îÈÄíÂ¢ûÁ≠ñÁï•\n‚îÇ   ‚îú‚îÄ‚îÄ ‰ª∑Ê†ºÁßªÂä®ÁΩëÊ†º          # Âä®ÊÄÅË∑üÈöè‰ª∑Ê†º\n‚îÇ   ‚îú‚îÄ‚îÄ Ââ•Â§¥ÁöÆÊ®°Âºè            # Âø´ÈÄüÊ≠¢ÊçüÁ≠ñÁï•\n‚îÇ   ‚îú‚îÄ‚îÄ Êô∫ËÉΩÂâ•Â§¥ÁöÆ            # Â§öÊ¨°Ê∑±Ë∑åÊ£ÄÊµã\n‚îÇ   ‚îú‚îÄ‚îÄ Êú¨Èáë‰øùÊä§Ê®°Âºè          # Ëá™Âä®Ê≠¢Êçü‰øùÊä§\n‚îÇ   ‚îú‚îÄ‚îÄ Ê≠¢ÁõàÊ®°Âºè              # Âà∞ËææÁõÆÊ†áËá™Âä®Âπ≥‰ªì\n‚îÇ   ‚îî‚îÄ‚îÄ Áé∞Ë¥ßÈ¢ÑÁïôÁÆ°ÁêÜ          # Áé∞Ë¥ßÂ∏ÅÁßçÈ¢ÑÁïô\n‚îú‚îÄ‚îÄ üîç ÁΩëÊ†ºÊ≥¢Âä®ÁéáÊâ´ÊèèÂô® (Grid Volatility Scanner)\n‚îÇ   ‚îú‚îÄ‚îÄ ËôöÊãüÁΩëÊ†ºÊ®°Êãü          # Êó†ÈúÄÂÆûÈôÖ‰∏ãÂçïÁöÑÊ®°ÊãüÁΩëÊ†º\n‚îÇ   ‚îú‚îÄ‚îÄ ÂÆûÊó∂APRËÆ°ÁÆó           # ÂáÜÁ°ÆÈ¢ÑÊµãÂπ¥ÂåñÊî∂ÁõäÁéá\n‚îÇ   ‚îú‚îÄ‚îÄ ‰ª£Â∏ÅÊéíË°åÊ¶ú            # ÊåâÊ≥¢Âä®ÁéáÂíåAPRÊéíÂ∫è\n‚îÇ   ‚îú‚îÄ‚îÄ Êô∫ËÉΩËØÑÁ∫ßÁ≥ªÁªü          # S/A/B/C/DÁ≠âÁ∫ßËØÑ‰º∞\n‚îÇ   ‚îî‚îÄ‚îÄ ÁªàÁ´Ø UI              # Rich ÂÆûÊó∂ÁõëÊéßÁïåÈù¢\n‚îú‚îÄ‚îÄ üíπ Âà∑Èáè‰∫§ÊòìÁ≥ªÁªü (Volume Maker)\n‚îÇ   ‚îú‚îÄ‚îÄ ÊåÇÂçïÊ®°Âºè              # Èôê‰ª∑ÂçïÂà∑ÈáèÔºàBackpackÔºâ\n‚îÇ   ‚îî‚îÄ‚îÄ Â∏Ç‰ª∑Ê®°Âºè              # Â∏Ç‰ª∑ÂçïÂø´ÈÄüÂà∑ÈáèÔºàLighterÔºâ\n‚îú‚îÄ‚îÄ üîÑ Â•óÂà©ÁõëÊéßÁ≥ªÁªü (Arbitrage Monitor)\n‚îÇ   ‚îú‚îÄ‚îÄ ‰ª∑Ê†ºÁõëÊéß              # ÂÆûÊó∂‰ª∑Ê†ºÂ∑ÆÁõëÊéß\n‚îÇ   ‚îú‚îÄ‚îÄ ËµÑÈáëË¥πÁéáÁõëÊéß          # Ë∑®‰∫§ÊòìÊâÄË¥πÁéáÂ∑ÆÂºÇ\n‚îÇ   ‚îú‚îÄ‚îÄ Â•óÂà©Êú∫‰ºöËØÜÂà´          # ‰ª∑Â∑ÆÂíåË¥πÁéáÂ•óÂà©\n‚îÇ   ‚îú‚îÄ‚îÄ ÁªàÁ´Ø UI              # Rich ÂÆûÊó∂ÁõëÊéßÁïåÈù¢\n‚îÇ   ‚îî‚îÄ‚îÄ ‰∫§ÊòìÂØπËá™Âä®ÂèëÁé∞        # Â§ö‰∫§ÊòìÊâÄ‰∫§ÊòìÂØπÂåπÈÖç\n‚îú‚îÄ‚îÄ üîî ‰ª∑Ê†ºÊèêÈÜíÁ≥ªÁªü (Price Alert)\n‚îÇ   ‚îú‚îÄ‚îÄ ‰ª∑Ê†ºÁ™ÅÁ†¥ÁõëÊéß          # ‰ª∑Ê†ºËß¶ÂèäÁõÆÊ†áÊèêÈÜí\n‚îÇ   ‚îú‚îÄ‚îÄ Â§ö‰∫§ÊòìÊâÄÊîØÊåÅ          # ÊîØÊåÅÊâÄÊúâÊé•ÂÖ•ÁöÑ‰∫§ÊòìÊâÄ\n‚îÇ   ‚îú‚îÄ‚îÄ ÁªàÁ´Ø UI              # ÂÆûÊó∂‰ª∑Ê†ºÊòæÁ§∫\n‚îÇ   ‚îî‚îÄ‚îÄ Â£∞Èü≥ÊèêÈÜí              # Á™ÅÁ†¥Êó∂Â£∞Èü≥ÈÄöÁü•\n‚îú‚îÄ‚îÄ üîó ‰∫§ÊòìÊâÄÈÄÇÈÖçÂ±Ç (Exchange Adapters)\n‚îÇ   ‚îú‚îÄ‚îÄ Hyperliquid ÈÄÇÈÖçÂô®    # Ê∞∏Áª≠ÂêàÁ∫¶ + Áé∞Ë¥ß\n‚îÇ   ‚îú‚îÄ‚îÄ Backpack ÈÄÇÈÖçÂô®       # Ê∞∏Áª≠ÂêàÁ∫¶\n‚îÇ   ‚îú‚îÄ‚îÄ Lighter ÈÄÇÈÖçÂô®        # Ê∞∏Áª≠ÂêàÁ∫¶Ôºà‰ΩéÊâãÁª≠Ë¥πÔºâ\n‚îÇ   ‚îú‚îÄ‚îÄ Binance ÈÄÇÈÖçÂô®        # Áé∞Ë¥ß + Ê∞∏Áª≠ÂêàÁ∫¶\n‚îÇ   ‚îú‚îÄ‚îÄ OKX ÈÄÇÈÖçÂô®            # Áé∞Ë¥ß + Ê∞∏Áª≠ÂêàÁ∫¶\n‚îÇ   ‚îú‚îÄ‚îÄ EdgeX ÈÄÇÈÖçÂô®          # Ê∞∏Áª≠ÂêàÁ∫¶\n‚îÇ   ‚îî‚îÄ‚îÄ Áªü‰∏ÄÊé•Âè£Ê†áÂáÜ          # Ê†áÂáÜÂåñ API Êé•Âè£\n‚îî‚îÄ‚îÄ üèõÔ∏è Âü∫Á°ÄËÆæÊñΩÂ±Ç (Infrastructure)\n    ‚îú‚îÄ‚îÄ ‰æùËµñÊ≥®ÂÖ•ÂÆπÂô®          # DI ÂÆπÂô®ÁÆ°ÁêÜ\n    ‚îú‚îÄ‚îÄ ‰∫ã‰ª∂Á≥ªÁªü              # ‰∫ã‰ª∂È©±Âä®Êû∂ÊûÑ\n    ‚îú‚îÄ‚îÄ Êó•ÂøóÁ≥ªÁªü              # ÁªìÊûÑÂåñÊó•Âøó\n    ‚îú‚îÄ‚îÄ ÈÖçÁΩÆÁÆ°ÁêÜ              # YAML ÈÖçÁΩÆÁ≥ªÁªü\n    ‚îî‚îÄ‚îÄ Êï∞ÊçÆËÅöÂêàÂô®            # Â§ö‰∫§ÊòìÊâÄÊï∞ÊçÆËÅöÂêà\n```\n\n## üöÄ Âø´ÈÄüÂºÄÂßã\n\n### Á≥ªÁªüË¶ÅÊ±Ç\n\n- Python 3.8+\n- ÊîØÊåÅÁöÑÊìç‰ΩúÁ≥ªÁªüÔºöLinux„ÄÅmacOS„ÄÅWindows\n- ÂèØÈÄâÔºötmuxÔºàÁî®‰∫éÂ§öËøõÁ®ãÁÆ°ÁêÜÔºâ\n\n### ÂÆâË£Ö‰æùËµñ\n\n```bash\n# ÂÆâË£Ö Python ‰æùËµñ\npip install -r requirements.txt\n```\n\n### ÈÖçÁΩÆ API ÂØÜÈí•\n\nÂú® `config/exchanges/` ÁõÆÂΩï‰∏ãÈÖçÁΩÆÂØπÂ∫î‰∫§ÊòìÊâÄÁöÑ API ÂØÜÈí•Ôºö\n\n```bash\nconfig/exchanges/\n‚îú‚îÄ‚îÄ hyperliquid_config.yaml   # Hyperliquid ÈÖçÁΩÆ\n‚îú‚îÄ‚îÄ backpack_config.yaml       # Backpack ÈÖçÁΩÆ\n‚îú‚îÄ‚îÄ lighter_config.yaml        # Lighter ÈÖçÁΩÆ\n‚îú‚îÄ‚îÄ binance_config.yaml        # Binance ÈÖçÁΩÆ\n‚îú‚îÄ‚îÄ okx_config.yaml            # OKX ÈÖçÁΩÆ\n‚îî‚îÄ‚îÄ edgex_config.yaml          # EdgeX ÈÖçÁΩÆ\n```\n\n### Âø´ÈÄüÂêØÂä®ÂêÑÁ≥ªÁªü\n\n#### ÁΩëÊ†º‰∫§ÊòìÁ≥ªÁªü\n```bash\npython3 run_grid_trading.py config/grid/lighter-long-perp-btc.yaml\n```\n\n#### Âà∑Èáè‰∫§ÊòìÁ≥ªÁªüÔºàBackpackÊåÇÂçïÊ®°ÂºèÔºâ\n```bash\npython3 run_volume_maker.py config/volume_maker/backpack_btc_volume_maker.yaml\n```\n\n#### Âà∑Èáè‰∫§ÊòìÁ≥ªÁªüÔºàLighterÂ∏Ç‰ª∑Ê®°ÂºèÔºâ\n```bash\npython3 run_lighter_volume_maker.py config/volume_maker/lighter_volume_maker.yaml\n```\n\n#### Â•óÂà©ÁõëÊéßÁ≥ªÁªü\n```bash\npython3 run_arbitrage_monitor.py\n```\n\n#### ‰ª∑Ê†ºÊèêÈÜíÁ≥ªÁªü\n```bash\npython3 run_price_alert.py config/price_alert/binance_alert.yaml\n```\n\n#### ÁΩëÊ†ºÊ≥¢Âä®ÁéáÊâ´ÊèèÂô®\n```bash\npython3 grid_volatility_scanner/run_scanner.py\n```\n\n## üìã Ê†∏ÂøÉÂäüËÉΩËØ¶Ëß£\n\n### 1Ô∏è‚É£ ÁΩëÊ†º‰∫§ÊòìÁ≥ªÁªü\n\n#### ÂäüËÉΩÁâπÊÄß\n\n- **Â§öÁßçÁΩëÊ†ºÊ®°Âºè**ÔºöÊôÆÈÄöÁΩëÊ†º„ÄÅÈ©¨‰∏ÅÁΩëÊ†º„ÄÅ‰ª∑Ê†ºÁßªÂä®ÁΩëÊ†º\n- **Êô∫ËÉΩÁ≠ñÁï•**ÔºöÂâ•Â§¥ÁöÆ„ÄÅÊô∫ËÉΩÂâ•Â§¥ÁöÆ„ÄÅÊú¨Èáë‰øùÊä§„ÄÅÊ≠¢ÁõàÊ®°Âºè\n- **ÂÅ•Â∫∑Ê£ÄÊü•**ÔºöËá™Âä®ËÆ¢ÂçïÊ†°È™åÂíå‰øÆÂ§çÊú∫Âà∂\n- **ÁªàÁ´Ø UI**ÔºöÂÆûÊó∂ÁõëÊéßÁïåÈù¢ÔºåÊòæÁ§∫ÊåÅ‰ªì„ÄÅÁõà‰∫è„ÄÅÁΩëÊ†ºÁä∂ÊÄÅ\n- **Áé∞Ë¥ßÊîØÊåÅ**ÔºöÁé∞Ë¥ßÈ¢ÑÁïôÁÆ°ÁêÜÔºàËá™Âä®Áª¥ÊåÅÂ∏ÅÁßç‰ΩôÈ¢ùÔºâ\n- **Â§ö‰∫§ÊòìÊâÄ**ÔºöÊîØÊåÅ Hyperliquid„ÄÅBackpack„ÄÅLighter\n\n#### ÈÖçÁΩÆÊñá‰ª∂‰ΩçÁΩÆ\n\n```\nconfig/grid/\n‚îú‚îÄ‚îÄ lighter_btc_perp_long.yaml              # Lighter BTC ÂÅöÂ§ö\n‚îú‚îÄ‚îÄ lighter_btc_perp_short.yaml             # Lighter BTC ÂÅöÁ©∫\n‚îú‚îÄ‚îÄ hyperliquid_btc_perp_long.yaml          # Hyperliquid BTC ÂÅöÂ§ö\n‚îú‚îÄ‚îÄ hyperliquid_btc_perp_short.yaml         # Hyperliquid BTC ÂÅöÁ©∫\n‚îú‚îÄ‚îÄ hyperliquid_btc_spot_long.yaml          # Hyperliquid Áé∞Ë¥ßÂÅöÂ§ö\n‚îú‚îÄ‚îÄ backpack_capital_protection_long_btc.yaml   # Backpack BTC Êú¨Èáë‰øùÊä§\n‚îú‚îÄ‚îÄ backpack_capital_protection_long_eth.yaml   # Backpack ETH Êú¨Èáë‰øùÊä§\n‚îú‚îÄ‚îÄ backpack_capital_protection_long_sol.yaml   # Backpack SOL Êú¨Èáë‰øùÊä§\n‚îú‚îÄ‚îÄ backpack_capital_protection_long_bnb.yaml   # Backpack BNB Êú¨Èáë‰øùÊä§\n‚îî‚îÄ‚îÄ backpack_capital_protection_long_hype.yaml  # Backpack HYPE Êú¨Èáë‰øùÊä§\n```\n\n#### ÂêØÂä®ÊñπÂºè\n\n```bash\n# ÊñπÂºè1ÔºöÁõ¥Êé•ÂêØÂä®ÔºàÊé®ËçêÔºâ\npython3 run_grid_trading.py config/grid/lighter_btc_perp_long.yaml\npython3 run_grid_trading.py config/grid/lighter_eth_perp_long.yaml\n\n# ÊñπÂºè2ÔºöDEBUG Ê®°ÂºèÂêØÂä®ÔºàÊü•ÁúãËØ¶ÁªÜÊó•ÂøóÔºâ\npython3 run_grid_trading.py config/grid/lighter_btc_perp_long.yaml --debug\n\n# ÊñπÂºè3Ôºö‰ΩøÁî® Shell ËÑöÊú¨ÊâπÈáèÂêØÂä®ÔºàtmuxÔºâ\n./scripts/start_all_grids.sh\n```\n\n#### Ê†∏ÂøÉÊñá‰ª∂\n\n| Êñá‰ª∂Ë∑ØÂæÑ | ËØ¥Êòé |\n|---------|------|\n| `run_grid_trading.py` | ÁΩëÊ†º‰∫§ÊòìÁ≥ªÁªü‰∏ªÂêØÂä®ËÑöÊú¨ |\n| `core/services/grid/coordinator/grid_coordinator.py` | ÁΩëÊ†ºÁ≥ªÁªüÂçèË∞ÉÂô®ÔºàÊ†∏ÂøÉÈÄªËæëÔºâ |\n| `core/services/grid/implementations/grid_engine_impl.py` | ÁΩëÊ†ºÊâßË°åÂºïÊìé |\n| `core/services/grid/implementations/grid_strategy_impl.py` | ÁΩëÊ†ºÁ≠ñÁï•ÂÆûÁé∞ |\n| `core/services/grid/implementations/position_tracker_impl.py` | ÊåÅ‰ªìË∑üË∏™Âô® |\n| `core/services/grid/implementations/order_health_checker.py` | ËÆ¢ÂçïÂÅ•Â∫∑Ê£ÄÊü•Âô® |\n| `core/services/grid/scalping/scalping_manager.py` | Ââ•Â§¥ÁöÆÁÆ°ÁêÜÂô® |\n| `core/services/grid/scalping/smart_scalping_tracker.py` | Êô∫ËÉΩÂâ•Â§¥ÁöÆËøΩË∏™Âô® |\n| `core/services/grid/capital_protection/capital_protection_manager.py` | Êú¨Èáë‰øùÊä§ÁÆ°ÁêÜÂô® |\n| `core/services/grid/terminal_ui.py` | ÁªàÁ´Ø UI ÁïåÈù¢ |\n\n### 2Ô∏è‚É£ Âà∑Èáè‰∫§ÊòìÁ≥ªÁªü\n\n#### ÂäüËÉΩÁâπÊÄß\n\n- **Âèå‰∫§ÊòìÊ®°Âºè**ÔºöÊåÇÂçïÊ®°ÂºèÔºàBackpackÔºâ„ÄÅÂ∏Ç‰ª∑Ê®°ÂºèÔºàLighterÔºâ\n- **‰ø°Âè∑Ê∫êÊîØÊåÅ**ÔºöBackpack REST API„ÄÅHyperliquid WebSocket\n- **Êô∫ËÉΩÂà§Êñ≠**Ôºö‰π∞ÂçñÂçïÊï∞ÈáèÂØπÊØî„ÄÅ‰ª∑Ê†ºÂèòÂä®ÁõëÊéß\n- **ÂÆûÊó∂ÁªüËÆ°**ÔºöÊàê‰∫§Èáè„ÄÅ",
      "default_branch": "main"
    },
    "fetched_at": "2025-11-22T02:22:23.074987"
  },
  {
    "basic_info": {
      "name": "Auto-login-netlib",
      "full_name": "eooce/Auto-login-netlib",
      "owner": "eooce",
      "description": "Ëá™Âä®ÁôªÂΩïnetlib.reÁΩëÁ´ô‰øùÊ¥ªÂüüÂêç",
      "url": "https://github.com/eooce/Auto-login-netlib",
      "clone_url": "https://github.com/eooce/Auto-login-netlib.git",
      "ssh_url": "git@github.com:eooce/Auto-login-netlib.git",
      "homepage": "",
      "created_at": "2025-11-06T11:43:07Z",
      "updated_at": "2025-11-22T01:29:35Z",
      "pushed_at": "2025-11-08T10:28:36Z"
    },
    "stats": {
      "stars": 1214,
      "forks": 2375,
      "watchers": 1214,
      "open_issues": 0,
      "size": 53
    },
    "tech_info": {
      "language": "JavaScript",
      "languages": {
        "JavaScript": 4308
      },
      "license": "GNU General Public License v3.0",
      "topics": []
    },
    "content": {
      "readme": "## Netlib Ëá™Âä®ÁôªÂΩï‰øùÊ¥ªËÑöÊú¨\nËøôÊòØ‰∏Ä‰∏™Áî®‰∫éËá™Âä®ÁôªÂΩï Netlib ÁΩëÁ´ô‰ª•‰øùÊåÅË¥¶Êà∑Ê¥ªË∑ÉÁöÑËÑöÊú¨ÔºåÈÖçÂêà GitHub Actions ÂÆûÁé∞Ëá™Âä®ÂÆöÊó∂ÊâßË°å„ÄÇ\n\nÊ≥®ÂÜåÂú∞ÂùÄÔºöhttps://www.netlib.re\n\nËßÜÈ¢ëÊïôÁ®ãÂú∞ÂùÄÔºöhttps://youtu.be/7laVmEfgC9o\n\n\n### ÂäüËÉΩÁâπÁÇπ\n- üîê Ëá™Âä®ÁôªÂΩï Netlib Ë¥¶Êà∑(ÂçïË¥¶Êà∑ÊàñÂ§öË¥¶Êà∑)\n- üë• ÊîØÊåÅÂ§öË¥¶Êà∑ÊâπÈáèÂ§ÑÁêÜ\n- ‚è∞ ÊØè60Â§©Ëá™Âä®ÊâßË°å‰∏ÄÊ¨°\n- üì± ÊâßË°åÁªìÊûúÂèØÈÄöËøá Telegram ÈÄöÁü•\n\n### ‰ΩøÁî®ÊñπÊ≥ï\n1. fork Ê≠§È°πÁõÆ\n2. Âú®ActionsËèúÂçïÂÖÅËÆ∏ `I understand my workflows, go ahead and enable them` ÊåâÈíÆ\n\n3. Âú® GitHub ‰ªìÂ∫ìÁöÑ Settings ‚Üí Secrets and variables ‚Üí Actions ‰∏≠Ê∑ªÂä†‰ª•‰∏ãÁéØÂ¢ÉÂèòÈáè\n-  Ë¥¶Âè∑ÂØÜÁ†Å‰πãÈó¥Áî®Ëã±ÊñáÂÜíÂè∑ÂàÜÈöîÔºåË¥¶Âè∑‰∏éË¥¶Âè∑‰πãÈó¥Ëã±ÊñáÈÄóÂè∑ÂàÜÈöî\n- `ACCOUNTS`  NetlibË¥¶Êà∑(ÂøÖÂ°´)ÔºåÊ†ºÂºè(ÂçïË¥¶Âè∑)Ôºöuser:pass   Ê†ºÂºè(Â§öË¥¶Âè∑)Ôºöuser1:pass1,user2:pass2 \n\n- telegram‰∏∫ÂèØÈÄâÁéØÂ¢ÉÂèòÈáè,‰∏çÈúÄË¶ÅÈÄöÁü•ÂèØ‰∏çÂ°´ÂÜô\n- `BOT_TOKEN` \tTelegramÊú∫Âô®‰∫∫Token https://t.me/BotFather ÂàõÂª∫botÂêéËé∑Âèñ\n- `CHAT_ID`     Telegram ËÅäÂ§©ID https://t.me/laowang_serv00_bot ÂèëÈÄÅ /start Ëé∑Âèñ\n\t    \n4. GitHub Actions ÂàùÂßãÊâãÂä®ÊâßË°åÊ£ÄÊü•ÊòØÂê¶ÊúâÈÖçÁΩÆÈîôËØØÔºåËÑöÊú¨‰ºöËá™Âä®ÊØè60Â§©ÊâßË°å‰∏ÄÊ¨°,ÂèØÊâãÂä®ÊâßË°å\n\n### Ê≥®ÊÑè‰∫ãÈ°π\n1. Á°Æ‰øù Netlib Ë¥¶Êà∑ÂØÜÁ†ÅÊ≠£Á°Æ\n2. È¶ñÊ¨°ËøêË°å GitHub Actions ÈúÄË¶ÅÊéàÊùÉ\n3. ËÑöÊú¨ÊâßË°åÊó∂Èó¥‰∏∫ UTC 0:00ÔºàÈ¶ôÊ∏ØÊó∂Èó¥ 8:00Ôºâ\n4. Â¶ÇÊûú‰∏çÈúÄË¶Å Telegram ÈÄöÁü•ÔºåÂèØ‰∏çÈÖçÁΩÆÁõ∏ÂÖ≥ÁéØÂ¢ÉÂèòÈáè\n\n\n### ËÆ∏ÂèØËØÅ\nGPL 3.0\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-11-22T02:22:24.591227"
  }
]