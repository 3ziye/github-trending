[
  {
    "basic_info": {
      "name": "OmniWorld",
      "full_name": "yangzhou24/OmniWorld",
      "owner": "yangzhou24",
      "description": "OmniWorld: A Multi-Domain and Multi-Modal Dataset for 4D World Modeling",
      "url": "https://github.com/yangzhou24/OmniWorld",
      "clone_url": "https://github.com/yangzhou24/OmniWorld.git",
      "ssh_url": "git@github.com:yangzhou24/OmniWorld.git",
      "homepage": "https://yangzhou24.github.io/OmniWorld/",
      "created_at": "2025-09-15T13:11:18Z",
      "updated_at": "2025-09-16T17:57:33Z",
      "pushed_at": "2025-09-16T05:02:35Z"
    },
    "stats": {
      "stars": 162,
      "forks": 1,
      "watchers": 162,
      "open_issues": 1,
      "size": 11541
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 12580,
        "Shell": 1242
      },
      "license": null,
      "topics": []
    },
    "content": {
      "readme": "<h1 align='center'>OmniWorld: A Multi-Domain and Multi-Modal Dataset for 4D World Modeling</h1>\n<div align='center'>\n    <a href='https://github.com/yangzhou24' target='_blank'>Yang Zhou</a><sup>1</sup> \n    <a href='https://github.com/yyfz' target='_blank'>Yifan Wang</a><sup>1</sup> \n    <a href='https://zhoutimemachine.github.io' target='_blank'>Jianjun Zhou</a><sup>1,2</sup> \n    <a href='https://github.com/AmberHeart' target='_blank'>Wenzheng Chang</a><sup>1</sup> \n    <a href='https://github.com/ghy0324' target='_blank'>Haoyu Guo</a><sup>1</sup> \n    <a href='https://github.com/LiZizun' target='_blank'>Zizun Li</a><sup>1</sup> \n    <a href='https://kaijing.space/' target='_blank'>Kaijing Ma</a><sup>1</sup> \n    \n</div>\n<div align='center'>\n<a href='https://scholar.google.com/citations?user=VuTRUg8AAAAJ' target='_blank'>Xinyue Li</a><sup>1</sup> \n    <a href='https://scholar.google.com/citations?user=5SuBWh0AAAAJ' target='_blank'>Yating Wang</a><sup>1</sup> \n    <a href='https://www.haoyizhu.site/' target='_blank'>Haoyi Zhu</a><sup>1</sup> \n    <a href='https://mingyulau.github.io/' target='_blank'>Mingyu Liu</a><sup>1,2</sup> \n    <a href='https://scholar.google.com/citations?user=FbSpETgAAAAJ' target='_blank'>Dingning Liu</a><sup>1</sup> \n    <a href='https://yangjiangeyjg.github.io/' target='_blank'>Jiange Yang</a><sup>1</sup>\n    <a href='https://github.com/Kr1sJFU' target='_blank'>Zhoujie Fu</a><sup>1</sup>  \n    \n</div>\n<div align='center'>\n    <a href='https://sotamak1r.github.io/' target='_blank'>Junyi Chen</a><sup>1</sup> \n    <a href='https://cshen.github.io' target='_blank'>Chunhua Shen</a><sup>2</sup> \n    <a href='https://oceanpang.github.io' target='_blank'>Jiangmiao Pang</a><sup>1</sup> \n    <a href='https://kpzhang93.github.io/' target='_blank'>Kaipeng Zhang</a><sup>1</sup>\n    <a href='https://tonghe90.github.io/' target='_blank'>Tong He</a><sup>1†</sup>\n</div>\n<div align='center'>\n    <sup>1</sup>Shanghai AI Lab  <sup>2</sup>ZJU \n</div>\n<br>\n<div align=\"center\">\n  <a href=\"https://yangzhou24.github.io/OmniWorld/\"><img src=\"https://img.shields.io/badge/Project Page-5745BB?logo=google-chrome&logoColor=white\"></a>  \n  <a href=\"https://arxiv.org/abs/2509.12201\"><img src=\"https://img.shields.io/static/v1?label=Paper&message=Arxiv&color=red&logo=arxiv\"></a>  \n  <a href=\"https://github.com/yangzhou24/OmniWorld\"><img src=\"https://img.shields.io/static/v1?label=Code&message=Github&color=blue&logo=github\"></a>  \n  <a href=\"https://huggingface.co/datasets/InternRobotics/OmniWorld\"><img src=\"https://img.shields.io/static/v1?label=Dataset&message=HuggingFace&color=yellow&logo=huggingface\"></a>  \n</div>\n\n<img src=\"assets/teaser.png\" width=\"1000px\">\n\n## 🎉 NEWS\n- [2025.9.16] 🔥 The first 1.2k splits release of **OmniWorld-Game** is now live on Hugging Face! **More data is coming soon, stay tuned!**\n\n## ✨ Overview\n\nOmniWorld is a large-scale, multi-domain, and multi-modal dataset specifically designed for 🌍**4D world modeling**, e.g. 4D geometric reconstruction, future prediction & camera-controlled video generation.\n\n### 🔑 Key Features\n\n- 📊 **Massive Scale**: 4000+ hours, 600K+ sequences, 300M+ frames\n- 🤖 **Diverse Domains**: sourced from simulartor, robot, human & the Internet\n- 🎨 **Rich Multi-Modality**: depth maps, camera poses, text captions, optical flow & foreground mask\n\n### 🎮 Introducing _OmniWorld-Game_\n\n_OmniWorld-Game_ is a newly collected high-quality synthetic subset of the main _OmniWorld_ dataset. It features:\n\n- 📊 **Scale**: 214 hours, 96K video clips, 18M+ frames\n- 🧩 **Resolution & Diversity**: 720P RGB image capatured from a wide range of dynamic game environments\n- 🎨 **Comprehensive Annotations**: cover all annotation types of the _OmniWorld_ dataset\n\n### 🏆 _OmniWorld-Game_ Benchmark\n\n_OmniWorld-Game_ Benchmark offers 4D world modeling evaluation for 3D Geometric Prediction &\nCamera Control Video Generation. Found: \n\n- 🚫 Current state-of-the-art approaches **still show great limitations** in modeling complex 4D environments, based on both quantitative metrics and qualitative results.\n- 📈 **Fine-tuning** existing SOTA methods on _OmniWorld_ leads to **significant performance gains** across 4D reconstruction and video generation tasks, highlighting the value of our dataset.\n\n\n## 💡 Dataset Download\nYou can download the entire OmniWorld dataset using the following command:\n```bash\n# 1. Install (if you haven't yet)\npip install --upgrade \"huggingface_hub[cli]\"\n\n# 2. Full download\nhf download InternRobotics/OmniWorld \\\n           --repo-type dataset \\\n           --local-dir /path/to/DATA_PATH\n```\nFor downloading specific files (instead of the full dataset), please refer to the [`dowanload_specific.py`](scripts/dowanload_specific.py).\n\n\n\n## 🚀 Visualize as Point Cloud\n\nThis script allows you to convert a scene from the dataset into a 3D point cloud for inspection.\n\n### 1\\. Prerequisites\n\nPlease follow the instructions in the \"Dataset Download\" section to acquire the dataset.\n\n### 2\\. Data Str",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-16T19:10:01.803112"
  },
  {
    "basic_info": {
      "name": "provenance-action",
      "full_name": "danielroe/provenance-action",
      "owner": "danielroe",
      "description": "GitHub Action that detects dependency provenance downgrades from lockfile changes (npm/pnpm/yarn).",
      "url": "https://github.com/danielroe/provenance-action",
      "clone_url": "https://github.com/danielroe/provenance-action.git",
      "ssh_url": "git@github.com:danielroe/provenance-action.git",
      "homepage": "",
      "created_at": "2025-09-16T11:08:14Z",
      "updated_at": "2025-09-16T19:06:40Z",
      "pushed_at": "2025-09-16T19:06:34Z"
    },
    "stats": {
      "stars": 126,
      "forks": 3,
      "watchers": 126,
      "open_issues": 1,
      "size": 79
    },
    "tech_info": {
      "language": "TypeScript",
      "languages": {
        "TypeScript": 44178
      },
      "license": null,
      "topics": []
    },
    "content": {
      "readme": "# `danielroe/provenance-action`\n\nDetect and fail CI when dependencies in your lockfile lose npm provenance or trusted publisher status.\n\n> [!WARNING]\n> This action is under active development and is only one tool to assist in securing your dependencies.\n\n## ✨ Features\n- supports `pnpm-lock.yaml`, `package-lock.json`, `yarn.lock` (v1 and v2+), `bun.lock`\n- handles transitives by comparing resolved versions\n- inline GitHub annotations at the lockfile line\n- JSON output and optional hard‑fail (default: on)\n- pure TypeScript, Node 24+\n\n👉 See it in action: [danielroe/provenance-action-test](https://github.com/danielroe/provenance-action-test)\n\n## 🚀 Quick start\n```yaml\nname: ci\non:\n  pull_request:\n    branches:\n      - main\npermissions:\n  contents: read\njobs:\n  check-provenance:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n        with:\n          fetch-depth: 0\n      - name: Check provenance downgrades\n        uses: danielroe/provenance-action@main\n        id: check\n        with:\n          fail-on-provenance-change: true # optional, default: false\n        #   lockfile: pnpm-lock.yaml      # optional\n        #   base-ref: origin/main         # optional, default: origin/main\n        #   fail-on-downgrade: true       # optional, default: true\n      - name: Print result\n        run: \"echo 'Downgraded: ${{ steps.check.outputs.downgraded }}'\"\n```\n\n## 🔧 Inputs\n- `lockfile` (optional): Path to the lockfile. Auto-detected if omitted.\n- `workspace-path` (optional): Path to workspace root. Default: `.`\n- `base-ref` (optional): Git ref to compare against. Default: `origin/main`.\n- `fail-on-downgrade` (optional): Controls failure behavior. Accepts `true`, `false`, `any`, or `only-provenance-loss`. Default: `true` (which is the same as `any`).\n- `fail-on-provenance-change` (optional): When `true`, fail on provenance repository/branch changes. Default: `false`.\n\n## 📤 Outputs\n- `downgraded`: JSON array of `{ name, from, to, downgradeType }` for detected downgrades. `downgradeType` is `provenance` or `trusted_publisher`.\n- `changed`: JSON array of provenance change events `{ name, from, to, type, previousRepository?, newRepository?, previousBranch?, newBranch? }`.\n\n## 🧠 How it works\n1. Diffs your lockfile against the base ref and collects changed resolved versions (including transitives).\n2. Checks npm provenance via the attestations API for each `name@version`.\n3. Falls back to version metadata for `dist.attestations`.\n4. Emits file+line annotations in the lockfile.\n5. If provenance exists for both the previous and new version, extracts GitHub `owner/repo` and branch from attestations and warns when they differ (repo changed or branch changed).\n\n## 🔒 Why this matters\nTrusted publishing links a package back to its source repo and build workflow, providing strong provenance guarantees. It helps ensure the package you install corresponds to audited source and CI.\n\nHowever, maintainers can still be phished or coerced into publishing without trusted publishing enabled, or switching to a non‑trusted path. In those cases, packages may still carry attestations, but the chain back to the trusted publisher can be weakened.\n\nThis action:\n- Detects when a dependency update loses npm provenance (no attestations) or loses trusted publisher (attestations but no trusted publisher marker), and\n- Fails CI by default (configurable), before that change lands in your main branch.\n\nThis is a stopgap until package managers enforce stronger policies natively. Until then, it offers a lightweight guardrail in CI.\n\n## ⚠️ Notes\n- Runs on Node 24+ and executes the TypeScript entrypoint directly.\n- `bun.lockb` is not supported. (You can generate a `bun.lock` with `bun install --save-text-lockfile`.)\n- Repository and branch change detection is best‑effort; attestation shapes vary and some packages omit repo/ref details.\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-16T19:10:02.924965"
  },
  {
    "basic_info": {
      "name": "dsts",
      "full_name": "current-ai-llc/dsts",
      "owner": "current-ai-llc",
      "description": null,
      "url": "https://github.com/current-ai-llc/dsts",
      "clone_url": "https://github.com/current-ai-llc/dsts.git",
      "ssh_url": "git@github.com:current-ai-llc/dsts.git",
      "homepage": null,
      "created_at": "2025-09-15T00:03:28Z",
      "updated_at": "2025-09-16T19:03:21Z",
      "pushed_at": "2025-09-15T16:15:49Z"
    },
    "stats": {
      "stars": 55,
      "forks": 1,
      "watchers": 55,
      "open_issues": 1,
      "size": 52
    },
    "tech_info": {
      "language": "TypeScript",
      "languages": {
        "TypeScript": 51252
      },
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "# DSTS — Dynamic Self‑improving TypeScript\n\nDSTS is a minimal, AI SDK–aligned prompt optimizer for TypeScript. It optimizes prompts for both generateObject (with Zod schemas) and generateText with the latest GEPA optimizer (soft G like “giraffe”). GEPA evolves a prompt along a Pareto frontier across multiple objectives that matter in practice: task performance, latency, and cost.\n\n- AI Gateway–aligned: pass model ids as strings; generateObject with a Zod schema for a structured object or generateText with a string.\n- Minimal abstractions: one default adapter to call `generateObject`/`generateText` and one optimizer.\n- Multi‑objective first: correctness + latency (and cost) tracked per iteration; Pareto front and hyper‑volume (2D) reported.\n- Persistence & budgets: checkpoint/resume, per‑call cost estimation (via tokenlens) and budget caps, seeded minibatching.\n\n## Install\n\n```bash\nnpm i @currentai/dsts zod\n```\n\n## Quick start\n\n```ts\nimport { z } from \"zod\";\nimport { optimize, DefaultAdapterTask } from \"@currentai/dsts\";\n\n// Define schema (generateObject)\nconst Item = z.object({ title: z.string(), url: z.string().url() });\n\n// Training data: use schema ⇒ generateObject; provide expectedOutput and/or a scorer\nconst trainset: DefaultAdapterTask<z.infer<typeof Item>>[] = [\n  {\n    input: \"link to TS docs\",\n    expectedOutput: {\n      title: \"TypeScript\",\n      url: \"https://www.typescriptlang.org\",\n    },\n    schema: Item,\n  },\n];\n\nconst result = await optimize({\n  seedCandidate: { system: \"Extract a title and a valid URL from the text.\" },\n  trainset,\n  // Optional valset (defaults to trainset)\n  taskLM: \"openai/gpt-5-nano\",\n  reflectionLM: \"openai/o3\",\n  maxIterations: 5,\n  maxMetricCalls: 200,\n  maxBudgetUSD: 50,\n  reflectionMinibatchSize: 3,\n  candidateSelectionStrategy: \"pareto\",\n  componentSelector: \"round_robin\",\n  logger: {\n    log: (lvl, msg, data) => {\n      if (lvl === \"info\") console.log(`[${lvl}] ${msg}`, data || \"\");\n    },\n  },\n  persistence: {\n    dir: \"runs/quickstart\",\n    checkpointEveryIterations: 1,\n    resume: true,\n  },\n});\n\nconsole.log(\"Best system prompt:\", result.bestCandidate.system);\n```\n\n## Examples\n\n- Email extraction to a rich object schema: [`examples/email-extraction.ts`](file:///home/swiecki/coding/dsts/examples/email-extraction.ts)\n- Message spam classification: [`examples/message-spam.ts`](file:///home/swiecki/coding/dsts/examples/message-spam.ts)\n\nEach example:\n\n- Loads `.env` locally (AI Gateway by default),\n- Prints total iterations, metric calls, cost (USD), and duration (ms),\n- Enables persistence to `runs/...`.\n\nRun:\n\n```bash\nnpm run example              # email extraction\nnpm run example:message-spam # spam classification\n```\n\n## How it works\n\n- Default adapter decides generateObject vs generateText based on `schema` presence in each task; collects per‑instance scores, latency_ms, and cost_usd (via tokenlens when usage is available).\n- GEPA optimizer maintains a candidate archive, runs minibatch reflection, and accepts improving children. It computes per‑candidate metrics:\n  - correctness = average(score[])\n  - latency = −avg(latency_ms) (stored negative so higher is better)\n  - cost is tracked cumulatively and enforced via `maxBudgetUSD`.\n- Pareto front and 2D hyper‑volume (when exactly two objectives) are logged per iteration and at the end.\n\nKey files:\n\n- Optimizer: [`src/gepa.ts`](file:///home/swiecki/coding/dsts/src/gepa.ts#L1-L495)\n- Adapter: [`src/adapters/default-adapter.ts`](file:///home/swiecki/coding/dsts/src/adapters/default-adapter.ts#L1-L267) (default maxConcurrency = 10)\n- Pareto utilities: [`src/pareto-utils.ts`](file:///home/swiecki/coding/dsts/src/pareto-utils.ts#L1-L241)\n- Types: [`src/types.ts`](file:///home/swiecki/coding/dsts/src/types.ts#L1-L158)\n- Persistence: [`src/persistence.ts`](file:///home/swiecki/coding/dsts/src/persistence.ts#L1-L63)\n\n## Design choices\n\n- No custom LLM classes: pass model ids as strings (Gateway format, e.g., `openai/gpt-5-nano`). The adapter uses AI SDK directly.\n- Minimal knobs: set budgets (`maxMetricCalls`, `maxBudgetUSD`), minibatch size, and selectors. Concurrency defaults to 10.\n- Multi‑objective by default: we optimise for correctness and latency together; add cost as an explicit objective later if desired.\n\n## Environment\n\n- AI Gateway by default. Set `AI_GATEWAY_API_KEY` in `.env` or export it in your shell.\n- If you prefer provider‑direct, swap to `@ai-sdk/openai` models and pass model objects; the adapter will forward them.\n\n## Roadmap\n\n- Centralized eval helper for exact metric‑call counting and pre‑call budget gates.\n- Parent minibatch result reuse to avoid duplicate evaluations.\n- Extend hyper‑volume and objectives (e.g., cost as a third dimension) with explicit reference points.\n- Reflection concurrency (optional) and parent/child evaluation parallelism.\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-16T19:10:04.038759"
  },
  {
    "basic_info": {
      "name": "VibeVoice-finetuning",
      "full_name": "voicepowered-ai/VibeVoice-finetuning",
      "owner": "voicepowered-ai",
      "description": "Unofficial WIP LoRa Finetuning repository for VibeVoice",
      "url": "https://github.com/voicepowered-ai/VibeVoice-finetuning",
      "clone_url": "https://github.com/voicepowered-ai/VibeVoice-finetuning.git",
      "ssh_url": "git@github.com:voicepowered-ai/VibeVoice-finetuning.git",
      "homepage": null,
      "created_at": "2025-09-16T10:57:57Z",
      "updated_at": "2025-09-16T19:06:39Z",
      "pushed_at": "2025-09-16T16:31:47Z"
    },
    "stats": {
      "stars": 50,
      "forks": 9,
      "watchers": 50,
      "open_issues": 1,
      "size": 189
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 312954
      },
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "\r\n  \r\n\r\n# Unofficial WIP Finetuning repo for VibeVoice\r\n\r\n  \r\n\r\n# Hardware requirements\r\n\r\n  \r\n\r\nTo train a VibeVoice 1.5B LoRa, a machine with at least 16gb VRAM is recommended.\r\n\r\nTo train a VibeVoice 7B LoRa, a machine with at least 48gb VRAM is recommended.\r\n\r\nKeep in mind longer audios increase VRAM requirements\r\n\r\n  \r\n\r\n# Installation\r\n\r\nIt is recommended to install this in a fresh environment. Specifically, the Dockerized environment `runpod/pytorch:2.8.0-py3.11-cuda12.8.1-cudnn-devel-ubuntu22.04` has been tested to work.\r\n\r\n  \r\n\r\nTransformers version 4.51.3 is known to work, while other versions have errors related to Qwen2 architecture.\r\n\r\n  \r\n```\r\ngit clone https://github.com/voicepowered-ai/VibeVoice-finetuning\r\n\r\npip install -e .\r\n\r\npip uninstall -y transformers && pip install transformers==4.51.3\r\n\r\n(OPTIONAL) wandb login\r\n\r\n(OPTIONAL) export HF_HOME=/workspace/hf_models\r\n```\r\n\r\n  \r\n\r\n# Usage\r\n\r\n  \r\n\r\n## VibeVoice 1.5B / 7B (LoRA) fine-tuning\r\n\r\n  \r\n\r\n  \r\n\r\nWe put some code together for training VibeVoice (7B) with LoRA. This uses the vendored VibeVoice model/processor and trains with a dual loss: masked CE on text tokens plus diffusion MSE on acoustic latents.\r\n\r\n  \r\n\r\n  \r\n\r\nRequirements:\r\n\r\n  \r\n\r\n- Download a compatible VibeVoice 7B or 1.5b checkpoint (config + weights) and its processor files (preprocessor_config.json) or run straight from HF model.\r\n\r\n- A 24khz audio dataset with audio files (target audio), text prompts (transcriptions) and optionally voice prompts (reference audio)\r\n\r\n  \r\n\r\n  \r\n  \r\n\r\n### Training with Hugging Face Dataset\r\n\r\n  \r\n```\r\npython -m src.finetune_vibevoice_lora \\\r\n\r\n--model_name_or_path aoi-ot/VibeVoice-Large \\\r\n\r\n--processor_name_or_path src/vibevoice/processor \\\r\n\r\n--dataset_name your/dataset \\\r\n\r\n--text_column_name text \\\r\n\r\n--audio_column_name audio \\\r\n\r\n--voice_prompts_column_name voice_prompts \\\r\n\r\n--output_dir outputTrain3 \\\r\n\r\n--per_device_train_batch_size 8 \\\r\n\r\n--gradient_accumulation_steps 16 \\\r\n\r\n--learning_rate 2.5e-5 \\\r\n\r\n--num_train_epochs 5 \\\r\n\r\n--logging_steps 10 \\\r\n\r\n--save_steps 100 \\\r\n\r\n--eval_steps 100 \\\r\n\r\n--report_to wandb \\\r\n\r\n--remove_unused_columns False \\\r\n\r\n--bf16 True \\\r\n\r\n--do_train \\\r\n\r\n--gradient_clipping \\\r\n\r\n--gradient_checkpointing False \\\r\n\r\n--ddpm_batch_mul 4 \\\r\n\r\n--diffusion_loss_weight 1.4 \\\r\n\r\n--train_diffusion_head True \\\r\n\r\n--ce_loss_weight 0.04 \\\r\n\r\n--voice_prompt_drop_rate 0.2 \\\r\n\r\n--lora_target_modules q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj \\\r\n\r\n--lr_scheduler_type cosine \\\r\n\r\n--warmup_ratio 0.03 \\\r\n\r\n--max_grad_norm 0.8\r\n```\r\n  \r\n\r\n----------\r\n\r\n  \r\n\r\n### Training with Local JSONL Dataset\r\n\r\n  \r\n```\r\npython -m src.finetune_vibevoice_lora \\\r\n\r\n--model_name_or_path aoi-ot/VibeVoice-Large \\\r\n\r\n--processor_name_or_path src/vibevoice/processor \\\r\n\r\n--train_jsonl prompts.jsonl \\\r\n\r\n--text_column_name text \\\r\n\r\n--audio_column_name audio \\\r\n\r\n--output_dir outputTrain3 \\\r\n\r\n--per_device_train_batch_size 8 \\\r\n\r\n--gradient_accumulation_steps 16 \\\r\n\r\n--learning_rate 2.5e-5 \\\r\n\r\n--num_train_epochs 5 \\\r\n\r\n--logging_steps 10 \\\r\n\r\n--save_steps 100 \\\r\n\r\n--report_to wandb \\\r\n\r\n--remove_unused_columns False \\\r\n\r\n--bf16 True \\\r\n\r\n--do_train \\\r\n\r\n--gradient_clipping \\\r\n\r\n--gradient_checkpointing False \\\r\n\r\n--ddpm_batch_mul 4 \\\r\n\r\n--diffusion_loss_weight 1.4 \\\r\n\r\n--train_diffusion_head True \\\r\n\r\n--ce_loss_weight 0.04 \\\r\n\r\n--voice_prompt_drop_rate 0.2 \\\r\n\r\n--lora_target_modules q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj \\\r\n\r\n--lr_scheduler_type cosine \\\r\n\r\n--warmup_ratio 0.03 \\\r\n\r\n--max_grad_norm 0.8\r\n```\r\n\r\n\r\n### JSONL format:\r\n\r\n  \r\n\r\nYou can provide an optional `voice_prompts` key. If it is omitted, a voice prompt will be automatically generated from the target audio.\r\n\r\n  \r\n\r\n**Example without a pre-defined voice prompt (will be auto-generated):**\r\n\r\n`{\"text\": \"Speaker 0: Speaker0 transcription.\", \"audio\": \"/workspace/wavs/segment_000000.wav\"}`\r\n\r\n  \r\n\r\n**Example with a pre-defined voice prompt:**\r\n\r\n`{\"text\": \"Speaker 0: Speaker0 transcription.\", \"audio\": \"/workspace/wavs/segment_000000.wav\", \"voice_prompts\": \"/path/to/a/different/prompt.wav\"}`\r\n\r\n  \r\n\r\n**Example with multiple speakers and voice prompts:**\r\n\r\n`{\"text\": \"Speaker 0: How is the project coming along?\\nSpeaker 1: It's going well, we should be finished by Friday.\", \"audio\": \"/data/conversations/convo_01.wav\", \"voice_prompts\": [\"/data/prompts/alice_voice_prompt.wav\", \"/data/prompts/bob_voice_prompt.wav\"]}`\r\n\r\n  \r\n  \r\n  \r\n\r\n# Notes:\r\n\r\n  \r\n\r\n- Audio is assumed to be 24 kHz; input audio will be loaded/resampled to 24 kHz.\r\n\r\n  \r\n\r\n- If you pass raw NumPy arrays or torch Tensors as audio (without sampling rate metadata), the collator assumes they are already 24 kHz. To trigger resampling, provide dicts like {\"array\": <np.ndarray>, \"sampling_rate\": <int>} or file paths.\r\n\r\n  \r\n\r\n- Tokenizers (acoustic/semantic) are frozen by default. LoRA is applied to the LLM (Qwen) and optionally to the diffusion head.\r\n\r\n  \r\n\r\n- The collator builds interleaved seq",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-16T19:10:05.178059"
  },
  {
    "basic_info": {
      "name": "AI-Outfit-Change",
      "full_name": "AI-Banana/AI-Outfit-Change",
      "owner": "AI-Banana",
      "description": "Ever wish you could hit 'remove layer' in Real Life? Same. At least your Pics can do it Now.  https://tinyurl.com/removerapp Go Test It Out!",
      "url": "https://github.com/AI-Banana/AI-Outfit-Change",
      "clone_url": "https://github.com/AI-Banana/AI-Outfit-Change.git",
      "ssh_url": "git@github.com:AI-Banana/AI-Outfit-Change.git",
      "homepage": "",
      "created_at": "2025-09-15T13:30:02Z",
      "updated_at": "2025-09-16T19:04:24Z",
      "pushed_at": "2025-09-15T13:31:47Z"
    },
    "stats": {
      "stars": 49,
      "forks": 0,
      "watchers": 49,
      "open_issues": 0,
      "size": 0
    },
    "tech_info": {
      "language": null,
      "languages": {},
      "license": null,
      "topics": [
        "ai-nude",
        "ai-nude-generator",
        "ai-nudes"
      ]
    },
    "content": {
      "readme": "# [Test It Free!](https://tinyurl.com/removerapp)\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-16T19:10:06.290378"
  },
  {
    "basic_info": {
      "name": "MediLogic",
      "full_name": "praneethavaranasi/MediLogic",
      "owner": "praneethavaranasi",
      "description": "Doctor Consultation Platform",
      "url": "https://github.com/praneethavaranasi/MediLogic",
      "clone_url": "https://github.com/praneethavaranasi/MediLogic.git",
      "ssh_url": "git@github.com:praneethavaranasi/MediLogic.git",
      "homepage": null,
      "created_at": "2025-09-15T13:40:05Z",
      "updated_at": "2025-09-16T03:36:38Z",
      "pushed_at": "2025-09-15T14:34:13Z"
    },
    "stats": {
      "stars": 48,
      "forks": 23,
      "watchers": 48,
      "open_issues": 0,
      "size": 14322
    },
    "tech_info": {
      "language": "JavaScript",
      "languages": {
        "JavaScript": 96909,
        "CSS": 5804
      },
      "license": null,
      "topics": []
    },
    "content": {
      "readme": "***\n\n# Healthcare Website - At Home Guidance and Support (MEDILOGIC)\n\n## Project Overview\n\nMillions face difficulty accessing timely healthcare guidance for non-emergency symptoms, ongoing wellness, or medication management. This project introduces a **virtual healthcare assistant** designed to deliver support and guidance conveniently at home, aiming to enhance health outcomes and promote self-care.[^1]\n\n## Problem Statement\n\n- Long waiting times for medical consultations cause delays and inconvenience.[^1]\n- Healthcare is often inaccessible in remote or under-served areas, risking poor health outcomes for vulnerable populations.[^2][^4]\n- There is a growing need for easy-to-use, preventive tools for early symptom checking, medication reminders, and general health advice.\n\n\n## Objectives\n\n- Provide preliminary medical guidance and symptom assessment from home.\n- Reduce avoidable hospital visits for routine inquiries.\n- Offer medication reminders, wellness tips, and health monitoring.\n- Enable remote consultations and easy access to health information using video conferencing and seamless communication.\n\n\n## Key Features\n\n- Symptom checker and actionable health advice.\n- Medication reminders and tracking.\n- Personalized health tips, diet, and wellness recommendations.\n- Integration with wearable devices for automated health monitoring.\n- Emergency alerts and contact options for nearby medical facilities.\n- Secure messaging with healthcare professionals for remote consultation.\n\n\n## Technology Stack\n\n- Backend:Node.js,Next.js,Zod,React Hook form,pdf js and under development\n- Frontend: React,ShadCn UI, Acternity UI\n- Database: NeonDB,Prisma\n- Deployment:netlify.com\n- Video & Chat SDK: Still in development\n\n## Use Cases\n\n- A patient enters symptoms to receive preliminary advice and next steps.\n- Users set up daily medication reminders and wellness notifications.\n- Remote appointment booking or instant emergency alerts.\n- Integration with fitness/wearable devices for continuous monitoring.\n\n\n## Future Scope\n\n- Advanced AI-based diagnosis support.\n- Multilingual and region-specific health guidance.\n- Secure video/audio consultation with specialists.\n- Enhanced data security, privacy, and compliance with health data regulations.\n\n\n## Contributors\n\n- [KARTHIKEYA CHUNDURU]\n- [PRANEETHA VARANASI]\n  \nTEAM MEMBERS\n- [SUNAND CHOUDARY V]\n- [SAI SRIHITHA PICHIKA]\n- [GAURANG P KHASNE]\n- [VARANASI ADITYA]\n\n\n\n\n\nThis is a [Next.js](https://nextjs.org) project bootstrapped with [`create-next-app`](https://github.com/vercel/next.js/tree/canary/packages/create-next-app).\n\n## Getting Started\n\nFirst, run the development server:\n\n```bash\nnpm run dev\n# or\nyarn dev\n# or\npnpm dev\n# or\nbun dev\n```\n\nOpen [http://localhost:3000](http://localhost:3000) with your browser to see the result.\n\nYou can start editing the page by modifying `app/page.js`. The page auto-updates as you edit the file.\n\nThis project uses [`next/font`](https://nextjs.org/docs/app/building-your-application/optimizing/fonts) to automatically optimize and load [Geist](https://vercel.com/font), a new font family for Vercel.\n\n## Learn More\n\nTo learn more about Next.js, take a look at the following resources:\n\n- [Next.js Documentation](https://nextjs.org/docs) - learn about Next.js features and API.\n- [Learn Next.js](https://nextjs.org/learn) - an interactive Next.js tutorial.\n\nYou can check out [the Next.js GitHub repository](https://github.com/vercel/next.js) - your feedback and contributions are welcome!\n\n## Deploy on Vercel\n\nThe easiest way to deploy your Next.js app is to use the [Vercel Platform](https://vercel.com/new?utm_medium=default-template&filter=next.js&utm_source=create-next-app&utm_campaign=create-next-app-readme) from the creators of Next.js.\n\nCheck out our [Next.js deployment documentation](https://nextjs.org/docs/app/building-your-application/deploying) for more details.\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-16T19:10:07.402933"
  },
  {
    "basic_info": {
      "name": "simkit",
      "full_name": "Fallomai/simkit",
      "owner": "Fallomai",
      "description": null,
      "url": "https://github.com/Fallomai/simkit",
      "clone_url": "https://github.com/Fallomai/simkit.git",
      "ssh_url": "git@github.com:Fallomai/simkit.git",
      "homepage": null,
      "created_at": "2025-09-15T19:50:12Z",
      "updated_at": "2025-09-16T18:24:35Z",
      "pushed_at": "2025-09-15T21:07:25Z"
    },
    "stats": {
      "stars": 42,
      "forks": 1,
      "watchers": 42,
      "open_issues": 0,
      "size": 45
    },
    "tech_info": {
      "language": "TypeScript",
      "languages": {
        "TypeScript": 12375
      },
      "license": null,
      "topics": []
    },
    "content": {
      "readme": "# SimKit\n\n[![npm version](https://badge.fury.io/js/%40fallom%2Fsimkit.svg)](https://www.npmjs.com/package/@fallom/simkit)\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n[![TypeScript](https://img.shields.io/badge/TypeScript-007ACC?style=flat&logo=typescript&logoColor=white)](https://www.typescriptlang.org/)\n[![Downloads](https://img.shields.io/npm/dm/@fallom/simkit.svg)](https://www.npmjs.com/package/@fallom/simkit)\n\n> 🤖 **A TypeScript simulation framework for testing and running AI agents**\n\n## What is SimKit?\n\nSimKit lets you build, test, and run AI agents in your own custom simulated environments. It gives you a simple game loop for running agents step-by-step, supports multiple agents, and includes built-in tools (OTEL) for tracking what happens during your simulations.\n\n### Agent Agnostic & No Vendor Lock-in\n\nSimKit works with any AI agent or LLM, no lock-in. Use your own models and run everything locally. OTEL logs can be saved to a local file or sent to a remote server.\n\n### Why Use Simulations?\n\nSimulations let you see how your AI agents perform on real world tasks, step by step, in a safe and controlled way.\n\nTraditional evals are great for simple tasks, but they don't give you the full picture. You can't see how your agents handle:\n\n- 🎯 Multi-step tasks that need planning and memory\n- 🛠️ Lots of different tools and actions\n- 🌍 Realistic data and changing situations\n- ⚡ Decisions that matter over time\n- 🔄 Long-term planning and decision-making\n- 📚 Processing and reasoning over large amounts of context and information\n\nSurprisingly, most AI agents begin to fail when they are asked to do anything more than a few simple tasks.\n\n## 🔄 Core: The Simulation Loop\n\nSimKit's heart is a simple but powerful tick-based loop:\n\n```typescript\nimport { createSimulation, type LoopState } from \"@fallom/simkit/simulation\";\n\ninterface SupportTestState extends LoopState {\n  totalIssues: number;\n  resolvedIssues: number;\n  averageResponseTime: number;\n  satisfactionScores: number[];\n}\n\nconst customerIssues = [\n  \"My account is locked and I can't access my files\",\n  \"Billing error - charged twice for same month\", \n  \"App crashes every time I try to upload\",\n  \"Can't find my downloaded files anywhere\"\n];\n\nconst simulation = createSimulation<SupportTestState>({\n  maxTicks: 10,\n  initialState: { totalIssues: 0, resolvedIssues: 0, averageResponseTime: 0, satisfactionScores: [] },\n  \n  onTick: async (state) => {\n    // Get today's customer issues\n    const dailyIssues = getRandomIssues(customerIssues, 2);\n    \n    for (const issue of dailyIssues) {\n      const startTime = Date.now();\n      \n      // Test your AI support agent\n      const agentResponse = await supportAgent.handle(issue);\n      \n      const responseTime = Date.now() - startTime;\n      const satisfaction = scoreResponse(agentResponse, issue);\n      \n      state.totalIssues++;\n      if (satisfaction > 7) state.resolvedIssues++;\n      state.satisfactionScores.push(satisfaction);\n      \n      // Update running averages\n      const avgSatisfaction = state.satisfactionScores.reduce((a,b) => a+b, 0) / state.satisfactionScores.length;\n      const resolutionRate = (state.resolvedIssues / state.totalIssues) * 100;\n      \n      console.log(`Resolution Rate: ${resolutionRate.toFixed(1)}% | Avg Satisfaction: ${avgSatisfaction.toFixed(1)}/10`);\n    }\n    \n    return state.tick < 9; // Test for 10 days\n  },\n  \n  onEnd: (state) => {\n    const finalSatisfaction = state.satisfactionScores.reduce((a,b) => a+b, 0) / state.satisfactionScores.length;\n    console.log(`🎯 Final Results: ${((state.resolvedIssues/state.totalIssues)*100).toFixed(1)}% resolution rate, ${finalSatisfaction.toFixed(1)}/10 satisfaction`);\n  }\n});\n\nawait simulation.run();\n```\n\n**What's happening here?** Each tick simulates a day of customer support. SimKit feeds random issues to your AI agent, measures response quality and speed, then tracks KPIs over time. Perfect for A/B testing different models, regression testing after prompt changes, or measuring performance before production deployment.\n\n## 🤖 Built for AI Agents\n\n### Global State Access\nAI agents need access to simulation state from anywhere:\n\n```typescript\nimport { setSimState, getSimState } from \"@fallom/simkit/state\";\n\n// In your simulation loop\nsetSimState(state);\n\n// In your AI tools\nconst currentState = getSimState<MyState>();\n```\n\n### Deterministic Testing\nReproduce exact scenarios with seeded randomness - perfect for fair model comparisons:\n\n```typescript\nimport { initializeRandom, choice, shuffle } from \"@fallom/simkit/random\";\n\n// Test Model A\ninitializeRandom(12345); // Same seed = same test scenarios\nconst modelA_results = await testSupportAgent(modelA);\n\n// Test Model B with identical scenarios\ninitializeRandom(12345); // Reset to same seed\nconst modelB_results = await testSupportAgent(modelB);\n\n// Now you can fairly compare: both models faced the exact same issues\nconsole.log(`Model",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-16T19:10:08.513339"
  },
  {
    "basic_info": {
      "name": "hometube",
      "full_name": "EgalitarianMonkey/hometube",
      "owner": "EgalitarianMonkey",
      "description": "HomeTube is a featured friendly video downloader managing video URL into organized content for home media library.",
      "url": "https://github.com/EgalitarianMonkey/hometube",
      "clone_url": "https://github.com/EgalitarianMonkey/hometube.git",
      "ssh_url": "git@github.com:EgalitarianMonkey/hometube.git",
      "homepage": "",
      "created_at": "2025-09-15T09:25:00Z",
      "updated_at": "2025-09-16T18:36:05Z",
      "pushed_at": "2025-09-16T13:09:40Z"
    },
    "stats": {
      "stars": 42,
      "forks": 0,
      "watchers": 42,
      "open_issues": 2,
      "size": 79466
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 156976,
        "Makefile": 7425,
        "Dockerfile": 1560,
        "Shell": 776
      },
      "license": "GNU Affero General Public License v3.0",
      "topics": [
        "docker",
        "homelab",
        "jellyfin",
        "open-source",
        "plex",
        "selfhosted",
        "sponsorblock",
        "youtube-downloader",
        "yt-dlp"
      ]
    },
    "content": {
      "readme": "<div align=\"center\">\n\n<br/>\n\n# 🎬 HomeTube\n\n<br/>\n\n[![Python](https://img.shields.io/badge/Python-3.10+-blue.svg)](https://python.org)\n[![Streamlit](https://img.shields.io/badge/Streamlit-1.49+-red.svg)](https://streamlit.io)\n[![Latest Release](https://img.shields.io/github/v/release/EgalitarianMonkey/hometube)](https://github.com/EgalitarianMonkey/hometube/releases)\n[![Docker Image](https://ghcr-badge.egpl.dev/egalitarianmonkey/hometube/latest_tag?trim=major&label=Docker)](https://github.com/EgalitarianMonkey/hometube/pkgs/container/hometube)\n[![License](https://img.shields.io/badge/License-AGPL--3.0-green.svg)](LICENSE)\n\n<br/>\n\n**🌐 Universal Video Downloader for your HomeLab**\n\n*Download, process and organize videos at Home*\n\n<br/>\n\n</div>\n\n<br/>\n<br/>\n\n<!-- --- -->\n\n<!-- ## 🎯 What is HomeTube? -->\n\n\n🎬 HomeTube is a simple web UI for downloading single videos from the internet with the highest quality available and moving them to specific local locations automatically managed and integrated by media server such as Plex or Jellyfin.\n\nA simple friendly solution for easily integrating preferred videos from Youtube and others platforms to local media server.\n\n### 🏠 **HomeLab Integration**\n- **🎬 Media server Ready**: Download best quality videos with explicit name and location directly in your HomeLab media server structure and get automatic watch experience on Plex, Jellyfin, Emby or even on your PC\n- **📱 Network Access**: Web interface videos download accessible from any device on your network\n\n### ⚡ **Features**\n- **🎯 One-Click Downloads**: Paste URL → Get perfectly organized video\n- **🚫 Ad-Free Content**: Block videos' sponsors and ads\n- **🎬 Advanced Processing**: Cut clips, embed subtitles, convert formats\n- **🔐 Unlock restricted videos**: Cookies support for member-only videos, restricted age, etc.\n- **📊 Quality Control**: Auto-select best quality or manual override\n- **🎥 Video Sources**: **YouTube**, Reddit, Vimeo, Dailymotion, TikTok, Twitch, Facebook, Instagra, etc. [See complete list (1800+)](docs/supported-platforms.md)\n\n<!-- ## ⚡ Technical Highlights\n\n<div align=\"center\">\n\n| 🎯 **Easy to Use** | 🔧 **Powerful** | 🏠 **HomeLab Ready** |\n|:---:|:---:|:---:|\n| Web interface | 1800+ platforms | Docker deployment |\n| One-click downloads | Advanced processing | Network accessible |\n| Auto-organization | Cookie authentication | Plex/Jellyfin ready |\n\n</div> -->\n\n<!-- --- -->\n\n<br/>\n<br/>\n\n![Application Demo](./docs/images/simple_ui_demo.gif)\n\n<br/>\n<br/>\n\n<!-- --- -->\n\n## 🛠️ HomeTube Options\n\n### 🚫 SponsorBlock Integration\n\n**Automatically skip sponsors, ads, and promotional content** with built-in SponsorBlock support. Just download your video and sponsors segments are automatically detected and marked.\n\n- ✅ **Auto-detection**: Sponsors segments automatically identified\n- ✅ **Manage sponsors to block**: Sponsors segments to block or mark can be managed in the UI\n- ✅ **Community-driven**: Powered by SponsorBlock's crowd-sourced database\n- ✅ **Zero configuration**: Works out of the box for YouTube videos\n\n[Learn more about SponsorBlock features](docs/usage.md#-sponsorblock-integration).\n\n### 🏠 HomeLab Integration\n\n**Perfect integration with self-hosted setup**:\n\n- **🐳 Docker Ready**: One-command deployment with Docker Compose\n- **🎬 Media Server Integration**: Direct integration with media server thanks to well named video files automatically moved to chosen locations watched by media server such as Plex, Jellyfin, or Emby.\n- **📱 Network Access**: Web interface accessible from any device on your network\n- **🔐 Secure**: No cloud dependencies, everything runs locally\n- **⚙️ Configurable**: Extensive customization through environment variables\n\n[Setup your HomeLab integration](docs/deployment.md).\n\n### 🍪 Unlock restricted videos (Cookies)\n\nPrivate content, age-restricted, or member-only videos are restricted without authentication on platforms like YouTube. We can unlock restricted content thanks to **cookies** authentication.\n\nWe can use **Browser cookies** if on a machine sharing a browser, otherwise **Cookies File** in HomeLab setup.\n\n[More details about Cookies authentication setup](docs/usage.md#-authentication--private-content).\n\n### ✂️ Advanced Video Processing\n\nTransform your downloads with **powerful built-in video processing tools**:\n\n- **🎬 Clip Extraction**: Cut specific segments from videos with precision timing\n- **📝 Subtitle Embedding**: Automatically embed subtitles in multiple languages\n- **🔄 Format Conversion**: Convert between video formats (MP4, MKV, WebM, etc.)\n- **🎵 Audio Extraction**: Extract audio-only versions in high quality\n- **📱 Mobile Optimization**: Optimize videos for mobile devices\n\n[Explore all processing options](docs/usage.md#-video-processing).\n\n### 🎯 Smart Download Management\n\n**Intelligent download system** that adapts to your needs:\n\n- **🏆 Quality Selection**: Auto-select best quality or manual override\n- **📁 Auto-Organization**: Videos organized by channel/creator automatical",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-16T19:10:09.630945"
  },
  {
    "basic_info": {
      "name": "CRT_Python_AI_A",
      "full_name": "gilshan-s/CRT_Python_AI_A",
      "owner": "gilshan-s",
      "description": "Coding",
      "url": "https://github.com/gilshan-s/CRT_Python_AI_A",
      "clone_url": "https://github.com/gilshan-s/CRT_Python_AI_A.git",
      "ssh_url": "git@github.com:gilshan-s/CRT_Python_AI_A.git",
      "homepage": null,
      "created_at": "2025-09-16T04:28:17Z",
      "updated_at": "2025-09-16T09:35:22Z",
      "pushed_at": "2025-09-16T05:24:03Z"
    },
    "stats": {
      "stars": 38,
      "forks": 34,
      "watchers": 38,
      "open_issues": 0,
      "size": 2
    },
    "tech_info": {
      "language": null,
      "languages": {},
      "license": null,
      "topics": []
    },
    "content": {
      "readme": "# CRT_Python_AI_A\nCoding\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-16T19:10:10.735952"
  },
  {
    "basic_info": {
      "name": "LLaVA-OneVision-1.5",
      "full_name": "EvolvingLMMs-Lab/LLaVA-OneVision-1.5",
      "owner": "EvolvingLMMs-Lab",
      "description": null,
      "url": "https://github.com/EvolvingLMMs-Lab/LLaVA-OneVision-1.5",
      "clone_url": "https://github.com/EvolvingLMMs-Lab/LLaVA-OneVision-1.5.git",
      "ssh_url": "git@github.com:EvolvingLMMs-Lab/LLaVA-OneVision-1.5.git",
      "homepage": null,
      "created_at": "2025-09-16T14:05:47Z",
      "updated_at": "2025-09-16T18:42:50Z",
      "pushed_at": "2025-09-16T14:10:29Z"
    },
    "stats": {
      "stars": 37,
      "forks": 1,
      "watchers": 37,
      "open_issues": 1,
      "size": 2636
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 7572960,
        "Jupyter Notebook": 521664,
        "Shell": 178138,
        "C++": 38717,
        "Cuda": 16941,
        "C": 2951,
        "Dockerfile": 2631,
        "HTML": 2625,
        "Makefile": 313
      },
      "license": null,
      "topics": []
    },
    "content": {
      "readme": "# LLaVA-OneVision-1.5: Fully Open Framework for Democratized Multimodal Training\n\n\n[🤗 Mid-Training-Data (Uploading!)](https://huggingface.co/datasets/lmms-lab/LLaVA-One-Vision-1.5-Mid-Training-85M) | \n[🤗 Insturct-Data (Uploading!)](https://huggingface.co/datasets/lmms-lab/LLaVA-One-Vision-1.5-Insturct-26M) \n\n**LLaVA-OneVision1.5** introduces a novel family of **fully open-source** Large Multimodal Models (LMMs) that achieves **state-of-the-art performance**  with substantially **lower cost** through training on **native resolution** images.\n\n1. **Superior Performance**\nA family of fully open-source large multimodal models demonstrating **superior performance** across multiple multimodal benchmarks, **outperforming Qwen2.5-VL** in most evaluation tasks.\n\n2. **High-Quality Data at Scale**\nMeticulously curated **pre-training and SFT data** with rigorous filtering and quality control, achieving **superior data efficiency** with only **64B tokens**.\n- Concept-balanced, highly diverse, high-quality caption data\n- Comprehensive instruction fine-tuning data covering a wide range of tasks\n\n3. **Ultra-Efficient Training Framework**\nComplete end-to-end training framework designed for maximum efficiency:\n- **$16K total budget** for full model training\n- **45% HFU efficiency** on A100 GPUs ($0.6 per GPU/Hour)\n- Built on **MegatronLM** with support for **MoE**, **FP8**, and **long sequence parallelization**\n- Optimized codebase for cost-effective scaling\n\n4. **Fully Open Framework** for community access and reproducibility:\n- ✅ High-quality pre-training & SFT data\n- ✅ Complete training framework & code\n- ✅ Training recipes & configurations\n- ✅ Base & instruct model checkpoints\n- ✅ Comprehensive training logs & metrics\n\n\n## Model\n\n| Model                  | #Vision Param | #Language Param | #Total Param | HF Link                                                                      |\n|------------------------|---------------|-----------------|--------------|------------------------------------------------------------------------------|\n| LLaVA-OV-1.5-4B-Instruct      | 0.3B          | 4.4B            | 4.7B         | [🤗 link]()                |\n| LLaVA-OV-1.5-8B-Instruct      | 0.3B          | 8.2B            | 8.5B         | [🤗 link](https://huggingface.co/lmms-lab/LLaVA-One-Vision-1.5-8B-Instruct)                |\n\n\n## Dataset\n\n![Dataset Visualization](asset/dataset.jpg)\n\n\n| Description | Link |\n|-------------|------|\n| Mid-training data for LLaVA-OneVision-1.5 | [🤗 Download (Uploading!)](https://huggingface.co/datasets/lmms-lab/LLaVA-One-Vision-1.5-Mid-Training-85M) |\n| SFT data for LLaVA-OneVision-1.5 | [🤗 Download (Uploading!)](https://huggingface.co/datasets/lmms-lab/LLaVA-One-Vision-1.5-Insturct-26M) |\n\n\n## Evaluation Results\n\n\nAll evaluations were conducted using lmms_eval.\n\n|                                  | **LLaVA-OV-1.5-8B** | **Qwen2.5 VL 7B** | **LLaVA-OV-1.5-4B** | **Qwen2.5 VL 3B** |\n|:----------------------------------|:---------------:|:-------------:|:---------------:|:-------------:|\n| MMMU (Validation)                 |    **55.44**    |     51.33     |    **51.44**    |     46.44     |\n| MMMU-Pro (Standard)               |    **37.40**    |     36.30     |    **33.24**    |     31.10     |\n| MMMU-Pro (Vision)                 |      25.15      |   **32.83**   |    **23.53**    |     21.27     |\n| MMBench (English; Test)           |    **84.14**    |     83.40     |    **82.29**    |     77.97     |\n| MMBench (Chinese; Test)           |      81.00      |   **81.61**   |    **76.73**    |     74.55     |\n| MME-RealWorld (English)           |    **62.31**    |     57.33     |    **57.16**    |     51.60     |\n| MME-RealWorld (Chinese)           |    **56.11**    |     51.50     |      21.38      |   **45.38**   |\n| AI2D (With Mask)                  |    **84.16**    |     82.58     |    **84.62**    |     78.56     |\n| AI2D (Without Mask)               |    **94.11**    |     93.36     |    **92.84**    |     90.74     |\n| CV-Bench                          |    **80.82**    |     79.95     |    **74.00**    |     71.53     |\n| VL-RewardBench                    |      45.90      |   **49.65**   |    **45.90**    |     42.06     |\n| V*                                |    **78.01**    |     76.96     |      66.49      |   **69.63**   |\n| PixmoCount                        |      62.19      |   **63.33**   |    **59.17**    |     50.85     |\n| CountBench                        |    **88.19**    |     86.35     |    **77.80**    |     72.51     |\n| ChartQA                           |    **86.48**    |     84.08     |    **85.11**    |     83.36     |\n| CharXiv (Direct Questions)        |    **74.10**    |     69.80     |    **70.70**    |     58.20     |\n| DocVQA (Test)                     |    **95.00**    |     94.93     |    **93.48**    |     92.67     |\n| InfoVQA (Test)                    |      78.42      |   **81.67**   |    **75.27**    |     75.63     |\n| WeMath                            |    **33.6",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-16T19:10:11.849914"
  },
  {
    "basic_info": {
      "name": "anthropic-claude-max-proxy",
      "full_name": "Pimzino/anthropic-claude-max-proxy",
      "owner": "Pimzino",
      "description": "OpenAI-compatible proxy for Claude Pro/Max subscriptions using OAuth.",
      "url": "https://github.com/Pimzino/anthropic-claude-max-proxy",
      "clone_url": "https://github.com/Pimzino/anthropic-claude-max-proxy.git",
      "ssh_url": "git@github.com:Pimzino/anthropic-claude-max-proxy.git",
      "homepage": "",
      "created_at": "2025-09-15T00:52:57Z",
      "updated_at": "2025-09-16T18:50:43Z",
      "pushed_at": "2025-09-16T12:02:00Z"
    },
    "stats": {
      "stars": 34,
      "forks": 8,
      "watchers": 34,
      "open_issues": 0,
      "size": 69
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 52223
      },
      "license": "Other",
      "topics": []
    },
    "content": {
      "readme": "# Anthropic Claude Max Proxy\n\nPure Anthropic proxy for Claude Pro/Max subscriptions using OAuth.\n\n## SUPPORT MY WORK\n<a href=\"https://buymeacoffee.com/Pimzino\" target=\"_blank\"><img src=\"https://cdn.buymeacoffee.com/buttons/v2/default-yellow.png\" alt=\"Buy Me A Coffee\" style=\"height: 60px !important;width: 217px !important;\" ></a>\n\n## DISCLAIMER\n\n**FOR EDUCATIONAL PURPOSES ONLY**\n\nThis tool:\n- Is NOT affiliated with or endorsed by Anthropic\n- Uses undocumented OAuth flows from Claude Code\n- May violate Anthropic's Terms of Service\n- Could stop working at any time without notice\n- Comes with NO WARRANTY or support\n\n**USE AT YOUR OWN RISK. The authors assume no liability for any consequences.**\n\nFor official access, use Claude Code or Anthropic's API with console API keys.\n\n## Prerequisites\n\n- Active Claude Pro or Claude Max subscription\n- Python 3.8+\n- pip\n\n## Quick Start\n\n1. **Virtual Environment Setup (Recommended)**\n```bash\npython -m venv venv\n```\n\n2. **Install:**\n```bash\nvenv/Scripts/Activate.ps1\npip install -r requirements.txt\n```\n\n3. **Configure (optional):**\n```bash\ncp config.example.json config.json\n```\n\n4. **Run:**\n```bash\npython cli.py\n```\n\n5. **Authenticate:**\n- Select option 2 (Login)\n- Browser opens automatically\n- Complete login at claude.ai\n- Copy the authorization code\n- Paste in terminal\n\n6. **Start proxy:**\n- Select option 1 (Start Proxy Server)\n- Server runs at `http://127.0.0.1:8081`\n\n## Client Configuration\n\nConfigure your Anthropic API client:\n\n- **Base URL:** `http://127.0.0.1:8081`\n- **API Key:** Any non-empty string (e.g., \"dummy\")\n- **Model:** `claude-sonnet-4-20250514` (or any available Claude model)\n- **Endpoint:** `/v1/messages`\n\n## Available Models\n\n- `claude-sonnet-4-20250514` - Claude 4 Sonnet (latest) **[RECOMMENDED]**\n- `claude-3-5-sonnet-20241022` - Claude 3.5 Sonnet (latest)\n- `claude-3-5-haiku-20241022` - Claude 3.5 Haiku (latest)\n- `claude-3-opus-20240229` - Claude 3 Opus\n- See proxy startup output for complete model list\n\n## Configuration Priority\n\n1. Environment variables (highest)\n2. config.json file\n3. Built-in defaults (lowest)\n\n## License\n\nMIT License - see [LICENSE](LICENSE) file\n\nThis software is provided for educational purposes only. Users assume all risks.\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-16T19:10:12.968570"
  },
  {
    "basic_info": {
      "name": "SOLANA_SNIPER_BOT",
      "full_name": "0xKronus/SOLANA_SNIPER_BOT",
      "owner": "0xKronus",
      "description": "A Solana sniper in the booming SOL ecosystem – a sophisticated automated tool built to instantly detect and acquire newly listed tokens.",
      "url": "https://github.com/0xKronus/SOLANA_SNIPER_BOT",
      "clone_url": "https://github.com/0xKronus/SOLANA_SNIPER_BOT.git",
      "ssh_url": "git@github.com:0xKronus/SOLANA_SNIPER_BOT.git",
      "homepage": "",
      "created_at": "2025-09-16T03:46:30Z",
      "updated_at": "2025-09-16T05:44:08Z",
      "pushed_at": "2025-09-16T04:52:11Z"
    },
    "stats": {
      "stars": 32,
      "forks": 209,
      "watchers": 32,
      "open_issues": 0,
      "size": 1263
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 446988,
        "PHP": 3934
      },
      "license": null,
      "topics": [
        "memecoin-sniping-bot",
        "pumpfun",
        "pumpfun-sniper-go",
        "raydium",
        "solana-memecoin-sniper-bot",
        "solana-rug",
        "solana-sniper",
        "solana-sniping",
        "solana-trading",
        "solanamemecoines",
        "solanamevbot"
      ]
    },
    "content": {
      "readme": "<h1 align=\"center\"> SOLANA BOT </h1> <br>\n<p align=\"center\">\n  <a href=\"\"> \n    \n  </a>  \n</p>\n \n<p align=\"center\">\n  A Bot in your pocket based on take profit or buy/sell on Raydium.\n</p>\n\n\n<!-- START doctoc generated TOC please keep comment here to allow auto update -->\n<!-- DON'T EDIT THIS SECTION, INSTEAD RE-RUN doctoc TO UPDATE -->\n## Table of Contents\n\n- [Introduction](#introduction)\n- [Features](#features)\n- [Usage](#Usage)\n- [Setting](#Setting-)\n- [Disclaimer ](#Disclaimer)\n- [Contact ](#Contact)\n\n<!-- END doctoc generated TOC please keep comment here to allow auto update -->\n\n## Introduction\n\nThe Solana Sniper/Trading Bot is a groundbreaking tool in the booming Solana ecosystem, designed to tackle a common issue faced by traders: missing out on profit opportunities after purchasing tokens on the Solana network, leading to token rug-pulls or dumps. This software not only integrates sniping functionality, allowing users to instantly acquire tokens upon their launch but also adds trading tools to optimize one’s position.\n\n**Available for both iOS and Android and PC .**\n\n\n![image](https://github.com/user-attachments/assets/8b825c7d-1f6e-4178-a68c-af6c4dc4877d)\n\n\n\n<p align=\"center\">\n  <a href =\"https://t.me/z3Zrsolana\">\n  \n  </a>\n</p>\n\n## Features\n\nA few of the things you can do with Bot:\n\n- Sniping: Execute buy transactions instantly when liquidity is added to an SPL token, ensuring you're among the first to buy in promising new tokens.\n- Take Profit: Automatically sell tokens at a predefined profit percentage, securing gains.\n- Buy/Sell x Times: Execute repeated buy orders to average down or scale into positions.\n- Sell Limit Order: Set your tokens to sell automatically at a predetermined price, locking in profits.\n- User friendly interface - hands-on interface\n- **Making the first to trade in new tokens.**\n\n<img width=\"1010\" height=\"269\" alt=\"Screenshot 2025-09-16 at 11 30 40\" src=\"https://github.com/user-attachments/assets/e99f66c4-26f6-412a-b5b5-b6b3995a8873\" />\n\n<img width=\"1009\" height=\"272\" alt=\"Screenshot 2025-09-16 at 11 31 13\" src=\"https://github.com/user-attachments/assets/982a188b-93a1-4626-b00d-324f0010659f\" />\n\n\n<img width=\"910\" alt=\"2\" src=\"https://github.com/user-attachments/assets/a85bd1f2-c152-42a3-8b27-c3bb31cb59e2\">\n\n\n## Installation\n\n- Downloads Python ( Recommend the latest version )  [Python 3.13.7](https://www.python.org/downloads/)\n-  ***VERY IMPORTANT***: When installing Python also install **\"Add python.exe to path\"** and ***\"Use admin privileges when installing py.exe:*** => Tick\n\n## Usage\n<video src=\"https://github.com/user-attachments/assets/fab474c3-3c18-40be-9fe8-9b108dad738e\" width=\"320\" height=\"240\" controls></video>\n\n\n\n\n- Update `pip` Run the following command to update pip to the latest version\n\n```python\npython -m pip install --upgrade pip\n```\n- Clone or download the project\n\n```git \ngit clone https://github.com/0xKronus/SOLANA_SNIPER_BOT.git\n```\n\nOption 2: Download the project directly\n\nGo to the project's GitHub page, click the \"Code\" button and select \"Download ZIP\". Unzip the downloaded ZIP file to get the project folder.\n\n- Navigate to the project folder\n\nOpen a terminal and navigate to the project folder\n\n```python\ncd SOLANA_SNIPER_BOT\n```\n\n- Install libraries\n\nRun the following command to install the required libraries for the project:\n\n```python\npip install -r requirements.txt\n```\n\n- Run the project\n\nRun the following command to start the project:\n\n\n\n```python\npython main.py\n```\n\n\n\n## Setting\n- **BALANCE** : Show Balance & Profit\n- **BUY DELAY** : In seconds after launch. Set to 0, Token will buy immediately after token launch\n- **TAKE PROFIT** : Take-Profit Order (TP) . Token places a sell order and confirms immediately after reaching the target\n- **SELL DELAY** : to the number of seconds you want to wait before selling the token. Set to 0, token will be sold immediately after it is bought.\n- **CHECK RUG** : Set to true to check the risk score and protect against rug pulls.\n\n\n\nExample: \n\n<img width=\"1176\" alt=\"s\" src=\"https://github.com/user-attachments/assets/97d97112-703d-48f8-8075-a2de60d85cb1\">\n\n\n![image](https://github.com/user-attachments/assets/8b825c7d-1f6e-4178-a68c-af6c4dc4877d)\n\n\n\n\n\n  \n\n\n\n## Disclaimer\n\n- This extension is not affiliated with Solana Foundation or Solana Labs. It is a non-profit community project.\n- Solana Snipe is in active development, so all the snippets are subject to change.\n- The snippets are unaudited. Use at your own risk.\n\n![Happy_GIF](https://media.giphy.com/media/erePhJFWkfYMwTpNT8/giphy.gif) \n",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-16T19:10:14.101049"
  },
  {
    "basic_info": {
      "name": "kipm_cs_javascript",
      "full_name": "debasishsahoo/kipm_cs_javascript",
      "owner": "debasishsahoo",
      "description": null,
      "url": "https://github.com/debasishsahoo/kipm_cs_javascript",
      "clone_url": "https://github.com/debasishsahoo/kipm_cs_javascript.git",
      "ssh_url": "git@github.com:debasishsahoo/kipm_cs_javascript.git",
      "homepage": null,
      "created_at": "2025-09-15T04:33:59Z",
      "updated_at": "2025-09-16T15:42:35Z",
      "pushed_at": "2025-09-16T10:48:45Z"
    },
    "stats": {
      "stars": 31,
      "forks": 11,
      "watchers": 31,
      "open_issues": 0,
      "size": 31
    },
    "tech_info": {
      "language": "JavaScript",
      "languages": {
        "JavaScript": 29593,
        "HTML": 7156,
        "CSS": 742
      },
      "license": null,
      "topics": []
    },
    "content": {
      "readme": "",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-16T19:10:15.206289"
  },
  {
    "basic_info": {
      "name": "mint-mono",
      "full_name": "yuru7/mint-mono",
      "owner": "yuru7",
      "description": "開発者向けフォントの Intel One Mono と日本語フォントの Circle M+ 等を合成したプログラミング向けフォント 「Mint Mono」",
      "url": "https://github.com/yuru7/mint-mono",
      "clone_url": "https://github.com/yuru7/mint-mono.git",
      "ssh_url": "git@github.com:yuru7/mint-mono.git",
      "homepage": "",
      "created_at": "2025-09-15T07:12:35Z",
      "updated_at": "2025-09-16T12:48:18Z",
      "pushed_at": "2025-09-15T07:23:57Z"
    },
    "stats": {
      "stars": 31,
      "forks": 0,
      "watchers": 31,
      "open_issues": 0,
      "size": 8910
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 48975,
        "PowerShell": 2629
      },
      "license": "SIL Open Font License 1.1",
      "topics": []
    },
    "content": {
      "readme": "# Mint Mono\n\nMint Mono は、開発者向けフォントの [Intel One Mono](https://github.com/intel/intel-one-mono) と日本語フォントの [Circle M+](https://itouhiro.github.io/mixfont-mplus-ipa/mplus/) 等を合成したプログラミング向けフォントです。\n\n視認性が高く機能的な Intel One Mono と、同じく視認性・機能性に優れた日本語フォント Circle M+ (M+ FONTS の派生版) を組み合わせました。\n\n[👉 ダウンロード](https://github.com/yuru7/mint-mono/releases/latest)  \n※「Assets」内の zip ファイルをダウンロードしてご利用ください。\n\n## 特徴\n\n以下の特徴を備えています。\n\n- Intel One Mono 由来の視認性が高く読みやすいラテン文字\n- Circle M+ 由来のモダンで読み易い日本語文字\n    - Circle M+ では元々読みやすい M+ FONTS をさらに発展させ、以下のような特徴を持っています\n        - 半濁点が大きい\n        - カ力 エ工 ロ口 ー一 ニ二 へヘ の区別\n        - 〜～（波ダッシュ・全角チルダ）の区別\n- 全角スペースの可視化\n- [IBM Plex Sans JP](https://github.com/IBM/plex) を追加合成することで「Adobe-Japan1-7」文字コレクションに対応\n\n### バリエーション\n\n| 種類 | 説明 | 命名パターン |\n| --- | --- | --- |\n| 文字幅比率 半角1:全角2 | Intel One Mono を縮小することで、半角1:全角2の文字幅比率となるように合成したバリエーション。 | `MintMono-*.ttf`<br>※ファイル名に `35` が含まれて **いない** もの |\n| 文字幅比率 半角3:全角5 | Intel One Mono を縮小せずに合成し、半角3:全角5の文字幅比率としたバリエーション。半角1:全角2と比べ、英数字などの半角文字がゆとりのある幅で表示される。| `MintMono35-*.ttf`<br>※ファイル名に `35` が含まれて **いる** もの |\n\n## 表示サンプル\n\n| 通常版 (幅比率 半角1:全角2) | 35版 (幅比率 半角3:全角5) |\n| :---: | :---: |\n| <img width=\"839\" height=\"474\" alt=\"image\" src=\"https://github.com/user-attachments/assets/d74c6526-d9f2-458d-b9f3-d4136410d5c3\" /> | <img width=\"812\" height=\"479\" alt=\"image\" src=\"https://github.com/user-attachments/assets/32a5b04f-0c5a-41e9-bf7b-ca83e7c5d94c\" /> |\n\n## ビルド\n\n### 環境\n\n- [FontForge](https://fontforge.org/en-US/)\n- Python 3.x\n\n### 依存パッケージのインストール\n\n```shell\npip install fonttools\n```\n\n### ビルドの実行 (PowerShell)\n\n```powershell\n./make.ps1\n```\n\nスクリプトが正常に完了すると、`build` ディレクトリにフォントファイルが生成されます。\n\n### ビルドオプション\n\n`make.ps1` を編集することで、`fontforge_script.py` に以下のオプションを渡して、生成するフォントをカスタマイズできます。\n\n- `--35`: 半角3:全角5 の幅にする\n- `--hidden-zenkaku-space`: 全角スペース可視化をしない\n\n## ライセンス\n\nSIL OPEN FONT LICENSE Version 1.1 が適用され、商用・非商用問わず利用可能です。\n\n各合成元フォントのライセンスは [source](./source/) ディレクトリに格納されています。\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-16T19:10:16.313320"
  },
  {
    "basic_info": {
      "name": "free-sqlite",
      "full_name": "fjb040911/free-sqlite",
      "owner": "fjb040911",
      "description": "Free SQLite for VSCode.Support writing SQL statements",
      "url": "https://github.com/fjb040911/free-sqlite",
      "clone_url": "https://github.com/fjb040911/free-sqlite.git",
      "ssh_url": "git@github.com:fjb040911/free-sqlite.git",
      "homepage": null,
      "created_at": "2025-09-16T06:20:19Z",
      "updated_at": "2025-09-16T18:23:34Z",
      "pushed_at": "2025-09-16T10:16:42Z"
    },
    "stats": {
      "stars": 30,
      "forks": 0,
      "watchers": 30,
      "open_issues": 0,
      "size": 9881
    },
    "tech_info": {
      "language": "TypeScript",
      "languages": {
        "TypeScript": 34158,
        "JavaScript": 16831
      },
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "# Free sqlite\n\nVSCode extension to explore and query SQLite databases.\nopen-source and free.\n\n## Features\n - Open any SQLite file - Simply click on .sqlite or .db files to open them instantly\n - Table Explorer - Browse all tables in your database from an integrated sidebar\n - Data Visualization - View table data in a clean, modern interface that adapts to your VS Code theme\n - SQL statement editor - SQL statement editor, Automatically complete SQL keywords, table names, and fields\n - Query result export - The query results can be exported as Excel or CSV\n - Favorites - Collect some of your most commonly used SQL statements\n\n## How to use\n\n### Install\nVSCode extension install!\n[Install free sqlite](https://marketplace.visualstudio.com/items?itemName=free-sqlite.free-sqlite)\n\n### Open database\nNow! Browse all tables in your database in the right panel\n![open](https://github.com/fjb040911/free-sqlite/blob/main/doc/open.gif?raw=true)\n\n### Multiple files\n![multiple](https://github.com/fjb040911/free-sqlite/blob/main/doc/multi.gif?raw=true)\n\n### SQL Editor\nAutomatically complete SQL keywords, table names, and fields\n![editor](https://github.com/fjb040911/free-sqlite/blob/main/doc/select.gif?raw=true)\n\n### Favorite\nQuickly execute or view your frequently used SQL\n![favorite](https://github.com/fjb040911/free-sqlite/blob/main/doc/favoites.gif?raw=true)\n\n### Export\n![export](https://github.com/fjb040911/free-sqlite/blob/main/doc/expot.gif?raw=true)\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-16T19:10:17.666533"
  },
  {
    "basic_info": {
      "name": "C3_CRT_Python",
      "full_name": "gilshan-s/C3_CRT_Python",
      "owner": "gilshan-s",
      "description": "Coding",
      "url": "https://github.com/gilshan-s/C3_CRT_Python",
      "clone_url": "https://github.com/gilshan-s/C3_CRT_Python.git",
      "ssh_url": "git@github.com:gilshan-s/C3_CRT_Python.git",
      "homepage": null,
      "created_at": "2025-09-16T09:17:49Z",
      "updated_at": "2025-09-16T09:46:04Z",
      "pushed_at": "2025-09-16T09:41:18Z"
    },
    "stats": {
      "stars": 30,
      "forks": 28,
      "watchers": 30,
      "open_issues": 0,
      "size": 2
    },
    "tech_info": {
      "language": null,
      "languages": {},
      "license": null,
      "topics": []
    },
    "content": {
      "readme": "# C3_CRT_Python\nCoding\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-16T19:10:18.768521"
  },
  {
    "basic_info": {
      "name": "grok2api",
      "full_name": "VeroFess/grok2api",
      "owner": "VeroFess",
      "description": "rewrite grok2api",
      "url": "https://github.com/VeroFess/grok2api",
      "clone_url": "https://github.com/VeroFess/grok2api.git",
      "ssh_url": "git@github.com:VeroFess/grok2api.git",
      "homepage": null,
      "created_at": "2025-09-16T12:48:33Z",
      "updated_at": "2025-09-16T18:33:37Z",
      "pushed_at": "2025-09-16T14:36:37Z"
    },
    "stats": {
      "stars": 28,
      "forks": 14,
      "watchers": 28,
      "open_issues": 0,
      "size": 36
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 137940,
        "HTML": 33516,
        "Dockerfile": 306
      },
      "license": null,
      "topics": []
    },
    "content": {
      "readme": "# Grok API Gateway\n\n## 与原版差异\n\n本 fork 版本相较于原版增加了以下功能：\n\n0. **基本全部重写了...**\n1. **自动获取 x-statsig-id** - 使用 Playwright 自动获取并管理认证头\n2. **流模式标签过滤** - 自动移除响应中的 `<xaiArtifact` 等标签\n3. **增强统计功能** - 改进的令牌使用统计和监控\n4. **Grok4支持** - 反正我能用.jpg\n\n## 环境变量配置\n\n### 必需配置\n\n| 环境变量 | 描述 | 默认值 | 示例 |\n|---------|------|--------|------|\n| `API_KEY` | API 访问密钥 | `sk-123456` | `sk-your-api-key` |\n| `SSO` | Grok SSO 令牌（普通） | - | `token1,token2,token3` |\n| `SSO_SUPER` | Grok SSO 令牌（超级） | - | `super_token1,super_token2` |\n\n### 可选配置\n\n| 环境变量 | 描述 | 默认值 | 有效值 | 示例 |\n|---------|------|--------|--------|------|\n| `IS_CUSTOM_SSO` | 允许动态 SSO 令牌 | `false` | `true/false` | `true` |\n| `IS_TEMP_CONVERSATION` | 临时对话模式 | `true` | `true/false` | `false` |\n| `SHOW_THINKING` | 显示推理过程 | `false` | `true/false` | `true` |\n| `SHOW_SEARCH_RESULTS` | 显示搜索结果 | `true` | `true/false` | `false` |\n| `IS_SUPER_GROK` | 启用超级 Grok 功能 | `false` | `true/false` | `true` |\n| `MANAGER_SWITCH` | 启用 Web 管理界面 | - | `true/false` | `true` |\n| `ADMINPASSWORD` | 管理界面密码 | - | 任意字符串 | `admin123` |\n| `PORT` | 服务端口 | `5200` | 数字 | `8080` |\n| `PROXY` | 代理服务器 | - | HTTP/SOCKS5 URL | `http://127.0.0.1:1080` |\n| `CF_CLEARANCE` | Cloudflare 令牌 | - | CF 令牌字符串 | `cf_clearance_token` |\n| `PICGO_KEY` | PICGO 图床 API 密钥 | - | 字符串 | `picgo_api_key` |\n| `TUMY_KEY` | Tumy 图床 API 密钥 | - | 字符串 | `tumy_api_key` |\n| `FILTERED_TAGS` | 过滤标签列表 | `xaiArtifact` | 逗号分隔 | `tag1,tag2,tag3` |\n\n## 快速开始\n\n### 使用 Docker Hub 镜像\n\n现在可以直接从 Docker Hub 拉取预构建的镜像：\n\n```bash\n# 拉取镜像\ndocker pull verofess/grok2api\n\n# 运行容器\ndocker run -d \\\n  --name grok2api \\\n  -p 5200:5200 \\\n  -e API_KEY=sk-your-api-key \\\n  -e SSO=your-sso-token \\\n  verofess/grok2api\n\n# 或者使用 docker-compose\ndocker-compose up -d\n```\n\n### Docker Compose 示例\n\n```yaml\nservices:\n  grok2api:\n    image: verofess/grok2api\n    container_name: grok2api\n    ports:\n      - \"5200:5200\"\n    environment:\n      - API_KEY=sk-your-api-key\n      - SSO=your-sso-token\n      - IS_TEMP_CONVERSATION=true\n      - SHOW_THINKING=false\n    restart: unless-stopped\n```",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-16T19:10:19.878129"
  },
  {
    "basic_info": {
      "name": "VLAC",
      "full_name": "InternRobotics/VLAC",
      "owner": "InternRobotics",
      "description": "VLAC: A Vision-Language-Action-Critic Model for Robotic Real-World Reinforcement Learning",
      "url": "https://github.com/InternRobotics/VLAC",
      "clone_url": "https://github.com/InternRobotics/VLAC.git",
      "ssh_url": "git@github.com:InternRobotics/VLAC.git",
      "homepage": null,
      "created_at": "2025-09-15T13:53:14Z",
      "updated_at": "2025-09-16T18:16:08Z",
      "pushed_at": "2025-09-16T12:02:26Z"
    },
    "stats": {
      "stars": 28,
      "forks": 0,
      "watchers": 28,
      "open_issues": 0,
      "size": 126176
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 156404
      },
      "license": null,
      "topics": []
    },
    "content": {
      "readme": "# VLAC: A Vision-Language-Action-Critic Model for Robotic Real-World Reinforcement Learning\n<div align=\"center\">\n\n[[paper]](data/VLAC_EAI.pdf)\n[[code]](https://github.com/InternRobotics/VLAC)\n[[model]](https://huggingface.co/InternRobotics/VLAC)\n\n</div>\n\n## 🚀 Interactive Demo & Homepage\n\n<div align=\"center\">\n\n### [🎮 **Try Interactive & Homepage**](https://vlac.intern-ai.org.cn/)\n> **Online Demo is available now in Homepage, Try as you like!!!**\n\n</div>\n\n<div align=\"center\">\n  <img src=\"data/title_banner-2.gif\" alt=\"VLAC banner\" width=\"800\"></img>\n</div>\n\n## VLAC\n\nVLAC is a general-purpose pair-wise critic and manipulation model which designed for real world robot reinforcement learning and data refinement. \n\nIt provides robust evaluation capabilities for task progress prediction and task completion verification base one images and task description.\n\nVLAC trained on 3000h+ human egocentric data, 1200h+ comprehensive public robotic manipulation data, and 15h+ self-collected manipulation data.\n\n## ✨ Key Features\n\n• **Pair-wise comparison mechanism** for improved progressing dense critic accuracy, better recognition of state changes, and each step can be the start of the trajectory.\n\n• **Multi-modal capabilities** - Supports process tracking, task completion judgment, task description estimation, visual question answering, and even embodied action output, equipped with VLA capabilities.\n\n• **Flexible zero-shot and one-shot** - in-context capabilities, maintaining excellent performance across entities, scenarios, and tasks.\n\n• **Human-task synesthesia** - Based on the ego4D human dataset, model understands common tasks and build synesthesia for real-world human tasks and embodied tasks.\n\n• **Trajectory quality screening** - VLAC can evaluate the collected trajectories and filters out low score trajectories based on the VOC value and mask the action with negative pair-wise score, that is, data with low fluency and quality, improving the effect and efficiency of imitation learning.\n\n## Framework\n\n<div align=\"center\">\n  <img src=\"data/framework.png\" alt=\"VLAC Framework\" width=\"800\"/>\n</div>\n\n*The VLAC model is trained on a combination of comprehensive public robotic manipulation datasets, human demonstration data, self-collected manipulation data, and various image understanding datasets. Video data is processed into pair-wise samples to learn the different task progress between any two frames, supplemented with task descriptions and task completion evaluation to enable task progress understanding and action generation, as illustrated in the bottom-left corner. As shown in the diagram on the right, the model demonstrates strong generalization capabilities to new robots, scenarios, and tasks not covered in the training dataset. It can predict task progress and distinguish failure action or trajectory, providing dense reward feedback for real-world reinforcement learning and offering guidance for data refinement. Additionally, the model can directly perform manipulation tasks, exhibiting zero-shot capabilities to handle different scenarios.*\n\n## Performance\n\nDetails about the model's performance and evaluation metrics can be found in the [Homepage](https://vlac.intern-ai.org.cn/).\n\n## 🛠️ Installation\n\nTo install from source:\n```shell\ngit clone https://github.com/InternRobotics/VLAC.git\ncd VLAC\npip install -e .\n```\nRunning Environment:\n\n|              | Range        | Recommended | Notes                                     |\n| ------------ |--------------| ----------- | ----------------------------------------- |\n| python       | >=3.9        | 3.10        |                                           |\n| cuda         |              | cuda12      | No need to install if using CPU, NPU, MPS |\n| torch        | >=2.0        |             |                                           |\n| transformers | >=4.51       | 4.51.3      |                                           |\n| peft | >=0.15.2       |      |                                           |\n| ms-swift |        | 3.3      |                                           |\n\n\n## 🚀 Quick Start\n\n```python\nfrom evo_vlac import GAC_model\nfrom evo_vlac.utils.video_tool import compress_video\nimport os\n#Consistent with the web interface, the value and citic rewards of video input can be evaluated.\n\n\n#assign local model path\nmodel_path=\"set to your local model path\"\n#download model form https://huggingface.co/InternRobotics/VLAC\n\n#assign video path and task description\ntest_video='evo_vlac/examples/videos/pick-bowl-test.mp4'\nref_video='evo_vlac/examples/videos/pick-bowl-ref.mov'\ntask_description='Put up the bowl and place it back in the white storage box.'\n\n#init model\nCritic=GAC_model(tag='critic')\nCritic.init_model(model_path=model_path,model_type='internvl2',device_map=f'cuda:0')\nCritic.temperature=0.5\nCritic.top_k=1\nCritic.set_config()\nCritic.set_system_prompt()\n\n# transform video\ntest_video_compressed = os.path.join(os.path.dirname(test_video),\"test.mp4\")\n_,output_fps=compress_",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-16T19:10:21.007668"
  },
  {
    "basic_info": {
      "name": "botguard-reverse",
      "full_name": "dsekz/botguard-reverse",
      "owner": "dsekz",
      "description": "Botguard full reverse",
      "url": "https://github.com/dsekz/botguard-reverse",
      "clone_url": "https://github.com/dsekz/botguard-reverse.git",
      "ssh_url": "git@github.com:dsekz/botguard-reverse.git",
      "homepage": null,
      "created_at": "2025-09-15T01:12:51Z",
      "updated_at": "2025-09-16T17:17:23Z",
      "pushed_at": "2025-09-15T09:39:33Z"
    },
    "stats": {
      "stars": 26,
      "forks": 0,
      "watchers": 26,
      "open_issues": 0,
      "size": 210
    },
    "tech_info": {
      "language": null,
      "languages": {},
      "license": null,
      "topics": []
    },
    "content": {
      "readme": "Visit this link for the full reverse of Botguard: [cdn.csolver.net/botguard](https://cdn.csolver.net/botguard)\r\n\r\nThis project belong to Cypa.\r\n\r\n\r\n# ReCaptcha BotGuard\r\n\r\nBefore we begin the process of reversing BotGuard, let's start with some information.\r\n\r\nBotGuard is one of the most sophisticated antibots; it is known as the most difficult antibot to reverse.\r\nIt is used on many Google products, but each script serves a different purpose.\r\nThe one I present in this repository is ReCaptcha's BotGuard; they use it on their primary payload (`/reload`), and in ReCaptcha v2, they use it on the submit endpoint as well (`/userverify`)\r\n\r\nNow that you know a little about where it is used, let's understand what exactly it is.\r\nBotGuard is a script that generates a token, which is used to verify the validity of requests to Google's servers. In ReCaptcha, it is not used as a fingerprint, which many believe it is; instead, it is used to verify that the request is from a browser, as it can only be generated if executed by a browser.\r\nWhile this may seem like a \"weak\" approach, it is actually very sophisticated; there are many checks against falsey environments, so patching and sandboxing are no longer viable options. If you do not wish to reverse it but still want to generate it, you will need a browser emulator that can bypass its detection. Tools such as Puppeteer or Playwright can be helpful for this purpose. They emulate a real browser so that the environment checks won’t be effective against them. Due to this, you won’t need to do much setup to create a valid token generator as long as it’s a real browser.\r\n\r\n# What Is a VM?\r\n\r\nBefore we delve into the fun stuff, we need to understand what a VM is and how we can reverse what we don't know. Let's talk about what it is and how it operates. A VM is designed to emulate a CPU running a binary program, but it's custom, written in JavaScript, and the opcodes are often not standard; they usually include some custom ones that aren't available on a CPU. Now, in the case of VMs, it uses this custom \"CPU\" to execute a \"binary\" that is the given bytecode. It then decodes or decrypts it, depending on the VM, and begins to implement it. This is one of the most powerful obfuscation techniques at this time; it beats any standard CFF (control flow flattening) by a lot, since you need to write a whole disassembler and decompiler to understand what it does, then, after that, you still have to find and reverse the algorithm that was compiled in the first place.\r\n\r\nWith VMs, there are multiple types; we have register-based, which emulate that of a modern CPU. The browser you're using? That binary is being run through your register-based CPU. Stack-based is based on older CPUs and JVMs (Java VMs) use stack-based; WASM (WebAssembly) uses stack-based. That should help you understand what a VM is and how it works. Now that we've got that out of the way, we can continue reversing BotGuard.\r\n\r\n# What Makes It Difficult?\r\n\r\nMany people have tried and failed to reverse this antibot, and because of this, it has a reputation for being the \"hardest\" antibot, but what exactly causes such trouble?\r\nThe script isn't like any other script, where you can debug a little, understand the algorithm, and translate it to a production language, and you're done.\r\nThis script utilizes a VM (Virtual Machine), which completely virtualizes the code, making it impossible to locate the algorithm or debug into it, unlike many other scripts.\r\nYou need to write a **disassembler** and **decompiler** for this custom bytecode interpreter, which makes it even harder. Even then, if you have experience with VMs, this shouldn't be so hard. **WRONG!** This VM has much more to it than standard ones you see in other VMs like **Kasada** or **TikTok**. They utilize numerous special features, including register encryptions and flow-changing opcodes.\r\n\r\nSo what? It has more features, but we can still debug to figure it out! Well, they've got some **anti-debug** measures which make it very hard to debug without the script throwing you onto a random path with incorrect flow.\r\nThe most obvious check they have is timing; they use `performance.now()` along with `+Date.now()` to check the time. This allows the script to determine if a breakpoint has been placed; if the time is too high, the user was likely broken previously. In the screenshot below, you can see how it changes the **seed**, which is used to determine the next byte, so it diverts the program when debugging is detected. \r\n\r\n# Anti-Debugging Methods\r\n\r\nNow, the `K` variable is the VM context; it holds critical globals the VM uses, including the **seed**, which in this case is `K.U`. Notice how it has some quick math operations to determine an amount to xor the seed by? This should remain at `0`, and if it isn't, the value changes; this also causes the following bytes in the program to change, making it a very effective anti-debug method.\r\n\r\n<img src=\"images/photo_1.jpg\" ",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-16T19:10:22.120877"
  },
  {
    "basic_info": {
      "name": "whaileys",
      "full_name": "canove/whaileys",
      "owner": "canove",
      "description": "Send and receive messages as WhatsApp web on Nodejs",
      "url": "https://github.com/canove/whaileys",
      "clone_url": "https://github.com/canove/whaileys.git",
      "ssh_url": "git@github.com:canove/whaileys.git",
      "homepage": null,
      "created_at": "2025-09-15T01:24:06Z",
      "updated_at": "2025-09-16T19:07:39Z",
      "pushed_at": "2025-09-16T19:07:35Z"
    },
    "stats": {
      "stars": 25,
      "forks": 17,
      "watchers": 25,
      "open_issues": 3,
      "size": 11235
    },
    "tech_info": {
      "language": "JavaScript",
      "languages": {
        "JavaScript": 8654722,
        "TypeScript": 414810,
        "Shell": 401
      },
      "license": null,
      "topics": []
    },
    "content": {
      "readme": "# Baileys - Typescript/Javascript WhatsApp Web API\n\nVersion based on [baileys 4.x](https://github.com/canove/whaileys/commit/6663357a37c6e295338cd82ecff5cabceeee1736). It should be compatible with v6.x of [whiskeysockets/baileys](https://github.com/WhiskeySockets/Baileys)\n\nThanks to all [whiskeysockets/baileys](https://github.com/WhiskeySockets/Baileys) contributors for their work along the way.\n\n## Why another fork of baileys?\n\n- Its not a new version. It has been used in production for over 3 years, with over 5,000 active connections.\n- Has been maintained mainly by [whaticket](https://github.com/canove/whaticket-community) creator, [canove](https://github.com/canove).\n- This version aims to maintain **simplicity and stability** over functionality.\n- Every change will be thoroughly tested before release. The main branch will always be stable.\n\n## Known limitations of this version\n\nThe focus of this version has always been to maintain the core functionality (one-on-one chat messaging). Therefore, some group features may require optimization.\n\nSupport for other features (Meta AI, catalog, tags, adding and removing contacts, etc.) may not be available and will be released in the future only if it doesn't add excessive maintenance complexity to the code.\n\n## Old description (to be updated)\n\nBaileys does not require Selenium or any other browser to be interface with WhatsApp Web, it does so directly using a **WebSocket**. Not running Selenium or Chromimum saves you like **half a gig** of ram :/\n\nBaileys supports interacting with the multi-device & web versions of WhatsApp.\n\nThank you to [@pokearaujo](https://github.com/pokearaujo/multidevice) for writing his observations on the workings of WhatsApp Multi-Device. Also, thank you to [@Sigalor](https://github.com/sigalor/whatsapp-web-reveng) for writing his observations on the workings of WhatsApp Web and thanks to [@Rhymen](https://github.com/Rhymen/go-whatsapp/) for the **go** implementation.\n\nBaileys is type-safe, extensible and simple to use. If you require more functionality than provided, it's super easy to write an extension. More on this [here](#WritingCustomFunctionality).\n\n## Example\n\nDo check out & run [example.ts](https://github.com/canove/whaileys/blob/master/Example/example.ts) to see an example usage of the library.\nThe script covers most common use cases.\nTo run the example script, download or clone the repo and then type the following in a terminal:\n\n1. `cd path/to/Baileys`\n2. `yarn`\n3. `yarn example`\n\n## Install\n\nUse the stable version:\n\n```\nnpm i whaileys\n```\n\nUse the edge version (no guarantee of stability, but latest fixes + features)\n\n```\nnpm i github:canove/whaileys\n```\n\nThen import your code using:\n\n```ts\nimport makeWASocket from \"whaileys\";\n```\n\n## Unit Tests\n\nTODO\n\n## Connecting\n\n```ts\nimport makeWASocket, { DisconnectReason } from \"whaileys\";\nimport { Boom } from \"@hapi/boom\";\n\nasync function connectToWhatsApp() {\n  const sock = makeWASocket({\n    // can provide additional config here\n    printQRInTerminal: true\n  });\n  sock.ev.on(\"connection.update\", update => {\n    const { connection, lastDisconnect } = update;\n    if (connection === \"close\") {\n      const shouldReconnect =\n        (lastDisconnect.error as Boom)?.output?.statusCode !==\n        DisconnectReason.loggedOut;\n      console.log(\n        \"connection closed due to \",\n        lastDisconnect.error,\n        \", reconnecting \",\n        shouldReconnect\n      );\n      // reconnect if not logged out\n      if (shouldReconnect) {\n        connectToWhatsApp();\n      }\n    } else if (connection === \"open\") {\n      console.log(\"opened connection\");\n    }\n  });\n  sock.ev.on(\"messages.upsert\", m => {\n    console.log(JSON.stringify(m, undefined, 2));\n\n    console.log(\"replying to\", m.messages[0].key.remoteJid);\n    await sock.sendMessage(m.messages[0].key.remoteJid!, {\n      text: \"Hello there!\"\n    });\n  });\n}\n// run in main file\nconnectToWhatsApp();\n```\n\nIf the connection is successful, you will see a QR code printed on your terminal screen, scan it with WhatsApp on your phone and you'll be logged in!\n\n**Note:** install `qrcode-terminal` using `yarn add qrcode-terminal` to auto-print the QR to the terminal.\n\n**Note:** the code to support the legacy version of WA Web (pre multi-device) has been removed in v5. Only the standard multi-device connection is now supported. This is done as WA seems to have completely dropped support for the legacy version.\n\n## Configuring the Connection\n\nYou can configure the connection by passing a `SocketConfig` object.\n\nThe entire `SocketConfig` structure is mentioned here with default values:\n\n```ts\ntype SocketConfig = {\n  /** the WS url to connect to WA */\n  waWebSocketUrl: string | URL;\n  /** Fails the connection if the socket times out in this interval */\n  connectTimeoutMs: number;\n  /** Default timeout for queries, undefined for no timeout */\n  defaultQueryTimeoutMs: number | undefined;\n  /** ping-pong interval for WS connection */\n  keepAliveIntervalMs: ",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-16T19:10:23.235690"
  }
]