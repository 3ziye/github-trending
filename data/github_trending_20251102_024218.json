[
  {
    "basic_info": {
      "name": "nanochat",
      "full_name": "karpathy/nanochat",
      "owner": "karpathy",
      "description": "The best ChatGPT that $100 can buy.",
      "url": "https://github.com/karpathy/nanochat",
      "clone_url": "https://github.com/karpathy/nanochat.git",
      "ssh_url": "git@github.com:karpathy/nanochat.git",
      "homepage": "",
      "created_at": "2025-10-13T13:46:35Z",
      "updated_at": "2025-11-02T02:38:12Z",
      "pushed_at": "2025-11-01T16:04:42Z"
    },
    "stats": {
      "stars": 34943,
      "forks": 3962,
      "watchers": 34943,
      "open_issues": 46,
      "size": 459
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 326965,
        "HTML": 20192,
        "Rust": 16627,
        "Shell": 13174
      },
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "# nanochat\n\n![nanochat logo](dev/nanochat.png)\n\n> The best ChatGPT that $100 can buy.\n\nThis repo is a full-stack implementation of an LLM like ChatGPT in a single, clean, minimal, hackable, dependency-lite codebase. nanochat is designed to run on a single 8XH100 node via scripts like [speedrun.sh](speedrun.sh), that run the entire pipeline start to end. This includes tokenization, pretraining, finetuning, evaluation, inference, and web serving over a simple UI so that you can talk to your own LLM just like ChatGPT. nanochat will become the capstone project of the course LLM101n being developed by Eureka Labs.\n\n## Talk to it\n\nTo get a sense of the endpoint of this repo, you can currently find [nanochat d32](https://github.com/karpathy/nanochat/discussions/8) hosted on [nanochat.karpathy.ai](https://nanochat.karpathy.ai/). \"d32\" means that this model has 32 layers in the Transformer neural network. This model has 1.9 billion parameters, it was trained on 38 billion tokens by simply running the single script [run1000.sh](run1000.sh), and the total cost of training was ~$800 (about 33 hours training time on 8XH100 GPU node). While today this is enough to outperform GPT-2 of 2019, it falls dramatically short of modern Large Language Models like GPT-5. When talking to these micro models, you'll see that they make a lot of mistakes, they are a little bit naive and silly and they hallucinate a ton, a bit like children. It's kind of amusing. But what makes nanochat unique is that it is fully yours - fully configurable, tweakable, hackable, and trained by you from start to end. To train and talk to your own, we turn to...\n\n## Quick start\n\nThe fastest way to feel the magic is to run the speedrun script [speedrun.sh](speedrun.sh), which trains and inferences the $100 tier of nanochat. On an 8XH100 node at $24/hr, this gives a total run time of about 4 hours. Boot up a new 8XH100 GPU box from your favorite provider (e.g. I use and like [Lambda](https://lambda.ai/service/gpu-cloud)), and kick off the training script:\n\n```bash\nbash speedrun.sh\n```\n\nAlternatively, since the script runs for 4 hours, I like to launch it like this inside a new screen session `speedrun` (and also log output to `speedrun.log`):\n\n```bash\nscreen -L -Logfile speedrun.log -S speedrun bash speedrun.sh\n```\n\nSee the [screen cheatsheet](https://gist.github.com/jctosta/af918e1618682638aa82) if you are less familiar. You can watch it go inside the screen session, or detach with `Ctrl-a d` and `tail speedrun.log` to view progress. Now wait 4 hours. Once it's done, you can talk to your LLM via the ChatGPT-like web UI. Make sure again that your local uv virtual environment is active (run `source .venv/bin/activate`), and serve it:\n\n```bash\npython -m scripts.chat_web\n```\n\nAnd then visit the URL shown. Make sure to access it correctly, e.g. on Lambda use the public IP of the node you're on, followed by the port, so for example [http://209.20.xxx.xxx:8000/](http://209.20.xxx.xxx:8000/), etc. Then talk to your LLM as you'd normally talk to ChatGPT! Get it to write stories or poems. Ask it to tell you who you are to see a hallucination. Ask it why the sky is blue. Or why it's green. The speedrun is a 4e19 FLOPs capability model so it's a bit like talking to a kindergartener :).\n\n---\n\n<img width=\"2672\" height=\"1520\" alt=\"image\" src=\"https://github.com/user-attachments/assets/ed39ddf8-2370-437a-bedc-0f39781e76b5\" />\n\n---\n\nYou can also `cat report.md` file which appeared in the project directory and contains the \"report card\" of the run, i.e. a bunch of evaluations and metrics. At the very end, you'll see a summary table, for example:\n\n---\n\n- Characters: 333,989\n- Lines: 8,304\n- Files: 44\n- Tokens (approx): 83,497\n- Dependencies (uv.lock lines): 2,004\n\n| Metric          | BASE     | MID      | SFT      | RL       |\n|-----------------|----------|----------|----------|----------|\n| CORE            | 0.2219   | -        | -        | -        |\n| ARC-Challenge   | -        | 0.2875   | 0.2807   | -        |\n| ARC-Easy        | -        | 0.3561   | 0.3876   | -        |\n| GSM8K           | -        | 0.0250   | 0.0455   | 0.0758   |\n| HumanEval       | -        | 0.0671   | 0.0854   | -        |\n| MMLU            | -        | 0.3111   | 0.3151   | -        |\n| ChatCORE        | -        | 0.0730   | 0.0884   | -        |\n\nTotal wall clock time: 3h51m\n\n---\n\n(Your table might be missing the RL number by default). For a lot more information around the speedrun script and what to look for and expect, please refer to the walkthrough that I posted in Discussions of the repo: [\"Introducing nanochat: The best ChatGPT that $100 can buy\"](https://github.com/karpathy/nanochat/discussions/1).\n\n## Bigger models\n\nUnsurprisingly, $100 is not enough to train a highly performant ChatGPT clone. In fact, LLMs are famous for their multi-million dollar capex. For our purposes, I think there are two more scales of interest. First is the ~$300 tier d26 model (i.e. depth=26) that trains in ~1",
      "default_branch": "master"
    },
    "fetched_at": "2025-11-02T02:42:19.804576"
  },
  {
    "basic_info": {
      "name": "DeepSeek-OCR",
      "full_name": "deepseek-ai/DeepSeek-OCR",
      "owner": "deepseek-ai",
      "description": "Contexts Optical Compression",
      "url": "https://github.com/deepseek-ai/DeepSeek-OCR",
      "clone_url": "https://github.com/deepseek-ai/DeepSeek-OCR.git",
      "ssh_url": "git@github.com:deepseek-ai/DeepSeek-OCR.git",
      "homepage": null,
      "created_at": "2025-10-17T06:14:27Z",
      "updated_at": "2025-11-02T02:37:43Z",
      "pushed_at": "2025-10-25T02:43:18Z"
    },
    "stats": {
      "stars": 19020,
      "forks": 1299,
      "watchers": 19020,
      "open_issues": 183,
      "size": 7948
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 113538
      },
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "<!-- markdownlint-disable first-line-h1 -->\n<!-- markdownlint-disable html -->\n<!-- markdownlint-disable no-duplicate-header -->\n\n\n<div align=\"center\">\n  <img src=\"assets/logo.svg\" width=\"60%\" alt=\"DeepSeek AI\" />\n</div>\n\n\n<hr>\n<div align=\"center\">\n  <a href=\"https://www.deepseek.com/\" target=\"_blank\">\n    <img alt=\"Homepage\" src=\"assets/badge.svg\" />\n  </a>\n  <a href=\"https://huggingface.co/deepseek-ai/DeepSeek-OCR\" target=\"_blank\">\n    <img alt=\"Hugging Face\" src=\"https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-DeepSeek%20AI-ffc107?color=ffc107&logoColor=white\" />\n  </a>\n\n</div>\n\n<div align=\"center\">\n\n  <a href=\"https://discord.gg/Tc7c45Zzu5\" target=\"_blank\">\n    <img alt=\"Discord\" src=\"https://img.shields.io/badge/Discord-DeepSeek%20AI-7289da?logo=discord&logoColor=white&color=7289da\" />\n  </a>\n  <a href=\"https://twitter.com/deepseek_ai\" target=\"_blank\">\n    <img alt=\"Twitter Follow\" src=\"https://img.shields.io/badge/Twitter-deepseek_ai-white?logo=x&logoColor=white\" />\n  </a>\n\n</div>\n\n\n\n<p align=\"center\">\n  <a href=\"https://huggingface.co/deepseek-ai/DeepSeek-OCR\"><b>ğŸ“¥ Model Download</b></a> |\n  <a href=\"https://github.com/deepseek-ai/DeepSeek-OCR/blob/main/DeepSeek_OCR_paper.pdf\"><b>ğŸ“„ Paper Link</b></a> |\n  <a href=\"https://arxiv.org/abs/2510.18234\"><b>ğŸ“„ Arxiv Paper Link</b></a> |\n</p>\n\n<h2>\n<p align=\"center\">\n  <a href=\"\">DeepSeek-OCR: Contexts Optical Compression</a>\n</p>\n</h2>\n\n<p align=\"center\">\n<img src=\"assets/fig1.png\" style=\"width: 1000px\" align=center>\n</p>\n<p align=\"center\">\n<a href=\"\">Explore the boundaries of visual-text compression.</a>       \n</p>\n\n## Release\n- [2025/10/23]ğŸš€ğŸš€ğŸš€ DeepSeek-OCR is now officially supported in upstream [vLLM](https://docs.vllm.ai/projects/recipes/en/latest/DeepSeek/DeepSeek-OCR.html#installing-vllm). Thanks to the [vLLM](https://github.com/vllm-project/vllm) team for their help.\n- [2025/10/20]ğŸš€ğŸš€ğŸš€ We release DeepSeek-OCR, a model to investigate the role of vision encoders from an LLM-centric viewpoint.\n\n## Contents\n- [Install](#install)\n- [vLLM Inference](#vllm-inference)\n- [Transformers Inference](#transformers-inference)\n  \n\n\n\n\n## Install\n>Our environment is cuda11.8+torch2.6.0.\n1. Clone this repository and navigate to the DeepSeek-OCR folder\n```bash\ngit clone https://github.com/deepseek-ai/DeepSeek-OCR.git\n```\n2. Conda\n```Shell\nconda create -n deepseek-ocr python=3.12.9 -y\nconda activate deepseek-ocr\n```\n3. Packages\n\n- download the vllm-0.8.5 [whl](https://github.com/vllm-project/vllm/releases/tag/v0.8.5) \n```Shell\npip install torch==2.6.0 torchvision==0.21.0 torchaudio==2.6.0 --index-url https://download.pytorch.org/whl/cu118\npip install vllm-0.8.5+cu118-cp38-abi3-manylinux1_x86_64.whl\npip install -r requirements.txt\npip install flash-attn==2.7.3 --no-build-isolation\n```\n**Note:** if you want vLLM and transformers codes to run in the same environment, you don't need to worry about this installation error like: vllm 0.8.5+cu118 requires transformers>=4.51.1\n\n## vLLM-Inference\n- VLLM:\n>**Note:** change the INPUT_PATH/OUTPUT_PATH and other settings in the DeepSeek-OCR-master/DeepSeek-OCR-vllm/config.py\n```Shell\ncd DeepSeek-OCR-master/DeepSeek-OCR-vllm\n```\n1. image: streaming output\n```Shell\npython run_dpsk_ocr_image.py\n```\n2. pdf: concurrency ~2500tokens/s(an A100-40G)\n```Shell\npython run_dpsk_ocr_pdf.py\n```\n3. batch eval for benchmarks\n```Shell\npython run_dpsk_ocr_eval_batch.py\n```\n\n**[2025/10/23] The version of upstream [vLLM](https://docs.vllm.ai/projects/recipes/en/latest/DeepSeek/DeepSeek-OCR.html#installing-vllm):**\n\n```shell\nuv venv\nsource .venv/bin/activate\n# Until v0.11.1 release, you need to install vLLM from nightly build\nuv pip install -U vllm --pre --extra-index-url https://wheels.vllm.ai/nightly\n```\n\n```python\nfrom vllm import LLM, SamplingParams\nfrom vllm.model_executor.models.deepseek_ocr import NGramPerReqLogitsProcessor\nfrom PIL import Image\n\n# Create model instance\nllm = LLM(\n    model=\"deepseek-ai/DeepSeek-OCR\",\n    enable_prefix_caching=False,\n    mm_processor_cache_gb=0,\n    logits_processors=[NGramPerReqLogitsProcessor]\n)\n\n# Prepare batched input with your image file\nimage_1 = Image.open(\"path/to/your/image_1.png\").convert(\"RGB\")\nimage_2 = Image.open(\"path/to/your/image_2.png\").convert(\"RGB\")\nprompt = \"<image>\\nFree OCR.\"\n\nmodel_input = [\n    {\n        \"prompt\": prompt,\n        \"multi_modal_data\": {\"image\": image_1}\n    },\n    {\n        \"prompt\": prompt,\n        \"multi_modal_data\": {\"image\": image_2}\n    }\n]\n\nsampling_param = SamplingParams(\n            temperature=0.0,\n            max_tokens=8192,\n            # ngram logit processor args\n            extra_args=dict(\n                ngram_size=30,\n                window_size=90,\n                whitelist_token_ids={128821, 128822},  # whitelist: <td>, </td>\n            ),\n            skip_special_tokens=False,\n        )\n# Generate output\nmodel_outputs = llm.generate(model_input, sampling_param)\n\n# Print output\nfor output in model_outputs:\n    print(output.outputs[0].text)\n```\n## ",
      "default_branch": "main"
    },
    "fetched_at": "2025-11-02T02:42:21.091430"
  },
  {
    "basic_info": {
      "name": "AI-Trader",
      "full_name": "HKUDS/AI-Trader",
      "owner": "HKUDS",
      "description": "\"AI-Trader: Can AI Beat the Market?\" Live Trading: https://hkuds.github.io/AI-Trader/",
      "url": "https://github.com/HKUDS/AI-Trader",
      "clone_url": "https://github.com/HKUDS/AI-Trader.git",
      "ssh_url": "git@github.com:HKUDS/AI-Trader.git",
      "homepage": "",
      "created_at": "2025-10-23T12:45:00Z",
      "updated_at": "2025-11-02T02:41:12Z",
      "pushed_at": "2025-11-01T11:23:43Z"
    },
    "stats": {
      "stars": 8167,
      "forks": 1115,
      "watchers": 8167,
      "open_issues": 29,
      "size": 10695
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 153330,
        "Shell": 1149
      },
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "<div align=\"center\">\n\n# ğŸš€ AI-Trader: Can AI Beat the Market?\n\n[![Python](https://img.shields.io/badge/Python-3.10+-blue.svg)](https://python.org)\n[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)\n[![GitHub stars](https://img.shields.io/github/stars/HKUDS/AI-Trader?style=social)](https://github.com/HKUDS/AI-Trader)\n[![Feishu](https://img.shields.io/badge/ğŸ’¬Feishu-Group-blue?style=flat)](./Communication.md) \n[![WeChat](https://img.shields.io/badge/WeChat-Group-green?style=flat&logo=wechat)](./Communication.md)\n\n**Five AIs battle for NASDAQ 100 supremacy. Zero human input. Pure competition.**\n\n## ğŸ† Current Championship Leaderboard ğŸ† \n[*Click Here: AI Live Trading*](https://hkuds.github.io/AI-Trader/)\n\n<div align=\"center\">\n\n###  **Championship Period: (Last Update 2025/10/30)**\n\n| ğŸ† Rank | ğŸ¤– AI Model | ğŸ“ˆ Total Earnings | \n|---------|-------------|----------------|\n| **ğŸ¥‡ 1st** | **DeepSeek** | ğŸš€ +13.89% |\n| ğŸ¥ˆ 2nd | MiniMax-M2 | ğŸ“Š +10.72% |\n| ğŸ¥‰ 3rd | Claude-3.7 | ğŸ“Š +7.12% |\n| 4th | GPT-5 | ğŸ“Š +7.11% |\n| Baseline | QQQ | ğŸ“Š +3.78% |\n| 5th | Qwen3-max | ğŸ“Š +3.44% |\n| 6th | Gemini-2.5-flash | ğŸ“Š -0.54% |\n\n### ğŸ“Š **Live Performance Dashboard**\n![rank](assets/rank.png)\n\n*Daily Performance Tracking of AI Models in NASDAQ 100 Trading*\n\n</div>\n\n---\n\n## **How to use this dataset**\n\nIt's simple! \n\nYou just need to submit a PR that includes at least: `./agent/{your_strategy}.py` (you can inherit from Basemodel to create your strategy!), `./configs/{yourconfig}`, and instructions on how to run your strategy. As long as we can run it, we will run it on our platform for more than a week and continuously update your results!\n\n---\n\n[ğŸš€ Quick Start](#-quick-start) â€¢ [ğŸ“ˆ Performance Analysis](#-performance-analysis) â€¢ [ğŸ› ï¸ Configuration Guide](#-configuration-guide) â€¢ [ä¸­æ–‡æ–‡æ¡£](README_CN.md)\n\n</div>\n\n---\n## ğŸ‰ Weekly Update (Oct 24-30, 2025)\n\nWe're excited to announce the following major updates completed this week:\n\n### â° Hourly Trading Support\n- âœ… **Hour-Level Precision Trading** - Upgraded from daily to hourly trading intervals for more granular market participation\n\n### ğŸ¨ Comprehensive UI Optimization\n- âœ… **Live Trading Dashboard** - Real-time visualization of all agent trading activities\n- âœ… **Agent Reasoning Display** - Complete transparency into AI decision-making processes with full reasoning chains\n- âœ… **Interactive Leaderboard** - Dynamic performance rankings with live updates\n\n## ğŸŒŸ Project Introduction\n\n> **AI-Trader enables five distinct AI models, each employing unique investment strategies, to compete autonomously in the same market and determine which can generate the highest profits in NASDAQ 100 trading!**\n\n### ğŸ¯ Core Features\n\n- ğŸ¤– **Fully Autonomous Decision-Making**: AI agents perform 100% independent analysis, decision-making, and execution without human intervention\n- ğŸ› ï¸ **Pure Tool-Driven Architecture**: Built on MCP toolchain, enabling AI to complete all trading operations through standardized tool calls\n- ğŸ† **Multi-Model Competition Arena**: Deploy multiple AI models (GPT, Claude, Qwen, etc.) for competitive trading\n- ğŸ“Š **Real-Time Performance Analytics**: Comprehensive trading records, position monitoring, and profit/loss analysis\n- ğŸ” **Intelligent Market Intelligence**: Integrated Jina search for real-time market news and financial reports\n- âš¡ **MCP Toolchain Integration**: Modular tool ecosystem based on Model Context Protocol\n- ğŸ”Œ **Extensible Strategy Framework**: Support for third-party strategies and custom AI agent integration\n- â° **Historical Replay Capability**: Time-period replay functionality with automatic future information filtering\n\n---\n\n### ğŸ® Trading Environment\nEach AI model starts with $10,000 to trade NASDAQ 100 stocks in a controlled environment with real market data and historical replay capabilities.\n\n- ğŸ’° **Initial Capital**: $10,000 USD starting balance\n- ğŸ“ˆ **Trading Universe**: NASDAQ 100 component stocks (top 100 technology stocks)\n- â° **Trading Schedule**: Weekday market hours with historical simulation support\n- ğŸ“Š **Data Integration**: Alpha Vantage API combined with Jina AI market intelligence\n- ğŸ”„ **Time Management**: Historical period replay with automated future information filtering\n\n---\n\n### ğŸ§  Agentic Trading Capabilities\nAI agents operate with complete autonomy, conducting market research, making trading decisions, and continuously evolving their strategies without human intervention.\n\n- ğŸ“° **Autonomous Market Research**: Intelligent retrieval and filtering of market news, analyst reports, and financial data\n- ğŸ’¡ **Independent Decision Engine**: Multi-dimensional analysis driving fully autonomous buy/sell execution\n- ğŸ“ **Comprehensive Trade Logging**: Automated documentation of trading rationale, execution details, and portfolio changes\n- ğŸ”„ **Adaptive Strategy Evolution**: Self-optimizing algorithms that adjust based on market performance feedback\n\n---\n\n### ğŸ Competition Rules\nAll AI models compete under identical conditions with the same capital, data access, tools",
      "default_branch": "main"
    },
    "fetched_at": "2025-11-02T02:42:22.403294"
  },
  {
    "basic_info": {
      "name": "TinyRecursiveModels",
      "full_name": "SamsungSAILMontreal/TinyRecursiveModels",
      "owner": "SamsungSAILMontreal",
      "description": null,
      "url": "https://github.com/SamsungSAILMontreal/TinyRecursiveModels",
      "clone_url": "https://github.com/SamsungSAILMontreal/TinyRecursiveModels.git",
      "ssh_url": "git@github.com:SamsungSAILMontreal/TinyRecursiveModels.git",
      "homepage": null,
      "created_at": "2025-10-07T13:24:28Z",
      "updated_at": "2025-11-02T02:36:19Z",
      "pushed_at": "2025-10-08T19:46:47Z"
    },
    "stats": {
      "stars": 5349,
      "forks": 746,
      "watchers": 5349,
      "open_issues": 29,
      "size": 1266
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 147529
      },
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "# Less is More: Recursive Reasoning with Tiny Networks\n\nThis is the codebase for the paper: \"Less is More: Recursive Reasoning with Tiny Networks\". TRM is a recursive reasoning approach that achieves amazing scores of 45% on ARC-AGI-1 and 8% on ARC-AGI-2 using a tiny 7M parameters neural network.\n\n[Paper](https://arxiv.org/abs/2510.04871)\n\n### Motivation\n\nTiny Recursion Model (TRM) is a recursive reasoning model that achieves amazing scores of 45% on ARC-AGI-1 and 8% on ARC-AGI-2 with a tiny 7M parameters neural network. The idea that one must rely on massive foundational models trained for millions of dollars by some big corporation in order to achieve success on hard tasks is a trap. Currently, there is too much focus on exploiting LLMs rather than devising and expanding new lines of direction. With recursive reasoning, it turns out that â€œless is moreâ€: you donâ€™t always need to crank up model size in order for a model to reason and solve hard problems. A tiny model pretrained from scratch, recursing on itself and updating its answers over time, can achieve a lot without breaking the bank.\n\nThis work came to be after I learned about the recent innovative Hierarchical Reasoning Model (HRM). I was amazed that an approach using small models could do so well on hard tasks like the ARC-AGI competition (reaching 40% accuracy when normally only Large Language Models could compete). But I kept thinking that it is too complicated, relying too much on biological arguments about the human brain, and that this recursive reasoning process could be greatly simplified and improved. Tiny Recursion Model (TRM) simplifies recursive reasoning to its core essence, which ultimately has nothing to do with the human brain, does not require any mathematical (fixed-point) theorem, nor any hierarchy.\n\n### How TRM works\n\n<p align=\"center\">\n  <img src=\"https://AlexiaJM.github.io/assets/images/TRM_fig.png\" alt=\"TRM\"  style=\"width: 30%;\">\n</p>\n\nTiny Recursion Model (TRM) recursively improves its predicted answer y with a tiny network. It starts with the embedded input question x and initial embedded answer y and latent z. For up to K improvements steps, it tries to improve its answer y. It does so by i) recursively updating n times its latent z given the question x, current answer y, and current latent z (recursive reasoning), and then ii) updating its answer y given the current answer y and current latent z. This recursive process allows the model to progressively improve its answer (potentially addressing any errors from its previous answer) in an extremely parameter-efficient manner while minimizing overfitting.\n\n### Requirements\n\n- Python 3.10 (or similar)\n- Cuda 12.6.0 (or similar)\n\n```bash\npip install --upgrade pip wheel setuptools\npip install --pre --upgrade torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu126 # install torch based on your cuda version\npip install -r requirements.txt # install requirements\npip install --no-cache-dir --no-build-isolation adam-atan2 \nwandb login YOUR-LOGIN # login if you want the logger to sync results to your Weights & Biases (https://wandb.ai/)\n```\n\n### Dataset Preparation\n\n```bash\n# ARC-AGI-1\npython -m dataset.build_arc_dataset \\\n  --input-file-prefix kaggle/combined/arc-agi \\\n  --output-dir data/arc1concept-aug-1000 \\\n  --subsets training evaluation concept \\\n  --test-set-name evaluation\n\n# ARC-AGI-2\npython -m dataset.build_arc_dataset \\\n  --input-file-prefix kaggle/combined/arc-agi \\\n  --output-dir data/arc2concept-aug-1000 \\\n  --subsets training2 evaluation2 concept \\\n  --test-set-name evaluation2\n\n## Note: You cannot train on both ARC-AGI-1 and ARC-AGI-2 and evaluate them both because ARC-AGI-2 training data contains some ARC-AGI-1 eval data\n\n# Sudoku-Extreme\npython dataset/build_sudoku_dataset.py --output-dir data/sudoku-extreme-1k-aug-1000  --subsample-size 1000 --num-aug 1000  # 1000 examples, 1000 augments\n\n# Maze-Hard\npython dataset/build_maze_dataset.py # 1000 examples, 8 augments\n```\n\n## Experiments\n\n### ARC-AGI-1 (assuming 4 H-100 GPUs):\n\n```bash\nrun_name=\"pretrain_att_arc1concept_4\"\ntorchrun --nproc-per-node 4 --rdzv_backend=c10d --rdzv_endpoint=localhost:0 --nnodes=1 pretrain.py \\\narch=trm \\\ndata_paths=\"[data/arc1concept-aug-1000]\" \\\narch.L_layers=2 \\\narch.H_cycles=3 arch.L_cycles=4 \\\n+run_name=${run_name} ema=True\n\n```\n\n*Runtime:* ~3 days\n\n### ARC-AGI-2 (assuming 4 H-100 GPUs):\n\n```bash\nrun_name=\"pretrain_att_arc2concept_4\"\ntorchrun --nproc-per-node 4 --rdzv_backend=c10d --rdzv_endpoint=localhost:0 --nnodes=1 pretrain.py \\\narch=trm \\\ndata_paths=\"[data/arc2concept-aug-1000]\" \\\narch.L_layers=2 \\\narch.H_cycles=3 arch.L_cycles=4 \\\n+run_name=${run_name} ema=True\n\n```\n\n*Runtime:* ~3 days\n\n### Sudoku-Extreme (assuming 1 L40S GPU):\n\n```bash\nrun_name=\"pretrain_mlp_t_sudoku\"\npython pretrain.py \\\narch=trm \\\ndata_paths=\"[data/sudoku-extreme-1k-aug-1000]\" \\\nevaluators=\"[]\" \\\nepochs=50000 eval_interval=5000 \\\nlr=1e-4 puzzle_emb_lr=1e-4 weight_decay=1.0 puzzle",
      "default_branch": "main"
    },
    "fetched_at": "2025-11-02T02:42:23.666536"
  },
  {
    "basic_info": {
      "name": "Skill_Seekers",
      "full_name": "yusufkaraaslan/Skill_Seekers",
      "owner": "yusufkaraaslan",
      "description": "Convert documentation websites, GitHub repositories, and PDFs into Claude AI skills with automatic conflict detection",
      "url": "https://github.com/yusufkaraaslan/Skill_Seekers",
      "clone_url": "https://github.com/yusufkaraaslan/Skill_Seekers.git",
      "ssh_url": "git@github.com:yusufkaraaslan/Skill_Seekers.git",
      "homepage": "",
      "created_at": "2025-10-17T14:43:48Z",
      "updated_at": "2025-11-02T02:36:54Z",
      "pushed_at": "2025-10-29T19:17:51Z"
    },
    "stats": {
      "stars": 3085,
      "forks": 310,
      "watchers": 3085,
      "open_issues": 118,
      "size": 694
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 644789,
        "Shell": 7544
      },
      "license": "MIT License",
      "topics": [
        "ai-tools",
        "ast-parser",
        "automation",
        "claude-ai",
        "claude-skills",
        "code-analysis",
        "conflict-detection",
        "documentation",
        "documentation-generator",
        "github",
        "github-scraper",
        "mcp",
        "mcp-server",
        "multi-source",
        "ocr",
        "pdf",
        "python",
        "web-scraping"
      ]
    },
    "content": {
      "readme": "[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/yusufkaraaslan-skill-seekers-badge.png)](https://mseep.ai/app/yusufkaraaslan-skill-seekers)\n\n# Skill Seeker\n\n[![Version](https://img.shields.io/badge/version-2.0.0-blue.svg)](https://github.com/yusufkaraaslan/Skill_Seekers/releases/tag/v2.0.0)\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)\n[![MCP Integration](https://img.shields.io/badge/MCP-Integrated-blue.svg)](https://modelcontextprotocol.io)\n[![Tested](https://img.shields.io/badge/Tests-299%20Passing-brightgreen.svg)](tests/)\n[![Project Board](https://img.shields.io/badge/Project-Board-purple.svg)](https://github.com/users/yusufkaraaslan/projects/2)\n\n**Automatically convert documentation websites, GitHub repositories, and PDFs into Claude AI skills in minutes.**\n\n> ğŸ“‹ **[View Development Roadmap & Tasks](https://github.com/users/yusufkaraaslan/projects/2)** - 134 tasks across 10 categories, pick any to contribute!\n\n## What is Skill Seeker?\n\nSkill Seeker is an automated tool that transforms documentation websites, GitHub repositories, and PDF files into production-ready [Claude AI skills](https://www.anthropic.com/news/skills). Instead of manually reading and summarizing documentation, Skill Seeker:\n\n1. **Scrapes** multiple sources (docs, GitHub repos, PDFs) automatically\n2. **Analyzes** code repositories with deep AST parsing\n3. **Detects** conflicts between documentation and code implementation\n4. **Organizes** content into categorized reference files\n5. **Enhances** with AI to extract best examples and key concepts\n6. **Packages** everything into an uploadable `.zip` file for Claude\n\n**Result:** Get comprehensive Claude skills for any framework, API, or tool in 20-40 minutes instead of hours of manual work.\n\n## Why Use This?\n\n- ğŸ¯ **For Developers**: Create skills from documentation + GitHub repos with conflict detection\n- ğŸ® **For Game Devs**: Generate skills for game engines (Godot docs + GitHub, Unity, etc.)\n- ğŸ”§ **For Teams**: Combine internal docs + code repositories into single source of truth\n- ğŸ“š **For Learners**: Build comprehensive skills from docs, code examples, and PDFs\n- ğŸ” **For Open Source**: Analyze repos to find documentation gaps and outdated examples\n\n## Key Features\n\n### ğŸŒ Documentation Scraping\n- âœ… **llms.txt Support** - Automatically detects and uses LLM-ready documentation files (10x faster)\n- âœ… **Universal Scraper** - Works with ANY documentation website\n- âœ… **Smart Categorization** - Automatically organizes content by topic\n- âœ… **Code Language Detection** - Recognizes Python, JavaScript, C++, GDScript, etc.\n- âœ… **8 Ready-to-Use Presets** - Godot, React, Vue, Django, FastAPI, and more\n\n### ğŸ“„ PDF Support (**v1.2.0**)\n- âœ… **Basic PDF Extraction** - Extract text, code, and images from PDF files\n- âœ… **OCR for Scanned PDFs** - Extract text from scanned documents\n- âœ… **Password-Protected PDFs** - Handle encrypted PDFs\n- âœ… **Table Extraction** - Extract complex tables from PDFs\n- âœ… **Parallel Processing** - 3x faster for large PDFs\n- âœ… **Intelligent Caching** - 50% faster on re-runs\n\n### ğŸ™ GitHub Repository Scraping (**v2.0.0**)\n- âœ… **Deep Code Analysis** - AST parsing for Python, JavaScript, TypeScript, Java, C++, Go\n- âœ… **API Extraction** - Functions, classes, methods with parameters and types\n- âœ… **Repository Metadata** - README, file tree, language breakdown, stars/forks\n- âœ… **GitHub Issues & PRs** - Fetch open/closed issues with labels and milestones\n- âœ… **CHANGELOG & Releases** - Automatically extract version history\n- âœ… **Conflict Detection** - Compare documented APIs vs actual code implementation\n- âœ… **MCP Integration** - Natural language: \"Scrape GitHub repo facebook/react\"\n\n### ğŸ”„ Unified Multi-Source Scraping (**NEW - v2.0.0**)\n- âœ… **Combine Multiple Sources** - Mix documentation + GitHub + PDF in one skill\n- âœ… **Conflict Detection** - Automatically finds discrepancies between docs and code\n- âœ… **Intelligent Merging** - Rule-based or AI-powered conflict resolution\n- âœ… **Transparent Reporting** - Side-by-side comparison with âš ï¸ warnings\n- âœ… **Documentation Gap Analysis** - Identifies outdated docs and undocumented features\n- âœ… **Single Source of Truth** - One skill showing both intent (docs) and reality (code)\n- âœ… **Backward Compatible** - Legacy single-source configs still work\n\n### ğŸ¤– AI & Enhancement\n- âœ… **AI-Powered Enhancement** - Transforms basic templates into comprehensive guides\n- âœ… **No API Costs** - FREE local enhancement using Claude Code Max\n- âœ… **MCP Server for Claude Code** - Use directly from Claude Code with natural language\n\n### âš¡ Performance & Scale\n- âœ… **Async Mode** - 2-3x faster scraping with async/await (use `--async` flag)\n- âœ… **Large Documentation Support** - Handle 10K-40K+ page docs with intelligent splitting\n- âœ… **Router/Hub Skills** - Intelligent routing to specialized sub-skills\n-",
      "default_branch": "development"
    },
    "fetched_at": "2025-11-02T02:42:24.950158"
  },
  {
    "basic_info": {
      "name": "agentic-design-patterns-cn",
      "full_name": "ginobefun/agentic-design-patterns-cn",
      "owner": "ginobefun",
      "description": "ã€ŠAgentic Design Patternsã€‹ä¸­æ–‡ç¿»è¯‘ç‰ˆ",
      "url": "https://github.com/ginobefun/agentic-design-patterns-cn",
      "clone_url": "https://github.com/ginobefun/agentic-design-patterns-cn.git",
      "ssh_url": "git@github.com:ginobefun/agentic-design-patterns-cn.git",
      "homepage": null,
      "created_at": "2025-10-09T04:36:28Z",
      "updated_at": "2025-11-02T02:32:23Z",
      "pushed_at": "2025-11-01T03:18:59Z"
    },
    "stats": {
      "stars": 2784,
      "forks": 296,
      "watchers": 2784,
      "open_issues": 3,
      "size": 8332
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 114397
      },
      "license": null,
      "topics": []
    },
    "content": {
      "readme": "[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/ginobefun-agentic-design-patterns-cn-badge.png)](https://mseep.ai/app/ginobefun-agentic-design-patterns-cn)\n\n# Agentic Design Patterns | <mark>æ™ºèƒ½ä½“è®¾è®¡æ¨¡å¼</mark>\n\n## A Hands-On Guide to Building Intelligent Systems | <mark>æ„å»ºæ™ºèƒ½ç³»ç»Ÿçš„å®è·µæŒ‡å—</mark>\n\n[![License: CC BY-NC 4.0](https://img.shields.io/badge/License-CC%20BY--NC%204.0-lightgrey.svg)](https://creativecommons.org/licenses/by-nc/4.0/)\n[![GitHub stars](https://img.shields.io/github/stars/ginobefun/agentic-design-patterns-cn)](https://github.com/ginobefun/agentic-design-patterns-cn/stargazers)\n[![GitHub forks](https://img.shields.io/github/forks/ginobefun/agentic-design-patterns-cn)](https://github.com/ginobefun/agentic-design-patterns-cn/network)\n\n**åŸä¹¦ä½œè€… (Author)**: [Antonio Gulli](https://www.linkedin.com/in/searchguy/)\n\n**åŸä¹¦é“¾æ¥ (Original Book)**: [Amazon](https://www.amazon.com/Agentic-Design-Patterns-Hands-Intelligent/dp/3032014018/)\n\n**åŸå§‹æ–‡æ¡£é“¾æ¥ (Original Book Link)**: [Google Docs](https://docs.google.com/document/d/1rsaK53T3Lg5KoGwvf8ukOUvbELRtH-V0LnOIFDxBryE/preview?tab=t.0#heading=h.pxcur8v2qagu)\n\n---\n\n## ğŸ“– é¡¹ç›®ç®€ä»‹ | Project Description\n\næœ¬é¡¹ç›®æ˜¯å¯¹ Antonio Gulli æ‰€è‘—ã€ŠAgentic Design Patterns: A Hands-On Guide to Building Intelligent Systemsã€‹çš„**ä¸­è‹±æ–‡å¯¹ç…§ç¿»è¯‘**ã€‚è¯¥ä¹¦æ˜¯ä¸€éƒ¨å…¨é¢çš„æŠ€æœ¯æŒ‡å—ï¼Œæ¶µç›–äº†ç°ä»£äººå·¥æ™ºèƒ½ç³»ç»Ÿä¸­æ™ºèƒ½ä½“ (Agent) è®¾è®¡çš„æ ¸å¿ƒæ¦‚å¿µå’Œå®è·µæ–¹æ³•ã€‚\n\nThis project is a **bilingual Chinese-English translation** of \"Agentic Design Patterns: A Hands-On Guide to Building Intelligent Systems\" by Antonio Gulli. The book is a comprehensive technical guide covering core concepts and practical approaches to agent design in modern AI systems.\n\n---\n\n## ğŸ¯ é¡¹ç›®ç‰¹è‰² | Key Features\n\n- ğŸ“š **ä¸­è‹±æ–‡å¯¹ç…§** - å®Œæ•´çš„åŒè¯­å¯¹ç…§ç¿»è¯‘\n- ğŸ¨ **é«˜äº®æ˜¾ç¤º** - ä¸­æ–‡å†…å®¹ä½¿ç”¨é»„è‰²é«˜äº®ï¼Œæ˜“äºåŒºåˆ†\n- ğŸ“ **æ ¼å¼è§„èŒƒ** - ä¸¥æ ¼éµå¾ª Markdown æ ‡å‡†å’Œç¿»è¯‘è§„èŒƒ\n- ğŸ”— **ä»£ç é“¾æ¥** - ä¿ç•™æ‰€æœ‰åŸä¹¦ä»£ç ç¤ºä¾‹é“¾æ¥\n- âš¡ **æŒç»­æ›´æ–°** - é€ç« ç¿»è¯‘ï¼ŒæŒç»­æ›´æ–°è¿›åº¦\n\n---\n\n## ğŸ“‹ ç¿»è¯‘è¿›åº¦ | Translation Progress\n\n**<mark>æ€»é¡µæ•°ï¼š424 é¡µ</mark>** | **Total: 424 Pages**\n\n### å‰ç½®å†…å®¹ | Front Matter\n\n| ç« èŠ‚ | æ¦‚è¿° | è´Ÿè´£äºº | AI ç¿»è¯‘ | äººå·¥è¯„å®¡ | äº¤å‰è¯„å®¡ |\n|------|------|--------|---------|----------|----------|\n| [çŒ®è¾](01-Dedication.md) | ä½œè€…çš„çŒ®è¾ä¸è‡´æ•¬ | @ginobefun | âœ… | âœ… | â³ |\n| [è‡´è°¢](02-Acknowledgment.md) | è‡´è°¢ä¸æ„Ÿè°¢åå• | @ginobefun | âœ… | âœ… | â³ |\n| [åºè¨€](03-Foreword.md) | æœ¬ä¹¦çš„åºè¨€ä¸èƒŒæ™¯ä»‹ç» | @ginobefun | âœ… | âœ… | â³ |\n| [æ€æƒ³é¢†è¢–çš„æ´è§](04-Thought-Leader.md) | æƒåŠ›ä¸è´£ä»»çš„æ·±åº¦æ€è€ƒ | @ginobefun | âœ… | âœ… | â³ |\n| [ä»‹ç»](05-Introduction.md) | å…¨ä¹¦å¼•è¨€ä¸æ ¸å¿ƒæ¦‚å¿µ | @ginobefun | âœ… | âœ… | â³ |\n| [ä»€ä¹ˆæ˜¯\"æ™ºèƒ½ä½“\"ï¼Ÿ](06-What-Makes-Agent.md) | å®šä¹‰ AI ç³»ç»Ÿçš„\"æ™ºèƒ½ä½“\"ç‰¹å¾ | @ginobefun | âœ… | âœ… | â³ |\n\n### ç¬¬ä¸€éƒ¨åˆ†ï¼šæ ¸å¿ƒè®¾è®¡æ¨¡å¼ | Part One: Core Patterns (103 é¡µ)\n\n| ç« èŠ‚ | è®¾è®¡æ¨¡å¼æ¦‚è¿° | è´Ÿè´£äºº | AI ç¿»è¯‘ | äººå·¥è¯„å®¡ | äº¤å‰è¯„å®¡ |\n|------|-------------|--------|---------|----------|----------|\n| [ç¬¬ 1 ç« ï¼šæç¤ºé“¾](07-Chapter-01-Prompt-Chaining.md) | åˆ†è€Œæ²»ä¹‹çš„ä»»åŠ¡åˆ†è§£æ¨¡å¼ï¼Œå°†å¤æ‚ä»»åŠ¡åˆ†è§£ä¸ºå¤„ç†æµæ°´çº¿ | @ginobefun | âœ… | âœ… | â³ |\n| [ç¬¬ 2 ç« ï¼šè·¯ç”±](08-Chapter-02-Routing.md) | æ™ºèƒ½å†³ç­–ä¸åŠ¨æ€åˆ†å‘ï¼Œæ ¹æ®æƒ…å¢ƒé€‰æ‹©æœ€ä½³è¡ŒåŠ¨è·¯å¾„ | @ginobefun | âœ… | âœ… | â³ |\n| [ç¬¬ 3 ç« ï¼šå¹¶è¡ŒåŒ–](09-Chapter-03-Parallelization.md) | å¹¶å‘æ‰§è¡Œä¸æ€§èƒ½æå‡ï¼ŒåŒæ—¶æ‰§è¡Œå¤šä¸ªç‹¬ç«‹ä»»åŠ¡ | @ginobefun | âœ… | âœ… | â³ |\n| [ç¬¬ 4 ç« ï¼šåæ€](10-Chapter-04-Reflection.md) | è‡ªæˆ‘è¯„ä¼°å’Œè¿­ä»£æ”¹è¿›ï¼Œé€šè¿‡åé¦ˆå¾ªç¯ä¼˜åŒ–è¾“å‡ºè´¨é‡ | @ginobefun | âœ… | âœ… | â³ |\n| [ç¬¬ 5 ç« ï¼šå·¥å…·ä½¿ç”¨](11-Chapter-05-Tool-Use.md) | å¤–éƒ¨å·¥å…·ä¸ API é›†æˆï¼Œæ‰©å±•æ™ºèƒ½ä½“èƒ½åŠ›è¾¹ç•Œ | @ginobefun | âœ… | âœ… | â³ |\n| [ç¬¬ 6 ç« ï¼šè§„åˆ’](12-Chapter-06-Planning.md) | å¤šæ­¥éª¤è®¡åˆ’åˆ¶å®šä¸æ‰§è¡Œï¼Œå®ç°å¤æ‚ç›®æ ‡åˆ†è§£ | @ginobefun | âœ… | âœ… | â³ |\n| [ç¬¬ 7 ç« ï¼šå¤šæ™ºèƒ½ä½“åä½œ](13-Chapter-07-Multi-Agent-Collaboration.md) | ååŒå·¥ä½œæ¶æ„ï¼Œå¤šä¸ªæ™ºèƒ½ä½“é…åˆå®Œæˆä»»åŠ¡ | @ginobefun | âœ…  | âœ… | â³ |\n\n### ç¬¬äºŒéƒ¨åˆ†ï¼šé«˜çº§è®¾è®¡æ¨¡å¼ | Part Two: Advanced Patterns (61 é¡µ)\n\n| ç« èŠ‚ | è®¾è®¡æ¨¡å¼æ¦‚è¿° | è´Ÿè´£äºº | AI ç¿»è¯‘ | äººå·¥è¯„å®¡ | äº¤å‰è¯„å®¡ |\n|------|-------------|--------|---------|----------|----------|\n| [ç¬¬ 8 ç« ï¼šè®°å¿†ç®¡ç†](14-Chapter-08-Memory-Management.md) | çŸ­æœŸå’Œé•¿æœŸè®°å¿†ç®¡ç†ï¼Œç»´æŒä¸Šä¸‹æ–‡è¿ç»­æ€§ | @éƒ‘æ¶› | âœ… | âœ… | âŒ |\n| [ç¬¬ 9 ç« ï¼šå­¦ä¹ ä¸é€‚åº”](15-Chapter-09-Learning-and-Adaptation.md) | ä»ç»éªŒä¸­å­¦ä¹ ï¼ŒæŒç»­ä¼˜åŒ–æ™ºèƒ½ä½“è¡Œä¸º | @é™ˆè¯—ä¸­ | â³ | âŒ | âŒ |\n| [ç¬¬ 10 ç« ï¼šæ¨¡å‹ä¸Šä¸‹æ–‡åè®®](16-Chapter-10-Model-Context-Protocol.md) | æ ‡å‡†åŒ–äº¤äº’åè®®ï¼Œè§„èŒƒæ™ºèƒ½ä½“é€šä¿¡æ–¹å¼ | @éƒ‘æ¶› | â³ | âŒ | âŒ |\n| [ç¬¬ 11 ç« ï¼šç›®æ ‡è®¾å®šä¸ç›‘æ§](17-Chapter-11-Goal-Setting-and-Monitoring.md) | åŠ¨æ€ç›®æ ‡ç®¡ç†ï¼Œå®æ—¶è¿½è¸ªä»»åŠ¡è¿›å±• | [@ææµªæºª](https://github.com/seabornlee) | âœ… | âœ… | â³ |\n\n### ç¬¬ä¸‰éƒ¨åˆ†ï¼šé›†æˆè®¾è®¡æ¨¡å¼ | Part Three: Integration Patterns (34 é¡µ)\n\n| ç« èŠ‚ | è®¾è®¡æ¨¡å¼æ¦‚è¿° | è´Ÿè´£äºº | AI ç¿»è¯‘ | äººå·¥è¯„å®¡ | äº¤å‰è¯„å®¡ |\n|------|-------------|--------|---------|----------|----------|\n| [ç¬¬ 12 ç« ï¼šå¼‚å¸¸å¤„ç†ä¸æ¢å¤](18-Chapter-12-Exception-Handling-and-Recovery.md) | ä¼˜é›…é”™è¯¯å¤„ç†ï¼Œç¡®ä¿ç³»ç»Ÿç¨³å®šæ€§ | @EE | âŒ | âŒ | âŒ |\n| [ç¬¬ 13 ç« ï¼šäººæœºåä½œ](19-Chapter-13-Human-in-the-Loop.md) | äººæœºåä½œå†³ç­–ï¼Œèåˆäººç±»æ™ºæ…§ä¸ AI èƒ½åŠ› | @æ›¾æ±‰ | âœ… | âœ… | â³ |\n| [ç¬¬ 14 ç« ï¼šçŸ¥è¯†æ£€ç´¢ (RAG)](20-Chapter-14-Knowledge-Retrieval.md) | æ£€ç´¢å¢å¼ºç”ŸæˆæŠ€æœ¯ï¼Œç»“åˆå¤–éƒ¨çŸ¥è¯†åº“ | @EE | âœ… | âœ… | â³ |\n\n### ç¬¬å››éƒ¨åˆ†ï¼šç”Ÿäº§è®¾è®¡æ¨¡å¼ | Part Four: Production Patterns (114 é¡µ)\n\n| ç« èŠ‚ | è®¾è®¡æ¨¡å¼æ¦‚è¿° | è´Ÿè´£äºº | AI ç¿»è¯‘ | äººå·¥è¯„å®¡ | äº¤å‰è¯„å®¡ |\n|------|-------------|--------|---------|----------|----------|\n| [ç¬¬ 15 ç« ï¼šæ™ºèƒ½ä½“é—´é€šä¿¡ (A2A)](21-Chapter-15-Inter-Agent-Communication.md) | æ™ºèƒ½ä½“é€šä¿¡åè®®ï¼Œå®ç°æ™ºèƒ½ä½“é—´é«˜æ•ˆäº¤äº’ | @æœµæœµè‚¥ | âœ… | âœ… | âŒ |\n| [ç¬¬ 16 ç« ï¼šèµ„æºæ„ŸçŸ¥ä¼˜åŒ–](22-Chapter-16-Resource-Aware-Optimization.md) | èµ„æºä¼˜åŒ–ç®¡ç†ï¼Œå¹³è¡¡æ€§èƒ½ä¸æˆæœ¬ | @IsaacZhaoo | âœ… | âœ… | â³ |\n| [ç¬¬ 17 ç« ï¼šæ¨ç†æŠ€æœ¯](23-Chapter-17-Reasoning-Techniques.md) | å¢å¼ºæ¨ç†èƒ½åŠ›ï¼Œæå‡å†³ç­–è´¨é‡ | @Diqing | âŒ | âŒ | âŒ |\n| [ç¬¬ 18 ç« ï¼šæŠ¤æ /å®‰å…¨æ¨¡å¼](24-Chapter-18-Guardrails-Safety-Patterns.md) | å®‰å…¨ä¿éšœæœºåˆ¶ï¼Œé˜²æ­¢ä¸å½“è¡Œä¸º | @IsaacZhaoo | â³ | âŒ | âŒ |\n| [ç¬¬ 19 ç« ï¼šè¯„ä¼°ä¸ç›‘æ§](25-Chapter-19-Evaluation-and-Monitoring.md) | æ€§èƒ½è¯„ä¼°ä½“ç³»ï¼Œé‡åŒ–æ™ºèƒ½ä½“è¡¨ç° | @æœµæœµè‚¥ | âŒ | âŒ ",
      "default_branch": "main"
    },
    "fetched_at": "2025-11-02T02:42:26.223843"
  },
  {
    "basic_info": {
      "name": "dexter",
      "full_name": "virattt/dexter",
      "owner": "virattt",
      "description": "An autonomous agent for deep financial research",
      "url": "https://github.com/virattt/dexter",
      "clone_url": "https://github.com/virattt/dexter.git",
      "ssh_url": "git@github.com:virattt/dexter.git",
      "homepage": null,
      "created_at": "2025-10-14T21:02:00Z",
      "updated_at": "2025-11-01T23:25:53Z",
      "pushed_at": "2025-11-01T17:20:03Z"
    },
    "stats": {
      "stars": 2626,
      "forks": 325,
      "watchers": 2626,
      "open_issues": 8,
      "size": 93
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 65140,
        "JavaScript": 228
      },
      "license": null,
      "topics": []
    },
    "content": {
      "readme": "# Dexter ğŸ¤–\n\nDexter is an autonomous financial research agent that thinks, plans, and learns as it works. It performs analysis using task planning, self-reflection, and real-time market data. Think Claude Code, but built specifically for financial research.\n\n\n<img width=\"979\" height=\"651\" alt=\"Screenshot 2025-10-14 at 6 12 35â€¯PM\" src=\"https://github.com/user-attachments/assets/5a2859d4-53cf-4638-998a-15cef3c98038\" />\n\n## Overview\n\nDexter takes complex financial questions and turns them into clear, step-by-step research plans. It runs those tasks using live market data, checks its own work, and refines the results until it has a confident, data-backed answer.  \n\nItâ€™s not just another chatbot.  Itâ€™s an agent that plans ahead, verifies its progress, and keeps iterating until the job is done.\n\n**Key Capabilities:**\n- **Intelligent Task Planning**: Automatically decomposes complex queries into structured research steps\n- **Autonomous Execution**: Selects and executes the right tools to gather financial data\n- **Self-Validation**: Checks its own work and iterates until tasks are complete\n- **Real-Time Financial Data**: Access to income statements, balance sheets, and cash flow statements\n- **Safety Features**: Built-in loop detection and step limits to prevent runaway execution\n\n[![Twitter Follow](https://img.shields.io/twitter/follow/virattt?style=social)](https://twitter.com/virattt)\n\n### Prerequisites\n\n- Python 3.10 or higher\n- [uv](https://github.com/astral-sh/uv) package manager\n- OpenAI API key (get [here](https://platform.openai.com/api-keys))\n- Financial Datasets API key (get [here](https://financialdatasets.ai))\n\n### Installation\n\n1. Clone the repository:\n```bash\ngit clone https://github.com/virattt/dexter.git\ncd dexter\n```\n\n2. Install dependencies with uv:\n```bash\nuv sync\n```\n\n3. Set up your environment variables:\n```bash\n# Copy the example environment file\ncp env.example .env\n\n# Edit .env and add your API keys\n# OPENAI_API_KEY=your-openai-api-key\n# FINANCIAL_DATASETS_API_KEY=your-financial-datasets-api-key\n```\n\n### Usage\n\nRun Dexter in interactive mode:\n```bash\nuv run dexter-agent\n```\n\n### Example Queries\n\nTry asking Dexter questions like:\n- \"What was Apple's revenue growth over the last 4 quarters?\"\n- \"Compare Microsoft and Google's operating margins for 2023\"\n- \"Analyze Tesla's cash flow trends over the past year\"\n- \"What is Amazon's debt-to-equity ratio based on recent financials?\"\n\nDexter will automatically:\n1. Break down your question into research tasks\n2. Fetch the necessary financial data\n3. Perform calculations and analysis\n4. Provide a comprehensive, data-rich answer\n\n## Architecture\n\nDexter uses a multi-agent architecture with specialized components:\n\n- **Planning Agent**: Analyzes queries and creates structured task lists\n- **Action Agent**: Selects appropriate tools and executes research steps\n- **Validation Agent**: Verifies task completion and data sufficiency\n- **Answer Agent**: Synthesizes findings into comprehensive responses\n\n## Project Structure\n\n```\ndexter/\nâ”œâ”€â”€ src/\nâ”‚   â”œâ”€â”€ dexter/\nâ”‚   â”‚   â”œâ”€â”€ agent.py      # Main agent orchestration logic\nâ”‚   â”‚   â”œâ”€â”€ model.py      # LLM interface\nâ”‚   â”‚   â”œâ”€â”€ tools.py      # Financial data tools\nâ”‚   â”‚   â”œâ”€â”€ prompts.py    # System prompts for each component\nâ”‚   â”‚   â”œâ”€â”€ schemas.py    # Pydantic models\nâ”‚   â”‚   â”œâ”€â”€ utils/        # Utility functions\nâ”‚   â”‚   â””â”€â”€ cli.py        # CLI entry point\nâ”œâ”€â”€ pyproject.toml\nâ””â”€â”€ uv.lock\n```\n\n## Configuration\n\nDexter supports configuration via the `Agent` class initialization:\n\n```python\nfrom dexter.agent import Agent\n\nagent = Agent(\n    max_steps=20,              # Global safety limit\n    max_steps_per_task=5       # Per-task iteration limit\n)\n```\n\n## How to Contribute\n\n1. Fork the repository\n2. Create a feature branch\n3. Commit your changes\n4. Push to the branch\n5. Create a Pull Request\n\n**Important**: Please keep your pull requests small and focused.  This will make it easier to review and merge.\n\n\n## License\n\nThis project is licensed under the MIT License.\n\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-11-02T02:42:27.488322"
  },
  {
    "basic_info": {
      "name": "kimi-cli",
      "full_name": "MoonshotAI/kimi-cli",
      "owner": "MoonshotAI",
      "description": "Kimi CLI is your next CLI agent.",
      "url": "https://github.com/MoonshotAI/kimi-cli",
      "clone_url": "https://github.com/MoonshotAI/kimi-cli.git",
      "ssh_url": "git@github.com:MoonshotAI/kimi-cli.git",
      "homepage": null,
      "created_at": "2025-10-15T12:58:03Z",
      "updated_at": "2025-11-02T02:20:52Z",
      "pushed_at": "2025-11-01T05:45:56Z"
    },
    "stats": {
      "stars": 2255,
      "forks": 166,
      "watchers": 2255,
      "open_issues": 45,
      "size": 1583
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 457207,
        "Makefile": 2817
      },
      "license": "Apache License 2.0",
      "topics": []
    },
    "content": {
      "readme": "# Kimi CLI\n\n[![Commit Activity](https://img.shields.io/github/commit-activity/w/MoonshotAI/kimi-cli)](https://github.com/MoonshotAI/kimi-cli/graphs/commit-activity)\n[![Checks](https://img.shields.io/github/check-runs/MoonshotAI/kimi-cli/main)](https://github.com/MoonshotAI/kimi-cli/actions)\n[![Version](https://img.shields.io/pypi/v/kimi-cli)](https://pypi.org/project/kimi-cli/)\n[![Downloads](https://img.shields.io/pypi/dw/kimi-cli)](https://pypistats.org/packages/kimi-cli)\n\n[ä¸­æ–‡](https://www.kimi.com/coding/docs/kimi-cli.html)\n\nKimi CLI is a new CLI agent that can help you with your software development tasks and terminal operations.\n\n> [!IMPORTANT]\n> Kimi CLI is currently in technical preview.\n\n## Key features\n\n- Shell-like UI and raw shell command execution\n- Zsh integration\n- [Agent Client Protocol] support\n- MCP support\n- And more to come...\n\n[Agent Client Protocol]: https://github.com/agentclientprotocol/agent-client-protocol\n\n## Installation\n\n> [!IMPORTANT]\n> Kimi CLI currently only supports macOS and Linux. Windows support is coming soon.\n\nKimi CLI is published as a Python package on PyPI. We highly recommend installing it with [uv](https://docs.astral.sh/uv/). If you have not installed uv yet, please follow the instructions [here](https://docs.astral.sh/uv/getting-started/installation/) to install it first.\n\nOnce uv is installed, you can install Kimi CLI with:\n\n```sh\nuv tool install --python 3.13 kimi-cli\n```\n\nRun `kimi --help` to check if Kimi CLI is installed successfully.\n\n> [!IMPORTANT]\n> Due to the security checks on macOS, the first time you run `kimi` command may take 10 seconds or more depending on your system environment.\n\n## Upgrading\n\nUpgrade Kimi CLI to the latest version with:\n\n```sh\nuv tool upgrade kimi-cli --no-cache\n```\n\n## Usage\n\nRun `kimi` command in the directory you want to work on, then send `/setup` to setup Kimi CLI:\n\n![](./docs/images/setup.png)\n\nAfter setup, Kimi CLI will be ready to use. You can send `/help` to get more information.\n\n## Features\n\n### Shell mode\n\nKimi CLI is not only a coding agent, but also a shell. You can switch the mode by pressing `Ctrl-X`. In shell mode, you can directly run shell commands without leaving Kimi CLI.\n\n> [!NOTE]\n> Built-in shell commands like `cd` are not supported yet.\n\n### Zsh integration\n\nYou can use Kimi CLI together with Zsh, to empower your shell experience with AI agent capabilities.\n\nInstall the [zsh-kimi-cli](https://github.com/MoonshotAI/zsh-kimi-cli) plugin via:\n\n```sh\ngit clone https://github.com/MoonshotAI/zsh-kimi-cli.git \\\n  ${ZSH_CUSTOM:-~/.oh-my-zsh/custom}/plugins/kimi-cli\n```\n\n> [!NOTE]\n> If you are using a plugin manager other than Oh My Zsh, you may need to refer to the plugin's README for installation instructions.\n\nThen add `kimi-cli` to your Zsh plugin list in `~/.zshrc`:\n\n```sh\nplugins=(... kimi-cli)\n```\n\nAfter restarting Zsh, you can switch to agent mode by pressing `Ctrl-X`.\n\n### ACP support\n\nKimi CLI supports [Agent Client Protocol] out of the box. You can use it together with any ACP-compatible editor or IDE.\n\nFor example, to use Kimi CLI with [Zed](https://zed.dev/), add the following configuration to your `~/.config/zed/settings.json`:\n\n```json\n{\n  \"agent_servers\": {\n    \"Kimi CLI\": {\n      \"command\": \"kimi\",\n      \"args\": [\"--acp\"],\n      \"env\": {}\n    }\n  }\n}\n```\n\nThen you can create Kimi CLI threads in Zed's agent panel.\n\n### Using MCP tools\n\nKimi CLI supports the well-established MCP config convention. For example:\n\n```json\n{\n  \"mcpServers\": {\n    \"context7\": {\n      \"url\": \"https://mcp.context7.com/mcp\",\n      \"headers\": {\n        \"CONTEXT7_API_KEY\": \"YOUR_API_KEY\"\n      }\n    },\n    \"chrome-devtools\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"chrome-devtools-mcp@latest\"]\n    }\n  }\n}\n```\n\nRun `kimi` with `--mcp-config-file` option to connect to the specified MCP servers:\n\n```sh\nkimi --mcp-config-file /path/to/mcp.json\n```\n\n## Development\n\nTo develop Kimi CLI, run:\n\n```sh\ngit clone https://github.com/MoonshotAI/kimi-cli.git\ncd kimi-cli\n\nmake prepare  # prepare the development environment\n```\n\nThen you can start working on Kimi CLI.\n\nRefer to the following commands after you make changes:\n\n```sh\nuv run kimi  # run Kimi CLI\n\nmake format  # format code\nmake check  # run linting and type checking\nmake test  # run tests\nmake help  # show all make targets\n```\n\n## Contributing\n\nWe welcome contributions to Kimi CLI! Please refer to [CONTRIBUTING.md](./CONTRIBUTING.md) for more information.\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-11-02T02:42:28.755708"
  },
  {
    "basic_info": {
      "name": "PokeeResearchOSS",
      "full_name": "Pokee-AI/PokeeResearchOSS",
      "owner": "Pokee-AI",
      "description": "Pokee Deep Research Model Open Source Repo",
      "url": "https://github.com/Pokee-AI/PokeeResearchOSS",
      "clone_url": "https://github.com/Pokee-AI/PokeeResearchOSS.git",
      "ssh_url": "git@github.com:Pokee-AI/PokeeResearchOSS.git",
      "homepage": null,
      "created_at": "2025-10-17T07:15:18Z",
      "updated_at": "2025-11-01T21:30:16Z",
      "pushed_at": "2025-10-22T06:57:29Z"
    },
    "stats": {
      "stars": 1571,
      "forks": 1004,
      "watchers": 1571,
      "open_issues": 2,
      "size": 1835
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 386590,
        "Shell": 3417
      },
      "license": "Apache License 2.0",
      "topics": []
    },
    "content": {
      "readme": "This repository hosts Pokeeâ€™s state-of-the-art 7B DeepResearch Agent, which integrates web search and content reading capabilities to answer complex questions using the most up-to-date information available online.\n\n*We also offer an API hosting our proprietary deep research agent, which is up to 75% cheaper than OpenAI, Gemini, and Perplexity. It delivers comprehensive, citation-rich research reports with no hidden costs and no API key management required. (For more information about the API, visit [pokee.ai/deepresearch-preview](https://pokee.ai/deepresearch-preview))*\n\n<div align=\"center\">\n\n[![GitHub Stars](https://img.shields.io/github/stars/Pokee-AI/PokeeResearchOSS?style=social)](https://github.com/Pokee-AI/PokeeResearchOSS)\n[![arXiv](https://img.shields.io/badge/arXiv-2510.15862-b31b1b.svg)](https://arxiv.org/pdf/2510.15862)\n[![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Model-yellow)](https://huggingface.co/PokeeAI/pokee_research_7b)\n\n[![LinkedIn](https://img.shields.io/badge/LinkedIn-Follow-blue?style=social&logo=linkedin)](https://linkedin.com/company/pokee-ai)\n[![X (Twitter)](https://img.shields.io/badge/X-Follow-1DA1F2?style=social&logo=x)](https://x.com/pokee_ai)\n[![Discord](https://img.shields.io/badge/Discord-Join-5865F2?style=social&logo=discord)](https://discord.gg/VJXWQvyd)\n[![WeChat](https://img.shields.io/badge/WeChat-Join-07C160?style=social&logo=wechat)](https://i.postimg.cc/wv099v5w/wechat-group-pokee.jpg)\n\n<img src=\"Logo.png\" alt=\"Pokee AI Logo\" width=\"200\"/>\n<p style=\"text-align:center;\">\n  <a href=\"https://pokee.ai\" target=\"_blank\" rel=\"noopener noreferrer\"><strong>pokee.ai</strong></a>\n</p>\n\n</div>\n\n# PokeeResearch-7B Agent\n\nPokee's state-of-the-art 7B DeepResearch Agent that leverages web search and content reading capabilities to answer complex questions using the most up-to-date information available online.\n<div align=\"center\">\n<img src=\"hle_gaia_bro.png\" alt=\"HLE, GAIA and BrowseComp Performance\" width=\"600\"/>\n</div>\n<div align=\"center\">\n<img src=\"qa.png\" alt=\"7 QA Benchmark Performance\" width=\"800\"/>\n</div>\n\n## ğŸš€ Features\n\n- **Multi-turn Research**: Performs iterative web searches and content analysis\n- **Tool Integration**: Seamlessly integrates web search, content reading, and browsing tools\n- **Comprehensive Evaluation**: Includes benchmark evaluation across multiple QA datasets\n- **High Performance**: Achieves superior results on complex reasoning tasks\n- **Scalable Architecture**: Built on efficient 7B parameter model for optimal performance\n\n\n## ğŸ“‹ Requirements\n\n### Hardware\n- **Compute Node**: We tested the code on a single 80GB A100 GPU (GPUs with less memory may also work, though we have not tested them). Using multiple GPUs can further accelerate inference. For reference, the driver version is 570.133.20 and the CUDA toolkit version is 12.8.\n\n### Software\n- **Docker**: Environment to run the code will be provided as a docker image.\n\n### API Keys\nYou will need the following API keys:\n- **Serper API**: For web search functionality\n- **Jina API**: For web content reading and extraction\n- **Gemini API**: For content summarization and result evaluation\n- **HuggingFace Token**: For downloading the model from HuggingFace\n\n## ğŸ› ï¸ Quick Start\n\n### 1. Environment Setup\nWe provide a Docker image for easy deployment:\n```bash\ndocker pull verlai/verl:app-verl0.5-transformers4.55.4-sglang0.4.10.post2-mcore0.13.0-te2.2\ndocker create --runtime=nvidia --gpus all --net=host --shm-size=\"80g\"  -v .:/workspace/ --name pokeeresearch verlai/verl:app-verl0.5-transformers4.55.4-sglang0.4.10.post2-mcore0.13.0-te2.2 sleep infinity\ndocker start pokeeresearch\ndocker exec -it pokeeresearch  bash\nssh-keygen -t ed25519 -C <USER_NAME>\n# copy /root/.ssh/id_ed25519.pub to github ssh keys\ngit clone git@github.com:Pokee-AI/PokeeResearchOSS.git --recursive\ncd PokeeResearchOSS\npip install colorlog\npip install -U google-genai\nhf auth login # enter your huggingface token, the tokens needs to have permission to use Pokee AI's models\ncd verl\npip install -e .\ncd ..\n```\n\nCreate a `.env` file in the project root and add your API keys:\n```bash\nSERPER_API_KEY=your_serper_api_key_here\nJINA_API_KEY=your_jina_api_key_here\nGEMINI_API_KEY=your_gemini_api_key_here\n```\n\n### 2. Modify ```run.sh``` to use more than one GPUs (optional)\nRunning the experiment with more GPUs is faster. By default the experiment uses one GPU.\nIf you want to use more GPUs, simply modify \n```bash\ntrainer.n_gpus_per_node=1 \\\n```\nin ```run.sh``` to \n```bash\ntrainer.n_gpus_per_node=<NUM_GPUS_TO_USE> \\\n```\n### 3. Run Benchmark Evaluation\n\n**Step 1: Start the Tool Server**\n```bash\npython start_tool_server.py \\\n--port <PORT_NUMBER> \\ # to specify the port to listen to (default 8888)\n--enable-cache # to enable caching tool results (recommended to save api credits)\n```\n\n**Step 2: Run the Evaluation**\n\nStart a new terminal, then run the experiment.\n```bash\ndocker exec -it pokeeresearch bash\ncd PokeeResearchOSS\nbash run.sh\n```\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-11-02T02:42:30.039442"
  },
  {
    "basic_info": {
      "name": "pico-banana-400k",
      "full_name": "apple/pico-banana-400k",
      "owner": "apple",
      "description": null,
      "url": "https://github.com/apple/pico-banana-400k",
      "clone_url": "https://github.com/apple/pico-banana-400k.git",
      "ssh_url": "git@github.com:apple/pico-banana-400k.git",
      "homepage": null,
      "created_at": "2025-10-21T21:15:35Z",
      "updated_at": "2025-11-02T01:37:38Z",
      "pushed_at": "2025-10-28T20:51:33Z"
    },
    "stats": {
      "stars": 1424,
      "forks": 65,
      "watchers": 1424,
      "open_issues": 2,
      "size": 7260
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 3369
      },
      "license": "Other",
      "topics": []
    },
    "content": {
      "readme": "# ğŸŒ Pico-Banana-400K: A Large-Scale Dataset for Text-Guided Image Editing\n\n<font size=7><div align='center' > [[ğŸ“– Paper](https://www.arxiv.org/pdf/2510.19808)]  </div></font>\n\n**Pico-Banana-400K** is a large-scale dataset of **~400K textâ€“imageâ€“edit triplets** designed to advance research in **text-guided image editing**.  \nEach example contains:\n- an **original image** (from [Open Images](https://storage.googleapis.com/openimages/web/factsfigures.html)),  \n- a **human-like edit instruction**, and  \n- the **edited result** generated by Nano-Banana and verified by Gemini-2.5-Pro.\n\nThe dataset spans **35 edit operations** across **8 semantic categories**, covering diverse transformationsâ€”from low-level color adjustments to high-level object, scene, and stylistic edits.\n\n---\n\n\n\n## ğŸ§© Key Features\n\n| Feature | Description |\n|----------|-------------|\n| **Total Samples** | ~257K single-turn textâ€“imageâ€“edit triplets for SFT, ~56K single-turn text-image(positive) - image(negative)-edit for preference learning, and ~72K multi-turn texts-images-edits for multi-turn applications|\n| **Source** | [Open Images](https://storage.googleapis.com/openimages/web/factsfigures.html) |\n| **Edit Operations** | 35 across 8 semantic categories |\n| **Categories** | Pixel & Photometric, Object-Level, Scene Composition, Stylistic, Text & Symbol, Human-Centric, Scale & Perspective, Spatial/Layout |\n| **Image Resolution** | 512â€“1024 px |\n| **Prompt Generator** | [Gemini-2.5-Flash](https://deepmind.google/models/gemini/flash/) |\n| **Editing Model** | Nano-Banana |\n| **Self-Evaluation** | Automated judging pipeline using Gemini-2.5-Pro for edit quality |\n\n---\n\n## ğŸ—ï¸ Dataset Construction\n\nPico-Banana-400K is built using a **two-stage multimodal generation pipeline**:\n\n1. **Instruction Generation**  \n   Each Open Images sample is passed to *Gemini-2.5-Flash*, which writes concise, natural-language editing instructions grounded in visible content. We also provide short instructions summarized by Qwen-2.5-Instruct-7B. \n   Example:  \n   ```json\n   {\n     \"instruction\": \"Change the red car to blue.\"\n   }\n   \n\n2. **Editing + Self-Evaluation**\n   The Nano-Banana model performs the edit, then automatically evaluates the result using a structured quality prompt that measures:\n   Instruction Compliance (40%)\n   Editing Realism (25%)\n   Preservation Balance (20%)\n   Technical Quality (15%)\n   Only edits scoring above a strict threshold (~0.7) are labeled as successful, forming the main dataset; the remaining ~56K are retained as failure cases for robustness and preference learning.\n\n## ğŸ“Š Dataset Statistics\n\n**Nano-Banana-400K** contains **~400K image editing data**, covering a wide visual and semantic range drawn from real-world imagery.\n\n---\n\n### ğŸ§­ Category Distribution\n\n| Category | Description | Percentage |\n|:----------|:-------------|:------------:|\n| **Object-Level Semantic** | Add, remove, replace, or relocate objects | **35%** |\n| **Scene Composition & Multi-Subject** | Contextual and environmental transformations | **20%** |\n| **Human-Centric** | Edits involving clothing, expression, or appearance | **18%** |\n| **Stylistic** | Domain and artistic style transfer | **10%** |\n| **Text & Symbol** | Edits involving visible text, signs, or symbols | **8%** |\n| **Pixel & Photometric** | Brightness, contrast, and tonal adjustments | **5%** |\n| **Scale & Perspective** | Zoom, viewpoint, or framing changes | **2%** |\n| **Spatial / Layout** | Outpainting, composition, or canvas extension | **2%** |\n\n---\n\n### ğŸ“‚ Data Composition\n\n- **Single-Turn SFT samples (successful edits):** ~257K  \n- **Single-Turn Preference samples (failure cases):** ~56K\n- **Multi-Turn SFT samples (successful cases):** ~72K  \n- **Gemini-generated instructions:** concise, natural, and image-aware\n- **Edit coverage:** 35 edit types across 8 semantic categories  \n- **Image diversity:** includes humans, objects, text-rich scenes, etc from Open Images  \n\n---\n\n### ğŸ–¼ï¸ Visualization\n\nBelow are representative examples from different categories:\n\n| Category | Example |\n|:----------|:---------|\n| Object-Level | â€œReplace the red apple with a green one.â€ |\n| Scene Composition | â€œAdd sunlight streaming through the window.â€ |\n| Human-Centric | â€œChange the personâ€™s expression to smiling.â€ |\n| Text & Symbol | â€œUppercase the text on the billboard.â€ |\n| Stylistic | â€œConvert the image to a Van Gogh painting style.â€ |\n\n---\n\nPico-Banana-400K provides both **breadth** (diverse edit operations) and **depth** (quality-controlled multimodal supervision), making it a strong foundation for training and evaluating text-guided image editing models.\n\n## ğŸ§  Applications\n\n**Pico-Banana-400K** serves as a versatile resource for advancing controllable and instruction-aware image editing.  \nBeyond single-step editing, the dataset enables **multi-turn, conversational editing** and **reward-based training paradigms**.\n\n\n\n## ğŸ“¦ Dataset Download Guide\n\nThe **Pico-Banana-400K** dataset is hosted on Appleâ€™s public CDN.  \nYou ",
      "default_branch": "main"
    },
    "fetched_at": "2025-11-02T02:42:31.311004"
  },
  {
    "basic_info": {
      "name": "Paper2Video",
      "full_name": "showlab/Paper2Video",
      "owner": "showlab",
      "description": "Automatic Video Generation from Scientific Papers",
      "url": "https://github.com/showlab/Paper2Video",
      "clone_url": "https://github.com/showlab/Paper2Video.git",
      "ssh_url": "git@github.com:showlab/Paper2Video.git",
      "homepage": "https://showlab.github.io/Paper2Video/",
      "created_at": "2025-10-03T08:50:16Z",
      "updated_at": "2025-11-01T21:09:12Z",
      "pushed_at": "2025-10-20T12:20:48Z"
    },
    "stats": {
      "stars": 1392,
      "forks": 186,
      "watchers": 1392,
      "open_issues": 0,
      "size": 392267
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 931762,
        "TeX": 250773,
        "BibTeX Style": 26973,
        "Shell": 5155
      },
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "# Paper2Video\n\n<p align=\"right\">\n  <b>English</b> | <a href=\"./README-CN.md\">ç®€ä½“ä¸­æ–‡</a>\n</p>\n\n\n<p align=\"center\">\n  <b>Paper2Video: Automatic Video Generation from Scientific Papers</b>\n<br>\nä»å­¦æœ¯è®ºæ–‡è‡ªåŠ¨ç”Ÿæˆæ¼”è®²è§†é¢‘\n</p>\n\n<p align=\"center\">\n  <a href=\"https://zeyu-zhu.github.io/webpage/\">Zeyu Zhu*</a>,\n  <a href=\"https://qhlin.me/\">Kevin Qinghong Lin*</a>,\n  <a href=\"https://scholar.google.com/citations?user=h1-3lSoAAAAJ&hl=en\">Mike Zheng Shou</a> <br>\n  Show Lab, National University of Singapore\n</p>\n\n\n<p align=\"center\">\nÂ  <a href=\"https://arxiv.org/abs/2510.05096\">ğŸ“„ Paper</a> &nbsp; | &nbsp;\n  <a href=\"https://huggingface.co/papers/2510.05096\">ğŸ¤— Daily Paper</a> &nbsp; | &nbsp;\nÂ  <a href=\"https://huggingface.co/datasets/ZaynZhu/Paper2Video\">ğŸ“Š Dataset</a> &nbsp; | &nbsp;\nÂ  <a href=\"https://showlab.github.io/Paper2Video/\">ğŸŒ Project Website</a> &nbsp; | &nbsp;\nÂ  <a href=\"https://x.com/KevinQHLin/status/1976105129146257542\">ğŸ’¬ X (Twitter)</a>\n</p>\n\n- **Input:** a paper â• an image â• an audio\n  \n| Paper | Image | Audio |\n|--------|--------|--------|\n| <img src=\"https://github.com/showlab/Paper2Video/blob/page/assets/hinton/paper.png\" width=\"180\"/><br>[ğŸ”— Paper link](https://arxiv.org/pdf/1509.01626) | <img src=\"https://github.com/showlab/Paper2Video/blob/page/assets/hinton/hinton_head.jpeg\" width=\"180\"/> <br>Hinton's photo| <img src=\"assets/sound.png\" width=\"180\"/><br>[ğŸ”— Audio sample](https://github.com/showlab/Paper2Video/blob/page/assets/hinton/ref_audio_10.wav) |\n\n\n- **Output:** a presentation video\n\n\n\nhttps://github.com/user-attachments/assets/39221a9a-48cb-4e20-9d1c-080a5d8379c4\n\n\n\n\nCheck out more examples at [ğŸŒ project page](https://showlab.github.io/Paper2Video/).\n\n## ğŸ”¥ Update\n**Any contributions are welcome!**\n- [x] [2025.10.15] We update a new version without talking-head for fast generation!\n- [x] [2025.10.11] Our work receives attention on [YC Hacker News](https://news.ycombinator.com/item?id=45553701).\n- [x] [2025.10.9] Thanks AK for sharing our work on [Twitter](https://x.com/_akhaliq/status/1976099830004072849)!\n- [x] [2025.10.9] Our work is reported by [Medium](https://medium.com/@dataism/how-ai-learned-to-make-scientific-videos-from-slides-to-a-talking-head-0d807e491b27).\n- [x] [2025.10.8] Check out our demo video below!\n- [x] [2025.10.7] We release the [arxiv paper](https://arxiv.org/abs/2510.05096).\n- [x] [2025.10.6] We release the [code](https://github.com/showlab/Paper2Video) and [dataset](https://huggingface.co/datasets/ZaynZhu/Paper2Video).\n- [x] [2025.9.28] Paper2Video has been accepted to the **Scaling Environments for Agents Workshop([SEA](https://sea-workshop.github.io/)) at NeurIPS 2025**.\n\n\nhttps://github.com/user-attachments/assets/a655e3c7-9d76-4c48-b946-1068fdb6cdd9\n\n\n\n\n---\n\n### Table of Contents\n- [ğŸŒŸ Overview](#-overview)\n- [ğŸš€ Quick Start: PaperTalker](#-try-papertalker-for-your-paper-)\n  - [1. Requirements](#1-requirements)\n  - [2. Configure LLMs](#2-configure-llms)\n  - [3. Inference](#3-inference)\n- [ğŸ“Š Evaluation: Paper2Video](#-evaluation-paper2video)\n- [ğŸ˜¼ Fun: Paper2Video for Paper2Video](#-fun-paper2video-for-paper2video)\n- [ğŸ™ Acknowledgements](#-acknowledgements)\n- [ğŸ“Œ Citation](#-citation)\n\n---\n\n## ğŸŒŸ Overview\n<p align=\"center\">\n  <img src=\"assets/teaser.png\" alt=\"Overview\" width=\"100%\">\n</p>\n\nThis work solves two core problems for academic presentations:\n\n- **Left: How to create a presentation video from a paper?**  \n  *PaperTalker* â€” an agent that integrates **slides**, **subtitling**, **cursor grounding**, **speech synthesis**, and **talking-head video rendering**.\n\n- **Right: How to evaluate a presentation video?**  \n  *Paper2Video* â€” a benchmark with well-designed metrics to evaluate presentation quality.\n\n\n---\n\n## ğŸš€ Try PaperTalker for your Paper!\n<p align=\"center\">\n  <img src=\"assets/method.png\" alt=\"Approach\" width=\"100%\">\n</p>\n\n### 1. Requirements\nPrepare the environment:\n```bash\ncd src\nconda create -n p2v python=3.10\nconda activate p2v\npip install -r requirements.txt\nconda install -c conda-forge tectonic\n```\n**[Optional] [Skip](#2-configure-llms) this part if you do not need a human presenter.**\n\nDownload the dependent code and follow the instructions in **[Hallo2](https://github.com/fudan-generative-vision/hallo2)** to download the model weight.\n```bash\ngit clone https://github.com/fudan-generative-vision/hallo2.git\n```\nYou need to **prepare the environment separately for talking-head generation** to potential avoide package conflicts, please refer to  <a href=\"git clone https://github.com/fudan-generative-vision/hallo2.git\">Hallo2</a>. After installing, use `which python` to get the python environment path.\n```bash\ncd hallo2\nconda create -n hallo python=3.10\nconda activate hallo\npip install -r requirements.txt\n```\n\n### 2. Configure LLMs\nExport your **API credentials**:\n```bash\nexport GEMINI_API_KEY=\"your_gemini_key_here\"\nexport OPENAI_API_KEY=\"your_openai_key_here\"\n```\nThe best practice is to use **GPT4.1** or **Gemini2.5-Pro** for both LLM and VLMs. We also support locally depl",
      "default_branch": "main"
    },
    "fetched_at": "2025-11-02T02:42:32.610680"
  },
  {
    "basic_info": {
      "name": "wechat-selkies",
      "full_name": "nickrunning/wechat-selkies",
      "owner": "nickrunning",
      "description": "åŸºäºSelkiesçš„Linuxç½‘é¡µç‰ˆå¾®ä¿¡/QQï¼Œæ”¯æŒæœ¬åœ°ä¸­æ–‡è¾“å…¥æ³•ï¼Œæ”¯æŒAMD64å’ŒARM64ã€‚",
      "url": "https://github.com/nickrunning/wechat-selkies",
      "clone_url": "https://github.com/nickrunning/wechat-selkies.git",
      "ssh_url": "git@github.com:nickrunning/wechat-selkies.git",
      "homepage": "https://hub.docker.com/r/nickrunning/wechat-selkies",
      "created_at": "2025-10-12T10:03:40Z",
      "updated_at": "2025-11-02T02:25:02Z",
      "pushed_at": "2025-10-29T04:59:28Z"
    },
    "stats": {
      "stars": 1368,
      "forks": 107,
      "watchers": 1368,
      "open_issues": 6,
      "size": 140
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 14807,
        "Dockerfile": 4168,
        "Shell": 1770
      },
      "license": "MIT License",
      "topics": [
        "docker",
        "qq",
        "vnc",
        "web",
        "wechat"
      ]
    },
    "content": {
      "readme": "# WeChat Selkies\n\n[![GitHub Stars](https://img.shields.io/github/stars/nickrunning/wechat-selkies?style=flat-square&logo=github&color=yellow)](https://github.com/nickrunning/wechat-selkies/stargazers)\n[![GitHub Forks](https://img.shields.io/github/forks/nickrunning/wechat-selkies?style=flat-square&logo=github&color=blue)](https://github.com/nickrunning/wechat-selkies/network/members)\n[![GitHub Issues](https://img.shields.io/github/issues/nickrunning/wechat-selkies?style=flat-square&logo=github&color=red)](https://github.com/nickrunning/wechat-selkies/issues)\n[![GitHub License](https://img.shields.io/github/license/nickrunning/wechat-selkies?style=flat-square&color=green)](https://github.com/nickrunning/wechat-selkies/blob/master/LICENSE)\n[![Docker Pulls](https://img.shields.io/docker/pulls/nickrunning/wechat-selkies?style=flat-square&logo=docker&color=blue)](https://hub.docker.com/r/nickrunning/wechat-selkies)\n[![Docker Image Size](https://img.shields.io/docker/image-size/nickrunning/wechat-selkies?style=flat-square&logo=docker&color=orange)](https://hub.docker.com/r/nickrunning/wechat-selkies)\n[![GitHub Release](https://img.shields.io/github/v/release/nickrunning/wechat-selkies?style=flat-square&logo=github&include_prereleases)](https://github.com/nickrunning/wechat-selkies/releases)\n[![GitHub Workflow Status](https://img.shields.io/github/actions/workflow/status/nickrunning/wechat-selkies/docker.yml?style=flat-square&logo=github-actions&label=build)](https://github.com/nickrunning/wechat-selkies/actions)\n[![GitHub Last Commit](https://img.shields.io/github/last-commit/nickrunning/wechat-selkies?style=flat-square&logo=github&color=purple)](https://github.com/nickrunning/wechat-selkies/commits)\n\nä¸­æ–‡ | [English](README_en.md)\n\nåŸºäº Docker çš„å¾®ä¿¡/QQ Linux å®¢æˆ·ç«¯ï¼Œä½¿ç”¨ Selkies WebRTC æŠ€æœ¯æä¾›æµè§ˆå™¨è®¿é—®æ”¯æŒã€‚\n\n## é¡¹ç›®ç®€ä»‹\n\næœ¬é¡¹ç›®å°†å®˜æ–¹å¾®ä¿¡/QQ Linux å®¢æˆ·ç«¯å°è£…åœ¨ Docker å®¹å™¨ä¸­ï¼Œé€šè¿‡ Selkies æŠ€æœ¯å®ç°åœ¨æµè§ˆå™¨ä¸­ç›´æ¥ä½¿ç”¨å¾®ä¿¡/QQï¼Œæ— éœ€åœ¨æœ¬åœ°å®‰è£…å¾®ä¿¡/QQ å®¢æˆ·ç«¯ã€‚é€‚ç”¨äºæœåŠ¡å™¨éƒ¨ç½²ã€è¿œç¨‹åŠå…¬ç­‰åœºæ™¯ã€‚\n\n## å‡çº§æ³¨æ„äº‹é¡¹\n\n> å¦‚æœå‡çº§åéƒ¨åˆ†åŠŸèƒ½ç¼ºå¤±ï¼Œè¯·å…ˆæ¸…ç©ºæœ¬åœ°æŒ‚è½½ç›®å½•ä¸‹çš„openboxç›®å½•(å¦‚`./config/.config/openbox`)ã€‚\n\n## åŠŸèƒ½ç‰¹æ€§\n\n- ğŸŒ **æµè§ˆå™¨è®¿é—®**ï¼šé€šè¿‡ Web æµè§ˆå™¨ç›´æ¥ä½¿ç”¨å¾®ä¿¡ï¼Œæ— éœ€æœ¬åœ°å®‰è£…\n- ğŸ³ **DockeråŒ–éƒ¨ç½²**ï¼šç®€å•çš„å®¹å™¨åŒ–éƒ¨ç½²ï¼Œç¯å¢ƒéš”ç¦»\n- ğŸ”’ **æ•°æ®æŒä¹…åŒ–**ï¼šæ”¯æŒé…ç½®å’ŒèŠå¤©è®°å½•æŒä¹…åŒ–å­˜å‚¨\n- ğŸ¨ **ä¸­æ–‡æ”¯æŒ**ï¼šå®Œæ•´çš„ä¸­æ–‡å­—ä½“å’Œæœ¬åœ°åŒ–æ”¯æŒï¼Œæ”¯æŒæœ¬åœ°ä¸­æ–‡è¾“å…¥æ³•\n- ğŸ–¼ï¸ **å›¾ç‰‡å¤åˆ¶**ï¼šæ”¯æŒé€šè¿‡ä¾§è¾¹æ é¢æ¿å¼€å¯å›¾ç‰‡å¤åˆ¶\n- ğŸ“ **æ–‡ä»¶ä¼ è¾“**ï¼šæ”¯æŒé€šè¿‡ä¾§è¾¹æ é¢æ¿è¿›è¡Œæ–‡ä»¶ä¼ è¾“\n- ğŸ–¥ï¸ **AMD64å’ŒARM64æ¶æ„æ”¯æŒ**ï¼šå…¼å®¹ä¸»æµCPUæ¶æ„\n- ğŸ”§ **ç¡¬ä»¶åŠ é€Ÿ**ï¼šå¯é€‰çš„ GPU ç¡¬ä»¶åŠ é€Ÿæ”¯æŒ\n- ğŸªŸ **çª—å£åˆ‡æ¢å™¨**ï¼šå·¦ä¸Šè§’å¢åŠ åˆ‡æ¢æ‚¬æµ®çª—ï¼Œæ–¹ä¾¿åˆ‡æ¢åˆ°åå°çª—å£ï¼Œä¸ºåç»­æ·»åŠ å…¶å®ƒåŠŸèƒ½åšåŸºç¡€\n- ğŸ¤– **è‡ªåŠ¨å¯åŠ¨**ï¼šå¯é…ç½®è‡ªåŠ¨å¯åŠ¨å¾®ä¿¡å’ŒQQå®¢æˆ·ç«¯ï¼ˆå¯é€‰ï¼‰\n\n## æˆªå›¾å±•ç¤º\n![å¾®ä¿¡æˆªå›¾](./docs/images/wechat-selkies-1.jpg)\n![QQæˆªå›¾](./docs/images/wechat-selkies-2.jpg)\n\n## å¿«é€Ÿå¼€å§‹\n\n### ç¯å¢ƒè¦æ±‚\n\n- Docker\n- Docker Compose\n- æ”¯æŒWebRTCçš„ç°ä»£æµè§ˆå™¨ï¼ˆChromeã€Firefoxã€Safariç­‰ï¼‰\n\n### å¿«é€Ÿéƒ¨ç½²\n\n1. **ç›´æ¥ä½¿ç”¨å·²æ„å»ºçš„é•œåƒè¿›è¡Œå¿«é€Ÿéƒ¨ç½²**\n\nGitHub Container Registryé•œåƒï¼š\n```bash\ndocker run -it -p 3001:3001 -v ./config:/config --device /dev/dri:/dev/dri ghcr.io/nickrunning/wechat-selkies:latest\n```\n\nDocker Hubé•œåƒï¼š\n```bash\ndocker run -it -p 3001:3001 -v ./config:/config --device /dev/dri:/dev/dri nickrunning/wechat-selkies:latest\n```\n\n2. **è®¿é—®å¾®ä¿¡**\n   \n   åœ¨æµè§ˆå™¨ä¸­è®¿é—®ï¼š`https://localhost:3001` æˆ– `https://<æœåŠ¡å™¨IP>:3001`\n   > **æ³¨æ„ï¼š** æ˜ å°„3000ç«¯å£ç”¨äºHTTPè®¿é—®ï¼Œ3001ç«¯å£ç”¨äºHTTPSè®¿é—®ï¼Œå»ºè®®ä½¿ç”¨HTTPSã€‚\n\n### docker-compose éƒ¨ç½²\n1. **åˆ›å»ºé¡¹ç›®ç›®å½•å¹¶è¿›å…¥**\n   ```bash\n   mkdir wechat-selkies\n   cd wechat-selkies\n   ```\n2. **åˆ›å»º docker-compose.yml æ–‡ä»¶**\n   ```yaml\n    services:\n      wechat-selkies:\n        image: nickrunning/wechat-selkies:latest    # or ghcr.io/nickrunning/wechat-selkies:latest\n        container_name: wechat-selkies\n        ports:\n          - \"3000:3000\"       # http port\n          - \"3001:3001\"       # https port\n        restart: unless-stopped\n        volumes:\n          - ./config:/config\n        devices:\n          - /dev/dri:/dev/dri # optional, for hardware acceleration\n        environment:\n          - PUID=1000                    # user ID\n          - PGID=100                     # group ID\n          - TZ=Asia/Shanghai             # timezone\n          - LC_ALL=zh_CN.UTF-8           # locale\n          - AUTO_START_WECHAT=true       # default is true\n          - AUTO_START_QQ=false          # default is false\n          # - CUSTOM_USER=<Your Name>      # recommended to set a custom user name\n          # - PASSWORD=<Your Password>     # recommended to set a password for selkies web ui\n    ```\n3. **å¯åŠ¨æœåŠ¡**\n   ```bash\n   docker-compose up -d\n   ```\n\n### æºç éƒ¨ç½²\n\n1. **å…‹éš†é¡¹ç›®**\n   ```bash\n   git clone https://github.com/nickrunning/wechat-selkies.git\n   cd wechat-selkies\n   ```\n\n2. **å¯åŠ¨æœåŠ¡**\n   ```bash\n   docker-compose up -d\n   ```\n\n3. **è®¿é—®å¾®ä¿¡**\n\n   åœ¨æµè§ˆå™¨ä¸­è®¿é—®ï¼š`https://localhost:3001` æˆ– `https://<æœåŠ¡å™¨IP>:3001`\n\n### é…ç½®è¯´æ˜\n\næ›´å¤šè‡ªå®šä¹‰é…ç½®è¯·å‚è€ƒ [Selkies Base Images from LinuxServer](https://github.com/linuxserver/docker-baseimage-selkies)ã€‚\n\n#### Docker Hub æ¨é€é…ç½®\n\næœ¬é¡¹ç›®æ”¯æŒåŒæ—¶æ¨é€åˆ° GitHub Container Registry å’Œ Docker Hubã€‚å¦‚éœ€å¯ç”¨ Docker Hub æ¨é€åŠŸèƒ½ï¼Œè¯·åœ¨ä»“åº“ä¸‹æ·»åŠ Environment Secretså’ŒEnvironment Variables:\n\n**Environment Secrets:**\n* DOCKERHUB_USERNAME: ä½ çš„ Docker Hub ç”¨æˆ·å\n* DOCKERHUB_TOKEN: ä½ çš„ Docker Hub Access Token\n**Environment Variables:**\n* ENABLE_DOCKERHUB: è®¾ç½®ä¸º `true` æ¥å¯ç”¨ Docker Hub æ¨é€\n\n#### ç¯å¢ƒå˜é‡é…ç½®\n\nåœ¨ `docker-compose.yml` ä¸­å¯ä»¥é…ç½®ä»¥ä¸‹ç¯å¢ƒå˜é‡ï¼š\n\n| å˜é‡å | é»˜è®¤å€¼ | è¯´æ˜ |\n|--------|--------|------|\n| `TITLE` | `WeChat Selkies` |",
      "default_branch": "master"
    },
    "fetched_at": "2025-11-02T02:42:33.896984"
  },
  {
    "basic_info": {
      "name": "DeepAnalyze",
      "full_name": "ruc-datalab/DeepAnalyze",
      "owner": "ruc-datalab",
      "description": "DeepAnalyze is the first agentic LLM for autonomous data science.",
      "url": "https://github.com/ruc-datalab/DeepAnalyze",
      "clone_url": "https://github.com/ruc-datalab/DeepAnalyze.git",
      "ssh_url": "git@github.com:ruc-datalab/DeepAnalyze.git",
      "homepage": "",
      "created_at": "2025-10-11T11:19:21Z",
      "updated_at": "2025-11-02T02:38:07Z",
      "pushed_at": "2025-10-29T01:05:51Z"
    },
    "stats": {
      "stars": 1269,
      "forks": 148,
      "watchers": 1269,
      "open_issues": 8,
      "size": 22889
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 5372510,
        "Shell": 220988,
        "Jupyter Notebook": 61761,
        "Jinja": 3956,
        "Dockerfile": 1056,
        "Makefile": 997,
        "Batchfile": 764,
        "Scheme": 403
      },
      "license": "MIT License",
      "topics": [
        "agent",
        "agentic",
        "agentic-ai",
        "ai",
        "ai-scientist",
        "chatbot",
        "chatgpt",
        "data",
        "data-analysis",
        "data-engineering",
        "data-science",
        "data-visualization",
        "database",
        "gpt",
        "llama",
        "llm",
        "qwen",
        "science",
        "structured-data",
        "vllm"
      ]
    },
    "content": {
      "readme": "<p align=\"center\" width=\"100%\">\n<img src=\"assets/logo.png\" alt=\"DeepAnalyze\" style=\"width: 60%; min-width: 300px; display: block; margin: auto;\">\n</p>\n\n# DeepAnalyze: Agentic Large Language Models for Autonomous Data Science\n[![arXiv](https://img.shields.io/badge/arXiv-2510.16872-b31b1b.svg?logo=arXiv)](https://arxiv.org/abs/2510.16872)\n[![homepage](https://img.shields.io/badge/%F0%9F%8C%90%20Homepage%20-DeepAnalyze%20Cases-blue.svg)](https://ruc-deepanalyze.github.io/)\n[![model](https://img.shields.io/badge/%F0%9F%A4%97%20Huggingface%20-DeepAnalyze--8B-orange.svg)](https://huggingface.co/RUC-DataLab/DeepAnalyze-8B)\n[![data](https://img.shields.io/badge/%F0%9F%93%9A%20Datasets%20-DataScience--Instruct--500K-darkgreen.svg)](https://huggingface.co/datasets/RUC-DataLab/DataScience-Instruct-500K)\n[![star](https://img.shields.io/github/stars/ruc-datalab/DeepAnalyze?style=social&label=Code+Stars)](https://github.com/ruc-datalab/DeepAnalyze)\n![Badge](https://hitscounter.dev/api/hit?url=https%3A%2F%2Fgithub.com%2Fruc-datalab%2FDeepAnalyze&label=Visitors&icon=graph-up&color=%23dc3545&message=&style=flat&tz=UTC)  [![wechat](https://img.shields.io/badge/WeChat-%E5%8A%A0%E5%85%A5DeepAnalyze%E4%BA%A4%E6%B5%81%E8%AE%A8%E8%AE%BA%E7%BE%A4-black?logo=wechat&logoColor=07C160)](./assets/wechat.jpg) \n\n[![twitter](https://img.shields.io/badge/@Brian%20Roemmele-gray?logo=x&logoColor=white&labelColor=black)](https://x.com/BrianRoemmele/status/1981015483823571352) [![twitter](https://img.shields.io/badge/@Dr%20Singularity-gray?logo=x&logoColor=white&labelColor=black)](https://x.com/Dr_Singularity/status/1981010771338498241) [![twitter](https://img.shields.io/badge/@Gorden%20Sun-gray?logo=x&logoColor=white&labelColor=black)](https://x.com/Gorden_Sun/status/1980573407386423408) [![twitter](https://img.shields.io/badge/@AIGCLINK-gray?logo=x&logoColor=white&labelColor=black)](https://x.com/aigclink/status/1980554517126246642) [![twitter](https://img.shields.io/badge/@Python%20Developer-gray?logo=x&logoColor=white&labelColor=black)](https://x.com/Python_Dv/status/1980667557318377871) [![twitter](https://img.shields.io/badge/@meng%20shao-gray?logo=x&logoColor=white&labelColor=black)](https://x.com/shao__meng/status/1980623242114314531) \n\n\n> **Authors**: **[Shaolei Zhang](https://zhangshaolei1998.github.io/), [Ju Fan*](http://iir.ruc.edu.cn/~fanj/), [Meihao Fan](https://scholar.google.com/citations?user=9RTm2qoAAAAJ), [Guoliang Li](https://dbgroup.cs.tsinghua.edu.cn/ligl/), [Xiaoyong Du](http://info.ruc.edu.cn/jsky/szdw/ajxjgcx/jsjkxyjsx1/js2/7374b0a3f58045fc9543703ccea2eb9c.htm)**\n>\n> Renmin University of China, Tsinghua University\n\n\n**DeepAnalyze** is the first agentic LLM for autonomous data science. It can autonomously complete a wide range of data-centric tasks without human intervention, supporting:\n- ğŸ›  **Entire data science pipeline**: Automatically perform any data science tasks such as data preparation, analysis, modeling, visualization, and report generation.\n- ğŸ” **Open-ended data research**: Conduct deep research on diverse data sources, including structured data (Databases, CSV, Excel), semi-structured data (JSON, XML, YAML), and unstructured data (TXT, Markdown), and finally produce analyst-grade research reports.\n- ğŸ“Š **Fully open-source**: The [model](https://huggingface.co/RUC-DataLab/DeepAnalyze-8B), [code](https://github.com/ruc-datalab/DeepAnalyze), [training data](https://huggingface.co/datasets/RUC-DataLab/DataScience-Instruct-500K), and [demo](https://huggingface.co/RUC-DataLab/DeepAnalyze-8B) of DeepAnalyze are all open-sourced, allowing you to deploy or extend your own data analysis assistant.\n\n<p align=\"center\" width=\"100%\">\n<img src=\"./assets/deepanalyze.jpg\" alt=\"deepanalyze\" style=\"width: 70%; min-width: 300px; display: block; margin: auto;\">\n</p>\n\n\n## ğŸ”¥ News\n- **[2025.10.28]**: We welcome all contributions, including improving the DeepAnalyze and sharing use cases (see [`CONTRIBUTION.md`](CONTRIBUTION.md)). All merged PRs will be listed as contributors.\n- **[2025.10.27]**: DeepAnalyze has attracted widespread attention, gaining **1K+** GitHub stars and **200K+** Twitter views within a week.\n- **[2025.10.21]**: DeepAnalyze's [paper](https://arxiv.org/abs/2510.16872), [code](https://github.com/ruc-datalab/DeepAnalyze), [model](https://huggingface.co/RUC-DataLab/DeepAnalyze-8B), [training data](https://huggingface.co/datasets/RUC-DataLab/DataScience-Instruct-500K) are released!\n\n## ğŸ–¥ Demo\n\n\n<p align=\"center\" width=\"100%\">\nUpload the data, DeepAnalyze can perform data-oriented deep research ğŸ” and any data-centric tasks ğŸ› \n</p>\n\nhttps://github.com/user-attachments/assets/04184975-7ee7-4ae0-8761-7a7550c5c8fe\n\n> [!TIP]\n>\n> Clone this repository to deploy DeepAnalyze locally as your data analyst, completing any data science tasks without any workflow or closed-source APIs.\n>\n> ğŸ”¥ The UI of the demo is an initial version. Welcome to further develop it, and we will include you as a contributor.\n\n\n- Clone this repo and dow",
      "default_branch": "main"
    },
    "fetched_at": "2025-11-02T02:42:35.193187"
  },
  {
    "basic_info": {
      "name": "DeekSeek-OCR---Dockerized-API",
      "full_name": "Bogdanovich77/DeekSeek-OCR---Dockerized-API",
      "owner": "Bogdanovich77",
      "description": null,
      "url": "https://github.com/Bogdanovich77/DeekSeek-OCR---Dockerized-API",
      "clone_url": "https://github.com/Bogdanovich77/DeekSeek-OCR---Dockerized-API.git",
      "ssh_url": "git@github.com:Bogdanovich77/DeekSeek-OCR---Dockerized-API.git",
      "homepage": null,
      "created_at": "2025-10-21T23:30:09Z",
      "updated_at": "2025-11-01T21:17:46Z",
      "pushed_at": "2025-10-22T19:32:31Z"
    },
    "stats": {
      "stars": 958,
      "forks": 103,
      "watchers": 958,
      "open_issues": 9,
      "size": 117
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 160119,
        "Batchfile": 2335,
        "Dockerfile": 2214
      },
      "license": null,
      "topics": []
    },
    "content": {
      "readme": "# DeepSeek-OCR: PDF to Markdown Converter\n\nA powerful OCR solution that converts PDF documents to Markdown format using DeepSeek-OCR with FastAPI backend. This project provides both a batch processing script and a REST API for flexible document conversion.\n\n## ğŸš€ Quick Start\n\n### Option 1: Batch Processing with pdf_to_markdown_processor.py\n\n1. Place your PDF files in the `data/` directory\n2. Ensure the DeepSeek-OCR API is running (see Docker setup below)\n3. Run the processor:\n\n```bash\npython pdf_to_markdown_processor.py\n```\n\n### Option 2: REST API with Docker Backend\n\n1. Build and start the Docker container\n2. Use the API endpoints to process documents\n3. Integrate with your applications\n\n---\n\n## ğŸ“‹ Prerequisites\n\n### Hardware Requirements\n- **NVIDIA GPU** with CUDA 11.8+ support\n- **GPU Memory**: Minimum 12GB VRAM (Model takes ~9GB)\n- **System RAM**: Minimum 32GB (recommended: 64GB+)\n- **Storage**: 50GB+ free space for model and containers\n\n### Software Requirements\n- **Python 3.8+** (for local processing)\n- **Docker** 20.10+ with GPU support\n- **Docker Compose** 2.0+\n- **NVIDIA Container Toolkit** installed\n- **CUDA 11.8** compatible drivers\n\n---\n\n## ğŸ³ Docker Backend Setup\n\n### 1. Download Model Weights\n\nCreate a directory for model weights and download the DeepSeek-OCR model:\n\n```bash\n# Create models directory\nmkdir -p models\n\n# Download using Hugging Face CLI\npip install huggingface_hub\nhuggingface-cli download deepseek-ai/DeepSeek-OCR --local-dir models/deepseek-ai/DeepSeek-OCR\n\n# Or using git\ngit clone https://huggingface.co/deepseek-ai/DeepSeek-OCR models/deepseek-ai/DeepSeek-OCR\n```\n\n### 2. Build and Run the Docker Container\n\n#### Windows Users\n\n```cmd\nREM Build the Docker image\nbuild.bat\n\nREM Start the service\ndocker-compose up -d\n\nREM Check logs\ndocker-compose logs -f deepseek-ocr\n```\n\n#### Linux/macOS Users\n\n```bash\n# Build the Docker image\ndocker-compose build\n\n# Start the service\ndocker-compose up -d\n\n# Check logs\ndocker-compose logs -f deepseek-ocr\n```\n\n### 3. Verify Installation\n\n```bash\n# Health check\ncurl http://localhost:8000/health\n\n# Expected response:\n{\n  \"status\": \"healthy\",\n  \"model_loaded\": true,\n  \"model_path\": \"/app/models/deepseek-ai/DeepSeek-OCR\",\n  \"cuda_available\": true,\n  \"cuda_device_count\": 1\n}\n```\n\n---\n\n## ğŸ“„ PDF Processing Scripts\n\nThis project provides several PDF processing scripts, each designed for different use cases. All scripts scan the `data/` directory for PDF files and convert them to Markdown format with different prompts and post-processing options.\n\n### Output Naming Convention\n\nAll processors append a suffix to the output filename to indicate the processing method used:\n- **-MD.md**: Markdown conversion (preserves document structure)\n- **-OCR.md**: Plain OCR extraction (raw text without formatting)\n- **-CUSTOM.md**: Custom prompt processing (uses prompt from YAML file)\n\nFor example, processing `document.pdf` will create:\n- `document-MD.md` (markdown processors)\n- `document-OCR.md` (OCR processor)\n- `document-CUSTOM.md` (custom prompt processors)\n\n---\n\n### 1. pdf_to_markdown_processor.py\n\n**Purpose**: Basic PDF to Markdown conversion using the standard markdown prompt\n\n**Features**:\n- Uses prompt: `'<image>\\n<|grounding|>Convert the document to markdown.'`\n- Converts PDFs to structured Markdown format\n- Simple processing without image extraction\n- Outputs files with `-MD.md` suffix\n\n**Usage**:\n```bash\n# Place PDF files in the data directory\ncp your_document.pdf data/\n\n# Run the processor\npython pdf_to_markdown_processor.py\n\n# Check results\nls data/*-MD.md\n```\n\n---\n\n### 2. pdf_to_markdown_processor_enhanced.py\n\n**Purpose**: Enhanced PDF to Markdown conversion with post-processing\n\n**Features**:\n- Uses the same markdown prompt as the basic version\n- **Post-processing features**:\n  - Image extraction and saving to `data/images/` folder\n  - Special token cleanup\n  - Reference processing for layout information\n  - Content cleaning and formatting\n- Outputs files with `-MD.md` suffix\n\n**Usage**:\n```bash\n# Place PDF files in the data directory\ncp your_document.pdf data/\n\n# Run the enhanced processor\npython pdf_to_markdown_processor_enhanced.py\n\n# Check results (including extracted images)\nls data/*-MD.md\nls data/images/\n```\n\n---\n\n### 3. pdf_to_ocr_enhanced.py\n\n**Purpose**: Plain OCR text extraction without markdown formatting\n\n**Features**:\n- Uses OCR prompt: `'<image>\\nFree OCR.'`\n- Extracts raw text without markdown structure\n- Includes the same post-processing features as the enhanced markdown processor\n- Outputs files with `-OCR.md` suffix\n\n**Usage**:\n```bash\n# Place PDF files in the data directory\ncp your_document.pdf data/\n\n# Run the OCR processor\npython pdf_to_ocr_enhanced.py\n\n# Check results\nls data/*-OCR.md\n```\n\n---\n\n### 4. pdf_to_custom_prompt.py\n\n**Purpose**: PDF processing with custom prompts (raw output)\n\n**Features**:\n- Uses custom prompt loaded from `custom_prompt.yaml`\n- Returns raw model response without post-processing\n- Ideal for testing and debugging ",
      "default_branch": "main"
    },
    "fetched_at": "2025-11-02T02:42:36.443529"
  },
  {
    "basic_info": {
      "name": "World-in-Novel-View",
      "full_name": "tianrun-chen/World-in-Novel-View",
      "owner": "tianrun-chen",
      "description": "Scaling Novel View Synthesis for Static and Dynamic Scenes",
      "url": "https://github.com/tianrun-chen/World-in-Novel-View",
      "clone_url": "https://github.com/tianrun-chen/World-in-Novel-View.git",
      "ssh_url": "git@github.com:tianrun-chen/World-in-Novel-View.git",
      "homepage": null,
      "created_at": "2025-10-17T14:43:19Z",
      "updated_at": "2025-10-31T06:53:25Z",
      "pushed_at": "2025-10-26T15:22:16Z"
    },
    "stats": {
      "stars": 869,
      "forks": 0,
      "watchers": 869,
      "open_issues": 0,
      "size": 9996
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 187502,
        "Shell": 4789
      },
      "license": null,
      "topics": []
    },
    "content": {
      "readme": "# World-in-Novel-View\n\n[![Stars](https://img.shields.io/github/stars/tianrun-chen/World-in-Novel-View?style=social)](https://github.com/tianrun-chen/World-in-Novel-View/stargazers)\n[![Python](https://img.shields.io/badge/Python-3.8%2B-blue)](https://www.python.org/)\n\n**Scaling Novel View Synthesis for Static and Dynamic Scenes**\n\n![(im3.gif)](https://github.com/tianrun-chen/World-in-Novel-View/blob/main/im3.gif)\n\n- **Distributed Training**: FSDP and DDP support for large-scale training\n- **Mixed Precision**: Automatic mixed precision (AMP) for faster training\n---\nTrain and Inference with Ascend 910b NPU\nTorch.NPU is requiredã€‚\n\n\n## Project Structure\n```\n.\nâ”œâ”€â”€ geope_core/              # Core GeoPE implementation\nâ”‚   â”œâ”€â”€ torch.py             # PyTorch GeoPE attention\nâ”‚   â””â”€â”€ utils/               # Utility modules\nâ”‚       â”œâ”€â”€ config.py        # Configuration utilities\nâ”‚       â”œâ”€â”€ functional.py    # Functional utilities\nâ”‚       â”œâ”€â”€ mha.py           # Multi-head attention\nâ”‚       â”œâ”€â”€ runner.py        # Training runner\nâ”‚       â””â”€â”€ transformer.py   # Transformer components\nâ”œâ”€â”€ src/                     # Application code\nâ”‚   â”œâ”€â”€ geope_attention/     # GeoPE attention wrapper\nâ”‚   â”œâ”€â”€ geope_utils/         # GeoPE utilities wrapper\nâ”‚   â”œâ”€â”€ nvs_models/          # Novel view synthesis models\nâ”‚   â”œâ”€â”€ nvs_data/            # Data loading and preprocessing\nâ”‚   â””â”€â”€ nvs_training/        # Training and evaluation\nâ”œâ”€â”€ tests/                   # Unit tests\nâ””â”€â”€ scripts/                 # Utility scripts\n```\n### Training\n\n```bash\n# Single NPU training\npython -m src.main konet\n\n# Multi-NPU training with FSDP\ntorchrun --standalone --nproc-per-node=4 -m src.main konet-fsdp\n```\n\n\n\n# Dataset: KOKONI-WorldVID-1A\n\nKOKONI-WorldVID-1A is a large-scale video dataset designed for **Novel View Synthesis** research. It contains over **10,000 unique videos** sourced from **Bilibili**, one of China's leading video-sharing platforms.\n\n## ğŸ’¡ Dataset Highlights\n\nUnlike most existing novel view synthesis datasets, KOKONI-WorldVID-1A provides videos from real-world, diverse scenarios with a unique data domain. These videos are created by a wide range of content creators, covering everything from static landscapes and object displays to dynamic human activities and lifestyle recordings.\n\n- **Data Source**: All videos are sourced from Bilibili, providing the research community with a perspective distinct from Western-dominated datasets.\n- **Scale**: Contains over 10,000 unique videos, offering sufficient data for deep learning model training.\n- **Content Diversity**: Videos encompass a wide variety of content, helping to improve model generalization in complex real-world scenarios.\n- **Static & Dynamic**: The dataset includes both static and partially dynamic videos. For static scene videos, we additionally provide human-screened static segments to facilitate more fine-grained model training and evaluation.\n\n## ğŸ“Š Dataset Statistics\n\n| Category | Count | Description |\n|----------|-------|-------------|\n| **Total Videos** | 10,000 | Unique videos from Bilibili |\n| **Static Videos** | ~5,000 | Videos with static scenes and annotated segments |\n| **Dynamic Videos** | ~5,000 | Videos with dynamic content (e.g., walking, movement) |\n| **Data Domain** | Chinese UGC | User-generated content from China |\n| **Application** | Novel View Synthesis | Training and evaluation of NVS models |\n\n## ğŸ“‚ Data Structure\n\nThe dataset is organized using two CSV files that contain metadata for all videos:\n\n### 1. `static.csv` - Static Scene Videos\n\nContains videos with static scenes and human-annotated time segments.\n\n**Format:**\n```\nåºå·,URL,è§†é¢‘æ ‡é¢˜,é™æ€å¼€å§‹æ—¶é—´1,é™æ€ç»“æŸæ—¶é—´1,é™æ€å¼€å§‹æ—¶é—´2,é™æ€ç»“æŸæ—¶é—´2\n1,https://www.bilibili.com/video/BV1xx411c7mD,Beautiful Landscape,00:10,00:35,01:20,02:15\n2,https://www.bilibili.com/video/BV1Ab411q7yH,Object Display,00:00,00:45,,\n```\n\n**Columns:**\n- `åºå·` (Index): Sequential number\n- `URL`: Bilibili video URL\n- `è§†é¢‘æ ‡é¢˜` (Video Title): Original video title\n- `é™æ€å¼€å§‹æ—¶é—´1` (Static Start Time 1): Start time of first static segment (format: MM:SS or HH:MM:SS)\n- `é™æ€ç»“æŸæ—¶é—´1` (Static End Time 1): End time of first static segment\n- `é™æ€å¼€å§‹æ—¶é—´2` (Static Start Time 2): Start time of second static segment (optional)\n- `é™æ€ç»“æŸæ—¶é—´2` (Static End Time 2): End time of second static segment (optional)\n\n**Note:** Additional segment pairs may exist (é™æ€å¼€å§‹æ—¶é—´3, é™æ€ç»“æŸæ—¶é—´3, etc.)\n\n### 2. `walk.csv` - Dynamic Scene Videos\n\nContains videos with dynamic content such as walking, movement, or changing scenes.\n\n**Format:**\n```\nåºå·,URL,è§†é¢‘æ ‡é¢˜\n1,https://www.bilibili.com/video/BV1yZ4y1u7fA,City Walk Tour\n2,https://www.bilibili.com/video/BV1Hx411v7iP,Campus Walking\n```\n\n**Columns:**\n- `åºå·` (Index): Sequential number\n- `URL`: Bilibili video URL\n- `è§†é¢‘æ ‡é¢˜` (Video Title): Original video title\n\n## ğŸ“¥ Download & Usage\n\nWe provide a Python script (`download_videos.py`) to help users batch download videos from the dataset. Please follow these steps:\n\n### 1. Install Dependencies\n\n```bash\npip install you-get pandas\n```\n\nOr alternativel",
      "default_branch": "main"
    },
    "fetched_at": "2025-11-02T02:42:37.729642"
  },
  {
    "basic_info": {
      "name": "Emu3.5",
      "full_name": "baaivision/Emu3.5",
      "owner": "baaivision",
      "description": "Native Multimodal Models are World Learners",
      "url": "https://github.com/baaivision/Emu3.5",
      "clone_url": "https://github.com/baaivision/Emu3.5.git",
      "ssh_url": "git@github.com:baaivision/Emu3.5.git",
      "homepage": "",
      "created_at": "2025-10-29T13:40:19Z",
      "updated_at": "2025-11-02T02:37:25Z",
      "pushed_at": "2025-11-02T01:56:18Z"
    },
    "stats": {
      "stars": 868,
      "forks": 26,
      "watchers": 868,
      "open_issues": 15,
      "size": 22315
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 221124
      },
      "license": "Apache License 2.0",
      "topics": []
    },
    "content": {
      "readme": "<div align='center'>\n<h1>Emu3.5: Native Multimodal Models are World Learners</h1>\n\nEmu3.5 Team, BAAI\n\n[Project Page](https://emu.world/) | [ğŸ¤—HF Models](https://huggingface.co/collections/BAAI/emu35) | [Paper](https://arxiv.org/pdf/2510.26583)\n</div>\n\n\n<div align='center'>\n<img src=\"./assets/arch.png\" class=\"interpolation-image\" alt=\"arch.\" height=\"100%\" width=\"100%\" />\n</div>\n\n\n<div align='center'>\n<img src=\"./assets/co.png\" class=\"interpolation-image\" alt=\"arch.\" height=\"90%\" width=\"90%\" />\n</div>\n\n\n|  ğŸ”¹ | **Core Concept**                         | **Description**                                                                                                                            |\n| :-: | :--------------------------------------- | :----------------------------------------------------------------------------------------------------------------------------------------- |\n|  ğŸ§  | **Unified World Modeling**               | Predicts the **next state jointly across vision and language**, enabling coherent **world modeling** and **generation**.              |\n|  ğŸ§© | **End-to-End Pretraining**               | Trained with a **unified next-token prediction** objective over **interleaved visionâ€“language sequences**.                                 |\n|  ğŸ“š | **Over 10T+ Multimodal Tokens**               | Pre-trained on **over 10 trillion interleaved tokens** from **video frames** and **transcripts**, capturing **spatiotemporal structure**.       |\n|  ğŸ”„ | **Native Multimodal I/O**                | Processes and generates **interleaved visualâ€“text sequences** without **modality adapters** or **task-specific heads**.                    |\n|  ğŸ¯ | **RL Post-Training**                     | Large-scale **reinforcement learning** enhances **reasoning**, **compositionality**, and **generation quality**.                           |\n|  âš¡  | **Discrete Diffusion Adaptation (DiDA)** | Converts **sequential decoding â†’ bidirectional parallel prediction**, achieving **â‰ˆ20Ã— faster inference without performance loss**.      |\n| ğŸ–¼ï¸ | **Versatile Generation**                 | Excels in **long-horizon visionâ€“language generation**, **any-to-image (X2I)** synthesis, and **text-rich image creation**.                 |\n|  ğŸŒ | **Generalizable World Modeling**         | Enables **spatiotemporally consistent world exploration**, and **open-world embodied manipulation** across diverse scenarios.          |\n|  ğŸ† | **Performance Benchmark**                | Matches **Gemini 2.5 Flash Image (Nano Banana)** on **image generation/editing**, and **outperforms** on **interleaved generation tasks**. |\n\n\n\n## Table of Contents\n\n1. [Model & Weights](#1-model--weights)\n2. [Quick Start](#2-quick-start)\n3. [Schedule](#3-schedule)\n4. [Citation](#4-citation)\n\n## 1. Model & Weights\n\n| Model name               | HF Weight |\n| ------------------------ | --------- |\n| Emu3.5               | [ğŸ¤— HF link](https://huggingface.co/BAAI/Emu3.5/tree/main) |\n| Emu3.5-Image                | [ğŸ¤— HF link](https://huggingface.co/BAAI/Emu3.5-Image/tree/main) |\n| Emu3.5-VisionTokenizer     | [ğŸ¤— HF link](https://huggingface.co/BAAI/Emu3.5-VisionTokenizer/tree/main) |\n\n\n*Note:*  \n- **Emu3.5** supports general multimodal predictions, including interleaved image-text generation and image editing.\n- **Emu3.5-Image** is further optimized for high-fidelity text-to-image generation and single/multiple image editing.  \n- Both models are pure next-token predictors without DiDA acceleration (each image may take several minutes to generate).  \n- âš¡ **Stay tuned for DiDA-accelerated weights.**\n\n> ğŸ’¡ **Usage tip:**  \n> For **interleaved image-text generation**, use **Emu3.5**.  \n> For **single-image generation** (T2I and X2I editing), use **Emu3.5-Image** for the best quality.\n\n\n\n## 2. Quick Start\n\n### Environment Setup\n\n```bash\n# Python 3.10 or higher is required.\ngit clone https://github.com/baaivision/Emu3.5\ncd Emu3.5\npip install -r requirements.txt\npip install flash_attn==2.8.3 --no-build-isolation\n```\n### Configuration\n\nEdit `configs/config.py` to set:\n\n- Paths: `model_path`, `vq_path`\n- Task template: `task_type in {t2i, x2i, howto, story, explore, vla}`\n- Input image: `use_image` (True to provide reference images, controls <|IMAGE|> token); set `reference_image` in each prompt to specify the image path. For x2i task, 'reference_image' should be a list rather than a single path to be compatible with multi-image input.\n- Sampling: `sampling_params` (classifier_free_guidance, temperature, top_k/top_p, etc.)\n\n### Run Inference\n\n```bash\npython inference.py --cfg configs/config.py\n```\n\n\n#### Example Configurations by Task\nBelow are example commands for different tasks.\nMake sure to set CUDA_VISIBLE_DEVICES according to your available GPUs.\n\n\n```bash\n# ğŸ–¼ï¸ Text-to-Image (T2I) task\nCUDA_VISIBLE_DEVICES=0 python inference.py --cfg configs/example_config_t2i.py\n\n# ğŸ”„ Any-to-Image (X2I) task\nCUDA_VISIBLE_DEVICES=0,1 python inference.py --cfg configs/example_config_x2i.py\n\n# ğŸ¯ Visual Guidance task",
      "default_branch": "main"
    },
    "fetched_at": "2025-11-02T02:42:39.018694"
  },
  {
    "basic_info": {
      "name": "LongCat-Video",
      "full_name": "meituan-longcat/LongCat-Video",
      "owner": "meituan-longcat",
      "description": null,
      "url": "https://github.com/meituan-longcat/LongCat-Video",
      "clone_url": "https://github.com/meituan-longcat/LongCat-Video.git",
      "ssh_url": "git@github.com:meituan-longcat/LongCat-Video.git",
      "homepage": null,
      "created_at": "2025-10-25T06:49:49Z",
      "updated_at": "2025-11-01T23:58:01Z",
      "pushed_at": "2025-10-29T03:22:38Z"
    },
    "stats": {
      "stars": 865,
      "forks": 56,
      "watchers": 865,
      "open_issues": 4,
      "size": 1300747
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 327279
      },
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "# LongCat-Video\n\n<div align=\"center\">\n  <img src=\"assets/longcat-video_logo.svg\" width=\"45%\" alt=\"LongCat-Video\" />\n</div>\n<hr>\n\n<div align=\"center\" style=\"line-height: 1;\">\n  <a href='https://meituan-longcat.github.io/LongCat-Video/'><img src='https://img.shields.io/badge/Project-Page-green'></a>\n  <a href='https://arxiv.org/abs/2510.22200'><img src='https://img.shields.io/badge/Technique-Report-red'></a>\n  <a href='https://huggingface.co/meituan-longcat/LongCat-Video'><img src='https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Model-blue'></a>\n</div>\n\n<div align=\"center\" style=\"line-height: 1;\">\n  <a href='https://github.com/meituan-longcat/LongCat-Flash-Chat/blob/main/figures/wechat_official_accounts.png'><img src='https://img.shields.io/badge/WeChat-LongCat-brightgreen?logo=wechat&logoColor=white'></a>  \n  <a href='https://x.com/Meituan_LongCat'><img src='https://img.shields.io/badge/Twitter-LongCat-white?logo=x&logoColor=white'></a>\n</div>\n\n<div align=\"center\" style=\"line-height: 1;\">\n  <a href='LICENSE'><img src='https://img.shields.io/badge/License-MIT-f5de53?&color=f5de53'></a>\n</div>\n\n## Model Introduction\nWe introduce LongCat-Video, a foundational video generation model with 13.6B parameters, delivering strong performance across *Text-to-Video*, *Image-to-Video*, and *Video-Continuation* generation tasks. It particularly excels in efficient and high-quality long video generation, representing our first step toward world models.\n\n### Key Features\n- ğŸŒŸ **Unified architecture for multiple tasks**: LongCat-Video unifies *Text-to-Video*, *Image-to-Video*, and *Video-Continuation* tasks within a single video generation framework. It natively supports all these tasks with a single model and consistently delivers strong performance across each individual task.\n- ğŸŒŸ **Long video generation**: LongCat-Video is natively pretrained on *Video-Continuation* tasks, enabling it to produce minutes-long videos without color drifting or quality degradation.\n- ğŸŒŸ **Efficient inference**: LongCat-Video generates $720p$, $30fps$ videos within minutes by employing a coarse-to-fine generation strategy along both the temporal and spatial axes. Block Sparse Attention further enhances efficiency, particularly at high resolutions\n- ğŸŒŸ **Strong performance with multi-reward RLHF**: Powered by multi-reward Group Relative Policy Optimization (GRPO), comprehensive evaluations on both internal and public benchmarks demonstrate that LongCat-Video achieves performance comparable to leading open-source video generation models as well as the latest commercial solutions.\n\nFor more detail, please refer to the comprehensive [***LongCat-Video Technical Report***](https://arxiv.org/abs/2510.22200).\n\n## ğŸ¥ Teaser Video\n\n<div align=\"center\">\n  <video src=\"https://github.com/user-attachments/assets/00fa63f0-9c4e-461a-a79e-c662ad596d7d\" width=\"2264\" height=\"384\"> </video>\n</div>\n\n## Quick Start\n\n### Installation\n\nClone the repo:\n\n```shell\ngit clone --single-branch --branch main https://github.com/meituan-longcat/LongCat-Video\ncd LongCat-Video\n```\n\nInstall dependencies:\n\n```shell\n# create conda environment\nconda create -n longcat-video python=3.10\nconda activate longcat-video\n\n# install torch (configure according to your CUDA version)\npip install torch==2.6.0+cu124 torchvision==0.21.0+cu124 torchaudio==2.6.0 --index-url https://download.pytorch.org/whl/cu124\n\n# install flash-attn-2\npip install ninja \npip install psutil \npip install packaging \npip install flash_attn==2.7.4.post1\n\n# install other requirements\npip install -r requirements.txt\n```\n\nFlashAttention-2 is enabled in the model config by default; you can also change the model config (\"./weights/LongCat-Video/dit/config.json\") to use FlashAttention-3 or xformers once installed.\n\n### Model Download\n\n| Models | Download Link |\n| --- | --- |\n| LongCat-Video | ğŸ¤— [Huggingface](https://huggingface.co/meituan-longcat/LongCat-Video) |\n\nDownload models using huggingface-cli:\n```shell\npip install \"huggingface_hub[cli]\"\nhuggingface-cli download meituan-longcat/LongCat-Video --local-dir ./weights/LongCat-Video\n```\n\n### Run Text-to-Video\n\n```shell\n# Single-GPU inference\ntorchrun run_demo_text_to_video.py --checkpoint_dir=./weights/LongCat-Video --enable_compile\n\n# Multi-GPU inference\ntorchrun --nproc_per_node=2 run_demo_text_to_video.py --context_parallel_size=2 --checkpoint_dir=./weights/LongCat-Video --enable_compile\n```\n\n### Run Image-to-Video\n\n```shell\n# Single-GPU inference\ntorchrun run_demo_image_to_video.py --checkpoint_dir=./weights/LongCat-Video --enable_compile\n\n# Multi-GPU inference\ntorchrun --nproc_per_node=2 run_demo_image_to_video.py --context_parallel_size=2 --checkpoint_dir=./weights/LongCat-Video --enable_compile\n```\n\n### Run Video-Continuation\n\n```shell\n# Single-GPU inference\ntorchrun run_demo_video_continuation.py --checkpoint_dir=./weights/LongCat-Video --enable_compile\n\n# Multi-GPU inference\ntorchrun --nproc_per_node=2 run_demo_video_continuation.py --context_parallel_size=2 -",
      "default_branch": "main"
    },
    "fetched_at": "2025-11-02T02:42:40.282354"
  },
  {
    "basic_info": {
      "name": "MimicKit",
      "full_name": "xbpeng/MimicKit",
      "owner": "xbpeng",
      "description": "Suite of motion imitation methods for training motion controllers.",
      "url": "https://github.com/xbpeng/MimicKit",
      "clone_url": "https://github.com/xbpeng/MimicKit.git",
      "ssh_url": "git@github.com:xbpeng/MimicKit.git",
      "homepage": "",
      "created_at": "2025-10-08T15:33:01Z",
      "updated_at": "2025-11-01T23:47:17Z",
      "pushed_at": "2025-10-30T22:18:46Z"
    },
    "stats": {
      "stars": 853,
      "forks": 83,
      "watchers": 853,
      "open_issues": 1,
      "size": 10336
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 349230
      },
      "license": "BSD 3-Clause \"New\" or \"Revised\" License",
      "topics": [
        "animation",
        "reinforcement-learning",
        "robotics"
      ]
    },
    "content": {
      "readme": "# MimicKit\n\n\n![Teaser](images/MimicKit_teaser.gif)\n\nThis framework provides a suite of motion imitation methods for training motion controllers. This codebase is designed to be clean and lightweight, with minimal dependencies. A more detailed overview of MimicKit is available in the [Starter Guide](https://arxiv.org/abs/2510.13794). This codebase includes implementations of:\n- [DeepMimic](https://xbpeng.github.io/projects/DeepMimic/index.html)\n- [AMP](https://xbpeng.github.io/projects/AMP/index.html)\n- [ASE](https://xbpeng.github.io/projects/ASE/index.html)\n- [ADD](https://xbpeng.github.io/projects/ADD/index.html)\n\nWe also include the following RL algorithms:\n- [PPO](https://arxiv.org/abs/1707.06347)\n- [AWR](https://xbpeng.github.io/projects/AWR/index.html)\n\n---\n\n## Installation\n\nInstall IsaacGym: https://developer.nvidia.com/isaac-gym\n\nInstall requirements:\n```\npip install -r requirements.txt\n```\nDownload assets and motion data from [here](https://1sfu-my.sharepoint.com/:u:/g/personal/xbpeng_sfu_ca/EclKq9pwdOBAl-17SogfMW0Bved4sodZBQ_5eZCiz9O--w?e=bqXBaa), then extract the contents into [`data/`](data/).\n\n---\n\n## Training\n\nTo train a model, run the following command:\n```\npython mimickit/run.py --mode train --num_envs 4096 --env_config data/envs/deepmimic_humanoid_env.yaml --agent_config data/agents/deepmimic_humanoid_ppo_agent.yaml --visualize true --log_file output/log.txt --out_model_file output/model.pt\n```\n- `--mode` selects either `train` or `test` mode.\n- `--num_envs` specifies the number of parallel environments used for simulation.\n- `--env_config` specifies the configuration file for the environment.\n- `--agent_config` specifies configuration file for the agent.\n- `--visualize` enables visualization. Rendering should be disabled for faster training.\n- `--log_file` specifies the output log file, which will keep track of statistics during training.\n- `--out_model_file` specifies the output model file, which contains the model parameters.\n- `--logger` specifies the logger used to record training stats. The options are TensorBoard `tb` or `wandb`.\n\nInstead of specifying all arguments through the command line, arguments can also be loaded from an `arg_file`:\n```\npython mimickit/run.py --arg_file args/deepmimic_humanoid_ppo_args.txt --visualize true\n```\nThe arguments in `arg_file` are treated the same as command line arguments. Arguments for all algorithms are provided in [`args/`](args/).\n\n\n## Testing\n\nTo test a model, run the following command:\n```\npython mimickit/run.py --arg_file args/deepmimic_humanoid_ppo_args.txt --num_envs 4 --visualize true --mode test --model_file data/models/deepmimic_humanoid_spinkick_model.pt\n```\n- `--model_file` specifies the `.pt` file that contains the parameters of the trained model. Pretrained models are available in [`data/models/`](data/models/), and the corresponding training log files are available in [`data/logs/`](data/logs/).\n\n\n## Distributed Training\n\nTo use distributed training with multi-CPU or multi-GPU:\n```\npython mimickit/run.py --arg_file args/deepmimic_humanoid_ppo_args.txt --num_workers 2 --device cuda:0\n```\n- `--num_workers` specifies the number of worker processes used to parallelize training. \n- `--device` specifies the device used for training, which can be `cpu` or `cuda:0`. When training with multiple GPUs, the number of worker processes used to parallelize training must be less than or equal to the number of GPUs available on the system.\n\n## Visualizing Training Logs\n\nWhen using the TensorBoard logger during training, a TensorBoard `events` file will be saved the same output directory as the log file. The log can be viewed with:\n```\ntensorboard --logdir=output/ --port=6006 --samples_per_plugin scalars=999999\n```\nThe output log `.txt` file can also be plotted using the plotting script [`plot_log.py`](tools/plot_log/plot_log.py).\n\n---\n\n## Motion Data\nMotion data is stored in [`data/motions/`](data/motions/). The `motion_file` field in the environment configuration file can be used to specify the reference motion clip. In addition to imitating individual motion clips, `motion_file` can also specify a dataset file, located in [`data/datasets/`](data/datasets/), which will train a model to imitate a dataset containing multiple motion clips.\n\nThe `view_motion` environment can be used to visualize motion clips:\n```\npython mimickit/run.py --mode test --arg_file args/view_motion_humanoid_args.txt --visualize true\n```\n\nMotion clips are represented by the `Motion` class implemented in [`motion.py`](mimickit/anim/motion.py). Each motion clip is stored in a `.pkl` file. Each frame in a motion specifies the pose of the character according to\n```\n[root position (3D), root rotation (3D), joint rotations]\n```\nwhere 3D rotations are specified using 3D exponential maps. Joint rotations are recorded in the order that the joints are specified in the `.xml` file (i.e. depth-first traversal of the kinematic tree). For example, in the case of [`humanoid.xml`](data/assets",
      "default_branch": "main"
    },
    "fetched_at": "2025-11-02T02:42:41.623788"
  },
  {
    "basic_info": {
      "name": "tiny8",
      "full_name": "sql-hkr/tiny8",
      "owner": "sql-hkr",
      "description": "A tiny CPU simulator written in Python",
      "url": "https://github.com/sql-hkr/tiny8",
      "clone_url": "https://github.com/sql-hkr/tiny8.git",
      "ssh_url": "git@github.com:sql-hkr/tiny8.git",
      "homepage": "https://sql-hkr.github.io/tiny8/",
      "created_at": "2025-10-20T16:28:30Z",
      "updated_at": "2025-11-02T02:22:17Z",
      "pushed_at": "2025-11-01T06:46:18Z"
    },
    "stats": {
      "stars": 759,
      "forks": 17,
      "watchers": 759,
      "open_issues": 5,
      "size": 1685
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 225911,
        "Dockerfile": 332
      },
      "license": "MIT License",
      "topics": [
        "8-bit-computer",
        "assembler",
        "visualization"
      ]
    },
    "content": {
      "readme": "# Tiny8\n\n[![PyPI version](https://img.shields.io/pypi/v/tiny8)](https://pypi.org/project/tiny8/)\n[![License](https://img.shields.io/github/license/sql-hkr/tiny8)](LICENSE)\n[![Python versions](https://img.shields.io/pypi/pyversions/tiny8)](https://pypi.org/project/tiny8/)\n[![CI](https://github.com/sql-hkr/tiny8/actions/workflows/ci.yml/badge.svg)](https://github.com/sql-hkr/tiny8/actions/workflows/ci.yml)\n[![codecov](https://codecov.io/github/sql-hkr/tiny8/graph/badge.svg?token=OBM58R8MCL)](https://codecov.io/github/sql-hkr/tiny8)\n\n> **An educational 8-bit CPU simulator with interactive visualization**\n\nTiny8 is a lightweight and educational toolkit for exploring the fundamentals of computer architecture through hands-on assembly programming and real-time visualization. Designed for learning and experimentation, it features an AVR-inspired 8-bit CPU with 32 registers, a rich instruction set, and powerful debugging tools â€” all with zero heavy dependencies.\n\n\n<div align=\"center\">\n  <img src=\"https://github.com/user-attachments/assets/ffbcb2c4-2c3a-469f-b7b7-e6e86eb374da\" alt=\"Animated bubble sort visualization\" width=\"600\">\n  <p><em>Real-time visualization of a bubble sort algorithm executing on Tiny8</em></p>\n</div>\n\n## âœ¨ Features\n\n### ğŸ¯ **Interactive Terminal Debugger**\n<img width=\"600\" src=\"https://github.com/user-attachments/assets/0bbd4382-806e-4b5a-af0b-54d83417fcfb\" alt=\"CLI visualizer screenshot\">\n\n- **Vim-style navigation**: Step through execution with intuitive keyboard controls\n- **Change highlighting**: See exactly what changed at each step (registers, flags, memory)\n- **Advanced search**: Find instructions, track register/memory changes, locate PC addresses\n- **Marks and bookmarks**: Set and jump to important execution points\n- **Vertical scrolling**: Handle programs with large memory footprints\n\n### ğŸ¬ **Graphical Animation**\n- Generate high-quality GIF/MP4 videos of program execution\n- Visualize register evolution, memory access patterns, and flag changes\n- Perfect for presentations, documentation, and learning materials\n\n### ğŸ—ï¸ **Complete 8-bit Architecture**\n- **32 general-purpose registers** (R0-R31)\n- **8-bit ALU** with arithmetic, logical, and bit manipulation operations\n- **Status register (SREG)** with 8 condition flags\n- **2KB address space** for unified memory and I/O\n- **Stack operations** with dedicated stack pointer\n- **AVR-inspired instruction set** with 60+ instructions\n\n### ğŸ“š **Educational Focus**\n- Clean, readable Python implementation\n- Comprehensive examples (Fibonacci, bubble sort, factorial, and more)\n- Step-by-step execution traces for debugging\n- Full API documentation and instruction set reference\n\n## ğŸš€ Quick Start\n\n### Installation\n\n```bash\npip install tiny8\n```\n\n### Your First Program\n\nCreate `fibonacci.asm`:\n```asm\n; Fibonacci Sequence Calculator\n; Calculates the 10th Fibonacci number (F(10) = 55)\n; F(0) = 0, F(1) = 1, F(n) = F(n-1) + F(n-2)\n;\n; Results stored in registers:\n; R16 and R17 hold the two most recent Fibonacci numbers\n\n    ldi r16, 0          ; F(0) = 0\n    ldi r17, 1          ; F(1) = 1\n    ldi r18, 9          ; Counter: 9 more iterations to reach F(10)\n\nloop:\n    add r16, r17        ; F(n) = F(n-1) + F(n-2)\n    mov r19, r16        ; Save result temporarily\n    mov r16, r17        ; Shift: previous = current\n    mov r17, r19        ; Shift: current = new result\n    dec r18             ; Decrement counter\n    brne loop           ; Continue if counter != 0\n\ndone:\n    jmp done            ; Infinite loop at end\n```\n\nRun it:\n```bash\ntiny8 fibonacci.asm # Interactive debugger\ntiny8 fibonacci.asm -m ani -o fibonacci.gif # Generate animation\n```\n\n### Python API\n\n```python\nfrom tiny8 import CPU, assemble_file\n\nasm = assemble_file(\"fibonacci.asm\")\ncpu = CPU()\ncpu.load_program(asm)\ncpu.run(max_steps=1000)\n\nprint(f\"Result: R17 = {cpu.read_reg(17)}\")  # Final Fibonacci number\n```\n\n## ğŸ’¡ Why Tiny8?\n\n**For Students** â€” Write assembly, see immediate results with visual feedback. Understand how each instruction affects CPU state without abstractions.\n\n**For Educators** â€” Interactive demonstrations, easy assignment creation, and generate animations for lectures.\n\n**For Hobbyists** â€” Rapid algorithm prototyping at the hardware level with minimal overhead and an extensible, readable codebase.\n\n## ğŸ“– Documentation\n\n- [**Full Documentation**](https://sql-hkr.github.io/tiny8/) â€” Complete API reference and guides\n- [**Instruction Set Reference**](#instruction-set-reference) â€” All 60+ instructions\n- [**CLI Guide**](#interactive-cli-controls) â€” Terminal debugger keyboard shortcuts\n- [**Examples**](#examples) â€” Sample programs with explanations\n- [**Contributing**](CONTRIBUTING.md) â€” Guidelines for contributors\n\n## ğŸ® Interactive CLI Controls\n\nThe terminal-based debugger provides powerful navigation and inspection capabilities.\n\n### Navigation & Playback\n\n- `l` / `h` or `â†’` / `â†` â€” Step forward/backward\n- `w` / `b` â€” Jump Â±10 steps\n- `0` / `$` â€” Jump to first/last step\n- `Space` â€” Play/p",
      "default_branch": "main"
    },
    "fetched_at": "2025-11-02T02:42:42.882154"
  },
  {
    "basic_info": {
      "name": "AITradeGame",
      "full_name": "chadyi/AITradeGame",
      "owner": "chadyi",
      "description": null,
      "url": "https://github.com/chadyi/AITradeGame",
      "clone_url": "https://github.com/chadyi/AITradeGame.git",
      "ssh_url": "git@github.com:chadyi/AITradeGame.git",
      "homepage": "",
      "created_at": "2025-10-20T07:23:51Z",
      "updated_at": "2025-11-02T01:56:08Z",
      "pushed_at": "2025-10-28T22:46:02Z"
    },
    "stats": {
      "stars": 728,
      "forks": 221,
      "watchers": 728,
      "open_issues": 1,
      "size": 21956
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 64226,
        "JavaScript": 40008,
        "CSS": 15588,
        "HTML": 15410,
        "Dockerfile": 161
      },
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "# AITradeGame - Open Source AI Trading Simulator\n\n[English](README.md) | [ä¸­æ–‡](README_ZH.md)\n\n[![Python 3.9+](https://img.shields.io/badge/python-3.9+-blue.svg)](https://www.python.org/downloads/)\n[![Flask](https://img.shields.io/badge/flask-3.0+-green.svg)](https://flask.palletsprojects.com/)\n[![License](https://img.shields.io/badge/license-MIT-blue.svg)](LICENSE)\n\nAITradeGame is an AI trading simulator that supports both local and online versions.\n\nProvides an online version with interactive features and leaderboards.\n\nLocal version stores all data on your computer, no cloud storage, no tracking.\n\nIncludes a Windows one-click standalone executable that runs without installation.\n\n## Features\n\n### Desktop Version (Local)\n\nAI-driven trading strategies based on large language models, compatible with OpenAI, DeepSeek, Claude, and other models. Leveraged portfolio management with ECharts visualizations. 100% privacy with all data stored in local database. Trading fee configuration supported to simulate real trading environment.\n\n**Latest Features:**\n- API Provider Management: Unified management of multiple AI service provider API configurations\n- Smart Model Selection: Automatically fetch available model lists for each provider\n- Aggregated View: View aggregated assets and performance comparison across all models\n- System Settings: Configurable trading frequency and fee rates\n\n### Online Version (Public)\n\nLeaderboard functionality to compete with AI enthusiasts worldwide. Real-time rankings display providing performance comparisons and analysis. Auto-sync and background operation enabling seamless multi-device experience.\n\n## Quick Start\n\n### Try Online Version\n\nLaunch the online version at https://aitradegame.com without any installation.\n\n### Desktop Version\n\nDownload AITradeGame.exe from GitHub releases. Double-click the executable to run. The interface will open automatically. Start adding AI models and begin trading.\n\nAlternatively, clone the repository from GitHub. Install dependencies with pip install -r requirements.txt. Run the application with python app.py and visit http://localhost:5000.\n\n### Docker Deployment\n\nYou can also run AITradeGame using Docker:\n\n**Using docker-compose (recommended):**\n```bash\n# Build and start the container\ndocker-compose up -d\n\n# Access the application at http://localhost:5000\n```\n\n**Using docker directly:**\n```bash\n# Build the image\ndocker build -t aitradegame .\n\n# Run the container\ndocker run -d -p 5000:5000 -v $(pwd)/data:/app/data aitradegame\n\n# Access the application at http://localhost:5000\n```\n\nThe data directory will be created automatically to store the SQLite database. To stop the container, run `docker-compose down`.\n\n## Configuration\n\n### API Provider Setup\nFirst, add AI service providers:\n1. Click the \"API Provider\" button\n2. Enter provider name, API URL, and API key\n3. Manually input available models or click \"Fetch Models\" to auto-fetch\n4. Click save to complete configuration\n\n### Adding Trading Models\nAfter configuring providers, add trading models:\n1. Click the \"Add Model\" button\n2. Select a configured API provider\n3. Choose a specific model from the dropdown\n4. Enter display name and initial capital\n5. Click submit to start trading\n\n### System Settings\nClick the \"Settings\" button to configure:\n- Trading Frequency: Control AI decision interval (1-1440 minutes)\n- Trading Fee Rate: Commission rate per trade (default 0.1%)\n\n## Supported AI Models\n\nSupports all OpenAI-compatible APIs. This includes OpenAI models like gpt-4 and gpt-3.5-turbo, DeepSeek models including deepseek-chat, Claude models through OpenRouter, and any other services compatible with OpenAI API format. More protocols are being added.\n\n## Usage\n\nStart the server by running AITradeGame.exe or python app.py. Add AI model configuration through the web interface at http://localhost:5000. The system automatically begins trading simulation based on your configuration. Trading fees are charged for each open and close position according to the set rate, ensuring AI strategies operate under realistic cost conditions.\n\n## Privacy and Security\n\nAll data is stored in the AITradeGame.db SQLite file in the same directory as the executable. No external servers are contacted except your specified AI API endpoints. No user accounts or login required - everything runs locally.\n\n## Development\n\nDevelopment requires Python 3.9 or later. Internet connection is needed for market data and AI API calls.\n\nInstall all dependencies with: pip install -r requirements.txt\n\n## Contributing\n\nCommunity contributions are welcome.\n\n## Disclaimer\n\nThis is a simulated trading platform for testing AI models and strategies. It is not real trading and no actual money is involved. Always conduct your own research and analysis before making investment decisions. No warranties are provided regarding trading outcomes or AI performance.\n\n## Links\n\nOnline version with leaderboard and social features: https://aitradegame.com\n\nDesktop bui",
      "default_branch": "main"
    },
    "fetched_at": "2025-11-02T02:42:44.164378"
  }
]