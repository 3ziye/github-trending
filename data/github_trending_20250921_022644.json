[
  {
    "basic_info": {
      "name": "map-anything",
      "full_name": "facebookresearch/map-anything",
      "owner": "facebookresearch",
      "description": "MapAnything: Universal Feed-Forward Metric 3D Reconstruction",
      "url": "https://github.com/facebookresearch/map-anything",
      "clone_url": "https://github.com/facebookresearch/map-anything.git",
      "ssh_url": "git@github.com:facebookresearch/map-anything.git",
      "homepage": "",
      "created_at": "2025-09-04T14:37:36Z",
      "updated_at": "2025-09-21T02:17:17Z",
      "pushed_at": "2025-09-19T10:17:35Z"
    },
    "stats": {
      "stars": 1233,
      "forks": 48,
      "watchers": 1233,
      "open_issues": 13,
      "size": 6164
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 1727697,
        "Shell": 157713
      },
      "license": "Apache License 2.0",
      "topics": [
        "3d-reconstruction",
        "ai",
        "calibration",
        "depth-completion",
        "depth-estimation",
        "image-to-3d",
        "multi-view-stereo",
        "robotics",
        "sfm"
      ]
    },
    "content": {
      "readme": "<div align=\"center\">\n<h1>MapAnything: Universal Feed-Forward Metric <br>3D Reconstruction</h1>\n<a href=\"https://map-anything.github.io/assets/MapAnything.pdf\"><img src=\"https://img.shields.io/badge/Paper-blue\" alt=\"Paper\"></a>\n<a href=\"https://arxiv.org/abs/2509.13414\"><img src=\"https://img.shields.io/badge/arXiv-2509.13414-b31b1b\" alt=\"arXiv\"></a>\n<a href=\"https://map-anything.github.io/\"><img src=\"https://img.shields.io/badge/Project_Page-green\" alt=\"Project Page\"></a>\n<a href=\"https://huggingface.co/spaces/facebook/map-anything\"><img src='https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Demo-blue'></a>\n<br>\n<br>\n<strong>\n<a href=\"https://nik-v9.github.io/\">Nikhil Keetha<sup>1,2</sup></a>\n&nbsp;&nbsp;\n<a href=\"https://sirwyver.github.io/\">Norman Müller<sup>1</sup></a>\n&nbsp;&nbsp;\n<a href=\"https://demuc.de/\">Johannes Schönberger<sup>1</sup></a>\n&nbsp;&nbsp;\n<a href=\"https://www.linkedin.com/in/lorenzoporzi\">Lorenzo Porzi<sup>1</sup></a>\n&nbsp;&nbsp;\n<a href=\"https://infinity1096.github.io/\">Yuchen Zhang<sup>2</sup></a>\n<br>\n<a href=\"https://tobiasfshr.github.io/\">Tobias Fischer<sup>1</sup></a>\n&nbsp;&nbsp;\n<a href=\"https://www.linkedin.com/in/arno-knapitsch\">Arno Knapitsch<sup>1</sup></a>\n&nbsp;&nbsp;\n<a href=\"https://www.linkedin.com/in/duncan-zauss\">Duncan Zauss<sup>1</sup></a>\n&nbsp;&nbsp;\n<a href=\"https://ethanweber.me/\">Ethan Weber<sup>1</sup></a>\n&nbsp;&nbsp;\n<a href=\"https://www.linkedin.com/in/nelsonantunes7\">Nelson Antunes<sup>1</sup></a>\n<br>\n<a href=\"https://x.com/jonathonluiten?lang=en\">Jonathon Luiten<sup>1</sup></a>\n&nbsp;&nbsp;\n<a href=\"https://m.lopezantequera.com/\">Manuel Lopez-Antequera<sup>1</sup></a>\n&nbsp;&nbsp;\n<a href=\"https://scholar.google.com/citations?user=484sccEAAAAJ\">Samuel Rota Bulò<sup>1</sup></a>\n&nbsp;&nbsp;\n<a href=\"https://richardt.name/\">Christian Richardt<sup>1</sup></a>\n<br>\n<a href=\"https://www.cs.cmu.edu/~deva/\">Deva Ramanan<sup>2</sup></a>\n&nbsp;&nbsp;\n<a href=\"https://theairlab.org/team/sebastian/\">Sebastian Scherer<sup>2</sup></a>\n&nbsp;&nbsp;\n<a href=\"https://www.linkedin.com/in/peter-kontschieder-2a6410134\">Peter Kontschieder<sup>1</sup></a>\n<br>\n<br>\n<sup>1</sup> Meta &nbsp;&nbsp;\n<sup>2</sup> Carnegie Mellon University\n</strong>\n\n</div>\n\n## Overview\n\nMapAnything is a simple, end-to-end trained transformer model that directly regresses the factored metric 3D geometry of a scene given various types of inputs (images, calibration, poses, or depth). A single feed-forward model supports over 12 different 3D reconstruction tasks including multi-image sfm, multi-view stereo, monocular metric depth estimation, registration, depth completion and more.\n\n![Overview](./assets/teaser.png)\n\n## Table of Contents\n\n- [Quick Start](#quick-start)\n  - [Installation](#installation)\n  - [Image-Only Inference](#image-only-inference)\n  - [Multi-Modal Inference](#multi-modal-inference)\n- [Interactive Demos](#interactive-demos)\n  - [Online Demo](#online-demo)\n  - [Local Gradio Demo](#local-gradio-demo)\n  - [Rerun Demo](#rerun-demo)\n- [COLMAP & GSplat Support](#colmap--gsplat-support)\n  - [Exporting to COLMAP Format](#exporting-to-colmap-format)\n  - [Integration with Gaussian Splatting](#integration-with-gaussian-splatting)\n- [Data Processing for Training & Benchmarking](#data-processing-for-training--benchmarking)\n- [Training](#training)\n- [Benchmarking](#benchmarking)\n- [Code License](#code-license)\n- [Models](#models)\n- [Building Blocks for MapAnything](#building-blocks-for-mapanything)\n- [Acknowledgments](#acknowledgments)\n- [Citation](#citation)\n\n## Quick Start\n\n### Installation\n\n```bash\ngit clone https://github.com/facebookresearch/map-anything.git\ncd map-anything\n\n# Create and activate conda environment\nconda create -n mapanything python=3.12 -y\nconda activate mapanything\n\n# Optional: Install torch, torchvision & torchaudio specific to your system\n# Install MapAnything\npip install -e .\n\n# For all optional dependencies\n# See pyproject.toml for more details\npip install -e \".[all]\"\npre-commit install\n```\n\nNote that we don't pin a specific version of PyTorch or CUDA in our requirements. Please feel free to install PyTorch based on your specific system.\n\n### Image-Only Inference\n\nFor metric 3D reconstruction from images without additional geometric inputs:\n\n```python\n# Optional config for better memory efficiency\nimport os\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n\n# Required imports\nimport torch\nfrom mapanything.models import MapAnything\nfrom mapanything.utils.image import load_images\n\n# Get inference device\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# Init model - This requries internet access or the huggingface hub cache to be pre-downloaded\n# For Apache 2.0 license model, use \"facebook/map-anything-apache\"\nmodel = MapAnything.from_pretrained(\"facebook/map-anything\").to(device)\n\n# Load and preprocess images from a folder or list of paths\nimages = \"path/to/your/images/\"  # or [\"path/to/img1.jpg\", \"path/to/img2.jpg\", ...]\nviews = loa",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-21T02:26:45.254818"
  },
  {
    "basic_info": {
      "name": "multi-agent-coding-system",
      "full_name": "Danau5tin/multi-agent-coding-system",
      "owner": "Danau5tin",
      "description": "Reached #13 on Stanford's Terminal Bench leaderboard. Orchestrator, explorer & coder agents working together with intelligent context sharing.",
      "url": "https://github.com/Danau5tin/multi-agent-coding-system",
      "clone_url": "https://github.com/Danau5tin/multi-agent-coding-system.git",
      "ssh_url": "git@github.com:Danau5tin/multi-agent-coding-system.git",
      "homepage": "",
      "created_at": "2025-08-30T19:23:45Z",
      "updated_at": "2025-09-20T13:42:29Z",
      "pushed_at": "2025-09-07T16:01:23Z"
    },
    "stats": {
      "stars": 1185,
      "forks": 145,
      "watchers": 1185,
      "open_issues": 2,
      "size": 849
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 139397,
        "Shell": 483
      },
      "license": null,
      "topics": []
    },
    "content": {
      "readme": "# 🤓 Orchestrator: A multi-agent AI coder. Reached #13 on Stanford's TerminalBench. Open sourced!\n\nTL;DR:\n- Over the weekend, quite unexpectedly, I made a multi-agent AI system that places slightly higher than Claude Code on Stanford's TerminalBench leaderboard (13th place).\n- This AI system consists of an orchestration agent that dispatches multiple explorer and coder agents to do all the work.\n- The orchestrator explicitly defines what knowledge artifacts subagents must return, then reuses and synthesises these artifacts across future tasks - creating compound intelligence where each action builds meaningfully on previous discoveries.\n\n![Orchestrator with claude-sonnet-4 on standford's terminal bench](./readme_imgs/orchestrator-sonnet-4-stanford-terminal-bench-leaderboard.png)\n\n## How the System Works\n\n![System architecture overview](readme_imgs/orch_agent_sys_arch.png)\n\nThe orchestrator acts as the brain of the operation - it receives the user's task but never touches code directly. Instead, it:\n\n1. **Analyses** the task and breaks it into focused subtasks\n2. **Dispatches** explorer agents to understand the system\n3. **Delegates** implementation work to coder agents with precise instructions\n4. **Verifies** all changes through additional explorer agents\n5. **Maintains** the context store with all discovered knowledge\n\nThe orchestrator's lack of direct code access forces proper delegation and verification patterns, leading to more strategic solutions.\n\nFor a full breakdown of this project's code structure, see [here](./PROJECT_STRUCTURE.md)\n\n\n## 📈 Evaluation Results\n\n### Performance on TerminalBench\n\n[Terminal bench](https://www.tbench.ai/) is a brilliant benchmark created by Stanford and [Laude Institute](https://www.laude.org/) to quantify agents' ability to complete complex tasks in the terminal. My Orchestrator system achieved **13th place** on the leaderboard, demonstrating competitive performance against leading AI coding assistants.\n\nI ran the Orchestrator evaluations with both Claude-4-Sonnet and also Qwen3-Coder-480B-A35B:\n\n![Performance comparison chart](readme_imgs/perf_chart.png)\n![Orchestrator with qwen-3-coder on standford's terminal bench](./readme_imgs/orchestrator-qwen-3-coder-stanford-terminal-bench-leaderboard.png)\n\nThis image shows Qwen-3-Coder performance on the benchmark. The screenshot towards the top of this README shows Sonnet-4 performance.\n\n### Cost & Efficiency\n\nOne of the most striking results is the amount of tokens used by Sonnet-4 as opposed to Qwen3-Coder.\n\nThe below table shows the total tokens (input and output included) processed across the TerminalBench evaluation run (5 attempts at 80 tasks = 400 trajectories).\n\n| Model | Success Rate | Total Evaluation Cost | Token Usage |\n|-------|--------------|------------|-------------|\n| **Claude Sonnet-4** | 37.0% | $263.56* | 93.2M tokens |\n| **Qwen-3-Coder** | 19.7% | $217.83 | 14.7M tokens |\n\n*Claude Sonnet-4 costs reflect heavy caching usage, reducing actual API costs\n\n\n## 🤖 The Agents\n\nWhile all agents use the same underlying LLM, each operates with its own context window, specialised system message, and distinct toolset. This creates functionally different agents optimised for their specific roles.\n\n### 🎯 Orchestrator Agent\n[System message](./src/agents/system_msgs/md_files/orchestrator_sys_msg_v0.1.md)\n**Role:** Strategic coordinator and persistent intelligence layer  \n**Capabilities:** Task decomposition, context management, subagent delegation  \n**Tools:** Task creation, subagent launching, context store management  \n**Restrictions:** Cannot read or modify code directly - operates purely at architectural level  \n\nThe orchestrator maintains the complete picture across all tasks, tracking discoveries and progress. It crafts precise task descriptions that explicitly specify what contexts subagents should return, ensuring focused and valuable information gathering.\n\n**Trust Calibration Strategy:**  \nThe orchestrator employs adaptive delegation based on task complexity:\n- **Low Complexity Tasks**: Grants extremely high autonomy to the coder agent for simple modifications and bug fixes\n- **Medium/Large Tasks**: Maintains strong trust but uses iterative decomposition - breaking complex problems into atomic, verifiable steps\n- **Verification Philosophy**: Uses explorer agents liberally to verify progress, especially when tasks involve critical functionality\n\n\n### 🔍 Explorer Agent \n[System message](./src/agents/system_msgs/md_files/explorer_sys_msg_v0.1.md) \n**Role:** Read-only investigation and verification specialist  \n**Capabilities:** System exploration, code analysis, test execution, verification  \n**Tools:** File reading, search operations (grep/glob), bash commands, temporary script creation  \n**Restrictions:** Cannot modify existing files - strictly read-only operations  \n\nExplorers gather intelligence about the codebase, verify implementations, and discover system behaviors. They create knowledge artifacts that eliminat",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-21T02:26:46.531425"
  },
  {
    "basic_info": {
      "name": "HunyuanWorld-Voyager",
      "full_name": "Tencent-Hunyuan/HunyuanWorld-Voyager",
      "owner": "Tencent-Hunyuan",
      "description": "Voyager is an interactive RGBD video generation model conditioned on camera input, and supports real-time 3D reconstruction.",
      "url": "https://github.com/Tencent-Hunyuan/HunyuanWorld-Voyager",
      "clone_url": "https://github.com/Tencent-Hunyuan/HunyuanWorld-Voyager.git",
      "ssh_url": "git@github.com:Tencent-Hunyuan/HunyuanWorld-Voyager.git",
      "homepage": "https://3d-models.hunyuan.tencent.com/world/",
      "created_at": "2025-08-27T09:34:10Z",
      "updated_at": "2025-09-20T01:01:58Z",
      "pushed_at": "2025-09-10T08:21:15Z"
    },
    "stats": {
      "stars": 1139,
      "forks": 74,
      "watchers": 1139,
      "open_issues": 14,
      "size": 850180
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 468553,
        "Shell": 1060
      },
      "license": "Other",
      "topics": [
        "3d",
        "3d-generation",
        "aigc",
        "hunyuan3d",
        "image-to-3d",
        "image-to-video",
        "scene-generation",
        "world-model",
        "world-models"
      ]
    },
    "content": {
      "readme": "[中文阅读](README_zh.md)\n\n# **HunyuanWorld-Voyager**\n\n<p align=\"center\">\n  <img src=\"assets/teaser.png\">\n</p>\n\n<div align=\"center\">\n  <a href=\"https://3d-models.hunyuan.tencent.com/world/\" target=\"_blank\"><img src=\"https://img.shields.io/static/v1?label=Project%20Page&message=Web&color=green\" height=22px></a>\n  <a href=\"https://3d-models.hunyuan.tencent.com/voyager/voyager_en/assets/HYWorld_Voyager.pdf\" target=\"_blank\"><img src=\"https://img.shields.io/static/v1?label=Tech%20Report&message=arxiv&color=red\" height=22px></a>\n  <a href=\"https://huggingface.co/tencent/HunyuanWorld-Voyager\" target=\"_blank\"><img src=\"https://img.shields.io/static/v1?label=HunyuanWorld-Voyager&message=HuggingFace&color=yellow\" height=22px></a>\n</div>\n\n-----\n\nWe introduce HunyuanWorld-Voyager, a novel video diffusion framework that generates world-consistent 3D point-cloud sequences from a single image with user-defined camera path. Voyager can generate 3D-consistent scene videos for world exploration following custom camera trajectories. It can also generate aligned depth and RGB video for efficient and direct 3D reconstruction.\n\n\n## 🔥🔥🔥 News!!\n* Sep 2, 2025: 👋 We release the code and model weights of HunyuanWorld-Voyager. [Download](ckpts/README.md).\n\n> Join our **[Wechat](#)** and **[Discord](https://discord.gg/dNBrdrGGMa)** group to discuss and find help from us.\n\n| Wechat Group                                     | Xiaohongshu                                           | X                                           | Discord                                           |\n|--------------------------------------------------|-------------------------------------------------------|---------------------------------------------|---------------------------------------------------|\n| <img src=\"assets/qrcode/wechat.png\"  height=140> | <img src=\"assets/qrcode/xiaohongshu.png\"  height=140> | <img src=\"assets/qrcode/x.png\"  height=140> | <img src=\"assets/qrcode/discord.png\"  height=140> | \n\n## 🎥 Demo\n### Demo Video\n\n<div align=\"center\">\n  <video src=\"https://github.com/user-attachments/assets/2eb844c9-30ba-4770-8066-189c123affee\" width=\"80%\" poster=\"\"> </video>\n</div>\n\n### Camera-Controllable Video Generation\n\n|  Input | Generated Video  |\n|:----------------:|:----------------:|\n|  <img src=\"assets/demo/camera/input1.png\" width=\"80%\">        |       <video src=\"https://github.com/user-attachments/assets/2b03ecd5-9a8f-455c-bf04-c668d3a61b04\" width=\"100%\"> </video>        |\n| <img src=\"assets/demo/camera/input2.png\" width=\"80%\">         |       <video src=\"https://github.com/user-attachments/assets/45844ac0-c65a-4e04-9f7d-4c72d47e0339\" width=\"100%\"> </video>        | \n| <img src=\"assets/demo/camera/input3.png\" width=\"80%\">         |       <video src=\"https://github.com/user-attachments/assets/f7f48473-3bb5-4a30-bd22-af3ca95ee8dc\" width=\"100%\"> </video>        |\n\n### Multiple Applications\n\n- Video Reconstruction\n\n| Generated Video | Reconstructed Point Cloud |\n|:---------------:|:--------------------------------:|\n| <video src=\"https://github.com/user-attachments/assets/72a41804-63fc-4596-963d-1497e68f7790\" width=\"100%\"> </video> | <video src=\"https://github.com/user-attachments/assets/67574e9c-9e21-4ed6-9503-e65d187086a2\" width=\"100%\"> </video> |\n\n- Image-to-3D Generation\n\n| | |\n|:---------------:|:---------------:|\n| <video src=\"https://github.com/user-attachments/assets/886aa86d-990e-4b86-97a5-0b9110862d14\" width=\"100%\"> </video> | <video src=\"https://github.com/user-attachments/assets/4c1734ba-4e78-4979-b30e-3c8c97aa984b\" width=\"100%\"> </video> |\n\n- Video Depth Estimation\n\n| | |\n|:---------------:|:---------------:|\n| <video src=\"https://github.com/user-attachments/assets/e4c8b729-e880-4be3-826f-429a5c1f12cd\" width=\"100%\"> </video> | <video src=\"https://github.com/user-attachments/assets/7ede0745-cde7-42f1-9c28-e4dca90dac52\" width=\"100%\"> </video> |\n\n\n## ☯️ **HunyuanWorld-Voyager Introduction**\n###  Architecture\n\nVoyager consists of two key components:\n\n(1) World-Consistent Video Diffusion: A unified architecture that jointly generates aligned RGB and depth video sequences, conditioned on existing world observation to ensure global coherence.\n\n(2) Long-Range World Exploration: An efficient world cache with point culling and an auto-regressive inference with smooth video sampling for iterative scene extension with context-aware consistency.\n\nTo train Voyager, we propose a scalable data engine, i.e., a video reconstruction pipeline that automates camera pose estimation and metric depth prediction for arbitrary videos, enabling large-scale, diverse training data curation without manual 3D annotations. Using this pipeline, we compile a dataset of over 100,000 video clips, combining real-world captures and synthetic Unreal Engine renders.\n\n<p align=\"center\">\n  <img src=\"assets/backbone.jpg\"  height=500>\n</p>\n\n### Performance\n\n<table class=\"comparison-table\">\n  <thead>\n    <tr>\n      <th>Method</th>\n      <th>WorldScore Average</th>\n      <th>Camera Con",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-21T02:26:47.822143"
  },
  {
    "basic_info": {
      "name": "USO",
      "full_name": "bytedance/USO",
      "owner": "bytedance",
      "description": "🔥🔥 Open-sourced unified customization model",
      "url": "https://github.com/bytedance/USO",
      "clone_url": "https://github.com/bytedance/USO.git",
      "ssh_url": "git@github.com:bytedance/USO.git",
      "homepage": "https://bytedance.github.io/USO/",
      "created_at": "2025-08-22T10:45:42Z",
      "updated_at": "2025-09-20T17:03:30Z",
      "pushed_at": "2025-09-12T08:11:32Z"
    },
    "stats": {
      "stars": 1107,
      "forks": 67,
      "watchers": 1107,
      "open_issues": 24,
      "size": 30399
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 106558
      },
      "license": "Apache License 2.0",
      "topics": []
    },
    "content": {
      "readme": "<h3 align=\"center\">\n    <img src=\"assets/uso.webp\" alt=\"Logo\" style=\"vertical-align: middle; width: 95px; height: auto;\">\n    </br>\n    Unified Style and Subject-Driven Generation via Disentangled and Reward Learning\n</h3>\n\n<p align=\"center\"> \n<a href=\"https://bytedance.github.io/USO/\"><img alt=\"Build\" src=\"https://img.shields.io/badge/Project%20Page-USO-blue\"></a> \n<a href=\"https://arxiv.org/abs/2508.18966\"><img alt=\"Build\" src=\"https://img.shields.io/badge/Tech%20Report-USO-b31b1b.svg\"></a>\n<a href=\"https://huggingface.co/bytedance-research/USO\"><img src=\"https://img.shields.io/static/v1?label=%F0%9F%A4%97%20Hugging%20Face&message=Model&color=green\"></a>\n<a href=\"https://huggingface.co/spaces/bytedance-research/USO\"><img src=\"https://img.shields.io/static/v1?label=%F0%9F%A4%97%20Hugging%20Face&message=demo&color=orange\"></a>\n</p>\n</p>\n\n><p align=\"center\"> <span style=\"color:#137cf3; font-family: Gill Sans\">Shaojin Wu,</span><sup></sup></a>  <span style=\"color:#137cf3; font-family: Gill Sans\">Mengqi Huang,</span></a> <span style=\"color:#137cf3; font-family: Gill Sans\">Yufeng Cheng,</span><sup></sup></a>  <span style=\"color:#137cf3; font-family: Gill Sans\">Wenxu Wu,</span><sup></sup> </a> <span style=\"color:#137cf3; font-family: Gill Sans\">Jiahe Tian,</span><sup></sup></a> <span style=\"color:#137cf3; font-family: Gill Sans\">Yiming Luo,</span><sup></sup></a> <span style=\"color:#137cf3; font-family: Gill Sans\">Fei Ding,</span></a> <span style=\"color:#137cf3; font-family: Gill Sans\">Qian He</span></a> <br> \n><span style=\"font-size: 13.5px\">UXO Team</span><br> \n><span style=\"font-size: 12px\">Intelligent Creation Lab, Bytedance</span></p>\n\n### 🚩 Updates\n* **2025.09.12** 🔥 Our new family member [UMO](https://github.com/bytedance/UMO) is here! It focuses on multiple identities and subject-driven generation. You can visit the <a href=\"https://bytedance.github.io/UMO/\" target=\"_blank\">UMO project page</a> for more examples.\n\n* **2025.09.03** 🎉 USO is now natively supported in ComfyUI, see official tutorial [USO in ComfyUI](https://docs.comfy.org/tutorials/flux/flux-1-uso) and our provided examples in `./workflow`. More tips are available in the [README below](https://github.com/bytedance/USO#%EF%B8%8F-comfyui-examples).\n<p align=\"center\">\n<img src=\"assets/usoxcomfyui_official.jpeg\" width=1024 height=\"auto\">\n</p>\n\n* **2025.08.28** 🔥 The [demo](https://huggingface.co/spaces/bytedance-research/USO) of USO is released. Try it Now! ⚡️\n* **2025.08.28** 🔥 Update fp8 mode as a primary low vmemory usage support (please scroll down). Gift for consumer-grade GPU users. The peak Vmemory usage is ~16GB now.\n* **2025.08.27** 🔥 The [inference code](https://github.com/bytedance/USO) and [model](https://huggingface.co/bytedance-research/USO) of USO are released.\n* **2025.08.27** 🔥 The [project page](https://bytedance.github.io/USO) of USO is created.\n* **2025.08.27** 🔥 The [technical report](https://arxiv.org/abs/2508.18966) of USO is released.\n\n## 📖 Introduction\nExisting literature typically treats style-driven and subject-driven generation as two disjoint tasks: the former prioritizes stylistic similarity, whereas the latter insists on subject consistency, resulting in an apparent antagonism. We argue that both objectives can be unified under a single framework because they ultimately concern the disentanglement and re-composition of “content” and “style”, a long-standing theme in style-driven research. To this end, we present USO, a Unified framework for Style driven and subject-driven GeneratiOn. First, we construct a large-scale triplet dataset consisting of content images, style images, and their corresponding stylized content images. Second, we introduce a disentangled learning scheme that simultaneously aligns style features and disentangles content from style through two complementary objectives, style-alignment training and content–style disentanglement training. Third, we incorporate a style reward-learning paradigm to further enhance the model’s performance.\n<p align=\"center\">\n    <img src=\"assets/teaser.webp\" width=\"1024\"/>\n</p>\n\n## ⚡️ Quick Start\n\n### 🔧 Requirements and Installation\n\nInstall the requirements\n```bash\n## create a virtual environment with python >= 3.10 <= 3.12, like\npython -m venv uso_env\nsource uso_env/bin/activate\n## or\nconda create -n uso_env python=3.10 -y\nconda activate uso_env\n\n## install torch\n## recommended version:\npip install torch==2.4.0 torchvision==0.19.0 --index-url https://download.pytorch.org/whl/cu124 \n\n## then install the requirements by you need\npip install -r requirements.txt # legacy installation command\n```\n\nThen download checkpoints:\n```bash\n# 1. set up .env file\ncp example.env .env\n\n# 2. set your huggingface token in .env (open the file and change this value to your token)\nHF_TOKEN=your_huggingface_token_here\n\n#3. download the necessary weights (comment any weights you don't need)\npip install huggingface_hub\npython ./weights/downloader.py\n```\n- **IF YOU HAVE WEIGHTS, COMMENT OUT W",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-21T02:26:49.107051"
  },
  {
    "basic_info": {
      "name": "VoxCPM",
      "full_name": "OpenBMB/VoxCPM",
      "owner": "OpenBMB",
      "description": "VoxCPM: Tokenizer-Free TTS for Context-Aware Speech Generation and True-to-Life Voice Cloning",
      "url": "https://github.com/OpenBMB/VoxCPM",
      "clone_url": "https://github.com/OpenBMB/VoxCPM.git",
      "ssh_url": "git@github.com:OpenBMB/VoxCPM.git",
      "homepage": "",
      "created_at": "2025-09-16T03:41:49Z",
      "updated_at": "2025-09-21T02:18:40Z",
      "pushed_at": "2025-09-19T05:44:44Z"
    },
    "stats": {
      "stars": 974,
      "forks": 94,
      "watchers": 974,
      "open_issues": 20,
      "size": 1461
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 107192
      },
      "license": "Apache License 2.0",
      "topics": []
    },
    "content": {
      "readme": "## 🎙️ VoxCPM: Tokenizer-Free TTS for Context-Aware Speech Generation and True-to-Life Voice Cloning\n\n\n[![Project Page](https://img.shields.io/badge/Project%20Page-GitHub-blue)](https://github.com/OpenBMB/VoxCPM/) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-OpenBMB-yellow)](https://huggingface.co/openbmb/VoxCPM-0.5B) [![ModelScope](https://img.shields.io/badge/ModelScope-OpenBMB-purple)](https://modelscope.cn/models/OpenBMB/VoxCPM-0.5B)  [![Live Playground](https://img.shields.io/badge/Live%20PlayGround-Demo-orange)](https://huggingface.co/spaces/OpenBMB/VoxCPM-Demo) [![Samples](https://img.shields.io/badge/Page-Samples-red)](https://openbmb.github.io/VoxCPM-demopage)\n\n\n\n<div align=\"center\">\n  <img src=\"assets/voxcpm_logo.png\" alt=\"VoxCPM Logo\" width=\"40%\">\n</div>\n\n<div align=\"center\">\n\n👋 Contact us on [WeChat](assets/wechat.png)\n\n</div>\n\n## News \n* [2025.09.16] 🔥 🔥 🔥  We Open Source the VoxCPM-0.5B [weights](https://huggingface.co/openbmb/VoxCPM-0.5B)!\n* [2025.09.16] 🎉 🎉 🎉  We Provide the [Gradio PlayGround](https://huggingface.co/spaces/OpenBMB/VoxCPM-Demo) for VoxCPM-0.5B, try it now! \n\n## Overview\n\nVoxCPM is a novel tokenizer-free Text-to-Speech (TTS) system that redefines realism in speech synthesis. By modeling speech in a continuous space, it overcomes the limitations of discrete tokenization and enables two flagship capabilities: context-aware speech generation and true-to-life zero-shot voice cloning.\n\nUnlike mainstream approaches that convert speech to discrete tokens, VoxCPM uses an end-to-end diffusion autoregressive architecture that directly generates continuous speech representations from text. Built on [MiniCPM-4](https://huggingface.co/openbmb/MiniCPM4-0.5B) backbone, it achieves implicit semantic-acoustic decoupling through hierachical language modeling and FSQ constraints, greatly enhancing both expressiveness and generation stability.\n\n<div align=\"center\">\n  <img src=\"assets/voxcpm_model.png\" alt=\"VoxCPM Model Architecture\" width=\"90%\">\n</div>\n\n\n###  🚀 Key Features\n- **Context-Aware, Expressive Speech Generation** - VoxCPM comprehends text to infer and generate appropriate prosody, delivering speech with remarkable expressiveness and natural flow. It spontaneously adapts speaking style based on content, producing highly fitting vocal expression trained on a massive 1.8 million-hour bilingual corpus.\n- **True-to-Life Voice Cloning** - With only a short reference audio clip, VoxCPM performs accurate zero-shot voice cloning, capturing not only the speaker’s timbre but also fine-grained characteristics such as accent, emotional tone, rhythm, and pacing to create a faithful and natural replica.\n- **High-Efficiency Synthesis** - VoxCPM supports streaming synthesis with a Real-Time Factor (RTF) as low as 0.17 on a consumer-grade NVIDIA RTX 4090 GPU, making it possible for real-time applications.\n\n\n\n\n\n##  Quick Start\n\n### 🔧 Install from PyPI\n``` sh\npip install voxcpm\n```\n### 1.  Model Download (Optional)\nBy default, when you first run the script, the model will be downloaded automatically, but you can also download the model in advance.\n- Download VoxCPM-0.5B\n    ```\n    from huggingface_hub import snapshot_download\n    snapshot_download(\"openbmb/VoxCPM-0.5B\")\n    ```\n- Download ZipEnhancer and SenseVoice-Small. We use ZipEnhancer to enhance speech prompts and SenseVoice-Small for speech prompt ASR in the web demo.\n    ```\n    from modelscope import snapshot_download\n    snapshot_download('iic/speech_zipenhancer_ans_multiloss_16k_base')\n    snapshot_download('iic/SenseVoiceSmall')\n    ```\n\n### 2. Basic Usage\n```python\nimport soundfile as sf\nfrom voxcpm import VoxCPM\n\nmodel = VoxCPM.from_pretrained(\"openbmb/VoxCPM-0.5B\")\n\nwav = model.generate(\n    text=\"VoxCPM is an innovative end-to-end TTS model from ModelBest, designed to generate highly expressive speech.\",\n    prompt_wav_path=None,      # optional: path to a prompt speech for voice cloning\n    prompt_text=None,          # optional: reference text\n    cfg_value=2.0,             # LM guidance on LocDiT, higher for better adherence to the prompt, but maybe worse\n    inference_timesteps=10,   # LocDiT inference timesteps, higher for better result, lower for fast speed\n    normalize=True,           # enable external TN tool\n    denoise=True,             # enable external Denoise tool\n    retry_badcase=True,        # enable retrying mode for some bad cases (unstoppable)\n    retry_badcase_max_times=3,  # maximum retrying times\n    retry_badcase_ratio_threshold=6.0, # maximum length restriction for bad case detection (simple but effective), it could be adjusted for slow pace speech\n)\n\nsf.write(\"output.wav\", wav, 16000)\nprint(\"saved: output.wav\")\n```\n\n### 3. CLI Usage\n\nAfter installation, the entry point is `voxcpm` (or use `python -m voxcpm.cli`).\n\n```bash\n# 1) Direct synthesis (single text)\nvoxcpm --text \"VoxCPM is an innovative end-to-end TTS model from ModelBest, designed to generate highly expressive speech.\" --out",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-21T02:26:50.378142"
  },
  {
    "basic_info": {
      "name": "SRPO",
      "full_name": "Tencent-Hunyuan/SRPO",
      "owner": "Tencent-Hunyuan",
      "description": "Directly Aligning the Full Diffusion Trajectory with Fine-Grained Human Preference",
      "url": "https://github.com/Tencent-Hunyuan/SRPO",
      "clone_url": "https://github.com/Tencent-Hunyuan/SRPO.git",
      "ssh_url": "git@github.com:Tencent-Hunyuan/SRPO.git",
      "homepage": "https://tencent.github.io/srpo-project-page/",
      "created_at": "2025-09-09T07:36:49Z",
      "updated_at": "2025-09-21T02:02:43Z",
      "pushed_at": "2025-09-16T06:21:00Z"
    },
    "stats": {
      "stars": 882,
      "forks": 22,
      "watchers": 882,
      "open_issues": 10,
      "size": 12251
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 712645,
        "Shell": 4058
      },
      "license": "Other",
      "topics": []
    },
    "content": {
      "readme": "<div align=“center” style=“font-family: charter;”>\n<h1 align=\"center\">Directly Aligning the Full Diffusion Trajectory with Fine-Grained Human Preference </h1>\n<div align=\"center\">\n  <a href='https://arxiv.org/abs/2509.06942'><img src='https://img.shields.io/badge/ArXiv-red?logo=arxiv'></a>  &nbsp;\n  <a href='https://huggingface.co/tencent/SRPO/'><img src='https://img.shields.io/badge/Model-blue?logo=huggingface'></a> &nbsp; \n  <a href='https://tencent.github.io/srpo-project-page/'><img src='https://img.shields.io/badge/%F0%9F%92%BB_Project-SRPO-blue'></a> &nbsp;\n</div>\n<div align=\"center\">\n  Xiangwei Shen<sup>1,2,3*</sup>,\n  <a href=\"https://scholar.google.com/citations?user=Lnr1FQEAAAAJ&hl=zh-CN\" target=\"_blank\"><b>Zhimin Li</b></a><sup>1*</sup>,\n  <a href=\"https://scholar.google.com.hk/citations?user=Fz3X5FwAAAAJ\" target=\"_blank\"><b>Zhantao Yang</b></a><sup>1</sup>, \n  <a href=\"https://shiyi-zh0408.github.io/\" target=\"_blank\"><b>Shiyi Zhang</b></a><sup>3</sup>,\n  Yingfang Zhang<sup>1</sup>,\n  Donghao Li<sup>1</sup>,\n  <br>\n  <a href=\"https://scholar.google.com/citations?user=VXQV5xwAAAAJ&hl=en\" target=\"_blank\"><b>Chunyu Wang</b></a><sup>1✝</sup>,\n  <a href=\"https://openreview.net/profile?id=%7EQinglin_Lu2\" target=\"_blank\"><b>Qinglin Lu</b></a><sup>1</sup>,\n  <a href=\"https://andytang15.github.io\" target=\"_blank\"><b>Yansong Tang</b></a><sup>3,✉️</sup>\n</div>\n<div align=\"center\">\n  <sup>1</sup>Hunyuan, Tencent \n  <br>\n  <sup>2</sup>School of Science and Engineering, The Chinese University of Hong Kong, Shenzhen \n  <br>\n  <sup>3</sup>Shenzhen International Graduate School, Tsinghua University \n  <br>\n  <sup>*</sup>Equal contribution \n  <sup>✝</sup>Project lead \n  <sup>✉️</sup>Corresponding author\n</div>\n\n![head](assets/head.jpg)\n\n## 🎉 Key Features\n1. **Direct Align**: We introduce a new sampling strategy for diffusion fine-tuning that can effectively restore highly noisy images, leading to an optimization process that is more stable and less computationally demanding, especially during the initial timesteps.\n2. **Faster Training**:   By rolling out only a single image and optimizing directly with analytical gradients—a key distinction from GRPO—our method achieves significant performance improvements for FLUX.1.dev in under 10 minutes of training. To further accelerate the process, our method supports replacing online rollouts entirely with a small dataset of real images; we find that fewer than 1500 images are sufficient to effectively train FLUX.1.dev.\n3. **Free of Reward Hacking**: We have improved the training strategy for method that direct backpropagation on reward signal (such as ReFL and DRaFT). Moreover, we directly regularize the model using negative rewards, without the need for KL divergence or a separate reward system. In our experiments, this approach achieves comparable performance with multiple different rewards, improving the perceptual quality of FLUX.1.dev without suffering from reward hacking issues, such as overfitting to color or oversaturation preferences.\n4. **Potential for Controllable Fine-tuning**: For the first time in online RL, we incorporate dynamically controllable text conditions, enabling on-the-fly adjustment of reward preference towards styles within the scope of the reward model.\n\n## 🔥 News\n\n- __[2025.9.12]__:  🎉 We released the complete training code. We also share tips and experiences to help you train your models. You’re welcome to discuss and ask questions in the issues! 💬✨\n- __[2025.9.12]__:  🎉 We provide a standard workflow—feel free to use it in ComfyUI.\n- __[2025.9.8]__:   🎉 We released the paper, checkpoint, inference code.\n\n## 📑 Open-source Plan\n- [X] The training code is under internal review and will be open-sourced by this weekend at the latest.\n- [ ] Release a quantized version for the FLUX community.\n- [ ] Extend support to other models.\n\n## 🛠️ Dependencies and Installation\n\n```bash\nconda create -n SRPO python=3.10.16 -y\nconda activate SRPO\nbash ./env_setup.sh \n```\n💡 The environment dependency is basically the same as DanceGRPO\n\n## 🤗 Download Models\n\n1. Model Cards\n\n|       Model       |                           Huggingface Download URL                                      |  \n|:-----------------:|:---------------------------------------------------------------------------------------:|\n|       SRPO        |           [diffusion_pytorch_model](https://huggingface.co/tencent/SRPO/tree/main)      |\n\n2. Download our `diffusion_pytorch_model.safetensors` in [https://huggingface.co/tencent/SRPO]\n```bash\nmkdir ./srpo\nhuggingface-cli login\nhuggingface-cli download --resume-download Tencent/SRPO diffusion_pytorch_model.safetensors --local-dir ./srpo/\n```\n3. Load your FLUX cache or use the `black-forest-labs/FLUX.1-dev`[https://huggingface.co/black-forest-labs/FLUX.1-dev]\n```bash\nmkdir ./data/flux\nhuggingface-cli login\nhuggingface-cli download --resume-download  black-forest-labs/FLUX.1-dev --local-dir ./data/flux\n```\n\n## 🔑 Inference\n\n### Using ComfyUI\n\nYou can use",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-21T02:26:51.622982"
  },
  {
    "basic_info": {
      "name": "VibeVoice-ComfyUI",
      "full_name": "Enemyx-net/VibeVoice-ComfyUI",
      "owner": "Enemyx-net",
      "description": "A comprehensive ComfyUI integration for Microsoft's VibeVoice text-to-speech model, enabling high-quality single and multi-speaker voice synthesis directly within your ComfyUI workflows.",
      "url": "https://github.com/Enemyx-net/VibeVoice-ComfyUI",
      "clone_url": "https://github.com/Enemyx-net/VibeVoice-ComfyUI.git",
      "ssh_url": "git@github.com:Enemyx-net/VibeVoice-ComfyUI.git",
      "homepage": "",
      "created_at": "2025-08-27T17:45:23Z",
      "updated_at": "2025-09-21T01:17:23Z",
      "pushed_at": "2025-09-11T06:16:03Z"
    },
    "stats": {
      "stars": 849,
      "forks": 141,
      "watchers": 849,
      "open_issues": 17,
      "size": 445
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 351550
      },
      "license": "MIT License",
      "topics": [
        "ai-audio",
        "ai-tts",
        "ai-voice",
        "ai-voice-clone",
        "ai-voice-clonining",
        "comfyui-custom-node",
        "comfyui-custom-nodes-text-to-speech",
        "comfyui-nodes",
        "t2s",
        "text-to-speech",
        "tts",
        "vibevoice",
        "vibevoice-microsoft",
        "voice-cloning",
        "voice-generation",
        "voice-generator"
      ]
    },
    "content": {
      "readme": "# VibeVoice ComfyUI Nodes\n\nA comprehensive ComfyUI integration for Microsoft's VibeVoice text-to-speech model, enabling high-quality single and multi-speaker voice synthesis directly within your ComfyUI workflows.\n\n## Features\n\n### Core Functionality\n- 🎤 **Single Speaker TTS**: Generate natural speech with optional voice cloning\n- 👥 **Multi-Speaker Conversations**: Support for up to 4 distinct speakers\n- 🎯 **Voice Cloning**: Clone voices from audio samples\n- 📝 **Text File Loading**: Load scripts from text files\n- 📚 **Automatic Text Chunking**: Handles long texts seamlessly with configurable chunk size\n- ⏸️ **Custom Pause Tags**: Insert silences with `[pause]` and `[pause:ms]` tags (wrapper feature)\n- 🔄 **Node Chaining**: Connect multiple VibeVoice nodes for complex workflows\n- ⏹️ **Interruption Support**: Cancel operations before or between generations\n\n### Model Options\n- 🚀 **Three Model Variants**: \n  - VibeVoice 1.5B (faster, lower memory)\n  - VibeVoice-Large (best quality, ~17GB VRAM)\n  - VibeVoice-Large-Quant-4Bit (balanced, ~7GB VRAM)\n- 🔧 **Flexible Configuration**: Control temperature, sampling, and guidance scale\n\n### Performance & Optimization\n- ⚡ **Attention Mechanisms**: Choose between auto, eager, sdpa, flash_attention_2 or sage\n- 🎛️ **Diffusion Steps**: Adjustable quality vs speed trade-off (default: 20)\n- 💾 **Memory Management**: Toggle automatic VRAM cleanup after generation\n- 🧹 **Free Memory Node**: Manual memory control for complex workflows\n- 🍎 **Apple Silicon Support**: Native GPU acceleration on M1/M2/M3 Macs via MPS\n- 🔢 **4-Bit Quantization**: Reduced memory usage with minimal quality loss\n\n### Compatibility & Installation\n- 📦 **Self-Contained**: Embedded VibeVoice code, no external dependencies\n- 🔄 **Universal Compatibility**: Adaptive support for transformers v4.51.3+\n- 🖥️ **Cross-Platform**: Works on Windows, Linux, and macOS\n- 🎮 **Multi-Backend**: Supports CUDA, CPU, and MPS (Apple Silicon)\n\n## Video Demo\n<p align=\"center\">\n  <a href=\"https://www.youtube.com/watch?v=fIBMepIBKhI\">\n    <img src=\"https://img.youtube.com/vi/fIBMepIBKhI/maxresdefault.jpg\" alt=\"VibeVoice ComfyUI Wrapper Demo\" />\n  </a>\n  <br>\n  <strong>Click to watch the demo video</strong>\n</p>\n\n## Installation\n\n### Automatic Installation (Recommended)\n1. Clone this repository into your ComfyUI custom nodes folder:\n```bash\ncd ComfyUI/custom_nodes\ngit clone https://github.com/Enemyx-net/VibeVoice-ComfyUI\n```\n\n2. Restart ComfyUI - the nodes will automatically install requirements on first use\n\n## Available Nodes\n\n### 1. VibeVoice Load Text From File\nLoads text content from files in ComfyUI's input/output/temp directories.\n- **Supported formats**: .txt\n- **Output**: Text string for TTS nodes\n\n### 2. VibeVoice Single Speaker\nGenerates speech from text using a single voice.\n- **Text Input**: Direct text or connection from Load Text node\n- **Models**: VibeVoice-1.5B or VibeVoice-Large\n- **Voice Cloning**: Optional audio input for voice cloning\n- **Parameters** (in order):\n  - `text`: Input text to convert to speech\n  - `model`: VibeVoice-1.5B, VibeVoice-Large or VibeVoice-Large-Quant-4Bit\n  - `attention_type`: auto, eager, sdpa, flash_attention_2 or sage (default: auto)\n  - `free_memory_after_generate`: Free VRAM after generation (default: True)\n  - `diffusion_steps`: Number of denoising steps (5-100, default: 20)\n  - `seed`: Random seed for reproducibility (default: 42)\n  - `cfg_scale`: Classifier-free guidance (1.0-2.0, default: 1.3)\n  - `use_sampling`: Enable/disable deterministic generation (default: False)\n- **Optional Parameters**:\n  - `voice_to_clone`: Audio input for voice cloning\n  - `temperature`: Sampling temperature (0.1-2.0, default: 0.95)\n  - `top_p`: Nucleus sampling parameter (0.1-1.0, default: 0.95)\n  - `max_words_per_chunk`: Maximum words per chunk for long texts (100-500, default: 250)\n\n### 3. VibeVoice Multiple Speakers\nGenerates multi-speaker conversations with distinct voices.\n- **Speaker Format**: Use `[N]:` notation where N is 1-4\n- **Voice Assignment**: Optional voice samples for each speaker\n- **Recommended Model**: VibeVoice-Large for better multi-speaker quality\n- **Parameters** (in order):\n  - `text`: Input text with speaker labels\n  - `model`: VibeVoice-1.5B, VibeVoice-Large or VibeVoice-Large-Quant-4Bit\n  - `attention_type`: auto, eager, sdpa, flash_attention_2 or sage (default: auto)\n  - `free_memory_after_generate`: Free VRAM after generation (default: True)\n  - `diffusion_steps`: Number of denoising steps (5-100, default: 20)\n  - `seed`: Random seed for reproducibility (default: 42)\n  - `cfg_scale`: Classifier-free guidance (1.0-2.0, default: 1.3)\n  - `use_sampling`: Enable/disable deterministic generation (default: False)\n- **Optional Parameters**:\n  - `speaker1_voice` to `speaker4_voice`: Audio inputs for voice cloning\n  - `temperature`: Sampling temperature (0.1-2.0, default: 0.95)\n  - `top_p`: Nucleus sampling parameter (0.1-1.0, default: 0.95)\n\n### 4. VibeVoice Free Memory\nManually frees ",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-21T02:26:52.867620"
  },
  {
    "basic_info": {
      "name": "SpikingBrain-7B",
      "full_name": "BICLab/SpikingBrain-7B",
      "owner": "BICLab",
      "description": null,
      "url": "https://github.com/BICLab/SpikingBrain-7B",
      "clone_url": "https://github.com/BICLab/SpikingBrain-7B.git",
      "ssh_url": "git@github.com:BICLab/SpikingBrain-7B.git",
      "homepage": null,
      "created_at": "2025-09-03T10:46:46Z",
      "updated_at": "2025-09-21T00:49:40Z",
      "pushed_at": "2025-09-18T09:49:18Z"
    },
    "stats": {
      "stars": 832,
      "forks": 99,
      "watchers": 832,
      "open_issues": 7,
      "size": 7276
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 325821
      },
      "license": null,
      "topics": []
    },
    "content": {
      "readme": "# SpikingBrain：Spiking Brain-inspired Large Models\r\n\r\n📄 Technical Report: [Chinese](SpikingBrain_Report_Chi.pdf) | [English](SpikingBrain_Report_Eng.pdf)  \r\n🚀 Arxiv: [arXiv:2509.05276](https://www.arxiv.org/abs/2509.05276)  \r\n🧩 Models: [Available Models](#available-models)   \r\n\r\n---\r\n\r\n## About SpikingBrain\r\n\r\nInspired by brain mechanisms, **SpikingBrain** integrates **hybrid efficient attention**, **MoE modules**, and **spike encoding** into its architecture, supported by a universal conversion pipeline compatible with the open-source model ecosystem. This enables continual pre-training with less than 2\\% of the data while achieving performance comparable to mainstream open-source models. We further adapt frameworks, operators, parallel strategies, and communication primitives for **non-NVIDIA (MetaX) clusters**, ensuring stable large-scale training and inference. SpikingBrain achieves over 100× speedup in TTFT for 4M-token sequences, while spiking delivers over 69\\% sparsity at the micro level. Combined with macro-level MoE sparsity, these advances provide valuable guidance for the design of next-generation neuromorphic chips.\r\n\r\n![](assets/fig1.png)\r\n\r\n---\r\n\r\n## Project Structure\r\nThis repository provides the full implementation and weights of **SpikingBrain-7B**, including the **HuggingFace version**, **vLLM inference version**, and **quantized version**, enabling flexible deployment and research across different scenarios.\r\n\r\n```\r\nSpikingBrain-7B/\r\n├── hf_7B_model/ # HuggingFace version\r\n├── run_model/   # Model run examples\r\n├── vllm_hymeta/ # vLLM plugins and inference support\r\n├── W8ASpike/    # Quantized inference version\r\n├── setup.py\r\n├── requirements.txt \r\n└── README.md \r\n```\r\n\r\n--- \r\n\r\n## vLLM-HyMeta\r\n\r\n**vllm-hymeta** is the plugin adaptation of HyMeta (Hybrid Models built on MetaX GPUs) for the [vLLM inference framework](https://github.com/vllm-project/vllm/tree/main), providing efficient inference support on NVIDIA GPUs.\r\n\r\nBy leveraging the [plugins mechanism](https://blog.vllm.ai/2025/05/12/hardware-plugin.html) in vLLM, hardware backends can be integrated in a modular fashion, bringing the following benefits:\r\n\r\n- **Decoupled codebase**: Backend-specific code remains independent, keeping the vLLM core cleaner.\r\n\r\n- **Reduced maintenance cost**: vLLM developers can focus on general functionality without being affected by backend-specific implementations.\r\n\r\n- **Faster integration**: New backends can be integrated quickly and evolve independently with less engineering effort.\r\n\r\n### Container Deployment (NVIDIA)\r\n```bash\r\nsudo docker run -itd \\\r\n    --entrypoint /bin/bash \\\r\n    --network host \\\r\n    --name hymeta-bench \\\r\n    --shm-size 160g \\\r\n    --gpus all \\\r\n    --privileged \\\r\n    -v /host_path:/container_path \\\r\n    docker.1ms.run/vllm/vllm-openai:v0.10.0\r\n```\r\n\r\n### Plugin Installation\r\n```bash\r\ngit clone https://github.com/BICLab/SpikingBrain-7B.git\r\ncd SpikingBrain-7B\r\npip install .\r\n```\r\n\r\nRecommended environment for installing **vllm-hymeta** on NVIDIA GPUs:\r\n\r\n```makefile\r\ntorch==2.7.1\r\ntransformers==4.55.2\r\ntriton==3.3.1\r\nflash_attn==2.7.3\r\nflash-linear-attention==0.1\r\nvllm==0.10.0\r\nscipy\r\npyyaml\r\ndecorator\r\nsetuptools\r\nsetuptools-scm\r\n```\r\n\r\nDue to the conflict between `flash-linear-attention==0.1` and `transformers==4.55.2`, You can refer to [fla-org/flash-linear-attention#425](https://github.com/fla-org/flash-linear-attention/issues/425) to modify `<your_path_to_python3.12>/dist-packages/fla/models/bitnet/init.py` as follows:\r\n\r\nChange from:\r\n```python\r\nAutoConfig.register(ABCConfig.model_type, ABCConfig)\r\nAutoModel.register(ABCConfig, ABCModel)\r\nAutoModelForCausalLM.register(ABCConfig, ABCForCausalLM)\r\n```\r\nto:\r\n```python\r\nAutoConfig.register(ABCConfig.model_type, ABCConfig, exist_ok=True)\r\nAutoModel.register(ABCConfig, ABCModel, exist_ok=True)\r\nAutoModelForCausalLM.register(ABCConfig, ABCForCausalLM, exist_ok=True)\r\n```\r\nThat is, add `exist_ok=True` to each line.\r\n\r\n### Run with vLLM\r\n\r\nYou can serve a model with vLLM in the simplest way using the following command:\r\n\r\n```bash\r\nvllm serve <your_model_path> \\\r\n  --served-model-name <model_name> \\\r\n  --gpu-memory-utilization <ratio> \\\r\n  --block-size <size> \\\r\n  --dtype bfloat16 \\\r\n  --port <port_number>\r\n```\r\n\r\nYou may also set `--tensor-parallel-size` and `--pipeline-parallel-size` when launching if you want to run with multiple GPUs. \r\n\r\n---\r\n\r\n## W8ASpike\r\n\r\n**W8ASpike** is the quantized inference version of SpikingBrain-7B, aiming to reduce inference cost under low-precision settings and explore the potential of Spiking Neural Networks (SNNs).\r\n\r\nThe current implementation adopts **pseudo-spiking**, where activations are approximated as spike-like signals at the tensor level, rather than true asynchronous event-driven spiking on neuromorphic hardware.\r\n\r\n- **Pseudo-spiking**: Efficient approximation at the tensor level, suitable for prototyping and research.\r\n\r\n- **True-spiking**: Requires asynchronous hardware and eve",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-21T02:26:54.108606"
  },
  {
    "basic_info": {
      "name": "cc-sessions",
      "full_name": "GWUDCAP/cc-sessions",
      "owner": "GWUDCAP",
      "description": "An opinionated extension set for Claude Code (hooks, subagents, commands, task/git management infrastructure)",
      "url": "https://github.com/GWUDCAP/cc-sessions",
      "clone_url": "https://github.com/GWUDCAP/cc-sessions.git",
      "ssh_url": "git@github.com:GWUDCAP/cc-sessions.git",
      "homepage": "",
      "created_at": "2025-08-25T19:07:25Z",
      "updated_at": "2025-09-20T22:45:27Z",
      "pushed_at": "2025-09-17T10:49:39Z"
    },
    "stats": {
      "stars": 824,
      "forks": 65,
      "watchers": 824,
      "open_issues": 31,
      "size": 382
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 68837,
        "JavaScript": 34623,
        "Shell": 29738,
        "PowerShell": 1197,
        "Batchfile": 1148
      },
      "license": null,
      "topics": []
    },
    "content": {
      "readme": "```\n███████╗███████╗███████╗███████╗██╗ ██████╗ ███╗   ██╗███████╗\n██╔════╝██╔════╝██╔════╝██╔════╝██║██╔═══██╗████╗  ██║██╔════╝\n███████╗█████╗  ███████╗███████╗██║██║   ██║██╔██╗ ██║███████╗\n╚════██║██╔══╝  ╚════██║╚════██║██║██║   ██║██║╚██╗██║╚════██║\n███████║███████╗███████║███████║██║╚██████╔╝██║ ╚████║███████║\n╚══════╝╚══════╝╚══════╝╚══════╝╚═╝ ╚═════╝ ╚═╝  ╚═══╝╚══════╝\n```\n\n<sub>_A public good brought to you by GWUDCAP and Three AIrrows Capital_</sub>\n\n---\n\n<br>\n<div align=\"center\">\n\n##### ⚡ SHOCKING REPORT REVEALS ⚡\n\n# **Vibe coding is shitty and confusing and produces garbage software that sucks to work on.**<br>\n\n### _Claude Code makes it less shitty, but not by enough._\n\n---\n</div>\nexplainer: coming soon...\n<br>\nworking demo: https://www.youtube.com/watch?v=B-sIBb-XvO4\n\n## 🔥 The Problem\n\n<details>\n<summary><strong>How you got here</strong> <em>(click to read the painful journey)</em></summary>\n\n<br>\n\nI'm going to guess how you got here and you can tell me if I get it right:\n\n- 💭 The LLM programmer hype gave you a nerd chub  \n- 😬 The people hyping LLM programming made your nerd chub crawl back into your body <br> <sup>_(are you ready to 'scale your impact', dog?)_</sup> \n- 🤮 You held your nose and downloaded Cursor/added Cline or Roo Code/npm installed Claude Code\n\nAt first this was obviously novel and interesting. Some things were shitty but mostly you were enjoying not having to write a context manager or even recognize that you needed one for your dumb client wrapper.\n\n**You were _scaling_ your _impact_** _(whew)_.\n\nBut then Claude started doing some concerning things. \n\nYou asked it to add error handling to **one** function. It added error handling to **_every function in the file_**. And changed your error types. And your logging format. And somehow your indentation is different now?\n\nThe context window thing started getting annoying. You're explaining the same architecture for the fifth time today. Claude's like _'let me look for the database'_ **Brother. We've been using Postgres for six hours. You were just in there.**\n\nYour CLAUDE.md is now longer than your actual code. \n- `'NEVER use class components.'` \n- `'ALWAYS use the existing auth middleware.'` \n- `'DO NOT refactor unrelated code.'` \n- `'REMEMBER we use PostgreSQL.'` \n\nClaude reads the first line and then macrodoses window pane LSD for the rest.\n\nYou tried the subagents, but quickly realized that **you can't even talk to these things.** 10 minutes into a \"code review\" and the agent hits some kind of API error and returns to your main thread with no explanation of what it did or what it discovered. \n\nRun it again, I guess? \n\n_This fucking sucks_.\n\nNow you're here. Your codebase is 'done' but you couldn't, in a million years, explain what that means or how it satisfies the definition. \n\nThere's three different global clients for the same database connection and two of them use hallucinated environment variables (the other just yeets your client secret into the service code). \n\nYou've got utility functions that are duplicated in four files because Claude kept forgetting they exist.\n\n20% of your code lines are comments explaining why something *isn't* there and *is* somewhere else.\n\nYou don't even know exactly what's wrong and fixing it means understanding code you didn't write in patterns you don't recognize using approaches you wouldn't choose.\n\n### **Are you scaling your impact yet?**\n\n</details>\n\n## 💊 The Solution\n\n<details>\n<summary><strong>What We Fixed</strong> <em>(what you get when you use cc-sessions)</em></summary>\n\n<br>\n\nSo, now you're here. Since this is exclusively about Claude Code I'm going to assume that you are a CC user and you are looking to make that better. **Lit.**\n\nLet's talk about Claude Code.\n\nOf the major AI programming IDEs/scaffolds, Claude Code is probably the best and Claude models are probably the best _(though Google is kinda coming for that ass)_.\n\nBut, Claude Code is not without its **major faults, flaws, and flaccidity-inducing frustrations**.\n\nFor instance, **it would be nice if**:\n\n- Claude had to talk to you before writing code so you didn't end up with 500 lines of implementation for a one-line change.\n\n- you didn't lose everything when the context window died and Claude actually remembered what you were working on tomorrow.\n\n- you didn't have to explain your entire architecture every. single. session. and Claude actually inherited understanding from previous work.\n\n- Claude couldn't randomly refactor working code while you're trying to add a button.\n\n- you didn't have to manually check which branch you're on in five different repos and Claude actually stopped you before you edited the wrong one.\n\n- Claude followed the patterns in your codebase instead of inventing new ones every time it touches a file.\n\n- you didn't have to write increasingly desperate rules in CLAUDE.md and Claude was actually forced to follow consistent behavior.\n\n### **This is what Sessions does.**\n\nIt makes all of these n",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-21T02:26:55.388644"
  },
  {
    "basic_info": {
      "name": "checkpoint-engine",
      "full_name": "MoonshotAI/checkpoint-engine",
      "owner": "MoonshotAI",
      "description": "Checkpoint-engine is a simple middleware to update model weights in LLM inference engines",
      "url": "https://github.com/MoonshotAI/checkpoint-engine",
      "clone_url": "https://github.com/MoonshotAI/checkpoint-engine.git",
      "ssh_url": "git@github.com:MoonshotAI/checkpoint-engine.git",
      "homepage": "",
      "created_at": "2025-09-08T08:04:20Z",
      "updated_at": "2025-09-20T22:27:38Z",
      "pushed_at": "2025-09-20T10:39:12Z"
    },
    "stats": {
      "stars": 722,
      "forks": 48,
      "watchers": 722,
      "open_issues": 3,
      "size": 328
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 55357
      },
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "# Checkpoint Engine\nCheckpoint-engine is a simple middleware to update model weights in LLM inference engines -- a critical step in reinforcement learning.\nWe provide an efficient and lightweight implementation for inplace weight update:\nupdating our [Kimi-K2](https://github.com/MoonshotAI/Kimi-K2) model (1 Trillion parameters) across thousands of GPUs takes about 20s.\n\n\n<div align=\"center\">\n  <picture>\n      <img src=\"figures/checkpoint-engine.png\" width=\"80%\" alt=\"ckpt-engine\">\n  </picture>\n</div>\n\n## Architecture\n\nThe core weight update logic is in `ParameterServer` class, a service colocated with inference engines. It provides two implementations of weight update: Broadcast and P2P.\n\n- **Broadcast**: Used when a large number of inference instances need to update weights in synchronous. This is the fastest implementation and should be used as the default update method. See `_update_per_bucket`.\n- **P2P**: Used when new inference instances are dynamically added (due to restarts or dynamic availability) while the existing instances are already serving requests. Under this scenario, to avoid affecting the workloads on existing instances, we use the [`mooncake-transfer-engine`](https://github.com/kvcache-ai/Mooncake?tab=readme-ov-file#use-python-package) to P2P send weights from CPUs in existing instances to GPUs in new instances. See `_update_per_bucket_p2p`.\n\n### Optimized Weight Broadcast\nIn the *Broadcast* implementation, the checkpoint-engine holds references to sharded weights in CPU memory, and need to efficiently broadcast them to a cluster of inference instances, often under a different sharding pattern.\nWe arrange the data transfer into 3 stages:\n1. H2D: moving weights to GPU memory. These weights may come from disk or the training engine.\n2. broadcast: broadcast among checkpoint engine workers; the data results in a CUDA IPC buffer shared with inference engine.\n3. reload: inference engine decides what subset of weights to copy from the broadcasted data.\n\nCheckpoint-engine orchestrates the entire transfer process. It first gathers necessary metadata to create a plan, including deciding the proper bucket size for data transfer.\nIt then executes the transfer, where it controls the inference engine through a ZeroMQ socket. To maximize performance, it organizes the data transfers into a pipeline with overlapped communication and copy, illustrated below. The details can be found in [Kimi-K2 Technical Report](https://arxiv.org/abs/2507.20534).\n\n\n<div align=\"center\">\n  <picture>\n      <img src=\"figures/pipeline.png\" width=\"80%\" alt=\"pipeline\">\n  </picture>\n</div>\n\nPipelining naturally requires more GPU memory. When memory is not enough, checkpoint-engine will fallback to serial execution.\n\n## Benchmark\n\n| Model                                | Device Info  | GatherMetas | Update (Broadcast) | Update (P2P)            |\n| :----------------------------------- | :----------- | :---------- |:-------------------| :---------------------- |\n| GLM-4.5-Air (BF16)                   | 8xH800 TP8  | 0.17s       | 3.94s (1.42GiB)    | 8.83s (4.77GiB)         |\n| Qwen3-235B-A22B-Instruct-2507 (BF16) | 8xH800 TP8  | 0.46s       | 6.75s (2.69GiB)    | 16.47s (4.05GiB)        |\n| DeepSeek-V3.1 (FP8)                  | 16xH20 TP16  | 1.44s       | 12.22s (2.38GiB)   | 25.77s (3.61GiB)        |\n| Kimi-K2-Instruct (FP8)               | 16xH20 TP16  | 1.81s       | 15.45s (2.93GiB)   | 36.24s (4.46GiB)        |\n| DeepSeek-V3.1 (FP8)                  | 256xH20 TP16 | 1.40s       | 13.88s (2.54GiB)   | 33.30s (3.86 GiB) |\n| Kimi-K2-Instruct (FP8)               | 256xH20 TP16 | 1.88s       | 21.50s (2.99GiB)   | 34.49s (4.57 GiB) |\n\nAll results above are tested by [`examples/update.py`](./examples/update.py) and use [vLLM v0.10.2rc1](https://github.com/vllm-project/vllm/tree/v0.10.2rc1) as inference engine. Some notes:\n\n* FP8 test needs additional vLLM patches, see [FP8 quantization](#fp8-quantization).\n* Device Info: we tested various combination of devices and parallelism setups. For example, a 256-GPU TP16 setup means that we deploy 16 vLLM instances, each with 16-way tensor parallelism.\n* Since update duration is related to IPC bucket size, we provide the bucket size in the table.\n* The P2P time were tested for updating no more than two nodes (16 GPUs) (`ParameterServer.update(ranks=range(0, 16))`) out of the entire cluster.\n\n## Installation\n\nUse the fastest broadcast implementation\n\n```Bash\npip install checkpoint-engine\n```\n\nUse the flexible P2P implementation, notice this will install `mooncake-transfer-engine` to support RDMA transfer between different ranks.\n\n```Bash\npip install 'checkpoint-engine[p2p]'\n```\n\nIf set `NCCL_IB_HCA` env, checkpoint-engine will use it to auto select net devices for different ranks. If not set, it will read all RDMA devices and try to divide them into each rank.\n\n## Getting Started\n\nPrepare an H800 or H20 machine with 8 GPUs with latest vLLM. Be sure to include [/collective_rpc API endpoint](ht",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-21T02:26:56.733064"
  },
  {
    "basic_info": {
      "name": "batch_invariant_ops",
      "full_name": "thinking-machines-lab/batch_invariant_ops",
      "owner": "thinking-machines-lab",
      "description": null,
      "url": "https://github.com/thinking-machines-lab/batch_invariant_ops",
      "clone_url": "https://github.com/thinking-machines-lab/batch_invariant_ops.git",
      "ssh_url": "git@github.com:thinking-machines-lab/batch_invariant_ops.git",
      "homepage": null,
      "created_at": "2025-09-10T00:08:08Z",
      "updated_at": "2025-09-20T22:27:48Z",
      "pushed_at": "2025-09-10T16:58:34Z"
    },
    "stats": {
      "stars": 671,
      "forks": 40,
      "watchers": 671,
      "open_issues": 3,
      "size": 8
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 18886
      },
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "# Batch Invariant Ops\n\nA companion library release to https://thinkingmachines.ai/blog/defeating-nondeterminism-in-llm-inference/. This library contains some batch-invariant kernels as well as an example of achieving deterministic vLLM inference.\n\n## Overview\n\nThis library primarily leverages torch.Library to sub out existing PyTorch kernels with \"batch-invariant\" ones. This allows many existing PyTorch models to use the batch-invariant ops with low overhead and non-intrusive code changes.\n\n## Installation\n\n```bash\npip install -e .\n```\n\n## Quick Start\n\n```python\nimport torch\nfrom batch_invariant_ops import set_batch_invariant_mode\n\n# Enable batch-invariant mode\nwith set_batch_invariant_mode():\n    # Your inference code here\n    model = YourModel()\n    output = model(input_tensor)\n```\n\n## Testing Batch-Invariance\n\nThe following example shows how batch size can affect results in standard PyTorch:\n\n```python\nimport torch\nfrom batch_invariant_ops import set_batch_invariant_mode\ntorch.set_default_device('cuda')\n\n# Just to get the logging out of the way haha\nwith set_batch_invariant_mode(True):\n    pass\n\ndef test_batch_invariance():\n    B, D = 2048, 4096\n    a = torch.linspace(-100, 100, B*D).reshape(B, D)\n    b = torch.linspace(-100, 100, D*D).reshape(D, D)\n    \n    # Method 1: Matrix-vector multiplication (batch size 1)\n    out1 = torch.mm(a[:1], b)\n    \n    # Method 2: Matrix-matrix multiplication, then slice (full batch)\n    out2 = torch.mm(a, b)[:1]\n    \n    # Check if results are identical\n    diff = (out1 - out2).abs().max()\n    print(f\"Difference: {diff.item()}\")\n    return diff.item() == 0\n\n# Test with standard PyTorch (likely to show differences)\nprint(\"Standard PyTorch:\")\nwith set_batch_invariant_mode(False):\n    is_deterministic = test_batch_invariance()\n    print(f\"Deterministic: {is_deterministic}\")\n\n# Test with batch-invariant operations\nprint(\"\\nBatch-Invariant Mode:\")\nwith set_batch_invariant_mode(True):\n    is_deterministic = test_batch_invariance()\n    print(f\"Deterministic: {is_deterministic}\")\n\n```\n\n## Deterministic Inference in vLLM\n`deterministic_vllm_inference.py` shows an proof of concept of validating that vLLM can be made deterministic with a minor upstream PR to use this library. Without the upstream PR, we see that out of 1000 random length 100 completions we see 18 unique samples. After the upstream PR, there is only one unique sample.\n\n## Supported Operations\n\n### Matrix Operations\n- `torch.mm()` - Matrix multiplication\n- `torch.addmm()` - Matrix multiplication with bias addition\n\n### Activation Functions\n- `torch.log_softmax()` - Log-softmax activation\n\n### Reduction Operations\n- `torch.mean()` - Mean computation along specified dimensions\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-21T02:26:58.027921"
  },
  {
    "basic_info": {
      "name": "PromptEnhancer",
      "full_name": "Hunyuan-PromptEnhancer/PromptEnhancer",
      "owner": "Hunyuan-PromptEnhancer",
      "description": "PromptEnhancer is a prompt-rewriting tool, refining prompts into clearer, structured versions for better image generation.",
      "url": "https://github.com/Hunyuan-PromptEnhancer/PromptEnhancer",
      "clone_url": "https://github.com/Hunyuan-PromptEnhancer/PromptEnhancer.git",
      "ssh_url": "git@github.com:Hunyuan-PromptEnhancer/PromptEnhancer.git",
      "homepage": "https://hunyuan-promptenhancer.github.io/",
      "created_at": "2025-09-09T08:56:47Z",
      "updated_at": "2025-09-21T02:22:04Z",
      "pushed_at": "2025-09-19T08:51:33Z"
    },
    "stats": {
      "stars": 642,
      "forks": 52,
      "watchers": 642,
      "open_issues": 4,
      "size": 11876
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 20985
      },
      "license": "Other",
      "topics": [
        "hunyuan",
        "hunyuan-image",
        "prompt",
        "prompt-engineering",
        "prompt-enhancer",
        "vlm"
      ]
    },
    "content": {
      "readme": "<div align=\"center\">\n\n# PromptEnhancer: A Simple Approach to Enhance Text-to-Image Models via Chain-of-Thought Prompt Rewriting\n\n[**Linqing Wang**](https://scholar.google.com/citations?hl=en&view_op=list_works&gmla=AH8HC4z9rmDHYjp5o28xKk8U4ddD_n7BuMnk8UZFP-jygFBtHUSz6pf-5FP32B_yKMpRU9VpDY3iT8eM0zORHA&user=Hy12lcEAAAAJ) · \n[**Ximing Xing**](https://ximinng.github.io/) · \n[**Yiji Cheng**](https://scholar.google.com/citations?user=Plo8ZSYAAAAJ&hl=en) · \nZhiyuan Zhao · \nDonghao Li ·\nTiankai Hang ·\n[**Jiale Tao**](https://scholar.google.com/citations?user=WF5DPWkAAAAJ&hl=en) · \n[**QiXun Wang**](https://github.com/wangqixun) · \n[**Ruihuang Li**](https://scholar.google.com/citations?user=8CfyOtQAAAAJ&hl=en) · \nComi Chen ·\nXin Li · \n[**Mingrui Wu**](https://scholar.google.com/citations?user=sbCKwnYAAAAJ&hl=en) · \nXinchi Deng · \nShuyang Gu · \n[**Chunyu Wang**](https://scholar.google.com/citations?user=VXQV5xwAAAAJ&hl=en)<sup>†</sup> · \n[**Qinglin Lu**](https://luqinglin.weebly.com/)<sup>*</sup>\n\nTencent Hunyuan\n\n<sup>†</sup>Project Lead · <sup>*</sup>Corresponding Author\n\n</div>\n\n<p align=\"center\">\n  <a href=\"https://www.arxiv.org/abs/2509.04545\"><img src=\"https://img.shields.io/badge/Paper-arXiv:2509.04545-red?logo=arxiv\" alt=\"arXiv\"></a>\n  <a href=\"https://zhuanlan.zhihu.com/p/1949013083109459515\"><img src=\"https://img.shields.io/badge/知乎-技术解读-0084ff?logo=zhihu\" alt=\"Zhihu\"></a>\n  <a href=\"https://huggingface.co/tencent/HunyuanImage-2.1/tree/main/reprompt\"><img src=\"https://img.shields.io/badge/Model-PromptEnhancer_7B-blue?logo=huggingface\" alt=\"HuggingFace Model\"></a>\n  <!-- <a href=\"https://huggingface.co/PromptEnhancer/PromptEnhancer-32B\"><img src=\"https://img.shields.io/badge/Model-PromptEnhancer_32B-blue?logo=huggingface\" alt=\"HuggingFace Model\"></a> -->\n  <a href=\"https://huggingface.co/datasets/PromptEnhancer/T2I-Keypoints-Eval\"><img src=\"https://img.shields.io/badge/Benchmark-T2I_Keypoints_Eval-blue?logo=huggingface\" alt=\"T2I-Keypoints-Eval Dataset\"></a>\n  <a href=\"https://hunyuan-promptenhancer.github.io/\"><img src=\"https://img.shields.io/badge/Homepage-PromptEnhancer-1abc9c?logo=homeassistant&logoColor=white\" alt=\"Homepage\"></a>\n  <a href=\"https://github.com/Tencent-Hunyuan/HunyuanImage-2.1\"><img src=\"https://img.shields.io/badge/Code-HunyuanImage2.1-2ecc71?logo=github\" alt=\"HunyuanImage2.1 Code\"></a>\n  <a href=\"https://huggingface.co/tencent/HunyuanImage-2.1\"><img src=\"https://img.shields.io/badge/Model-HunyuanImage2.1-3498db?logo=huggingface\" alt=\"HunyuanImage2.1 Model\"></a>\n  <a href=https://x.com/TencentHunyuan target=\"_blank\"><img src=https://img.shields.io/badge/Hunyuan-black.svg?logo=x height=22px></a>\n</p>\n\n---\n\n<p align=\"center\">\n  <img src=\"assets/teaser-1.png\" alt=\"PromptEnhancer Teaser\"/>\n</p>\n\n## Overview\n\nHunyuan-PromptEnhancer is a prompt rewriting utility. It restructures an input prompt while preserving the original intent, producing clearer, layered, and logically consistent prompts suitable for downstream image generation or similar tasks.\n\n- Preserves intent across key elements (subject/action/quantity/style/layout/relations/attributes/text, etc.).\n- Encourages a \"global–details–summary\" narrative, describing primary elements first, then secondary/background elements, ending with a concise style/type summary.\n- Robust output parsing with graceful fallback: prioritizes `<answer>...</answer>`; if missing, removes `<think>...</think>` and extracts clean text; otherwise falls back to the original input.\n- Configurable inference parameters (temperature, top_p, max_new_tokens) for balancing determinism and diversity.\n\n## 🔥🔥🔥Updates\n- [2025-09-18] ✨ Try the [PromptEnhancer-32B](https://huggingface.co/PromptEnhancer/PromptEnhancer-32B) for higher-quality prompt enhancement!\n- [2025-09-16] Release [T2I-Keypoints-Eval dataset](https://huggingface.co/datasets/PromptEnhancer/T2I-Keypoints-Eval).\n- [2025-09-07] Release [PromptEnhancer-7B model](https://huggingface.co/tencent/HunyuanImage-2.1/tree/main/reprompt).\n- [2025-09-07] Release [technical report](https://arxiv.org/abs/2509.04545).\n\n## Installation\n\n```bash\npip install -r requirements.txt\n```\n\n## Model Download\n\n```bash\n# for PromptEnhancer-7B model\nhuggingface-cli download tencent/HunyuanImage-2.1/reprompt --local-dir ./models/promptenhancer-7b\n```\n\n## Quickstart\n\n### Using HunyuanPromptEnhancer\n\n```python\nfrom inference.prompt_enhancer import HunyuanPromptEnhancer\n\nmodels_root_path = \"./models/promptenhancer-7b\"\n\nenhancer = HunyuanPromptEnhancer(models_root_path=models_root_path, device_map=\"auto\")\n\n# Enhance a prompt (Chinese or English)\nuser_prompt = \"Third-person view, a race car speeding on a city track...\"\nnew_prompt = enhancer.predict(\n    prompt_cot=user_prompt,\n    # Default system prompt is tailored for image prompt rewriting; override if needed\n    temperature=0.7,   # >0 enables sampling; 0 uses deterministic generation\n    top_p=0.9,\n    max_new_tokens=256,\n)\n\nprint(\"Enhanced:\", new_prompt)\n```\n\n## Parameters\n\n- `models_root_pa",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-21T02:26:59.298468"
  },
  {
    "basic_info": {
      "name": "AI-Video-Transcriber",
      "full_name": "wendy7756/AI-Video-Transcriber",
      "owner": "wendy7756",
      "description": " Transcribe and summarize video content using AI. Open-source, multi-platform, and supports multiple languages.",
      "url": "https://github.com/wendy7756/AI-Video-Transcriber",
      "clone_url": "https://github.com/wendy7756/AI-Video-Transcriber.git",
      "ssh_url": "git@github.com:wendy7756/AI-Video-Transcriber.git",
      "homepage": "",
      "created_at": "2025-08-28T06:27:27Z",
      "updated_at": "2025-09-21T00:25:20Z",
      "pushed_at": "2025-09-06T04:51:09Z"
    },
    "stats": {
      "stars": 620,
      "forks": 83,
      "watchers": 620,
      "open_issues": 1,
      "size": 295
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 105866,
        "JavaScript": 32792,
        "HTML": 16765,
        "Shell": 2219,
        "Dockerfile": 771
      },
      "license": "Apache License 2.0",
      "topics": [
        "aitool",
        "tiktok",
        "transcribe",
        "videototext",
        "youtube"
      ]
    },
    "content": {
      "readme": "<div align=\"center\">\n\n# AI Video Transcriber\n\nEnglish | [中文](README_ZH.md)\n\nAn AI-powered video transcription and summarization tool that supports multiple video platforms including YouTube, Tiktok, Bilibili, and 30+ platforms.\n\n![Interface](en-video.png)\n\n</div>\n\n## ✨ Features\n\n- 🎥 **Multi-Platform Support**: Works with YouTube, Tiktok, Bilibili, and 30+ more\n- 🗣️ **Intelligent Transcription**: High-accuracy speech-to-text using Faster-Whisper\n- 🤖 **AI Text Optimization**: Automatic typo correction, sentence completion, and intelligent paragraphing\n- 🌍 **Multi-Language Summaries**: Generate intelligent summaries in multiple languages\n- ⚡ **Real-Time Progress**: Live progress tracking and status updates\n- ⚙️ **Conditional Translation**: When the selected summary language differs from the detected transcript language, the system auto-translates with GPT‑4o\n- 📱 **Mobile-Friendly**: Perfect support for mobile devices\n\n## 🚀 Quick Start\n\n### Prerequisites\n\n- Python 3.8+\n- FFmpeg\n- Optional: OpenAI API key (for AI summary features)\n\n### Installation\n\n#### Method 1: Automatic Installation\n\n```bash\n# Clone the repository\ngit clone https://github.com/wendy7756/AI-Video-Transcriber.git\ncd AI-Video-Transcriber\n\n# Run installation script\nchmod +x install.sh\n./install.sh\n```\n\n#### Method 2: Docker\n\n```bash\n# Clone the repository\ngit clone https://github.com/wendy7756/AI-Video-Transcriber.git\ncd AI-Video-Transcriber\n\n# Using Docker Compose (easiest)\ncp .env.example .env\n# Edit .env file and set your OPENAI_API_KEY\ndocker-compose up -d\n\n# Or using Docker directly\ndocker build -t ai-video-transcriber .\ndocker run -p 8000:8000 -e OPENAI_API_KEY=\"your_api_key_here\" ai-video-transcriber\n```\n\n#### Method 3: Manual Installation\n\n1. **Install Python Dependencies**\n```bash\n# macOS (PEP 668) strongly recommends using a virtualenv\npython3 -m venv .venv\nsource .venv/bin/activate\npython -m pip install --upgrade pip\npip install -r requirements.txt\n```\n\n2. **Install FFmpeg**\n```bash\n# macOS\nbrew install ffmpeg\n\n# Ubuntu/Debian\nsudo apt update && sudo apt install ffmpeg\n\n# CentOS/RHEL\nsudo yum install ffmpeg\n```\n\n3. **Configure Environment Variables**\n```bash\n# Required for AI summary/translation features\nexport OPENAI_API_KEY=\"your_api_key_here\"\n\n# Optional: only if you use a custom OpenAI-compatible gateway\n\n### Start the Service\n\n```bash\npython3 start.py\n```\n\nAfter the service starts, open your browser and visit `http://localhost:8000`\n\n#### Production Mode (Recommended for long videos)\n\nTo avoid SSE disconnections during long processing, start in production mode (hot-reload disabled):\n\n```bash\npython3 start.py --prod\n```\n\nThis keeps the SSE connection stable throughout long tasks (30–60+ min).\n\n#### Run with explicit env (example)\n\n```bash\nsource .venv/bin/activate\nexport OPENAI_API_KEY=your_api_key_here\n# export OPENAI_BASE_URL=https://oneapi.basevec.com/v1   # if using a custom endpoint\npython3 start.py --prod\n```\n\n## 📖 Usage Guide\n\n1. **Enter Video URL**: Paste a video link from YouTube, Bilibili, or other supported platforms\n2. **Select Summary Language**: Choose the language for the generated summary\n3. **Start Processing**: Click the \"Start\" button\n4. **Monitor Progress**: Watch real-time progress through multiple stages:\n   - Video download and parsing\n   - Audio transcription with Faster-Whisper\n   - AI-powered transcript optimization (typo correction, sentence completion, intelligent paragraphing)\n   - AI summary generation in selected language\n5. **View Results**: Review the optimized transcript and intelligent summary\n   - If transcript language ≠ selected summary language, a third tab “Translation” is shown containing a translated transcript\n6. **Download Files**: Click download buttons to save Markdown-formatted files (Transcript / Translation / Summary)\n\n## 🛠️ Technical Architecture\n\n### Backend Stack\n- **FastAPI**: Modern Python web framework\n- **yt-dlp**: Video downloading and processing\n- **Faster-Whisper**: Efficient speech transcription\n- **OpenAI API**: Intelligent text summarization\n\n### Frontend Stack\n- **HTML5 + CSS3**: Responsive interface design\n- **JavaScript (ES6+)**: Modern frontend interactions\n- **Marked.js**: Markdown rendering\n- **Font Awesome**: Icon library\n\n### Project Structure\n```\nAI-Video-Transcriber/\n├── backend/                 # Backend code\n│   ├── main.py             # FastAPI main application\n│   ├── video_processor.py  # Video processing module\n│   ├── transcriber.py      # Transcription module\n│   ├── summarizer.py       # Summary module\n│   └── translator.py       # Translation module\n├── static/                 # Frontend files\n│   ├── index.html          # Main page\n│   └── app.js              # Frontend logic\n├── temp/                   # Temporary files directory\n├── Dockerfile              # Docker image configuration\n├── docker-compose.yml      # Docker Compose configuration\n├── .dockerignore           # Docker ignore rules\n├── .env.example            # Environment variables templ",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-21T02:27:00.602924"
  },
  {
    "basic_info": {
      "name": "Genie",
      "full_name": "High-Logic/Genie",
      "owner": "High-Logic",
      "description": "GPT-SoVITS ONNX Inference Engine & Model Converter",
      "url": "https://github.com/High-Logic/Genie",
      "clone_url": "https://github.com/High-Logic/Genie.git",
      "ssh_url": "git@github.com:High-Logic/Genie.git",
      "homepage": "",
      "created_at": "2025-08-25T11:30:09Z",
      "updated_at": "2025-09-20T18:21:26Z",
      "pushed_at": "2025-08-31T15:25:51Z"
    },
    "stats": {
      "stars": 619,
      "forks": 31,
      "watchers": 619,
      "open_issues": 2,
      "size": 805
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 104235,
        "Dockerfile": 495
      },
      "license": "MIT License",
      "topics": [
        "gpt-sovits",
        "text-to-speech",
        "tts",
        "vits",
        "voice-clone",
        "voice-cloning"
      ]
    },
    "content": {
      "readme": "<div align=\"center\">\n<pre>\n██████╗  ███████╗███╗   ██╗██╗███████╗\n██╔════╝ ██╔════╝████╗  ██║██║██╔════╝\n██║  ███╗█████╗  ██╔██╗ ██║██║█████╗  \n██║   ██║██╔══╝  ██║╚██╗██║██║██╔══╝  \n╚██████╔╝███████╗██║ ╚████║██║███████╗\n ╚═════╝ ╚══════╝╚═╝  ╚═══╝╚═╝╚══════╝\n</pre>\n</div>\n\n<div align=\"center\">\n\n# 🔮 GENIE: [GPT-SoVITS](https://github.com/RVC-Boss/GPT-SoVITS) Lightweight Inference Engine\n\n**Experience near-instantaneous speech synthesis on your CPU**\n\n[简体中文](./README_zh.md) | [English](./README.md)\n\n</div>\n\n---\n\n**GENIE** is a lightweight inference engine built on the open-source TTS\nproject [GPT-SoVITS](https://github.com/RVC-Boss/GPT-SoVITS). It integrates TTS inference, ONNX model conversion, API\nserver, and other core features, aiming to provide ultimate performance and convenience.\n\n* **✅ Supported Model Version:** GPT-SoVITS V2\n* **✅ Supported Language:** Japanese\n* **✅ Supported Python Version:** >= 3.9\n\n---\n\n## 🎬 Demo Video\n\n- **[➡️ Watch the demo video (Chinese)](https://www.bilibili.com/video/BV1d2hHzJEz9)**\n\n---\n\n## 🚀 Performance Advantages\n\nGENIE optimizes the original model for outstanding CPU performance.\n\n| Feature                     |  🔮 GENIE   | Official PyTorch Model | Official ONNX Model |\n|:----------------------------|:-----------:|:----------------------:|:-------------------:|\n| **First Inference Latency** |  **1.13s**  |         1.35s          |        3.57s        |\n| **Runtime Size**            | **\\~200MB** |      \\~several GB      |  Similar to GENIE   |\n| **Model Size**              | **\\~230MB** |    Similar to GENIE    |       \\~750MB       |\n\n> 📝 **Note:** Since GPU inference latency does not significantly improve over CPU for the first packet, we currently\n> only provide a CPU version to ensure the best out-of-the-box experience.\n>\n> 📝 **Latency Test Info:** All latency data is based on a test set of 100 Japanese sentences (\\~20 characters each),\n> averaged. Tested on CPU i7-13620H.\n\n---\n\n## 🏁 QuickStart\n\n> **⚠️ Important:** It is recommended to run GENIE in **Administrator mode** to avoid potential performance degradation.\n\n### 📦 Installation\n\nInstall via pip:\n\n```bash\npip install genie-tts\n```\n\n> 📝 **You may encounter an installation failure when trying to install pyopenjtalk. This is because pyopenjtalk\n> is a library that includes C extensions, and the publisher does not currently provide pre-compiled binary packages (\n> wheels).\n> For Windows users, this requires\ninstalling [Visual Studio Build Tools](https://visualstudio.microsoft.com/visual-cpp-build-tools/). Specifically, you\nmust select the \"Desktop\n> development with C++\" workload during the installation process.**\n\n### ⚡️ Quick Tryout\n\nNo GPT-SoVITS model yet? No problem!\nGENIE includes predefined speaker characters for immediate use without any model\nfiles. Run the code below to hear it in action:\n\n```python\nimport genie_tts as genie\nimport time\n\n# Automatically downloads required files on first run\ngenie.load_predefined_character('misono_mika')\n\ngenie.tts(\n    character_name='misono_mika',\n    text='どうしようかな……やっぱりやりたいかも……！',\n    play=True,  # Play the generated audio directly\n)\n\ntime.sleep(10)  # Add delay to ensure audio playback completes\n```\n\n### 🎤 TTS Best Practices\n\nA simple TTS inference example:\n\n```python\nimport genie_tts as genie\n\n# Step 1: Load character voice model\ngenie.load_character(\n    character_name='<CHARACTER_NAME>',  # Replace with your character name\n    onnx_model_dir=r\"<PATH_TO_CHARACTER_ONNX_MODEL_DIR>\",  # Folder containing ONNX model\n)\n\n# Step 2: Set reference audio (for emotion and intonation cloning)\ngenie.set_reference_audio(\n    character_name='<CHARACTER_NAME>',  # Must match loaded character name\n    audio_path=r\"<PATH_TO_REFERENCE_AUDIO>\",  # Path to reference audio\n    audio_text=\"<REFERENCE_AUDIO_TEXT>\",  # Corresponding text\n)\n\n# Step 3: Run TTS inference and generate audio\ngenie.tts(\n    character_name='<CHARACTER_NAME>',  # Must match loaded character\n    text=\"<TEXT_TO_SYNTHESIZE>\",  # Text to synthesize\n    play=True,  # Play audio directly\n    save_path=\"<OUTPUT_AUDIO_PATH>\",  # Output audio file path\n)\n\nprint(\"🎉 Audio generation complete!\")\n```\n\n---\n\n## 🔧 Model Conversion\n\nTo convert original GPT-SoVITS models for GENIE, ensure `torch` is installed:\n\n```bash\npip install torch\n```\n\nUse the built-in conversion tool:\n\n> **Tip:** `convert_to_onnx` currently supports only V2 models.\n\n```python\nimport genie_tts as genie\n\ngenie.convert_to_onnx(\n    torch_pth_path=r\"<YOUR .PTH MODEL FILE>\",  # Replace with your .pth file\n    torch_ckpt_path=r\"<YOUR .CKPT CHECKPOINT FILE>\",  # Replace with your .ckpt file\n    output_dir=r\"<ONNX MODEL OUTPUT DIRECTORY>\"  # Directory to save ONNX model\n)\n```\n\n---\n\n## 🌐 Launch FastAPI Server\n\nGENIE includes a lightweight FastAPI server:\n\n```python\nimport genie_tts as genie\n\n# Start server\ngenie.start_server(\n    host=\"0.0.0.0\",  # Host address\n    port=8000,  # Port\n    workers=1  # Number of workers\n)\n```\n\n> For request formats and API details, se",
      "default_branch": "master"
    },
    "fetched_at": "2025-09-21T02:27:01.900005"
  },
  {
    "basic_info": {
      "name": "HunyuanImage-2.1",
      "full_name": "Tencent-Hunyuan/HunyuanImage-2.1",
      "owner": "Tencent-Hunyuan",
      "description": "HunyuanImage-2.1: An Efficient Diffusion Model for High-Resolution (2K) Text-to-Image Generation​",
      "url": "https://github.com/Tencent-Hunyuan/HunyuanImage-2.1",
      "clone_url": "https://github.com/Tencent-Hunyuan/HunyuanImage-2.1.git",
      "ssh_url": "git@github.com:Tencent-Hunyuan/HunyuanImage-2.1.git",
      "homepage": "https://hunyuan.tencent.com/image/en?tabIndex=0",
      "created_at": "2025-09-04T06:46:19Z",
      "updated_at": "2025-09-20T12:13:24Z",
      "pushed_at": "2025-09-19T08:51:47Z"
    },
    "stats": {
      "stars": 595,
      "forks": 42,
      "watchers": 595,
      "open_issues": 16,
      "size": 19743
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 245897
      },
      "license": "Other",
      "topics": [
        "aigc",
        "diffusion-models",
        "diffusion-transformer",
        "image-generation",
        "text-to-image"
      ]
    },
    "content": {
      "readme": "\n[中文阅读](./README_CN.md)\n\n<p align=\"center\">\n  <img src=\"./assets/logo.png\"  height=100>\n</p>\n\n<div align=\"center\">\n\n# HunyuanImage-2.1: An Efficient Diffusion Model for High-Resolution (2K) Text-to-Image Generation​\n\n</div>\n\n\n<p align=\"center\"> &nbsp&nbsp🤗 <a href=\"https://huggingface.co/tencent/HunyuanImage-2.1\">HuggingFace</a>&nbsp&nbsp | \n💻 <a href=\"https://hunyuan.tencent.com/modelSquare/home/play?modelId=286&from=/visual\">Official website(官网) Try our model!</a>&nbsp&nbsp\n</p>\n\n\n<!-- <div align=\"center\">\n  <a href=https://github.com/Tencent-Hunyuan/HunyuanImage-2.1 target=\"_blank\"><img src=https://img.shields.io/badge/Code-black.svg?logo=github height=22px></a>\n  <a href=\"https://huggingface.co/spaces/tencent/HunyuanImage-2.1\" target=\"_blank\">\n    <img src=\"https://img.shields.io/badge/Demo%20Page-blue\" height=\"22px\"></a>\n  <a href=https://huggingface.co/tencent/HunyuanImage-2.1 target=\"_blank\"><img src=https://img.shields.io/badge/%F0%9F%A4%97%20Models-d96902.svg height=22px></a>\n  <a href=\"#\" target=\"_blank\"><img src=\"https://img.shields.io/badge/Report-Coming%20Soon-blue\" height=\"22px\"></a><br/>\n  <a href=\"https://www.arxiv.org/abs/2509.04545\" target=\"https://arxiv.org/abs/2509.04545\"><img src=\"https://img.shields.io/badge/PromptEnhancer-Report-yellow\" height=\"22px\"></a>\n  <a href= https://hunyuan-promptenhancer.github.io/ target=\"_blank\"><img src=https://img.shields.io/badge/PromptEnhancer-bb8a2e.svg?logo=github height=22px></a><br/>\n  <a href=https://x.com/TencentHunyuan target=\"_blank\"><img src=https://img.shields.io/badge/Hunyuan-black.svg?logo=x height=22px></a>\n</div> -->\n\n<p align=\"center\">\n    👏 Join our <a href=\"assets/WECHAT.md\" target=\"_blank\">WeChat</a> and <a href=\"https://discord.gg/ehjWMqF5wY\">Discord</a>\n</p>\n\n\n-----\n\nThis repo contains PyTorch model definitions, pretrained weights and inference/sampling code for our HunyuanImage-2.1. You can <span style=\"color:red\">**directly try our model**</span> on [Official website(官网)](https://hunyuan.tencent.com/modelSquare/home/play?modelId=286&from=/visual) and find more visualizations on our [project page](https://hunyuan.tencent.com/image/en?tabIndex=0).\n\n\n<div align=\"center\">\n  <img src=\"./assets/demo.jpg\" width=100% alt=\"HunyuanImage 2.1 Demo\">\n</div>\n\n## 🔥🔥🔥 Latest Updates\n- September 18, 2025: ✨ Try the [PromptEnhancer-32B model](https://huggingface.co/PromptEnhancer/PromptEnhancer-32B) for higher-quality prompt enhancement!​.\n- September 18, 2025: ✨ [ComfyUI workflow of HunyuanImage-2.1](https://github.com/KimbingNg/ComfyUI-HunyuanImage2.1) is available now!\n- September 16, 2025: 👑 We achieved the Top1 on Arena's leaderboard for text-to-image open-source models. [Leaderboard](https://artificialanalysis.ai/text-to-image/arena/leaderboard-text)\n- September 12, 2025: 🚀 Released FP8 quantized models! Making it possible to generate 2K images with only 24GB GPU memory!\n- September 8, 2025: 🚀 Released inference code and model weights for HunyuanImage-2.1.\n\n\n<!-- ## 🎥 Demo\n\n<div align=\"center\">\n  <img src=\"./assets/demo.jpg\" width=100% alt=\"HunyuanImage 2.1 Demo\">\n</div> -->\n\n<!-- ## Contents\n- [HunyuanImage-2.1: An Efficient Diffusion Model for High-Resolution (2K) Text-to-Image Generation​](#hunyuanimage-21-an-efficient-diffusion-model-for-high-resolution-2k-text-to-image-generation)\n  - [🔥🔥🔥 Latest Updates](#-latest-updates)\n  - [🎥 Demo](#-demo)\n  - [Contents](#contents)\n  - [Abstract](#abstract)\n  - [HunyuanImage-2.1 Overall Pipeline](#hunyuanimage-21-overall-pipeline)\n    - [Training Data and Caption](#training-data-and-caption)\n    - [Text-to-Image Model Architecture](#text-to-image-model-architecture)\n    - [Reinforcement Learning from Human Feedback](#reinforcement-learning-from-human-feedback)\n    - [Rewriting Model](#rewriting-model)\n    - [Model distillation](#model-distillation)\n  - [🎉 HunyuanImage-2.1 Key Features](#-hunyuanimage-21-key-features)\n  - [Prompt Enhanced Demo](#prompt-enhanced-demo)\n  - [📈 Comparisons](#-comparisons)\n    - [SSAE Evaluation](#ssae-evaluation)\n    - [GSB Evaluation](#gsb-evaluation)\n  - [📜 System Requirements](#-system-requirements)\n  - [🛠️ Dependencies and Installation](#️-dependencies-and-installation)\n  - [🧱 Download Pretrained Models](#-download-pretrained-models)\n  - [🔑 Usage](#-usage)\n  - [🔗 BibTeX](#-bibtex)\n  - [Acknowledgements](#acknowledgements)\n  - [Github Star History](#github-star-history)\n\n--- -->\n<!-- - [🧩 Community Contributions](#-community-contributions) -->\n## Introduction\nWe are excited to introduce **HunyuanImage-2.1**, a 17B text-to-image model that is capable of generating **2K (2048 × 2048) resolution** images. \n\n<!-- Leveraging an extensive dataset and structured captions involving multiple expert models, we significantly enhance text-image alignment capabilities. The model employs a highly expressive VAE with a (32 × 32) spatial compression ratio, substantially reducing computational costs. -->\n\nOur architecture consists of two stages:\n1. **​Base text-to-image Model**:​​ The f",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-21T02:27:03.167430"
  },
  {
    "basic_info": {
      "name": "youtu-graphrag",
      "full_name": "TencentCloudADP/youtu-graphrag",
      "owner": "TencentCloudADP",
      "description": "Official repository of Youtu-GraphRAG: Vertically Unified Agents for Graph Retrieval-Augmented Complex Reasoning",
      "url": "https://github.com/TencentCloudADP/youtu-graphrag",
      "clone_url": "https://github.com/TencentCloudADP/youtu-graphrag.git",
      "ssh_url": "git@github.com:TencentCloudADP/youtu-graphrag.git",
      "homepage": "https://arxiv.org/abs/2508.19855",
      "created_at": "2025-09-01T02:52:13Z",
      "updated_at": "2025-09-21T01:56:32Z",
      "pushed_at": "2025-09-18T12:57:54Z"
    },
    "stats": {
      "stars": 592,
      "forks": 78,
      "watchers": 592,
      "open_issues": 32,
      "size": 33976
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 321756,
        "HTML": 83435,
        "Shell": 2582,
        "Dockerfile": 856
      },
      "license": "Other",
      "topics": [
        "agent",
        "graph",
        "graphrag",
        "llm",
        "rag"
      ]
    },
    "content": {
      "readme": "<div align=\"center\">\n\n# <img src=\"assets/logo.svg\" alt=\"Youtu-agent Logo\" height=\"26px\"> Youtu-GraphRAG: <br>Vertically Unified Agents for Graph Retrieval-Augmented Complex Reasoning\n\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n[![Paper](https://img.shields.io/badge/Paper-Latest-blue.svg)](https://arxiv.org/abs/2508.19855)\n[![WeChat Community](https://img.shields.io/badge/Community-WeChat-32CD32)](assets/wechat_qr.png)\n[![Discord Community](https://img.shields.io/badge/Community-Discord-8A2BE2)](https://discord.gg/QjqhkHQVVM)\n[![GitHub stars](https://img.shields.io/github/stars/TencentCloudADP/youtu-graphrag?style=social)](https://github.com/TencentCloudADP/youtu-graphrag)\n\n*🚀 Revolutionary framework moving Pareto Frontier with 33.6% lower token cost and 16.62% higher accuracy over SOTA baselines*\n\n[🔖 中文版](README-CN.md) • [⭐ Contributions](#contributions) • [📊 Benchmarks](https://huggingface.co/datasets/Youtu-Graph/AnonyRAG) • [🚀 Getting Started](#quickstart)\n\n</div>\n\n## 🎯 Brief Introduction\n**Youtu-GraphRAG** is a vertically unified agentic paradigm that jointly connects the entire framework as an intricate integration based on graph schema. We allow seamless domain transfer with minimal intervention on the graph schema, providing insights of the next evolutionary GraphRAG paradigm for real-world applications with remarkable adaptability.\n\n<img src=\"assets/logo.png\" alt=\"Youtu-GrapHRAG Logo\" width=\"140\" align=\"left\" style=\"margin-right:20px;\">\n\n\n### 🎨 When and Why to use Youtu-GraphRAG\n\n🔗 Multi-hop Reasoning/Summarization/Conclusion: Complex questions requiring multi-step reasoning<br>\n📚 Knowledge-Intensive Tasks: Questions dependent on large amounts of structured/private/domain knowledge<br>\n🌐 Domain Scalability: Easily support encyclopedias, academic papers, commercial/private knowledge base and other domains with minimal intervention on the schema<br><br>\n\n\n## 🏗️ Framework Architecture\n\n<div align=\"center\">\n<img src=\"assets/framework.png\" alt=\"Youtu-GraphRAG Framework Architecture\" width=\"95%\"/><br>\nA sketched overview of our proposed framework Youtu-GraphRAG.\n</div>\n\n## 📲 Interactive interface\n\n<div align=\"center\">\n\n[//]: # (<img src=\"assets/dashboard_demo.png\" alt=\"Dashboard\" width=\"32%\"/>)\n<img src=\"assets/graph_demo.png\" alt=\"Graph Construction\" width=\"45.9%\"/>\n<img src=\"assets/retrieval_demo.png\" alt=\"Retrieval\" width=\"49.4%\"/>\n</div>\n\n<a id=\"contributions\"></a>\n## 🚀 Contributions and Novelty\n\nBased on our unified agentic paradigm for Graph Retrieval-Augmented Generation (GraphRAG), Youtu-GraphRAG introduces several key innovations that jointly connect the entire framework as an intricate integration:\n\n\n<strong>🏗️ 1. Schema-Guided Hierarchical Knowledge Tree Construction</strong>\n\n- 🌱 **Seed Graph Schema**: Introduces targeted entity types, relations, and attribute types to bound automatic extraction agents\n- 📈 **Scalable Schema Expansion**: Continuously expands schemas for adaptability over unseen domains\n- 🏢 **Four-Level Architecture**: \n  - **Level 1 (Attributes)**: Entity property information\n  - **Level 2 (Relations)**: Entity relationship triples\n  - **Level 3 (Keywords)**: Keyword indexing\n  - **Level 4 (Communities)**: Hierarchical community structure\n- ⚡ **Quick Adaptation to industrial applications**: We allow seamless domain transfer with minimal intervention on the schema\n\n\n<strong>🌳 2. Dually-Perceived Community Detection</strong>\n\n- 🔬 **Novel Community Detection Algorithm**: Fuses structural topology with subgraph semantics for comprehensive knowledge organization\n- 📊 **Hierarchical Knowledge Tree**: Naturally yields a structure supporting both top-down filtering and bottom-up reasoning that performs better than traditional Leiden and Louvain algorithms\n- 📝 **Community Summaries**: LLM-enhanced community summarization for higher-level knowledge abstraction\n\n<div align=\"center\">\n<img src=\"assets/comm.png\" alt=\"Youtu-GraphRAG Community Detection\" width=\"60%\"/>\n</div>\n\n<strong>🤖 3. Agentic Retrieval</strong>\n\n- 🎯 **Schema-Aware Decomposition**: Interprets the same graph schema to transform complex queries into tractable and parallel sub-queries\n- 🔄 **Iterative Reflection**: Performs reflection for more advanced reasoning through IRCoT (Iterative Retrieval Chain of Thought)\n\n<div align=\"center\">\n<img src=\"assets/agent.png\" alt=\"Youtu-GraphRAG Agentic Decomposer\" width=\"50%\"/>\n</div>\n\n<strong>🧠 4. Advanced Construction and Reasoning Capabilities for real-world deployment</strong>\n\n- 🎯 **Performance Enhancement**: Less token costs and higher accuracy with optimized prompting, indexing and retrieval strategies\n- 🤹‍♀️ **User friendly visualization**: In ```output/graphs/```, the four-level knowledge tree supports visualization with neo4j import，making reasoning paths and knowledge organization vividly visable to users\n- ⚡ **Parallel Sub-question Processing**: Concurrent handling of decomposed questions for efficiency and complex ",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-21T02:27:04.451585"
  },
  {
    "basic_info": {
      "name": "FireRedTTS2",
      "full_name": "FireRedTeam/FireRedTTS2",
      "owner": "FireRedTeam",
      "description": "Long-form streaming TTS system for multi-speaker dialogue generation",
      "url": "https://github.com/FireRedTeam/FireRedTTS2",
      "clone_url": "https://github.com/FireRedTeam/FireRedTTS2.git",
      "ssh_url": "git@github.com:FireRedTeam/FireRedTTS2.git",
      "homepage": null,
      "created_at": "2025-09-02T06:25:28Z",
      "updated_at": "2025-09-21T01:42:13Z",
      "pushed_at": "2025-09-17T12:29:15Z"
    },
    "stats": {
      "stars": 590,
      "forks": 56,
      "watchers": 590,
      "open_issues": 13,
      "size": 777
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 124201
      },
      "license": "Apache License 2.0",
      "topics": []
    },
    "content": {
      "readme": "<div align=\"center\">\n    <h1>\n    FireRedTTS-2\n    </h1>\n    <p>\n    Official PyTorch code for <br>\n    <b><em>FireRedTTS-2: Towards Long Conversational Speech Generation for Podcast and Chatbot</em></b>\n    </p>\n    <p>\n    <!-- <img src=\"assets/XiaoHongShu_Logo.png\" alt=\"Institution 4\" style=\"width: 102px; height: 48px;\"> -->\n    <img src=\"assets/FireRedTTS_Logo.png\" alt=\"FireRedTTS_Logo\" style=\"width: 248px; height: 68px;\">\n    </p>\n    <p>\n    </p>\n    <a href=\"https://arxiv.org/abs/2509.02020\"><img src=\"https://img.shields.io/badge/Paper-ArXiv-red\" alt=\"technical report\"></a>\n    <a href=\"https://fireredteam.github.io/demos/firered_tts_2/\"><img src=\"https://img.shields.io/badge/Demo-Page-lightgrey\" alt=\"version\"></a>\n    <a href=\"https://huggingface.co/FireRedTeam/FireRedTTS2\"><img src=\"https://img.shields.io/badge/Hugging%20Face-Model%20Page-yellow\" alt=\"HF-model\"></a>\n    <a href=\"https://github.com/FireRedTeam/FireRedTTS\"><img src=\"https://img.shields.io/badge/License-Apache%202.0-blue.svg\" alt=\"Apache-2.0\"></a>\n</div>\n\n## Overview\n\nFireRedTTS‑2 is a long-form streaming TTS system for **multi-speaker dialogue generation**, delivering stable, natural speech with reliable speaker switching and context-aware prosody.\n\n## Highlight🔥\n\n- **Long Conversational Speech Generation**: It currently supports 3 minutes dialogues with 4 speakers and can be easily scaled to longer conversations\nwith more speakers by extending training corpus.\n- **Multilingual Support**: It supports multiple languages including English, Chinese, Japanese, Korean, French, German, and Russian. Support zero-shot voice cloning for cross-lingual and code-switching scenarios.\n- **Ultra-Low Latency**: Building on the new **12.5Hz streaming** speech tokenizer, we employ a dual-transformer architecture that operates on a text–speech interleaved sequence, enabling flexible sentence-bysentence generation and reducing first-packet latency，Specifically, on an L20 GPU, our first-packet latency as low as 140ms while maintaining high-quality audio output.\n- **Strong Stability**：Our model achieves high similarity and low WER/CER in both monologue and dialogue tests.\n- **Random Timbre Generation**:Useful for creating ASR/speech interaction data.\n\n## Demo Examples\n\n**Random Timbre Generation & Multilingual Support**\n<div align=\"center\">\n\n<https://github.com/user-attachments/assets/804e9e67-fb15-4557-9715-43cd46a1b3e8>\n\n</div>\n\n**Zero-Shot Podcast Generation**\n<div align=\"center\">\n\n<https://github.com/user-attachments/assets/e68b1b7e-1329-47bb-a16f-8589cf227579>\n\n</div>\n\n**Speaker-Specific Finetuned Podcast Generation**\n\n⚠️ Speaker voices: hosts \"肥杰\" and \"惠子\" from the podcast \"肥话连篇\". Use without authorization is forbidden.\n\n⚠️ 声音来源：播客 \"肥话连篇\" 主播 \"肥杰\" 和 \"惠子\"，未经授权不能使用。\n<div align=\"center\">\n\n<https://github.com/user-attachments/assets/21f626cb-eaf4-4f5c-920c-3d5d4c8cfa8b>\n\n</div>\n\nFor more examples, see [demo page](https://fireredteam.github.io/demos/firered_tts_2/).\n\n## News\n\n- [2025/09/12] 🔥 **We have added a UI tool to the dialogue generation.**\n- [2025/09/08] 🔥 We release the [pre-trained checkpoints](https://huggingface.co/FireRedTeam/FireRedTTS2) and inference code.\n- [2025/09/02] 🔥 We release the [technical report](https://arxiv.org/abs/2509.02020) and [demo page](https://fireredteam.github.io/demos/firered_tts_2/)\n\n## Roadmap\n\n- [x] 2025/09\n  - [x] Release the pre-trained checkpoints and inference code.\n  - [x] Add web UI tool.\n\n- [ ] 2025/10\n  - [ ] Release a base model with enhanced multilingual support.\n  - [ ] **Provide fine-tuning code & tutorial for specific dialogue/multilingual data.**\n  - [ ] **End-to-end text-to-blog pipeline.**\n\n## Install & Model Download\n\n### Clone and install\n\n- **Clone the repo**\n\n    ``` sh\n    git clone https://github.com/FireRedTeam/FireRedTTS2.git\n    cd FireRedTTS2\n    ```\n\n- **Create Conda env**:\n\n    ``` sh\n    conda create --name fireredtts2 python==3.11\n    conda activate fireredtts2\n\n    # Step 1. PyTorch Installation (if required)\n    pip install torch==2.7.1 torchvision==0.22.1 torchaudio==2.7.1 --index-url https://download.pytorch.org/whl/cu126\n\n    # Step 2. Install Dependencies\n    pip install -e .\n    pip install -r requirements.txt\n    ```\n\n- **Model download**\n\n    ```sh\n    git lfs install\n    git clone https://huggingface.co/FireRedTeam/FireRedTTS2 pretrained_models/FireRedTTS2\n    ```\n\n## Basic Usage\n\n**Dialogue Generation with Web UI**\n\nGenerate dialogue through an easy-to-use web interface that supports both voice cloning and randomized voices.\n\n```sh\npython gradio_demo.py --pretrained-dir \"./pretrained_models/FireRedTTS2\"\n```\n\n<div align=\"center\">\n\n<p>\n<img src=\"assets/gradio.png\" alt=\"FireRedTTS_Logo\" style=\"width: 997px; height: 515px;\">\n</p>\n\n</div>\n\n**Dialogue Generation**\n\n```python\nimport os\nimport sys\nimport torch\nimport torchaudio\nfrom fireredtts2.fireredtts2 import FireRedTTS2\n\ndevice = \"cuda\"\n\nfireredtts2 = FireRedTTS2(\n    pretrained_dir=\"./pretrained_models/FireRedTTS2\",\n    gen_ty",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-21T02:27:05.704384"
  },
  {
    "basic_info": {
      "name": "HuMo",
      "full_name": "Phantom-video/HuMo",
      "owner": "Phantom-video",
      "description": "HuMo: Human-Centric Video Generation via Collaborative Multi-Modal Conditioning",
      "url": "https://github.com/Phantom-video/HuMo",
      "clone_url": "https://github.com/Phantom-video/HuMo.git",
      "ssh_url": "git@github.com:Phantom-video/HuMo.git",
      "homepage": "https://phantom-video.github.io/HuMo/",
      "created_at": "2025-09-09T06:18:13Z",
      "updated_at": "2025-09-20T17:43:54Z",
      "pushed_at": "2025-09-17T05:44:15Z"
    },
    "stats": {
      "stars": 570,
      "forks": 46,
      "watchers": 570,
      "open_issues": 11,
      "size": 101087
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 315221,
        "Shell": 3520
      },
      "license": "Apache License 2.0",
      "topics": []
    },
    "content": {
      "readme": "<div align=\"center\">\n<h1> HuMo: Human-Centric Video Generation via Collaborative Multi-Modal Conditioning </h1>\n\n<a href=\"https://arxiv.org/abs/2509.08519\"><img src=\"https://img.shields.io/badge/arXiv%20paper-2509.08519-b31b1b.svg\"></a>\n<a href=\"https://phantom-video.github.io/HuMo/\"><img src=\"https://img.shields.io/badge/Project_page-More_visualizations-green\"></a>\n<a href=\"https://huggingface.co/bytedance-research/HuMo\"><img src=\"https://img.shields.io/static/v1?label=%F0%9F%A4%97%20Hugging%20Face&message=Model&color=orange\"></a>\n\n[Liyang Chen](https://scholar.google.com/citations?user=jk6jWXgAAAAJ&hl)<sup> * </sup>, [Tianxiang Ma](https://tianxiangma.github.io/)<sup> * </sup>, [Jiawei Liu](https://scholar.google.com/citations?user=X21Fz-EAAAAJ), [Bingchuan Li](https://scholar.google.com/citations?user=ac5Se6QAAAAJ)<sup> &dagger; </sup>, <br>[Zhuowei Chen](https://scholar.google.com/citations?user=ow1jGJkAAAAJ), [Lijie Liu](https://liulj13.github.io/), [Xu He](https://scholar.google.com/citations?user=KMrFk2MAAAAJ&hl), [Gen Li](https://scholar.google.com/citations?user=wqA7EIoAAAAJ), [Qian He](https://scholar.google.com/citations?user=9rWWCgUAAAAJ), [Zhiyong Wu](https://scholar.google.com/citations?user=7Xl6KdkAAAAJ)<sup> § </sup><br>\n<sup> * </sup>Equal contribution, <sup> &dagger; </sup>Project lead, <sup> § </sup>Corresponding author  \nTsinghua University | Intelligent Creation Team, ByteDance\n\n</div>\n\n<p align=\"center\">\n<img src=\"assets/teaser.png\" width=95%>\n<p>\n\n## 🔥 Latest News\n\n* A Best-Practice Guide for HuMo will be released soon. Stay tuned.\n* Sep 17, 2025: 🔥🔥 [ComfyUI](https://github.com/comfyanonymous/ComfyUI/pull/9903) officially supports HuMo-1.7B!\n* Sep 16, 2025: 🔥🔥 We release the [1.7B weights](https://huggingface.co/bytedance-research/HuMo/tree/main/HuMo-1.7B), which generate a 480P video in 8 minutes on a 32G GPU. The visual quality is lower than that of the 17B model, but the audio-visual sync remains nearly unaffected.\n* Sep 13, 2025: 🔥🔥 The 17B model is merged into [ComfyUI-Wan](https://github.com/kijai/ComfyUI-WanVideoWrapper), which can be run on a NVIDIA 3090 GPU. Thank [kijai](https://github.com/kijai) for the update!\n* Sep 10, 2025: 🔥🔥 We release the [17B weights](https://huggingface.co/bytedance-research/HuMo/tree/main/HuMo-17B) and inference codes.\n* Sep 9, 2025: We release the [Project Page](https://phantom-video.github.io/HuMo/) and [Technique Report](https://arxiv.org/abs/2509.08519/) of **HuMo**.\n\n\n## ✨ Key Features\nHuMo is a unified, human-centric video generation framework designed to produce high-quality, fine-grained, and controllable human videos from multimodal inputs—including text, images, and audio. It supports strong text prompt following, consistent subject preservation, synchronized audio-driven motion.\n\n> - **​​VideoGen from Text-Image**​​ - Customize character appearance, clothing, makeup, props, and scenes using text prompts combined with reference images.\n> - **​​VideoGen from Text-Audio**​​ - Generate audio-synchronized videos solely from text and audio inputs, removing the need for image references and enabling greater creative freedom.\n> - **​​VideoGen from Text-Image-Audio**​​ - Achieve the higher level of customization and control by combining text, image, and audio guidance.\n\n## 📑 Todo List\n- [x] Release Paper\n- [x] Checkpoint of HuMo-17B\n- [x] Checkpoint of HuMo-1.7B\n- [x] Inference Codes\n  - [ ] Text-Image Input\n  - [x] Text-Audio Input\n  - [x] Text-Image-Audio Input\n- [x] Multi-GPU Inference\n- [ ] Best-Practice Guide of HuMo for Movie-Level Generation\n- [ ] Checkpoint for Longer Generation\n- [ ] Prompts to Generate Demo of ***Faceless Thrones***\n- [ ] Training Data\n\n## ⚡️ Quickstart\n\n### Installation\n```\nconda create -n humo python=3.11\nconda activate humo\npip install torch==2.5.1 torchvision==0.20.1 torchaudio==2.5.1 --index-url https://download.pytorch.org/whl/cu124\npip install flash_attn==2.6.3\npip install -r requirements.txt\nconda install -c conda-forge ffmpeg\n```\n\n### Model Preparation\n| Models       | Download Link                                                                                                                                           |    Notes                      |\n|--------------|---------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------|\n| HuMo-17B      | 🤗 [Huggingface](https://huggingface.co/bytedance-research/HuMo/tree/main/HuMo-17B)   | Supports 480P & 720P \n| HuMo-1.7B | 🤗 [Huggingface](https://huggingface.co/bytedance-research/HuMo/tree/main/HuMo-1.7B) | Lightweight on 32G GPU\n| HuMo-Longer | 🤗 [Huggingface](https://huggingface.co/bytedance-research/HuMo) | Longer generation to be released in Oct.\n| Wan-2.1 | 🤗 [Huggingface](https://huggingface.co/Wan-AI/Wan2.1-T2V-1.3B) | VAE & Text encoder\n| Whisper-large-v3 |      🤗 [Huggingface](https://huggingface.co/openai/whisper-large-v3)          | ",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-21T02:27:06.957301"
  },
  {
    "basic_info": {
      "name": "DeepMCPAgent",
      "full_name": "cryxnet/DeepMCPAgent",
      "owner": "cryxnet",
      "description": "Model-agnostic plug-n-play LangChain/LangGraph agents powered entirely by MCP tools over HTTP/SSE.",
      "url": "https://github.com/cryxnet/DeepMCPAgent",
      "clone_url": "https://github.com/cryxnet/DeepMCPAgent.git",
      "ssh_url": "git@github.com:cryxnet/DeepMCPAgent.git",
      "homepage": "https://cryxnet.github.io/DeepMCPAgent/",
      "created_at": "2025-08-29T13:49:43Z",
      "updated_at": "2025-09-20T23:58:32Z",
      "pushed_at": "2025-09-08T18:15:58Z"
    },
    "stats": {
      "stars": 557,
      "forks": 81,
      "watchers": 557,
      "open_issues": 0,
      "size": 179
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 29117
      },
      "license": "Apache License 2.0",
      "topics": [
        "agent-framework",
        "agentic-ai",
        "agents",
        "ai",
        "ai-agents",
        "ai-framework",
        "artificial-intelligence",
        "autonomous-agents",
        "deep-agents",
        "developer-tools",
        "langchain",
        "langgraph",
        "llm-agents",
        "mcp",
        "opensource-agents",
        "python",
        "react-agents"
      ]
    },
    "content": {
      "readme": "<!-- Banner / Title -->\n<div align=\"center\">\n  <img src=\"docs/images/icon.png\" width=\"120\" alt=\"DeepMCPAgent Logo\"/>\n\n  <h1>🤖 DeepMCPAgent</h1>\n  <p><strong>Model-agnostic LangChain/LangGraph agents powered entirely by <a href=\"https://modelcontextprotocol.io/\">MCP</a> tools over HTTP/SSE.</strong></p>\n\n  <!-- Badges -->\n  <p>\n    <a href=\"https://cryxnet.github.io/DeepMCPAgent\">\n      <img alt=\"Docs\" src=\"https://img.shields.io/badge/docs-latest-brightgreen.svg\">\n    </a>\n    <a href=\"#\"><img alt=\"Python\" src=\"https://img.shields.io/badge/Python-3.10%2B-blue.svg\"></a>\n    <a href=\"#\"><img alt=\"License\" src=\"https://img.shields.io/badge/License-Apache%202.0-blue.svg\"></a>\n    <a href=\"#\"><img alt=\"Status\" src=\"https://img.shields.io/badge/status-beta-orange.svg\"></a>\n\n<p>\n  <a href=\"https://www.producthunt.com/products/deep-mcp-agents?utm_source=badge-featured&utm_medium=badge&utm_source=badge-deep-mcp-agents\" target=\"_blank\">\n    <img src=\"https://api.producthunt.com/widgets/embed-image/v1/featured.svg?post_id=1011071&theme=light\" alt=\"Deep MCP Agents on Product Hunt\" style=\"width: 250px; height: 54px;\" width=\"250\" height=\"54\" />\n  </a>\n</p> \n  </p>\n\n  <p>\n    <em>Discover MCP tools dynamically. Bring your own LangChain model. Build production-ready agents—fast.</em>\n  </p>\n\n  <p>\n    📚 <a href=\"https://cryxnet.github.io/deepmcpagent/\">Documentation</a> • 🛠 <a href=\"https://github.com/cryxnet/deepmcpagent/issues\">Issues</a>\n  </p>\n</div>\n\n<hr/>\n\n## ✨ Why DeepMCPAgent?\n\n- 🔌 **Zero manual tool wiring** — tools are discovered dynamically from MCP servers (HTTP/SSE)\n- 🌐 **External APIs welcome** — connect to remote MCP servers (with headers/auth)\n- 🧠 **Model-agnostic** — pass any LangChain chat model instance (OpenAI, Anthropic, Ollama, Groq, local, …)\n- ⚡ **DeepAgents (optional)** — if installed, you get a deep agent loop; otherwise robust LangGraph ReAct fallback\n- 🛠️ **Typed tool args** — JSON-Schema → Pydantic → LangChain `BaseTool` (typed, validated calls)\n- 🧪 **Quality bar** — mypy (strict), ruff, pytest, GitHub Actions, docs\n\n> **MCP first.** Agents shouldn’t hardcode tools — they should **discover** and **call** them. DeepMCPAgent builds that bridge.\n\n---\n\n## 🚀 Installation\n\nInstall from [PyPI](https://pypi.org/project/deepmcpagent/):\n\n```bash\npip install \"deepmcpagent[deep]\"\n```\n\nThis installs DeepMCPAgent with **DeepAgents support (recommended)** for the best agent loop.\nOther optional extras:\n\n- `dev` → linting, typing, tests\n- `docs` → MkDocs + Material + mkdocstrings\n- `examples` → dependencies used by bundled examples\n\n```bash\n# install with deepagents + dev tooling\npip install \"deepmcpagent[deep,dev]\"\n```\n\n⚠️ If you’re using **zsh**, remember to quote extras:\n\n```bash\npip install \"deepmcpagent[deep,dev]\"\n```\n\n---\n\n## 🚀 Quickstart\n\n### 1) Start a sample MCP server (HTTP)\n\n```bash\npython examples/servers/math_server.py\n```\n\nThis serves an MCP endpoint at: **[http://127.0.0.1:8000/mcp](http://127.0.0.1:8000/mcp)**\n\n### 2) Run the example agent (with fancy console output)\n\n```bash\npython examples/use_agent.py\n```\n\n**What you’ll see:**\n\n![screenshot](/docs/images/screenshot_output.png)\n\n---\n\n## 🧑‍💻 Bring-Your-Own Model (BYOM)\n\nDeepMCPAgent lets you pass **any LangChain chat model instance** (or a provider id string if you prefer `init_chat_model`):\n\n```python\nimport asyncio\nfrom deepmcpagent import HTTPServerSpec, build_deep_agent\n\n# choose your model:\n# from langchain_openai import ChatOpenAI\n# model = ChatOpenAI(model=\"gpt-4.1\")\n\n# from langchain_anthropic import ChatAnthropic\n# model = ChatAnthropic(model=\"claude-3-5-sonnet-latest\")\n\n# from langchain_community.chat_models import ChatOllama\n# model = ChatOllama(model=\"llama3.1\")\n\nasync def main():\n    servers = {\n        \"math\": HTTPServerSpec(\n            url=\"http://127.0.0.1:8000/mcp\",\n            transport=\"http\",    # or \"sse\"\n            # headers={\"Authorization\": \"Bearer <token>\"},\n        ),\n    }\n\n    graph, _ = await build_deep_agent(\n        servers=servers,\n        model=model,\n        instructions=\"Use MCP tools precisely.\"\n    )\n\n    out = await graph.ainvoke({\"messages\":[{\"role\":\"user\",\"content\":\"add 21 and 21 with tools\"}]})\n    print(out)\n\nasyncio.run(main())\n```\n\n> Tip: If you pass a **string** like `\"openai:gpt-4.1\"`, we’ll call LangChain’s `init_chat_model()` for you (and it will read env vars like `OPENAI_API_KEY`). Passing a **model instance** gives you full control.\n\n---\n\n## 🖥️ CLI (no Python required)\n\n```bash\n# list tools from one or more HTTP servers\ndeepmcpagent list-tools \\\n  --http name=math url=http://127.0.0.1:8000/mcp transport=http \\\n  --model-id \"openai:gpt-4.1\"\n\n# interactive agent chat (HTTP/SSE servers only)\ndeepmcpagent run \\\n  --http name=math url=http://127.0.0.1:8000/mcp transport=http \\\n  --model-id \"openai:gpt-4.1\"\n```\n\n> The CLI accepts **repeated** `--http` blocks; add `header.X=Y` pairs for auth:\n>\n> ```\n> --http name=ext url=https://api.example.com/mcp transport=http header.Authorization=\"Bearer TOKEN",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-21T02:27:08.260841"
  },
  {
    "basic_info": {
      "name": "Hunyuan-MT",
      "full_name": "Tencent-Hunyuan/Hunyuan-MT",
      "owner": "Tencent-Hunyuan",
      "description": null,
      "url": "https://github.com/Tencent-Hunyuan/Hunyuan-MT",
      "clone_url": "https://github.com/Tencent-Hunyuan/Hunyuan-MT.git",
      "ssh_url": "git@github.com:Tencent-Hunyuan/Hunyuan-MT.git",
      "homepage": null,
      "created_at": "2025-08-28T09:54:40Z",
      "updated_at": "2025-09-20T19:02:02Z",
      "pushed_at": "2025-09-10T01:27:59Z"
    },
    "stats": {
      "stars": 543,
      "forks": 45,
      "watchers": 543,
      "open_issues": 18,
      "size": 4011
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 17607,
        "Shell": 4928
      },
      "license": "Other",
      "topics": []
    },
    "content": {
      "readme": "\n<p align=\"left\">\n    <a href=\"README_CN.md\">中文</a>&nbsp ｜ English</a>\n</p>\n<br><br>\n\n<p align=\"center\">\n <img src=\"https://dscache.tencent-cloud.cn/upload/uploader/hunyuan-64b418fd052c033b228e04bc77bbc4b54fd7f5bc.png\" width=\"400\"/> <br>\n</p><p></p>\n\n\n<p align=\"center\">\n    🤗&nbsp;<a href=\"https://huggingface.co/collections/tencent/hunyuan-mt-68b42f76d473f82798882597\"><b>Hugging Face</b></a>&nbsp;&nbsp;|&nbsp;&nbsp;\n    <img src=\"https://avatars.githubusercontent.com/u/109945100?s=200&v=4\" width=\"16\"/>&nbsp;<a href=\"https://modelscope.cn/collections/Hunyuan-MT-2ca6b8e1b4934f\"><b>ModelScope</b></a>&nbsp;&nbsp;|&nbsp;&nbsp;\n</p>\n\n<p align=\"center\">\n    🖥️&nbsp;<a href=\"https://hunyuan.tencent.com\" style=\"color: red;\"><b>Official Website</b></a>&nbsp;&nbsp;|&nbsp;&nbsp;\n    🕹️&nbsp;<a href=\"https://hunyuan.tencent.com/chat/HunyuanDefault?from=modelSquare&modelId=hunyuan-mt-7b\"><b>Demo</b></a>&nbsp;&nbsp;&nbsp;&nbsp;\n</p>\n\n<p align=\"center\">\n    <a href=\"https://github.com/Tencent-Hunyuan/Hunyuan-MT\"><b>GITHUB</b></a>&nbsp;&nbsp;|&nbsp;&nbsp;\n    <a href=\"https://www.arxiv.org/pdf/2509.05209\"><b>Technical Report</b> </a>\n</p>\n\n\n## Model Introduction\n\nThe Hunyuan-MT comprises a translation model, Hunyuan-MT-7B, and an ensemble model, Hunyuan-MT-Chimera. The translation model is used to translate source text into the target language, while the ensemble model integrates multiple translation outputs to produce a higher-quality result. It primarily supports mutual translation among 33 languages, including five ethnic minority languages in China.\n\n## Key Features and Advantages\n\n- In the WMT25 competition, the model achieved first place in 30 out of the 31 language categories it participated in.\n- Hunyuan-MT-7B achieves industry-leading performance among models of comparable scale\n- Hunyuan-MT-Chimera-7B is the industry’s first open-source translation ensemble model, elevating translation quality to a new level\n- A comprehensive training framework for translation models has been proposed, spanning from pretrain → continue pretraining (CPT) → supervised fine-tuning (SFT) → translation rl → ensemble rl, achieving state-of-the-art (SOTA) results for models of similar size\n\n## Related News\n* 2025.9.1 We have open-sourced  **Hunyuan-MT-7B** , **Hunyuan-MT-Chimera-7B** on Hugging Face.\n<br>\n\n\n## Performance\n\n<div align='center'>\n<img src=\"imgs/overall_performance.png\" width = \"80%\" />\n</div>\nYou can refer to our technical report for more experimental results and analysis.\n\n<a href=\"https://www.arxiv.org/pdf/2509.05209\"><b>Technical Report</b> </a>\n\n&nbsp;\n\n## Model Links\n| Model Name  | Description | Download |\n| ----------- | ----------- |-----------\n| Hunyuan-MT-7B  | Hunyuan 7B translation model |🤗 [Model](https://huggingface.co/tencent/Hunyuan-MT-7B)|\n| Hunyuan-MT-7B-fp8 | Hunyuan 7B translation model，fp8 quant    | 🤗 [Model](https://huggingface.co/tencent/Hunyuan-MT-7B-fp8)|\n| Hunyuan-MT-Chimera | Hunyuan 7B translation ensemble model    | 🤗 [Model](https://huggingface.co/tencent/Hunyuan-MT-Chimera-7B)|\n| Hunyuan-MT-Chimera-fp8 | Hunyuan 7B translation ensemble model，fp8 quant     | 🤗 [Model](https://huggingface.co/tencent/Hunyuan-MT-Chimera-7B-fp8)|\n\n## Prompts\n\n### Prompt Template for ZH<=>XX Translation.\n```\n把下面的文本翻译成<target_language>，不要额外解释。\n\n<source_text>\n```\n\n\n### Prompt Template for XX<=>XX Translation, excluding ZH<=>XX.\n```\nTranslate the following segment into <target_language>, without additional explanation.\n\n<source_text>\n```\n\n### Prompt Template for Hunyuan-MT-Chimera-7B\n\n```\nAnalyze the following multiple <target_language> translations of the <source_language> segment surrounded in triple backticks and generate a single refined <target_language> translation. Only output the refined translation, do not explain.\n\nThe <source_language> segment:\n```<source_text>```\n\nThe multiple `<target_language>` translations:\n1. ```<translated_text1>```\n2. ```<translated_text2>```\n3. ```<translated_text3>```\n4. ```<translated_text4>```\n5. ```<translated_text5>```\n6. ```<translated_text6>```\n```\n\n&nbsp;\n\n### Use with transformers\nFirst, please install transformers, recommends v4.56.0\n```SHELL\npip install transformers==4.56.0\n```\n\n*!!! If you want to load fp8 model with transformers, you need to change the name\"ignored_layers\" in config.json to \"ignore\" and upgrade the compressed-tensors to compressed-tensors-0.11.0.*\n\nThe following code snippet shows how to use the transformers library to load and apply the model.\n\nwe use tencent/Hunyuan-MT-7B for example\n\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nimport os\n\nmodel_name_or_path = \"tencent/Hunyuan-MT-7B\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\nmodel = AutoModelForCausalLM.from_pretrained(model_name_or_path, device_map=\"auto\")  # You may want to use bfloat16 and/or move to GPU here\nmessages = [\n    {\"role\": \"user\", \"content\": \"Translate the following segment into Chinese, without additional explanation.\\n\\nIt’s on the house.",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-21T02:27:09.534915"
  }
]