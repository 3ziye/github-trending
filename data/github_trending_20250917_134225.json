[
  {
    "basic_info": {
      "name": "GuitarPedal",
      "full_name": "torvalds/GuitarPedal",
      "owner": "torvalds",
      "description": null,
      "url": "https://github.com/torvalds/GuitarPedal",
      "clone_url": "https://github.com/torvalds/GuitarPedal.git",
      "ssh_url": "git@github.com:torvalds/GuitarPedal.git",
      "homepage": null,
      "created_at": "2025-09-17T01:01:29Z",
      "updated_at": "2025-09-17T13:39:42Z",
      "pushed_at": "2025-09-17T01:02:41Z"
    },
    "stats": {
      "stars": 229,
      "forks": 4,
      "watchers": 229,
      "open_issues": 5,
      "size": 322
    },
    "tech_info": {
      "language": null,
      "languages": {},
      "license": null,
      "topics": []
    },
    "content": {
      "readme": "## Random guitar pedal board design\n\n### Background\n\nThis is a personal toy project that has gone through several phases, but\nthe common theme has been that it makes absolutely no sense outside of\nthe very specific niche of \"Linus is trying to learn random things about\nelectronics\".\n\nSo keep that in mind: there is very little point to any of this to\nanybody else.  Don't expect some great useful guitar pedal experience.\n\nI call it my \"LEGO for adults\" hobby, because this got started when I\nwanted to extend my traditional after-Christmas activity (which was\nreceiving and building _actual_ LEGO kits, which has been a thing for me\nsince I was a wee tyke) with something else.\n\nSo for Christmas 2024, I got a new soldering iron and randomly started\ndoing guitar pedal kits.  And so over the next month or two, I built at\nleast two dozen kits, and had to literally look for victims to give them\naway to because I had no use for them myself.\n\n> [!NOTE]\n> Of all the kits I built, the ones I enjoyed the most were the Aion FX\n> ones, and if you are looking for a kit build of traditional analog\n> guitar pedals, I can heartily recommend them.\n>\n> The documentation, the customer service, the components, and the\n> enclosures were all top notch. See [\"Aion FX\"](https://aionfx.com/)\n\nAnyway, after building a lot of these traditional analog guitar pedal\nkits I decided I really wanted to actually understand what they did,\nbecause I really had very little experience with any analog circuits.\n\nWhile I've done some very limited electronics most of my life, almost\nall of it has been related to computers, so it's been either digital\nlogic or power supplies for them.\n\nAlso, I was looking for a different kind of soldering experience where\nthere was less snipping of legs of through-hole components.  I actually\nlike soldering SMT components, but that doesn't tend to be what those\nguitar pedal kits do.\n\nI had done some very limited PCB design with kicad a few years ago, so I\ndecided to just start learning more about analog circuits.  And then it\nkind of grew from that.\n\n### Electrical design\n\nThis is the \"fourth generation\" of my guitar pedal design journey, and\nis a new repository because the goal of the learning experience has\nevolved.\n\nWhat started out being about the analog circuits (and the power rails:\nthose were always a big thing) got to the point where I realized I\nreally want to do a mixed signal design: understanding what the circuits\ndo is one thing, re-creating some analog design from the 70s when you\ndon't actually care about the sound is another thing entirely.\n\nAlso, on the actual analog signal side, I started out using op-amps, but\nas I was attempting to learn how things actually worked, I had switched\nover to a \"discrete components only\" model, and this continues that\ntrend (except for the whole digital side, of course).\n\n> [!NOTE]\n> To me \"discrete components\" does include more optimized packages:\n> things like dual diodes or matched transistors, but not more complex\n> circuits like a op-amp (or a 555 timer or D Flip-flop or other classic\n> logic IC)\n\nAlso, because I don't typically *listen* to the end result, but look at\nit with a signal generator and an oscilloscope, I've grown to detest\npower supply noise.\n\nNot knowing what I was doing, quite a lot of my circuits have been very\nnoisy indeed, and have coupled in noise from the power supply into the\nsignal chain, and you can really see that on an oscilloscope even when\nit's not always audible.\n\nEven in op-amp designs, where the op-amp itself has a very high PSRR and\nisn't mixing power supply noise into the signal, my biasing circuits\nwere often not great, and so the op-amp would see not just the signal\nbut the power supply noise coming in through the DC biasing.\n\nAnd every time I tried a dual power rail (so that I could just keep the\nsignal ground-referenced), the noise from the switching ended up just\nalways noticeable, and the extra complexity was annoying when a lot of\neffects then didn't have any real use for the dual rail.\n\nFiltering obviously helps, but this is just a long-winded explanation\nfor why I ended up really appreciating the \"bias to ground\" JFET model\nfor the signal input side, and the common drain follower in particular.\n\nThat works with a single JFET (the MMBF5103 worked well for me), but my\nfavorite design so far is a dual-JFET LS844 with the second matched JFET\nused as a current sink.  It has basically infinite input impedance (and\ncould be DC coupled, although I do the coupling capacitor with resistor\nto ground) and gives a good output signal somewhere roughly in the\nmiddle of the single-supply 9V rail.\n\nSee [https://www.linearsystems.com/_files/ugd/7e8069_52b1022fbded45fab609459acb337629.pdf](LS844 Application note)\n\nWhy do I mention this in particular? Mainly because it's a great example\nof how completely *insane* my designs are.  That LS844 is used as a\nvoltage follower with a noticeable DC offset, and that single dual-JFET\nSOT-23-6 component is m",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-17T13:42:26.356566"
  },
  {
    "basic_info": {
      "name": "Asus-ROG-Aml-Deep-Dive",
      "full_name": "Zephkek/Asus-ROG-Aml-Deep-Dive",
      "owner": "Zephkek",
      "description": "A deep dive into the ACPI.sys DPC latency problems on Asus ROG laptops",
      "url": "https://github.com/Zephkek/Asus-ROG-Aml-Deep-Dive",
      "clone_url": "https://github.com/Zephkek/Asus-ROG-Aml-Deep-Dive.git",
      "ssh_url": "git@github.com:Zephkek/Asus-ROG-Aml-Deep-Dive.git",
      "homepage": null,
      "created_at": "2025-09-16T22:03:00Z",
      "updated_at": "2025-09-17T13:24:43Z",
      "pushed_at": "2025-09-17T11:22:15Z"
    },
    "stats": {
      "stars": 214,
      "forks": 6,
      "watchers": 214,
      "open_issues": 3,
      "size": 1456
    },
    "tech_info": {
      "language": "ASL",
      "languages": {
        "ASL": 8072935
      },
      "license": null,
      "topics": []
    },
    "content": {
      "readme": "# The ASUS Gaming Laptop ACPI Firmware Bug: A Deep Technical Investigation\r\n\r\n## If You're Here, You Know The Pain\r\n\r\nYou own a high-end ASUS ROG laptop perhaps a Strix, Scar, or Zephyrus. It's specifications are impressive: an RTX 30/40 series GPU, a top-tier Intel processor, and plenty of RAM. Yet, it stutters during basic tasks like watching a YouTube video, audio crackles and pops on Discord calls, the mouse cursor freezes for a split second, just long enough to be infuriating.\r\n\r\nYou've likely tried all the conventional fixes:\r\n- Updating every driver imaginable, multiple times.\r\n- Performing a \"clean\" reinstallation of Windows.\r\n- Disabling every conceivable power-saving option.\r\n- Manually tweaking processor interrupt affinities.\r\n- Following convoluted multi-step guides from Reddit threads.\r\n- Even installing Linux, only to find the problem persists.\r\n\r\nIf none of that worked, it's because the issue isn't with the operating system or a driver. The problem is far deeper, embedded in the machine's firmware, the BIOS.\r\n\r\n## Initial Symptoms and Measurement\r\n\r\n### The Pattern Emerges\r\n\r\nThe first tool in any performance investigator's toolkit for these symptoms is LatencyMon. It acts as a canary in the coal mine for system-wide latency issues. On an affected ASUS Zephyrus M16, the results are immediate and damning:\r\n\r\n```\r\nCONCLUSION\r\nYour system appears to be having trouble handling real-time audio and other tasks. \r\nYou are likely to experience buffer underruns appearing as drop outs, clicks or pops.\r\n\r\nHIGHEST MEASURED INTERRUPT TO PROCESS LATENCY\r\nHighest measured interrupt to process latency (Œºs):   65,816.60\r\nAverage measured interrupt to process latency (Œºs):   23.29\r\n\r\nHIGHEST REPORTED ISR ROUTINE EXECUTION TIME\r\nHighest ISR routine execution time (Œºs):              536.80\r\nDriver with highest ISR routine execution time:       ACPI.sys\r\n\r\nHIGHEST REPORTED DPC ROUTINE EXECUTION TIME  \r\nHighest DPC routine execution time (Œºs):              5,998.83\r\nDriver with highest DPC routine execution time:       ACPI.sys\r\n```\r\n\r\nThe data clearly implicates `ACPI.sys`. However, the per-CPU data reveals a more specific pattern:\r\n\r\n```\r\nCPU 0 Interrupt cycle time (s):                       208.470124\r\nCPU 0 ISR highest execution time (Œºs):                536.804674\r\nCPU 0 DPC highest execution time (Œºs):                5,998.834725\r\nCPU 0 DPC total execution time (s):                   90.558238\r\n```\r\n\r\nCPU 0 is taking the brunt of the impact, spending over 90 seconds processing interrupts while other cores remain largely unaffected. This isn't a failure of load balancing; it's a process locked to a single core.\r\n\r\nA similar test on a Scar 15 from 2022 shows the exact same culprit: high DPC latency originating from `ACPI.sys`.\r\n\r\n<img width=\"974\" height=\"511\" alt=\"latencymon\" src=\"https://github.com/user-attachments/assets/fdf6f26a-dda8-4561-82c7-349fc8c298ab\" />\r\n\r\nIt's easy to blame a Windows driver, but `ACPI.sys` is not a typical driver. It primarily functions as an interpreter for ACPI Machine Language (AML), the code provided by the laptop's firmware (BIOS). If `ACPI.sys` is slow, it's because the firmware is feeding it inefficient or flawed AML code to execute. These slowdowns are often triggered by General Purpose Events (GPEs) and traffic from the Embedded Controller (EC). To find the true source, we must dig deeper.\r\n\r\n## Capturing the Problem in More Detail: ETW Tracing\r\n\r\n### Setting Up Advanced ACPI Tracing\r\n\r\nTo understand what `ACPI.sys` is doing during these latency spikes, we can use Event Tracing for Windows (ETW) to capture detailed logs from the ACPI providers.\r\n\r\n```powershell\r\n# Find the relevant ACPI ETW providers\r\nlogman query providers | findstr /i acpi\r\n# This returns two key providers:\r\n# Microsoft-Windows-Kernel-Acpi {C514638F-7723-485B-BCFC-96565D735D4A}\r\n# Microsoft-ACPI-Provider {DAB01D4D-2D48-477D-B1C3-DAAD0CE6F06B}\r\n\r\n# Start a comprehensive trace session\r\nlogman start ACPITrace -p {DAB01D4D-2D48-477D-B1C3-DAAD0CE6F06B} 0xFFFFFFFF 5 -o C:\\Temp\\acpi.etl -ets\r\nlogman update ACPITrace -p {C514638F-7723-485B-BCFC-96565D735D4A} 0xFFFFFFFF 5 -ets\r\n\r\n# Then once we're done we can stop the trace and check the etl file and save the data in csv format aswell.\r\nlogman stop ACPITrace -ets\r\ntracerpt C:\\Temp\\acpi_providers.etl -o C:\\Temp\\acpi_events.csv -of CSV\r\n```\r\n\r\n### An Unexpected Discovery\r\n\r\nAnalyzing the resulting trace file in the Windows Performance Analyzer reveals a crucial insight. The spikes aren't random; they are periodic, occurring like clockwork every 30 to 60 seconds.\r\n\r\n<img width=\"1673\" height=\"516\" alt=\"61c7abb1-d7aa-4b69-9a88-22cca7352f00\" src=\"https://github.com/user-attachments/assets/2aac7320-3e06-4025-841c-86129f9d5b62\" />\r\n\r\nRandom interruptions often suggest hardware faults or thermal throttling. A perfectly repeating pattern points to a systemic issue, a timer or a scheduled event baked into the system's logic.\r\n\r\nThe raw event data confirms this pattern:\r\n```c",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-17T13:42:27.671468"
  },
  {
    "basic_info": {
      "name": "provenance-action",
      "full_name": "danielroe/provenance-action",
      "owner": "danielroe",
      "description": "GitHub Action that detects dependency provenance downgrades from lockfile changes (npm/pnpm/yarn).",
      "url": "https://github.com/danielroe/provenance-action",
      "clone_url": "https://github.com/danielroe/provenance-action.git",
      "ssh_url": "git@github.com:danielroe/provenance-action.git",
      "homepage": "",
      "created_at": "2025-09-16T11:08:14Z",
      "updated_at": "2025-09-17T13:41:14Z",
      "pushed_at": "2025-09-17T13:41:10Z"
    },
    "stats": {
      "stars": 201,
      "forks": 3,
      "watchers": 201,
      "open_issues": 1,
      "size": 83
    },
    "tech_info": {
      "language": "TypeScript",
      "languages": {
        "TypeScript": 57180
      },
      "license": null,
      "topics": []
    },
    "content": {
      "readme": "# `danielroe/provenance-action`\n\nDetect and fail CI when dependencies in your lockfile lose npm provenance or trusted publisher status.\n\n> [!WARNING]\n> This action is under active development and is only one tool to assist in securing your dependencies.\n\n## ‚ú® Features\n- supports `pnpm-lock.yaml`, `package-lock.json`, `yarn.lock` (v1 and v2+), `bun.lock`\n- handles transitives by comparing resolved versions\n- inline GitHub annotations at the lockfile line\n- JSON output and optional hard‚Äëfail (default: on)\n- pure TypeScript, Node 24+\n\nüëâ See it in action: [danielroe/provenance-action-test](https://github.com/danielroe/provenance-action-test)\n\n## üöÄ Quick start\n```yaml\nname: ci\non:\n  pull_request:\n    branches:\n      - main\npermissions:\n  contents: read\njobs:\n  check-provenance:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n        with:\n          fetch-depth: 0\n      - name: Check provenance downgrades\n        uses: danielroe/provenance-action@main\n        id: check\n        with:\n          fail-on-provenance-change: true # optional, default: false\n        #   lockfile: pnpm-lock.yaml      # optional\n        #   base-ref: origin/main         # optional, default: origin/main\n        #   fail-on-downgrade: true       # optional, default: true\n      - name: Print result\n        run: \"echo 'Downgraded: ${{ steps.check.outputs.downgraded }}'\"\n```\n\n## üîß Inputs\n- `lockfile` (optional): Path to the lockfile. Auto-detected if omitted.\n- `workspace-path` (optional): Path to workspace root. Default: `.`\n- `base-ref` (optional): Git ref to compare against. Default: `origin/main`.\n- `fail-on-downgrade` (optional): Controls failure behavior. Accepts `true`, `false`, `any`, or `only-provenance-loss`. Default: `true` (which is the same as `any`).\n- `fail-on-provenance-change` (optional): When `true`, fail on provenance repository/branch changes. Default: `false`.\n\n## üì§ Outputs\n- `downgraded`: JSON array of `{ name, from, to, downgradeType }` for detected downgrades. `downgradeType` is `provenance` or `trusted_publisher`.\n- `changed`: JSON array of provenance change events `{ name, from, to, type, previousRepository?, newRepository?, previousBranch?, newBranch? }`.\n\n## üß† How it works\n1. Diffs your lockfile against the base ref and collects changed resolved versions (including transitives).\n2. Checks npm provenance via the attestations API for each `name@version`.\n3. Falls back to version metadata for `dist.attestations`.\n4. Emits file+line annotations in the lockfile.\n5. If provenance exists for both the previous and new version, extracts GitHub `owner/repo` and branch from attestations and warns when they differ (repo changed or branch changed).\n\n## üîí Why this matters\nTrusted publishing links a package back to its source repo and build workflow, providing strong provenance guarantees. It helps ensure the package you install corresponds to audited source and CI.\n\nHowever, maintainers can still be phished or coerced into publishing without trusted publishing enabled, or switching to a non‚Äëtrusted path. In those cases, packages may still carry attestations, but the chain back to the trusted publisher can be weakened.\n\nThis action:\n- Detects when a dependency update loses npm provenance (no attestations) or loses trusted publisher (attestations but no trusted publisher marker), and\n- Fails CI by default (configurable), before that change lands in your main branch.\n\nThis is a stopgap until package managers enforce stronger policies natively. Until then, it offers a lightweight guardrail in CI.\n\n## ‚ö†Ô∏è Notes\n- Runs on Node 24+ and executes the TypeScript entrypoint directly.\n- `bun.lockb` is not supported. (You can generate a `bun.lock` with `bun install --save-text-lockfile`.)\n- Repository and branch change detection is best‚Äëeffort; attestation shapes vary and some packages omit repo/ref details.\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-17T13:42:28.986843"
  },
  {
    "basic_info": {
      "name": "VoxCPM",
      "full_name": "OpenBMB/VoxCPM",
      "owner": "OpenBMB",
      "description": "VoxCPM: Tokenizer-Free TTS for Context-Aware Speech Generation and True-to-Life Voice Cloning",
      "url": "https://github.com/OpenBMB/VoxCPM",
      "clone_url": "https://github.com/OpenBMB/VoxCPM.git",
      "ssh_url": "git@github.com:OpenBMB/VoxCPM.git",
      "homepage": "",
      "created_at": "2025-09-16T03:41:49Z",
      "updated_at": "2025-09-17T13:34:17Z",
      "pushed_at": "2025-09-17T13:18:47Z"
    },
    "stats": {
      "stars": 159,
      "forks": 13,
      "watchers": 159,
      "open_issues": 1,
      "size": 1507
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 107143
      },
      "license": "Apache License 2.0",
      "topics": []
    },
    "content": {
      "readme": "## üéôÔ∏è VoxCPM: Tokenizer-Free TTS for Context-Aware Speech Generation and True-to-Life Voice Cloning\n\n\n[![Project Page](https://img.shields.io/badge/Project%20Page-GitHub-blue)](https://github.com/OpenBMB/VoxCPM/) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-OpenBMB-yellow)](https://huggingface.co/openbmb/VoxCPM-0.5B) [![ModelScope](https://img.shields.io/badge/ModelScope-OpenBMB-purple)](https://modelscope.cn/models/OpenBMB/VoxCPM-0.5B)  [![Live Playground](https://img.shields.io/badge/Live%20PlayGround-Demo-orange)](https://huggingface.co/spaces/OpenBMB/VoxCPM-Demo) [![Samples](https://img.shields.io/badge/Page-Samples-red)](https://openbmb.github.io/VoxCPM-demopage)\n\n\n\n<div align=\"center\">\n  <img src=\"assets/voxcpm_logo.png\" alt=\"VoxCPM Logo\" width=\"40%\">\n</div>\n\n<div align=\"center\">\n\nüëã Contact us on [WeChat](assets/wechat.png)\n\n</div>\n\n## News \n* [2025.09.16] üî• üî• üî•  We Open Source the VoxCPM-0.5B [weights](https://huggingface.co/openbmb/VoxCPM-0.5B)!\n* [2025.09.16] üéâ üéâ üéâ  We Provide the [Gradio PlayGround](https://huggingface.co/spaces/OpenBMB/VoxCPM-Demo) for VoxCPM-0.5B, try it now! \n\n## Overview\n\nVoxCPM is a novel tokenizer-free Text-to-Speech (TTS) system that redefines realism in speech synthesis. By modeling speech in a continuous space, it overcomes the limitations of discrete tokenization and enables two flagship capabilities: context-aware speech generation and true-to-life zero-shot voice cloning.\n\nUnlike mainstream approaches that convert speech to discrete tokens, VoxCPM uses an end-to-end diffusion autoregressive architecture that directly generates continuous speech representations from text. Built on [MiniCPM-4](https://huggingface.co/openbmb/MiniCPM4-0.5B) backbone, it achieves implicit semantic-acoustic decoupling through hierachical language modeling and FSQ constraints, greatly enhancing both expressiveness and generation stability.\n\n<div align=\"center\">\n  <img src=\"assets/voxcpm_model.png\" alt=\"VoxCPM Model Architecture\" width=\"90%\">\n</div>\n\n\n###  üöÄ Key Features\n- **Context-Aware, Expressive Speech Generation** - VoxCPM comprehends text to infer and generate appropriate prosody, delivering speech with remarkable expressiveness and natural flow. It spontaneously adapts speaking style based on content, producing highly fitting vocal expression trained on a massive 1.8 million-hour bilingual corpus.\n- **True-to-Life Voice Cloning** - With only a short reference audio clip, VoxCPM performs accurate zero-shot voice cloning, capturing not only the speaker‚Äôs timbre but also fine-grained characteristics such as accent, emotional tone, rhythm, and pacing to create a faithful and natural replica.\n- **High-Efficiency Synthesis** - VoxCPM supports streaming synthesis with a Real-Time Factor (RTF) as low as 0.17 on a consumer-grade NVIDIA RTX 4090 GPU, making it possible for real-time applications.\n\n\n\n\n\n##  Quick Start\n\n### üîß Install from PyPI\n``` sh\npip install voxcpm\n```\n### 1.  Model Download (Optional)\nBy default, when you first run the script, the model will be downloaded automatically, but you can also download the model in advance.\n- Download VoxCPM-0.5B\n    ```\n    from huggingface_hub import snapshot_download\n    snapshot_download(\"openbmb/VoxCPM-0.5B\",local_files_only=local_files_only)\n    ```\n- Download ZipEnhancer and SenseVoice-Small. We use ZipEnhancer to enhance speech prompts and SenseVoice-Small for speech prompt ASR in the web demo.\n    ```\n    from modelscope import snapshot_download\n    snapshot_download('iic/speech_zipenhancer_ans_multiloss_16k_base')\n    snapshot_download('iic/SenseVoiceSmall')\n    ```\n\n### 2. Basic Usage\n```python\nimport soundfile as sf\nfrom voxcpm import VoxCPM\n\nmodel = VoxCPM.from_pretrained(\"openbmb/VoxCPM-0.5B\")\n\nwav = model.generate(\n    text=\"VoxCPM is an innovative end-to-end TTS model from ModelBest, designed to generate highly expressive speech.\",\n    prompt_wav_path=None,      # optional: path to a prompt speech for voice cloning\n    prompt_text=None,          # optional: reference text\n    cfg_value=2.0,             # LM guidance on LocDiT, higher for better adherence to the prompt, but maybe worse\n    inference_timesteps=10,   # LocDiT inference timesteps, higher for better result, lower for fast speed\n    normalize=True,           # enable external TN tool\n    denoise=True,             # enable external Denoise tool\n    retry_badcase=True,        # enable retrying mode for some bad cases (unstoppable)\n    retry_badcase_max_times=3,  # maximum retrying times\n    retry_badcase_ratio_threshold=6.0, # maximum length restriction for bad case detection (simple but effective), it could be adjusted for slow pace speech\n)\n\nsf.write(\"output.wav\", wav, 16000)\nprint(\"saved: output.wav\")\n```\n\n### 3. CLI Usage\n\nAfter installation, the entry point is `voxcpm` (or use `python -m voxcpm.cli`).\n\n```bash\n# 1) Direct synthesis (single text)\nvoxcpm --text \"VoxCPM is an innovative end-to-end TTS model from ModelBest, designed to generat",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-17T13:42:30.307699"
  },
  {
    "basic_info": {
      "name": "LLaVA-OneVision-1.5",
      "full_name": "EvolvingLMMs-Lab/LLaVA-OneVision-1.5",
      "owner": "EvolvingLMMs-Lab",
      "description": null,
      "url": "https://github.com/EvolvingLMMs-Lab/LLaVA-OneVision-1.5",
      "clone_url": "https://github.com/EvolvingLMMs-Lab/LLaVA-OneVision-1.5.git",
      "ssh_url": "git@github.com:EvolvingLMMs-Lab/LLaVA-OneVision-1.5.git",
      "homepage": null,
      "created_at": "2025-09-16T14:05:47Z",
      "updated_at": "2025-09-17T13:41:59Z",
      "pushed_at": "2025-09-17T13:41:56Z"
    },
    "stats": {
      "stars": 134,
      "forks": 7,
      "watchers": 134,
      "open_issues": 2,
      "size": 2643
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 7579110,
        "Jupyter Notebook": 521664,
        "Shell": 178138,
        "C++": 38717,
        "Cuda": 16941,
        "C": 2951,
        "Dockerfile": 2631,
        "HTML": 2625,
        "Makefile": 313
      },
      "license": null,
      "topics": []
    },
    "content": {
      "readme": "# LLaVA-OneVision-1.5: Fully Open Framework for Democratized Multimodal Training\n\n\n[ü§ó Mid-Training-Data (Uploading!)](https://huggingface.co/datasets/lmms-lab/LLaVA-One-Vision-1.5-Mid-Training-85M) | \n[ü§ó Insturct-Data (Uploading!)](https://huggingface.co/datasets/lmms-lab/LLaVA-One-Vision-1.5-Insturct-26M) \n\n**LLaVA-OneVision1.5** introduces a novel family of **fully open-source** Large Multimodal Models (LMMs) that achieves **state-of-the-art performance**  with substantially **lower cost** through training on **native resolution** images.\n\n1. **Superior Performance**\nA family of fully open-source large multimodal models demonstrating **superior performance** across multiple multimodal benchmarks, **outperforming Qwen2.5-VL** in most evaluation tasks.\n\n2. **High-Quality Data at Scale**\nMeticulously curated **pre-training and SFT data** with rigorous filtering and quality control, achieving **superior data efficiency** with only **64B tokens**.\n- Concept-balanced, highly diverse, high-quality caption data\n- Comprehensive instruction fine-tuning data covering a wide range of tasks\n\n3. **Ultra-Efficient Training Framework**\nComplete end-to-end training framework designed for maximum efficiency:\n- **$16K total budget** for full model training\n- **45% HFU efficiency** on A100 GPUs ($0.6 per GPU/Hour)\n- Built on **MegatronLM** with support for **MoE**, **FP8**, and **long sequence parallelization**\n- Optimized codebase for cost-effective scaling\n\n4. **Fully Open Framework** for community access and reproducibility:\n- ‚úÖ High-quality pre-training & SFT data\n- ‚úÖ Complete training framework & code\n- ‚úÖ Training recipes & configurations\n- ‚úÖ Base & instruct model checkpoints\n- ‚úÖ Comprehensive training logs & metrics\n\n\n## Model\n\n| Model                  | #Vision Param | #Language Param | #Total Param | HF Link                                                                      |\n|------------------------|---------------|-----------------|--------------|------------------------------------------------------------------------------|\n| LLaVA-OV-1.5-4B-Instruct      | 0.3B          | 4.4B            | 4.7B         | [ü§ó link]()                |\n| LLaVA-OV-1.5-8B-Instruct      | 0.3B          | 8.2B            | 8.5B         | [ü§ó link](https://huggingface.co/lmms-lab/LLaVA-OneVision-1.5-8B-Instruct) |\n\n\n## Dataset\n\n![Dataset Visualization](asset/dataset.jpg)\n\n\n| Description | Link |\n|-------------|------|\n| Mid-training data for LLaVA-OneVision-1.5 | [ü§ó Download (Uploading!)](https://huggingface.co/datasets/lmms-lab/LLaVA-One-Vision-1.5-Mid-Training-85M) |\n| SFT data for LLaVA-OneVision-1.5 | [ü§ó Download (Uploading!)](https://huggingface.co/datasets/lmms-lab/LLaVA-OneVision-1.5-Insturct-Data) |\n\n\n## Evaluation Results\n\n\nAll evaluations were conducted using lmms_eval.\n\n|                                  | **LLaVA-OV-1.5-8B** | **Qwen2.5 VL 7B** | **LLaVA-OV-1.5-4B** | **Qwen2.5 VL 3B** |\n|:----------------------------------|:---------------:|:-------------:|:---------------:|:-------------:|\n| MMMU (Validation)                 |    **55.44**    |     51.33     |    **51.44**    |     46.44     |\n| MMMU-Pro (Standard)               |    **37.40**    |     36.30     |    **33.24**    |     31.10     |\n| MMMU-Pro (Vision)                 |      25.15      |   **32.83**   |    **23.53**    |     21.27     |\n| MMBench (English; Test)           |    **84.14**    |     83.40     |    **82.29**    |     77.97     |\n| MMBench (Chinese; Test)           |      81.00      |   **81.61**   |    **76.73**    |     74.55     |\n| MME-RealWorld (English)           |    **62.31**    |     57.33     |    **57.16**    |     51.60     |\n| MME-RealWorld (Chinese)           |    **56.11**    |     51.50     |      21.38      |   **45.38**   |\n| AI2D (With Mask)                  |    **84.16**    |     82.58     |    **84.62**    |     78.56     |\n| AI2D (Without Mask)               |    **94.11**    |     93.36     |    **92.84**    |     90.74     |\n| CV-Bench                          |    **80.82**    |     79.95     |    **74.00**    |     71.53     |\n| VL-RewardBench                    |      45.90      |   **49.65**   |    **45.90**    |     42.06     |\n| V*                                |    **78.01**    |     76.96     |      66.49      |   **69.63**   |\n| PixmoCount                        |      62.19      |   **63.33**   |    **59.17**    |     50.85     |\n| CountBench                        |    **88.19**    |     86.35     |    **77.80**    |     72.51     |\n| ChartQA                           |    **86.48**    |     84.08     |    **85.11**    |     83.36     |\n| CharXiv (Direct Questions)        |    **74.10**    |     69.80     |    **70.70**    |     58.20     |\n| DocVQA (Test)                     |    **95.00**    |     94.93     |    **93.48**    |     92.67     |\n| InfoVQA (Test)                    |      78.42      |   **81.67**   |    **75.27**    |     75.63     |\n| WeMath                            |    **33.62**    |     33.",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-17T13:42:31.598634"
  },
  {
    "basic_info": {
      "name": "VibeVoice-finetuning",
      "full_name": "voicepowered-ai/VibeVoice-finetuning",
      "owner": "voicepowered-ai",
      "description": "Unofficial WIP LoRa Finetuning repository for VibeVoice",
      "url": "https://github.com/voicepowered-ai/VibeVoice-finetuning",
      "clone_url": "https://github.com/voicepowered-ai/VibeVoice-finetuning.git",
      "ssh_url": "git@github.com:voicepowered-ai/VibeVoice-finetuning.git",
      "homepage": null,
      "created_at": "2025-09-16T10:57:57Z",
      "updated_at": "2025-09-17T13:26:57Z",
      "pushed_at": "2025-09-16T16:31:47Z"
    },
    "stats": {
      "stars": 109,
      "forks": 25,
      "watchers": 109,
      "open_issues": 0,
      "size": 189
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 312954
      },
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "\r\n  \r\n\r\n# Unofficial WIP Finetuning repo for VibeVoice\r\n\r\n  \r\n\r\n# Hardware requirements\r\n\r\n  \r\n\r\nTo train a VibeVoice 1.5B LoRa, a machine with at least 16gb VRAM is recommended.\r\n\r\nTo train a VibeVoice 7B LoRa, a machine with at least 48gb VRAM is recommended.\r\n\r\nKeep in mind longer audios increase VRAM requirements\r\n\r\n  \r\n\r\n# Installation\r\n\r\nIt is recommended to install this in a fresh environment. Specifically, the Dockerized environment `runpod/pytorch:2.8.0-py3.11-cuda12.8.1-cudnn-devel-ubuntu22.04` has been tested to work.\r\n\r\n  \r\n\r\nTransformers version 4.51.3 is known to work, while other versions have errors related to Qwen2 architecture.\r\n\r\n  \r\n```\r\ngit clone https://github.com/voicepowered-ai/VibeVoice-finetuning\r\n\r\npip install -e .\r\n\r\npip uninstall -y transformers && pip install transformers==4.51.3\r\n\r\n(OPTIONAL) wandb login\r\n\r\n(OPTIONAL) export HF_HOME=/workspace/hf_models\r\n```\r\n\r\n  \r\n\r\n# Usage\r\n\r\n  \r\n\r\n## VibeVoice 1.5B / 7B (LoRA) fine-tuning\r\n\r\n  \r\n\r\n  \r\n\r\nWe put some code together for training VibeVoice (7B) with LoRA. This uses the vendored VibeVoice model/processor and trains with a dual loss: masked CE on text tokens plus diffusion MSE on acoustic latents.\r\n\r\n  \r\n\r\n  \r\n\r\nRequirements:\r\n\r\n  \r\n\r\n- Download a compatible VibeVoice 7B or 1.5b checkpoint (config + weights) and its processor files (preprocessor_config.json) or run straight from HF model.\r\n\r\n- A 24khz audio dataset with audio files (target audio), text prompts (transcriptions) and optionally voice prompts (reference audio)\r\n\r\n  \r\n\r\n  \r\n  \r\n\r\n### Training with Hugging Face Dataset\r\n\r\n  \r\n```\r\npython -m src.finetune_vibevoice_lora \\\r\n\r\n--model_name_or_path aoi-ot/VibeVoice-Large \\\r\n\r\n--processor_name_or_path src/vibevoice/processor \\\r\n\r\n--dataset_name your/dataset \\\r\n\r\n--text_column_name text \\\r\n\r\n--audio_column_name audio \\\r\n\r\n--voice_prompts_column_name voice_prompts \\\r\n\r\n--output_dir outputTrain3 \\\r\n\r\n--per_device_train_batch_size 8 \\\r\n\r\n--gradient_accumulation_steps 16 \\\r\n\r\n--learning_rate 2.5e-5 \\\r\n\r\n--num_train_epochs 5 \\\r\n\r\n--logging_steps 10 \\\r\n\r\n--save_steps 100 \\\r\n\r\n--eval_steps 100 \\\r\n\r\n--report_to wandb \\\r\n\r\n--remove_unused_columns False \\\r\n\r\n--bf16 True \\\r\n\r\n--do_train \\\r\n\r\n--gradient_clipping \\\r\n\r\n--gradient_checkpointing False \\\r\n\r\n--ddpm_batch_mul 4 \\\r\n\r\n--diffusion_loss_weight 1.4 \\\r\n\r\n--train_diffusion_head True \\\r\n\r\n--ce_loss_weight 0.04 \\\r\n\r\n--voice_prompt_drop_rate 0.2 \\\r\n\r\n--lora_target_modules q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj \\\r\n\r\n--lr_scheduler_type cosine \\\r\n\r\n--warmup_ratio 0.03 \\\r\n\r\n--max_grad_norm 0.8\r\n```\r\n  \r\n\r\n----------\r\n\r\n  \r\n\r\n### Training with Local JSONL Dataset\r\n\r\n  \r\n```\r\npython -m src.finetune_vibevoice_lora \\\r\n\r\n--model_name_or_path aoi-ot/VibeVoice-Large \\\r\n\r\n--processor_name_or_path src/vibevoice/processor \\\r\n\r\n--train_jsonl prompts.jsonl \\\r\n\r\n--text_column_name text \\\r\n\r\n--audio_column_name audio \\\r\n\r\n--output_dir outputTrain3 \\\r\n\r\n--per_device_train_batch_size 8 \\\r\n\r\n--gradient_accumulation_steps 16 \\\r\n\r\n--learning_rate 2.5e-5 \\\r\n\r\n--num_train_epochs 5 \\\r\n\r\n--logging_steps 10 \\\r\n\r\n--save_steps 100 \\\r\n\r\n--report_to wandb \\\r\n\r\n--remove_unused_columns False \\\r\n\r\n--bf16 True \\\r\n\r\n--do_train \\\r\n\r\n--gradient_clipping \\\r\n\r\n--gradient_checkpointing False \\\r\n\r\n--ddpm_batch_mul 4 \\\r\n\r\n--diffusion_loss_weight 1.4 \\\r\n\r\n--train_diffusion_head True \\\r\n\r\n--ce_loss_weight 0.04 \\\r\n\r\n--voice_prompt_drop_rate 0.2 \\\r\n\r\n--lora_target_modules q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj \\\r\n\r\n--lr_scheduler_type cosine \\\r\n\r\n--warmup_ratio 0.03 \\\r\n\r\n--max_grad_norm 0.8\r\n```\r\n\r\n\r\n### JSONL format:\r\n\r\n  \r\n\r\nYou can provide an optional `voice_prompts` key. If it is omitted, a voice prompt will be automatically generated from the target audio.\r\n\r\n  \r\n\r\n**Example without a pre-defined voice prompt (will be auto-generated):**\r\n\r\n`{\"text\": \"Speaker 0: Speaker0 transcription.\", \"audio\": \"/workspace/wavs/segment_000000.wav\"}`\r\n\r\n  \r\n\r\n**Example with a pre-defined voice prompt:**\r\n\r\n`{\"text\": \"Speaker 0: Speaker0 transcription.\", \"audio\": \"/workspace/wavs/segment_000000.wav\", \"voice_prompts\": \"/path/to/a/different/prompt.wav\"}`\r\n\r\n  \r\n\r\n**Example with multiple speakers and voice prompts:**\r\n\r\n`{\"text\": \"Speaker 0: How is the project coming along?\\nSpeaker 1: It's going well, we should be finished by Friday.\", \"audio\": \"/data/conversations/convo_01.wav\", \"voice_prompts\": [\"/data/prompts/alice_voice_prompt.wav\", \"/data/prompts/bob_voice_prompt.wav\"]}`\r\n\r\n  \r\n  \r\n  \r\n\r\n# Notes:\r\n\r\n  \r\n\r\n- Audio is assumed to be 24 kHz; input audio will be loaded/resampled to 24 kHz.\r\n\r\n  \r\n\r\n- If you pass raw NumPy arrays or torch Tensors as audio (without sampling rate metadata), the collator assumes they are already 24 kHz. To trigger resampling, provide dicts like {\"array\": <np.ndarray>, \"sampling_rate\": <int>} or file paths.\r\n\r\n  \r\n\r\n- Tokenizers (acoustic/semantic) are frozen by default. LoRA is applied to the LLM (Qwen) and optionally to the diffusion head.\r\n\r\n  \r\n\r\n- The collator builds interleaved seq",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-17T13:42:33.020075"
  },
  {
    "basic_info": {
      "name": "Qwen3-ASR-Toolkit",
      "full_name": "QwenLM/Qwen3-ASR-Toolkit",
      "owner": "QwenLM",
      "description": "Official Python toolkit for the Qwen3-ASR API. Parallel high‚Äëthroughput calls, robust long‚Äëaudio transcription, multi‚Äësample‚Äërate support.",
      "url": "https://github.com/QwenLM/Qwen3-ASR-Toolkit",
      "clone_url": "https://github.com/QwenLM/Qwen3-ASR-Toolkit.git",
      "ssh_url": "git@github.com:QwenLM/Qwen3-ASR-Toolkit.git",
      "homepage": "",
      "created_at": "2025-09-16T09:03:49Z",
      "updated_at": "2025-09-17T13:37:30Z",
      "pushed_at": "2025-09-17T07:22:44Z"
    },
    "stats": {
      "stars": 81,
      "forks": 3,
      "watchers": 81,
      "open_issues": 0,
      "size": 10
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 11717
      },
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "# Qwen3-ASR-Toolkit\n\n[![PyPI version](https://badge.fury.io/py/qwen3-asr-toolkit.svg)](https://badge.fury.io/py/qwen3-asr-toolkit)\n[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://www.python.org/downloads/)\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n\nAn advanced, high-performance Python command-line toolkit for using the **Qwen-ASR API** (formerly Qwen3-ASR-Flash). This implementation overcomes the API's 3-minute audio length limitation by intelligently splitting long audio/video files and processing them in parallel, enabling rapid transcription of hours-long content.\n\n## üöÄ Key Features\n\n-   **Break the 3-Minute Limit**: Seamlessly transcribe audio and video files of any length by bypassing the official API's duration constraint.\n-   **Smart Audio Splitting**: Utilizes **Voice Activity Detection (VAD)** to split audio into meaningful chunks at natural silent pauses. This ensures that words and sentences are not awkwardly cut off.\n-   **High-Speed Parallel Processing**: Leverages multi-threading to send audio chunks to the Qwen-ASR API concurrently, dramatically reducing the total transcription time for long files.\n-   **Automatic Audio Resampling**: Automatically converts audio from any sample rate and channel count to the 16kHz mono format required by the Qwen-ASR API. You can use any audio file without worrying about pre-processing.\n-   **Universal Media Support**: Supports virtually any audio and video format (e.g., `.mp4`, `.mov`, `.mkv`, `.mp3`, `.wav`, `.m4a`) thanks to its reliance on FFmpeg.\n-   **Simple & Easy to Use**: A straightforward command-line interface allows you to get started with just a single command.\n\n## ‚öôÔ∏è How It Works\n\nThis tool follows a robust pipeline to deliver fast and accurate transcriptions for long-form media:\n\n1.  **Media Loading**: The script first loads your local audio or video file.\n2.  **VAD-based Chunking**: It analyzes the audio stream using Voice Activity Detection (VAD) to identify silent segments.\n3.  **Intelligent Splitting**: The audio is then split into smaller chunks based on the detected silences. Each chunk is kept under the 3-minute API limit, preventing mid-sentence cuts.\n4.  **Parallel API Calls**: A thread pool is initiated to upload and process these chunks concurrently using the DashScope Qwen-ASR API.\n5.  **Result Aggregation**: The transcribed text segments from all chunks are collected, re-ordered, and saved.\n\n## üèÅ Getting Started\n\nFollow these steps to set up and run the project on your local machine.\n\n### Prerequisites\n\n-   Python 3.8 or higher.\n-   **FFmpeg**: The script requires FFmpeg to be installed on your system to handle media files.\n    -   **Ubuntu/Debian**: `sudo apt update && sudo apt install ffmpeg`\n    -   **macOS**: `brew install ffmpeg`\n    -   **Windows**: Download from the [official FFmpeg website](https://ffmpeg.org/download.html) and add it to your system's PATH.\n-   **DashScope API Key**: You need an API key from Alibaba Cloud's DashScope.\n    -   You can obtain one from the [DashScope Console](https://dashscope.console.aliyun.com/apiKey). If you are calling the API services of Tongyi Qwen for the first time, you can follow the tutorial on [this website](https://help.aliyun.com/zh/model-studio/first-api-call-to-qwen) to create your own API Key.\n    -   For better security and convenience, it is **highly recommended** to set your API key as an environment variable named `DASHSCOPE_API_KEY`. The script will automatically use it, and you won't need to pass the `--api-key` argument in the command.\n\n        **On Linux/macOS:**\n        ```bash\n        export DASHSCOPE_API_KEY=\"your_api_key_here\"\n        ```\n        *(To make this permanent, add the line to your `~/.bashrc`, `~/.zshrc`, or `~/.profile` file.)*\n\n        **On Windows (Command Prompt):**\n        ```cmd\n        set DASHSCOPE_API_KEY=\"your_api_key_here\"\n        ```\n\n        **On Windows (PowerShell):**\n        ```powershell\n        $env:DASHSCOPE_API_KEY=\"your_api_key_here\"\n        ```\n        *(For a permanent setting on Windows, search for \"Edit the system environment variables\" in the Start Menu and add `DASHSCOPE_API_KEY` to your user variables.)*\n\n### Installation\n\nWe recommend installing the tool directly from PyPI for the simplest setup.\n\n#### Option 1: Install from PyPI (Recommended)\n\nSimply run the following command in your terminal. This will install the package and make the `qwen3-asr` command available system-wide.\n\n```bash\npip install qwen3-asr-toolkit\n```\n\n#### Option 2: Install from Source\n\nIf you want to install the latest development version or contribute to the project, you can install from the source code.\n\n1.  Clone the repository:\n    ```bash\n    git clone https://github.com/QwenLM/Qwen3-ASR-Toolkit.git\n    cd Qwen3-ASR-Toolkit\n    ```\n\n2.  Install the package:\n    ```bash\n    pip install .\n    ```\n\n## üìñ Usage\n\nOnce installed, you can use the `qwen3-asr` command dire",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-17T13:42:34.324984"
  },
  {
    "basic_info": {
      "name": "aisdk-prompt-optimizer",
      "full_name": "Scale3-Labs/aisdk-prompt-optimizer",
      "owner": "Scale3-Labs",
      "description": "A tool kit for generating high quality prompts for AISDK using DSPy GEPA optimizer",
      "url": "https://github.com/Scale3-Labs/aisdk-prompt-optimizer",
      "clone_url": "https://github.com/Scale3-Labs/aisdk-prompt-optimizer.git",
      "ssh_url": "git@github.com:Scale3-Labs/aisdk-prompt-optimizer.git",
      "homepage": null,
      "created_at": "2025-09-16T17:54:46Z",
      "updated_at": "2025-09-17T13:19:47Z",
      "pushed_at": "2025-09-16T18:07:03Z"
    },
    "stats": {
      "stars": 74,
      "forks": 3,
      "watchers": 74,
      "open_issues": 0,
      "size": 106
    },
    "tech_info": {
      "language": "TypeScript",
      "languages": {
        "TypeScript": 87779,
        "Python": 8751,
        "CSS": 4475,
        "JavaScript": 605
      },
      "license": "Other",
      "topics": []
    },
    "content": {
      "readme": "# AISDK Prompt Optimizer\n\n[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)\n[![Open Source](https://badges.frapsoft.com/os/v1/open-source.svg?v=103)](https://opensource.org/)\n\nTransform your AI interactions with intelligent prompt optimization. Teach your AI, collect ideal samples, and generate optimized prompts using the powerful AISDK Prompt Optimizer.\n\n**Fully Open Source** - Built by the team that created [Langtrace AI](https://langtrace.ai) and [Zest](https://heyzest.ai)\n\n## What is GEPA?\n\n**GEPA** (Genetic-Pareto) is a reflective optimizer that adaptively evolves textual components (such as prompts) of AI systems. Unlike traditional optimization methods that only use scalar scores, GEPA leverages rich textual feedback to guide the optimization process, allowing it to propose high-performing prompts in very few rollouts.\n\nKey features of GEPA:\n- **Reflective Prompt Mutation**: Uses LLMs to reflect on execution traces and propose new instructions\n- **Rich Textual Feedback**: Leverages any textual feedback beyond just scalar rewards\n- **Pareto-based Selection**: Maintains a frontier of candidates that excel in different scenarios\n\nLearn more: [GEPA Documentation](https://dspy.ai/api/optimizers/GEPA/)\n\n## How It Works\n\n1. **Start Conversation**: Begin chatting with the AI and teach it desired behaviors through examples\n2. **Mark Examples**: Save ideal conversation samples that represent perfect responses\n3. **Run Optimization**: Let AISDK Prompt Optimizer analyze patterns and generate optimized prompts\n4. **Deploy Results**: Use the optimized prompts in your applications\n\n## Features\n\n- **Teach Your AI**: Guide your AI through interactive conversations and demonstrate the ideal responses you want to achieve\n- **Collect Ideal Samples**: Gather high-quality conversation examples that represent perfect AI behavior for your use case\n- **AISDK Prompt Optimizer**: Leverage advanced optimization algorithms to automatically generate and refine prompt candidates\n\n## Quick Start\n\n### Prerequisites\n- Node.js (18+ recommended)\n- `uv` package manager for Python\n- OpenAI API key\n- AI Gateway API key\n\n### Environment Setup\n\nBefore running the application, you need to set up your environment variables:\n\n1. Copy the example environment file:\n   ```bash\n   cp .env.example .env\n   ```\n\n2. Edit the `.env` file and add your API keys:\n   ```bash\n   # Required: OpenAI API key for AI model access\n   OPENAI_API_KEY=your_actual_openai_api_key_here\n   \n   # Required: AI Gateway API key for prompt optimization\n   AI_GATEWAY_API_KEY=your_actual_ai_gateway_api_key_here\n   ```\n\n**Important**: Never commit your actual API keys to version control. The `.env` file is already included in `.gitignore`.\n\n### Installation & Setup\n\n```bash\n# Clone the repository\ngit clone https://github.com/Scale3-Labs/aisdk-prompt-optimizer\ncd aisdk-prompt-optimizer\n\n# Install dependencies\nnpm install\n\n# Start both services (recommended)\nnpm run dev:all\n```\n\n### Alternative: Start Services Separately\n\n```bash\n# Terminal 1: Start the Python optimizer\ncd python_optimizer\nuv run app.py\n\n# Terminal 2: Start the web app\nnpm run dev\n```\n\nThe web app will be available at `http://localhost:3000` and the Python optimizer at `http://localhost:8000`. Both services need to be running for the optimization features to work.\n\n## Available Scripts\n\n- `npm run dev` - Start the Next.js development server\n- `npm run dev:py` - Start the Python optimizer server\n- `npm run dev:all` - Start both services concurrently\n\n## Architecture\n\n### Python Optimizer (dspy.GEPA)\n\nThe repository includes a lightweight Flask server exposing the `dspy.GEPA` optimizer, managed with `uv`. The Next.js `/api/optimize` route calls this server and writes optimization artifacts to:\n- `data/prompt.md` - Generated optimized prompts\n- `data/complete-optimization.json` - Complete optimization results and metadata\n\n### Web Application\n\nBuilt with Next.js and shadcn/ui components, the web interface provides:\n- Interactive chat interface for teaching the AI\n- Sample collection and management\n- Optimization trigger and results visualization\n- Modern, responsive UI with dark/light mode support\n\n## Technology Stack\n\n- **Frontend**: Next.js, TypeScript, Tailwind CSS, shadcn/ui\n- **Backend**: Python Flask server with dspy.GEPA optimizer\n- **Package Management**: npm (frontend), uv (Python)\n\n## Learn More\n\n- [DSPy Documentation](https://dspy.ai/)\n- [GEPA Optimizer API](https://dspy.ai/api/optimizers/GEPA/)\n- [Next.js Documentation](https://nextjs.org/docs)\n\n## Deployment\n\nThe easiest way to deploy your Next.js app is to use the [Vercel Platform](https://vercel.com/new?utm_medium=default-template&filter=next.js&utm_source=create-next-app&utm_campaign=create-next-app-readme) from the creators of Next.js.\n\nFor the Python optimizer, you'll need to deploy it to a Python-compatible hosting service and update the API endpoints accordingly.\n\nCheck out the [N",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-17T13:42:35.620513"
  },
  {
    "basic_info": {
      "name": "vibe-coding-playbook",
      "full_name": "RiyaParikh0112/vibe-coding-playbook",
      "owner": "RiyaParikh0112",
      "description": null,
      "url": "https://github.com/RiyaParikh0112/vibe-coding-playbook",
      "clone_url": "https://github.com/RiyaParikh0112/vibe-coding-playbook.git",
      "ssh_url": "git@github.com:RiyaParikh0112/vibe-coding-playbook.git",
      "homepage": null,
      "created_at": "2025-09-16T13:58:41Z",
      "updated_at": "2025-09-17T13:38:20Z",
      "pushed_at": "2025-09-16T17:58:27Z"
    },
    "stats": {
      "stars": 74,
      "forks": 29,
      "watchers": 74,
      "open_issues": 1,
      "size": 50
    },
    "tech_info": {
      "language": null,
      "languages": {},
      "license": null,
      "topics": []
    },
    "content": {
      "readme": "# vibe-coding-playbook",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-17T13:42:37.035715"
  },
  {
    "basic_info": {
      "name": "free-sqlite",
      "full_name": "fjb040911/free-sqlite",
      "owner": "fjb040911",
      "description": "Free SQLite for VSCode.Support writing SQL statements",
      "url": "https://github.com/fjb040911/free-sqlite",
      "clone_url": "https://github.com/fjb040911/free-sqlite.git",
      "ssh_url": "git@github.com:fjb040911/free-sqlite.git",
      "homepage": null,
      "created_at": "2025-09-16T06:20:19Z",
      "updated_at": "2025-09-17T13:35:34Z",
      "pushed_at": "2025-09-17T13:35:31Z"
    },
    "stats": {
      "stars": 70,
      "forks": 0,
      "watchers": 70,
      "open_issues": 0,
      "size": 9882
    },
    "tech_info": {
      "language": "TypeScript",
      "languages": {
        "TypeScript": 34158,
        "JavaScript": 16831
      },
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "# Free sqlite\n\nVSCode extension to explore and query SQLite databases.\nopen-source and free.\n\n## ‚ú® Features\n - üìÉ Open any SQLite file - Simply click on .sqlite or .db files to open them instantly\n - üõ° Table Explorer - Browse all tables in your database from an integrated sidebar\n - üåà Data Visualization - View table data in a clean, modern interface that adapts to your VS Code theme\n - üñ• SQL statement editor - SQL statement editor, Automatically complete SQL keywords, table names, and fields\n - üì¶ Query result export - The query results can be exported as Excel or CSV\n - ‚ù§Ô∏è Favorites - Collect some of your most commonly used SQL statements\n\n## How to use\n\n### Install\nVSCode extension install!\n[Install free sqlite](https://marketplace.visualstudio.com/items?itemName=free-sqlite.free-sqlite)\n\n### Open database\nNow! Browse all tables in your database in the right panel\n![open](https://github.com/fjb040911/free-sqlite/blob/main/doc/open.gif?raw=true)\n\n### Multiple files\n![multiple](https://github.com/fjb040911/free-sqlite/blob/main/doc/multi.gif?raw=true)\n\n### SQL Editor\nAutomatically complete SQL keywords, table names, and fields\n![editor](https://github.com/fjb040911/free-sqlite/blob/main/doc/select.gif?raw=true)\n\n### Favorite\nQuickly execute or view your frequently used SQL\n![favorite](https://github.com/fjb040911/free-sqlite/blob/main/doc/favoites.gif?raw=true)\n\n### Export\n![export](https://github.com/fjb040911/free-sqlite/blob/main/doc/expot.gif?raw=true)\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-17T13:42:38.316139"
  },
  {
    "basic_info": {
      "name": "grok2api",
      "full_name": "VeroFess/grok2api",
      "owner": "VeroFess",
      "description": "rewrite grok2api",
      "url": "https://github.com/VeroFess/grok2api",
      "clone_url": "https://github.com/VeroFess/grok2api.git",
      "ssh_url": "git@github.com:VeroFess/grok2api.git",
      "homepage": null,
      "created_at": "2025-09-16T12:48:33Z",
      "updated_at": "2025-09-17T11:20:39Z",
      "pushed_at": "2025-09-17T07:19:37Z"
    },
    "stats": {
      "stars": 45,
      "forks": 20,
      "watchers": 45,
      "open_issues": 0,
      "size": 53
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 160146,
        "HTML": 33516,
        "Dockerfile": 306
      },
      "license": null,
      "topics": []
    },
    "content": {
      "readme": "# Grok API Gateway\n\n## ‰∏éÂéüÁâàÂ∑ÆÂºÇ\n\nÊú¨ fork ÁâàÊú¨Áõ∏ËæÉ‰∫éÂéüÁâàÂ¢ûÂä†‰∫Ü‰ª•‰∏ãÂäüËÉΩÔºö\n\n0. **Âü∫Êú¨ÂÖ®ÈÉ®ÈáçÂÜô‰∫Ü...**\n1. **Ëá™Âä®Ëé∑Âèñ x-statsig-id** - ‰ΩøÁî® Playwright Ëá™Âä®Ëé∑ÂèñÂπ∂ÁÆ°ÁêÜËÆ§ËØÅÂ§¥\n2. **ÊµÅÊ®°ÂºèÊ†áÁ≠æËøáÊª§** - Ëá™Âä®ÁßªÈô§ÂìçÂ∫î‰∏≠ÁöÑ `<xaiArtifact` Á≠âÊ†áÁ≠æ\n3. **Â¢ûÂº∫ÁªüËÆ°ÂäüËÉΩ** - ÊîπËøõÁöÑ‰ª§Áâå‰ΩøÁî®ÁªüËÆ°ÂíåÁõëÊéß\n4. **Grok4ÊîØÊåÅ** - ÂèçÊ≠£ÊàëËÉΩÁî®.jpg\n\n## ÁéØÂ¢ÉÂèòÈáèÈÖçÁΩÆ\n\n### ÂøÖÈúÄÈÖçÁΩÆ\n\n| ÁéØÂ¢ÉÂèòÈáè | ÊèèËø∞ | ÈªòËÆ§ÂÄº | Á§∫‰æã |\n|---------|------|--------|------|\n| `API_KEY` | API ËÆøÈóÆÂØÜÈí• | `sk-123456` | `sk-your-api-key` |\n| `SSO` | Grok SSO ‰ª§ÁâåÔºàÊôÆÈÄöÔºâ | - | `token1,token2,token3` |\n| `SSO_SUPER` | Grok SSO ‰ª§ÁâåÔºàË∂ÖÁ∫ßÔºâ | - | `super_token1,super_token2` |\n\n### ÂèØÈÄâÈÖçÁΩÆ\n\n| ÁéØÂ¢ÉÂèòÈáè | ÊèèËø∞ | ÈªòËÆ§ÂÄº | ÊúâÊïàÂÄº | Á§∫‰æã |\n|---------|------|--------|--------|------|\n| `IS_CUSTOM_SSO` | ÂÖÅËÆ∏Âä®ÊÄÅ SSO ‰ª§Áâå | `false` | `true/false` | `true` |\n| `IS_TEMP_CONVERSATION` | ‰∏¥Êó∂ÂØπËØùÊ®°Âºè | `true` | `true/false` | `false` |\n| `SHOW_THINKING` | ÊòæÁ§∫Êé®ÁêÜËøáÁ®ã | `false` | `true/false` | `true` |\n| `SHOW_SEARCH_RESULTS` | ÊòæÁ§∫ÊêúÁ¥¢ÁªìÊûú | `true` | `true/false` | `false` |\n| `IS_SUPER_GROK` | ÂêØÁî®Ë∂ÖÁ∫ß Grok ÂäüËÉΩ | `false` | `true/false` | `true` |\n| `MANAGER_SWITCH` | ÂêØÁî® Web ÁÆ°ÁêÜÁïåÈù¢ | - | `true/false` | `true` |\n| `ADMINPASSWORD` | ÁÆ°ÁêÜÁïåÈù¢ÂØÜÁ†Å | - | ‰ªªÊÑèÂ≠óÁ¨¶‰∏≤ | `admin123` |\n| `PORT` | ÊúçÂä°Á´ØÂè£ | `5200` | Êï∞Â≠ó | `8080` |\n| `PROXY` | ‰ª£ÁêÜÊúçÂä°Âô® | - | HTTP/SOCKS5 URL | `http://127.0.0.1:1080` |\n| `CF_CLEARANCE` | Cloudflare ‰ª§Áâå | - | CF ‰ª§ÁâåÂ≠óÁ¨¶‰∏≤ | `cf_clearance_token` |\n| `DISABLE_DYNAMIC_HEADERS` | Á¶ÅÁî®Âä®ÊÄÅÂ§¥ÈÉ®Ëé∑ÂèñÔºàÁ¶ÅÁî® Playwright Ëá™Âä®Ëé∑Âèñ x-statsig-idÔºâ | `false` | `true/false` | `true` |\n| `FILTERED_TAGS` | ËøáÊª§Ê†áÁ≠æÂàóË°® | `xaiartifact,xai:tool_usage_card,grok:render,details,summary` | ÈÄóÂè∑ÂàÜÈöî | `tag1,tag2,tag3` |\n| `TAG_CONFIG` | ËøáÊª§Ê†áÁ≠æÈÖçÁΩÆ | `{\"xaiartifact\":{\"behavior\":\"preserve_content\"},\"xai:tool_usage_card\":{\"behavior\":\"remove_all\"},\"grok:render\":{\"behavior\":\"remove_all\"},\"details\":{\"behavior\":\"preserve_content\"},\"summary\":{\"behavior\":\"preserve_content\"}}` | json | `{\"xaiartifact\":{\"behavior\":\"preserve_content\"},\"xai:tool_usage_card\":{\"behavior\":\"remove_all\"},\"grok:render\":{\"behavior\":\"remove_all\"},\"details\":{\"behavior\":\"preserve_content\"},\"summary\":{\"behavior\":\"preserve_content\"}}` |\n| `CONTENT_TYPE_MAPPINGS` | ËøáÊª§Ê†áÁ≠æÈáçÂÜôÈÖçÁΩÆ | Â§™Èïø‰∫Ü,ÁúãÊ∫êÁ†Å | json | {\"text/plain\":{\"stag\":\"```\",\"etag\":\"```\"},\"text/python\":{\"stag\":\"```python\\n\",\"etag\":\"\\n```\"}} |\n\n### Ê†áÁ≠æËøáÊª§ÈÖçÁΩÆ\n\nÊ∑ªÂä†‰∫ÜÈ´òÁ∫ßÊ†áÁ≠æËøáÊª§ÂäüËÉΩÔºåÂèØÂú®ÊµÅÂºèÂìçÂ∫î‰∏≠Ëá™Âä®Â§ÑÁêÜÁâπÂÆöÁöÑ XML/HTML Ê†áÁ≠æ„ÄÇ\n\nÊ≥®ÊÑèÈÖçÁΩÆÈîôËØØ‰ºöÁõ¥Êé•Á†¥ÂùèËæìÂá∫!!!\n\n#### FILTERED_TAGS\n\n**ÊèèËø∞**ÔºöÊ†áÁ≠æËøáÊª§ÂàóË°®, ÂΩìÈÅáÂà∞‰∏çÂú®ÂàóË°®‰∏≠ÁöÑÊ†áÁ≠æÊó∂‰ºöÁ´ãÂç≥ÊîæÂºÉÂêéÁª≠ÈáçÂÜô\n\n**Ê†ºÂºè**ÔºöÈÄóÂè∑ÂàÜÈöîÁöÑÊ†áÁ≠æÂêçÁß∞ÔºåÂ∞èÂÜô\n\n**ÈªòËÆ§ÂÄº**Ôºö`xaiartifact,xai:tool_usage_card,grok:render,details,summary`\n\n**Á§∫‰æã**Ôºö\n```bash\nFILTERED_TAGS=xaiartifact,grok:render,grok:thinking\n```\n\n#### TAG_CONFIG\n\n**ÊèèËø∞**ÔºöÈ´òÁ∫ßÊ†áÁ≠æË°å‰∏∫ÈÖçÁΩÆÔºåÊîØÊåÅ‰∏∫‰∏çÂêåÊ†áÁ≠æËÆæÁΩÆ‰∏çÂêåÁöÑÂ§ÑÁêÜÁ≠ñÁï•„ÄÇ\n\n**Ê†ºÂºè**ÔºöJSON ÂØπË±°ÔºåÈîÆ‰∏∫Ê†áÁ≠æÂêçÁß∞ÔºàÂ∞èÂÜôÔºâÔºåÂÄº‰∏∫ÈÖçÁΩÆÂØπË±°\n\n**ÈÖçÁΩÆÈÄâÈ°π**Ôºö\n- `behavior`: Ê†áÁ≠æË°å‰∏∫\n  - `\"preserve_content\"`: ‰øùÁïôÂÜÖÂÆπÔºåÊ∑ªÂä†Ê†ºÂºèÂåñÊ†áËÆ∞\n  - `\"remove_all\"`: ÂÆåÂÖ®ÁßªÈô§Ê†áÁ≠æÂíåÂÜÖÂÆπ\n\n**ÈªòËÆ§ÂÄº**ÔºöÂü∫‰∫é FILTERED_TAGS Ëá™Âä®ÁîüÊàê\n\n**Á§∫‰æã**Ôºö\n```json\n{\n  \"xaiartifact\": {\"behavior\": \"preserve_content\"},\n  \"xai:tool_usage_card\": {\"behavior\": \"remove_all\"},\n  \"grok:render\": {\"behavior\": \"remove_all\"},\n  \"details\": {\"behavior\": \"preserve_content\"},\n  \"summary\": {\"behavior\": \"preserve_content\"}\n}\n```\n\n**Âú® docker-compose.yml ‰∏≠ÈÖçÁΩÆ**Ôºö\n```yaml\nenvironment:\n  TAG_CONFIG: '{\"xaiartifact\":{\"behavior\":\"preserve_content\"},\"xai:tool_usage_card\":{\"behavior\":\"remove_all\"},\"grok:render\":{\"behavior\":\"remove_all\"},\"details\":{\"behavior\":\"preserve_content\"},\"summary\":{\"behavior\":\"preserve_content\"}}'\n```\n\n#### CONTENT_TYPE_MAPPINGS\n\n**ÊèèËø∞**ÔºöÂÜÖÂÆπÁ±ªÂûãÊò†Â∞ÑÈÖçÁΩÆÔºåÂÆö‰πâ‰∏çÂêå contentType ÁöÑÊ†ºÂºèÂåñÊ†áËÆ∞„ÄÇ\n\n**Ê†ºÂºè**ÔºöJSON ÂØπË±°ÔºåÈîÆ‰∏∫ MIME Á±ªÂûãÔºåÂÄº‰∏∫ÂåÖÂê´ stagÔºàÂºÄÂßãÊ†áËÆ∞ÔºâÂíå etagÔºàÁªìÊùüÊ†áËÆ∞ÔºâÁöÑÂØπË±°\n\n**ÈªòËÆ§Êò†Â∞Ñ**Ôºö\n```json\n{\n  \"text/plain\": {\"stag\": \"```\", \"etag\": \"```\"},\n  \"text/markdown\": {\"stag\": \"\", \"etag\": \"\"},\n  \"application/json\": {\"stag\": \"```json\\n\", \"etag\": \"\\n```\"}\n}\n```\n\n**Á§∫‰æãÈÖçÁΩÆ**Ôºö\n```yaml\nenvironment:\n  CONTENT_TYPE_MAPPINGS: '{\"text/plain\":{\"stag\":\"```\",\"etag\":\"```\"},\"text/python\":{\"stag\":\"```python\\n\",\"etag\":\"\\n```\"}}'\n```\n\n**Â∑•‰ΩúÂéüÁêÜ**Ôºö\n1. ÂΩìÈÅáÂà∞ `preserve_content` Ë°å‰∏∫ÁöÑÊ†áÁ≠æÊó∂Ôºå‰ºöÊü•ÊâæÊ†áÁ≠æÁöÑ `contentType` Â±ûÊÄß\n2. Ê†πÊçÆ `contentType` Âú®Êò†Â∞ÑË°®‰∏≠Êü•ÊâæÂØπÂ∫îÁöÑÊ†ºÂºèÂåñÊ†áËÆ∞\n3. Áî® `stag` + ÂÜÖÂÆπ + `etag` ÊõøÊç¢ÂéüÂßãÊ†áÁ≠æÂíåÂØπÂ∫îÁöÑÂ∞ÅÈó≠Ê†áÁ≠æ\n\n\n## Âø´ÈÄüÂºÄÂßã\n\n### ‰ΩøÁî® Docker Hub ÈïúÂÉè\n\nÁé∞Âú®ÂèØ‰ª•Áõ¥Êé•‰ªé Docker Hub ÊãâÂèñÈ¢ÑÊûÑÂª∫ÁöÑÈïúÂÉèÔºö\n\n```bash\n# ÊãâÂèñÈïúÂÉè\ndocker pull verofess/grok2api\n\n# ËøêË°åÂÆπÂô®\ndocker run -d \\\n  --name grok2api \\\n  -p 5200:5200 \\\n  -e API_KEY=sk-your-api-key \\\n  -e SSO=your-sso-token \\\n  verofess/grok2api\n\n# ÊàñËÄÖ‰ΩøÁî® docker-compose\ndocker-compose up -d\n```\n\n### Docker Compose Á§∫‰æã\n\n```yaml\nservices:\n  grok2api:\n    image: verofess/grok2api\n    container_name: grok2api\n    ports:\n      - \"5200:5200\"\n    environment:\n      - API_KEY=sk-your-api-key\n      - SSO=your-sso-token\n      - IS_TEMP_CONVERSATION=true\n      - SHOW_THINKING=false\n    restart: unless-stopped\n```",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-17T13:42:39.620391"
  },
  {
    "basic_info": {
      "name": "autogen-financial-analysis",
      "full_name": "liangdabiao/autogen-financial-analysis",
      "owner": "liangdabiao",
      "description": "‰∏Ä‰∏™Âü∫‰∫éÂæÆËΩØAutoGenÊ°ÜÊû∂ÁöÑ‰ºÅ‰∏öÁ∫ßÈáëËûçÂàÜÊûêÁ≥ªÁªüÔºå‰ΩøÁî®Â§öAgentÊû∂ÊûÑÊèê‰æõÂÖ®Èù¢ÁöÑË¥¢Âä°ÂàÜÊûê„ÄÅÈ£éÈô©ËØÑ‰º∞ÂíåÈáèÂåñÊäïËµÑÂàÜÊûêÂäüËÉΩ„ÄÇ Ê†∏ÂøÉÂäüËÉΩ: Â§öÊ∫êÊï∞ÊçÆÊî∂ÈõÜ : Êï¥ÂêàYahoo Finance„ÄÅAlpha Vantage„ÄÅQuandlÁ≠âÂ§ö‰∏™ÈáëËûçÊï∞ÊçÆÊ∫ê, Êô∫ËÉΩË¥¢Âä°ÂàÜÊûê : Âü∫‰∫éAutoGenÁöÑÂ§öAgentÂçè‰ΩúÂàÜÊûê, È£éÈô©ËØÑ‰º∞ : VaRËÆ°ÁÆó„ÄÅÂéãÂäõÊµãËØï„ÄÅËíôÁâπÂç°Ê¥õÊ®°Êãü, ÈáèÂåñÂàÜÊûê : Âõ†Â≠êÊ®°Âûã„ÄÅÊäïËµÑÁªÑÂêà‰ºòÂåñ„ÄÅÊú∫Âô®Â≠¶‰π†È¢ÑÊµã, ÂÆûÊó∂ÁõëÊéß : Á≥ªÁªüÊÄßËÉΩÁõëÊéßÂíåÂëäË≠¶, Êï∞ÊçÆÂèØËßÜÂåñ : ‰∫§‰∫íÂºèÂõæË°®ÂíåÊä•ÂëäÁîüÊàê,",
      "url": "https://github.com/liangdabiao/autogen-financial-analysis",
      "clone_url": "https://github.com/liangdabiao/autogen-financial-analysis.git",
      "ssh_url": "git@github.com:liangdabiao/autogen-financial-analysis.git",
      "homepage": null,
      "created_at": "2025-09-16T08:42:33Z",
      "updated_at": "2025-09-17T13:37:34Z",
      "pushed_at": "2025-09-17T00:24:38Z"
    },
    "stats": {
      "stars": 42,
      "forks": 10,
      "watchers": 42,
      "open_issues": 0,
      "size": 349
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 873205,
        "HTML": 52025,
        "PLpgSQL": 7113,
        "Dockerfile": 917
      },
      "license": null,
      "topics": []
    },
    "content": {
      "readme": "# AutoGen Financial Analysis System\n\n‰∏Ä‰∏™Âü∫‰∫éÂæÆËΩØAutoGenÊ°ÜÊû∂ÁöÑ‰ºÅ‰∏öÁ∫ßÈáëËûçÂàÜÊûêÁ≥ªÁªüÔºå‰ΩøÁî®Â§öAgentÊû∂ÊûÑÊèê‰æõÂÖ®Èù¢ÁöÑË¥¢Âä°ÂàÜÊûê„ÄÅÈ£éÈô©ËØÑ‰º∞ÂíåÈáèÂåñÊäïËµÑÂàÜÊûêÂäüËÉΩ„ÄÇ\n\n## üöÄ ÂäüËÉΩÁâπÊÄß\n\n### üîç Ê†∏ÂøÉÂäüËÉΩ\n- **Â§öÊ∫êÊï∞ÊçÆÊî∂ÈõÜ**: Êï¥ÂêàYahoo Finance„ÄÅAlpha VantageÁ≠âÂ§ö‰∏™ÈáëËûçÊï∞ÊçÆÊ∫ê\n- **Êô∫ËÉΩË¥¢Âä°ÂàÜÊûê**: Âü∫‰∫éAutoGenÁöÑÂ§öAgentÂçè‰ΩúÂàÜÊûê\n- **È£éÈô©ËØÑ‰º∞**: VaRËÆ°ÁÆó„ÄÅÂéãÂäõÊµãËØï„ÄÅËíôÁâπÂç°Ê¥õÊ®°Êãü\n- **ÈáèÂåñÂàÜÊûê**: Âõ†Â≠êÊ®°Âûã„ÄÅÊäïËµÑÁªÑÂêà‰ºòÂåñ„ÄÅÁ≠ñÁï•ÂõûÊµã„ÄÅÊú∫Âô®Â≠¶‰π†È¢ÑÊµã\n- **ÂÆûÊó∂ÁõëÊéß**: Á≥ªÁªüÊÄßËÉΩÁõëÊéßÂíåÂëäË≠¶\n- **Êï∞ÊçÆÂèØËßÜÂåñ**: ‰∫§‰∫íÂºèÂõæË°®ÂíåÊä•ÂëäÁîüÊàê\n\n### üèóÔ∏è ÊäÄÊúØÊû∂ÊûÑ\n- **ÂæÆÊúçÂä°Êû∂ÊûÑ**: Ê®°ÂùóÂåñËÆæËÆ°ÔºåÊîØÊåÅÊ∞¥Âπ≥Êâ©Â±ï\n- **ÂºÇÊ≠•Â§ÑÁêÜ**: È´òÊÄßËÉΩÂºÇÊ≠•‰ªªÂä°Â§ÑÁêÜ\n- **ÁºìÂ≠òÁ≥ªÁªü**: Â§öÁ∫ßÁºìÂ≠òÁ≠ñÁï•ÔºåÊèêÂçáÂìçÂ∫îÈÄüÂ∫¶\n- **ÂÆâÂÖ®ÊÄß**: ÂÆåÊï¥ÁöÑË∫´‰ªΩËÆ§ËØÅ„ÄÅÊéàÊùÉÂíåÂä†ÂØÜ\n- **ÁõëÊéßÂëäË≠¶**: Prometheus + GrafanaÁõëÊéß‰ΩìÁ≥ª\n- **ÂÆπÂô®Âåñ**: DockerÂíåKubernetesÈÉ®ÁΩ≤ÊîØÊåÅ\n\n## üì¶ ÂÆâË£ÖÊåáÂçó\n\n### Á≥ªÁªüË¶ÅÊ±Ç\n- Python 3.8+\n- Redis 6.0+\n- PostgreSQL 12+\n- Docker (ÂèØÈÄâ)\n\n### Âø´ÈÄüÂÆâË£Ö\n\n1. **ÂÖãÈöÜÈ°πÁõÆ**\n```bash\ngit clone https://github.com/your-username/autogen-financial-analysis.git\ncd autogen-financial-analysis\n```\n\n2. **ÂàõÂª∫ËôöÊãüÁéØÂ¢É**\n```bash\npython -m venv venv\nsource venv/bin/activate  # Linux/Mac\n# Êàñ\nvenv\\Scripts\\activate     # Windows\n```\n\n3. **ÂÆâË£Ö‰æùËµñ**\n```bash\npip install -r requirements.txt\n```\n\n4. **ÈÖçÁΩÆÁéØÂ¢ÉÂèòÈáè**\n```bash\ncp .env.example .env\n# ÁºñËæë .env Êñá‰ª∂ÔºåÊ∑ªÂä†ÂøÖË¶ÅÁöÑAPIÂØÜÈí•\n```\n\n### DockerÈÉ®ÁΩ≤\n\n```bash\n# ÊûÑÂª∫Âπ∂ÂêØÂä®ÊâÄÊúâÊúçÂä°\ndocker-compose up -d\n\n# Êü•ÁúãÊúçÂä°Áä∂ÊÄÅ\ndocker-compose ps\n\n# Êü•ÁúãÊó•Âøó\ndocker-compose logs -f\n```\n\n## üõ†Ô∏è ‰ΩøÁî®ÊñπÊ≥ï\n\n### ÂëΩ‰ª§Ë°åÁïåÈù¢\n\n#### Âü∫Êú¨Áî®Ê≥ï\n```bash\n# ÂàÜÊûêÂçï‰∏™ÂÖ¨Âè∏\npython -m src.main analyze AAPL\n\n# ÂàÜÊûêÊäïËµÑÁªÑÂêà\npython -m src.main portfolio AAPL MSFT GOOG\n\n# ‰∫§‰∫íÊ®°Âºè\npython -m src.main interactive\n```\n\n#### È´òÁ∫ßÈÄâÈ°π\n```bash\n# ÊåáÂÆöÂàÜÊûêÁ±ªÂûã\npython -m src.main analyze AAPL --type comprehensive\n\n# ÂØºÂá∫Êä•Âëä\npython -m src.main analyze AAPL --format html,pdf\n\n# Ëá™ÂÆö‰πâÈÖçÁΩÆ\npython -m src.main analyze AAPL --config custom_config.yaml\n```\n\n#### ÈáèÂåñÂàÜÊûêÈÄâÈ°π\n```bash\n# ÂØπÂçï‰∏™ËÇ°Á•®ËøõË°åÈáèÂåñÂàÜÊûê\npython -m src.main quant AAPL\n\n# ‰ΩøÁî®ÁâπÂÆöÂõ†Â≠êËøõË°åÂàÜÊûê\npython -m src.main quant AAPL --factors momentum value growth\n\n# ‰ΩøÁî®ÁâπÂÆöÂõ†Â≠êÊ®°Âûã\npython -m src.main quant AAPL --method carhart\n\n# ÂØºÂá∫ÈáèÂåñÂàÜÊûêÊä•Âëä\npython -m src.main quant AAPL --format html,pdf,json\n```\n\n#### Á≠ñÁï•ÂõûÊµãÈÄâÈ°π\n```bash\n# ËøêË°åÂä®ÈáèÁ≠ñÁï•ÂõûÊµã\npython -m src.main backtest --strategy momentum --start-date 2020-01-01 --end-date 2023-01-01\n\n# ËÆæÁΩÆÂõûÊµãÂèÇÊï∞\npython -m src.main backtest --strategy momentum --start-date 2020-01-01 --end-date 2023-01-01 --initial-capital 100000 --commission 0.001\n\n# ÂØºÂá∫ÂõûÊµãÊä•Âëä\npython -m src.main backtest --strategy momentum --start-date 2020-01-01 --end-date 2023-01-01 --format html,pdf\n```\n\n#### Á≠ñÁï•‰ºòÂåñÈÄâÈ°π\n```bash\n# ‰ºòÂåñÁ≠ñÁï•ÂèÇÊï∞\npython -m src.main optimize --strategy momentum --param window=5,10,15,20\n\n# ËÆæÁΩÆ‰ºòÂåñÊó∂Èó¥ËåÉÂõ¥\npython -m src.main optimize --strategy momentum --param window=5,10,15,20 --start-date 2020-01-01 --end-date 2023-01-01\n```\n\n#### ÊäïËµÑÁªÑÂêà‰ºòÂåñÈÄâÈ°π\n```bash\n# ‰ΩøÁî®ÂùáÂÄº-ÊñπÂ∑Æ‰ºòÂåñÊñπÊ≥ï\npython -m src.main optimize-portfolio --symbols AAPL MSFT GOOG --method mean_variance\n\n# ‰ΩøÁî®È£éÈô©Âπ≥‰ª∑‰ºòÂåñÊñπÊ≥ï\npython -m src.main optimize-portfolio --symbols AAPL MSFT GOOG --method risk_parity\n\n# ËÆæÁΩÆÈ£éÈô©ÂéåÊÅ∂Á≥ªÊï∞\npython -m src.main optimize-portfolio --symbols AAPL MSFT GOOG --method mean_variance --risk-aversion 1.5\n```\n\n### WebÁïåÈù¢\n\nÂêØÂä®WebÊúçÂä°Ôºö\n```bash\npython -m src.api.app\n```\n\nËÆøÈóÆ `http://localhost:8000` ‰ΩøÁî®WebÁïåÈù¢„ÄÇ\n\n### APIÊé•Âè£\n\n#### ÂàõÂª∫ÂàÜÊûê‰ªªÂä°\n```bash\ncurl -X POST \"http://localhost:8000/api/v1/analysis\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"symbols\": [\"AAPL\", \"MSFT\"],\n    \"analysis_type\": \"comprehensive\",\n    \"export_formats\": [\"html\", \"pdf\"]\n  }'\n```\n\n#### Êü•Áúã‰ªªÂä°Áä∂ÊÄÅ\n```bash\ncurl -X GET \"http://localhost:8000/api/v1/analysis/{task_id}\"\n```\n\n#### WebSocketÂÆûÊó∂Êõ¥Êñ∞\n```javascript\nconst ws = new WebSocket('ws://localhost:8000/ws');\nws.onmessage = function(event) {\n    const data = JSON.parse(event.data);\n    console.log('‰ªªÂä°Êõ¥Êñ∞:', data);\n};\n```\n\n## üìä ÂàÜÊûêÊä•Âëä\n\n### Ë¥¢Âä°ÂàÜÊûêÊä•Âëä\n- **ÁõàÂà©ËÉΩÂäõÂàÜÊûê**: ROE„ÄÅROA„ÄÅÊØõÂà©Áéá„ÄÅÂáÄÂà©Áéá\n- **ÂÅøÂÄ∫ËÉΩÂäõÂàÜÊûê**: ËµÑ‰∫ßË¥üÂÄ∫Áéá„ÄÅÊµÅÂä®ÊØîÁéá„ÄÅÈÄüÂä®ÊØîÁéá\n- **ËøêËê•ÊïàÁéáÂàÜÊûê**: ÊÄªËµÑ‰∫ßÂë®ËΩ¨Áéá„ÄÅÂ≠òË¥ßÂë®ËΩ¨Áéá\n- **ÊàêÈïøÊÄßÂàÜÊûê**: Êî∂ÂÖ•Â¢ûÈïøÁéá„ÄÅÂà©Ê∂¶Â¢ûÈïøÁéá\n- **ÊùúÈÇ¶ÂàÜÊûê**: ROEÂàÜËß£‰∏∫ÂáÄÂà©Ê∂¶Áéá„ÄÅËµÑ‰∫ßÂë®ËΩ¨ÁéáÂíåÊùÉÁõä‰πòÊï∞\n\n### È£éÈô©ËØÑ‰º∞Êä•Âëä\n- **Â∏ÇÂú∫È£éÈô©**: VaR„ÄÅCVaR„ÄÅBetaÁ≥ªÊï∞\n- **‰ø°Áî®È£éÈô©**: Z-Score„ÄÅAltmanÊ®°Âûã\n- **ÊµÅÂä®ÊÄßÈ£éÈô©**: ÊµÅÂä®ÊÄßË¶ÜÁõñÁéá„ÄÅÂáÄÁ®≥ÂÆöËµÑÈáëÁéá\n- **Êìç‰ΩúÈ£éÈô©**: ÂéÜÂè≤Ê®°Êãü„ÄÅËíôÁâπÂç°Ê¥õÊ®°Êãü\n- **ÂéãÂäõÊµãËØï**: ÊûÅÁ´ØÂ∏ÇÂú∫ÊÉÖÊôØÂàÜÊûê\n\n### ÈáèÂåñÂàÜÊûêÊä•Âëä\n- **Âõ†Â≠êÂàÜÊûê**: Â§öÂõ†Â≠êÊö¥Èú≤„ÄÅÂõ†Â≠êÊî∂ÁõäÁéá„ÄÅ‰ø°ÊÅØÁ≥ªÊï∞\n- **ÊäïËµÑÁªÑÂêà‰ºòÂåñ**: ÊúâÊïàÂâçÊ≤ø„ÄÅÈ£éÈô©Âπ≥‰ª∑„ÄÅÊúÄÂ§ßÂàÜÊï£Âåñ\n- **Á≠ñÁï•ÂõûÊµã**: Á¥ØËÆ°Êî∂Áõä„ÄÅÊúÄÂ§ßÂõûÊí§„ÄÅÂ§èÊôÆÊØîÁéá\n- **È£éÈô©Ë¥°ÁåÆÂàÜÊûê**: ÂêÑËµÑ‰∫ßÂØπÁªÑÂêàÈ£éÈô©ÁöÑË¥°ÁåÆÂ∫¶\n- **Áª©ÊïàÂΩíÂõ†**: Êî∂ÁõäÊù•Ê∫êÂàÜËß£\n\n### ÊäïËµÑÁªÑÂêàÂàÜÊûêÊä•Âëä\n- **ÊúâÊïàÂâçÊ≤ø**: È£éÈô©Êî∂ÁõäÊúÄ‰ºòÂåñÁªÑÂêà\n- **Â§èÊôÆÊØîÁéá**: È£éÈô©Ë∞ÉÊï¥ÂêéÊî∂Áõä\n- **ÊúÄÂ§ßÂõûÊí§**: ÂéÜÂè≤ÊúÄÂ§ßÊçüÂ§±\n- **Áõ∏ÂÖ≥Á≥ªÊï∞**: ËµÑ‰∫ßÈó¥Áõ∏ÂÖ≥ÊÄßÂàÜÊûê\n- **È£éÈô©Âπ≥‰ª∑**: È£éÈô©Ë¥°ÁåÆÂ∫¶‰ºòÂåñ\n\n## üîß ÈÖçÁΩÆËØ¥Êòé\n\n### ‰∏ªË¶ÅÈÖçÁΩÆÊñá‰ª∂\n\n#### config.yaml\n```yaml\n# AutoGenÈÖçÁΩÆ\nautogen:\n  gpt_model: \"gpt-4\"\n  temperature: 0.7\n  max_tokens: 4000\n\n# Êï∞ÊçÆÊ∫êÈÖçÁΩÆ\ndata_sources:\n  yahoo_finance:\n    timeout: 30\n    retry_count: 3\n  alpha_vantage:\n    api_key: \"${ALPHA_VANTAGE_API_KEY}\"\n    calls_per_minute: 5\n```\n\n#### ÁéØÂ¢ÉÂèòÈáè\n```bash\n# APIÂØÜÈí•\nYAHOO_FINANCE_API_KEY=your_key_here\nALPHA_VANTAGE_API_KEY=your_key_here\n\n# Êï∞ÊçÆÂ∫ìÈÖçÁΩÆ\nDATABASE_URL=postgresql://user:password@localhost:5432/autogen_financial\nREDIS_URL=redis://localhost:6379/0\n\n# ÂÆâÂÖ®ÈÖçÁΩÆ\nSECRET_KEY=your_secret_key_here\nJWT_SECRET=your_jwt_secret_here\n```\n\n## üß™ ÊµãËØï\n\n### ËøêË°åÊµãËØï\n```bash\n# ËøêË°åÊâÄÊúâÊµãËØï\npytest\n\n# ËøêË°åÁâπÂÆöÊ®°ÂùóÊµãËØï\npytest tests/test_data.py\n\n# ËøêË°åAPIÊµãËØï\npytest tests/test_api.py\n\n# ÁîüÊàêË¶ÜÁõñÁéáÊä•Âëä\npytest --cov=src --cov-report=html\n```\n\n### ÊµãËØïË¶ÜÁõñÁéá\n- Êï∞ÊçÆÊî∂ÈõÜÊ®°Âùó: 95%\n- Ë¥¢Âä°ÂàÜÊûêÊ®°Âùó: 92%\n- È£éÈô©ÂàÜÊûêÊ®°Âùó: 90%\n- APIÊé•Âè£: 88%\n- Êï¥‰ΩìË¶ÜÁõñÁéá: 93%\n\n## üìà ÊÄßËÉΩÁõëÊéß\n\n### Á≥ªÁªüÊåáÊ†á\n- CPU‰ΩøÁî®Áéá\n- ÂÜÖÂ≠ò‰ΩøÁî®Èáè\n- Á£ÅÁõòI/O\n- ÁΩëÁªúÂêûÂêêÈáè\n- Êï∞ÊçÆÂ∫ìËøûÊé•Êï∞\n- RedisÂëΩ‰∏≠Áéá\n\n### ‰∏öÂä°ÊåáÊ†á\n- Êï∞ÊçÆÊî∂ÈõÜÊàêÂäüÁéá\n- ÂàÜÊûê‰ªªÂä°ÊâßË°åÊó∂Èó¥\n- APIÂìçÂ∫îÊó∂Èó¥\n- ÈîôËØØÁéá\n- Áî®Êà∑Ê¥ªË∑ÉÂ∫¶\n\n### ËÆøÈóÆÁõëÊéßÁïåÈù¢\n```bash\n# Grafana‰ª™Ë°®Êùø\nhttp://localhost:3000\n\n# PrometheusÊü•ËØ¢ÁïåÈù¢\nhttp://localhost:9090\n```\n\n## üîí ÂÆâ",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-17T13:42:40.907288"
  },
  {
    "basic_info": {
      "name": "CRT_Python_AI_A",
      "full_name": "gilshan-s/CRT_Python_AI_A",
      "owner": "gilshan-s",
      "description": "Coding",
      "url": "https://github.com/gilshan-s/CRT_Python_AI_A",
      "clone_url": "https://github.com/gilshan-s/CRT_Python_AI_A.git",
      "ssh_url": "git@github.com:gilshan-s/CRT_Python_AI_A.git",
      "homepage": null,
      "created_at": "2025-09-16T04:28:17Z",
      "updated_at": "2025-09-17T05:01:25Z",
      "pushed_at": "2025-09-16T05:24:03Z"
    },
    "stats": {
      "stars": 39,
      "forks": 37,
      "watchers": 39,
      "open_issues": 0,
      "size": 2
    },
    "tech_info": {
      "language": null,
      "languages": {},
      "license": null,
      "topics": []
    },
    "content": {
      "readme": "# CRT_Python_AI_A\nCoding\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-17T13:42:42.241544"
  },
  {
    "basic_info": {
      "name": "ECE_F_CRT_PYTHON",
      "full_name": "gilshan-s/ECE_F_CRT_PYTHON",
      "owner": "gilshan-s",
      "description": "CODING",
      "url": "https://github.com/gilshan-s/ECE_F_CRT_PYTHON",
      "clone_url": "https://github.com/gilshan-s/ECE_F_CRT_PYTHON.git",
      "ssh_url": "git@github.com:gilshan-s/ECE_F_CRT_PYTHON.git",
      "homepage": null,
      "created_at": "2025-09-17T04:49:52Z",
      "updated_at": "2025-09-17T05:02:14Z",
      "pushed_at": "2025-09-17T04:49:52Z"
    },
    "stats": {
      "stars": 38,
      "forks": 40,
      "watchers": 38,
      "open_issues": 0,
      "size": 0
    },
    "tech_info": {
      "language": null,
      "languages": {},
      "license": null,
      "topics": []
    },
    "content": {
      "readme": "# ECE_F_CRT_PYTHON\nCODING\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-17T13:42:43.528200"
  },
  {
    "basic_info": {
      "name": "InternVLA-M1",
      "full_name": "InternRobotics/InternVLA-M1",
      "owner": "InternRobotics",
      "description": null,
      "url": "https://github.com/InternRobotics/InternVLA-M1",
      "clone_url": "https://github.com/InternRobotics/InternVLA-M1.git",
      "ssh_url": "git@github.com:InternRobotics/InternVLA-M1.git",
      "homepage": null,
      "created_at": "2025-09-16T12:47:28Z",
      "updated_at": "2025-09-17T13:41:17Z",
      "pushed_at": "2025-09-17T13:10:46Z"
    },
    "stats": {
      "stars": 35,
      "forks": 0,
      "watchers": 35,
      "open_issues": 0,
      "size": 14232
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 587378,
        "Shell": 10751,
        "Makefile": 661
      },
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "# InternVLA-M1\n\n**InternVLA-M1** is an open-source, end-to-end **vision‚Äìlanguage‚Äìaction (VLA) framework** for building and researching generalist robot policies.\n\nhttps://github.com/user-attachments/assets/e83ae046-a503-46a8-95e4-ef381919b7f8\n\n[![Paper](https://img.shields.io/badge/Paper-arXiv-red.svg)](https://github.com/InternRobotics/InternVLA-M1/blob/InternVLA-M1/assets/InternVLA_M1.pdf) [![Website](https://img.shields.io/badge/Website-GitHub%20Pages-blue.svg)](https://internrobotics.github.io/internvla-m1.github.io) [![Demo](https://img.shields.io/badge/Demo-YouTube-red.svg)](https://youtu.be/n129VDqJCk4) [![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)\n\n![](assets/teaser.png)\n\n## üî• Key Features\n\n1. **Modular & Extensible**  \n   All core components (model architecture, training data, training strategies, evaluation pipeline) are fully decoupled, enabling independent development, debugging, and extension of each module.\n\n\n2. **Dual-System and Dual-Supervision**\n   InternVLA-M1 integrates both a language head and an action head under a unified framework, enabling collaborative training with dual supervision. \n\n3. **Efficient Training & Fast Convergence**\n   Learns spatial and visual priors from large-scale multimodal pretraining and transfers them via spatial prompt fine-tuning. Achieves strong performance (e.g., SOTA-level convergence on  in \\~2.5 epochs without separate action pretraining). \n\n## üéØ Target Audience\n\n1. Users who want to leverage open-source VLMs (e.g., Qwen2.5-VL) for robot control.\n2. Teams co-training action datasets jointly with multimodal (vision‚Äìlanguage) data.\n3. Researchers exploring alternative VLA architectures and training strategies.\n\n## üìä Experimental Results\n|             | WindowX | Google Robot(VA) | Google Robot(VM) | LIBERO |\n|-------------|---------|------------------|------------------|--------|\n| pi0         | 27.1    | 54.8             | 58.8             | 94.2   |\n| gr00t       | 61.9    | 44.5             | 35.2             | 93.9   |\n| InternVLA-M1 |**71.7** |**76.0**          |**80.7**          |**95.9**|\n|             |         |                  |                  |        |\n\n\n\n\n\n\n# üöÄ Quick Start\n\n## üõ† Environment Setup\n\n```bash\n# Clone the repo\ngit clone https://github.com/InternRobotics/InternVLA-M1\n\n# Create conda environment\nconda create -n internvla-m1 python=3.10 -y\nconda activate internvla-m1\n\n# Install requirements\npip install -r requirements.txt\n\n# Install FlashAttention2\npip install flash-attn --no-build-isolation\n\n# Install InternVLA-M1\npip install -e .\n```\n\n## üìò Examples\n\nWe provide several end-to-end examples for reference:\n\n* **Reproduce InternVLA-M1 in simplerEnv**\n  [Example](/examples/simplerEnv/setup.md)\n\n* **Training/Deployment on real robots**\n  [Example](/examples/real_robot/setup.md)\n\n* **Extending InternVLA-M1**\n  [Example](examples/extending_m1/README.md)\n\n## üìà Model Zoo\nWe will release a series of pretrained models and checkpoints to facilitate reproduction and downstream use.\n\n- Full list and download links: assets/MODEL_ZOO.md\n\nStatus: rolling release. If you need early access or encounter broken links, please open an issue.\n\n# üó∫Ô∏è Roadmap\n\n* [ ] Release model weights (Stay tuned, coming soon)\n* [ ] Add multi-task mixed training examples\n* [ ] Release real-robot demo\n* [ ] Unify evaluation scripts and metrics\n\n# ü§ù Contributing\n\nWe welcome contributions via Pull Requests or Issues.\nPlease include detailed logs and reproduction steps when reporting bugs.\n\n# üìú Citation\n\nIf you find this useful in your research, please consider citing:\n\n```bibtex\n@misc{internvla2024,\n  title  = {InternVLA-M1: Latent Spatial Grounding for Instruction-Following Robotic Manipulation},\n  author = {InternVLA-M1 Contributors},\n  year   = {2025},\n  booktitle={arXiv},\n}\n```\n\n# üì¨ Contact\n\n* Issues: Submit via GitHub Issues with detailed logs and steps\n\n# üôè Acknowledgements\n\nWe thank the open-source community for their inspiring work. This project builds upon and is inspired by the following projects (alphabetical order):\n- [IPEC-COMMUNITY](https://huggingface.co/IPEC-COMMUNITY): Curated OXE / LIBERO style multi-task datasets and formatting examples.\n- [Isaac-GR00T](https://github.com/NVIDIA/Isaac-GR00T): Standardized action data loader (GR00T-LeRobot).\n- [Qwen2.5-VL](https://github.com/QwenLM/Qwen2.5-VL/blob/main/qwen-vl-finetune/README.md): Multimodal input/output format, data loader, and pretrained VLM backbone.\n- [CogACT](https://github.com/microsoft/CogACT/tree/main/action_model): Reference for a DiT-style action head design.\n- [llavavla](https://github.com/JinhuiYE/llavavla): Baseline code structure and engineering design references.\n- [GenManip Simulation Platform](https://github.com/InternRobotics/GenManip): Simulation platform for generalizable pick-and-place based on Isaac Sim.\n\n\nNotes:\n- If any required attribution or license header is missing, please open an issue and we will correct it promptly.\n- All third-party resources remai",
      "default_branch": "InternVLA-M1"
    },
    "fetched_at": "2025-09-17T13:42:44.817358"
  },
  {
    "basic_info": {
      "name": "C3_CRT_Python",
      "full_name": "gilshan-s/C3_CRT_Python",
      "owner": "gilshan-s",
      "description": "Coding",
      "url": "https://github.com/gilshan-s/C3_CRT_Python",
      "clone_url": "https://github.com/gilshan-s/C3_CRT_Python.git",
      "ssh_url": "git@github.com:gilshan-s/C3_CRT_Python.git",
      "homepage": null,
      "created_at": "2025-09-16T09:17:49Z",
      "updated_at": "2025-09-16T09:46:04Z",
      "pushed_at": "2025-09-16T09:41:18Z"
    },
    "stats": {
      "stars": 30,
      "forks": 29,
      "watchers": 30,
      "open_issues": 0,
      "size": 2
    },
    "tech_info": {
      "language": null,
      "languages": {},
      "license": null,
      "topics": []
    },
    "content": {
      "readme": "# C3_CRT_Python\nCoding\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-17T13:42:46.089015"
  },
  {
    "basic_info": {
      "name": "ai-vids",
      "full_name": "smelnik12071968-design/ai-vids",
      "owner": "smelnik12071968-design",
      "description": "https://nsgens.com",
      "url": "https://github.com/smelnik12071968-design/ai-vids",
      "clone_url": "https://github.com/smelnik12071968-design/ai-vids.git",
      "ssh_url": "git@github.com:smelnik12071968-design/ai-vids.git",
      "homepage": "",
      "created_at": "2025-09-17T11:10:01Z",
      "updated_at": "2025-09-17T12:56:30Z",
      "pushed_at": "2025-09-17T11:10:01Z"
    },
    "stats": {
      "stars": 30,
      "forks": 0,
      "watchers": 30,
      "open_issues": 0,
      "size": 0
    },
    "tech_info": {
      "language": null,
      "languages": {},
      "license": null,
      "topics": [
        "ai-image-to-video-nsfw",
        "nsfw-ai-image-generator-no-limit",
        "nsfw-ai-video-generator",
        "nsfw-ai-video-generator-no-limit"
      ]
    },
    "content": {
      "readme": "# ai-vids\nhttps://nsgens.com\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-17T13:42:47.411626"
  },
  {
    "basic_info": {
      "name": "Cyberlivre",
      "full_name": "pedrosilvaevangelista/Cyberlivre",
      "owner": "pedrosilvaevangelista",
      "description": "Curso completo de Seguran√ßa Cibern√©tica com conte√∫dos gratuitos em portugu√™s.",
      "url": "https://github.com/pedrosilvaevangelista/Cyberlivre",
      "clone_url": "https://github.com/pedrosilvaevangelista/Cyberlivre.git",
      "ssh_url": "git@github.com:pedrosilvaevangelista/Cyberlivre.git",
      "homepage": null,
      "created_at": "2025-09-16T12:41:59Z",
      "updated_at": "2025-09-17T12:44:53Z",
      "pushed_at": "2025-09-16T12:48:57Z"
    },
    "stats": {
      "stars": 24,
      "forks": 2,
      "watchers": 24,
      "open_issues": 0,
      "size": 515
    },
    "tech_info": {
      "language": null,
      "languages": {},
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "<p align=\"center\">\n  <img src=\"assets/CYBERLIVRE.png\" alt=\"Banner do Curso CiberLivre\" width=\"800\"/>\n</p>\n\n<div align=\"center\">\n\n# Curso CyberLivre\n\n</div>\n\n> **Um curso completo e gratuito para formar profissionais em Seguran√ßa Cibern√©tica**\n\nEste projeto √© um reposit√≥rio inspirado no [Ci√™ncia da Computa√ß√£o](https://github.com/Universidade-Livre/ciencia-da-computacao) e consiste em um **curso completo para formar profissionais na √°rea de seguran√ßa cibern√©tica**, com conte√∫do suficiente para voc√™ ingressar no mercado de trabalho e continuar se aprimorando cada vez mais.  \n\nUtilizamos materiais diversos encontrados em **plataformas gratuitas**, o que permite aprender sem gastar nada. Acreditamos que o acesso √† educa√ß√£o deve ser poss√≠vel para todos.  \n\nO conte√∫do do curso pode ser feito **individualmente ou em grupo** ‚Äî sinta-se livre para estudar no seu tempo e do jeito que achar melhor!  \n\nTodos os cursos escolhidos aqui s√£o conhecidos por sua **qualidade e f√°cil entendimento**. Este projeto ser√° constantemente aprimorado ao longo do tempo.\n\nEste curso n√£o descarta a possibilidade de se aprofundar nos t√≥picos apresentados ainda mais, recomendamos que o fa√ßa.\n\nRecomenda-se a cria√ß√£o de atividades e desafios sobre os topicos estudados para melhor entendimento.\n\n**Bons estudos !!!**  \n\n## üó∫Ô∏è **Estrutura do Curso**\n\n| N√≠vel | Foco | Dura√ß√£o Estimada | Status |\n|-------|------|------------------|--------|\n| [**0Ô∏è‚É£ Soft Skills**](#0Ô∏è‚É£-n√≠vel-0---soft-skills) | Comunica√ß√£o, Lideran√ßa, Pensamento Cr√≠tico | 2-3 semanas | ‚úÖ |\n| [**1Ô∏è‚É£ Hardware**](#1Ô∏è‚É£-n√≠vel-1---hardware) | Fundamentos de Hardware e Arquitetura | 2-3 semanas | ‚úÖ |\n| [**2Ô∏è‚É£ Sistemas Operacionais**](#2Ô∏è‚É£-n√≠vel-2---sistemas-operacionais) | Windows, Linux, Virtualiza√ß√£o | 4-6 semanas | ‚úÖ |\n| [**3Ô∏è‚É£ Redes**](#3Ô∏è‚É£-n√≠vel-3---redes-de-computadores) | TCP/IP, Protocolos, Infraestrutura | 6-8 semanas | ‚úÖ |\n| [**4Ô∏è‚É£ Programa√ß√£o**](#4Ô∏è‚É£-n√≠vel-4---programa√ß√£o) | Python, Automa√ß√£o, Scripting | 8-10 semanas | ‚úÖ |\n| [**5Ô∏è‚É£ Seguran√ßa Cibern√©tica**](#5Ô∏è‚É£-n√≠vel-5---seguran√ßa-cibern√©tica) | Red/Blue Team, SOC, Pentest | 12+ semanas | ‚úÖ |\n\n---\n\n## **0Ô∏è‚É£ N√≠vel 0 - Soft Skills**\n\n**üéØ Objetivo:** Desenvolver habilidades interpessoais essenciais para o sucesso profissional.\n\nCompet√™ncias t√©cnicas s√£o fundamentais, mas soft skills fazem a diferen√ßa na carreira. Este n√≠vel desenvolve comunica√ß√£o, lideran√ßa, pensamento cr√≠tico e intelig√™ncia emocional.\n\n### üì∫ V√≠deos/Playlists\n\n| N¬∫ | Conte√∫do | Canal | Descri√ß√£o |\n|----|----------|-------|-----------|\n| 1 | [**Aulas de Filosofia**](https://youtube.com/playlist?list=PLhuwT6UtzNbnkUlRUZvY6DBgr9LInBtrZ&si=yg5hFpEGxui1FW8d) | Isto n√£o √© Filosofia | Pensamento cr√≠tico e an√°lise l√≥gica atrav√©s da filosofia |\n| 2 | [**T√©cnicas de Vendas e Negocia√ß√£o**](https://youtube.com/playlist?list=PL5lL7Dm426seGa5Hmi1jIu0p8OYtNs2Xm&si=5D_Xs-e81_ZzBsE7) | Profissional Marketing | Persuas√£o, negocia√ß√£o e fechamento de neg√≥cios |\n| 3 | [**Apresenta√ß√£o de Slides**](https://youtube.com/playlist?list=PL6OEUipU7xC7vCVVaOFikOjVqf1BhTVR0&si=6vfhiXAG4Dtpy6Uv) | TechEnfim | Cria√ß√£o de apresenta√ß√µes impactantes |\n| 4 | [**Comunica√ß√£o e Orat√≥ria**](https://youtube.com/playlist?list=PLF0WnKFaIjV2s2-vU2up7-cqpOACSx73v&si=KUKu_te4h9HpLadg) | El Professor da Orat√≥ria | Desenvolvimento da comunica√ß√£o verbal |\n| 5 | [**Networking Profissional**](https://youtu.be/jr8rv8JCcI0) | Jovens de Neg√≥cios | Constru√ß√£o de relacionamentos estrat√©gicos |\n| 6 | [**Constru√ß√£o da Identidade**](https://youtube.com/playlist?list=PLwinAdFkfTrVLsK1yBMkmf8SkTg-aR7r-&si=Jt1sK4TkqNiUvfDs) | Projeto C√£o Pastor | Autoconhecimento e desenvolvimento pessoal |\n| 7 | [**Performance e Foco**](https://youtube.com/playlist?list=PLwinAdFkfTrU2l2iQuvw6U-n2xTdDWUZZ&si=F3aBeV1A1paIN0Yl) | Projeto C√£o Pastor | Disciplina e alta performance |\n| 8 | [**Filosofia Aplicada**](https://youtube.com/playlist?list=PLN50oHosyDdg4P68_1pzRMqHpUnSfJGEF&si=JgmK4wbYFAS5ZxRT) | Cl√≥vis de Barros Filho | √âtica e reflex√µes pr√°ticas |\n\n### üéì Cursos Estruturados\n\n| N¬∫ | Curso | Plataforma | Descri√ß√£o |\n|----|-------|------------|-----------|\n| 1 | [**Vendas: Prospec√ß√£o ao Fechamento**](https://kultivi.com/cursos/negocios/vendas-da-prospeccao-ao-fechamento) | Kultivi | T√©cnicas completas de vendas |\n| 2 | [**Escrita Criativa**](https://kultivi.com/cursos/negocios/curso-escrita-criativa) | Kultivi | Melhore sua comunica√ß√£o escrita |\n| 3 | [**Networking Estrat√©gico**](https://kultivi.com/cursos/negocios/conexoes-estrategicas-e-networking-intencional) | Kultivi | Relacionamentos profissionais eficazes |\n| 4 | [**Gest√£o de Compras**](https://kultivi.com/cursos/negocios/compras-iniciando-uma-carreira-de-sucesso) | Kultivi | Negocia√ß√£o e gest√£o estrat√©gica |\n| 5 | [**Social Media Marketing**](https://kultivi.com/cursos/negocios/social-media) | Kultivi | Marketing digital e presen√ßa online |\n| 6 | [**Orat√≥ria de Alta Performance**](https://kultivi.com/cursos/negocios/oratoria-de-alta-performance) | Kultivi | T√©cnicas avan√ßadas de a",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-17T13:42:48.735769"
  },
  {
    "basic_info": {
      "name": "cartpole-ts",
      "full_name": "haydenkz/cartpole-ts",
      "owner": "haydenkz",
      "description": "gpt-5 codex cartpole ",
      "url": "https://github.com/haydenkz/cartpole-ts",
      "clone_url": "https://github.com/haydenkz/cartpole-ts.git",
      "ssh_url": "git@github.com:haydenkz/cartpole-ts.git",
      "homepage": "",
      "created_at": "2025-09-16T17:44:34Z",
      "updated_at": "2025-09-17T13:09:14Z",
      "pushed_at": "2025-09-17T09:19:43Z"
    },
    "stats": {
      "stars": 23,
      "forks": 5,
      "watchers": 23,
      "open_issues": 0,
      "size": 62
    },
    "tech_info": {
      "language": "TypeScript",
      "languages": {
        "TypeScript": 29448,
        "JavaScript": 605,
        "CSS": 550
      },
      "license": null,
      "topics": []
    },
    "content": {
      "readme": "GPT-5 Codex made this, don't expect bug fixes or upgrades ",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-17T13:42:50.024011"
  },
  {
    "basic_info": {
      "name": "ComfyUI-QI-QwenEditSafe",
      "full_name": "wallen0322/ComfyUI-QI-QwenEditSafe",
      "owner": "wallen0322",
      "description": "ComfyUI-QI-QwenEditSafe",
      "url": "https://github.com/wallen0322/ComfyUI-QI-QwenEditSafe",
      "clone_url": "https://github.com/wallen0322/ComfyUI-QI-QwenEditSafe.git",
      "ssh_url": "git@github.com:wallen0322/ComfyUI-QI-QwenEditSafe.git",
      "homepage": null,
      "created_at": "2025-09-16T09:17:36Z",
      "updated_at": "2025-09-17T07:33:18Z",
      "pushed_at": "2025-09-17T06:54:20Z"
    },
    "stats": {
      "stars": 23,
      "forks": 1,
      "watchers": 23,
      "open_issues": 0,
      "size": 83
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 15523
      },
      "license": null,
      "topics": []
    },
    "content": {
      "readme": "# ComfyUI-QI-QwenEditSafe\n\n[‰∏≠ÊñáÁâàÊú¨](README.zh.md) | English\n\n## Quick Guide (EN)\n- No‚Äëresize grid padding; CLIP & VAE share the same source.\n- `reference_pixels` are **VAE recon** (same domain). Use `prompt_emphasis` to trade text adherence vs. reference strength.\n- Nodes: TextEncodeQwenImageEdit (EN), ÊñáÁîüÂõæÁºñËæë (CN), VAE Decode.\n\n**Wiring**: `CLIP/IMAGE/VAE` ‚Üí TextEncodeQwenImageEdit ‚Üí `(conditioning, image, latent)` ‚Üí sampler ‚Üí VAE Decode.\n\n**Key params**:\n- inject_mode: both / latents / pixels (default both)\n- pixels_source: recon (default) / input\n- pixels_shaping: colorfield_64 (default) / colorfield_32 / full\n- prompt_emphasis: 0‚Äì1 (default 0.5; ‚â•0.8 = strong text), enable mild high-frequency boost (radius 5, amount 0.10‚Äì0.15), and increase target total pixels when possible.\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-17T13:42:51.316055"
  }
]