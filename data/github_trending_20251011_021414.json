[
  {
    "basic_info": {
      "name": "chrome-devtools-mcp",
      "full_name": "ChromeDevTools/chrome-devtools-mcp",
      "owner": "ChromeDevTools",
      "description": "Chrome DevTools for coding agents",
      "url": "https://github.com/ChromeDevTools/chrome-devtools-mcp",
      "clone_url": "https://github.com/ChromeDevTools/chrome-devtools-mcp.git",
      "ssh_url": "git@github.com:ChromeDevTools/chrome-devtools-mcp.git",
      "homepage": "https://npmjs.org/package/chrome-devtools-mcp",
      "created_at": "2025-09-11T10:39:55Z",
      "updated_at": "2025-10-11T02:14:00Z",
      "pushed_at": "2025-10-10T15:13:58Z"
    },
    "stats": {
      "stars": 10276,
      "forks": 572,
      "watchers": 10276,
      "open_issues": 38,
      "size": 1470
    },
    "tech_info": {
      "language": "TypeScript",
      "languages": {
        "TypeScript": 219424,
        "JavaScript": 6211
      },
      "license": "Apache License 2.0",
      "topics": [
        "browser",
        "chrome",
        "chrome-devtools",
        "debugging",
        "devtools",
        "mcp",
        "mcp-server",
        "puppeteer"
      ]
    },
    "content": {
      "readme": "# Chrome DevTools MCP\n\n[![npm chrome-devtools-mcp package](https://img.shields.io/npm/v/chrome-devtools-mcp.svg)](https://npmjs.org/package/chrome-devtools-mcp)\n\n`chrome-devtools-mcp` lets your coding agent (such as Gemini, Claude, Cursor or Copilot)\ncontrol and inspect a live Chrome browser. It acts as a Model-Context-Protocol\n(MCP) server, giving your AI coding assistant access to the full power of\nChrome DevTools for reliable automation, in-depth debugging, and performance analysis.\n\n## [Tool reference](./docs/tool-reference.md) | [Changelog](./CHANGELOG.md) | [Contributing](./CONTRIBUTING.md) | [Troubleshooting](./docs/troubleshooting.md)\n\n## Key features\n\n- **Get performance insights**: Uses [Chrome\n  DevTools](https://github.com/ChromeDevTools/devtools-frontend) to record\n  traces and extract actionable performance insights.\n- **Advanced browser debugging**: Analyze network requests, take screenshots and\n  check the browser console.\n- **Reliable automation**. Uses\n  [puppeteer](https://github.com/puppeteer/puppeteer) to automate actions in\n  Chrome and automatically wait for action results.\n\n## Disclaimers\n\n`chrome-devtools-mcp` exposes content of the browser instance to the MCP clients\nallowing them to inspect, debug, and modify any data in the browser or DevTools.\nAvoid sharing sensitive or personal information that you don't want to share with\nMCP clients.\n\n## Requirements\n\n- [Node.js](https://nodejs.org/) v20.19 or a newer [latest maintenance LTS](https://github.com/nodejs/Release#release-schedule) version.\n- [Chrome](https://www.google.com/chrome/) current stable version or newer.\n- [npm](https://www.npmjs.com/).\n\n## Getting started\n\nAdd the following config to your MCP client:\n\n```json\n{\n  \"mcpServers\": {\n    \"chrome-devtools\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"chrome-devtools-mcp@latest\"]\n    }\n  }\n}\n```\n\n> [!NOTE]  \n> Using `chrome-devtools-mcp@latest` ensures that your MCP client will always use the latest version of the Chrome DevTools MCP server.\n\n### MCP Client configuration\n\n<details>\n  <summary>Claude Code</summary>\n    Use the Claude Code CLI to add the Chrome DevTools MCP server (<a href=\"https://docs.anthropic.com/en/docs/claude-code/mcp\">guide</a>):\n\n```bash\nclaude mcp add chrome-devtools npx chrome-devtools-mcp@latest\n```\n\n</details>\n\n<details>\n  <summary>Cline</summary>\n  Follow https://docs.cline.bot/mcp/configuring-mcp-servers and use the config provided above.\n</details>\n\n<details>\n  <summary>Codex</summary>\n  Follow the <a href=\"https://github.com/openai/codex/blob/main/docs/advanced.md#model-context-protocol-mcp\">configure MCP guide</a>\n  using the standard config from above. You can also install the Chrome DevTools MCP server using the Codex CLI:\n\n```bash\ncodex mcp add chrome-devtools -- npx chrome-devtools-mcp@latest\n```\n\n**On Windows 11**\n\nConfigure the Chrome install location and increase the startup timeout by updating `.codex/config.toml` and adding the following `env` and `startup_timeout_ms` parameters:\n\n```\n[mcp_servers.chrome-devtools]\ncommand = \"cmd\"\nargs = [\n    \"/c\",\n    \"npx\",\n    \"-y\",\n    \"chrome-devtools-mcp@latest\",\n]\nenv = { SystemRoot=\"C:\\\\Windows\", PROGRAMFILES=\"C:\\\\Program Files\" }\nstartup_timeout_ms = 20_000\n```\n\n</details>\n\n<details>\n  <summary>Copilot CLI</summary>\n\nStart Copilot CLI:\n\n```\ncopilot\n```\n\nStart the dialog to add a new MCP server by running:\n\n```\n/mcp add\n```\n\nConfigure the following fields and press `CTRL+S` to save the configuration:\n\n- **Server name:** `chrome-devtools`\n- **Server Type:** `[1] Local`\n- **Command:** `npx`\n- **Arguments:** `-y, chrome-devtools-mcp@latest`\n\n</details>\n\n<details>\n  <summary>Copilot / VS Code</summary>\n  Follow the MCP install <a href=\"https://code.visualstudio.com/docs/copilot/chat/mcp-servers#_add-an-mcp-server\">guide</a>,\n  with the standard config from above. You can also install the Chrome DevTools MCP server using the VS Code CLI:\n  \n  ```bash\n  code --add-mcp '{\"name\":\"chrome-devtools\",\"command\":\"npx\",\"args\":[\"chrome-devtools-mcp@latest\"]}'\n  ```\n</details>\n\n<details>\n  <summary>Cursor</summary>\n\n**Click the button to install:**\n\n[<img src=\"https://cursor.com/deeplink/mcp-install-dark.svg\" alt=\"Install in Cursor\">](https://cursor.com/en/install-mcp?name=chrome-devtools&config=eyJjb21tYW5kIjoibnB4IC15IGNocm9tZS1kZXZ0b29scy1tY3BAbGF0ZXN0In0%3D)\n\n**Or install manually:**\n\nGo to `Cursor Settings` -> `MCP` -> `New MCP Server`. Use the config provided above.\n\n</details>\n\n<details>\n  <summary>Gemini CLI</summary>\nInstall the Chrome DevTools MCP server using the Gemini CLI.\n\n**Project wide:**\n\n```bash\ngemini mcp add chrome-devtools npx chrome-devtools-mcp@latest\n```\n\n**Globally:**\n\n```bash\ngemini mcp add -s user chrome-devtools npx chrome-devtools-mcp@latest\n```\n\nAlternatively, follow the <a href=\"https://github.com/google-gemini/gemini-cli/blob/main/docs/tools/mcp-server.md#how-to-set-up-your-mcp-server\">MCP guide</a> and use the standard config from above.\n\n</details>\n\n<details>\n  <summary>Gem",
      "default_branch": "main"
    },
    "fetched_at": "2025-10-11T02:14:15.848527"
  },
  {
    "basic_info": {
      "name": "Dayflow",
      "full_name": "JerryZLiu/Dayflow",
      "owner": "JerryZLiu",
      "description": "Generate a timeline of your day, automatically",
      "url": "https://github.com/JerryZLiu/Dayflow",
      "clone_url": "https://github.com/JerryZLiu/Dayflow.git",
      "ssh_url": "git@github.com:JerryZLiu/Dayflow.git",
      "homepage": "",
      "created_at": "2025-09-23T01:58:21Z",
      "updated_at": "2025-10-11T01:54:21Z",
      "pushed_at": "2025-10-10T20:25:37Z"
    },
    "stats": {
      "stars": 3238,
      "forks": 132,
      "watchers": 3238,
      "open_issues": 7,
      "size": 41876
    },
    "tech_info": {
      "language": "Swift",
      "languages": {
        "Swift": 850152,
        "Shell": 27102
      },
      "license": "MIT License",
      "topics": [
        "gemini",
        "lmstudio",
        "ollama",
        "productivity",
        "productivity-tools",
        "swift",
        "time",
        "timeline"
      ]
    },
    "content": {
      "readme": "\n<div align=\"center\">\n  <img src=\"docs/images/dayflow_header.png\" alt=\"Dayflow\" width=\"400\">\n</div>\n\n<div align=\"center\">\n  <em>A timeline of your day, automatically.</em><br>\n  Turns your screen activity into a clean timeline with AI summaries and distraction highlights.\n</div>\n\n<div align=\"center\">\n  <!-- Badges -->\n  <img src=\"https://img.shields.io/badge/macOS-13%2B-000?logo=apple\" alt=\"Platform: macOS 13+\">\n  <img src=\"https://img.shields.io/badge/SwiftUI-✓-orange\" alt=\"SwiftUI\">\n  <img src=\"https://img.shields.io/badge/Updates-Sparkle-informational\" alt=\"Updates: Sparkle\">\n  <img src=\"https://img.shields.io/badge/AI-Gemini%20or%20Local-blue\" alt=\"AI: Gemini / Local\">\n  <img src=\"https://img.shields.io/badge/License-MIT-green\" alt=\"License: MIT\">\n</div>\n\n<div align=\"center\">\n  <img src=\"docs/images/hero_animation_1080p.gif\" alt=\"Dayflow Hero Animation\" width=\"800\">\n</div>\n\n<div align=\"center\">\n  <a href=\"https://github.com/JerryZLiu/Dayflow/releases/latest\">\n    <img src=\"https://img.shields.io/badge/Download%20for%20Mac-⬇%20%20Dayflow.dmg-blue?style=for-the-badge&logo=apple\" alt=\"Download for Mac\">\n  </a>\n</div>\n\n<p align=\"center\">\n  <a href=\"#quickstart\">Quickstart</a> •\n  <a href=\"#why-i-built-dayflow\">Why I built Dayflow</a> •\n  <a href=\"#features\">Features</a> •\n  <a href=\"#how-it-works\">How it works</a> •\n  <a href=\"#installation\">Installation</a> •\n  <a href=\"#data--privacy\">Data & Privacy</a> •\n  <a href=\"#debug--developer-tools\">Debug & Developer Tools</a> •\n  <a href=\"#auto-updates-sparkle\">Auto‑updates</a> •\n  <a href=\"#contributing\">Contributing</a>\n</p>\n\n---\n\n## What is Dayflow?\n\nDayflow is a **native macOS app** (SwiftUI) that records your screen at **1 FPS**, analyzes it **every 15 minutes** with AI, and generates a **timeline** of your activities with summaries. \nIt's lightweight (25MB app size) and uses ~100MB of RAM and <1% cpu. \n\n> _Privacy‑minded by design_: You choose your AI provider. Use **Gemini** (bring your own API key) or **local models** (Ollama / LM Studio). See **Data & Privacy** for details.\n\n\n## Why I built Dayflow\n\nI built Dayflow after realizing that my calendar wasn't the source of truth for how I actually spent my time. My screen was. I wanted a calm, trustworthy timeline that let me see my workday without turning into yet another dashboard I had to maintain.\n\nDayflow stands for ownership and privacy by default. You control the data, you choose the AI provider, and you can keep everything local if that's what makes you comfortable. It's MIT licensed and fully open source because anything that watches your screen all day should be completely transparent about what it does with that information. The app should feel like a quiet assistant: respectful of your attention, honest about what it captures, and easy to shut off.\n\n\n---\n\n## Features\n\n- **Automatic timeline** of your day with concise summaries.\n- **1 FPS recording** - minimal CPU/storage impact.\n- **15-minute analysis intervals** for timely updates.\n- **Watch timelapses of your day**.\n- **Auto storage cleanup** - removes old recordings after 3 days.\n- **Distraction highlights** to see what pulled you off‑task.\n- **Native UX** built with **SwiftUI**.\n- **Auto‑updates** with **Sparkle** (daily check + background download).\n\n### Coming soon\n\n- **Infinitely customizable dashboard** — ask any question about your workday, pipe the answers into tiles you arrange yourself, and track trends over time.\n\n  <div align=\"center\">\n    <img src=\"docs/images/DashboardPreview.png\" alt=\"Dayflow dashboard preview\" width=\"800\">\n  </div>\n\n- **Daily journal** — review the highlights Dayflow captured, reflect with guided prompts, and drop screenshots or notes alongside your generated timeline.\n\n  <div align=\"center\">\n    <img src=\"docs/images/JournalPreview.png\" alt=\"Dayflow journal preview\" width=\"800\">\n  </div>\n\n## How it works\n\n1) **Capture** — Records screen at 1 FPS in 15-second chunks.\n2) **Analyze** — Every 15 minutes, sends recent footage to AI.\n3) **Generate** — AI creates timeline cards with activity summaries.\n4) **Display** — Shows your day as a visual timeline.\n5) **Cleanup** — Auto-deletes recordings older than 3 days.\n\n### AI Processing Pipeline\n\nThe efficiency of your timeline generation depends on your chosen AI provider:\n\n```mermaid\nflowchart LR\n    subgraph Gemini[\"Gemini Flow: 2 LLM Calls\"]\n        direction LR\n        GV[Video] --> GU[Upload + Transcribe<br/>1 LLM call] --> GC[Generate Cards<br/>1 LLM call] --> GD[Done]\n    end\n\n    subgraph Local[\"Local Flow: 33+ LLM Calls\"]\n        direction LR\n        LV[Video] --> LE[Extract 30 frames] --> LD[30 descriptions<br/>30 LLM calls] --> LM[Merge<br/>1 call] --> LT[Title<br/>1 call] --> LC[Merge Check<br/>1 call] --> LMC[Merge Cards<br/>1 call] --> LD2[Done]\n    end\n\n    %% Styling\n    classDef geminiFlow fill:#e8f5e8,stroke:#4caf50,stroke-width:2px\n    classDef localFlow fill:#fff8e1,stroke:#ff9800,stroke-width:2px\n    classDef geminiStep fill:#4caf50,color:#fff\n    cla",
      "default_branch": "main"
    },
    "fetched_at": "2025-10-11T02:14:17.113532"
  },
  {
    "basic_info": {
      "name": "TinyRecursiveModels",
      "full_name": "SamsungSAILMontreal/TinyRecursiveModels",
      "owner": "SamsungSAILMontreal",
      "description": null,
      "url": "https://github.com/SamsungSAILMontreal/TinyRecursiveModels",
      "clone_url": "https://github.com/SamsungSAILMontreal/TinyRecursiveModels.git",
      "ssh_url": "git@github.com:SamsungSAILMontreal/TinyRecursiveModels.git",
      "homepage": null,
      "created_at": "2025-10-07T13:24:28Z",
      "updated_at": "2025-10-11T02:14:06Z",
      "pushed_at": "2025-10-08T19:46:47Z"
    },
    "stats": {
      "stars": 3076,
      "forks": 341,
      "watchers": 3076,
      "open_issues": 9,
      "size": 1266
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 147529
      },
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "# Less is More: Recursive Reasoning with Tiny Networks\n\nThis is the codebase for the paper: \"Less is More: Recursive Reasoning with Tiny Networks\". TRM is a recursive reasoning approach that achieves amazing scores of 45% on ARC-AGI-1 and 8% on ARC-AGI-2 using a tiny 7M parameters neural network.\n\n[Paper](https://arxiv.org/abs/2510.04871)\n\n### Motivation\n\nTiny Recursion Model (TRM) is a recursive reasoning model that achieves amazing scores of 45% on ARC-AGI-1 and 8% on ARC-AGI-2 with a tiny 7M parameters neural network. The idea that one must rely on massive foundational models trained for millions of dollars by some big corporation in order to achieve success on hard tasks is a trap. Currently, there is too much focus on exploiting LLMs rather than devising and expanding new lines of direction. With recursive reasoning, it turns out that “less is more”: you don’t always need to crank up model size in order for a model to reason and solve hard problems. A tiny model pretrained from scratch, recursing on itself and updating its answers over time, can achieve a lot without breaking the bank.\n\nThis work came to be after I learned about the recent innovative Hierarchical Reasoning Model (HRM). I was amazed that an approach using small models could do so well on hard tasks like the ARC-AGI competition (reaching 40% accuracy when normally only Large Language Models could compete). But I kept thinking that it is too complicated, relying too much on biological arguments about the human brain, and that this recursive reasoning process could be greatly simplified and improved. Tiny Recursion Model (TRM) simplifies recursive reasoning to its core essence, which ultimately has nothing to do with the human brain, does not require any mathematical (fixed-point) theorem, nor any hierarchy.\n\n### How TRM works\n\n<p align=\"center\">\n  <img src=\"https://AlexiaJM.github.io/assets/images/TRM_fig.png\" alt=\"TRM\"  style=\"width: 30%;\">\n</p>\n\nTiny Recursion Model (TRM) recursively improves its predicted answer y with a tiny network. It starts with the embedded input question x and initial embedded answer y and latent z. For up to K improvements steps, it tries to improve its answer y. It does so by i) recursively updating n times its latent z given the question x, current answer y, and current latent z (recursive reasoning), and then ii) updating its answer y given the current answer y and current latent z. This recursive process allows the model to progressively improve its answer (potentially addressing any errors from its previous answer) in an extremely parameter-efficient manner while minimizing overfitting.\n\n### Requirements\n\n- Python 3.10 (or similar)\n- Cuda 12.6.0 (or similar)\n\n```bash\npip install --upgrade pip wheel setuptools\npip install --pre --upgrade torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu126 # install torch based on your cuda version\npip install -r requirements.txt # install requirements\npip install --no-cache-dir --no-build-isolation adam-atan2 \nwandb login YOUR-LOGIN # login if you want the logger to sync results to your Weights & Biases (https://wandb.ai/)\n```\n\n### Dataset Preparation\n\n```bash\n# ARC-AGI-1\npython -m dataset.build_arc_dataset \\\n  --input-file-prefix kaggle/combined/arc-agi \\\n  --output-dir data/arc1concept-aug-1000 \\\n  --subsets training evaluation concept \\\n  --test-set-name evaluation\n\n# ARC-AGI-2\npython -m dataset.build_arc_dataset \\\n  --input-file-prefix kaggle/combined/arc-agi \\\n  --output-dir data/arc2concept-aug-1000 \\\n  --subsets training2 evaluation2 concept \\\n  --test-set-name evaluation2\n\n## Note: You cannot train on both ARC-AGI-1 and ARC-AGI-2 and evaluate them both because ARC-AGI-2 training data contains some ARC-AGI-1 eval data\n\n# Sudoku-Extreme\npython dataset/build_sudoku_dataset.py --output-dir data/sudoku-extreme-1k-aug-1000  --subsample-size 1000 --num-aug 1000  # 1000 examples, 1000 augments\n\n# Maze-Hard\npython dataset/build_maze_dataset.py # 1000 examples, 8 augments\n```\n\n## Experiments\n\n### ARC-AGI-1 (assuming 4 H-100 GPUs):\n\n```bash\nrun_name=\"pretrain_att_arc1concept_4\"\ntorchrun --nproc-per-node 4 --rdzv_backend=c10d --rdzv_endpoint=localhost:0 --nnodes=1 pretrain.py \\\narch=trm \\\ndata_paths=\"[data/arc1concept-aug-1000]\" \\\narch.L_layers=2 \\\narch.H_cycles=3 arch.L_cycles=4 \\\n+run_name=${run_name} ema=True\n\n```\n\n*Runtime:* ~3 days\n\n### ARC-AGI-2 (assuming 4 H-100 GPUs):\n\n```bash\nrun_name=\"pretrain_att_arc2concept_4\"\ntorchrun --nproc-per-node 4 --rdzv_backend=c10d --rdzv_endpoint=localhost:0 --nnodes=1 pretrain.py \\\narch=trm \\\ndata_paths=\"[data/arc2concept-aug-1000]\" \\\narch.L_layers=2 \\\narch.H_cycles=3 arch.L_cycles=4 \\\n+run_name=${run_name} ema=True\n\n```\n\n*Runtime:* ~3 days\n\n### Sudoku-Extreme (assuming 1 L40S GPU):\n\n```bash\nrun_name=\"pretrain_mlp_t_sudoku\"\npython pretrain.py \\\narch=trm \\\ndata_paths=\"[data/sudoku-extreme-1k-aug-1000]\" \\\nevaluators=\"[]\" \\\nepochs=50000 eval_interval=5000 \\\nlr=1e-4 puzzle_emb_lr=1e-4 weight_decay=1.0 puzzle",
      "default_branch": "main"
    },
    "fetched_at": "2025-10-11T02:14:18.371118"
  },
  {
    "basic_info": {
      "name": "bdh",
      "full_name": "pathwaycom/bdh",
      "owner": "pathwaycom",
      "description": "Baby Dragon Hatchling (BDH) – Architecture and Code",
      "url": "https://github.com/pathwaycom/bdh",
      "clone_url": "https://github.com/pathwaycom/bdh.git",
      "ssh_url": "git@github.com:pathwaycom/bdh.git",
      "homepage": "",
      "created_at": "2025-09-30T12:05:01Z",
      "updated_at": "2025-10-11T00:04:17Z",
      "pushed_at": "2025-10-07T14:53:05Z"
    },
    "stats": {
      "stars": 3006,
      "forks": 96,
      "watchers": 3006,
      "open_issues": 2,
      "size": 999
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 8722
      },
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "## Baby Dragon Hatchling\nThis repository contains source code from the paper: Adrian Kosowski, Przemysław Uznański, Jan Chorowski, Zuzanna Stamirowska, Michał Bartoszkiewicz, _\"The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain\"_, [link](https://doi.org/10.48550/arXiv.2509.26507).\n\n## Architecture\n<img src=\"figs/architecture.png\" width=\"600\"/> \n\n## Relation to Tranformers\n<img src=\"figs/vocab.png\" width=\"600\"/> \n\n## Scaling laws\n<img src=\"figs/bdh_scaling.png\" width=\"600\"/> \n\n## Abstract:\nThe relationship between computing systems and the brain has served as motivation for pioneering theoreticians since John von Neumann and Alan Turing. \nUniform, scale-free biological networks, such as the brain, have powerful properties, including generalizing over time, which is the main barrier for Machine Learning on the path to Universal Reasoning Models.\n\nWe introduce `Dragon Hatchling' (BDH), a new Large Language Model architecture based on a scale-free biologically inspired network of $n$ locally-interacting neuron particles. BDH couples strong theoretical foundations and inherent interpretability without sacrificing Transformer-like performance.\n\nBDH is a practical, performant state-of-the-art \nattention-based state space sequence learning architecture. \nIn addition to being a graph model, BDH admits a GPU-friendly formulation.\nIt exhibits Transformer-like scaling laws: we find empirically that BDH rivals GPT2-architecture Transformer performance on language and translation tasks, at the same number of parameters (10M to 1B), for the same training data.\n\nBDH provides theoretical foundations for understanding model behavior in the limit of large size and reasoning time. \nOur results, formalized as a chain of reductions of expressiveness in the framework of computational Complexity Theory and Distributed Computing, and combined with findings on the BDH model, show a macro-to-micro correspondence of function between the general attention mechanisms in state-of-the-art Language Models, and attention mechanisms observed in the brain. These attention mechanisms formally converge as closed-form local graph dynamics at neurons and synapses: _the equations of reasoning_.\n\nBDH can be represented as a brain model. It contains $n$ neurons, organized as an excitatory circuit and an inhibitory circuit with integrate-and-fire thresholding of input signals at neurons. The working memory of BDH during inference entirely relies on synaptic plasticity with Hebbian learning using spiking neurons, at potentiation scales of minutes for the brain (up to hundreds of tokens). We confirm empirically that specific, individual synapses strengthen connection whenever BDH hears or reasons about a specific concept while processing language inputs. The neuron interaction network of BDH is a graph of high modularity with heavy-tailed degree distribution. The BDH model is biologically plausible, explaining one possible mechanism which human neurons could use to achieve speech.\n\nBDH is designed for interpretability. Activation vectors of BDH are sparse and positive. We demonstrate monosemanticity in BDH on language tasks, including representation of concept abstractions, which happens even for small models, below 100M-parameter scale. Interpretability of state, which goes beyond interpretability of neurons and model parameters, is an inherent feature of the BDH architecture. \n\nWe believe BDH opens the door to a new theory of _Thermodynamic Limit_ behavior for language and reasoning models, with the ultimate goal of Probably Approximately Correct (PAC)-like bounds for generalization of reasoning over time.\n\n## Running the code\n\nTo train and sample from the BDH model on a toy language modeling task please do:\n1. `pip install -r requirements.txt`\n2. `python train.py`\n\n## Acknowledgements\nWe thank Andrej Karpathy for the [nanoGPT](https://github.com/karpathy/nanoGPT/) code and the tiny Shapespeare dataset used in this demonstration.\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-10-11T02:14:19.645126"
  },
  {
    "basic_info": {
      "name": "RustGPT",
      "full_name": "tekaratzas/RustGPT",
      "owner": "tekaratzas",
      "description": "An transformer based LLM. Written completely in Rust",
      "url": "https://github.com/tekaratzas/RustGPT",
      "clone_url": "https://github.com/tekaratzas/RustGPT.git",
      "ssh_url": "git@github.com:tekaratzas/RustGPT.git",
      "homepage": null,
      "created_at": "2025-09-13T22:05:55Z",
      "updated_at": "2025-10-11T01:39:48Z",
      "pushed_at": "2025-10-10T19:47:38Z"
    },
    "stats": {
      "stars": 2836,
      "forks": 234,
      "watchers": 2836,
      "open_issues": 6,
      "size": 201
    },
    "tech_info": {
      "language": "Rust",
      "languages": {
        "Rust": 64634
      },
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "# 🦀 Rust LLM from Scratch\n\n[![Check](https://github.com/tekaratzas/RustGPT/actions/workflows/check.yml/badge.svg)](https://github.com/tekaratzas/RustGPT/actions/workflows/check.yml) [![Test](https://github.com/tekaratzas/RustGPT/actions/workflows/test.yml/badge.svg)](https://github.com/tekaratzas/RustGPT/actions/workflows/test.yml)\n\n\nhttps://github.com/user-attachments/assets/ec4a4100-b03a-4b3c-a7d6-806ea54ed4ed\n\nA complete **Large Language Model implementation in pure Rust** with no external ML frameworks. Built from the ground up using only `ndarray` for matrix operations.\n\n## 🚀 What This Is\n\nThis project demonstrates how to build a transformer-based language model from scratch in Rust, including:\n- **Pre-training** on factual text completion\n- **Instruction tuning** for conversational AI\n- **Interactive chat mode** for testing\n- **Full backpropagation** with gradient clipping\n- **Modular architecture** with clean separation of concerns\n\n## ❌ What This Isn't\n\nThis is not a production grade LLM. It is so far away from the larger models.\n\nThis is just a toy project that demonstrates how these models work under the hood.\n\n## 🔍 Key Files to Explore\n\nStart with these two core files to understand the implementation:\n\n- **[`src/main.rs`](src/main.rs)** - Training pipeline, data preparation, and interactive mode\n- **[`src/llm.rs`](src/llm.rs)** - Core LLM implementation with forward/backward passes and training logic\n\n## 🏗️ Architecture\n\nThe model uses a **transformer-based architecture** with the following components:\n\n```\nInput Text → Tokenization → Embeddings → Transformer Blocks → Output Projection → Predictions\n```\n\n### Project Structure\n\n```\nsrc/\n├── main.rs              # 🎯 Training pipeline and interactive mode\n├── llm.rs               # 🧠 Core LLM implementation and training logic\n├── lib.rs               # 📚 Library exports and constants\n├── transformer.rs       # 🔄 Transformer block (attention + feed-forward)\n├── self_attention.rs    # 👀 Multi-head self-attention mechanism\n├── feed_forward.rs      # ⚡ Position-wise feed-forward networks\n├── embeddings.rs        # 📊 Token embedding layer\n├── output_projection.rs # 🎰 Final linear layer for vocabulary predictions\n├── vocab.rs            # 📝 Vocabulary management and tokenization\n├── layer_norm.rs       # 🧮 Layer normalization\n└── adam.rs             # 🏃 Adam optimizer implementation\n\ntests/\n├── llm_test.rs         # Tests for core LLM functionality\n├── transformer_test.rs # Tests for transformer blocks\n├── self_attention_test.rs # Tests for attention mechanisms\n├── feed_forward_test.rs # Tests for feed-forward layers\n├── embeddings_test.rs  # Tests for embedding layers\n├── vocab_test.rs       # Tests for vocabulary handling\n├── adam_test.rs        # Tests for optimizer\n└── output_projection_test.rs # Tests for output layer\n```\n\n## 🧪 What The Model Learns\n\nThe implementation includes two training phases:\n\n1. **Pre-training**: Learns basic world knowledge from factual statements\n   - \"The sun rises in the east and sets in the west\"\n   - \"Water flows downhill due to gravity\"\n   - \"Mountains are tall and rocky formations\"\n\n2. **Instruction Tuning**: Learns conversational patterns\n   - \"User: How do mountains form? Assistant: Mountains are formed through tectonic forces...\"\n   - Handles greetings, explanations, and follow-up questions\n\n## 🚀 Quick Start\n\n```bash\n# Clone and run\ngit clone https://github.com/tekaratzas/RustGPT.git\ncd RustGPT\ncargo run\n\n# The model will:\n# 1. Build vocabulary from training data\n# 2. Pre-train on factual statements (100 epochs)\n# 3. Instruction-tune on conversational data (100 epochs)\n# 4. Enter interactive mode for testing\n```\n\n## 🎮 Interactive Mode\n\nAfter training, test the model interactively:\n\n```\nEnter prompt: How do mountains form?\nModel output: Mountains are formed through tectonic forces or volcanism over long geological time periods\n\nEnter prompt: What causes rain?\nModel output: Rain is caused by water vapor in clouds condensing into droplets that become too heavy to remain airborne\n```\n\n## 🧮 Technical Implementation\n\n### Model Configuration\n- **Vocabulary Size**: Dynamic (built from training data)\n- **Embedding Dimension**: 128 (defined by `EMBEDDING_DIM` in `src/lib.rs`)\n- **Hidden Dimension**: 256 (defined by `HIDDEN_DIM` in `src/lib.rs`)\n- **Max Sequence Length**: 80 tokens (defined by `MAX_SEQ_LEN` in `src/lib.rs`)\n- **Architecture**: 3 Transformer blocks + embeddings + output projection\n\n### Training Details\n- **Optimizer**: Adam with gradient clipping\n- **Pre-training LR**: 0.0005 (100 epochs)\n- **Instruction Tuning LR**: 0.0001 (100 epochs)\n- **Loss Function**: Cross-entropy loss\n- **Gradient Clipping**: L2 norm capped at 5.0\n\n### Key Features\n- **Custom tokenization** with punctuation handling\n- **Greedy decoding** for text generation\n- **Gradient clipping** for training stability\n- **Modular layer system** with clean interfaces\n- **Comprehensive test coverage** for all components\n\n## 🔧 Development\n\n```bash\n# Run ",
      "default_branch": "main"
    },
    "fetched_at": "2025-10-11T02:14:20.903799"
  },
  {
    "basic_info": {
      "name": "Mole",
      "full_name": "tw93/Mole",
      "owner": "tw93",
      "description": "🐹 Dig deep like a mole to clean you Mac. 像鼹鼠一样深入挖掘来清理你的 Mac",
      "url": "https://github.com/tw93/Mole",
      "clone_url": "https://github.com/tw93/Mole.git",
      "ssh_url": "git@github.com:tw93/Mole.git",
      "homepage": "",
      "created_at": "2025-09-23T06:38:40Z",
      "updated_at": "2025-10-11T02:03:45Z",
      "pushed_at": "2025-10-11T02:00:35Z"
    },
    "stats": {
      "stars": 2828,
      "forks": 94,
      "watchers": 2828,
      "open_issues": 0,
      "size": 325
    },
    "tech_info": {
      "language": "Shell",
      "languages": {
        "Shell": 279283
      },
      "license": "MIT License",
      "topics": [
        "clean",
        "cleaner",
        "cleaner-script",
        "macos"
      ]
    },
    "content": {
      "readme": "<div align=\"center\">\n  <h1>Mole</h1>\n  <p><em>Dig deep like a mole to clean your Mac.</em></p>\n</div>\n\n<p align=\"center\">\n  <a href=\"https://github.com/tw93/mole/stargazers\"><img src=\"https://img.shields.io/github/stars/tw93/mole?style=flat-square\" alt=\"Stars\"></a>\n  <a href=\"https://github.com/tw93/mole/releases\"><img src=\"https://img.shields.io/github/v/tag/tw93/mole?label=version&style=flat-square\" alt=\"Version\"></a>\n  <a href=\"LICENSE\"><img src=\"https://img.shields.io/badge/license-MIT-blue.svg?style=flat-square\" alt=\"License\"></a>\n  <a href=\"https://github.com/tw93/mole/commits\"><img src=\"https://img.shields.io/github/commit-activity/m/tw93/mole?style=flat-square\" alt=\"Commits\"></a>\n  <a href=\"https://twitter.com/HiTw93\"><img src=\"https://img.shields.io/badge/follow-Tw93-red?style=flat-square&logo=Twitter\" alt=\"Twitter\"></a>\n  <a href=\"https://t.me/+GclQS9ZnxyI2ODQ1\"><img src=\"https://img.shields.io/badge/chat-Telegram-blueviolet?style=flat-square&logo=Telegram\" alt=\"Telegram\"></a>\n</p>\n\n<p align=\"center\">\n  <img src=\"https://cdn.tw93.fun/img/mole.jpeg\" alt=\"Mole - 95.50GB freed\" width=\"800\" />\n</p>\n\n## Features\n\n- **Deep System Cleanup** - Cleans way more junk than CleanMyMac/Lemon - caches, logs, temp files\n- **Thorough Uninstall** - Scans 22+ locations to remove app leftovers, not just the .app file\n- **Interactive Disk Analyzer** - Navigate folders with arrow keys, find and delete large files quickly\n- **Fast & Lightweight** - Terminal-based with arrow-key navigation, pagination, and Touch ID support\n\n## Quick Start\n\n**Tips:**\n\n- If your Mac is mission-critical, consider waiting for Mole to mature further - safety first\n- 如果这台 Mac 对你非常重要，建议等 Mole 更成熟时来使用，安全第一\n\n**Install:**\n\n```bash\ncurl -fsSL https://raw.githubusercontent.com/tw93/mole/main/install.sh | bash\n```\n\nOr via Homebrew:\n\n```bash\nbrew install tw93/tap/mole\n```\n\n**Run:**\n\n```bash\nmo                      # Interactive menu\nmo clean                # System cleanup\nmo clean --dry-run      # Preview mode\nmo clean --whitelist    # Manage protected caches\nmo uninstall            # Uninstall apps\nmo analyze              # Disk analyzer\nmo update               # Update Mole\nmo remove               # Remove Mole from system\nmo --help               # Show help\nmo --version            # Show installed version\n```\n\n> Start with `mo clean --dry-run` to preview what will be deleted.\n>\n> Use `mo clean --whitelist` to protect important caches that are slow to re-download.\n\n## Features in Detail\n\n### Deep System Cleanup\n\n```bash\n$ mo clean\n\n▶ System essentials\n  ✓ User app cache (45.2GB)\n  ✓ User app logs (2.1GB)\n  ✓ Trash (12.3GB)\n\n▶ Browser cleanup\n  ✓ Chrome cache (8.4GB)\n  ✓ Safari cache (2.1GB)\n\n▶ Developer tools\n  ✓ Xcode derived data (9.1GB)\n  ✓ Node.js cache (14.2GB)\n\n▶ Others\n  ✓ Dropbox cache (5.2GB)\n  ✓ Spotify cache (3.1GB)\n\n====================================================================\nCLEANUP COMPLETE!\nSpace freed: 95.50GB | Free space now: 223.5GB\n====================================================================\n```\n\n### Smart App Uninstaller\n\n```bash\n$ mo uninstall\n\nSelect Apps to Remove\n═══════════════════════════\n▶ ☑ Adobe Creative Cloud      (12.4G) | Old\n  ☐ WeChat                    (2.1G) | Recent\n  ☐ Final Cut Pro             (3.8G) | Recent\n\nUninstalling: Adobe Creative Cloud\n  ✓ Removed application              # /Applications/\n  ✓ Cleaned 52 related files         # ~/Library/ across 12 locations\n    - Support files & caches         # Application Support, Caches\n    - Preferences & logs             # Preferences, Logs\n    - WebKit storage & cookies       # WebKit, HTTPStorages\n    - Extensions & plugins           # Internet Plug-Ins, Services\n    - System files with sudo         # /Library/, Launch daemons\n\n====================================================================\nUNINSTALLATION COMPLETE!\nSpace freed: 12.8GB\n====================================================================\n```\n\n### Disk Space Analyzer\n\n```bash\n$ mo analyze\n\nAnalyzing: /Users/You\n═══════════════════════════════════════════════════════\nTotal: 156.8GB\n\n├─ 📁 Library                                        45.2GB\n│  ├─ 📁 Caches                                      28.4GB\n│  └─ 📁 Application Support                         16.8GB\n├─ 📁 Downloads                                      32.6GB\n│  ├─ 📄 Xcode-14.3.1.dmg                            12.3GB\n│  ├─ 📄 backup_2023.zip                             8.6GB\n│  └─ 📄 old_projects.tar.gz                         5.2GB\n├─ 📁 Movies                                         28.9GB\n│  ├─ 📄 vacation_2023.mov                           15.4GB\n│  └─ 📄 screencast_raw.mp4                          8.8GB\n├─ 📁 Documents                                      18.4GB\n└─ 📁 Desktop                                        12.7GB\n```\n\n## FAQ\n\n- **Is Mole safe?** Mole only cleans caches and logs, it doesn't touch app settings, user documents, or system files. Start with `mo clean --dry-run` to preview what will be removed.\n\n- **How o",
      "default_branch": "main"
    },
    "fetched_at": "2025-10-11T02:14:22.160488"
  },
  {
    "basic_info": {
      "name": "neutts-air",
      "full_name": "neuphonic/neutts-air",
      "owner": "neuphonic",
      "description": "On-device TTS model by Neuphonic",
      "url": "https://github.com/neuphonic/neutts-air",
      "clone_url": "https://github.com/neuphonic/neutts-air.git",
      "ssh_url": "git@github.com:neuphonic/neutts-air.git",
      "homepage": null,
      "created_at": "2025-10-02T12:48:55Z",
      "updated_at": "2025-10-11T02:05:18Z",
      "pushed_at": "2025-10-10T13:57:44Z"
    },
    "stats": {
      "stars": 2751,
      "forks": 227,
      "watchers": 2751,
      "open_issues": 22,
      "size": 1912
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 14928
      },
      "license": "Apache License 2.0",
      "topics": []
    },
    "content": {
      "readme": "# NeuTTS Air ☁️\n\nHuggingFace 🤗: [Model](https://huggingface.co/neuphonic/neutts-air), [Q8 GGUF](https://huggingface.co/neuphonic/neutts-air-q8-gguf), [Q4 GGUF](https://huggingface.co/neuphonic/neutts-air-q4-gguf) [Spaces](https://huggingface.co/spaces/neuphonic/neutts-air)\n\n[Demo Video](https://github.com/user-attachments/assets/020547bc-9e3e-440f-b016-ae61ca645184)\n\n*Created by [Neuphonic](http://neuphonic.com/) - building faster, smaller, on-device voice AI*\n\nState-of-the-art Voice AI has been locked behind web APIs for too long. NeuTTS Air is the world’s first super-realistic, on-device, TTS speech language model with instant voice cloning. Built off a 0.5B LLM backbone, NeuTTS Air brings natural-sounding speech, real-time performance, built-in security and speaker cloning to your local device - unlocking a new category of embedded voice agents, assistants, toys, and compliance-safe apps.\n\n## Key Features\n\n- 🗣Best-in-class realism for its size - produces natural, ultra-realistic voices that sound human\n- 📱Optimised for on-device deployment - provided in GGML format, ready to run on phones, laptops, or even Raspberry Pis\n- 👫Instant voice cloning - create your own speaker with as little as 3 seconds of audio\n- 🚄Simple LM + codec architecture built off a 0.5B backbone - the sweet spot between speed, size, and quality for real-world applications\n\n> [!CAUTION]\n> Websites like neutts.com are popping up and they're not affliated with Neuphonic, our github or this repo.\n>\n> We are on neuphonic.com only. Please be careful out there! 🙏\n\n## Model Details\n\nNeuTTS Air is built off Qwen 0.5B - a lightweight yet capable language model optimised for text understanding and generation - as well as a powerful combination of technologies designed for efficiency and quality:\n- **Supported Languages**: English\n- **Audio Codec**: [NeuCodec](https://huggingface.co/neuphonic/neucodec) - our 50hz neural audio codec that achieves exceptional audio quality at low bitrates using a single codebook\n- **Context Window**: 2048 tokens, enough for processing ~30 seconds of audio (including prompt duration)\n- **Format**: Available in GGML format for efficient on-device inference\n- **Responsibility**: Watermarked outputs\n- **Inference Speed**: Real-time generation on mid-range devices\n- **Power Consumption**: Optimised for mobile and embedded devices\n\n## Get Started\n\n1. **Clone Git Repo**\n   ```bash\n   git clone https://github.com/neuphonic/neutts-air.git\n   ```\n   ```bash\n   cd neutts-air\n   ```\n\n2. **Install `espeak` (required dependency)**\n\n   Please refer to the following link for instructions on how to install `espeak`:\n\n   https://github.com/espeak-ng/espeak-ng/blob/master/docs/guide.md\n\n   ```bash\n   # Mac OS\n   brew install espeak\n\n   # Ubuntu/Debian\n   sudo apt install espeak\n   ```\n\n   Mac users may need to put the following lines at the top of the neutts.py file.\n   ```python\n   from phonemizer.backend.espeak.wrapper import EspeakWrapper\n   _ESPEAK_LIBRARY = '/opt/homebrew/Cellar/espeak/1.48.04_1/lib/libespeak.1.1.48.dylib'  #use the Path to the library.\n   EspeakWrapper.set_library(_ESPEAK_LIBRARY)\n   ```\n\n   Windows users may need to run (see https://github.com/bootphon/phonemizer/issues/163)\n   ```pwsh\n   $env:PHONEMIZER_ESPEAK_LIBRARY = \"c:\\Program Files\\eSpeak NG\\libespeak-ng.dll\"\n   $env:PHONEMIZER_ESPEAK_PATH = \"c:\\Program Files\\eSpeak NG\"\n   setx PHONEMIZER_ESPEAK_LIBRARY \"c:\\Program Files\\eSpeak NG\\libespeak-ng.dll\"\n   setx PHONEMIZER_ESPEAK_PATH \"c:\\Program Files\\eSpeak NG\"\n   ```\n\n3. **Install Python dependencies**\n\n   The requirements file includes the dependencies needed to run the model with PyTorch.\n   When using an ONNX decoder or a GGML model, some dependencies (such as PyTorch) are no longer required.\n\n   The inference is compatible and tested on `python>=3.11`.\n\n    ```\n    pip install -r requirements.txt\n    ```\n\n4. **(Optional) Install Llama-cpp-python to use the `GGUF` models.**\n   ```\n   pip install llama-cpp-python\n   ```\n   To run llama-cpp with GPU suport (CUDA, MPS) support please refer to:\n   https://pypi.org/project/llama-cpp-python/\n\n5. **(Optional) Install onnxruntime to use the `.onnx` decoder.**\n   If you want to run the onnxdecoder\n   ```\n   pip install onnxruntime\n   ```\n\n## Running the Model\n\nRun the basic example script to synthesize speech:\n```bash\npython -m examples.basic_example \\\n  --input_text \"My name is Dave, and um, I'm from London\" \\\n  --ref_audio samples/dave.wav \\\n  --ref_text samples/dave.txt\n```\n\nTo specify a particular model repo for the backbone or codec, add the `--backbone` argument. Available backbones are listed in [NeuTTS-Air huggingface collection](https://huggingface.co/collections/neuphonic/neutts-air-68cc14b7033b4c56197ef350).\n\nSeveral examples are available, including a Jupyter notebook in the `examples` folder.\n\n### One-Code Block Usage\n\n```python\nfrom neuttsair.neutts import NeuTTSAir\nimport soundfile as sf\n\ntts = NeuTTSAir(\n   backbone_repo=\"neuphonic/neutts-air\", #",
      "default_branch": "main"
    },
    "fetched_at": "2025-10-11T02:14:23.428284"
  },
  {
    "basic_info": {
      "name": "Qwen3-Omni",
      "full_name": "QwenLM/Qwen3-Omni",
      "owner": "QwenLM",
      "description": "Qwen3-omni is a natively end-to-end, omni-modal LLM developed by the Qwen team at Alibaba Cloud, capable of understanding text, audio, images, and video, as well as generating speech in real time.",
      "url": "https://github.com/QwenLM/Qwen3-Omni",
      "clone_url": "https://github.com/QwenLM/Qwen3-Omni.git",
      "ssh_url": "git@github.com:QwenLM/Qwen3-Omni.git",
      "homepage": null,
      "created_at": "2025-09-21T09:46:10Z",
      "updated_at": "2025-10-11T01:35:14Z",
      "pushed_at": "2025-10-09T11:13:39Z"
    },
    "stats": {
      "stars": 2560,
      "forks": 134,
      "watchers": 2560,
      "open_issues": 6,
      "size": 26943
    },
    "tech_info": {
      "language": "Jupyter Notebook",
      "languages": {
        "Jupyter Notebook": 38834506,
        "Python": 29676
      },
      "license": "Apache License 2.0",
      "topics": []
    },
    "content": {
      "readme": "# Qwen3-Omni\n\n<br>\n\n<p align=\"center\">\n    <img src=\"https://qianwen-res.oss-cn-beijing.aliyuncs.com//Qwen3-Omni/qwen3_omni_logo.png\" width=\"400\"/>\n<p>\n\n<p align=\"center\">\n        💜 <a href=\"https://chat.qwen.ai/\"><b>Qwen Chat</b></a>&nbsp&nbsp | &nbsp&nbsp🤗 <a href=\"https://huggingface.co/collections/Qwen/qwen3-omni-68d100a86cd0906843ceccbe\">Hugging Face</a>&nbsp&nbsp | &nbsp&nbsp🤖 <a href=\"https://modelscope.cn/collections/Qwen3-Omni-867aef131e7d4f\">ModelScope</a>&nbsp&nbsp | &nbsp&nbsp📑 <a href=\"https://qwen.ai/blog?id=65f766fc2dcba7905c1cb69cc4cab90e94126bf4&from=research.latest-advancements-list\">Blog</a>&nbsp&nbsp | &nbsp&nbsp📚 <a href=\"https://github.com/QwenLM/Qwen3-Omni/tree/main/cookbooks\">Cookbooks</a>&nbsp&nbsp | &nbsp&nbsp📑 <a href=\"https://arxiv.org/pdf/2509.17765\">Paper</a>&nbsp&nbsp\n<br>\n🖥️ <a href=\"https://huggingface.co/spaces/Qwen/Qwen3-Omni-Demo\">Hugging Face Demo</a>&nbsp&nbsp | &nbsp&nbsp 🖥️ <a href=\"https://modelscope.cn/studios/Qwen/Qwen3-Omni-Demo\">ModelScope Demo</a>&nbsp&nbsp | &nbsp&nbsp💬 <a href=\"https://github.com/QwenLM/Qwen/blob/main/assets/wechat.png\">WeChat (微信)</a>&nbsp&nbsp | &nbsp&nbsp🫨 <a href=\"https://discord.gg/CV4E9rpNSD\">Discord</a>&nbsp&nbsp | &nbsp&nbsp📑 <a href=\"https://help.aliyun.com/zh/model-studio/user-guide/qwen-omni\">API</a>\n\n</p>\n\nWe release **Qwen3-Omni**, the natively end-to-end multilingual omni-modal foundation models. It is designed to process diverse inputs including text, images, audio, and video, while delivering real-time streaming responses in both text and natural speech. Click the video below for more information 😃\n\n<details open>\n<summary>English Version</summary>\n<a href=\"https://youtu.be/_zdOrPju4_g\" target=\"_blank\">\n  <img src=\"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/videocover.png\" alt=\"Open English Video\"/>\n</a>\n</details>\n\n<details>\n<summary>Chinese Version</summary>\n<a href=\"https://youtu.be/Wtjsw5deXfQ\" target=\"_blank\">\n  <img src=\"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/videocover.png\" alt=\"打开中文视频\"/>\n</a>\n</details>\n\n\n## News\n* 2025.09.26: ⭐️⭐️⭐️ Qwen3-Omni reaches top-1 on Hugging Face Trending! \n* 2025.09.22: 🎉🎉🎉 We have released [Qwen3-Omni](https://huggingface.co/collections/Qwen/qwen3-omni-68d100a86cd0906843ceccbe). For more details, please check our [blog](https://qwen.ai/blog?id=65f766fc2dcba7905c1cb69cc4cab90e94126bf4&from=research.latest-advancements-list)!\n\n## Contents <!-- omit in toc -->\n\n- [Overview](#overview)\n  - [Introduction](#introduction)\n  - [Model Architecture](#model-architecture)\n  - [Cookbooks for Usage Cases](#cookbooks-for-usage-cases)\n- [QuickStart](#quickstart)\n  - [Model Description and Download](#model-description-and-download)\n  - [Transformers Usage](#transformers-usage)\n  - [vLLM Usage](#vllm-usage)\n  - [DashScope API Usage](#dashscope-api-usage)\n  - [Usage Tips (Recommended Reading)](#usage-tips-recommended-reading)\n- [Interaction with Qwen3-Omni](#interaction-with-qwen3-omni)\n  - [Online Demo](#online-demo)\n  - [Real-Time Interaction](#real-time-interaction)\n  - [Launch Local Web UI Demo](#launch-local-web-ui-demo)\n- [Docker](#-docker)\n- [Evaluation](#evaluation)\n  - [Performance of Qwen3-Omni](#performance-of-qwen3-omni)\n  - [Setting for Evaluation](#setting-for-evaluation)\n- [Citation](#citation)\n\n## Overview\n### Introduction\n\n<p align=\"center\">\n    <img src=\"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/q3o_introduction.png\" width=\"90%\"/>\n<p>\n\nQwen3-Omni is the natively end-to-end multilingual omni-modal foundation models. It processes text, images, audio, and video, and delivers real-time streaming responses in both text and natural speech. We introduce several architectural upgrades to improve performance and efficiency. Key features:\n\n* **State-of-the-art across modalities**: Early text-first pretraining and mixed multimodal training provide native multimodal support. While achieving strong audio and audio-video results, unimodal text and image performance does not regress. Reaches SOTA on 22 of 36 audio/video benchmarks and open-source SOTA on 32 of 36; ASR, audio understanding, and voice conversation performance is comparable to Gemini 2.5 Pro.\n\n* **Multilingual**: Supports 119 text languages, 19 speech input languages, and 10 speech output languages.\n  - **Speech Input**: English, Chinese, Korean, Japanese, German, Russian, Italian, French, Spanish, Portuguese, Malay, Dutch, Indonesian, Turkish, Vietnamese, Cantonese, Arabic, Urdu.\n  - **Speech Output**: English, Chinese, French, German, Russian, Italian, Spanish, Portuguese, Japanese, Korean.\n\n* **Novel Architecture**: MoE-based Thinker–Talker design with AuT pretraining for strong general representations, plus a multi-codebook design that drives latency to a minimum.\n\n* **Real-time Audio/Video Interaction**: Low-latency streaming with natural turn-taking and immediate text or speech responses.\n\n* **Flexible Control**: Customize behavior via system prompts for fine-grained control and easy ",
      "default_branch": "main"
    },
    "fetched_at": "2025-10-11T02:14:24.762957"
  },
  {
    "basic_info": {
      "name": "jscamp",
      "full_name": "midudev/jscamp",
      "owner": "midudev",
      "description": "Contenido y ejercicios del JSCamp InfoJobs",
      "url": "https://github.com/midudev/jscamp",
      "clone_url": "https://github.com/midudev/jscamp.git",
      "ssh_url": "git@github.com:midudev/jscamp.git",
      "homepage": "https://jscamp.dev",
      "created_at": "2025-09-28T13:28:51Z",
      "updated_at": "2025-10-11T01:53:29Z",
      "pushed_at": "2025-10-09T08:40:43Z"
    },
    "stats": {
      "stars": 2289,
      "forks": 233,
      "watchers": 2289,
      "open_issues": 0,
      "size": 140
    },
    "tech_info": {
      "language": "HTML",
      "languages": {
        "HTML": 15238,
        "CSS": 13537,
        "JavaScript": 1233
      },
      "license": null,
      "topics": [
        "bootcamp"
      ]
    },
    "content": {
      "readme": "![CleanShot 2025-10-01 at 20 11 30@2x](https://github.com/user-attachments/assets/b6ef8402-d367-4a99-b939-8f11dedf91d7)\n\n# 🚀 JSCamp InfoJobs\n\nBienvenidos al bootcamp intensivo de JavaScript y desarrollo web full-stack diseñado para llevarte desde los fundamentos hasta las tecnologías más avanzadas del ecosistema JavaScript. Veremos HTML, CSS, JavaScript, TypeScript, Node.js, SQL, CI/CD y Docker.\n\n## 🎨 El Proyecto Práctico\n\n![CleanShot 2025-10-01 at 20 26 08@2x](https://github.com/user-attachments/assets/d9abec4d-ac41-4962-845c-93006bfe768b)\n\nA lo largo de este bootcamp, construiremos un proyecto completo **desde cero y paso a paso**, aplicando todos los conocimientos de cada módulo.\n\n👉 [Ver diseño del proyecto](https://stitch.withgoogle.com/projects/7508115667617706440)\n\nEste proyecto te permitirá consolidar todo lo aprendido y tener una aplicación real en tu portafolio.\n\n## 📺 La Plataforma\n\nEn **[JSCamp.dev](https://jscamp.dev)** encontrarás todos los videos y contenido del bootcamp para que puedas revisarlo cuando quieras. El registro es gratis.\nLos videos y materiales se irán subiendo **poco a poco** a medida que avancemos en el bootcamp.\n\n### ¿Tiene certificado?\n\nSí, existe un certificado opcional y muy limitado de pago que incluye:\n\n- 🎓 **Certificado Digital** - Certifica tus logros en el bootcamp\n- 📝 **Seguimiento de Ejercicios** - Revisaremos y corregiremos tus ejercicios\n- 💬 **Canal Exclusivo en Discord** - Comunidad premium y soporte directo\n- 🎥 **Directos Exclusivos** - Clases de repaso exclusivas con dudas y preguntas\n- 📄 **Revisión de tu CV** - Equipo de expertos revisan tu CV y te dan feedback\n- 🏢 **Workshop Presencial** - Entrada asegurada a los workshops de Barcelona y Madrid\n\n**Entra a [https://jscamp.dev](https://jscamp.dev), inicia sesión y consigue acceso.**\n\n## 📚 Contenido del Bootcamp\n\n- **00** - HTML & CSS\n- **01** - JavaScript\n- **02** - React\n- **03** - Estado Global y React Router\n- **04** - Node.js\n- **05** - TypeScript\n- **06** - SQL\n- **07** - CI/CD\n- **08** - Docker\n\n## 💻 Requisitos de Instalación\n\nAntes de comenzar, asegúrate de tener instalado el siguiente software:\n\n- **Navegador moderno** - Chrome, Firefox, Edge o Safari actualizado\n- **[Visual Studio Code](https://code.visualstudio.com/)** - Editor de código (recomendado)\n- **[Extensión Live Preview](https://marketplace.visualstudio.com/items?itemName=ms-vscode.live-server)** - Extensión para ver HTML/CSS\n- **[Node.js](https://nodejs.org/)** (versión 20 o superior) - Runtime de JavaScript\n- **[Git](https://git-scm.com/)** - Control de versiones\n- **[Docker](https://www.docker.com/)** - Para el módulo de Docker\n- **[Terminal Warp](https://midu.link/warp)** - Terminal con IA y Agentes\n\n## 👨‍💻 Instructor\n\nEste bootcamp es impartido por **midudev**, desarrollador y creador de contenido educativo con una gran comunidad en español.\n\n### 🌐 Redes Sociales\n\n- 🐦 **X**: [@midudev](https://twitter.com/midudev)\n- 📺 **YouTube**: [@midudev](https://youtube.com/@midudev)\n- 🎮 **Twitch**: [midudev](https://twitch.tv/midudev)\n- 📸 **Instagram**: [@midu.dev](https://instagram.com/midu.dev)\n- 💼 **LinkedIn**: [midudev](https://linkedin.com/in/midudev)\n- 🌍 **Web**: [midu.dev](https://midu.dev)\n\n## 🎯 Objetivos\n\nAl finalizar JSCAMP serás capaz de:\n\n- ✅ Construir aplicaciones web completas desde cero\n- ✅ Dominar el ecosistema de JavaScript moderno\n- ✅ Crear APIs REST con Node.js\n- ✅ Desarrollar interfaces con React\n- ✅ Implementar bases de datos SQL\n- ✅ Configurar pipelines de CI/CD\n- ✅ Containerizar aplicaciones con Docker\n- ✅ Aplicar TypeScript en proyectos reales\n\n## 🚀 Cómo Empezar\n\nCada módulo contiene ejercicios prácticos y proyectos reales. Navega a la carpeta correspondiente y sigue las instrucciones.\n\n```bash\n# Clona el repositorio\ngit clone git@github.com:midudev/jscamp.git\n\n# Navega al módulo que desees\ncd jscamp/00-html-css\n\n# ¡Comienza a aprender!\n```\n\n---\n\n⭐️ Si este contenido te resulta útil, no olvides dar una estrella al repositorio\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-10-11T02:14:26.025361"
  },
  {
    "basic_info": {
      "name": "HunyuanImage-3.0",
      "full_name": "Tencent-Hunyuan/HunyuanImage-3.0",
      "owner": "Tencent-Hunyuan",
      "description": "HunyuanImage-3.0: A Powerful Native Multimodal Model for Image Generation",
      "url": "https://github.com/Tencent-Hunyuan/HunyuanImage-3.0",
      "clone_url": "https://github.com/Tencent-Hunyuan/HunyuanImage-3.0.git",
      "ssh_url": "git@github.com:Tencent-Hunyuan/HunyuanImage-3.0.git",
      "homepage": "https://hunyuan.tencent.com/image",
      "created_at": "2025-09-27T07:18:47Z",
      "updated_at": "2025-10-11T02:12:58Z",
      "pushed_at": "2025-10-02T06:24:24Z"
    },
    "stats": {
      "stars": 2064,
      "forks": 79,
      "watchers": 2064,
      "open_issues": 24,
      "size": 34775
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 374880,
        "Shell": 806
      },
      "license": "Other",
      "topics": [
        "image-generation",
        "native-multimodal-model"
      ]
    },
    "content": {
      "readme": "[中文文档](./README_zh_CN.md)\n\n<div align=\"center\">\n\n<img src=\"./assets/logo.png\" alt=\"HunyuanImage-3.0 Logo\" width=\"600\">\n\n# 🎨 HunyuanImage-3.0: A Powerful Native Multimodal Model for Image Generation\n\n</div>\n\n\n<div align=\"center\">\n<img src=\"./assets/banner.png\" alt=\"HunyuanImage-3.0 Banner\" width=\"800\">\n\n</div>\n\n<div align=\"center\">\n  <a href=https://hunyuan.tencent.com/image target=\"_blank\"><img src=https://img.shields.io/badge/Official%20Site-333399.svg?logo=homepage height=22px></a>\n  <a href=https://huggingface.co/tencent/HunyuanImage-3.0 target=\"_blank\"><img src=https://img.shields.io/badge/%F0%9F%A4%97%20Models-d96902.svg height=22px></a>\n  <a href=https://github.com/Tencent-Hunyuan/HunyuanImage-3.0 target=\"_blank\"><img src= https://img.shields.io/badge/Page-bb8a2e.svg?logo=github height=22px></a>\n  <a href=https://arxiv.org/pdf/2509.23951 target=\"_blank\"><img src=https://img.shields.io/badge/Report-b5212f.svg?logo=arxiv height=22px></a>\n  <a href=https://x.com/TencentHunyuan target=\"_blank\"><img src=https://img.shields.io/badge/Hunyuan-black.svg?logo=x height=22px></a>\n  <a href=https://docs.qq.com/doc/DUVVadmhCdG9qRXBU target=\"_blank\"><img src=https://img.shields.io/badge/📚-PromptHandBook-blue.svg?logo=book height=22px></a>\n</div>\n\n\n<p align=\"center\">\n    👏 Join our <a href=\"./assets/WECHAT.md\" target=\"_blank\">WeChat</a> and <a href=\"https://discord.gg/ehjWMqF5wY\">Discord</a> | \n💻 <a href=\"https://hunyuan.tencent.com/modelSquare/home/play?modelId=289&from=/visual\">Official website(官网) Try our model!</a>&nbsp&nbsp\n</p>\n\n## 🔥🔥🔥 News\n- **September 28, 2025**: 📖 **HunyuanImage-3.0 Technical Report Released** - Comprehensive technical documentation now available\n- **September 28, 2025**: 🚀 **HunyuanImage-3.0 Open Source Release** - Inference code and model weights publicly available\n\n\n## 🧩 Community Contributions\n\nIf you develop/use HunyuanImage-3.0 in your projects, welcome to let us know.\n\n## 📑 Open-source Plan\n\n- HunyuanImage-3.0 (Image Generation Model)\n  - [x] Inference \n  - [x] HunyuanImage-3.0 Checkpoints\n  - [ ] HunyuanImage-3.0-Instruct Checkpoints (with reasoning)\n  - [ ] VLLM Support\n  - [ ] Distilled Checkpoints\n  - [ ] Image-to-Image Generation\n  - [ ] Multi-turn Interaction\n\n\n## 🗂️ Contents\n- [🔥🔥🔥 News](#-news)\n- [🧩 Community Contributions](#-community-contributions)\n- [📑 Open-source Plan](#-open-source-plan)\n- [📖 Introduction](#-introduction)\n- [✨ Key Features](#-key-features)\n- [🛠️ Dependencies and Installation](#-dependencies-and-installation)\n  - [💻 System Requirements](#-system-requirements)\n  - [📦 Environment Setup](#-environment-setup)\n  - [📥 Install Dependencies](#-install-dependencies)\n  - [Performance Optimizations](#performance-optimizations)\n- [🚀 Usage](#-usage)\n  - [🔥 Quick Start with Transformers](#-quick-start-with-transformers)\n  - [🏠 Local Installation & Usage](#-local-installation--usage)\n  - [🎨 Interactive Gradio Demo](#-interactive-gradio-demo)\n- [🧱 Models Cards](#-models-cards)\n- [📝 Prompt Guide](#-prompt-guide)\n  - [Manually Writing Prompts](#manually-writing-prompts)\n  - [System Prompt For Automatic Rewriting the Prompt](#system-prompt-for-automatic-rewriting-the-prompt)\n  - [Advanced Tips](#advanced-tips)\n  - [More Cases](#more-cases)\n- [📊 Evaluation](#-evaluation)\n- [📚 Citation](#-citation)\n- [🙏 Acknowledgements](#-acknowledgements)\n- [🌟🚀  Github Star History](#-github-star-history)\n\n---\n\n## 📖 Introduction\n\n**HunyuanImage-3.0** is a groundbreaking native multimodal model that unifies multimodal understanding and generation within an autoregressive framework. Our text-to-image module achieves performance **comparable to or surpassing** leading closed-source models.\n\n\n<div align=\"center\">\n  <img src=\"./assets/framework.png\" alt=\"HunyuanImage-3.0 Framework\" width=\"90%\">\n</div>\n\n## ✨ Key Features\n\n* 🧠 **Unified Multimodal Architecture:** Moving beyond the prevalent DiT-based architectures, HunyuanImage-3.0 employs a unified autoregressive framework. This design enables a more direct and integrated modeling of text and image modalities, leading to surprisingly effective and contextually rich image generation.\n\n* 🏆 **The Largest Image Generation MoE Model:** This is the largest open-source image generation Mixture of Experts (MoE) model to date. It features 64 experts and a total of 80 billion parameters, with 13 billion activated per token, significantly enhancing its capacity and performance.\n\n* 🎨 **Superior Image Generation Performance:** Through rigorous dataset curation and advanced reinforcement learning post-training, we've achieved an optimal balance between semantic accuracy and visual excellence. The model demonstrates exceptional prompt adherence while delivering photorealistic imagery with stunning aesthetic quality and fine-grained details.\n\n* 💭 **Intelligent World-Knowledge Reasoning:** The unified multimodal architecture endows HunyuanImage-3.0 with powerful reasoning capabilities. It leverages its extensive world knowledge to intelligently interpret user int",
      "default_branch": "main"
    },
    "fetched_at": "2025-10-11T02:14:27.271602"
  },
  {
    "basic_info": {
      "name": "The-Accidental-CTO",
      "full_name": "subhashchy/The-Accidental-CTO",
      "owner": "subhashchy",
      "description": "How I Scaled from Zero to a Million Store on Dukaan,  Without a CS Degree.  .. A System Design Handbook by  Subhash Choudhary ",
      "url": "https://github.com/subhashchy/The-Accidental-CTO",
      "clone_url": "https://github.com/subhashchy/The-Accidental-CTO.git",
      "ssh_url": "git@github.com:subhashchy/The-Accidental-CTO.git",
      "homepage": "",
      "created_at": "2025-09-26T09:07:20Z",
      "updated_at": "2025-10-11T01:36:34Z",
      "pushed_at": "2025-10-08T12:45:33Z"
    },
    "stats": {
      "stars": 1960,
      "forks": 148,
      "watchers": 1960,
      "open_issues": 8,
      "size": 4194
    },
    "tech_info": {
      "language": null,
      "languages": {},
      "license": null,
      "topics": [
        "scaling",
        "system-design"
      ]
    },
    "content": {
      "readme": "\n# **The Accidental CTO**\n\n## **How I Scaled from Zero to a Million Stores on Dukaan, Without a CS Degree**\n\nI never set out to be a CTO. In fact, I didn’t even have a computer science degree. But somewhere between firefighting server crashes at 3 a.m. and obsessing over replication lag graphs, I found myself building systems that would eventually power over a **million online stores** at Dukaan.\n\nThis book, *The Accidental CTO*, is my behind-the-scenes account of that journey. It’s not a dry academic manual filled with abstract diagrams. Instead, it’s a story-driven handbook — one that mixes late-night startup battles with the **hard system design lessons** that only come from being in the trenches.\n\nFrom scaling a scrappy MVP to running massive distributed pipelines, I’ll take you through the challenges we faced and the decisions that made (or nearly broke) us.\n\n---\n\n### What You’ll Learn Inside\n\n* **Scaling applications**: How we went from thousands to millions of users without falling apart.\n* **Replication, sharding, caching, queues**: When to use them, when *not* to, and what tradeoffs they carry.\n* **Observability as survival**: Why metrics, logs, traces, SLAs, and SLOs aren’t optional — they’re lifelines.\n* **Resilience engineering**: Circuit breakers, retries, graceful degradation — designing for failure, not against it.\n* **The hidden costs of cloud**: Why at scale, your AWS bill can become your biggest investor, and when it makes sense to go self-hosted.\n* **The consistency/availability/latency triangle**: Why you can never fully win, and how to navigate the tradeoffs in real systems.\n\n---\n\n### Why I Wrote This Book\n\nI didn’t want to write another \"theory of distributed systems\" book. There are already plenty of those.\n\nWhat I wanted to share is the **practical side** of system design — the part you only learn when a real company, with real customers and real money at stake, is on fire. The part where you’re not solving toy interview questions but dealing with:\n\n* angry merchants refreshing dashboards,\n* Kafka pipelines silently choking on one bad partition,\n* a database replica 10 minutes behind and nobody knowing why.\n\nThis is the stuff no textbook teaches you.\n\n---\n\n### Who This Book Is For\n\nWhether you’re a **software engineer**, **architect**, or **startup founder**, I wrote this book to help you see distributed systems not as academic puzzles, but as **living, evolving machines** that you can actually build, operate, and grow.\n\nIf you’ve ever wondered *how real companies actually scale* — not in theory, but in practice — this is my candid, first-hand story.\n\nAnd maybe, just maybe, you’ll find a bit of yourself in *The Accidental CTO*.",
      "default_branch": "main"
    },
    "fetched_at": "2025-10-11T02:14:28.643409"
  },
  {
    "basic_info": {
      "name": "openzl",
      "full_name": "facebook/openzl",
      "owner": "facebook",
      "description": "A novel data compression framework",
      "url": "https://github.com/facebook/openzl",
      "clone_url": "https://github.com/facebook/openzl.git",
      "ssh_url": "git@github.com:facebook/openzl.git",
      "homepage": "https://openzl.org",
      "created_at": "2025-09-30T18:30:07Z",
      "updated_at": "2025-10-11T02:11:19Z",
      "pushed_at": "2025-10-09T20:03:59Z"
    },
    "stats": {
      "stars": 1726,
      "forks": 63,
      "watchers": 1726,
      "open_issues": 45,
      "size": 15526
    },
    "tech_info": {
      "language": "C",
      "languages": {
        "C": 8860592,
        "C++": 5543044,
        "Python": 239843,
        "TypeScript": 124470,
        "Starlark": 116347,
        "CMake": 62505,
        "Makefile": 29737,
        "Assembly": 14169,
        "CSS": 11168,
        "PowerShell": 6170,
        "Shell": 5823,
        "Thrift": 4215,
        "Batchfile": 3464,
        "JavaScript": 1150,
        "HTML": 462
      },
      "license": "Other",
      "topics": []
    },
    "content": {
      "readme": "# OpenZL\n\nOpenZL delivers high compression ratios _while preserving high speed_, a level of performance that is out of reach for generic compressors. **Check out the [blog post](https://engineering.fb.com/2025/10/06/developer-tools/openzl-open-source-format-aware-compression-framework/) and [whitepaper](https://arxiv.org/abs/2510.03203) for a breakdown of how it works.**\n\nOpenZL takes a description of your data and builds from it a specialized compressor optimized for your specific format. [Learn how it works →](https://facebook.github.io/openzl/getting-started/introduction/)\n\nOpenZL consists of a core library and tools to generate specialized compressors —\nall compatible with a single universal decompressor.\nIt is designed for engineers that deal with large quantities of specialized datasets (like AI workloads for example) and require high speed for their processing pipelines.\n\nSee our [docs](https://facebook.github.io/openzl) for more information and our [quickstart guide](https://facebook.github.io/openzl/getting-started/quick-start) to get started with a guided tutorial.\n\n## Project Status\n\nThis project is under active development. The API, the compressed format, and the set of codecs and graphs included in OpenZL are all subject to (and will!) change as the project matures.\n\nHowever, we intend to maintain some stability guarantees in the face of that evolution. In particular, payloads compressed with any release-tagged version of the library will remain decompressible by new releases of the library for at least the next several years. And new releases of the library will be able to generate frames compatible with at least the previous release.\n\n(Commits on the `dev` branch offer no guarantees whatsoever. Use only release-tagged commits for any non-experimental deployments.)\n\nDespite the big scary warnings above, we consider the core to have reached production-readiness, and OpenZL is used extensively in production at Meta.\n\n## Building OpenZL\n\n### Build with `make`\n\nThe OpenZL library and essential tools can be built using `make`:\n\n```sh\nmake\n```\n\n#### Build Options\n\nThe `Makefile` supports all standard build variables, such as `CC`, `CFLAGS`, `CPPFLAGS`, `LDFLAGS`, `LDLIBS`, etc.\n\nIt builds with multi-threading by default, auto-detecting the local number of cores, and can be overridden using standard `-j#` flag (ex: `make -j8`).\n\n#### Build Types\n\nBinary generation can be altered by explicitly requesting a build type:\n\nExample:\n```sh\nmake lib BUILD_TYPE=DEV\n```\n\nBuild types are documented in `make help`, and their exact flags are detailed with `make show-config`.\n\nUsual ones are:\n\n* `BUILD_TYPE=DEV`: debug build with asserts enabled and ASAN / UBSAN enabled\n* `BUILD_TYPE=OPT`: optimized build with asserts disabled (default)\n\n### Build with `cmake`\n\nOpenZL can be built using `cmake`. Basic usage is as follows:\n\n```sh\nmkdir build\ncd build\ncmake -DCMAKE_BUILD_TYPE=Release -DOPENZL_BUILD_TESTS=ON ..\nmake -j\nmake -j test\n```\n\nDetails on setting CMake variables is below.\n\n#### Build Modes\n\nBy default, we ship several different predefined build modes which can be set with the `OPENZL_BUILD_MODE` variable:\n\n* `none` (default): CMake default build mode controlled by `CMAKE_BUILD_TYPE`\n* `dev`: debug build with asserts enabled and ASAN / UBSAN enabled\n* `dev-nosan`: debug build with asserts enabled\n* `opt`: optimized build with asserts disabled\n* `opt-asan`: optimized build with asserts disabled and ASAN / UBSAN enabled\n* `dbgo`: optimized build with asserts enabled\n* `dbgo-asan`: optimized build with asserts enabled and ASAN / UBSAN enabled\n\n> [!CAUTION]\n> When switching between build modes, make sure to purge the CMake cache and re-configure the build. For instance,\n> `cmake --fresh -DOPENZL_BUILD_MODE=dev-nosan ..`\n\nFor ASAN / UBSAN, ensure that `libasan` and `libubsan` are installed on the machine.\n\n#### Editor Integration\n\nOpenZL ships with settings to configure VSCode to work with the CMake build system. To enable it install two extensions:\n\n1. `cmake-tools`\n2. `clangd` (or any other C++ language server that works with `compile_commands.json`)\n\n**Important:** For proper C++ language server support, you need to generate `compile_commands.json`:\n\nThe preferred method is to use the CMake Tools extension command \"`CMake: Configure`\".\n\nIf it doesn't work, or is too difficult to setup, you can use the manual setup:\n\n```bash\nmkdir -p cmakebuild\ncmake -B cmakebuild -DOPENZL_BUILD_TESTS=ON -DCMAKE_EXPORT_COMPILE_COMMANDS=ON .\ncp cmakebuild/compile_commands.json .\n```\n\n**When to regenerate:**\n\n* After cloning the repository (first-time setup)\n* When adding/removing source files\n* When modifying `CMakeLists.txt`\n\n#### CMake Variables\n\n* `CMAKE_C_COMPILER` = Set the C compiler for OpenZL & dependency builds\n* `CMAKE_CXX_COMPILER` = Set the C++ compiler for OpenZL & dependency builds\n* `CMAKE_C_FLAGS` = C flags for OpenZL & dependency builds\n* `CMAKE_CXX_FLAGS` = C++ flags for OpenZL & dependency builds\n* `OPENZL_BUIL",
      "default_branch": "dev"
    },
    "fetched_at": "2025-10-11T02:14:29.921723"
  },
  {
    "basic_info": {
      "name": "VoxCPM",
      "full_name": "OpenBMB/VoxCPM",
      "owner": "OpenBMB",
      "description": "VoxCPM: Tokenizer-Free TTS for Context-Aware Speech Generation and True-to-Life Voice Cloning",
      "url": "https://github.com/OpenBMB/VoxCPM",
      "clone_url": "https://github.com/OpenBMB/VoxCPM.git",
      "ssh_url": "git@github.com:OpenBMB/VoxCPM.git",
      "homepage": "",
      "created_at": "2025-09-16T03:41:49Z",
      "updated_at": "2025-10-10T14:36:46Z",
      "pushed_at": "2025-10-09T05:22:50Z"
    },
    "stats": {
      "stars": 1679,
      "forks": 173,
      "watchers": 1679,
      "open_issues": 21,
      "size": 1456
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 113165
      },
      "license": "Apache License 2.0",
      "topics": []
    },
    "content": {
      "readme": "## 🎙️ VoxCPM: Tokenizer-Free TTS for Context-Aware Speech Generation and True-to-Life Voice Cloning\n\n\n[![Project Page](https://img.shields.io/badge/Project%20Page-GitHub-blue)](https://github.com/OpenBMB/VoxCPM/) [![Technical Report](https://img.shields.io/badge/Technical%20Report-Arxiv-red)](https://arxiv.org/abs/2509.24650) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-OpenBMB-yellow)](https://huggingface.co/openbmb/VoxCPM-0.5B) [![ModelScope](https://img.shields.io/badge/ModelScope-OpenBMB-purple)](https://modelscope.cn/models/OpenBMB/VoxCPM-0.5B)  [![Live Playground](https://img.shields.io/badge/Live%20PlayGround-Demo-orange)](https://huggingface.co/spaces/OpenBMB/VoxCPM-Demo) [![Samples](https://img.shields.io/badge/Audio%20Samples-Page-green)](https://openbmb.github.io/VoxCPM-demopage)\n\n\n\n<div align=\"center\">\n  <img src=\"assets/voxcpm_logo.png\" alt=\"VoxCPM Logo\" width=\"40%\">\n</div>\n\n<div align=\"center\">\n\n👋 Contact us on [WeChat](assets/wechat.png)\n\n</div>\n\n## News \n* [2025.09.30] 🔥 🔥 🔥  We Release VoxCPM [Technical Report](https://arxiv.org/abs/2509.24650)!\n* [2025.09.16] 🔥 🔥 🔥  We Open Source the VoxCPM-0.5B [weights](https://huggingface.co/openbmb/VoxCPM-0.5B)!\n* [2025.09.16] 🎉 🎉 🎉  We Provide the [Gradio PlayGround](https://huggingface.co/spaces/OpenBMB/VoxCPM-Demo) for VoxCPM-0.5B, try it now! \n\n## Overview\n\nVoxCPM is a novel tokenizer-free Text-to-Speech (TTS) system that redefines realism in speech synthesis. By modeling speech in a continuous space, it overcomes the limitations of discrete tokenization and enables two flagship capabilities: context-aware speech generation and true-to-life zero-shot voice cloning.\n\nUnlike mainstream approaches that convert speech to discrete tokens, VoxCPM uses an end-to-end diffusion autoregressive architecture that directly generates continuous speech representations from text. Built on [MiniCPM-4](https://huggingface.co/openbmb/MiniCPM4-0.5B) backbone, it achieves implicit semantic-acoustic decoupling through hierachical language modeling and FSQ constraints, greatly enhancing both expressiveness and generation stability.\n\n<div align=\"center\">\n  <img src=\"assets/voxcpm_model.png\" alt=\"VoxCPM Model Architecture\" width=\"90%\">\n</div>\n\n\n###  🚀 Key Features\n- **Context-Aware, Expressive Speech Generation** - VoxCPM comprehends text to infer and generate appropriate prosody, delivering speech with remarkable expressiveness and natural flow. It spontaneously adapts speaking style based on content, producing highly fitting vocal expression trained on a massive 1.8 million-hour bilingual corpus.\n- **True-to-Life Voice Cloning** - With only a short reference audio clip, VoxCPM performs accurate zero-shot voice cloning, capturing not only the speaker’s timbre but also fine-grained characteristics such as accent, emotional tone, rhythm, and pacing to create a faithful and natural replica.\n- **High-Efficiency Synthesis** - VoxCPM supports streaming synthesis with a Real-Time Factor (RTF) as low as 0.17 on a consumer-grade NVIDIA RTX 4090 GPU, making it possible for real-time applications.\n\n\n\n\n\n##  Quick Start\n\n### 🔧 Install from PyPI\n``` sh\npip install voxcpm\n```\n### 1.  Model Download (Optional)\nBy default, when you first run the script, the model will be downloaded automatically, but you can also download the model in advance.\n- Download VoxCPM-0.5B\n    ```\n    from huggingface_hub import snapshot_download\n    snapshot_download(\"openbmb/VoxCPM-0.5B\")\n    ```\n- Download ZipEnhancer and SenseVoice-Small. We use ZipEnhancer to enhance speech prompts and SenseVoice-Small for speech prompt ASR in the web demo.\n    ```\n    from modelscope import snapshot_download\n    snapshot_download('iic/speech_zipenhancer_ans_multiloss_16k_base')\n    snapshot_download('iic/SenseVoiceSmall')\n    ```\n\n### 2. Basic Usage\n```python\nimport soundfile as sf\nimport numpy as np\nfrom voxcpm import VoxCPM\n\nmodel = VoxCPM.from_pretrained(\"openbmb/VoxCPM-0.5B\")\n\n# Non-streaming\nwav = model.generate(\n    text=\"VoxCPM is an innovative end-to-end TTS model from ModelBest, designed to generate highly expressive speech.\",\n    prompt_wav_path=None,      # optional: path to a prompt speech for voice cloning\n    prompt_text=None,          # optional: reference text\n    cfg_value=2.0,             # LM guidance on LocDiT, higher for better adherence to the prompt, but maybe worse\n    inference_timesteps=10,   # LocDiT inference timesteps, higher for better result, lower for fast speed\n    normalize=True,           # enable external TN tool\n    denoise=True,             # enable external Denoise tool\n    retry_badcase=True,        # enable retrying mode for some bad cases (unstoppable)\n    retry_badcase_max_times=3,  # maximum retrying times\n    retry_badcase_ratio_threshold=6.0, # maximum length restriction for bad case detection (simple but effective), it could be adjusted for slow pace speech\n)\n\nsf.write(\"output.wav\", wav, 16000)\nprint(\"saved: output.wav\")\n\n# Streaming\nchunks = []",
      "default_branch": "main"
    },
    "fetched_at": "2025-10-11T02:14:31.168175"
  },
  {
    "basic_info": {
      "name": "Super-Mario-Bros.-Remastered-Public",
      "full_name": "JHDev2006/Super-Mario-Bros.-Remastered-Public",
      "owner": "JHDev2006",
      "description": "A Remake / Celebration of the original 'Super Mario Bros.' games. Features new levels, custom modes, new characters, alongside a full level editor / custom level system!",
      "url": "https://github.com/JHDev2006/Super-Mario-Bros.-Remastered-Public",
      "clone_url": "https://github.com/JHDev2006/Super-Mario-Bros.-Remastered-Public.git",
      "ssh_url": "git@github.com:JHDev2006/Super-Mario-Bros.-Remastered-Public.git",
      "homepage": "",
      "created_at": "2025-09-13T15:29:58Z",
      "updated_at": "2025-10-10T21:38:22Z",
      "pushed_at": "2025-10-10T20:29:27Z"
    },
    "stats": {
      "stars": 1662,
      "forks": 201,
      "watchers": 1662,
      "open_issues": 201,
      "size": 56164
    },
    "tech_info": {
      "language": "GDScript",
      "languages": {
        "GDScript": 973919,
        "GAP": 36590,
        "C#": 10724,
        "GDShader": 2322
      },
      "license": "GNU General Public License v3.0",
      "topics": []
    },
    "content": {
      "readme": "# Super Mario Bros Remastered\nA Remake / Celebration of the original 'Super Mario Bros.' games. Features new levels, custom modes, new characters, alongside a full level editor / custom level system!\n\n<img width=\"3840\" height=\"2160\" alt=\"SMB1R_BANNER_printable\" src=\"https://github.com/user-attachments/assets/ed0e97a8-614a-44e2-b69f-2654fca6196c\" />\n\n### Art by [@krystalphantasm.bsky.social](https://bsky.app/profile/krystalphantasm.bsky.social/post/3lvgmgvjeks2f)\n\n### Download: https://github.com/JHDev2006/Super-Mario-Bros.-Remastered-Public/releases\n\n# Requires an original SMB1 NES ROM to play! None of the original assets are contained in the source code, unless it was originally made by us!\n\n# This does NOT act as a replacement for the original Super Mario Bros. games. Super Mario Bros. & Super Mario Bros.: The Lost Levels, can be played now on Nintendo Switch, through Nintendo Switch Online\n\n## Features\n- Super Mario Bros., Super Mario Bros.: The Lost Levels, Super Mario Bros. Special and All Night Nippon: Super Mario Bros. Fully recreated from the ground up!\n- Improved physics / level design\n- Resource Packs! Fully customize how the game looks and sounds.\n- Custom Characters - Add in your own characters to use in game.\n- Fully Open Source!\n- Level Share Square Partnered\n- Portable mode by creating `portable.txt` in the executable directory\n\n## Downloading\n\n### Windows/Linux\n1. Go to the 'Releases' page\n2. Look for the latest version\n3. Download the .zip for your OS\n4. Extract and run\n5. Enjoy!\n\n### macOS (Unofficial)\n1. Go to the [macOS repo](https://github.com/yuriko-shimizu/Super-Mario-Bros.-Remastered-Public-Mac/releases)\n2. (NOTE: THIS IS AN UNOFFICIAL FORK OF THE GAME)\n3. Look for the latest version\n4. Download the .zip file\n5. Extract, drag into the 'Applictions' folder and run\n6. Enjoy!\n\n## Importing for editing\n1. Download the source\n2. Download Godot 4.5 beta 3\n3. Import the project\n4. Enjoy!\n\n## Contributing\nYou are more than welcome to contribute any fixes / improvements you'd like, simply open a pull request, and I'll review it ASAP!\n\n## System Requirements\n\nPlease refer to the Godot engine requirements for minimum and recommended hardware specifications.\n\n[Minimum Requirements](https://docs.godotengine.org/en/latest/about/system_requirements.html#desktop-or-laptop-pc-minimum)\n\n[Recommended Requirements](https://docs.godotengine.org/en/latest/about/system_requirements.html#id3)\n\n\n## Issues\nWhen opening an issue, please keep it to one report, per post, and try and be as helpful as possible, when telling me what has occured, so that its as easy to fix as possible.\nPlease do not open issues, for feature requests, suggestions, or opinions. BUG REPORTS ONLY\n\n## Known Issues\nThere are a couple known issues, mainly due to being built off of Godot, and these issues existing in the engine itself.\n- Steam deck controls do not work natively, you can circumvent this by setting up controller bindings to emulate keys instead, apologies.\n- Physics are weird, when interacting with corners + the camera barrier\n- Drop shadows jitter when playing with \"Smooth Rendering\"\n- Several entities jitter at times.\n- Blocks + coins, respawn when reloading resource packs\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-10-11T02:14:32.457016"
  },
  {
    "basic_info": {
      "name": "sidekick.nvim",
      "full_name": "folke/sidekick.nvim",
      "owner": "folke",
      "description": "Your Neovim AI sidekick",
      "url": "https://github.com/folke/sidekick.nvim",
      "clone_url": "https://github.com/folke/sidekick.nvim.git",
      "ssh_url": "git@github.com:folke/sidekick.nvim.git",
      "homepage": "",
      "created_at": "2025-09-26T10:26:48Z",
      "updated_at": "2025-10-11T01:47:06Z",
      "pushed_at": "2025-10-10T23:04:47Z"
    },
    "stats": {
      "stars": 1443,
      "forks": 31,
      "watchers": 1443,
      "open_issues": 3,
      "size": 438
    },
    "tech_info": {
      "language": "Lua",
      "languages": {
        "Lua": 193566,
        "Shell": 127
      },
      "license": "Apache License 2.0",
      "topics": [
        "claude-code",
        "codex-cli",
        "copilot",
        "copilot-cli",
        "gemini-cli",
        "neovim",
        "neovim-plugin",
        "nvim",
        "nvim-plugin"
      ]
    },
    "content": {
      "readme": "# 🤖 `sidekick.nvim`\n\n**sidekick.nvim** is your Neovim AI sidekick that integrates Copilot LSP's\n\"Next Edit Suggestions\" with a built-in terminal for any AI CLI.\nReview and apply diffs, chat with AI assistants, and streamline your coding,\nwithout leaving your editor.\n\n<img width=\"2311\" height=\"1396\" alt=\"image\" src=\"https://github.com/user-attachments/assets/63a33610-9a8e-45e2-bbd0-b7e3a4fde621\" />\n\n## ✨ Features\n\n- **🤖 Next Edit Suggestions (NES) powered by Copilot LSP**\n  - 🪄 **Automatic Suggestions**: Fetches suggestions automatically when you pause typing or move the cursor.\n  - 🎨 **Rich Diffs**: Visualizes changes with inline and block-level diffs, featuring Treesitter-based syntax highlighting with granular diffing down to the word or character level.\n  - 🧭 **Hunk-by-Hunk Navigation**: Jump through edits to review them one by one before applying.\n  - 📊 **Statusline Integration**: Shows Copilot LSP's status, request progress, and preview text in your statusline.\n\n- **💬 Integrated AI CLI Terminal**\n  - 🚀 **Direct Access to AI CLIs**: Interact with your favorite AI command-line tools without leaving Neovim.\n  - 📦 **Pre-configured for Popular Tools**: Out-of-the-box support for Claude, Gemini, Grok, Codex, Copilot CLI, and more.\n  - ✨ **Context-Aware Prompts**: Automatically include file content, cursor position, and diagnostics in your prompts.\n  - 📝 **Prompt Library**: A library of pre-defined prompts for common tasks like explaining code, fixing issues, or writing tests.\n  - 🔄 **Session Persistence**: Keep your CLI sessions alive with `tmux` and `zellij` integration.\n  - 📂 **Automatic File Watching**: Automatically reloads files in Neovim when they are modified by AI tools.\n\n- **🔌 Extensible and Customizable**\n  - ⚙️ **Flexible Configuration**: Fine-tune every aspect of the plugin to your liking.\n  - 🧩 **Plugin-Friendly API**: A rich API for integrating with other plugins and building custom workflows.\n  - 🎨 **Customizable UI**: Change the appearance of diffs, signs, and more.\n\n## 📋 Requirements\n\n- **Neovim** `>= 0.11.2` or newer\n- The official [copilot-language-server](https://github.com/github/copilot-language-server-release) LSP server,\n  enabled with `vim.lsp.enable`. Can be installed in multiple ways:\n  1. install using `npm` or your OS's package manager\n  2. install with [mason-lspconfig.nvim](https://github.com/mason-org/mason-lspconfig.nvim)\n  3. [copilot.lua](https://github.com/zbirenbaum/copilot.lua) and [copilot.vim](https://github.com/github/copilot.vim)\n     both bundle the LSP Server in their plugin.\n- A working `lsp/copilot.lua` configuration.\n  - **TIP:** Included in [nvim-lspconfig](https://github.com/neovim/nvim-lspconfig)\n- [snacks.nvim](https://github.com/folke/snacks.nvim) for better prompt/tool selection **_(optional)_**\n- [nvim-treesitter-textobjects](https://github.com/nvim-treesitter/nvim-treesitter-textobjects) **_(`main` branch)_** for `{function}` and `{class}` context variables **_(optional)_**\n- AI cli tools, such as Codex, Claude, Copilot, Gemini, … **_(optional)_**\n  see the [🤖 AI CLI Integration](#-ai-cli-integration) section for details.\n- [lsof](https://man7.org/linux/man-pages/man8/lsof.8.html) and [ps](https://man7.org/linux/man-pages/man1/ps.1.html) are used\n  on Unix-like systems to detect running AI CLI tool sessions. **_(optional, but recommended)_**\n\n## 🚀 Quick Start\n\n1. **Install** the plugin with your package manager (see below)\n2. **Configure Copilot LSP** - must be enabled with `vim.lsp.enable`\n3. **Check health**: `:checkhealth sidekick`\n4. **Sign in to Copilot**: `:LspCopilotSignIn`\n5. **Try it out**:\n   - Type some code and pause - watch for Next Edit Suggestions appearing\n   - Press `<Tab>` to navigate through or apply suggestions\n   - Use `<leader>aa` to open AI CLI tools\n\n> [!NOTE]\n> **New to Next Edit Suggestions?** Unlike inline completions, NES suggests entire refactorings or multi-line changes anywhere in your file - think of it as Copilot's \"big picture\" suggestions.\n\n## 📦 Installation\n\nInstall with your favorite manager. With [lazy.nvim](https://github.com/folke/lazy.nvim):\n\n<!-- setup_base:start -->\n\n```lua\n{\n  \"folke/sidekick.nvim\",\n  opts = {\n    -- add any options here\n    cli = {\n      mux = {\n        backend = \"zellij\",\n        enabled = true,\n      },\n    },\n  },\n  keys = {\n    {\n      \"<tab>\",\n      function()\n        -- if there is a next edit, jump to it, otherwise apply it if any\n        if not require(\"sidekick\").nes_jump_or_apply() then\n          return \"<Tab>\" -- fallback to normal tab\n        end\n      end,\n      expr = true,\n      desc = \"Goto/Apply Next Edit Suggestion\",\n    },\n    {\n      \"<c-.>\",\n      function() require(\"sidekick.cli\").toggle() end,\n      desc = \"Sidekick Toggle\",\n      mode = { \"n\", \"t\", \"i\", \"x\" },\n    },\n    {\n      \"<leader>aa\",\n      function() require(\"sidekick.cli\").toggle() end,\n      desc = \"Sidekick Toggle CLI\",\n    },\n    {\n      \"<leader>as\",\n      function() require(\"sidekick.cli\").select() end,\n ",
      "default_branch": "main"
    },
    "fetched_at": "2025-10-11T02:14:33.728662"
  },
  {
    "basic_info": {
      "name": "astron-agent",
      "full_name": "iflytek/astron-agent",
      "owner": "iflytek",
      "description": "An enterprise-ready AI agent development platform combining intelligent RPA, enabling agents to not only think but also act—automating workflows across digital and desktop environments.",
      "url": "https://github.com/iflytek/astron-agent",
      "clone_url": "https://github.com/iflytek/astron-agent.git",
      "ssh_url": "git@github.com:iflytek/astron-agent.git",
      "homepage": null,
      "created_at": "2025-09-19T08:46:01Z",
      "updated_at": "2025-10-11T02:02:08Z",
      "pushed_at": "2025-10-11T02:02:04Z"
    },
    "stats": {
      "stars": 1389,
      "forks": 132,
      "watchers": 1389,
      "open_issues": 2,
      "size": 78705
    },
    "tech_info": {
      "language": "TypeScript",
      "languages": {
        "TypeScript": 4015557,
        "Python": 3769100,
        "Java": 3110607,
        "PLpgSQL": 1590068,
        "SCSS": 407596,
        "JavaScript": 133840,
        "Makefile": 88598,
        "Go": 67935,
        "Shell": 38887,
        "C": 13024,
        "Dockerfile": 7699,
        "CSS": 6547,
        "HTML": 3038
      },
      "license": "Apache License 2.0",
      "topics": []
    },
    "content": {
      "readme": "# Astron Agent - Agent Platform\n\n<div align=\"center\">\n\n![Logo](docs/logo.svg)\n\n**Astron Agent is an enterprise-grade Agent development platform designed for AI developers and small to medium-sized enterprises.**\n\n[![License](https://img.shields.io/badge/license-apache2.0-blue.svg)](LICENSE)\n[![Version](https://img.shields.io/github/v/release/iflytek/astron-agent)](https://github.com/iflytek/astron-agent/releases)\n[![Build Status](https://img.shields.io/github/actions/workflow/status/iflytek/astron-agent/ci.yml)](https://github.com/iflytek/astron-agent/actions)\n[![Coverage](https://img.shields.io/codecov/c/github/iflytek/astron-agent)](https://codecov.io/gh/iflytek/astron-agent)\n[![GitHub Stars](https://img.shields.io/github/stars/iflytek/astron-agent?style=social)](https://github.com/iflytek/astron-agent/stargazers)\n\nEnglish | [简体中文](README-zh.md)\n\n</div>\n\n## 📑 Table of Contents\n\n- [🔭 What is Astron Agent?](#-What-is-Astron-Agent)\n- [🛠️ Tech Stack](#%EF%B8%8F-tech-stack)\n- [🚀 Quick Start](#-quick-start)\n  - [Using Docker](#using-docker)\n- [📖 Usage Guide](#-usage-guide)\n- [📚 Documentation](#-documentation)\n- [🤝 Contributing](#-contributing)\n- [🌟 Star History](#-star-history)\n- [📞 Support](#-support)\n- [📄 License](#-license)\n\n## 🔭 What is Astron Agent\nAstron Agent is an enterprise-grade Agent development platform designed for AI developers and small to medium-sized enterprises.  \nIt not only provides end-to-end capabilities covering model hosting, application development, performance optimization, and access control, but also innovatively integrates intelligent RPA. This allows Agents not only to “think,” but also to truly “act,” completing complex task chains across digital systems and desktop environments.\n\n### Why Choose Astron Agent?\n- **Consistent and Reliable:** Shares the same core technology with [iFLYTEK Astron Agent Platform](https://agent.xfyun.cn), inheriting its proven enterprise-grade stability.  \n- **Closed Loop of Thinking + Acting:** Deep integration of intelligent RPA enables Agents to move from “generating answers” to “automatically completing tasks.”  \n- **Dual Value for Developers and Enterprises:** Developers can quickly get started and expand, while SMEs can efficiently implement digital workflows.  \n\n### Key Features\n- **Enterprise-Grade High Availability:** Full-stack capabilities for development, building, optimization, and management. Supports one-click deployment with strong reliability.  \n- **Intelligent RPA Integration:** Enables cross-system process automation, empowering Agents with controllable execution to achieve a complete loop “from decision to action.”  \n- **Ready-to-Use Tool Ecosystem:** Integrates massive AI capabilities and tools from the [iFLYTEK Open Platform](https://www.xfyun.cn), validated by millions of developers, supporting plug-and-play access without extra development.  \n- **Flexible Model Support:** Offers diverse access methods, from rapid API-based model validation to one-click deployment of enterprise-level MaaS (Model as a Service) local clusters, meeting needs of all scales.  \n\n### Developer Support\n- **Multi-language Backend:** Supports mainstream languages such as Java, Go, and Python; frontend adapted to TypeScript + React stack.  \n- **Comprehensive Toolchain:** Provides API documentation, deployment guides, and troubleshooting manuals to reduce learning and maintenance costs.  \n- **One-Click Deployment:** Built-in Dockerized environment for out-of-the-box setup and rapid project launch.  \n\n## 🛠️ Tech Stack\n\n- **Backend**: Java 21, Spring Boot 3, Go, Python 3.11\n- **Frontend**: TypeScript 5, React 18\n- **Database**: MySQL 8\n- **Cache**: Redis\n- **Queue**: Apache Kafka\n- **Infrastructure**: Docker, MinIO\n- **Quality Tools**: Checkstyle, PMD, SpotBugs, ESLint, gocyclo, staticcheck, golangci-lint, black, isort, flake8, mypy, pylint\n\n## 🚀 Quick Start\n\n### Using Docker\n\n```bash\n# Clone the repository\ngit clone https://github.com/iflytek/astron-agent.git\ncd astron-agent\n\n# Start the stack\ndocker-compose up -d\n```\n\n- Visit `http://localhost:8080` in your browser.\n\n## 📖 Usage Guide\n\nFor detailed usage instructions, please refer to [Usage Documentation](docs/USAGE.md)\n\n## 📚 Documentation\n\n- [📖 Usage Documentation](docs/USAGE.md)\n- [🚀 Deployment Guide](docs/DEPLOYMENT.md)\n- [📖 API Reference](docs/API.md)\n- [🔧 Configuration](docs/CONFIGURATION.md)\n- [🐛 Troubleshooting](docs/TROUBLESHOOTING.md)\n- [📝 Changelog](CHANGELOG.md)\n\n## 🤝 Contributing\n\nWe welcome contributions of all kinds! Please see our [Contributing Guide](CONTRIBUTING.md)\n\n## 🌟 Star History\n\n<div align=\"center\">\n  <img src=\"https://api.star-history.com/svg?repos=iflytek/astron-agent&type=Date\" alt=\"Star History Chart\" width=\"600\">\n</div>\n\n## 📞 Support\n\n- 💬 Community Discussion: [GitHub Discussions](https://github.com/iflytek/astron-agent/discussions)\n- 🐛 Bug Reports: [Issues](https://github.com/iflytek/astron-agent/issues)\n\n## 📄 License\n\nThis project is licensed under the [Apache 2.0 License](LICENSE",
      "default_branch": "main"
    },
    "fetched_at": "2025-10-11T02:14:35.014772"
  },
  {
    "basic_info": {
      "name": "chatkit-js",
      "full_name": "openai/chatkit-js",
      "owner": "openai",
      "description": null,
      "url": "https://github.com/openai/chatkit-js",
      "clone_url": "https://github.com/openai/chatkit-js.git",
      "ssh_url": "org-14957082@github.com:openai/chatkit-js.git",
      "homepage": "https://openai.github.io/chatkit-js/",
      "created_at": "2025-10-04T21:00:32Z",
      "updated_at": "2025-10-11T01:30:43Z",
      "pushed_at": "2025-10-09T22:31:56Z"
    },
    "stats": {
      "stars": 1271,
      "forks": 71,
      "watchers": 1271,
      "open_issues": 21,
      "size": 89
    },
    "tech_info": {
      "language": "TypeScript",
      "languages": {
        "TypeScript": 35451,
        "MDX": 29395,
        "CSS": 17401,
        "Astro": 7445,
        "JavaScript": 1712
      },
      "license": "Apache License 2.0",
      "topics": []
    },
    "content": {
      "readme": "ChatKit is a batteries-included framework for building high-quality, AI-powered chat experiences. It’s designed for developers who want to add advanced conversational intelligence to their apps fast—with minimal setup and no reinventing the wheel. ChatKit delivers a complete, production-ready chat interface out of the box.\n\n**Key features include:**\n\n- **Deep UI customization** so that ChatKit feels like a first-class part of your app\n- **Built-in response streaming** for interactive, natural conversations\n- **Tool and workflow integration** for visualizing agentic actions and chain-of-thought reasoning\n- **Rich interactive widgets** rendered directly inside the chat\n- **Attachment handling** with support for file and image uploads\n- **Thread and message management** for organizing complex conversations\n- **Source annotations and entity tagging** for transparency and references\n\nSimply drop the ChatKit component into your app, configure a few options, and you're good to go.\n\n### What makes ChatKit different?\n\nChatKit is a framework-agnostic, drop-in chat solution.\nYou don’t need to build custom UIs, manage low-level chat state, or patch together various features yourself.\nJust add the ChatKit component, give it a client token, and customize the chat experience as needed, no extra work needed.\n\n## Quickstart\n\n1. Generate a client token on your server.\n\n   ```python\n   from fastapi import FastAPI\n   from pydantic import BaseModel\n   from openai import OpenAI\n   import os\n\n   app = FastAPI()\n   openai = OpenAI(api_key=os.environ[\"OPENAI_API_KEY\"])\n\n   @app.post(\"/api/chatkit/session\")\n   def create_chatkit_session():\n       session = openai.chatkit.sessions.create({\n         # ...\n       })\n       return { client_secret: session.client_secret }\n   ```\n\n2. Install the React bindings\n\n   ```bash\n   npm install @openai/chatkit-react\n   ```\n\n3. Add the ChatKit JS script to your page\n\n   ```html\n   <script\n     src=\"https://cdn.platform.openai.com/deployments/chatkit/chatkit.js\"\n     async\n   ></script>\n   ```\n\n4. Render ChatKit\n\n   ```tsx\n   import { ChatKit, useChatKit } from '@openai/chatkit-react';\n\n   export function MyChat() {\n     const { control } = useChatKit({\n       api: {\n         async getClientSecret(existing) {\n           if (existing) {\n             // implement session refresh\n           }\n\n           const res = await fetch('/api/chatkit/session', {\n             method: 'POST',\n             headers: {\n               'Content-Type': 'application/json',\n             },\n           });\n           const { client_secret } = await res.json();\n           return client_secret;\n         },\n       },\n     });\n\n     return <ChatKit control={control} className=\"h-[600px] w-[320px]\" />;\n   }\n   ```\n\n## See working examples\n\n- [Starter app](https://github.com/openai/openai-chatkit-starter-app) - Clone a repo to start with a fully working template\n- [Samples](https://github.com/openai/openai-chatkit-advanced-samples) - See working examples of ChatKit and get inspired\n\n## License\n\nThis project is licensed under the [Apache License 2.0](LICENSE).\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-10-11T02:14:36.265745"
  },
  {
    "basic_info": {
      "name": "Selene",
      "full_name": "MoonTechLab/Selene",
      "owner": "MoonTechLab",
      "description": "一个以 MoonTV v100 版本 / Helios 为后端的 Android/iOS 客户端，针对移动端体验优化",
      "url": "https://github.com/MoonTechLab/Selene",
      "clone_url": "https://github.com/MoonTechLab/Selene.git",
      "ssh_url": "git@github.com:MoonTechLab/Selene.git",
      "homepage": "",
      "created_at": "2025-09-20T08:29:29Z",
      "updated_at": "2025-10-11T02:07:07Z",
      "pushed_at": "2025-10-09T16:14:48Z"
    },
    "stats": {
      "stars": 1271,
      "forks": 161,
      "watchers": 1271,
      "open_issues": 0,
      "size": 7590
    },
    "tech_info": {
      "language": null,
      "languages": {},
      "license": null,
      "topics": []
    },
    "content": {
      "readme": "# Selene\n\n<div align=\"center\">\n  <img src=\"logo.jpg\" alt=\"Selene Logo\" width=\"120\">\n</div>\n\n> 🎬 **Selene** 是以 [MoonTV](https://github.com/MoonTechLab/LunaTV) v100 版本 / [Helios](https://github.com/MoonTechLab/Helios) 为后端的客户端，保证原汁原味的同时，优化了移动端操作体验。它基于 **Flutter** 构建，专为移动端打造，目前支持 Android-ArmV8 和 iOS 平台。\n\n<div align=\"center\">\n\n![Flutter](https://img.shields.io/badge/Flutter-3.4.3-02569B?logo=flutter)\n![Dart](https://img.shields.io/badge/Dart-3.4.3-0175C2?logo=dart)\n![Android](https://img.shields.io/badge/Android-21+-3DDC84?logo=android)\n![iOS](https://img.shields.io/badge/iOS-12.0+-000000?logo=ios)\n\n</div>\n\n<details>\n  <summary>点击查看项目截图</summary>\n  <img src=\"screenshot1.jpg\" alt=\"项目截图\" width=300>\n  <img src=\"screenshot2.jpg\" alt=\"项目截图\" width=300>\n  <img src=\"screenshot3.jpg\" alt=\"项目截图\" width=300>\n  <img src=\"screenshot4.jpg\" alt=\"项目截图\" width=300>\n  <img src=\"screenshot5.jpg\" alt=\"项目截图\" width=300>\n  <img src=\"screenshot6.jpg\" alt=\"项目截图\" width=300>\n</details>\n\n### 请不要在 B站、小红书、微信公众号、抖音、今日头条或其他中国大陆社交平台发布视频或文章宣传本项目，不授权任何“科技周刊/月刊”类项目或站点收录本项目。\n\n---\n\n## ✨ 功能特性\n\n### 🎯 核心功能\n- **多源聚合搜索** - 支持多个视频源的聚合搜索，快速找到想看的内容\n- **智能播放记录** - 自动记录播放进度，支持断点续播\n- **个人收藏夹** - 收藏喜欢的影视作品，方便随时观看\n- **多平台支持** - 支持电影、电视剧、动漫、综艺等多种内容类型\n- **DLNA 投屏** - 大屏看片就是爽\n\n### 🎨 用户体验\n- **现代化 UI** - 基于 Material Design 3 的现代化界面设计\n- **深色模式** - 支持深色/浅色主题切换，护眼更舒适\n- **流畅动画** - 丰富的交互动画，提升使用体验\n\n### 🔧 技术特性\n- **高性能播放** - 基于 FVP 播放器，支持多种视频格式\n- **智能缓存** - 豆瓣数据缓存机制，提升加载速度\n- **网络优化** - 支持 SSE 实时搜索，响应更迅速\n\n## 📱 支持平台\n\n- **Android** - 最低支持 Android 5.0 (API 21)\n- **iOS** - 最低支持 iOS 12.0\n\n## 📖 使用说明\n\n### 首次使用\n1. 启动应用后，系统会自动检查登录状态\n2. 如未登录，会跳转到登录页面\n3. 登录成功后进入主界面\n\n### 主要功能\n- **首页** - 查看热门内容、继续观看、个人收藏\n- **搜索** - 多源聚合搜索，支持实时搜索建议\n- **分类浏览** - 按电影、电视剧、动漫、综艺分类浏览\n- **播放器** - 支持多种播放控制，自动记录播放进度\n\n## 🏗️ 技术架构\n\n### 核心技术栈\n- **Flutter** - 跨平台 UI 框架\n- **Dart** - 编程语言\n- **Provider** - 状态管理\n- **Dio** - HTTP 网络请求\n- **FVP** - 视频播放器\n\n## ⚠️ 免责声明\n\n**重要提醒：**\n\n1. **仅供学习交流** - 本项目仅用于技术学习和交流目的，不提供任何商业服务。\n\n2. **内容来源** - 本应用聚合的内容来源于第三方平台，我们不对内容的合法性、准确性、完整性或可用性承担任何责任。\n\n3. **版权声明** - 所有影视内容的版权归原作者和版权方所有，请用户自觉遵守相关法律法规，支持正版。\n\n4. **使用风险** - 用户使用本应用所产生的任何直接或间接损失，开发者不承担任何责任。\n\n5. **合规使用** - 请用户在使用过程中遵守当地法律法规，不得用于任何违法用途。\n\n6. **数据安全** - 虽然我们重视用户隐私，但请用户自行承担数据安全风险。\n\n**使用本应用即表示您已阅读并同意上述免责声明。**\n\n## 🙏 致谢\n\n- [MoonTV](https://github.com/MoonTechLab/LunaTV) - 后端服务支持\n- [Flutter](https://flutter.dev/) - 跨平台开发框架\n- 所有用户的支持\n---\n\n<div align=\"center\">\n  <p>如果这个项目对您有帮助，请给个 ⭐️ 支持一下！</p>\n</div>\n\n[![Star History Chart](https://api.star-history.com/svg?repos=MoonTechLab/Selene&type=Date)](https://www.star-history.com/#MoonTechLab/Selene&Date)\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-10-11T02:14:37.503493"
  },
  {
    "basic_info": {
      "name": "openai-apps-sdk-examples",
      "full_name": "openai/openai-apps-sdk-examples",
      "owner": "openai",
      "description": "Example apps for the Apps SDK",
      "url": "https://github.com/openai/openai-apps-sdk-examples",
      "clone_url": "https://github.com/openai/openai-apps-sdk-examples.git",
      "ssh_url": "org-14957082@github.com:openai/openai-apps-sdk-examples.git",
      "homepage": null,
      "created_at": "2025-10-06T05:28:01Z",
      "updated_at": "2025-10-11T01:57:33Z",
      "pushed_at": "2025-10-08T18:45:49Z"
    },
    "stats": {
      "stars": 1258,
      "forks": 158,
      "watchers": 1258,
      "open_issues": 25,
      "size": 121
    },
    "tech_info": {
      "language": "JavaScript",
      "languages": {
        "JavaScript": 90208,
        "CSS": 64226,
        "TypeScript": 28354,
        "Python": 19614
      },
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "# Apps SDK Examples Gallery\n\n[![MIT License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)\n\nThis repository showcases example UI components to be used with the Apps SDK, as well as example MCP servers that expose a collection of components as tools.\nIt is meant to be used as a starting point and source of inspiration to build your own apps for ChatGPT.\n\n## MCP + Apps SDK overview\n\nThe Model Context Protocol (MCP) is an open specification for connecting large language model clients to external tools, data, and user interfaces. An MCP server exposes tools that a model can call during a conversation and returns results according to the tool contracts. Those results can include extra metadata—such as inline HTML—that the Apps SDK uses to render rich UI components (widgets) alongside assistant messages.\n\nWithin the Apps SDK, MCP keeps the server, model, and UI in sync. By standardizing the wire format, authentication, and metadata, it lets ChatGPT reason about your connector the same way it reasons about built-in tools. A minimal MCP integration for Apps SDK implements three capabilities:\n\n1. **List tools** – Your server advertises the tools it supports, including their JSON Schema input/output contracts and optional annotations (for example, `readOnlyHint`).\n2. **Call tools** – When a model selects a tool, it issues a `call_tool` request with arguments that match the user intent. Your server executes the action and returns structured content the model can parse.\n3. **Return widgets** – Alongside structured content, return embedded resources in the response metadata so the Apps SDK can render the interface inline in the Apps SDK client (ChatGPT).\n\nBecause the protocol is transport agnostic, you can host the server over Server-Sent Events or streaming HTTP—Apps SDK supports both.\n\nThe MCP servers in this demo highlight how each tool can light up widgets by combining structured payloads with `_meta.openai/outputTemplate` metadata returned from the MCP servers.\n\n## Repository structure\n\n- `src/` – Source for each widget example.\n- `assets/` – Generated HTML, JS, and CSS bundles after running the build step.\n- `pizzaz_server_node/` – MCP server implemented with the official TypeScript SDK.\n- `pizzaz_server_python/` – Python MCP server that returns the Pizzaz widgets.\n- `solar-system_server_python/` – Python MCP server for the 3D solar system widget.\n- `build-all.mts` – Vite build orchestrator that produces hashed bundles for every widget entrypoint.\n\n## Prerequisites\n\n- Node.js 18+\n- pnpm (recommended) or npm/yarn\n- Python 3.10+ (for the Python MCP server)\n\n## Install dependencies\n\nClone the repository and install the workspace dependencies:\n\n```bash\npnpm install\n```\n\n> Using npm or yarn? Install the root dependencies with your preferred client and adjust the commands below accordingly.\n\n## Build the components gallery\n\nThe components are bundled into standalone assets that the MCP servers serve as reusable UI resources.\n\n```bash\npnpm run build\n```\n\nThis command runs `build-all.mts`, producing versioned `.html`, `.js`, and `.css` files inside `assets/`. Each widget is wrapped with the CSS it needs so you can host the bundles directly or ship them with your own server.\n\nTo iterate locally, you can also launch the Vite dev server:\n\n```bash\npnpm run dev\n```\n\n## Serve the static assets\n\nIf you want to preview the generated bundles without the MCP servers, start the static file server after running a build:\n\n```bash\npnpm run serve\n```\n\nThe assets are exposed at [`http://localhost:4444`](http://localhost:4444) with CORS enabled so that local tooling (including MCP inspectors) can fetch them.\n\n## Run the MCP servers\n\nThe repository ships several demo MCP servers that highlight different widget bundles:\n\n- **Pizzaz (Node & Python)** – pizza-inspired collection of tools and components\n- **Solar system (Python)** – 3D solar system viewer\n\nEvery tool response includes plain text content, structured JSON, and `_meta.openai/outputTemplate` metadata so the Apps SDK can hydrate the matching widget.\n\n### Pizzaz Node server\n\n```bash\ncd pizzaz_server_node\npnpm start\n```\n\n### Pizzaz Python server\n\n```bash\npython -m venv .venv\nsource .venv/bin/activate\npip install -r pizzaz_server_python/requirements.txt\nuvicorn pizzaz_server_python.main:app --port 8000\n```\n\n### Solar system Python server\n\n```bash\npython -m venv .venv\nsource .venv/bin/activate\npip install -r solar-system_server_python/requirements.txt\nuvicorn solar-system_server_python.main:app --port 8000\n```\n\nYou can reuse the same virtual environment for all Python servers—install the dependencies once and run whichever entry point you need.\n\n## Testing in ChatGPT\n\nTo add these apps to ChatGPT, enable [developer mode](https://platform.openai.com/docs/guides/developer-mode), and add your apps in Settings > Connectors.\n\nTo add your local server without deploying it, you can use a tool like [ngrok](https://ngrok.com/) to expose your local server to the internet.\n\nFor ex",
      "default_branch": "main"
    },
    "fetched_at": "2025-10-11T02:14:38.766348"
  },
  {
    "basic_info": {
      "name": "sj.h",
      "full_name": "rxi/sj.h",
      "owner": "rxi",
      "description": "A tiny little JSON parsing library",
      "url": "https://github.com/rxi/sj.h",
      "clone_url": "https://github.com/rxi/sj.h.git",
      "ssh_url": "git@github.com:rxi/sj.h.git",
      "homepage": null,
      "created_at": "2025-09-19T18:20:20Z",
      "updated_at": "2025-10-10T15:06:45Z",
      "pushed_at": "2025-09-21T18:29:44Z"
    },
    "stats": {
      "stars": 1231,
      "forks": 27,
      "watchers": 1231,
      "open_issues": 4,
      "size": 10
    },
    "tech_info": {
      "language": "C",
      "languages": {
        "C": 4081
      },
      "license": "The Unlicense",
      "topics": []
    },
    "content": {
      "readme": "# sj.h\nA tiny little JSON parsing library\n\n- ~150 lines of C99\n- Zero-allocations with minimal state\n- Error messages with `line:column:` location\n- No number parsing: `strtod`, `atoi`? Handle them how you want\n- No string parsing: bring your own unicode surrogate pair handling (or don't)\n\n\n## Usage\nA small program to load a rectangle from a JSON string into a `Rect` struct:\n```c\nchar *json_text = \"{ \\\"x\\\": 10, \\\"y\\\": 20, \\\"w\\\": 30, \\\"h\\\": 40 }\";\n\ntypedef struct { int x, y, w, h; } Rect;\n\nbool eq(sj_Value val, char *s) {\n    size_t len = val.end - val.start;\n    return strlen(s) == len && !memcmp(s, val.start, len);\n}\n\nint main(void) {\n    Rect rect = {0};\n\n    sj_Reader r = sj_reader(json_text, strlen(json_text));\n    sj_Value obj = sj_read(&r);\n\n    sj_Value key, val;\n    while (sj_iter_object(&r, obj, &key, &val)) {\n        if (eq(key, \"x\")) { rect.x = atoi(val.start); }\n        if (eq(key, \"y\")) { rect.y = atoi(val.start); }\n        if (eq(key, \"w\")) { rect.w = atoi(val.start); }\n        if (eq(key, \"h\")) { rect.h = atoi(val.start); }\n    }\n\n    printf(\"rect: { %d, %d, %d, %d }\\n\", rect.x, rect.y, rect.w, rect.h);\n    return 0;\n}\n```\n\nSee the [**demo**](demo/) folder for further usage examples.\n\n\n## License\nThis is free and unencumbered software released into the public domain. See\n[LICENSE](LICENSE) for details.",
      "default_branch": "master"
    },
    "fetched_at": "2025-10-11T02:14:40.016989"
  }
]