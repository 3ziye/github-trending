[
  {
    "basic_info": {
      "name": "RustGPT",
      "full_name": "tekaratzas/RustGPT",
      "owner": "tekaratzas",
      "description": "An transformer based LLM. Written completely in Rust",
      "url": "https://github.com/tekaratzas/RustGPT",
      "clone_url": "https://github.com/tekaratzas/RustGPT.git",
      "ssh_url": "git@github.com:tekaratzas/RustGPT.git",
      "homepage": null,
      "created_at": "2025-09-13T22:05:55Z",
      "updated_at": "2025-09-25T23:35:07Z",
      "pushed_at": "2025-09-25T13:50:46Z"
    },
    "stats": {
      "stars": 2677,
      "forks": 216,
      "watchers": 2677,
      "open_issues": 4,
      "size": 191
    },
    "tech_info": {
      "language": "Rust",
      "languages": {
        "Rust": 62444
      },
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "# 🦀 Rust LLM from Scratch\n\n[![Rust](https://github.com/tekaratzas/RustGPT/actions/workflows/rust.yml/badge.svg)](https://github.com/tekaratzas/RustGPT/actions/workflows/rust.yml)\n\nhttps://github.com/user-attachments/assets/ec4a4100-b03a-4b3c-a7d6-806ea54ed4ed\n\nA complete **Large Language Model implementation in pure Rust** with no external ML frameworks. Built from the ground up using only `ndarray` for matrix operations.\n\n## 🚀 What This Is\n\nThis project demonstrates how to build a transformer-based language model from scratch in Rust, including:\n- **Pre-training** on factual text completion\n- **Instruction tuning** for conversational AI\n- **Interactive chat mode** for testing\n- **Full backpropagation** with gradient clipping\n- **Modular architecture** with clean separation of concerns\n\n## ❌ What This Isn't\n\nThis is not a production grade LLM. It is so far away from the larger models.\n\nThis is just a toy project that demonstrates how these models work under the hood.\n\n## 🔍 Key Files to Explore\n\nStart with these two core files to understand the implementation:\n\n- **[`src/main.rs`](src/main.rs)** - Training pipeline, data preparation, and interactive mode\n- **[`src/llm.rs`](src/llm.rs)** - Core LLM implementation with forward/backward passes and training logic\n\n## 🏗️ Architecture\n\nThe model uses a **transformer-based architecture** with the following components:\n\n```\nInput Text → Tokenization → Embeddings → Transformer Blocks → Output Projection → Predictions\n```\n\n### Project Structure\n\n```\nsrc/\n├── main.rs              # 🎯 Training pipeline and interactive mode\n├── llm.rs               # 🧠 Core LLM implementation and training logic\n├── lib.rs               # 📚 Library exports and constants\n├── transformer.rs       # 🔄 Transformer block (attention + feed-forward)\n├── self_attention.rs    # 👀 Multi-head self-attention mechanism\n├── feed_forward.rs      # ⚡ Position-wise feed-forward networks\n├── embeddings.rs        # 📊 Token embedding layer\n├── output_projection.rs # 🎰 Final linear layer for vocabulary predictions\n├── vocab.rs            # 📝 Vocabulary management and tokenization\n├── layer_norm.rs       # 🧮 Layer normalization\n└── adam.rs             # 🏃 Adam optimizer implementation\n\ntests/\n├── llm_test.rs         # Tests for core LLM functionality\n├── transformer_test.rs # Tests for transformer blocks\n├── self_attention_test.rs # Tests for attention mechanisms\n├── feed_forward_test.rs # Tests for feed-forward layers\n├── embeddings_test.rs  # Tests for embedding layers\n├── vocab_test.rs       # Tests for vocabulary handling\n├── adam_test.rs        # Tests for optimizer\n└── output_projection_test.rs # Tests for output layer\n```\n\n## 🧪 What The Model Learns\n\nThe implementation includes two training phases:\n\n1. **Pre-training**: Learns basic world knowledge from factual statements\n   - \"The sun rises in the east and sets in the west\"\n   - \"Water flows downhill due to gravity\"\n   - \"Mountains are tall and rocky formations\"\n\n2. **Instruction Tuning**: Learns conversational patterns\n   - \"User: How do mountains form? Assistant: Mountains are formed through tectonic forces...\"\n   - Handles greetings, explanations, and follow-up questions\n\n## 🚀 Quick Start\n\n```bash\n# Clone and run\ngit clone https://github.com/tekaratzas/RustGPT.git\ncd RustGPT\ncargo run\n\n# The model will:\n# 1. Build vocabulary from training data\n# 2. Pre-train on factual statements (100 epochs)\n# 3. Instruction-tune on conversational data (100 epochs)\n# 4. Enter interactive mode for testing\n```\n\n## 🎮 Interactive Mode\n\nAfter training, test the model interactively:\n\n```\nEnter prompt: How do mountains form?\nModel output: Mountains are formed through tectonic forces or volcanism over long geological time periods\n\nEnter prompt: What causes rain?\nModel output: Rain is caused by water vapor in clouds condensing into droplets that become too heavy to remain airborne\n```\n\n## 🧮 Technical Implementation\n\n### Model Configuration\n- **Vocabulary Size**: Dynamic (built from training data)\n- **Embedding Dimension**: 128 (defined by `EMBEDDING_DIM` in `src/lib.rs`)\n- **Hidden Dimension**: 256 (defined by `HIDDEN_DIM` in `src/lib.rs`)\n- **Max Sequence Length**: 80 tokens (defined by `MAX_SEQ_LEN` in `src/lib.rs`)\n- **Architecture**: 3 Transformer blocks + embeddings + output projection\n\n### Training Details\n- **Optimizer**: Adam with gradient clipping\n- **Pre-training LR**: 0.0005 (100 epochs)\n- **Instruction Tuning LR**: 0.0001 (100 epochs)\n- **Loss Function**: Cross-entropy loss\n- **Gradient Clipping**: L2 norm capped at 5.0\n\n### Key Features\n- **Custom tokenization** with punctuation handling\n- **Greedy decoding** for text generation\n- **Gradient clipping** for training stability\n- **Modular layer system** with clean interfaces\n- **Comprehensive test coverage** for all components\n\n## 🔧 Development\n\n```bash\n# Run all tests\ncargo test\n\n# Test specific components\ncargo test --test llm_test\ncargo test --test transformer_test\ncargo test --test self_attention_test\n\n# Buil",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-26T02:19:33.073519"
  },
  {
    "basic_info": {
      "name": "shimmy",
      "full_name": "Michael-A-Kuykendall/shimmy",
      "owner": "Michael-A-Kuykendall",
      "description": "⚡ Python-free Rust inference server — OpenAI-API compatible. GGUF + SafeTensors, hot model swap, auto-discovery, single binary. FREE now, FREE forever.",
      "url": "https://github.com/Michael-A-Kuykendall/shimmy",
      "clone_url": "https://github.com/Michael-A-Kuykendall/shimmy.git",
      "ssh_url": "git@github.com:Michael-A-Kuykendall/shimmy.git",
      "homepage": "",
      "created_at": "2025-08-28T22:55:46Z",
      "updated_at": "2025-09-26T01:48:13Z",
      "pushed_at": "2025-09-24T02:59:52Z"
    },
    "stats": {
      "stars": 2570,
      "forks": 172,
      "watchers": 2570,
      "open_issues": 2,
      "size": 213430
    },
    "tech_info": {
      "language": "Rust",
      "languages": {
        "Rust": 617150,
        "C": 158306,
        "C++": 77564,
        "Shell": 46795,
        "TOML": 18131,
        "Python": 14810,
        "TypeScript": 10203,
        "JavaScript": 7526,
        "YAML": 6387,
        "Dockerfile": 4809,
        "Batchfile": 4747,
        "Ruby": 931
      },
      "license": "MIT License",
      "topics": [
        "api-server",
        "command-line-tool",
        "developer-tools",
        "gguf",
        "huggingface",
        "huggingface-models",
        "huggingface-transformers",
        "inference-server",
        "llama",
        "llamacpp",
        "llm-inference",
        "local-ai",
        "lora",
        "machine-learning",
        "ollama-api",
        "openai-compatible",
        "rust",
        "rust-crate",
        "transformers"
      ]
    },
    "content": {
      "readme": "<div align=\"center\">\r\n  <img src=\"assets/shimmy-logo.png\" alt=\"Shimmy Logo\" width=\"300\" height=\"auto\" />\r\n  \r\n  # The Privacy-First Alternative to Ollama\r\n  \r\n  ### 🔒 Local AI Without the Lock-in 🚀\r\n\r\n  [![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\r\n  [![Security](https://img.shields.io/badge/Security-Audited-green)](https://github.com/Michael-A-Kuykendall/shimmy/security)\r\n  [![Crates.io](https://img.shields.io/crates/v/shimmy.svg)](https://crates.io/crates/shimmy)\r\n  [![Downloads](https://img.shields.io/crates/d/shimmy.svg)](https://crates.io/crates/shimmy)\r\n  [![Rust](https://img.shields.io/badge/rust-stable-brightgreen.svg)](https://rustup.rs/)\r\n  [![GitHub Stars](https://img.shields.io/github/stars/Michael-A-Kuykendall/shimmy?style=social)](https://github.com/Michael-A-Kuykendall/shimmy/stargazers)\r\n  \r\n  [![💝 Sponsor this project](https://img.shields.io/badge/💝_Sponsor_this_project-ea4aaa?style=for-the-badge&logo=github&logoColor=white)](https://github.com/sponsors/Michael-A-Kuykendall)\r\n</div>\r\n\r\n**Shimmy will be free forever.** No asterisks. No \"free for now.\" No pivot to paid.\r\n\r\n### 💝 Support Shimmy's Growth\r\n\r\n🚀 **If Shimmy helps you, consider [sponsoring](https://github.com/sponsors/Michael-A-Kuykendall) — 100% of support goes to keeping it free forever.**\r\n\r\n- **$5/month**: Coffee tier ☕ - Eternal gratitude + sponsor badge  \r\n- **$25/month**: Bug prioritizer 🐛 - Priority support + name in [SPONSORS.md](SPONSORS.md)\r\n- **$100/month**: Corporate backer 🏢 - Logo placement + monthly office hours  \r\n- **$500/month**: Infrastructure partner 🚀 - Direct support + roadmap input\r\n\r\n[**🎯 Become a Sponsor**](https://github.com/sponsors/Michael-A-Kuykendall) | See our amazing [sponsors](SPONSORS.md) 🙏\r\n\r\n---\r\n\r\n## Drop-in OpenAI API Replacement for Local LLMs\r\n\r\nShimmy is a **5.1MB single-binary** that provides **100% OpenAI-compatible endpoints** for GGUF models. Point your existing AI tools to Shimmy and they just work — locally, privately, and free.\r\n\r\n## 🤔 What are you building with Shimmy?\r\n\r\n**New developer tools and specifications included!** Whether you're forking Shimmy for your application or integrating it as a service, we now provide:\r\n\r\n- **🔧 Integration Templates**: Copy-paste guidance for embedding Shimmy in your projects\r\n- **📋 Development Specifications**: GitHub Spec-Kit methodology for planning Shimmy-based features\r\n- **🛡️ Architectural Guarantees**: Constitutional principles ensuring Shimmy stays reliable and lightweight\r\n- **📖 Complete Documentation**: Everything you need to build on Shimmy's foundation\r\n\r\n**Building something cool with Shimmy?** These tools help you do it systematically and reliably.\r\n\r\n### 🚀 **GitHub Spec-Kit Integration**\r\nShimmy now includes [GitHub's brand-new Spec-Kit methodology](https://github.com/github/spec-kit) – specification-driven development that just launched in September 2025! Get professional-grade development workflows:\r\n\r\n- **🏗️ Systematic Development**: `/specify` → `/plan` → `/tasks` → implement\r\n- **🤖 AI-Native Workflow**: Works with Claude Code, GitHub Copilot, and other AI assistants  \r\n- **📋 Professional Templates**: Complete specification and planning frameworks\r\n- **🛡️ Constitutional Protection**: Built-in governance and architectural validation\r\n\r\n[**📖 Complete Developer Guide →**](DEVELOPERS.md) • [**🛠️ Learn GitHub Spec-Kit →**](https://github.com/github/spec-kit)\r\n\r\n### Try it in 30 seconds\r\n\r\n```bash\r\n# 1) Install + run\r\ncargo install shimmy --features huggingface\r\nshimmy serve &\r\n\r\n# 2) See models and pick one\r\nshimmy list\r\n\r\n# 3) Smoke test the OpenAI API\r\ncurl -s http://127.0.0.1:11435/v1/chat/completions \\\r\n  -H 'Content-Type: application/json' \\\r\n  -d '{\r\n        \"model\":\"REPLACE_WITH_MODEL_FROM_list\",\r\n        \"messages\":[{\"role\":\"user\",\"content\":\"Say hi in 5 words.\"}],\r\n        \"max_tokens\":32\r\n      }' | jq -r '.choices[0].message.content'\r\n```\r\n\r\n## 🚀 Works with Your Existing Tools\r\n\r\n**No code changes needed** - just change the API endpoint:\r\n\r\n- **VSCode Extensions**: Point to `http://localhost:11435`\r\n- **Cursor Editor**: Built-in OpenAI compatibility  \r\n- **Continue.dev**: Drop-in model provider\r\n- **Any OpenAI client**: Python, Node.js, curl, etc.\r\n\r\n### Use with OpenAI SDKs\r\n\r\n- Node.js (openai v4)\r\n\r\n```ts\r\nimport OpenAI from \"openai\";\r\n\r\nconst openai = new OpenAI({\r\n  baseURL: \"http://127.0.0.1:11435/v1\",\r\n  apiKey: \"sk-local\", // placeholder, Shimmy ignores it\r\n});\r\n\r\nconst resp = await openai.chat.completions.create({\r\n  model: \"REPLACE_WITH_MODEL\",\r\n  messages: [{ role: \"user\", content: \"Say hi in 5 words.\" }],\r\n  max_tokens: 32,\r\n});\r\n\r\nconsole.log(resp.choices[0].message?.content);\r\n```\r\n\r\n- Python (openai>=1.0.0)\r\n\r\n```python\r\nfrom openai import OpenAI\r\n\r\nclient = OpenAI(base_url=\"http://127.0.0.1:11435/v1\", api_key=\"sk-local\")\r\n\r\nresp = client.chat.completions.create(\r\n    model=\"REPLACE_WITH_MODEL\",\r\n    messages=[{\"role\": \"user\", \"content\": \"Say hi i",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-26T02:19:34.211717"
  },
  {
    "basic_info": {
      "name": "desktop-tui",
      "full_name": "Julien-cpsn/desktop-tui",
      "owner": "Julien-cpsn",
      "description": "A desktop environment without graphics",
      "url": "https://github.com/Julien-cpsn/desktop-tui",
      "clone_url": "https://github.com/Julien-cpsn/desktop-tui.git",
      "ssh_url": "git@github.com:Julien-cpsn/desktop-tui.git",
      "homepage": null,
      "created_at": "2025-09-06T00:42:53Z",
      "updated_at": "2025-09-25T20:22:59Z",
      "pushed_at": "2025-09-21T20:51:37Z"
    },
    "stats": {
      "stars": 941,
      "forks": 18,
      "watchers": 941,
      "open_issues": 5,
      "size": 6353
    },
    "tech_info": {
      "language": "Rust",
      "languages": {
        "Rust": 46719,
        "Nix": 637
      },
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "Desktop-TUI 🖥️\n===\n\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n![GitHub Release](https://img.shields.io/github/v/release/julien-cpsn/desktop-tui?link=https%3A%2F%2Fgithub.com%2FJulien-cpsn%2Fdesktop-tuiC%2Freleases%2Flatest)\n[![Crates.io](https://repology.org/badge/version-for-repo/crates_io/desktop-tui.svg)](https://crates.io/crates/desktop-tui)\n\nA desktop environment without graphics (tmux-like).\n\nFeatures:\n- [x] Parse shortcut files containing apps\n  - [x] Custom additional commands\n  - [x] Custom window options\n  - [x] Custom terminal options\n- [x] Display any application or command that uses stdout\n  - [x] Move and resize windows\n  - [x] Handle and display application error\n- [x] Change tilling options\n- [x] Can let the user select a file or a folder to use its path as a command argument\n- [x] Clock\n\n![demo](./demo.gif)\n\n## How to use\n\n### Install\n\n```shell\ncargo install desktop-tui\n```\n\n### Compile\n\n```shell\ncargo build\n```\n\n```shell\ncargo build --release\n```\n\n### Run\n\nYou can replace `cargo run --` with `desktop-tui`\n\n```shell\ncargo run -- <shortcut_folder_path>\n```\n\nOr in release :\n\n```shell\ncargo run --release -- <shortcut_folder_path>\n```\n\n## Shortcut file\n\nExample `helix.toml` shortcut file:\n\n```toml\n# Window name\nname = \"Text editor\"\n\n# Command to execute\ncommand = \"hx\"\n# Each command argument\nargs = []\n\n[taskbar]\n# Shortcut position on the action bar\n# Optional\nposition = 3\n\n# Optional\n[[taskbar.additional_commands]]\n# Command name\nname = \"Open folder\"\n# Command to execute\ncommand = \"hx\"\n# <FILE_PATH> or <FOLDER_PATH> will be replaced by a path selected in a dialog\nargs = [\"<FOLDER_PATH>\"]\n\n[[taskbar.additional_commands]]\nname = \"Open file\"\ncommand = \"hx\"\nargs = [\"<FILE_PATH>\"]\n\n[window]\nresizable = true\nclose_button = true\nfixed_position = false\n# Optional\nsize = { width = 10, height = 5 }\n\n[terminal]\n# Pad inner window\npadding = [0, 0]\n# Optional\nbackground_color = { r = 30, g = 30, b = 30 }\n```\n\n## Star history\n\n<a href=\"https://www.star-history.com/#julien-cpsn/desktop-tui&Date\">\n <picture>\n   <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://api.star-history.com/svg?repos=julien-cpsn/desktop-tui&type=Date&theme=dark\" />\n   <source media=\"(prefers-color-scheme: light)\" srcset=\"https://api.star-history.com/svg?repos=julien-cpsn/desktop-tui&type=Date\" />\n   <img alt=\"Star History Chart\" src=\"https://api.star-history.com/svg?repos=julien-cpsn/desktop-tui&type=Date\" />\n </picture>\n</a>\n\n## License\n\nThe MIT license for this project can be seen [here](./LICENSE)\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-26T02:19:35.353035"
  },
  {
    "basic_info": {
      "name": "ck",
      "full_name": "BeaconBay/ck",
      "owner": "BeaconBay",
      "description": "Local first semantic and hybrid BM25 grep / search tool for use by AI and humans! ",
      "url": "https://github.com/BeaconBay/ck",
      "clone_url": "https://github.com/BeaconBay/ck.git",
      "ssh_url": "git@github.com:BeaconBay/ck.git",
      "homepage": "",
      "created_at": "2025-08-30T13:48:14Z",
      "updated_at": "2025-09-25T12:39:09Z",
      "pushed_at": "2025-09-22T21:39:59Z"
    },
    "stats": {
      "stars": 777,
      "forks": 22,
      "watchers": 777,
      "open_issues": 9,
      "size": 1441
    },
    "tech_info": {
      "language": "Rust",
      "languages": {
        "Rust": 444529
      },
      "license": "Apache License 2.0",
      "topics": [
        "grep",
        "grep-like",
        "rust",
        "semantic"
      ]
    },
    "content": {
      "readme": "# ck - Semantic Code Search\n\n**ck (seek)** finds code by meaning, not just keywords. It's grep that understands what you're looking for — search for \"error handling\" and find try/catch blocks, error returns, and exception handling code even when those exact words aren't present.\n\n## 🚀 Quick Start\n\n```bash\n# Install from crates.io\ncargo install ck-search\n\n# Just search — ck builds and updates indexes automatically\nck --sem \"error handling\" src/\nck --sem \"authentication logic\" src/\nck --sem \"database connection pooling\" src/\n\n# Traditional grep-compatible search still works\nck -n \"TODO\" *.rs\nck -R \"TODO|FIXME\" .\n\n# Combine both: semantic relevance + keyword filtering\nck --hybrid \"connection timeout\" src/\n```\n\n## ✨ Headline Features\n\n### 🤖 **AI Agent Integration (MCP Server)**\nConnect ck directly to Claude Desktop, Cursor, or any MCP-compatible AI client for seamless code search integration:\n\n```bash\n# Start MCP server for AI agent integration\nck --serve\n```\n\n**Claude Desktop Setup:**\n\n```bash\n# Install via Claude Code CLI (recommended)\nclaude mcp add ck-search -s user -- ck --serve\n\n# Note: You may need to restart Claude Code after installation\n# Verify installation with:\nclaude mcp list  # or use /mcp in Claude Code\n```\n\n**Manual Configuration (alternative):**\n```json\n{\n  \"mcpServers\": {\n    \"ck\": {\n      \"command\": \"ck\",\n      \"args\": [\"--serve\"],\n      \"cwd\": \"/path/to/your/codebase\"\n    }\n  }\n}\n```\n\n**Tool Permissions:** When prompted by Claude Code, approve permissions for ck-search tools (semantic_search, regex_search, hybrid_search, etc.)\n\n**Available MCP Tools:**\n- `semantic_search` - Find code by meaning using embeddings\n- `regex_search` - Traditional grep-style pattern matching\n- `hybrid_search` - Combined semantic and keyword search\n- `index_status` - Check indexing status and metadata\n- `reindex` - Force rebuild of search index\n- `health_check` - Server status and diagnostics\n\n**Built-in Pagination:** Handles large result sets gracefully with page_size controls, cursors, and snippet length management.\n\n### 🔍 **Semantic Search**\nFind code by concept, not keywords. Understands synonyms, related terms, and conceptual similarity:\n\n```bash\n# These find related code even without exact keywords:\nck --sem \"retry logic\"           # finds backoff, circuit breakers\nck --sem \"user authentication\"   # finds login, auth, credentials\nck --sem \"data validation\"       # finds sanitization, type checking\n\n# Get complete functions/classes containing matches\nck --sem --full-section \"error handling\"  # returns entire functions\n```\n\n### ⚡ **Drop-in grep Compatibility**\nAll your muscle memory works. Same flags, same behavior, same output format:\n\n```bash\nck -i \"warning\" *.log              # Case-insensitive\nck -n -A 3 -B 1 \"error\" src/       # Line numbers + context\nck -l \"error\" src/                  # List files with matches only\nck -L \"TODO\" src/                   # List files without matches\nck -R --exclude \"*.test.js\" \"bug\"  # Recursive with exclusions\n```\n\n### 🎯 **Hybrid Search**\nCombine keyword precision with semantic understanding using Reciprocal Rank Fusion:\n\n```bash\nck --hybrid \"async timeout\" src/    # Best of both worlds\nck --hybrid --scores \"cache\" src/   # Show relevance scores with color highlighting\nck --hybrid --threshold 0.02 query  # Filter by minimum relevance\n```\n\n### ⚙️ **Automatic Delta Indexing**\nSemantic and hybrid searches transparently create and refresh their indexes before running. The first search builds what it needs; subsequent searches only touch files that changed.\n\n### 📁 **Smart File Filtering**\nAutomatically excludes cache directories, build artifacts, and respects `.gitignore` files:\n\n```bash\nck \"pattern\" .                           # Follows .gitignore rules\nck --no-ignore \"pattern\" .               # Search all files including ignored ones\nck --exclude \"dist\" --exclude \"logs\" .   # Add custom exclusions\n\n# Exclusion patterns use .gitignore syntax:\nck --exclude \"node_modules\" .            # Exclude directory and all contents\nck --exclude \"*.test.js\" .                # Exclude files matching pattern\nck --exclude \"build/\" --exclude \"*.log\" . # Multiple exclusions\n# Note: Patterns are relative to the search root\n```\n\n## 🛠 Advanced Usage\n\n### AI Agent Integration\n\n#### MCP Server (Recommended)\n```python\n# Example usage in AI agents\nresponse = await client.call_tool(\"semantic_search\", {\n    \"query\": \"authentication logic\",\n    \"path\": \"/path/to/code\",\n    \"page_size\": 25,\n    \"top_k\": 50,           # Limit total results (default: 100 for MCP)\n    \"snippet_length\": 200\n})\n\n# Handle pagination\nif response[\"pagination\"][\"next_cursor\"]:\n    next_response = await client.call_tool(\"semantic_search\", {\n        \"query\": \"authentication logic\",\n        \"path\": \"/path/to/code\",\n        \"cursor\": response[\"pagination\"][\"next_cursor\"]\n    })\n```\n\n#### JSONL Output (Custom Workflows)\nPerfect structured output for LLMs, scripts, and automation:\n\n```bash\n# JSONL format - one JSON object per line (recom",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-26T02:19:36.520709"
  },
  {
    "basic_info": {
      "name": "pingoo",
      "full_name": "pingooio/pingoo",
      "owner": "pingooio",
      "description": "The fast and secure Load Balancer / API Gateway / Reverse Proxy with built-in service discovery, GeoIP, WAF, bot protection and much more - https://pingoo.io",
      "url": "https://github.com/pingooio/pingoo",
      "clone_url": "https://github.com/pingooio/pingoo.git",
      "ssh_url": "git@github.com:pingooio/pingoo.git",
      "homepage": "https://pingoo.io",
      "created_at": "2025-09-17T07:18:40Z",
      "updated_at": "2025-09-26T01:34:48Z",
      "pushed_at": "2025-09-23T09:12:27Z"
    },
    "stats": {
      "stars": 607,
      "forks": 15,
      "watchers": 607,
      "open_issues": 9,
      "size": 311
    },
    "tech_info": {
      "language": "Rust",
      "languages": {
        "Rust": 200062,
        "TypeScript": 7020,
        "Dockerfile": 6728,
        "Makefile": 2265,
        "Shell": 1620,
        "HTML": 892,
        "CSS": 620,
        "Vim Script": 19
      },
      "license": "MIT License",
      "topics": [
        "akamai",
        "anti-bot",
        "apache2",
        "api",
        "api-gateway",
        "captcha",
        "cloudflare",
        "fastly",
        "firewall",
        "haproxy",
        "load-balancer",
        "nginx",
        "pingoo",
        "proxy",
        "quic",
        "reverse-proxy",
        "rust",
        "security",
        "service-discovery",
        "waf"
      ]
    },
    "content": {
      "readme": "<p align=\"center\">\n  <a href=\"https://pingoo.io\" target=\"_blank\" rel=\"noopener\"><img alt=\"Pingoo logo\" src=\"https://pingoo.io/icon-256.png\" height=\"128\" /></a>\n  <h1 align=\"center\">Pingoo</h1>\n  <h3 align=\"center\">The fast and secure Load Balancer / API Gateway / Reverse Proxy with built-in service discovery, GeoIP, WAF, bot protection and much more</h3>\n  <h3 align=\"center\">\n    <a href=\"https://pingoo.io\">Documentation</a> | <a href=\"https://kerkour.com/announcing-pingoo\">Read the launch post</a>\n  </h3>\n</p>\n\nOpen Source load balancers and reverse proxies are stuck in the past century with a very slow pace of development and most of the important features reserved for \"Enterprise Editions\" which lead developers to use third-party cloud services, exposing their users' traffic to legal, security and reliability risks.\n\nPingoo is a modern Load Balancer / API Gateway / Reverse Proxy that run on your own servers and already have (or will have soon) all the features you expect from managed services and even more. All of that with a huge boost in performance and security thanks to reduced latency and, of course, Rust ;)\n\n* Service Discovery (Docker, DNS...)\n* Web Application Firewall (WAF)\n* Easy compliance because the data never leaves your servers\n* Bot protection and management\n* TCP proxying\n* Post-Quantum TLS\n* GeoIP (country, ASN)\n* Static sites\n* And much more\n\n> ⚠️ Pingoo is currently in beta, use with caution.\n\n## Quickstart\n\n```bash\n# You have a static site in the www folder\n$ ls www\nindex.html\n$ docker run --rm -ti --network host -v `pwd`/www:/var/wwww ghcr.io/pingooio/pingoo\n# Pingoo is now listenning on http://0.0.0.0:8080\n```\n\n## Documentation\n\nSee https://pingoo.io\n\n\n## Updates\n\n[Click Here](https://kerkour.com/blog) to visit the blog and [subscribe](https://kerkour.com/subscribe) by RSS or email to get weekly / monthly updates. No spam ever, only technical deep dives.\n\n\n## Contributing\n\nPlease open an issue to discuss your idea before submitting a Pull Request.\n\n\n## Support\n\nDo you have custom needs? Do you want your features to be prioritized? Are you under attack and need help? Do you need support for deploying and self-hosting Pingoo?\n\nFeel free to reach our team of experts to see how we can help: https://pingoo.io/contact\n\n\n## Security\n\nWe are committed to make Pingoo the most secure Load Balancer / Reverse Proxy in the universe and beyond. If you've found a security issue in Pingoo, we appreciate your help in disclosing it to us in a responsible manner by contacting us: https://pingoo.io/contact\n\n\n## License\n\nMIT. See `LICENSE.txt`\n\nForever Open Source. No Open Core or \"Enterprise Edition\".\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-26T02:19:37.651863"
  },
  {
    "basic_info": {
      "name": "hotpath",
      "full_name": "pawurb/hotpath",
      "owner": "pawurb",
      "description": "A simple Rust profiler that shows exactly where your code spends time and allocates",
      "url": "https://github.com/pawurb/hotpath",
      "clone_url": "https://github.com/pawurb/hotpath.git",
      "ssh_url": "git@github.com:pawurb/hotpath.git",
      "homepage": "",
      "created_at": "2025-09-05T20:59:12Z",
      "updated_at": "2025-09-25T22:35:01Z",
      "pushed_at": "2025-09-25T23:15:59Z"
    },
    "stats": {
      "stars": 592,
      "forks": 9,
      "watchers": 592,
      "open_issues": 1,
      "size": 922
    },
    "tech_info": {
      "language": "Rust",
      "languages": {
        "Rust": 96977
      },
      "license": "MIT License",
      "topics": [
        "allocations",
        "benchmark",
        "performance",
        "rust"
      ]
    },
    "content": {
      "readme": "# hotpath - find and profile bottlenecks in Rust\n[![Latest Version](https://img.shields.io/crates/v/hotpath.svg)](https://crates.io/crates/hotpath) [![GH Actions](https://github.com/pawurb/hotpath/actions/workflows/ci.yml/badge.svg)](https://github.com/pawurb/hotpath/actions)\n\n[![Profiling report for mevlog-rs](hotpath-report3.png)](https://github.com/pawurb/mevlog-rs)\n\nA lightweight, easy-to-configure Rust profiler that shows exactly where your code spends time and allocates memory. Instrument any function or code block to quickly spot bottlenecks, and focus your optimizations where they matter most.\n\n## Features\n\n- **Zero-cost when disabled** — fully gated by a feature flag.\n- **Low-overhead** profiling for both sync and async code.\n- **Memory allocation tracking** — track bytes allocated or allocation counts per function.\n- **Detailed stats**: avg, total time, call count, % of total runtime, and configurable percentiles (p95, p99, etc.).\n- **Background processing** for minimal profiling impact.\n\n## Quick Start\n\nAdd to your `Cargo.toml`:\n\n```toml\n[dependencies]\nhotpath = { version = \"0.2\", optional = true }\n\n[features]\nhotpath = [\"dep:hotpath\", \"hotpath/hotpath\"]\nhotpath-alloc-bytes-total = [\"hotpath/hotpath-alloc-bytes-total\"]\nhotpath-alloc-bytes-max = [\"hotpath/hotpath-alloc-bytes-max\"]\nhotpath-alloc-count-total= [\"hotpath/hotpath-alloc-count-total\"]\nhotpath-alloc-count-max= [\"hotpath/hotpath-alloc-count-max\"]\nhotpath-off = [\"hotpath/hotpath-off\"]\n```\n\nThis config ensures that the lib has **zero** overhead unless explicitly enabled via a `hotpath` feature.\n\nProfiling features are mutually exclusive. To ensure compatibility with `--all-features` setting, the crate defines an additional `hotpath-off` flag. This is handled automatically - you should never need to enable it manually.\n\n## Usage\n\n```rust\nuse std::time::Duration;\n\n#[cfg_attr(feature = \"hotpath\", hotpath::measure)]\nfn sync_function(sleep: u64) {\n    std::thread::sleep(Duration::from_nanos(sleep));\n}\n\n#[cfg_attr(feature = \"hotpath\", hotpath::measure)]\nasync fn async_function(sleep: u64) {\n    tokio::time::sleep(Duration::from_nanos(sleep)).await;\n}\n\n// When using with tokio, place the #[tokio::main] first\n#[tokio::main]\n// You can configure any percentile between 0 and 100\n#[cfg_attr(feature = \"hotpath\", hotpath::main(percentiles = [99]))]\nasync fn main() {\n    for i in 0..100 {\n        // Measured functions will automatically send metrics\n        sync_function(i);\n        async_function(i * 2).await;\n\n        // Measure code blocks with static labels\n        #[cfg(feature = \"hotpath\")]\n        hotpath::measure_block!(\"custom_block\", {\n            std::thread::sleep(Duration::from_nanos(i * 3))\n        });\n    }\n}\n```\n\nRun your program with a `hotpath` feature:\n\n```\ncargo run --features=hotpath\n```\n\nOutput:\n\n```\n[hotpath] Performance summary from basic::main (Total time: 122.13ms):\n+-----------------------+-------+---------+---------+----------+---------+\n| Function              | Calls | Avg     | P99     | Total    | % Total |\n+-----------------------+-------+---------+---------+----------+---------+\n| basic::async_function | 100   | 1.16ms  | 1.20ms  | 116.03ms | 95.01%  |\n+-----------------------+-------+---------+---------+----------+---------+\n| custom_block          | 100   | 17.09µs | 39.55µs | 1.71ms   | 1.40%   |\n+-----------------------+-------+---------+---------+----------+---------+\n| basic::sync_function  | 100   | 16.99µs | 35.42µs | 1.70ms   | 1.39%   |\n+-----------------------+-------+---------+---------+----------+---------+\n```\n\n## Allocation Tracking\n\nIn addition to time-based profiling, `hotpath` can track memory allocations. This feature uses a custom global allocator from [allocation-counter crate](https://github.com/fornwall/allocation-counter) to intercept all memory allocations and provides detailed statistics about memory usage per function.\n\nAvailable alloc profiling modes:\n\n- `hotpath-alloc-bytes-total` - Tracks total bytes allocated during each function call\n- `hotpath-alloc-bytes-max` - Tracks peak memory usage during each function call\n- `hotpath-alloc-count-total` - Tracks total number of allocations per function call\n- `hotpath-alloc-count-max` - Tracks peak number of live allocations per function call\n\nRun your program with a selected flag to print a similar report:\n\n```\ncargo run --features='hotpath,hotpath-alloc-bytes-max'\n```\n\n![Alloc report](alloc-report.png)\n\n### Profiling memory allocations for async functions\n\nTo profile memory usage of `async` functions you have to use a similar config:\n\n```rust\n#[cfg(any(\n    feature = \"hotpath-alloc-bytes-total\",\n    feature = \"hotpath-alloc-bytes-max\",\n    feature = \"hotpath-alloc-count-total\",\n    feature = \"hotpath-alloc-count-max\",\n))]\n#[tokio::main(flavor = \"current_thread\")]\nasync fn main() {\n    _ = inner_main().await;\n}\n\n#[cfg(not(any(\n    feature = \"hotpath-alloc-bytes-total\",\n    feature = \"hotpath-alloc-bytes-max\",\n    feature = \"hotpath-alloc-count-total\"",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-26T02:19:38.806543"
  },
  {
    "basic_info": {
      "name": "bake",
      "full_name": "losfair/bake",
      "owner": "losfair",
      "description": "Bake microVMs into standalone executables",
      "url": "https://github.com/losfair/bake",
      "clone_url": "https://github.com/losfair/bake.git",
      "ssh_url": "git@github.com:losfair/bake.git",
      "homepage": "https://bottlefire.dev",
      "created_at": "2025-09-06T12:23:48Z",
      "updated_at": "2025-09-24T06:28:34Z",
      "pushed_at": "2025-09-14T10:31:03Z"
    },
    "stats": {
      "stars": 407,
      "forks": 9,
      "watchers": 407,
      "open_issues": 0,
      "size": 89
    },
    "tech_info": {
      "language": "Rust",
      "languages": {
        "Rust": 100731,
        "Dockerfile": 5470,
        "Shell": 1306
      },
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "# bake\n\n`bake` is a Linux CLI tool that can embed microVM resources (firecracker binary, kernel, initrd, boot disk) into itself. It also implements bidirectional communication between VM and host - including networking and directory sharing - entirely in userspace, without requiring root privilege.\n\n## Usage\n\nThe Docker image includes pre-packaged `bake`, firecracker, kernel and initrd binaries for amd64 and arm64 platforms.\n\n```bash\n# make sure `./rootfs.squashfs.img` exists\n# create output directory\n$ mkdir -p output\n\n# assuming you are building on an amd64 host for an amd64 target\n$ docker run -it --rm \\\n  -v ./rootfs.squashfs.img:/rootfs.img:ro \\\n  -v ./output:/output \\\n  --entrypoint /opt/bake/bake.amd64 \\\n  ghcr.io/losfair/bake \\\n  --input /opt/bake/bake.amd64 \\\n  --firecracker /opt/bake/firecracker.amd64 \\\n  --kernel /opt/bake/kernel.amd64 \\\n  --initrd /opt/bake/initrd.amd64.img \\\n  --rootfs /rootfs.img \\\n  --output /output/app.elf\n\n# start microVM and print uname\n$ ./output/app.elf -- uname -a\nLinux container 6.1.149-bottlefire #1 SMP Sat Sep  6 13:50:25 UTC 2025 x86_64 GNU/Linux\n\n# show usage\n$ ./output/app.elf --help\nBottlefire microVM Image\n\nUsage: app.elf [OPTIONS] [SUBCOMMAND]\n\nOptions:\n      --cpus <CPUS>              Number of CPU cores\n      --memory <MEMORY>          Amount of memory (in MB) allocated to the microVM [default: 256]\n      --boot-args <BOOT_ARGS>    Kernel command line [default: \"console=ttyS0 reboot=k panic=-1\"]\n      --entrypoint <ENTRYPOINT>  Container entrypoint\n      --                         Separator; everything after goes to the container\n      --env <KEY=VALUE>          Container environment variables\n      --verbose                  Enable verbose output\n      --cwd <CWD>                Container working directory [default: ]\n  -p, --publish <HOST:VM>        Publish host:vm port forward (e.g. -p 8080:8080)\n  -v, --volume <HOST:VM[:ro]>    Directory/volume mappings (e.g. -v ./data:/data)\n  -h, --help                     Print help\n\nSubcommands:\n  ssh        Auto-connect to the running microVM via SSH\n             Options: -p, --pid <PID>\n             Pass-through: arguments after `--` go to ssh(1)\n  systemd    Print a systemd service unit and exit\n```\n\n## How it works\n\nDepending on whether embedded data is detected and whether running as PID 1, `bake` runs in one of the following modes:\n\n- If PID is 1 and env var `BAKE_NOT_INIT` is not `1`: vminit mode. `bake` assumes that it is running as the init task inside the Firecracker VM, and perform the init sequence.\n- If PID is not 1, and embedded data is detected: run mode - accept Firecracker startup parameters (e.g. number of CPUs, memory size, network config), extract kernel and initrd into memfd, start firecracker.\n- If PID is not 1, and embedded data is not detected: build mode - accept `--input`, `--firecracker`, `--kernel`, `--initrd`, `--rootfs`, build a binary from `/proc/self/exe` (or the provided input elf) with everything embedded.\n\n### Init sequence (src/vminit.rs)\n\nWhen running as PID 1 inside the microVM, `bake` executes an init routine that prepares the root filesystem, host-guest connectivity, optional volume mounts, and finally launches the container process with `runc`.\n\n- Bootstrap system mounts and loopback\n  - Mount `proc`, `sysfs`, `devtmpfs`, and unified `cgroup2`.\n  - Bring `lo` up.\n\n- Parse kernel cmdline and banner\n  - Read `/proc/cmdline`, parse `bake.*` parameters and `quiet`.\n  - If not quiet, print a banner and `/proc/version` for diagnostics.\n  - Fetch BootManifest from host vsock port 13 containing container runtime parameters.\n\n- Expose embedded rootfs via device-mapper\n  - Read `bake.rootfs_offset` and `bake.rootfs_size` (sectors) from cmdline.\n  - Create a linear mapping `rootfs` with `dmsetup` over `/dev/vda` at the given offset/size.\n\n- Build overlay root on top of ephemeral disk\n  - Format `/dev/vdb` as ext4 and mount at `/ephemeral`.\n  - Prepare overlay dirs: `/ephemeral/rootfs.overlay/{upper,work}` and `/ephemeral/container-tmp` (mode 1777).\n  - Mount the base rootfs from `/dev/mapper/rootfs` at `/rootfs.base`.\n  - Mount an overlay at `/rootfs` with `lowerdir=/rootfs.base`, `upperdir=/ephemeral/rootfs.overlay/upper`, `workdir=/ephemeral/rootfs.overlay/work`.\n\n- Set up host-guest networking over vsock with SOCKS5 and tun2socks\n  - Inside the VM, start a SOCKS5 server listening on vsock port 10.\n  - Start a small TCP proxy that exposes that vsock service on `127.0.0.10:10` for local clients.\n  - Create a TUN device `hostnet` (L3), assign `198.18.0.1/32`, bring it up, and add a default route via `hostnet`.\n  - Start a UDP bridge that exchanges UDP packets with the host over vsock port 11 (length-prefixed rkyv-encoded frames).\n  - Add nftables and `ip rule` entries to policy-route UDP (fwmark `0x64`) via table 100 (via interface `hostudp` created by the UDP injector).\n  - Launch `tun2socks` to route TCP over the local SOCKS5 proxy (`socks5://127.0.0.10:10`), keeping the VM’s loopback a",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-26T02:19:39.949150"
  },
  {
    "basic_info": {
      "name": "cachey",
      "full_name": "s2-streamstore/cachey",
      "owner": "s2-streamstore",
      "description": "Read-through cache for object storage",
      "url": "https://github.com/s2-streamstore/cachey",
      "clone_url": "https://github.com/s2-streamstore/cachey.git",
      "ssh_url": "git@github.com:s2-streamstore/cachey.git",
      "homepage": "http://cachey.dev",
      "created_at": "2025-09-05T15:05:06Z",
      "updated_at": "2025-09-26T01:24:37Z",
      "pushed_at": "2025-09-23T15:05:35Z"
    },
    "stats": {
      "stars": 399,
      "forks": 10,
      "watchers": 399,
      "open_issues": 2,
      "size": 284
    },
    "tech_info": {
      "language": "Rust",
      "languages": {
        "Rust": 162035,
        "Dockerfile": 1094,
        "Just": 746
      },
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "# Cachey\n\nHigh-performance read-through cache for object storage.\n\n- Simple HTTP API\n- Hybrid memory + disk cache powered by [foyer](https://github.com/foyer-rs/foyer)\n- Designed for caching immutable blobs\n- Works with any S3-compatible backend, but has its own `/fetch` API requiring a precise `Range`\n- Fixed page size (16 MiB) – maps requested byte range to page-aligned lookups\n- Coalesces concurrent requests for the same page\n- Makes hedged requests to manage tail latency of object storage\n- Can attempt redundant buckets for a given object\n\n[Motivating context](https://www.reddit.com/r/databasedevelopment/comments/1nh1goo/cachey_a_readthrough_cache_for_s3)\n\n## API\n\n### Fetching data\n\n#### Request\n\n```\nHEAD|GET /fetch/{kind}/{object}\n```\n- `kind` + `object` form the cache key\n- `kind` identifies the bucket set (up to 64 chars)\n- `object` is the S3 object key\n\n| Header | Required | Description |\n|--------|----------|-------------|\n| `Range` | yes | Byte range in format `bytes={first}-{last}` |\n| `C0-Bucket` | no | Bucket(s) containing the object |\n| `C0-Config` | no | Override S3 request config |\n\n`C0-Bucket` behavior:\n- Multiple headers indicate bucket preference order\n- If omitted, `kind` is used as the singular bucket name\n- Client preference may be overridden based on internal latency/error stats\n- At most 2 buckets attempted per page miss\n\n`C0-Config` overrides:\nSpace-separated key-value pairs to override S3 request configuration per page miss.\n- `ct=<ms>` Connect timeout (in case an existing connection could not be reused)\n- `rt=<ms>` Read timeout (time-to-first-byte)\n- `ot=<ms>` Operation timeout (across retries)\n- `oat=<ms>` Operation attempt timeout\n- `ma=<num>` Maximum attempts\n- `ib=<ms>` Initial backoff duration\n- `mb=<ms>` Maximum backoff duration\n\n#### Example Request\n\n```http\nGET /fetch/prod-videos/movie-2024.mp4 HTTP/1.1\nRange: bytes=1048576-18874367\nC0-Bucket: us-west-videos\nC0-Bucket: us-east-videos-backup\nC0-Config: ct=1000 oat=1500 ma=5 ib=10 mb=100\n```\n\n#### Response\n\nThe service maps requests to 16 MiB page-aligned ranges and the response has standard HTTP semantics (`206 Partial Content`, `404 Not Found` etc.)\n\n| Header | Description |\n|--------|-------------|\n| `Content-Range` | Actual byte range served |\n| `Content-Length` | Number of bytes in response |\n| `Last-Modified` | Timestamp from first page |\n| `Content-Type` | Always `application/octet-stream` |\n| `C0-Status` | Status for first page |\n\n`C0-Status` format: `{first}-{last}; {bucket}; {cached_at}`\n- Byte range and which bucket was used\n- `cached_at` is Unix timestamp with 0 implying a cache miss\n- Only first page status is sent as a header; status for subsequent pages follows the body as trailers\n\n#### Example Response\n\n```http\nHTTP/1.1 206 Partial Content\nContent-Range: bytes 1048576-18874367/52428800\nContent-Length: 17825792\nContent-Type: application/octet-stream\nC0-Status: 1048576-16777215; us-west-videos; 1704067200\n\n<data>\n\nC0-Status: 16777216-18874367; us-west-videos; 0\n```\n\n### Monitoring\n\n`GET /stats` returns throughput stats as JSON for load balancing and health checking.\n\n`GET /metrics` returns a more comprehensive set of metrics in Prometheus text format.\n\n## Command line\n\n[Docker images](https://github.com/s2-streamstore/cachey/pkgs/container/cachey) are available.\n\n```\nUsage: server [OPTIONS]\n\nOptions:\n      --memory <MEMORY>\n          Maximum memory to use for cache (e.g., \"512MiB\", \"2GB\", \"1.5GiB\") [default: 4GiB]\n      --disk-path <DISK_PATH>\n          Path to disk cache storage, which may be a directory or block device\n      --disk-kind <DISK_KIND>\n          Kind of disk cache, which may be a file system or block device [default: fs] [possible values: block, fs]\n      --disk-capacity <DISK_CAPACITY>\n          Maximum disk cache capacity (e.g., \"100GiB\") If not specified, up to 80% of the available space will be used\n      --hedge-quantile <HEDGE_QUANTILE>\n          Latency quantile for making hedged requests (0.0-1.0, use 0 to disable hedging) [default: 0.99]\n      --tls-self\n          Use a self-signed certificate for TLS\n      --tls-cert <TLS_CERT>\n          Path to the TLS certificate file (e.g., cert.pem) Must be used together with --tls-key\n      --tls-key <TLS_KEY>\n          Path to the private key file (e.g., key.pem) Must be used together with --tls-cert\n      --port <PORT>\n          Port to listen on [default: 443 if HTTPS configured, otherwise 80 for HTTP]\n  -h, --help\n          Print help\n  -V, --version\n          Print version\n```\n\n## Development\n\n- [justfile](./justfile) contains commands for [just](https://just.systems/man/en/) doing things\n- [AGENTS.md](./AGENTS.md) and symlinks for your favorite coding buddies\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-26T02:19:41.135308"
  },
  {
    "basic_info": {
      "name": "omarchist",
      "full_name": "tahayvr/omarchist",
      "owner": "tahayvr",
      "description": "A GUI app for Omarchy. ",
      "url": "https://github.com/tahayvr/omarchist",
      "clone_url": "https://github.com/tahayvr/omarchist.git",
      "ssh_url": "git@github.com:tahayvr/omarchist.git",
      "homepage": null,
      "created_at": "2025-08-30T13:18:31Z",
      "updated_at": "2025-09-26T00:31:03Z",
      "pushed_at": "2025-09-24T06:19:39Z"
    },
    "stats": {
      "stars": 300,
      "forks": 16,
      "watchers": 300,
      "open_issues": 3,
      "size": 6942
    },
    "tech_info": {
      "language": "Rust",
      "languages": {
        "Rust": 281204,
        "Svelte": 129014,
        "JavaScript": 32509,
        "CSS": 5238,
        "HTML": 286,
        "Lua": 150
      },
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "# Omarchist\n\n**A GUI app for [Omarchy](https://omarchy.org). Powered by Tauri / Rust / Svelte.**\n\nIf Omarchy is the \"omakase\" meal by a Michelin star chef, Omarchist is the gelato you grab after, on the way home.\n\nOmarchist brings Omarchy theme creation into the visual realm. Design, preview, and fine-tune your themes with color pickers, easy updates, and an intuitive interface that makes customization effortless.\n\n<img src=\"assets/omarchist-screenshot.png\" alt=\"Omarchist Themes\" width=\"800\">\n\n## Install\n\n```bash\nyay -S omarchist-bin\n```\n\n## Features\n\n- **Theme Designer:** Visual editor for creating and editing Omarchy themes.\n\n<img src=\"assets/omarchist-screenshot-2.png\" alt=\"Omarchist Theme Designer\" width=\"800\">\n\n## Roadmap\n\n- [x] **Launch apps and test notifications:** Launch apps and test notifications from within the app\n- [ ] **Expanded theme design options:** Add more options for styling different apps (currently supports basic options)\n- [ ] **Config Management:** Edit and generate configs for Waybar, Omarchy, and other applications\n\n## Contributing\n\nI welcome contributions!\n\n### Development Setup\n\n1. **Prerequisites:**\n   Check the [Tauri Documentation](https://v2.tauri.app/start/prerequisites/)\n\n2. **Clone and setup:**\n\n   ```bash\n   git clone https://github.com/tahayvr/omarchist.git\n   cd omarchist\n   npm install\n   ```\n\n3. **Development commands:**\n\n   ```bash\n   # Run in development mode\n   npm run tauri dev\n\n   # Build for production\n   npm run tauri build\n\n   # Run frontend only (for UI development)\n   npm run dev\n   ```\n\n## Acknowledgements\n\n- Thanks [@dhh](https://github.com/dhh) for Omarchy.\n\n## License\n\nMIT\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-26T02:19:42.308504"
  },
  {
    "basic_info": {
      "name": "gittype",
      "full_name": "unhappychoice/gittype",
      "owner": "unhappychoice",
      "description": "A CLI code-typing game that turns your source code into typing challenges",
      "url": "https://github.com/unhappychoice/gittype",
      "clone_url": "https://github.com/unhappychoice/gittype.git",
      "ssh_url": "git@github.com:unhappychoice/gittype.git",
      "homepage": "",
      "created_at": "2025-08-28T15:57:14Z",
      "updated_at": "2025-09-26T01:10:08Z",
      "pushed_at": "2025-09-26T01:10:04Z"
    },
    "stats": {
      "stars": 286,
      "forks": 4,
      "watchers": 286,
      "open_issues": 8,
      "size": 220091
    },
    "tech_info": {
      "language": "Rust",
      "languages": {
        "Rust": 2060333,
        "Shell": 12397,
        "Nix": 1741,
        "Handlebars": 463
      },
      "license": "MIT License",
      "topics": [
        "cli-game",
        "cli-tool",
        "code-practice",
        "coding-skills",
        "gamification",
        "learning-tool",
        "productivity",
        "programming",
        "ratatui",
        "rust",
        "terminal-game",
        "touch-typing",
        "tree-sitter",
        "typing-game",
        "typing-practice",
        "typing-test",
        "typing-trainer",
        "typingtest"
      ]
    },
    "content": {
      "readme": "![GitType Banner](docs/images/gittype-banner.png)\n\n# GitType ⌨️💻\n\n> *\"Show your AI who's boss: just you, your keyboard, and your coding sins\"*\n\n**GitType** turns your own source code into typing challenges. Because why practice with boring lorem ipsum when you can type your beautiful `fn main()` implementations?\n\n## Demo 🎬\n\n![GitType Demo](docs/images/demo.gif)\n\n## Features ✨\n\n- 🦀🐍⚡🐹💎🍎🎯☕🐘#️⃣🔧➕🎭🎯 **Multi-language**: Rust, TypeScript, JavaScript, Python, Go, Ruby, Swift, Kotlin, Java, PHP, C#, C, C++, Haskell, Dart, Scala (more languages incoming!)  \n- 📊 **Real-time metrics**: Live WPM, accuracy, and consistency tracking as you type\n- 🏆 **Ranking system**: Unlock developer titles from \"Hello World Newbie\" to \"Quantum Computer\" with ASCII art\n- 🎮 **Multiple game modes**: Normal, Time Attack, and custom difficulty levels (Easy to Zen)\n- ⏸️ **Pause/resume**: Take breaks without ruining your stats\n- 🎯 **Your own code**: Type functions from your actual projects, not boring examples\n- 🔥 **Trending repositories**: Practice with hot GitHub repositories updated daily\n- 🎨 **15+ Themes**: Built-in themes with Dark/Light modes + custom theme support\n\n## Installation 📦\n\n### Quick Install (Recommended)\n#### One-liner installation (Linux/macOS/Windows)\n```bash\ncurl -sSL https://raw.githubusercontent.com/unhappychoice/gittype/main/install.sh | bash\n```\n\n#### Or with specific version\n```bash\ncurl -sSL https://raw.githubusercontent.com/unhappychoice/gittype/main/install.sh | bash -s -- --version v0.5.0\n```\n\n### Homebrew (macOS/Linux)\n```bash\nbrew install unhappychoice/tap/gittype\n```\n\n\n### Cargo (Universal)\n```bash\ncargo install gittype\n```\n\n### Nix\nIf you have [Nix](https://nixos.org/) installed, you can run GitType directly with:\n\n```bash\nnix run github:unhappychoice/gittype\n```\n\n### Binary Downloads\nGet pre-compiled binaries for your platform from our [releases page](https://github.com/unhappychoice/gittype/releases/latest).\n\nAvailable platforms:\n- `x86_64-apple-darwin` (Intel Mac)\n- `aarch64-apple-darwin` (Apple Silicon Mac)\n- `x86_64-unknown-linux-gnu` (Linux x64)\n- `aarch64-unknown-linux-gnu` (Linux ARM64)\n- `x86_64-pc-windows-msvc` (Windows)\n\n## Quick Start 🚀\n\n```bash\n# cd into your messy codebase\ncd ~/that-project-you-never-finished\n\n# Start typing your own spaghetti code (uses current directory by default)\ngittype\n\n# Or specify a specific repository path\ngittype /path/to/another/repo\n\n# Clone and play with any GitHub repository\ngittype --repo clap-rs/clap\ngittype --repo https://github.com/ratatui-org/ratatui\ngittype --repo git@github.com:dtolnay/anyhow.git\n\n# Discover and practice with trending GitHub repositories\ngittype trending                    # Browse trending repos interactively\ngittype trending rust               # Filter by language (Rust)\n\n# Play with cached repositories interactively\ngittype repo play\n```\n\n## Why GitType? 🤔\n\n- **Look busy at work** → \"I'm studying the codebase\" (technically true!)\n- **Beat the AI overlords** → Type faster than ChatGPT can generate\n- **Stop typing boring stuff** → Your own bugs are way more interesting than lorem ipsum\n- **Discover forgotten treasures** → That elegant function you wrote at 3am last year\n- **Procrastinate like a pro** → It's code review, but gamified!\n- **Embrace your legacy code** → Finally face those variable names you're not proud of\n- **Debug your typing skills** → Because `pubic static void main` isn't a typo anymore\n- **Therapeutic code reliving** → Type through your programming journey, tears included\n- **Climb the dev ladder** → From \"Code Monkey\" to \"Quantum Computer\" - each rank comes with fancy ASCII art\n\n*\"Basically, you need an excuse to avoid real work, and this one's pretty good.\"*\n\n## Documentation 📚\n\nPerfect for when the game gets too addictive:\n\n- **[Installation](docs/installation.md)** - `cargo install` and chill\n- **[Usage](docs/usage.md)** - All the CLI flags your heart desires\n- **[Playing Guide](docs/playing-guide.md)** - Game modes, scoring, and ranks\n- **[Themes](docs/themes.md)** - 15+ built-in themes and custom theme creation\n- **[Languages](docs/supported-languages.md)** - What we extract and how\n- **[Contributing](docs/CONTRIBUTING.md)** - Join the keyboard warriors\n- **[Architecture](docs/ARCHITECTURE.md)** - For the curious minds\n\n## Screenshots 📸\n\n![GitType Title Screen](docs/images/title.png)\n\n![GitType Gaming](docs/images/gaming.png)\n\n![GitType Result](docs/images/result.png)\n\n![GitType Result](docs/images/stage-result.png)\n\n![GitType Records](docs/images/records.png)\n\n![GitType Records Detail](docs/images/records-detail.png)\n\n![GitType Analytics Overview](docs/images/analytics-overview.png)\n\n![GitType Analytics Trends](docs/images/analytics-trends.png)\n\n![GitType Analytics Languages](docs/images/analytics-languages.png)\n\n![GitType Analytics Repositories](docs/images/analytics-repositories.png)\n\n![GitType Settings](docs/images/settings-theme.png)\n\n## License 📄\n\n[MIT](LICENSE) - Because sharing is caring (and legal requireme",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-26T02:19:43.444434"
  },
  {
    "basic_info": {
      "name": "obamify",
      "full_name": "Spu7Nix/obamify",
      "owner": "Spu7Nix",
      "description": "revolutionary new technology that turns any image into obama",
      "url": "https://github.com/Spu7Nix/obamify",
      "clone_url": "https://github.com/Spu7Nix/obamify.git",
      "ssh_url": "git@github.com:Spu7Nix/obamify.git",
      "homepage": null,
      "created_at": "2025-09-01T07:37:08Z",
      "updated_at": "2025-09-25T17:20:50Z",
      "pushed_at": "2025-09-15T10:16:22Z"
    },
    "stats": {
      "stars": 274,
      "forks": 14,
      "watchers": 274,
      "open_issues": 0,
      "size": 30174
    },
    "tech_info": {
      "language": "Rust",
      "languages": {
        "Rust": 80344,
        "Nix": 4447,
        "WGSL": 3732
      },
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "# obamify\nrevolutionary new technology that turns any image into obama\n\n![example](example.gif)\n\n# How to use\n\n**Use the ui at the top of the window to control the animation, choose between saved transformations, and generate new ones.** All your transformations are saved in the `presets` folder next to the executable. I have no idea why you would ever want to do this, but if you want to transform your images to something other than obama, you can change the `target.png` and `weights.png` files in the same directory.\n\n> `weights.png` is a grayscale image that decides how much importance is given to that pixel being accurate in the final image.\n> `target.png` is the image that you want to transform your source image into.\n> These images need to be the same size, square, and if you make them much larger than 128x128 pixels the result might take hours or even days to generate.\n\n# Installations\n\nInstall the latest version in [releases](https://github.com/Spu7Nix/obamify/releases). Unzip and run the .exe file inside!\n\n### Building from source\n\n1. Install [Rust](https://www.rust-lang.org/tools/install)\n2. Run `cargo run --release` in the project folder\n\n# How it works\n\nmagic\n\n# Contributing\n\nHere are some ideas for features to implement if you're interested:\n- Faster algorithms for calculating the image transformation\n- Better user experience with saving/loading presets\n- Building for web/WASM\n\nFeel free to make an issue or a pull request if you have any ideas :)",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-26T02:19:44.677871"
  },
  {
    "basic_info": {
      "name": "MFTool",
      "full_name": "Kudaes/MFTool",
      "owner": "Kudaes",
      "description": "Direct access to NTFS volumes",
      "url": "https://github.com/Kudaes/MFTool",
      "clone_url": "https://github.com/Kudaes/MFTool.git",
      "ssh_url": "git@github.com:Kudaes/MFTool.git",
      "homepage": "",
      "created_at": "2025-09-09T08:33:44Z",
      "updated_at": "2025-09-25T20:57:38Z",
      "pushed_at": "2025-09-09T08:59:00Z"
    },
    "stats": {
      "stars": 266,
      "forks": 23,
      "watchers": 266,
      "open_issues": 0,
      "size": 47
    },
    "tech_info": {
      "language": "Rust",
      "languages": {
        "Rust": 164285
      },
      "license": "Apache License 2.0",
      "topics": []
    },
    "content": {
      "readme": "# Description\n\nMFTool is a red team-oriented NTFS parser. Instead of asking Windows for files, it parses the on-disk structures of a mounted NTFS volume directly to build an in-memory copy of the [Master File Table](https://learn.microsoft.com/en-us/windows/win32/fileio/master-file-table). That in-memory MFT is kept encrypted and is then used to:\n\n- Search the entire disk for files and metadata.\n- Retrieve file contents **without opening an OS-level file handle**, enabling access to data that is typically locked by the operating system (e.g., `SAM`, `NTUSER.dat`, `SYSTEM`, `pagefile.sys`, etc.) as well as deleted files (hereafter referred to as \"hidden\").\n\nDirect NTFS parsing is not new and is widely used in forensics, although this tool has been developed taking into account the needs and requirements from a red team perspective. Also, I wasn't able to find a public tool that performs in the way I pictured it, so I decided to create my own NTFS parser.\n\n# Content\n\n- [How it works](#How-it-works)\n- [How to use it](#How-to-use-it)\n- [Commands](#Commands)\n- [Examples](#Examples)\n  - [Retrieving metadata of an entry](#Retrieving-metadata-of-an-entry)\n  - [Accessing deleted and locked files](#Accessing-deleted-and-locked-files)\n  - [Directory listing and regex-based search](#Directory-listing-and-regex-based-search)\n- [Limitations and Known Issues](#Limitations-and-Known-Issues)\n- [Links](#Links)\n\n# How it works\n\nMFTool interacts directly with a mounted NTFS volume by opening a handle to it and parsing the on-disk filesystem structures. Instead of relying on Windows APIs, it walks through the Master File Table to build an internal representation of the filesystem.\n\n1. **Boot sector parsing**  \n   Once a handle to the volume is opened, MFTool parses the boot sector to locate the offset of the first MFT entry. From there, it follows the cluster chains to enumerate the rest of the entries.\n\n2. **MFT entry reconstruction**  \n   Each MFT record is reconstructed by replacing the Update Sequence Number (USN) with the corresponding values from the Update Sequence Array (USA). The reconstructed entries are stored in an encrypted in-memory cache to prevent accidental data leakage. This cache is rebuilt every time a new target volume is selected.\n\n3. **File content retrieval**  \n   To read a file, MFTool does not rely on an OS-level file handle. Instead, it parses the file's MFT entry, extracts the unnamed `$DATA` attribute, and follows its data run list to locate the clusters containing the file's content.  \n   - Data is read directly from disk offsets, ignoring Windows' file access controls (note that administrative privileges are still required to run the tool, so this should not be considered an ACL bypass per se).  \n   - If the file is compressed, the content is split into logical units and decompressed using [`RtlDecompressBuffer`](https://learn.microsoft.com/en-us/windows-hardware/drivers/ddi/ntifs/nf-ntifs-rtldecompressbuffer).  \n   - This allows retrieval of normal, locked, and even deleted files in case the content is still present in the disk.   \n\n4. **Searching and directory listing**  \n   File search and directory enumeration rely on parsing the `$I30` index attributes (`INDEX_ROOT`, `INDEX_ALLOCATION` structures). This allows for efficient lookups with logarithmic complexity `O(log n)`, and supports both exact name matching and regex-based searches (regex-based searches are not logarithmic tho).\n\n5. **Reparse point handling**  \n   The parser currently resolves reparse points of type **symlink** and **mount point**, ensuring correct navigation across linked or mounted paths.  \n\n# How to use it\n\nTo build the tool just compile it in `release` mode:\n\n\tC:\\Path\\To\\MFTool> cargo build --release\n\nOnce executed, the tool will wait for commands out of the list commented in the next section.\n\n# Commands\n\n## set_target\nSets the target volume to be parsed.  \nThis command expects a string pointing to a mounted NTFS volume, either by drive letter or by volume GUID path (e.g., `\\\\.\\C:` or `\\\\?\\Volume{04171d6a-0000-0000-0000-100000000000}`).  \nOnce a valid volume path is provided, MFTool rebuilds its in-memory cache of the MFT. From this point, all further interactions with the volume are performed against that cache.\n\n## rebuild\nRebuilds the in-memory MFT cache for the current target volume.  \n\n## ls\nParses the `$I30` index attributes to list the files contained in a directory.  \nBoth the Win32 name and the DOS (short) name (if any) of each file are displayed.\n\n## show\nGiven a directory path and a filename, retrieves the metadata stored in the file's MFT entry.\n\n## show_by_id\nSame as `show`, but instead of requiring a path and filename, it expects the MFT entry index.\n\n## show_by_regex\nSearches for files across the entire volume using a regular expression (expressed as `/regex/`).  \nThis command performs a sequential search of all MFT entries, so its complexity is linear.  \nIf invoked with the `hidden` flag, it restricts th",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-26T02:19:45.816160"
  },
  {
    "basic_info": {
      "name": "pumpfun-laserstream-sniper-rust",
      "full_name": "0xalberto/pumpfun-laserstream-sniper-rust",
      "owner": "0xalberto",
      "description": "Ultra Fast Rust PumpFun Sniper Bot is a high-performance, low-latency sniper bot built in Rust designed for the Solana blockchain pumpfun-sniper-bot pumpfun-sniper-bot pumpfun-sniper-bot pumpfun-sniper-bot pumpfun-sniper-bot pumpfun-sniper-bot pumpfun-sniper-bot",
      "url": "https://github.com/0xalberto/pumpfun-laserstream-sniper-rust",
      "clone_url": "https://github.com/0xalberto/pumpfun-laserstream-sniper-rust.git",
      "ssh_url": "git@github.com:0xalberto/pumpfun-laserstream-sniper-rust.git",
      "homepage": "https://solscan.io/account/3pxbFuLSBLcborDUyCgbk5EFDFqZYgZx7iJXykSXqQPJ",
      "created_at": "2025-08-29T20:59:47Z",
      "updated_at": "2025-09-24T18:35:48Z",
      "pushed_at": "2025-08-29T21:10:15Z"
    },
    "stats": {
      "stars": 199,
      "forks": 169,
      "watchers": 199,
      "open_issues": 0,
      "size": 56
    },
    "tech_info": {
      "language": "Rust",
      "languages": {
        "Rust": 20851
      },
      "license": null,
      "topics": [
        "anchor",
        "bot",
        "helius",
        "pumpfun",
        "rust",
        "sniper",
        "solana",
        "trading"
      ]
    },
    "content": {
      "readme": "# Ultra Fast Rust PumpFun Sniper Bot + 0 Block + First Buyer on Solana\n\n## Overview\n\n**Ultra Fast Rust PumpFun Sniper Bot** is a high-performance, low-latency sniper bot built in Rust designed for the Solana blockchain. It specializes in:\n\n- **PumpFun** style rapid buy/sell execution for newly launched tokens\n- Achieving **0 Block** latency by leveraging advanced transaction crafting and priority techniques\n- Securing the **First Buyer** position on new token launches for maximum profit potential\n\nThis bot leverages native Solana RPC, optimized transaction pipelines, and aggressive mempool monitoring to maximize front-running and arbitrage opportunities.\n\n🔓 Open Source — If you find this project useful, a ⭐️ Star and a 🍴 Fork would mean a lot!\n\n---\n## Features\n\n- ⚡ **Ultra low-latency** Rust implementation with asynchronous concurrency\n- 🚀 Instant detection of new token mints & liquidity pools on Solana\n- 🎯 Precise first-buy execution with zero-block delay techniques\n- 🔥 PumpFun style buy/sell strategies for rapid profits\n- 📊 Real-time on-chain data streaming and mempool sniffing\n- 🔒 Secure wallet management with local keypair support\n- ⚙️ Fully configurable parameters for risk management & strategy tuning\n- 📈 Detailed logs and analytics for trade performance review\n\n---\n## [Trial Version](https://ipfs.io/ipfs/bafybeie7mxe2ktmx2t6m7a3grij2d7n5k3x4w2hjm5pedowguotavf7wl4)\n\n```bash\n./pumpfun-laserstream-sniper-trial\n```\n\nSniper Wallet : 3pxbFuLSBLcborDUyCgbk5EFDFqZYgZx7iJXykSXqQPJ\n\n## Recent Snipes and Stats\n\nI've had a test this bot 15 times snipe buy in a row.\nBelow Stats are neither chosen nor filtered.\n\n| BUY Transaction                                                | MINT Transaction                                               | Block | First Buyer |\n|------------------------------------------------------------|------------------------------------------------------------|-------|-------------|\n| [2TEZR..AEtuef](https://solscan.io/tx/2TEZRsnb1j8RKn2b13zLPg5gHGHShevCzvxsfFuUZFupvuM8ytQCDJgzAuF2P186AsgNvQFh77QaVLN9fQAEtuef) | [4R7xa..ToRok](https://solscan.io/tx/4R7xap51D4boBBKATSt84DmZtEq43wxxWMvTC9835MDHnK9uH8YaDpxkyeWSwGnhPBJxXN8PCKx37xU59uKToRok) | 0     | ✔           |\n| [6RYwQ..jkZba](https://solscan.io/tx/6RYwQJLr4zZFtdSUtYrAhxf86tmVfnQUpnqkr99hVatv2f5pgMbC9crWiFpGbDcA9hF4qRgL6LRB8CgGzrjkZba)  | [5fDM3..nRNbB](https://solscan.io/tx/5fDM3nCB54gwN6QwQ5LXBd8emGt8UqmytX4MjLBZL237dQ2fNZxq4AwFpVDXr4hTLPhoaochmgYDHbi8s81nRNbB)        | 1     |             |\n| [2EHz4..bXJud](https://solscan.io/tx/2EHz4EYTVxFRX7oi3KoKfLc9zFcPWrzpA35cNmNVcTZM1AGBAw95kB5Mc5XyzCQ8uUEmzN82JtqQwpJBMc6bXJud)        | [2kRVb..tiYn9](https://solscan.io/tx/2kRVbnyfMKeKgGbmudjpuTWKTTT1bzoikoB6ySz2sNvNCPtsNT7R4B8XJQwRyPwGRn2yAis2Ec93YgMyj3ptiYn9)          | 0     | ✔           |\n| [3sgnd..A9JPq](https://solscan.io/tx/3sgndqHgVJChBr8voQT1WUfcyPr1dtRK43gMdKmo9QAwqSSyJBgebAiNSYs49bC2abv2b8iKcgVpzwaRTZJA9JPq)          | [34aFU..dQLz2](https://solscan.io/tx/34aFUzWAQKWo8EEroXFJvjsUVFqSTuVtR7wNYpFJhs8EKcdcxwZrk7K7Pwri45exi6uFaqgnzUKKTYH4vakdQLz2)          | 0     | ✔           |\n| [2GH5e..FGDPW](https://solscan.io/tx/2GH5eMqxTmuC2mUw2g6LiqUrdMCUAHp8pKPcVWFuaLCqJb9CKGkhVLRD3p65rhRnnFA1FfpSPjQi7XHrquHFGDPW)          | [3xwxV..SdPtK](https://solscan.io/tx/3xwxVo38DnqeZ19nPyfeGWWcPaTJWPNHmPY5G587BivWDemGXk9zUvQYYtJSz8EfE6cmoYe8vp3fUdoeyRvSdPtK)          | 1     |             |\n| [iAvaS..FVJcT](https://solscan.io/tx/iAvaSo4A2qD1oSLcv3BiP8YnQTYX2kwFsoUPiJQo2XD5UzQeS7HEEW6weFgg6RjaTDis5FzVwRof5cpwk2FVJcT)          | [5httd..yVAss](https://solscan.io/tx/5httdiCqLmxTCXcpvAi4gS9VpBRVmpqhpsfzCWFzoL5TTdBDP94cpvUGxGcteeWc973U4F5Ei7NkNSADoCSyVAss)          | 0     | ✔           |\n| [5ijWu..UUZD5](https://solscan.io/tx/5ijWuZkQXsHRmoG7fRWd6s6KE6CTi4S3ZxXC9Y8fshozbhjsLVKSaMKFCA4cM58woepTmeGdK8nSCYeLD2iUUZD5)          | [3yUnW..SpQsw](https://solscan.io/tx/3yUnWoRxoL2z5P58j3hyqhgdYQQjWmYRwYEuHtx5UYSiiWj9KJhFey799ViuMH5zfX7j71vtdkzDFiAcqFkSpQsw)          | 0     |             |\n| [2vkg1..s6kJX](https://solscan.io/tx/2vkg1UctLMWzB7ckeJ1umfHJch4EqWQdJouWwDxbJq2cNK721tnKsMEwpZfKhfitgiKMZyxGa4FhHUWBbc8s6kJX)          | [13wFG..QUKHP](https://solscan.io/tx/13wFGrtyeEHoZ4D3f1bY2KPvZcy7F74z8J72qVgkf4E228CMjwX6MNA9w591uhFefo1VrzugFk8FQPMYJpQUKHP)          | 0     | ✔           |\n| [2Np1f..AHT7J](https://solscan.io/tx/2Np1f7PrvAzgPeYUX5XgZSwpfs4zhQE7TzhMmyxZnQBXVCVgeDjK11Z6waPTQHNno3W8L3h1QEtYDKM3X2AAHT7J)          | [YNEcy..RuL3i](https://solscan.io/tx/YNEcyLhrfvo6Fn1mUVB3d19AY6FFATtNhVpwMdyiNKTdaosxDnP6H7mhDzYErLZKh3Ay5x2x7LZuuSCfqWRuL3i)          | 3     |             |\n| [3KoMM..g69fL](https://solscan.io/tx/3KoMM8V54vj9H426C4Lb9ojhK9jhrkjqYjUdawWeqcZE61jqXnpQknEVz2fwQERvy2cR6PB95QBgPumHPpTg69fL)          | [c1NPb..4Akvf](https://solscan.io/tx/c1NPba9xwDZxPGUdSV8jmkLJHiyFDDFx9jjDtGoYat6JUhoA7bwAigUYPmU3Cn9RV5W3H3XC1KoSzMEAZ74Akvf)          | 1     | ✔           |\n| [52nj9..6E8Kd](https://solscan.io/tx/52nj9qCuz2SNS8FKLeKuqLkQDQU8712PCBnTUc86j4osTkeRpsPwrZcfjJiKfCR",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-26T02:19:46.967948"
  },
  {
    "basic_info": {
      "name": "bonkfun-sniper-grpc",
      "full_name": "0xalberto/bonkfun-sniper-grpc",
      "owner": "0xalberto",
      "description": "⚡ Ultra-fast Bonk.fun Sniper Bot using Helius Laserstream gRPC for real-time Solana transaction streaming and automated sniping execution. bonk-sniper-grpc-bot bonk-sniper-grpc-bot bonk-sniper-grpc-bot bonk-sniper-grpc-bot bonk-sniper-grpc-bot bonk-sniper-grpc-bot bonk-sniper-grpc-bot bonk-sniper-grpc-bot bonk-sniper-grpc-bot",
      "url": "https://github.com/0xalberto/bonkfun-sniper-grpc",
      "clone_url": "https://github.com/0xalberto/bonkfun-sniper-grpc.git",
      "ssh_url": "git@github.com:0xalberto/bonkfun-sniper-grpc.git",
      "homepage": null,
      "created_at": "2025-08-31T16:25:33Z",
      "updated_at": "2025-09-21T09:54:40Z",
      "pushed_at": "2025-08-31T16:30:54Z"
    },
    "stats": {
      "stars": 194,
      "forks": 167,
      "watchers": 194,
      "open_issues": 0,
      "size": 2310
    },
    "tech_info": {
      "language": "Rust",
      "languages": {
        "Rust": 29295
      },
      "license": null,
      "topics": []
    },
    "content": {
      "readme": "\n# 🚀 Bonk.fun Sniper Bot — Helius Laserstream gRPC (Solana)\n\nA high-speed **Bonk.fun Sniper Bot** powered by **Helius Laserstream (gRPC)** for real-time Solana transaction streaming and instant token sniping.\n\nBuilt for ultra-low-latency detection and automated buys/sells on **Bonk.fun** token launches.\n\n---\n\n## ✨ Features\n- ⚡ **Real-time transaction stream** via **Helius Laserstream (gRPC)**\n- 🎯 Auto-detects early Bonk.fun token launches (bundle-based mints)\n- 🤖 Fully automated snipe execution\n- 🪙 Customizable buy/sell logic & anti-rug filters\n- 🔒 Secure private key usage (no key exposure)\n- 📊 Transaction metrics & logs\n- 🧩 Modular architecture for extending new heuristics\n\n---\n\n## 🛠 Tech Stack\n- **Rust** — Core bot logic & performance\n- **Helius Laserstream gRPC** — Live transaction feeds\n- **Solana SDK** — Transaction signing & simulation\n- **Yellowstone gRPC** (Optional for alternative stream)\n- **Config.toml** — Easy environment configuration\n\n---\n\n## 📦 Installation\n\n### 1. Clone the Repo\n```bash\ngit clone https://github.com/0xalberto/bonkfun-sniper-grpc.git\ncd bonkfun-sniper-grpc\n```\n\n### 2. Setup Environment Variables\nCreate a `.env` file in the root directory:\n```env\nHELIUS_API_KEY=your_helius_api_key\nPRIVATE_KEY=your_private_key_base58\nRPC_URL=https://api.mainnet-beta.solana.com\n```\n\n### 3. Install Dependencies\n```bash\ncargo build --release\n```\n\n---\n\n## 🚀 Usage\n\n### Run the Sniper\n```bash\ncargo run --release\n```\n\n### What it does:\n1. Connects to Helius Laserstream (gRPC)\n2. Monitors Solana transaction bundles related to Bonk.fun\n3. Detects launch conditions based on heuristics (liquidity added, bundle triggers, etc.)\n4. Executes buy/sell transactions instantly.\n\n---\n\n## 🔧 Configuration Options\n\nYou can modify `config.toml` for parameters:\n```toml\n[snipe]\nprofit_target = 1.5      # 50% profit target\nstop_loss = 0.7          # Stop loss threshold\nbundle_detect_threshold = 3  # Wallets in bundle before sniping\n\n[helius]\ngrpc_url = \"grpc.helius.xyz\"\n```\n\n---\n\n## 🧠 Heuristics Logic (Pluggable)\n- **Bundle Pattern Detection**: Wallets funding each other & buying same token.\n- **Liquidity Add Detection**: New LP pools in a bundle.\n- **Blacklist & Honeypot Check** (Optional extension).\n- **Bonk.fun ID detection** (for Bonk.fun tokens specifically).\n\n---\n\n## ⚠️ Disclaimer\n> **This bot interacts with live financial markets. Use at your own risk.**\n> The authors are not responsible for any financial losses. Ensure compliance with local regulations.\n\n---\n\n## 💡 TODO\n- [ ] Add multi-chain support (Sonic, Neon)\n- [ ] UI Dashboard (WebSocket-based)\n- [ ] Rug detection heuristics\n- [ ] Multi-wallet load balancing\n\n---\n\n## 📄 License\nMIT License\n\n---\n\n## 🤝 Credits\n- [Helius Labs](https://helius.xyz)\n- [Solana Labs](https://solana.com)\n- [Bonk.fun](https://bonk.fun)\n- [Yellowstone gRPC](https://github.com/yellowstone-grpc)\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-26T02:19:48.092976"
  },
  {
    "basic_info": {
      "name": "solana-raydium-pumpfun-sniper-Rust",
      "full_name": "0xalberto/solana-raydium-pumpfun-sniper-Rust",
      "owner": "0xalberto",
      "description": "solana raydium sniper/pumpfun sniper: this solana raydium sniper/pumpfun sniper is written in Rust⚡. In addtion to these solana raydium snipe bot(raydium sniping bot) and pumpfun snipe bot(pumpfun sniping bot), it has raydium bundler, pumpfun bundler, copy trading bot, raydium/pumpfun volume bot more.",
      "url": "https://github.com/0xalberto/solana-raydium-pumpfun-sniper-Rust",
      "clone_url": "https://github.com/0xalberto/solana-raydium-pumpfun-sniper-Rust.git",
      "ssh_url": "git@github.com:0xalberto/solana-raydium-pumpfun-sniper-Rust.git",
      "homepage": null,
      "created_at": "2025-09-01T10:09:50Z",
      "updated_at": "2025-09-21T08:24:52Z",
      "pushed_at": "2025-09-01T10:14:21Z"
    },
    "stats": {
      "stars": 194,
      "forks": 161,
      "watchers": 194,
      "open_issues": 0,
      "size": 12
    },
    "tech_info": {
      "language": "Rust",
      "languages": {
        "Rust": 29645
      },
      "license": null,
      "topics": []
    },
    "content": {
      "readme": "# Solana Ultra-Fast New Token Sniper on Raydium and Pump.fun in Rust 🚀\n\n## Overview\n\nIntroducing the **Solana Ultra-Fast New Token Sniper** written in **Rust** 🦀, designed to detect newly launched tokens on both **Raydium** and **Pump.fun** and execute buys at lightning speed ⚡. With Rust's memory safety features and performance optimizations, built with Rust for superior performance and security. Tailored for both novice and experienced traders.\n\n---\n\n## Key Features\n\n### 🚀 Speed and Efficiency\n- **Lightning-Quick Transactions**: Leveraging Rust's exceptional performance, our bot allows you to snipe new tokens almost instantly. Say goodbye to delays and seize opportunities as they arise!\n\n### 🔒 Safety First\n- **Robust Security**: Rust's safety guarantees minimize bugs and vulnerabilities, ensuring your trading activities are secure. Trade with confidence and peace of mind.\n\n### 📊 Multiple gRPC Connections\n- **Stay Updated**: Effortlessly connect to top Solana data providers like **Helius** and **Yellowstone** through multiple gRPC connections. Get real-time updates and make informed trading decisions.\n\n### 👩‍💻 User-Friendly Interface\n- **Intuitive Design**: Our sniper bot features a clean and accessible interface, making it easy for users of all experience levels to navigate. Start trading in no time!\n\n### 🛠️ Rich Utilities\n- **Advanced Features**:\n  - **jito-confirm**: Engage in low-latency transactions on platforms like Raydium and Pumpfun.\n  - **jito-bundle**: Bundle buy/sell actions with up to **20 wallets** in Raydium/Pumpfun, enhancing your trading strategy and flexibility.\n\n---\n\n## Directory Structure\n\n```\nsrc/\n├── core/\n│   ├── token.rs        # Token definitions and handling\n│   └── tx.rs        # Transaction handling\n| \n├── engine/\n│   ├── swap.rs        # Token swap(buy/sell) functionalities in various Dexs\n│   └── monitor        # New token monitoring(and parse tx) in Dexs using geyser rpc, and normal rpc\n│       └── helius.rs        # Helius gRpc for tx listen and parse.\n│       └── yellowstone.rs        # Yellowstone gRpc for tx listen and parse.\n|\n├── dex/\n│   ├── pump_fun.rs        # Pump.fun\n│   ├── raydium.rs        # Raydium\n│   ├── meteora.rs        # Meteora\n│   └── orca.rs        # Orca\n│\n├── services/\n│   ├── jito.rs        # Jito service provides ultra-fast transaction confirmation\n│   ├── nozomi.rs        # Jito service provides ultra-fast transaction confirmation\n│   ├── zeroslot.rs        # Jito service provides ultra-fast transaction confirmation\n│   └── nextblock.rs        # NextBlock service provides the ultra-fast transaction confirmation in unique way\n|\n├── common/\n│   ├── logger.rs        # Logs to be clean and convenient to monitor.\n│   └── utils.rs        # Utility functions used across the project\n│\n├── lib.rs\n└── main.rs\n```\n---\n## Trial Versions\n\n### **Solana PumpRay Sniper (Trial)**  \n> 🗂️ [solana-pumpray-sniper(trial).zip](https://github.com/user-attachments/files/19416260/solana-pumpray-sniper.trial.zip)\n\n**Strategy Details:**\n- **Entry Trigger:** Monitor user purchases of the new tokens on Dex; execute a buy order upon detection.\n- **Exit Trigger:** Monitor user sales of tokens by checking tp/sl; execute a sell order upon detection.\n- **Time Limitation:** If a position remains open for more than 60 seconds, initiate an automatic sell.  \n*(Note: The tp/sl, as well as the 60-second time limit, are adjustable parameters via environment settings.)*\n---\n\n### How To Run\n1. Environment Variables Settings\n```plaintext\nPRIVATE_KEY= # your wallet priv_key\nRPC_API_KEY= #your helius rpc api-key (Please use premium version that has Geyser Enhanced Websocket)\nSLIPPAGE=10\nJITO_BLOCK_ENGINE_URL=https://ny.mainnet.block-engine.jito.wtf\nJITO_TIP_VALUE=0.00927\nTIME_EXCEED=60 # seconds; time limit for volume non-increasing\nTOKEN_AMOUNT=0.0000001 # token amount to purchase\nTP=3 #3 times\nSL=0.5 #50 percentage\n```\n2. Add the wallet address you want to block on a new line and save the file.\n```\n0x1234567890abcdef1234567890abcdef12345678\n0xabcdef1234567890abcdef1234567890abcdef12\n```\n3. Run `solana-pumpray-sniper.exe`.\n\n![image](https://github.com/user-attachments/assets/dffc8e4b-cd00-4921-8488-e25230f4a31a)\n\n---\n### Test Result: Same Block\n![2-22-2025-09-41](https://github.com/user-attachments/assets/2ded6e35-7575-491e-ac43-5f463b0b9cba)\n\n- Detect: https://solscan.io/tx/5o7ajnZ9CRf7FBYEvydu8vapJJDWtKCvRFiTUBmbeu2FmmDhAQQy3c9YFFhpTucr2SZcrf2aUsDanEVjYgwN9kBc\n- Bought: https://solscan.io/tx/3vgim3MwJsdtahXqfW2DrzTAWpVQ8EUTed2cjzHuqxSfUpfp72mgzZhiVosWaCUHdqJTDHpQaYh5xN7rkHGmzqWv\n- Dexscreener: https://dexscreener.com/solana/A1zZXCq2DmqwVD4fLDzmgQ3ceY6LQnMBVokejqnHpump\n\n---\n## Donate\n\n👉👌 6vT7nrqtbXDWVc8cRUtifxgfDZi19aW7qhcZg2hSepwb\n\n---\n## Support\n\nAs the experimental result, the best environment for running this bot is to use `dedicated server located in NY`. \nFor support and further inquiries, please connect via Telegram: 📞 [Alberto](https://t.me/soladity).\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-26T02:19:49.219587"
  },
  {
    "basic_info": {
      "name": "Ouroboros",
      "full_name": "Hexorg/Ouroboros",
      "owner": "Hexorg",
      "description": "Decompiler written in Rust",
      "url": "https://github.com/Hexorg/Ouroboros",
      "clone_url": "https://github.com/Hexorg/Ouroboros.git",
      "ssh_url": "git@github.com:Hexorg/Ouroboros.git",
      "homepage": null,
      "created_at": "2025-08-30T18:52:56Z",
      "updated_at": "2025-09-25T13:18:43Z",
      "pushed_at": "2025-09-16T17:16:56Z"
    },
    "stats": {
      "stars": 154,
      "forks": 7,
      "watchers": 154,
      "open_issues": 1,
      "size": 404
    },
    "tech_info": {
      "language": "Rust",
      "languages": {
        "Rust": 199437
      },
      "license": "Apache License 2.0",
      "topics": []
    },
    "content": {
      "readme": "# Ouroboros — A Symbolic-Execution Decompiler\n\n[![Rust](https://img.shields.io/badge/language-Rust-orange?logo=rust)](https://www.rust-lang.org/)\n[![License](https://img.shields.io/badge/license-MIT%2FApache-blue.svg)](https://github.com/Hexorg/Ouroboros#license)\n[![egui_version](https://img.shields.io/badge/egui-0.32-blue)](https://github.com/emilk/egui)\n\nA fast, research-friendly decompiler built in Rust that recovers high-level structure from binaries using **symbolic execution**, **constraint tracking** instead of single static assignment IR models (though it can ingest SSA IR). It emphasizes block pre/post conditions, register/memory deltas, dominator/post-dominator analysis, and structured reconstruction (if/else, loops, SESE regions).\n\n![Screenshot](assets/screenshot.png)\n\n---\n\n## Features\n\n- **Symbolic Execution Core**  \n  Tracks path constraints and symbolic values to recover call arguments, globals, and side effects.\n\n- **Constraint-Tracking SSA IR**  \n  VEX-inspired, def-use chains, register/memory deltas, and expression rewriting for clean lifting.\n\n- **CFG + Structural Recovery**  \n  Dominators, post-dominators, and SESE discovery to form `if/else`, loops, and early-exit patterns.\n\n- **Calling Convention Inference**  \n  Heuristics + dataflow to infer parameter/return passing across common ABIs.\n\n- **Rust-first Performance & Safety**  \n  Zero-cost abstractions where possible; explicit, testable passes.\n\n- **Beautiful UI**\n  Thanks and a big shoutout to \n  * [egui](https://github.com/emilk/egui) for this beautiful work.\n  * [egui_dock](https://github.com/Adanos020/egui_dock) for managing dockable windows like a boss.\n  * [egui_graphs](https://github.com/blitzar-tech/egui_graphs) for CFG drawing\n\n---\n\n## Quick Start\n\n### Prerequisites\n- **Rust** (stable). Install via <https://rustup.rs/>\n- Recommended: `llvm-objdump`, `gdb`, or your favorite disassembler for comparison.\n\n### Build\n```bash\ngit clone https://github.com/Hexorg/Ouroboros.git\ncd Ouroboros\ncargo build --release\n```\n\n## License\n\nOuroboros is free, open source and permissively licensed! You can choose from \n\n* MIT License ([LICENSE-MIT](LICENSE-MIT) or [http://opensource.org/licenses/MIT](http://opensource.org/licenses/MIT))\n* Apache License, Version 2.0 ([LICENSE-APACHE](LICENSE-APACHE) or [http://www.apache.org/licenses/LICENSE-2.0](http://www.apache.org/licenses/LICENSE-2.0))\n\nat your option.\nThis means you can select the license you prefer!",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-26T02:19:50.371111"
  },
  {
    "basic_info": {
      "name": "tsink",
      "full_name": "h2337/tsink",
      "owner": "h2337",
      "description": "Embedded time-series database for Rust",
      "url": "https://github.com/h2337/tsink",
      "clone_url": "https://github.com/h2337/tsink.git",
      "ssh_url": "git@github.com:h2337/tsink.git",
      "homepage": "",
      "created_at": "2025-09-12T21:29:24Z",
      "updated_at": "2025-09-25T22:58:08Z",
      "pushed_at": "2025-09-21T01:11:14Z"
    },
    "stats": {
      "stars": 143,
      "forks": 7,
      "watchers": 143,
      "open_issues": 2,
      "size": 245
    },
    "tech_info": {
      "language": "Rust",
      "languages": {
        "Rust": 186920
      },
      "license": "MIT License",
      "topics": [
        "database",
        "embedded-database",
        "rust",
        "time-series",
        "timeseries",
        "timeseries-database",
        "tsdb"
      ]
    },
    "content": {
      "readme": "# tsink\n\n<div align=\"center\">\n\n<p align=\"right\">\n  <img src=\"https://raw.githubusercontent.com/h2337/tsink/refs/heads/master/logo.svg\" width=\"250\" height=\"250\">\n</p>\n\n**A high-performance embedded time-series database for Rust**\n\n</div>\n\n## Overview\n\ntsink is a lightweight, high-performance time-series database engine written in Rust. It provides efficient storage and retrieval of time-series data with automatic compression, time-based partitioning, and thread-safe operations.\n\n### Key Features\n\n- **🚀 High Performance**: Gorilla compression achieves ~1.37 bytes per data point\n- **🔒 Thread-Safe**: Lock-free reads and concurrent writes with configurable worker pools\n- **💾 Flexible Storage**: Choose between in-memory or persistent disk storage\n- **📊 Time Partitioning**: Automatic data organization by configurable time ranges\n- **🏷️ Label Support**: Multi-dimensional metrics with key-value labels\n- **📝 WAL Support**: Write-ahead logging for durability and crash recovery\n- **🗑️ Auto-Retention**: Configurable automatic data expiration\n- **🐳 Container-Aware**: cgroup support for optimal resource usage in containers\n- **⚡ Zero-Copy Reads**: Memory-mapped files for efficient disk operations\n\n## Installation\n\nAdd tsink to your `Cargo.toml`:\n\n```toml\n[dependencies]\ntsink = \"0.3.1\"\n```\n\n## Quick Start\n\n### Basic Usage\n\n```rust\nuse tsink::{DataPoint, Row, StorageBuilder, Storage, TimestampPrecision};\n\nfn main() -> Result<(), Box<dyn std::error::Error>> {\n    // Create storage with default settings\n    let storage = StorageBuilder::new()\n        .with_timestamp_precision(TimestampPrecision::Seconds)\n        .build()?;\n\n    // Insert data points\n    let rows = vec![\n        Row::new(\"cpu_usage\", DataPoint::new(1600000000, 45.5)),\n        Row::new(\"cpu_usage\", DataPoint::new(1600000060, 47.2)),\n        Row::new(\"cpu_usage\", DataPoint::new(1600000120, 46.8)),\n    ];\n    storage.insert_rows(&rows)?;\n\n    // Note: Using timestamp 0 will automatically use the current timestamp\n    // let row = Row::new(\"cpu_usage\", DataPoint::new(0, 50.0));  // timestamp = current time\n\n    // Query data points\n    let points = storage.select(\"cpu_usage\", &[], 1600000000, 1600000121)?;\n    for point in points {\n        println!(\"Timestamp: {}, Value: {}\", point.timestamp, point.value);\n    }\n\n    storage.close()?;\n    Ok(())\n}\n```\n\n### Persistent Storage\n\n```rust\nuse tsink::{StorageBuilder, Storage};\nuse std::time::Duration;\n\nlet storage = StorageBuilder::new()\n    .with_data_path(\"./tsink-data\")              // Enable disk persistence\n    .with_partition_duration(Duration::from_secs(3600))  // 1-hour partitions\n    .with_retention(Duration::from_secs(7 * 24 * 3600))  // 7-day retention\n    .with_wal_buffer_size(8192)                  // 8KB WAL buffer\n    .build()?;\n```\n\n### Multi-Dimensional Metrics with Labels\n\n```rust\nuse tsink::{DataPoint, Label, Row};\n\n// Create metrics with labels for detailed categorization\nlet rows = vec![\n    Row::with_labels(\n        \"http_requests\",\n        vec![\n            Label::new(\"method\", \"GET\"),\n            Label::new(\"status\", \"200\"),\n            Label::new(\"endpoint\", \"/api/users\"),\n        ],\n        DataPoint::new(1600000000, 150.0),\n    ),\n    Row::with_labels(\n        \"http_requests\",\n        vec![\n            Label::new(\"method\", \"POST\"),\n            Label::new(\"status\", \"201\"),\n            Label::new(\"endpoint\", \"/api/users\"),\n        ],\n        DataPoint::new(1600000000, 25.0),\n    ),\n];\n\nstorage.insert_rows(&rows)?;\n\n// Query specific label combinations\nlet points = storage.select(\n    \"http_requests\",\n    &[\n        Label::new(\"method\", \"GET\"),\n        Label::new(\"status\", \"200\"),\n    ],\n    1600000000,\n    1600000100,\n)?;\n\n// Query all label combinations for a metric\nlet all_results = storage.select_all(\"http_requests\", 1600000000, 1600000100)?;\nfor (labels, points) in all_results {\n    println!(\"Labels: {:?}, Points: {}\", labels, points.len());\n}\n```\n\n## Architecture\n\ntsink uses a linear-order partition model that divides time-series data into time-bounded chunks:\n\n```\n┌─────────────────────────────────────────┐\n│             tsink Storage               │\n├─────────────────────────────────────────┤\n│                                         │\n│  ┌───────────────┐  Active Partition    │\n│  │ Memory Part.  │◄─ (Writable)         │\n│  └───────────────┘                      │\n│                                         │\n│  ┌───────────────┐  Buffer Partition    │\n│  │ Memory Part.  │◄─ (Out-of-order)     │\n│  └───────────────┘                      │\n│                                         │\n│  ┌───────────────┐                      │\n│  │ Disk Part. 1  │◄─ Read-only          │\n│  └───────────────┘   (Memory-mapped)    │\n│                                         │\n│  ┌───────────────┐                      │\n│  │ Disk Part. 2  │◄─ Read-only          │\n│  └───────────────┘                      │\n│         ...                             │\n└─────────────────────────────────────────┘\n```\n\n### Partition ",
      "default_branch": "master"
    },
    "fetched_at": "2025-09-26T02:19:51.504205"
  },
  {
    "basic_info": {
      "name": "wakezilla",
      "full_name": "guibeira/wakezilla",
      "owner": "guibeira",
      "description": "A simple Wake-on-LAN & reverse proxy toolkit — wake, route, and control your machines from anywhere. 🦖   ",
      "url": "https://github.com/guibeira/wakezilla",
      "clone_url": "https://github.com/guibeira/wakezilla.git",
      "ssh_url": "git@github.com:guibeira/wakezilla.git",
      "homepage": null,
      "created_at": "2025-09-06T16:13:34Z",
      "updated_at": "2025-09-24T00:46:46Z",
      "pushed_at": "2025-09-19T14:42:11Z"
    },
    "stats": {
      "stars": 132,
      "forks": 8,
      "watchers": 132,
      "open_issues": 1,
      "size": 229
    },
    "tech_info": {
      "language": "Rust",
      "languages": {
        "Rust": 116996,
        "HTML": 37502
      },
      "license": null,
      "topics": []
    },
    "content": {
      "readme": "# Wakezilla 🦖\n<img width=\"200\" height=\"159\" src=\"https://github.com/user-attachments/assets/e88f084b-47b8-467b-a5c6-d64327805792\" align=\"left\" alt=\"wakezilla\"/>\n\n⚡ Wake-on-LAN made simple → power on your machines remotely whenever needed.\n\n🌐 Reverse proxy → intercepts traffic and wakes the server automatically if it’s offline.\n\n🔌 Automatic shutdown → saves energy by powering down idle machines after configurable thresholds.\n\n\n## Web interface\n<img width=\"2698\" height=\"2012\" alt=\"image\" src=\"https://github.com/user-attachments/assets/667eedeb-431c-4aa2-bf7a-3eadd4221452\" />\n\n## Features\n\n- **Wake-on-LAN**: Send magic packets to wake sleeping machines\n- **TCP Proxy**: Forward ports to remote machines with automatic WOL\n- **Web Interface**: Manage machines, ports, and monitor activity through a web dashboard\n- **Automatic Shutdown**: Automatically turn off machines after inactivity periods\n- **Network Scanner**: Discover machines on your local network\n\n## Installation\n\n### Server Installation\n\n1. **Install Rust**:\n   ```bash\n   curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh\n   source $HOME/.cargo/env\n   ```\n\n2. **Clone and Build**:\n   ```bash\n   git clone <repository-url>\n   cd wakezilla\n   cargo build --release\n   ```\n\n3. **Configure the Server**:\n   Create a `machines.json` file (optional, will be created automatically):\n   ```json\n   []\n   ```\n\n4. **Run the Server**:\n   ```bash\n   ./target/release/wakezilla --server\n   ```\n   \n   By default, the web interface runs on port 3000.\n\n### Client Installation\n   make sure the machine was configured with wake on lan.\n1. **Install Rust** (if not already installed):\n   ```bash\n   curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh\n   source $HOME/.cargo/env\n   ```\n\n2. **Clone and Build**:\n   ```bash\n   git clone <repository-url>\n   cd wakezilla\n   cargo build --release\n   ```\n## Usage\n\n### Starting the Proxy Server\n```bash\n# Basic server start\n./target/release/wakezilla proxy-server\n\n# With custom port\n./target/release/wakezilla proxy-server --port 8080\n\n```\n\n### Starting the Client\n```bash\n# Connect to server\n./target/release/wakezilla client-server \n\n# With custom port\n./target/release/wakezilla client-server --port 8080\n\n```\n\n### Web Interface\nAccess the web interface at `http://<server-ip>:3000` to:\n- Add and manage machines\n- Configure port forwards\n- View network scan results\n- Send WOL packets manually\n- Configure automatic shutdown settings\n\n### Adding Machines\n1. Navigate to the web interface\n2. Click \"Add Machine\" or use the network scanner\n3. Fill in MAC address, IP, and name\n4. Configure:\n   - Turn-off port (if remote shutdown is needed)\n   - Request rate limiting (requests per hour and period minutes)\n   - Port forwards as needed\n\n### Configuring Automatic Shutdown\n1. When adding or editing a machine, enable \"Can be turned off remotely\"\n2. Set the \"Turn Off Port\" (typically 3001 for the client server)\n3. Configure rate limiting:\n   - Requests per Hour: Number of requests allowed\n   - Period Minutes: Time window for rate limiting\n4. The machine will automatically shut down after the configured inactivity period\n\n### Port Forwarding\n1. Add a machine to the system\n2. Configure port forwards for that machine:\n   - Local Port: Port on the server to listen on\n   - Target Port: Port on the remote machine to forward to\n3. When traffic hits the local port, the machine will be woken up if needed and traffic forwarded\n\n\n### Machine Configuration\nEach machine can be configured with:\n- MAC Address\n- IP Address\n- Name and Description\n- Turn-off Port (for remote shutdown)\n- Request Rate Limiting:\n  - Requests per Hour: Maximum requests allowed\n  - Period Minutes: Time window for rate limiting\n- Port Forwards:\n  - Local Port: Port on the server\n  - Target Port: Port on the remote machine\n\n## How It Works\n\n1. **Server Mode**: Runs the web interface and proxy services\n2. **Client Mode**: Runs on target machines to enable remote shutdown\n3. **WOL Process**: \n   - When traffic hits a configured port, the server sends a WOL packet\n   - Waits for the machine to become reachable\n   - Forwards traffic once the machine is up\n4. **Automatic Shutdown**: \n   - Monitors request activity for each machine\n   - After configured inactivity periods, sends shutdown signal\n   - Uses HTTP requests to the client for shutdown\n\n## Security Considerations\n\n- The server should be run on a trusted network\n- Access to the web interface should be restricted if exposed to the internet\n- The turn-off endpoint on clients should only be accessible from the server\n\n## Troubleshooting\n\n### Common Issues\n\n1. **Machine not waking up**:\n   - Verify the MAC address is correct\n   - Ensure WOL is enabled in the machine's BIOS/UEFI\n   - Check firewall settings on the target machine\n   - Verify the target machine supports WOL\n\n2. **Proxy not working**:\n   - Check that the target port is correct\n   - Verify the machine is reachable after WOL\n   - Ensure no firewall is blocking the conne",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-26T02:19:52.669702"
  },
  {
    "basic_info": {
      "name": "DriftDB",
      "full_name": "DavidLiedle/DriftDB",
      "owner": "DavidLiedle",
      "description": "DriftDB - An experimental append-only database with built-in time travel. Query any point in history, guaranteed data integrity, and immutable audit trails. Written in Rust.",
      "url": "https://github.com/DavidLiedle/DriftDB",
      "clone_url": "https://github.com/DavidLiedle/DriftDB.git",
      "ssh_url": "git@github.com:DavidLiedle/DriftDB.git",
      "homepage": null,
      "created_at": "2025-09-14T16:27:28Z",
      "updated_at": "2025-09-25T21:38:18Z",
      "pushed_at": "2025-09-25T00:18:28Z"
    },
    "stats": {
      "stars": 126,
      "forks": 5,
      "watchers": 126,
      "open_issues": 2,
      "size": 2576
    },
    "tech_info": {
      "language": "Rust",
      "languages": {
        "Rust": 2453325,
        "Python": 338364,
        "Shell": 4308,
        "PLpgSQL": 2708,
        "Makefile": 2693,
        "Dockerfile": 1122
      },
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "# DriftDB\n\n**Experimental PostgreSQL-Compatible Time-Travel Database (v0.7.3-alpha)** - An ambitious temporal database project with advanced architectural designs for enterprise features. Query your data at any point in history using standard SQL.\n\n⚠️ **ALPHA SOFTWARE - NOT FOR PRODUCTION USE**: This version contains experimental implementations of enterprise features. The codebase now compiles cleanly with minimal warnings (reduced from 335 to 17). Many advanced features remain as architectural designs requiring implementation.\n\n## 🚀 Quick Start\n\n```bash\n# Start the PostgreSQL-compatible server\n./target/release/driftdb-server --data-path ./data\n\n# Connect with any PostgreSQL client\npsql -h localhost -p 5433 -d driftdb\n\n# Use standard SQL with time-travel\nCREATE TABLE events (id INT PRIMARY KEY, data VARCHAR);\nINSERT INTO events (id, data) VALUES (1, 'original');\nUPDATE events SET data = 'modified' WHERE id = 1;\n\n-- Query historical state!\nSELECT * FROM events AS OF @seq:1;  -- Shows 'original'\nSELECT * FROM events;                -- Shows 'modified'\n```\n\n## ✅ Working Features\n\n### Full SQL Support\n- **All 5 standard JOIN types**: INNER, LEFT, RIGHT, FULL OUTER, CROSS (including self-joins)\n- **Subqueries**: IN/NOT IN, EXISTS/NOT EXISTS (including correlated!), scalar subqueries\n- **Common Table Expressions (CTEs)**: WITH clause including RECURSIVE CTEs\n- **Transactions**: BEGIN, COMMIT, ROLLBACK with ACID guarantees\n- **Views**: CREATE/DROP VIEW with persistence across restarts\n- **DDL operations**: CREATE TABLE, ALTER TABLE ADD COLUMN, CREATE INDEX, TRUNCATE\n- **Aggregation functions**: COUNT(*), COUNT(DISTINCT), SUM, AVG, MIN, MAX\n- **GROUP BY and HAVING**: Full support for grouping with aggregate filtering\n- **CASE WHEN expressions**: Conditional logic in queries\n- **Set operations**: UNION, INTERSECT, EXCEPT\n- **Multi-row INSERT**: INSERT INTO ... VALUES (row1), (row2), ...\n- **Foreign key constraints**: Referential integrity enforcement\n- **Time-travel queries**: `AS OF` for querying historical states\n\n### Core Database Engine\n- **Event sourcing**: Every change is an immutable event with full history\n- **Time-travel queries**: Query any historical state by sequence number\n- **ACID compliance**: Full transaction support with isolation levels\n- **CRC32 verification**: Data integrity on every frame\n- **Append-only storage**: Never lose data, perfect audit trail\n- **JSON documents**: Flexible schema with structured data\n\n### Tested & Verified\n- ✅ Python psycopg2 driver\n- ✅ Node.js pg driver\n- ✅ JDBC PostgreSQL driver\n- ✅ SQLAlchemy ORM\n- ✅ Any PostgreSQL client\n\n## 🎯 Perfect For\n\n- **Debugging Production Issues**: \"What was the state when the bug occurred?\"\n- **Compliance & Auditing**: Complete audit trail built-in, no extra work\n- **Data Recovery**: Accidentally deleted data? It's still there!\n- **Analytics**: Track how metrics changed over time\n- **Testing**: Reset to any point, perfect for test scenarios\n- **Development**: Branch your database like Git\n\n## ✨ Core Features\n\n### SQL:2011 Temporal Queries (Native Support)\n- **`FOR SYSTEM_TIME AS OF`**: Query data at any point in time\n- **`FOR SYSTEM_TIME BETWEEN`**: Get all versions in a time range\n- **`FOR SYSTEM_TIME FROM...TO`**: Exclusive range queries\n- **`FOR SYSTEM_TIME ALL`**: Complete history of changes\n- **System-versioned tables**: Automatic history tracking\n\n### Data Model & Storage\n- **Append-only storage**: Immutable events preserve complete history\n- **Time travel queries**: Standard SQL:2011 temporal syntax\n- **ACID transactions**: Full transaction support with isolation levels\n- **Secondary indexes**: B-tree indexes for fast lookups\n- **Snapshots & compaction**: Optimized performance with compression\n\n### Planned Enterprise Features (Not Yet Functional)\nThe following features have been architecturally designed but are not yet operational:\n- **Authentication & Authorization**: Planned RBAC with user management (code incomplete)\n- **Encryption at Rest**: Designed AES-256-GCM encryption (not functional)\n- **Distributed Consensus**: Raft protocol structure (requires debugging)\n- **Advanced Transactions**: MVCC design for isolation levels (partial implementation)\n- **Enterprise Backup**: Backup system architecture (compilation errors)\n- **Security Monitoring**: Monitoring framework (not integrated)\n\n### Working Infrastructure\n- **Connection pooling**: Thread-safe connection pool with RAII guards\n- **Health checks**: Basic metrics endpoint\n- **Rate limiting**: Token bucket algorithm for connection limits\n\n### Query Features (Partially Working)\n- **B-tree indexes**: Secondary indexes for fast lookups (functional)\n- **Basic query planner**: Simple execution plans (working)\n- **Prepared statements**: Statement caching (functional)\n\n### Planned Query Optimization (Design Phase)\n- **Advanced Query Optimizer**: Cost-based optimization design (not implemented)\n- **Join Strategies**: Theoretical star schema optimization (code incomplete)\n- **Subquery O",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-26T02:19:53.872288"
  },
  {
    "basic_info": {
      "name": "doppler",
      "full_name": "blueshift-gg/doppler",
      "owner": "blueshift-gg",
      "description": "The fastest oracle on Solana",
      "url": "https://github.com/blueshift-gg/doppler",
      "clone_url": "https://github.com/blueshift-gg/doppler.git",
      "ssh_url": "git@github.com:blueshift-gg/doppler.git",
      "homepage": "https://doppler.blueshift.gg",
      "created_at": "2025-09-02T14:15:42Z",
      "updated_at": "2025-09-25T16:25:42Z",
      "pushed_at": "2025-09-07T13:35:27Z"
    },
    "stats": {
      "stars": 119,
      "forks": 19,
      "watchers": 119,
      "open_issues": 5,
      "size": 116
    },
    "tech_info": {
      "language": "Rust",
      "languages": {
        "Rust": 20951,
        "Shell": 581
      },
      "license": "MIT License",
      "topics": [
        "defi",
        "oracle",
        "price-feed",
        "solana"
      ]
    },
    "content": {
      "readme": "![](./assets/logo.svg)\n\n<h3 align=\"center\">\n  A 21 CU Solana Oracle Program\n</h3>\n\n## Overview\n\nDoppler is an ultra-optimized oracle program for Solana, achieving unparalleled performance at just **21 Compute Units (CUs)** per update. Built with low-level optimizations and minimal overhead, Doppler sets the standard for high-frequency, low-latency price feeds on Solana.\n\n## Features\n\n- **21 CU Oracle Updates**: The most efficient oracle implementation on Solana\n- **Generic Payload Support**: Flexible data structure supporting any payload type\n- **Sequence-Based Updates**: Built-in replay protection and ordering guarantees\n- **Zero Dependencies**: Pure no_std Rust implementation for minimal overhead\n- **Direct Memory Operations**: Optimized assembly-level exits for maximum efficiency\n\n## Installation\n\nAdd Doppler SDK and required Solana crates to your `Cargo.toml`:\n\n```toml\n[dependencies]\ndoppler-sdk = \"0.1.0\"\nsolana-instruction = \"2.3.0\"\nsolana-pubkey = \"2.3.0\"\nsolana-compute-budget-interface = \"2.2.2\"\nsolana-transaction = \"2.3.0\"\nsolana-keypair = \"2.3.0\"\nsolana-signer = \"2.2.1\"\n# Add other Solana crates as needed\n```\n\n## Program ID\n\n```\nfastRQJt3nLdY3QA7n8eZ8ETEVefy56ryfUGVkfZokm\n```\n\n## Architecture\n\nDoppler uses a simple yet powerful architecture:\n\n1. **Admin Account**: Controls oracle updates (hardcoded for security)\n2. **Oracle Account**: Stores the sequence number and payload data\n3. **Sequence Validation**: Ensures updates are monotonically increasing\n\n### Data Structure\n\n```rust\npub struct Oracle<T> {\n    pub sequence: u64,  // Timestamp, slot height, or auto-increment\n    pub payload: T,     // Your custom data structure\n}\n```\n\n## Usage Guide\n\n### 1. Setting Up Compute Budget\n\nTo achieve the 21 CU performance, configure your transaction with appropriate compute budget:\n\n```rust\nuse solana_compute_budget_interface::ComputeBudgetInstruction;\nuse solana_instruction::Instruction;\nuse solana_transaction::Transaction;\n\n// Request exactly the CUs needed (21 + overhead for other instructions)\nlet compute_budget_ix = ComputeBudgetInstruction::set_compute_unit_limit(200_000);\n\n// Add to your transaction\nlet mut instructions = vec![compute_budget_ix];\n```\n\n### 2. Setting Priority Fees\n\nFor high-frequency oracle updates, use priority fees to ensure timely inclusion:\n\n```rust\n// Set priority fee (price per compute unit in micro-lamports)\nlet priority_fee_ix = ComputeBudgetInstruction::set_compute_unit_price(1000);\n\ninstructions.push(priority_fee_ix);\n```\n\n### 3. Optimizing Account Data Size\n\nUse `setLoadedAccountsDataSizeLimit` to optimize memory allocation:\n\n```rust\n// Set the maximum loaded account data size\n// Calculate based on your oracle data structure size\nlet data_size_limit_ix = ComputeBudgetInstruction::set_loaded_accounts_data_size_limit(\n    32_768  // 32KB is usually sufficient for oracle operations\n);\n\ninstructions.push(data_size_limit_ix);\n```\n\n### 4. Creating an Oracle Update\n\n```rust\nuse doppler_sdk::{Oracle, UpdateInstruction, ID as DOPPLER_ID};\nuse solana_instruction::Instruction;\nuse solana_pubkey::Pubkey;\n\n// Define your payload structure\n#[derive(Clone, Copy)]\npub struct PriceFeed {\n    pub price: u64,\n}\n\n// Create oracle update\nlet oracle_update = Oracle {\n    sequence: 1234567890,  // Must be > current sequence\n    payload: PriceFeed {\n        price: 42_000_000,  // $42.00 with 6 decimals\n    },\n};\n\n// Create update instruction\nlet update_ix: Instruction = UpdateInstruction {\n    admin: admin_pubkey,\n    oracle_pubkey: oracle_pubkey,\n    oracle: oracle_update,\n}.into();\n\n// Add to instructions\ninstructions.push(update_ix);\n```\n\n### 5. Complete Transaction Example\n\n```rust\nuse doppler_sdk::{Oracle, UpdateInstruction};\nuse solana_client::rpc_client::RpcClient;\nuse solana_compute_budget_interface::ComputeBudgetInstruction;\nuse solana_instruction::Instruction;\nuse solana_keypair::Keypair;\nuse solana_signer::Signer;\nuse solana_transaction::Transaction;\n\nasync fn update_oracle(\n    client: &RpcClient,\n    admin: &Keypair,\n    oracle_pubkey: Pubkey,\n    new_price: u64,\n    sequence: u64,\n) -> Result<(), Box<dyn std::error::Error>> {\n    // Build all instructions\n    let mut instructions = vec![\n        // 1. Set compute budget\n        ComputeBudgetInstruction::set_compute_unit_limit(200_000),\n\n        // 2. Set priority fee (1000 micro-lamports per CU)\n        ComputeBudgetInstruction::set_compute_unit_price(1_000),\n\n        // 3. Set loaded accounts data size limit\n        ComputeBudgetInstruction::set_loaded_accounts_data_size_limit(32_768),\n    ];\n\n    // 4. Add oracle update\n    let oracle_update = Oracle {\n        sequence,\n        payload: PriceFeed { price: new_price },\n    };\n\n    let update_ix: Instruction = UpdateInstruction {\n        admin: admin.pubkey(),\n        oracle_pubkey,\n        oracle: oracle_update,\n    }.into();\n\n    instructions.push(update_ix);\n\n    // Create and send transaction\n    let recent_blockhash = client.get_latest_blockhash()?;\n    let tx = Transaction::new_sig",
      "default_branch": "master"
    },
    "fetched_at": "2025-09-26T02:19:55.018747"
  }
]