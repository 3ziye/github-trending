[
  {
    "basic_info": {
      "name": "ai-humanizer-api",
      "full_name": "HuzefaUsama25/ai-humanizer-api",
      "owner": "HuzefaUsama25",
      "description": "AI Humanizer API converts AI text into high quality undetectable human-like writing. Bypasses Turnitin, GPTZero, Originality.ai, CopyLeaks. Fast, natural, high quality output for students, marketers, and content creators",
      "url": "https://github.com/HuzefaUsama25/ai-humanizer-api",
      "clone_url": "https://github.com/HuzefaUsama25/ai-humanizer-api.git",
      "ssh_url": "git@github.com:HuzefaUsama25/ai-humanizer-api.git",
      "homepage": null,
      "created_at": "2025-09-18T08:21:03Z",
      "updated_at": "2025-09-18T20:11:30Z",
      "pushed_at": "2025-09-18T08:37:02Z"
    },
    "stats": {
      "stars": 40,
      "forks": 0,
      "watchers": 40,
      "open_issues": 0,
      "size": 4
    },
    "tech_info": {
      "language": null,
      "languages": {},
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "# AI Humanizer API\n\nThe **AI Humanizer API** is the **best and only truly effective AI humanizer API**. It transforms AI-generated text into **natural, human-like writing** that passes every major AI detector without losing meaning, tone, or quality. Unlike other tools that produce awkward or robotic rewrites, the AI Humanizer API generates fluent, authentic, and **indistinguishable human text**.\n\nThis repository provides examples, quick integration steps, and developer resources for the [WriteHybrid AI Humanizer API](https://writehybrid.com/api-docs).\n\n---\n\n## Table of Contents\n\n- [Why Use the AI Humanizer API](#why-use-the-ai-humanizer-api)  \n- [Key Features](#key-features)  \n- [Quick Start](#quick-start)  \n  - [Authentication](#authentication)  \n  - [Base URL](#base-url)  \n  - [Example Request (cURL)](#example-request-curl)  \n  - [Example Response](#example-response)  \n- [Parameters](#parameters)  \n- [Pricing](#pricing)  \n- [Who Is It For](#who-is-it-for)  \n- [Why Trust This API](#why-trust-this-api)  \n- [Get Started](#get-started-with-ai-humanizer-api)  \n\n---\n\n## Why Use the AI Humanizer API\n\nAI text often gets flagged by detectors such as **Turnitin, GPTZero, Originality.ai, Copyleaks, Sapling, and more**. Getting flagged can result in:\n\n- SEO penalties and lower Google rankings  \n- Academic or publishing rejection  \n- Damaged trust with clients, readers, or customers  \n\nThe **AI Humanizer API is the only solution that reliably bypasses every major detector** while preserving the original context and style. It is fast, scalable, and trusted by real users.\n\n---\n\n## Key Features\n\n- **Only Useful Humanizer API**: Every other tool falls short; this one works.  \n- **Detector-Proof**: Passes Turnitin, GPTZero, Originality.ai, Copyleaks, and more.  \n- **Transparent Billing**: 1 credit = 1 word.  \n- **High Quality**: Meaning and nuance are preserved.  \n- **Fast**: Humanize thousands of words in seconds.  \n- **Natural Output**: Fluent, human-like text every time.  \n- **Developer Friendly**: RESTful API with simple JSON responses.  \n- **Scalable**: From indie projects to enterprise-grade platforms.  \n\n---\n\n## Quick Start\n\n### Authentication\n\nAll requests require an API key. Add it to the `Authorization` header as a Bearer token.\n\n### Base URL\n\n[https://whbserver.com/api/v1](https://whbserver.com/api/v1)\n\n\n### Example Request (cURL)\n\n```bash\ncurl -X POST https://whbserver.com/api/v1/humanizer/ \\\n  -H \"Authorization: Bearer YOUR_API_KEY\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n        \"text\": \"AI-generated text goes here...\",\n        \"complexity\": \"medium\",\n        \"purpose\": \"general\"\n      }'\n````\n\n### Example Response\n\n```json\n{\n  \"success\": true,\n  \"humanized_text\": \"In today’s fast-paced world, technology continues to evolve at incredible speed...\",\n  \"original_length\": 245,\n  \"humanized_length\": 263,\n  \"processing_time\": 1.1,\n  \"detection_score\": 0.01,\n  \"credits_used\": 245\n}\n```\n\n---\n\n## Parameters\n\n| Field        | Type   | Description                                                      |\n| ------------ | ------ | ---------------------------------------------------------------- |\n| `text`       | string | The AI-generated text you want to humanize                       |\n| `complexity` | string | Output style: `simple`, `medium`, `complex`                      |\n| `purpose`    | string | Rewrite context: `general`, `seo`, `academic`, `marketing`, etc. |\n\n---\n\n## Pricing\n\n* **Starter** – 20,000 credits/month (20,000 words), email support\n* **Pro** – 60,000 credits/month (60,000 words), advanced tones, faster processing\n* **Agency** – 150,000 credits/month (150,000 words), team access, priority support\n* **Enterprise** – Custom high-volume solutions\n\n**Note:** 1 credit = 1 word.\n\nFull details: [writehybrid.com/api-docs](https://writehybrid.com/api-docs)\n\n---\n\n## Who Is It For?\n\n* **SEO Teams** – Safely scale content that ranks without AI penalties.\n* **Students & Academics** – Submit detection-proof work that reads as authentic.\n* **Content Agencies** – Deliver client-ready copy at scale with no manual rewrites.\n* **SaaS Builders** – Add humanization as a core feature inside your product.\n* **Freelancers** – Ensure client work is never penalized for AI generation.\n\n---\n\n## Why Trust This API\n\n* **Proven**: Successfully bypasses every major AI detector.\n* **Transparent**: 1 credit = 1 word, no hidden rules.\n* **Reliable**: Built on robust, scalable infrastructure.\n* **Trusted**: Used daily by agencies, startups, and independent creators worldwide.\n\n---\n\n## Get Started with AI Humanizer API\n\nStop wasting time on tools that don’t work. The **AI Humanizer API is the only real solution** for creating undetectable, human-like text at scale.\n\n👉 [**Get your API key today**](https://writehybrid.com/api-docs) and start humanizing AI text instantly.\n\n\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-19T00:54:26.228036"
  },
  {
    "basic_info": {
      "name": "RamTorch",
      "full_name": "lodestone-rock/RamTorch",
      "owner": "lodestone-rock",
      "description": "RAM is all you need",
      "url": "https://github.com/lodestone-rock/RamTorch",
      "clone_url": "https://github.com/lodestone-rock/RamTorch.git",
      "ssh_url": "git@github.com:lodestone-rock/RamTorch.git",
      "homepage": null,
      "created_at": "2025-09-18T12:16:49Z",
      "updated_at": "2025-09-19T00:50:38Z",
      "pushed_at": "2025-09-18T14:44:33Z"
    },
    "stats": {
      "stars": 28,
      "forks": 0,
      "watchers": 28,
      "open_issues": 0,
      "size": 14
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 9145
      },
      "license": "Apache License 2.0",
      "topics": []
    },
    "content": {
      "readme": "# RamTorch\n\n**RAM is All You Need** - A PyTorch library for memory-efficient deep learning that enables training and inference of large models that don't fit in GPU memory.\n\n## Overview\n\nRamTorch provides CPU-GPU hybrid implementations of neural network components that keep parameters in CPU memory and transfer them to GPU on-demand. This approach dramatically reduces GPU memory usage while maintaining computational efficiency through asynchronous CUDA streams and intelligent batching.\n\n## Key Features\n\n- **Memory-Efficient Linear Layers**: CPU-stored parameters with on-demand GPU transfer\n- **Asynchronous CUDA Streams**: Overlap computation with data transfer for minimal latency\n- **ZeRO-1 Optimizer Support**: Distributed optimizer state sharding across multiple GPUs\n- **Drop-in Replacement**: Compatible with existing PyTorch code\n- **Configurable Transfer Throttling**: Controllable memory usage\n\n## Installation\n\n```bash\npip install ramtorch\n```\n\nOr install from source:\n\n```bash\ngit clone https://github.com/lodestone-rock/RamTorch.git\ncd RamTorch\npip install -e .\n```\n\n## Quick Start\n\n### Basic Usage\n\nReplace `torch.nn.Linear` with `ramtorch.modules.Linear` for automatic memory optimization:\n\n```python\nimport torch\nimport ramtorch.modules as ram_modules\n\n# Standard PyTorch approach (high GPU memory usage)\n# linear = torch.nn.Linear(1000, 1000)\n\n# RamTorch approach (low GPU memory usage)\nlinear = ram_modules.Linear(1000, 1000, device=\"cuda\")\n\n# Use exactly like a normal PyTorch layer\nx = torch.randn(32, 1000, device=\"cuda\")\noutput = linear(x)  # Parameters automatically transferred from CPU to GPU\n```\n\n### Building Models\n\n```python\nimport torch.nn as nn\nimport ramtorch.modules as ram_modules\n\nclass MemoryEfficientModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layers = nn.Sequential(\n            ram_modules.Linear(1000, 2000),\n            nn.ReLU(),\n            ram_modules.Linear(2000, 2000),\n            nn.ReLU(),\n            ram_modules.Linear(2000, 100)\n        )\n    \n    def forward(self, x):\n        return self.layers(x)\n\nmodel = MemoryEfficientModel()\n```\n\n### ZeRO-1 Optimizer Sharding\n\nFor distributed training with optimizer state sharding:\n\n```python\nimport torch.distributed as dist\nfrom ramtorch.zero1 import create_zero_param_groups, broadcast_zero_params\n\n# Initialize distributed training\ndist.init_process_group(backend='nccl')\nmodel = YourModel()\nall_params = list(model.parameters())\nrank = dist.get_rank()\nworld_size = dist.get_world_size()\n\n# Create ZeRO-1 sharded optimizer\nparam_groups = [{'params': all_params, 'lr': 1e-3, 'weight_decay': 0.01}]\nsharded_groups, owner_ranks = create_zero_param_groups(param_groups, rank, world_size)\noptimizer = torch.optim.AdamW(sharded_groups)\n\n# Scheduler works normally with sharded optimizer\nscheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n\n# Training loop\nfor epoch in range(num_epochs):\n    for batch in dataloader:\n        # Forward/backward with gradient accumulation\n        for micro_batch in split_batch(batch):\n            loss = model(micro_batch)\n            loss.backward()\n\n        # All-reduce gradients across ranks (you need to implement this)\n        all_reduce_gradients(all_params)\n        \n        # Each rank updates only its owned parameters\n        optimizer.step()\n        \n        # Broadcast updated parameters from owners to all ranks\n        broadcast_zero_params(all_params, owner_ranks)\n        \n        model.zero_grad()\n        scheduler.step()\n```\n## Configuration\n\n### Environment Variables\n\n- `MAX_INFLIGHT`: Maximum number of concurrent CPU-to-GPU transfers (default: 2)\n  ```bash\n  export MAX_INFLIGHT=4  # Allow more concurrent transfers\n  ```\n\n### Transfer Stream Management\n\nRamTorch automatically manages CUDA streams for optimal performance. The library uses a dedicated transfer stream to overlap data movement with computation.\n\n## Performance Considerations\n\n### When to Use RamTorch\n\n**Best suited for:**\n- Large models that don't fit in GPU memory\n- Inference scenarios with memory constraints\n- Training with limited GPU memory but abundant CPU memory\n- Distributed training with many parameters\n\n**Less suitable for:**\n- Small models that fit comfortably in GPU memory\n- Scenarios where CPU-GPU bandwidth is the bottleneck\n- Real-time applications requiring minimal latency\n\n### Optimization Tips\n\n1. **Use Larger Batch Sizes**: Helps amortize transfer costs\n2. **Configure MAX_INFLIGHT**: Tune based on your GPU memory availability\n3. **Mixed Precision**: Combine with `torch.cuda.amp` for additional memory savings\n4. **Strategic Placement**: Use RamTorch layers for the largest components only\n\n## Architecture\n\n### CPU Bouncing Linear Layer\n\n\n1. Stores parameters on CPU memory (with `share_memory_()` for multiprocessing)\n2. Asynchronously transfers weights to GPU during forward pass\n3. Uses CUDA events for proper stream synchronization\n4. Automatically throttles transfers to preve",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-19T00:54:27.573484"
  },
  {
    "basic_info": {
      "name": "Telegram-Adding-Users",
      "full_name": "duongtsa/Telegram-Adding-Users",
      "owner": "duongtsa",
      "description": "An automation for the process of adding communitymembers to any target of your choice. Simple to use and up to date!",
      "url": "https://github.com/duongtsa/Telegram-Adding-Users",
      "clone_url": "https://github.com/duongtsa/Telegram-Adding-Users.git",
      "ssh_url": "git@github.com:duongtsa/Telegram-Adding-Users.git",
      "homepage": "",
      "created_at": "2025-09-18T11:54:18Z",
      "updated_at": "2025-09-18T16:13:09Z",
      "pushed_at": "2025-09-18T11:56:56Z"
    },
    "stats": {
      "stars": 23,
      "forks": 0,
      "watchers": 23,
      "open_issues": 0,
      "size": 19779
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 56
      },
      "license": null,
      "topics": [
        "adders",
        "memberss-scri",
        "scraper",
        "telegram",
        "telegram-add",
        "telegram-add-user",
        "telegram-add-users",
        "telegram-adding-members",
        "telegram-channel-clone",
        "telegram-copy-channel",
        "telegram-copy-members",
        "telegram-copy-users",
        "telegram-group-clone",
        "telegram-member-adder-2025",
        "telegram-member-adders-2025",
        "telegram-memberadder",
        "telegram-members-add",
        "telegram-members-adder-2025",
        "telegram-membersadder",
        "telegram-user-adders-2025"
      ]
    },
    "content": {
      "readme": "# Telegram-Adding-Users\nAn automation for the process of adding communitymembers to any target of your choice. Simple to use and up to date!\n\n# 📁 GET IT HERE: https://shorturl.at/MdvDy\n# CONTACT FOR QUESTIONS: https://shorturl.at/qRqTq\n\n<img src='UI1.png' width='450'>\n\n- EXTRACT MEMBERS, MESSAGES, MEDIA, CHANNELS AND MORE! (EVEN HIDDEN MEMBERS)\n![](scrap.gif)\n- ADD MEMBERS TO YOUR GROUPS/CHANNELS AUTOMATICALLY!\n- FILTERING ONLY PREMIUM MEMBERS POSSIBLE! (OPTIONAL)\n![](add.gif)\n- AUTOMATICALLY FORWARD ANY POST/MESSAGE/MEDIA TO ANY TARGET!\n- MASSDM ANYONE ON TELEGRAM!\n![](mass.gif)\n- CLONE AND COPY ANY CHANNELS/GROUPS!\n![](copy.gif)\n- JOIN TO TARGETS WITH ALL OF YOUR ACCOUNTS AUTOMATICALLY!\n![](join.gif)\n- GET RID OF YOUR COMPETITION EASILY!\n- GROW YOUR AUDIENCE EASILY!\n- GROW YOUR VIEWS AUTOMATICALLY!\n![](view_post.gif)\n- VOTE ON ANY POLLS AUTOMATICALLY!\n- UNSPAM AND UNFREEZE YOUR ACCOUNTS EASILY!\n- REACT TO ANY POST AUTOMATICALLY WITH EMOJI'S!\n- MAKE BACKUPS!\n- NO CODING SKILLS REQUIRED!\n- PROXY SUPPORTED (OPTIONAL)\n- THE ONLY TG TOOL WHICH IS UPDATED TO 2025!\n- SUPPORT AND UPDATES FOR LIFETIME!\n- AND MUCH MORE!\n\nNEW FEATURES WILL BE IMPLEMENTED AT WISH!\nIf you have any questions, make sure to contact us.\n\n# 📁 GET IT HERE: https://shorturl.at/MdvDy\n# CONTACT FOR QUESTIONS: https://shorturl.at/qRqTq",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-19T00:54:28.925308"
  },
  {
    "basic_info": {
      "name": "model-memory-calculator",
      "full_name": "KolosalAI/model-memory-calculator",
      "owner": "KolosalAI",
      "description": "Simple model memory requirements calculator for GGUF",
      "url": "https://github.com/KolosalAI/model-memory-calculator",
      "clone_url": "https://github.com/KolosalAI/model-memory-calculator.git",
      "ssh_url": "git@github.com:KolosalAI/model-memory-calculator.git",
      "homepage": null,
      "created_at": "2025-09-18T06:51:30Z",
      "updated_at": "2025-09-19T00:02:32Z",
      "pushed_at": "2025-09-18T23:59:26Z"
    },
    "stats": {
      "stars": 23,
      "forks": 2,
      "watchers": 23,
      "open_issues": 0,
      "size": 43
    },
    "tech_info": {
      "language": "HTML",
      "languages": {
        "HTML": 28776
      },
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "# GGUF Metadata Reader (Browser)\n\nA single-file, static web app to read GGUF model metadata directly in the browser and estimate memory usage (RAM/VRAM) for a chosen context window and KV cache quantization.\n\n- Works with remote URLs that support HTTP Range requests (e.g., many Hugging Face files)\n- Works with local `.gguf` files (drag-and-drop via file picker)\n- Detects sharded models (e.g., `-00001-of-00013`) and sums total size\n- No server required; everything runs client-side\n\n## Quick Start\n\nOption A: Open the file directly\n1. Open `index.html` in a modern browser (Chrome, Edge, Safari).\n2. Paste a GGUF URL or choose a local `.gguf` file and click the corresponding button.\n\nOption B: Serve locally (helps with some CORS setups)\n```bash\ncd path/to/model-memory-calculator\npython -m http.server 8000\n# Then open http://localhost:8000 in your browser\n```\n\n## Usage\n\n- GGUF URL: Paste a direct link to a `.gguf` (e.g., a Hugging Face “resolve/main” URL). Many hosts allow partial download via HTTP Range.\n- Or choose a local GGUF file: Uses the browser’s File API; no upload leaves your machine.\n- Context size (tokens): Select a preset (e.g., 4K, 16K, 128K) or choose \"Custom…\" and enter any positive integer token length.\n- KV cache quantization: Choose how keys/values are stored in memory. Options show approximate bytes per value.\n- Verbose: Prints debug logs of what’s read and how size is determined.\n\nClick “Read URL” or “Read File”. If successful, you’ll see:\n- Extracted params: `attention_heads`, `kv_heads`, `hidden_layers`, `hidden_size`, `split_count` (if present)\n- Memory estimate: model size + KV cache size at your chosen context/quantization\n\n## How It Works\n\n- GGUF parsing: Reads just enough of the GGUF header to extract:\n  - `.attention.head_count`\n  - `.attention.head_count_kv`\n  - `.block_count`\n  - `.embedding_length`\n  - `split.count` (if present)\n- Remote file size:\n  - Tries `HEAD` to get `Content-Length`.\n  - Falls back to a `Range: bytes=0-0` request and reads `Content-Range`.\n- Sharded models:\n  - Detects `-00001-of-000NN` style patterns in URLs or uses `split.count` metadata.\n  - Sums sizes across parts (remote) or estimates total from a single shard (local) when possible.\n- KV cache estimate:\n  - Uses a simplified formula: `bytes_per_value × hidden_size × hidden_layers × context_tokens`.\n  - Shows total as: `Model + KV` (MB/GB). Actual usage can vary by backend/implementation.\n\n## Notes & Limitations\n\n- GGUF versions: Supports GGUF v1–v3 headers for the fields listed above.\n- CORS & Range: Remote hosts must allow cross-origin requests and HTTP Range. If not, size detection may fail; download the file and use the local option instead.\n- Range ignored: Some servers respond `200` without honoring `Range`. The app avoids downloading the full body for size only; estimates can fail in this case.\n- Sanity limits: Very long strings/arrays in metadata are bounded to avoid huge reads.\n- Estimates only: KV cache math is intentionally simplified. Different runtimes store KV differently (e.g., layout, precision, per-head factors).\n\n## Troubleshooting\n\n- “Failed to read params.”\n  - The file may not be GGUF or uses unsupported/unexpected metadata. Try another file or update the URL.\n- “Could not determine file size or compute usage (CORS/Range?).”\n  - The remote host may block CORS or not report size via `HEAD`/`Range`. Try serving the page locally, a different host, or the local file picker.\n- Split detection issues\n  - Ensure URLs use a stable pattern (e.g., `-00001-of-000xx`) or that `split.count` is present in metadata.\n\n## Privacy\n\n- The local file option never uploads your file; parsing happens entirely in your browser.\n- For remote URLs, the app performs small range requests to read the header and determine file size. It aborts early once required metadata is read.\n\n## Development\n\n- No build step required. The app is a single page:\n  - `index.html` — All logic and UI\n- Open in a browser or serve with any static server.\n\n## License\n\nThis project is licensed under the terms in `LICENSE`.\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-19T00:54:30.249979"
  },
  {
    "basic_info": {
      "name": "obex",
      "full_name": "dis0rder0x00/obex",
      "owner": "dis0rder0x00",
      "description": "Obex – Blocking unwanted DLLs in user mode",
      "url": "https://github.com/dis0rder0x00/obex",
      "clone_url": "https://github.com/dis0rder0x00/obex.git",
      "ssh_url": "git@github.com:dis0rder0x00/obex.git",
      "homepage": null,
      "created_at": "2025-09-18T15:43:10Z",
      "updated_at": "2025-09-19T00:42:39Z",
      "pushed_at": "2025-09-18T15:47:17Z"
    },
    "stats": {
      "stars": 22,
      "forks": 3,
      "watchers": 22,
      "open_issues": 0,
      "size": 234
    },
    "tech_info": {
      "language": "C",
      "languages": {
        "C": 14552
      },
      "license": "BSD 3-Clause \"New\" or \"Revised\" License",
      "topics": []
    },
    "content": {
      "readme": "# Obex - DLL Blocking\n\n**Obex** is a PoC tool/technique that can be used to prevent unwanted modules (e.g., EDR or monitoring libraries) from being loaded into a newly started process during process initialization or at runtime.\n\n## Features\n- Spawns any process with arguments under debug control.\n- Blocks a configurable list of DLLs by name.\n- Works both for startup DLLs and dynamically loaded DLLs (`LoadLibrary*`).\n- Written in plain C with no external dependencies.\n\n## Usage\n```\nobex.exe \"<command with args>\" [dll1.dll,dll2.dll,...]\n```\n- If no DLL list is provided, a default blocklist is used (at the time of writing just `amsi.dll`).\n- DLL names are case-insensitive.\n\n## How Does It Work?\nBesides parsing cli arguments the PoC does the following (in a rough overview):\n![](./images/flow.png)\n\nFor deeper understanding check code (obviously) or contact me on discord or [twitter](https://x.com/dis0rder_0x00).\n## Screenshot\nThe screenshot shows `obex` spawning `powershell.exe` with the default blocklist (only `amsi.dll`).\nAdditionally you can see the spawned process’s module list to verify that `amsi.dll` was not loaded.\n\n![](./images/1.png)\n\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-19T00:54:31.576790"
  },
  {
    "basic_info": {
      "name": "qb-phone-pro",
      "full_name": "QBCoreStore/qb-phone-pro",
      "owner": "QBCoreStore",
      "description": "QBCore Advanced Phone for FiveM 📱 Inspired by iPhoe. Sleek UI, customizable themes, GPS, banking, calling, social apps & QBCore integration, free for the community! ",
      "url": "https://github.com/QBCoreStore/qb-phone-pro",
      "clone_url": "https://github.com/QBCoreStore/qb-phone-pro.git",
      "ssh_url": "git@github.com:QBCoreStore/qb-phone-pro.git",
      "homepage": "https://fivem-qbcore.com/",
      "created_at": "2025-09-18T09:23:28Z",
      "updated_at": "2025-09-18T23:53:49Z",
      "pushed_at": "2025-09-18T12:03:21Z"
    },
    "stats": {
      "stars": 19,
      "forks": 0,
      "watchers": 19,
      "open_issues": 0,
      "size": 33649
    },
    "tech_info": {
      "language": null,
      "languages": {},
      "license": null,
      "topics": [
        "fivem",
        "iphone",
        "nopixel",
        "qb-phone",
        "qb-scripts",
        "qbcore",
        "qbcore-framework",
        "qbcore-iphone",
        "qbcore-script",
        "qbcore-scripts",
        "qbcore-ui",
        "qbcorestore"
      ]
    },
    "content": {
      "readme": "## ⚡ About Us  \nWe are the official **QBCore Store**, active since 2020.  \nWhile we release free scripts for the community, our main focus is **Premium Pre-Made All-in-One Servers**.  \nIf you want a **ready-to-use, optimized, and complete QBCore roleplay server** with 400+ premium resources, maps, UIs, and monthly updates – check us out below:  \n\n🌍 Website: [fivem-qbcore.com](https://fivem-qbcore.com)  \n💬 Discord: [discord.gg/qbcoreframework](https://discord.gg/qbcoreframework)  \n\n# 📱 qb-phone pro version\n**QBCore Advanced Phone for FiveM – Inspired by iPhone**  \n\nWelcome to the **QBCore Phone Pro Version** – a modern, feature-packed smartphone system built for **FiveM roleplay servers**.  \nThis phone is designed with **sleek UI, customizable themes, GPS, banking, calling, social apps, and seamless QBCore integration** – all free for the community!  \n\n---\n\n## ✨ Why Use This Phone?  \n- Full optimized design, works perfectly without bugs.  \n- Inspired by real modern smartphones for the **best RP experience**.  \n- Community-focused: we release **free scripts** regularly.  \n- **Open-source & customizable** for developers and server owners.  \n- Do not resell it. You Will Get a Copyright Strike.\n\n---\n\n## ⭐ Goal: 300+ Stars!  \nIf you enjoy this release, please support us by giving a ⭐ star on GitHub – it motivates us to release more high-quality scripts for the community!  \n\n---\n---\n\n## 📸 Preview  \n\n![Preview](https://files.fivemerr.com/images/08b49fe2-55ca-4553-b22a-9df31da5f714.png)  \n\n---\n\n🚀 Enhance your FiveM server today with **qb-phone pro** – Free for all, Premium for those who want the next level!  \n",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-19T00:54:32.907180"
  },
  {
    "basic_info": {
      "name": "contracts",
      "full_name": "forum-online-protocol/contracts",
      "owner": "forum-online-protocol",
      "description": null,
      "url": "https://github.com/forum-online-protocol/contracts",
      "clone_url": "https://github.com/forum-online-protocol/contracts.git",
      "ssh_url": "git@github.com:forum-online-protocol/contracts.git",
      "homepage": null,
      "created_at": "2025-09-18T06:42:33Z",
      "updated_at": "2025-09-18T17:53:56Z",
      "pushed_at": "2025-09-18T15:26:15Z"
    },
    "stats": {
      "stars": 18,
      "forks": 0,
      "watchers": 18,
      "open_issues": 0,
      "size": 42
    },
    "tech_info": {
      "language": "Solidity",
      "languages": {
        "Solidity": 77818,
        "JavaScript": 46947,
        "Circom": 772
      },
      "license": null,
      "topics": []
    },
    "content": {
      "readme": "# Votta E-Voting System\n\n![Votta](https://via.placeholder.com/150?text=Votta)\n\nA secure, transparent, and decentralized electronic voting system built on blockchain technology with zero-knowledge proofs.\n\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n\n## Table of Contents\n\n- [Overview](#overview)\n- [System Architecture](#system-architecture)\n- [Smart Contracts](#smart-contracts)\n- [Off-Chain Services](#off-chain-services)\n- [Installation](#installation)\n- [Usage Guide](#usage-guide)\n  - [Running the System](#running-the-system)\n  - [Configuration](#configuration)\n  - [Monitoring](#monitoring)\n- [Development](#development)\n  - [Testing](#testing)\n  - [Deployment](#deployment)\n- [Security](#security)\n- [FAQ](#faq)\n- [Contributing](#contributing)\n- [License](#license)\n\n## Overview\n\nVotta is a next-generation e-voting system designed to provide unparalleled security, transparency, and user privacy. Built on blockchain technology with zero-knowledge proofs, it enables secure and auditable voting while maintaining voter anonymity.\n\n### Key Features\n\n- **Multiple Voting Polls**: Support for running multiple elections simultaneously\n- **Credential-Based Voting**: Secure voter registration with cryptographic credentials\n- **Batch Processing**: Efficient aggregation of votes into verifiable batches\n- **Zero-Knowledge Proofs**: Privacy-preserving vote verification\n- **Challenge Mechanism**: Security monitoring and fraud detection\n- **Gas-Optimized**: Subsidized transaction costs for voters\n- **Decentralized Governance**: Protocol-level management of system components\n\n## System Architecture\n\nThe Votta e-voting system consists of several interconnected components:\n\n\n1. **Smart Contracts**: Core blockchain components handling voting logic, credential management, and batch processing\n2. **Aggregator Service**: Off-chain service that collects voting receipts and submits them as batches\n3. **Watch-Tower Service**: Security monitoring service that challenges fraudulent batches\n4. **ZK-Prover**: Zero-knowledge proof generation and verification service\n5. **User Interface**: Web and mobile interfaces for voter interaction\n\n## Smart Contracts\n\nThe system includes the following smart contracts:\n\n- **VottaProtocol**: Central management contract that coordinates all other components\n- **CredentialRegistry**: Manages voter credentials and verifies eligibility\n- **VotingBatch**: Processes batched votes and handles challenges\n- **PenaltyVault**: Holds bonds and manages slashing for malicious actors\n- **VotingFactory**: Creates and manages voting polls\n- **VotingPaymaster**: Covers gas costs for voting operations\n- **PlonkVerifier**: Verifies zero-knowledge proofs for batched votes\n- **AAValidate**: Validates user operations using account abstraction\n\n## Off-Chain Services\n\n### Aggregator Service\n\nThe Aggregator Service, written in Rust, is responsible for:\n\n- Collecting voting receipts from users\n- Building Merkle trees of receipt hashes\n- Generating zero-knowledge proofs\n- Submitting batches to the VotingBatch contract\n\n### Watch-Tower Service\n\nThe Watch-Tower Service, written in Go, performs security monitoring:\n\n- Monitors chain events for new batch submissions\n- Verifies batch validity and detects fraud\n- Challenges fraudulent batches with evidence\n- Acts as a security backstop for the system\n\n### ZK-Prover Integration\n\nThe ZK-Prover component:\n\n- Generates zero-knowledge proofs for vote batches\n- Ensures vote counts match the claimed receipts\n- Preserves privacy while enabling verification\n\n## Installation\n\n### Prerequisites\n\n- Node.js v16+ and npm\n- Go v1.18+\n- Rust v1.65+\n- Solidity v0.8.24+\n- Hardhat\n\n### Installing Dependencies\n\n#### Smart Contracts\n\n```bash\n# Install JavaScript dependencies\nnpm install\n\n# Install Solidity dependencies\nnpm install @openzeppelin/contracts @openzeppelin/contracts-upgradeable\n```\n\n#### Watch-Tower Service\n\n```bash\ncd services/watchtower\ngo mod download\n```\n\n#### Aggregator Service\n\n```bash\ncd services/aggregator\ncargo build --release\n```\n\n## Usage Guide\n\n### Running the System\n\n#### Option 1: Native Execution\n\n1. **Start Local Blockchain**\n\n```bash\nnpx hardhat node\n```\n\n2. **Deploy Smart Contracts**\n\n```bash\nnpx hardhat run scripts/deploy.js --network localhost\n```\n\n3. **Start Aggregator Service**\n\n```bash\ncd services/aggregator\ncargo run --release -- --config config.toml\n```\n\n4. **Start Watch-Tower Service**\n\n```bash\ncd services/watchtower\ngo run main.go\n```\n\n#### Option 2: Docker Containers\n\nWe provide Docker containers for both the Aggregator and Watch-Tower services to simplify deployment and ensure consistent environments.\n\n1. **Build and Start Services with Docker Compose**\n\n```bash\n# From the project root directory\ndocker-compose up -d\n```\n\nThis will start both services as defined in the `docker-compose.yml` file.\n\n2. **View Service Logs**\n\n```bash\n# View logs for all services\ndocker-compose logs -f\n\n# View logs for a specifi",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-19T00:54:34.244407"
  },
  {
    "basic_info": {
      "name": "InternVLA-A1",
      "full_name": "InternRobotics/InternVLA-A1",
      "owner": "InternRobotics",
      "description": "InternVLA-A1: Unifying Understanding, Generation, and Action for Robotic Manipulation​",
      "url": "https://github.com/InternRobotics/InternVLA-A1",
      "clone_url": "https://github.com/InternRobotics/InternVLA-A1.git",
      "ssh_url": "git@github.com:InternRobotics/InternVLA-A1.git",
      "homepage": null,
      "created_at": "2025-09-18T01:00:55Z",
      "updated_at": "2025-09-18T15:59:56Z",
      "pushed_at": "2025-09-18T10:49:23Z"
    },
    "stats": {
      "stars": 17,
      "forks": 0,
      "watchers": 17,
      "open_issues": 0,
      "size": 437
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 517220
      },
      "license": null,
      "topics": []
    },
    "content": {
      "readme": "<div align=\"center\">\n\n# InternVLA-A1: Unifying Understanding, Generation, and Action for Robotic Manipulation​\n\n</div>\n\n---\n\nInternVLA-A1 is an end-to-end vision–language–action (VLA) framework unifing understanding, generation ,and action for robotic manipulation. It leverages predictive imagination of task evolution to guide execution, enabling enhanced manipulation in highly dynamic environments. \n\n## :fire: Highlights <a name=\"high\"></a>\n<img width=\"1000\" alt=\"seer\" src=\"assets/internvla_a1_framework.jpg\">\n\n- **Novel Model Archituecture**: A Mixture-of-Transformers architecture for unified understanding, generation, and action.\n- **Hybrid Synthetic-Real Data Corpus**: A hybrid synthetic-real manipulation dataset [InternData-A1](https://huggingface.co/datasets/InternRobotics/InternData-A1), integrating 5 heterogeneous robots, 15 skills, and 200+ scenes, emphasizing multi-robot collaboration under dynamic scenarios.\n- **Impressive Real-World performance**: InternVLA-A1 demonstrates strong effectiveness and generalization in highly dynamic scenarios involving dynamic grasping of conveyor belts and multi-robot collaboration.\n\n### 🏆 **Unified Understanding-Generation-Action Family**\n\n- **F1-VLA** (F1 is a prequel version of InternVLA-A1): [Paper](https://arxiv.org/abs/2509.06951) | [Code](https://github.com/InternRobotics/F1-VLA) | [Model](https://huggingface.co/InternRobotics/F1-VLA)\n- **InternVLA-A1**: [Code](https://github.com/InternRobotics/InternVLA-A1) | [Paper/Model (Scheduled for late September release)]()\n\n## 🤖 Real-World Robot Demonstrations\n\n### **Package grabbing and flipping in conveyor belt**\n<div align=\"center\">\n    <video src=\"https://github.com/user-attachments/assets/c7d8989c-be14-428e-b498-d02dc1fc1475\"\n         controls autoplay muted playsinline loop width=\"720\"></video>\n  <p><em>The model handles dynamically shaped packages on conveyor belts, tracking and predicting their trajectories in real-time to achieve high-speed stable grasping, while adaptively flipping packages and identifying express information from delivery notes.</em></p>\n</div>\n\n\n### **Multi-robot collaboration on long-horizon tasks in dynamic environments**\n<div align=\"center\">\n      <video src=\"https://github.com/user-attachments/assets/c438ff8a-4536-45b3-9117-e210c36ba8a0\"\n         controls autoplay muted playsinline loop width=\"720\"></video>\n  <p><em>The model swiftly identifies, locates, and grips high-speed ingredients based on task demands, showcasing its adaptability in complex environments.</em></p>\n</div>\n\n\n## 🚀 Quick Start\n\n### **Prerequisites**\n- Python ≥ 3.10\n- torch ≥ 2.6.0\n- CUDA ≥ 12.4\n\n### **Installation**\n```bash\n# Clone repository\ngit clone https://github.com/InternRobotics/InternVLA-A1.git\n\n# Create environment\nconda create -f internvla_a1 python==3.10\nconda activate internvla_a1\n\n# Install dependencies\npip install torch==2.6.0 torchvision==0.21.0 torchaudio==2.6.0 torchcodec==0.2.1 --index-url https://download.pytorch.org/whl/cu124\n\n# install other requirements\npip install -r requirements.txt\n\npip install numpy==1.26.4\n```\n\n## 📄 License\n\nThis project is licensed under the MIT License.\n\n## 🙏 Acknowledgments\n\n- [Lerobot](https://github.com/huggingface/lerobot)\n- [InternVL](https://github.com/OpenGVLab/InternVL)\n- [COSMOS](https://github.com/nvidia-cosmos)\n- [Any4lerobot](https://github.com/Tavish9/any4lerobot/)\n- [VAR](https://github.com/FoundationVision/VAR)\n",
      "default_branch": "master"
    },
    "fetched_at": "2025-09-19T00:54:35.565657"
  },
  {
    "basic_info": {
      "name": "Kali_Linux_MCP",
      "full_name": "i3T4AN/Kali_Linux_MCP",
      "owner": "i3T4AN",
      "description": "Two-component system bridging Kali Linux penetration testing tools with AI agents via MCP. Flask API server executes 10+ security tools (Nmap, SQLMap, Metasploit, etc.) while MCP client provides seamless AI integration for automated security workflows.",
      "url": "https://github.com/i3T4AN/Kali_Linux_MCP",
      "clone_url": "https://github.com/i3T4AN/Kali_Linux_MCP.git",
      "ssh_url": "git@github.com:i3T4AN/Kali_Linux_MCP.git",
      "homepage": "",
      "created_at": "2025-09-18T13:49:53Z",
      "updated_at": "2025-09-18T22:48:49Z",
      "pushed_at": "2025-09-18T17:31:10Z"
    },
    "stats": {
      "stars": 17,
      "forks": 0,
      "watchers": 17,
      "open_issues": 0,
      "size": 13
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 23338
      },
      "license": "MIT License",
      "topics": [
        "kali-linux",
        "mcp",
        "mcp-server",
        "pentesting",
        "security"
      ]
    },
    "content": {
      "readme": "# Kali_Linux_MCP\n\n## Overview\nKali_Linux_MCP exposes Kali tools through:\n- **Kali_Linux_Server.py**: Flask API wrapping tools like `nmap`, `gobuster`, `nikto`, `sqlmap`, `metasploit`, `hydra`, `john`, `wpscan`, `enum4linux`.  \n- **MCP_Server.py**: MCP bridge using FastMCP, forwarding requests from MCP clients to the API.\n\nUse it for **authorized labs, CTFs, HTB/THM machines**, or AI-assisted testing via MCP clients (Claude Desktop, 5ire, etc.).\n\n---\n\n## Requirements\n- Kali Linux (or Linux with tools installed in PATH).  \n- Python 3 with `flask`, `requests`, `mcp`.  \n- Install:  \n  pip install flask requests mcp\n\n---\n\n## Run\n\n1. Start API:  \n   python3 Kali_Linux_Server.py --port 5000  \n\n2. Health check:  \n   curl http://localhost:5000/health  \n\n3. Start MCP bridge:  \n   python3 MCP_Server.py --server http://localhost:5000 --timeout 300  \n\n---\n\n## API Endpoints\n- GET `/health` — tool status.  \n- POST `/api/command` — run any command.  \n- POST `/api/tools/<tool>` — wrappers for nmap, gobuster, dirb, nikto, sqlmap, metasploit, hydra, john, wpscan, enum4linux.  \n\nEach requires JSON body with tool-specific args (`target`, `url`, etc.).\n\n---\n\n## MCP Tools\nBridge registers MCP tools: `nmap_scan`, `gobuster_scan`, `dirb_scan`, `nikto_scan`, `sqlmap_scan`, `metasploit_run`, `hydra_attack`, `john_crack`, `wpscan_analyze`, `enum4linux_scan`, plus `execute_command` and `check_health`.\n\n---\n\n## Example Usage\n- Nmap:  \n  curl -X POST http://localhost:5000/api/tools/nmap -H \"Content-Type: application/json\" -d '{\"target\":\"scanme.nmap.org\",\"additional_args\":\"-sV\"}'  \n\n- WPScan:  \n  curl -X POST http://localhost:5000/api/tools/wpscan -H \"Content-Type: application/json\" -d '{\"url\":\"https://example.com\",\"additional_args\":\"--enumerate u\"}'  \n\n- MCP Client:  \n  Add MCP config pointing `python3 /path/to/MCP_Server.py --server http://LINUX_IP:5000`.\n\n---\n\n## Scenarios\n- Recon: `nmap_scan` to map services.  \n- Web enum: `gobuster_scan` or `dirb_scan`.  \n- Vuln triage: `nikto_scan`, `sqlmap_scan`.  \n- WordPress checks: `wpscan_analyze`.  \n- SMB recon: `enum4linux_scan`.  \n- Credential tests (lab only): `hydra_attack`.\n- Exploit check: `metasploit_run`.  \n\n---\n\n## Notes\n- Supports AI-assisted workflows: models suggest and run commands.  \n- Works with Claude Desktop, 5ire MCP clients.  \n- Extendable: other forensic tools (Volatility, SleuthKit) possible.  \n\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-19T00:54:36.878856"
  },
  {
    "basic_info": {
      "name": "Student-Success-Analysis",
      "full_name": "ankitsharma-tech/Student-Success-Analysis",
      "owner": "ankitsharma-tech",
      "description": "Student success analysis – statisitical data analyisis AI project",
      "url": "https://github.com/ankitsharma-tech/Student-Success-Analysis",
      "clone_url": "https://github.com/ankitsharma-tech/Student-Success-Analysis.git",
      "ssh_url": "git@github.com:ankitsharma-tech/Student-Success-Analysis.git",
      "homepage": "",
      "created_at": "2025-09-18T17:49:39Z",
      "updated_at": "2025-09-18T18:16:26Z",
      "pushed_at": "2025-09-18T17:53:18Z"
    },
    "stats": {
      "stars": 15,
      "forks": 0,
      "watchers": 15,
      "open_issues": 0,
      "size": 12094
    },
    "tech_info": {
      "language": "HTML",
      "languages": {
        "HTML": 641646
      },
      "license": "MIT License",
      "topics": [
        "ai",
        "artificial-intelligence",
        "r",
        "statistical-analysis",
        "statistical-data-analysis",
        "statistics"
      ]
    },
    "content": {
      "readme": "# 🎓 Student Success Analysis\n\n## Final report: [report.pdf](./report.pdf)\n\n## 📖 Project Overview\n\nThe main goal of this project was to create a comprehensive report explaining concepts of **statistical data analysis** applied to an existing dataset.\n \n- The choice of statistical methods was flexible, as long as they were relevant and covered in the course curriculum.\n- The report included test cases, either from the recommended list provided by faculty or created by team members.\n- **R language** was used for data analysis and report generation.\n\nProject evaluation was based on:\n\n1. **Report quality**\n2. **Oral examination** testing knowledge of theoretical concepts (e.g., when to use a test, test assumptions, and method details).\n\n---\n\n## 📊 Dataset\n\nThe dataset consists of survey responses and student grades in **mathematics** and **Portuguese** from two high schools.\n\nCollecting this type of data is essential for analyzing and improving the quality of the education system.  \nMore details: [pdfs/dataset_documentation.pdf](./pdfs/dataset_documentation.pdf)\n\n---\n\n## 📂 Directory Structure\n\n| Directory                     | Description                                        |\n| ----------------------------- | -------------------------------------------------- |\n| [auditorne](./auditorne/)     | Reference to existing problems and their solutions |\n| [cheatsheets](./cheatsheets/) | Tidyverse cheat sheets in PDF format               |\n| [pdfs](./pdfs/)               | Dataset and project descriptions                   |\n| [src](./src/)                 | R Markdown source files and dataset                |\n\n---\n\n## ⚙️ Installation\n\n### Windows\n\n- Install [RStudio](https://www.rstudio.com/products/rstudio/download/#download)\n- Install [R](https://cran.r-project.org/bin/windows/base/)\n\n### Linux\n\n- Install [RStudio](https://www.rstudio.com/products/rstudio/download/#download)\n- Install R and tidyverse dependencies:\n  ```bash\n  sudo apt-get install r-cran-curl r-cran-openssl r-cran-xml2 libxml2-dev\n  ```\n\n## 📦 R Packages Setup\n\n1. Open **RStudio** → Open Project → `student-success-analysis.Rproj`\n2. The file `student-success-analysis.Rmd` should open automatically\n   - If not, navigate to it in the **Files** panel and double-click\n3. Run the first code chunk (`Ctrl + Shift + Enter`) containing the `library` functions\n4. A popup will prompt to install required packages → click **Yes**\n5. Installation may take ~20 minutes\n\n---\n\n## 📝 Notes\n\n**KS Test:**\n\n- If _p = 1_ → data surely come from the same distribution\n- If _p = 0_ → data come from different distributions\n\n## 📋 To-Do\n\n- [ ] Spellcheck the report\n- [x] Write introduction to the problem\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-19T00:54:38.189526"
  },
  {
    "basic_info": {
      "name": "Time-Management-Tool",
      "full_name": "ankitsharma-tech/Time-Management-Tool",
      "owner": "ankitsharma-tech",
      "description": "An AI-powered time management tool built with Python to help users plan, prioritize, and optimize their daily tasks efficiently.",
      "url": "https://github.com/ankitsharma-tech/Time-Management-Tool",
      "clone_url": "https://github.com/ankitsharma-tech/Time-Management-Tool.git",
      "ssh_url": "git@github.com:ankitsharma-tech/Time-Management-Tool.git",
      "homepage": "",
      "created_at": "2025-09-18T17:44:29Z",
      "updated_at": "2025-09-18T18:16:28Z",
      "pushed_at": "2025-09-18T17:47:26Z"
    },
    "stats": {
      "stars": 15,
      "forks": 0,
      "watchers": 15,
      "open_issues": 0,
      "size": 498
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 7968,
        "Batchfile": 54
      },
      "license": "MIT License",
      "topics": [
        "ai-project",
        "ml",
        "ml-project",
        "project",
        "python",
        "python-project",
        "python3",
        "timemanagement"
      ]
    },
    "content": {
      "readme": "# Time Management Assistant Tool\n\nWelcome to the ultimate tool designed to streamline your tasks and manage your time efficiently. With its sleek, futuristic interface and cutting-edge functionality, TMA is here to revolutionize how you stay organized.\n\n## 🚀 Setup\n\n**Step 1: Clone the Repository**\n\n```bash\ngit clone https://github.com/ankitsharma-tech/Time-Management-Tool.git\n```\n\n**Step 2: Navigate to the Directory**\n\n```bash\ncd Time-Management-Tool\n```\n\n**Step 3: Navigate to the Directory**\n\n```bash\npip install -r requirements.txt\n```\n\n**Step 4: Open the Configuration File**\n\n- Open the `.bat` file in Notepad or your preferred text editor.\n\n**Step 5: Insert Your Main Script Path**\n\n- Copy the path to your `main.py` file.\n\n**Step 6: Update the BAT File**\n\n- Paste the copied path into the second line of the `.bat` file.\n\n## 🌟 Usage\n\nTo start managing your time like a pro:\n\n- Simply double-click the `.bat` file to launch the tool.\n- If you wan't to sheel the list use command\n\n```\nShow\n```\n\ninside the TMA - Tool\n\n- for Example Command type any thing wrong\n\n```\n                        ______ Available commands _____\n                      1. show - to show the Schedule file\n                      2. tell me - try like \"tell me to sleep at 09:00pm\"\n```\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-19T00:54:39.563274"
  },
  {
    "basic_info": {
      "name": "Student-Performance-Prediction",
      "full_name": "ankitsharma-tech/Student-Performance-Prediction",
      "owner": "ankitsharma-tech",
      "description": "This is a simple machine learning project using classifiers for predicting factors which affect student grades, using data from CSV file.",
      "url": "https://github.com/ankitsharma-tech/Student-Performance-Prediction",
      "clone_url": "https://github.com/ankitsharma-tech/Student-Performance-Prediction.git",
      "ssh_url": "git@github.com:ankitsharma-tech/Student-Performance-Prediction.git",
      "homepage": "",
      "created_at": "2025-09-18T17:48:17Z",
      "updated_at": "2025-09-18T18:16:27Z",
      "pushed_at": "2025-09-18T17:48:55Z"
    },
    "stats": {
      "stars": 15,
      "forks": 0,
      "watchers": 15,
      "open_issues": 0,
      "size": 10
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 10521
      },
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "# 🎓 Student Performance Prediction\nThis project demonstrates a simple machine learning approach to predict factors that influence student grades using a dataset stored in a CSV file.\n\n## 📊 Project Overview\n\nThe dataset contains information about students from different nationalities and grade levels, along with key determining factors such as:\n\n- Number of hands raised\n- Number of attendances\n- Hours studied\n- And more\n\nThe goal of this project is to analyze these factors and predict their impact on student performance.\n\n## 🧠 Machine Learning Models\n\nSeveral classifiers and machine learning models have been implemented and compared to achieve the most accurate predictions of the factors affecting student marks.\n\n## 📈 Visualizations\n\nTo better understand the results, visual aids have been generated, including:\n\n- Graphs for data insights\n- Confusion matrices for model evaluation\n\n---\n\nThis project provides a hands-on demonstration of applying machine learning techniques to an educational dataset, highlighting the relationship between study habits, engagement, and student success.\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-19T00:54:40.886012"
  },
  {
    "basic_info": {
      "name": "AI-Object-Detection",
      "full_name": "ankitsharma-tech/AI-Object-Detection",
      "owner": "ankitsharma-tech",
      "description": "A web AI object detection",
      "url": "https://github.com/ankitsharma-tech/AI-Object-Detection",
      "clone_url": "https://github.com/ankitsharma-tech/AI-Object-Detection.git",
      "ssh_url": "git@github.com:ankitsharma-tech/AI-Object-Detection.git",
      "homepage": "https://woody.pizza/tensorflow/object-detection/",
      "created_at": "2025-09-18T17:38:04Z",
      "updated_at": "2025-09-18T18:16:29Z",
      "pushed_at": "2025-09-18T17:42:09Z"
    },
    "stats": {
      "stars": 15,
      "forks": 0,
      "watchers": 15,
      "open_issues": 0,
      "size": 4
    },
    "tech_info": {
      "language": "JavaScript",
      "languages": {
        "JavaScript": 3145,
        "HTML": 1575,
        "CSS": 175
      },
      "license": "MIT License",
      "topics": [
        "ai",
        "artificial-intelligence",
        "artificial-intelligence-projects",
        "camera",
        "camera-detect",
        "css",
        "detection",
        "html",
        "javascript",
        "javascript-ai",
        "javascript-artificial-intelligence",
        "javascript-tensorflow",
        "ml5js",
        "object-detection",
        "object-detector",
        "tensorflow",
        "web"
      ]
    },
    "content": {
      "readme": "# 🤖 AI Object Detection\n\n## 👋 About this Project\nThis is a web-based AI object detection application that runs directly in your browser. It uses your device’s camera to detect objects in real time, making it easy and accessible without requiring any installation.\n\n---\n\n## ⚙️ Features\n- ✅ Toggle switch to enable or disable AI detection  \n- ✅ Range slider to control frame rate  \n- ✅ Real-time object detection through your camera  \n\n---\n\n## 🖼️ Preview\n<a href=\"https://ibb.co/JCNgfJr\"><img src=\"https://i.ibb.co/3kwQDZS/preview-combined.jpg\" alt=\"preview-combined\" width=\"100%\"></a>\n\n---\n\n## 💪 Try It\nNot convinced yet? Try it out yourself here:  \n👉 [Live Demo](https://woody.pizza/tensorflow/object-detection/)\n\n---\n\n## 🌐 Browser Support\nThe app works on most modern browsers. Below are the tested ones:\n\n### Desktop\n| Browser           | Supported |\n|-------------------|:---------:|\n| Firefox           | ✅ |\n| Chrome            | ✅ |\n| Edge              | ✅ |\n| Internet Explorer | ❌ |\n\n### Mobile\n| Browser  | Supported |\n|----------|:---------:|\n| Firefox  | ✅ |\n| Chrome   | ✅ |\n\n---\n\n## ✌️ Credits\n- [Materialize](https://materializecss.com/) – for UI components  \n- [ml5.js](https://ml5js.org/) – for machine learning in the browser  \n",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-19T00:54:42.190622"
  },
  {
    "basic_info": {
      "name": "Book-video-generate",
      "full_name": "SheenHalo/Book-video-generate",
      "owner": "SheenHalo",
      "description": "一个自动化书籍推广视频生成工具，可以根据书名自动生成带有配音和字幕的短视频。",
      "url": "https://github.com/SheenHalo/Book-video-generate",
      "clone_url": "https://github.com/SheenHalo/Book-video-generate.git",
      "ssh_url": "git@github.com:SheenHalo/Book-video-generate.git",
      "homepage": null,
      "created_at": "2025-09-18T07:37:45Z",
      "updated_at": "2025-09-18T17:41:18Z",
      "pushed_at": "2025-09-18T08:16:20Z"
    },
    "stats": {
      "stars": 14,
      "forks": 6,
      "watchers": 14,
      "open_issues": 0,
      "size": 50414
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 43289
      },
      "license": null,
      "topics": []
    },
    "content": {
      "readme": "# 📚 Book Video Generator\n\n[![Python](https://img.shields.io/badge/Python-3.7%2B-blue.svg)](https://www.python.org/)\n[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)\n[![FFmpeg](https://img.shields.io/badge/FFmpeg-required-red.svg)](https://ffmpeg.org/)\n\n一个自动化书籍推广视频生成工具，可以根据书名自动生成带有配音和字幕的短视频。\n\n## 🖼️ 效果预览\n\n[示例视频](https://github.com/user-attachments/assets/385a804c-904a-4aae-a595-58f9240a66b9)\n\n\n### 视频特性\n生成的视频包含：\n- 🎬 **动态封面展示效果** - 书籍封面滑动动画，4秒片头效果，书籍封面在`resource/covers/`中随机获取\n- 🖼️ **背景图片自动切换** - 每10秒切换背景，营造氛围，背景图片随机从`resource/backgrounds/`中获取\n- 📝 **同步字幕显示** - 根据音频时长精准同步，底部居中显示\n- 🎵 **多音轨混合** - 配音 + 背景音乐 + 音效，音量自动平衡，背景音乐随机从`resource/bgm/`中获取\n\n### 使用建议\n1. **首次使用**: 建议先下载示例视频查看效果\n2. **测试运行**: 使用简单的书籍名称进行测试\n3. **参数调整**: 根据需要调整视频参数和语音选择\n\n## 📋 系统要求\n\n- **Python**: 3.7+\n- **FFmpeg**: 必须安装并添加到系统PATH\n- **操作系统**: Windows / macOS / Linux\n\n## 🚀 快速开始\n\n### 1. 克隆项目\n\n```bash\ngit clone https://github.com/SheenHalo/Book-video-generate.git\ncd Book-video-generate\n```\n\n### 2. 创建虚拟环境\n\n**Windows:**\n```bash\npython -m venv venv\nvenv\\Scripts\\activate\n```\n\n**macOS/Linux:**\n```bash\npython3 -m venv venv\nsource venv/bin/activate\n```\n\n### 3. 安装依赖\n\n```bash\npip install -r requirements.txt\n```\n\n### 4. 检查FFmpeg\n\n项目依赖FFmpeg进行视频合成，请确保已正确安装：\n\n```bash\npython video_processor.py\n```\n\n如果显示\"ffmpeg 可用\"，则安装成功。如果显示\"ffmpeg 不可用\"，请按以下步骤安装：\n\n#### Windows FFmpeg安装\n1. 下载FFmpeg: https://ffmpeg.org/download.html\n2. 解压到 `C:\\ffmpeg`\n3. 添加 `C:\\ffmpeg\\bin` 到系统PATH环境变量\n4. 重启命令行并验证：`ffmpeg -version`\n\n#### macOS FFmpeg安装\n```bash\nbrew install ffmpeg\n```\n\n#### Linux FFmpeg安装\n```bash\n# Ubuntu/Debian\nsudo apt update && sudo apt install ffmpeg\n\n# CentOS/RHEL/Fedora\nsudo yum install ffmpeg\n```\n\n### 5. 配置LLM API\n提供了一个免费的LLM API接口。如果失效了，请自行配置。\n编辑 `llm.py` 文件，配置你的LLM API信息：\n\n```python\n# 在LLMClient类中修改\nself.api_url = \"你的API地址\"\nself.api_key = \"你的API密钥\"\n```\n\n### 6. 准备资源文件\n\n确保以下目录包含必要的文件：\n\n```\nresource/\n├── backgrounds/    # 背景图片 (jpg/png)\n├── bgm/           # 背景音乐 (mp3)\n├── covers/        # 书籍封面存储位置\n├── effects/       # 音效文件 (mp3)\n└── fonts/         # 字体文件 (包含msyh.ttc)\n```\n\n### 7. 运行程序\n\n```bash\npython main.py\n```\n\n按照提示输入书名，程序将自动生成视频：\n\n```\n请输入书名: 巴别塔\n正在获取书籍信息...\n正在生成文案...\n正在生成语音...\n正在生成视频...\n开始合成音视频...\n最终视频已保存到: appdata/巴别塔/final_video.mp4\n```\n\n## 🛠️ 高级配置\n\n### 修改语音类型\n\n在 `main.py` 中修改语音选择：\n\n```python\n# 查看所有可用语音\nprint(voice_dict.keys())\n\n# 选择特定语音\nvoice = voice_dict.get(\"晓秋-女\")\n```\n\n### 自定义视频参数\n\n在 `app.py` 的 `make_movie` 函数中可以调整：\n- 屏幕尺寸\n- 动画时长\n- 音量大小\n- 背景切换时间\n\n### 支持的语音列表\n\n项目支持43种中文语音变体：\n\n| 语音名称 | 类型 | 特点 |\n|---------|------|------|\n| 晓晓（标准）-女 | 标准 | 温暖，全面，生动 |\n| 晓辰（标准）-女 | 标准 | 友好，休闲，乐观 |\n| 云峰-男 | 标准 | 自信，生动，情感 |\n| 晓晓（多语言）-女 | 多语言 | 温暖，生动，明亮 |\n| 晓通（吴语）-女 | 方言 | 温暖，友好，舒缓 |\n| 晓敏（粤语）-女 | 方言 | 明亮，清晰，自信 |\n| ...更多语音详见代码 | | |\n\n## 📁 项目结构\n\n```\nBook-video-generate/\n├── main.py              # 主入口文件\n├── app.py               # 视频生成核心\n├── spider.py            # 豆瓣爬虫\n├── llm.py               # LLM客户端\n├── tts_generator.py     # TTS生成器\n├── video_processor.py   # 视频处理工具\n├── requirements.txt     # 依赖列表\n├── appdata/            # 生成的文件\n└── resource/           # 资源文件\n```\n\n## 📄 许可证\n\n本项目采用MIT许可证 - 详见 [LICENSE](LICENSE) 文件\n\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-19T00:54:43.513344"
  },
  {
    "basic_info": {
      "name": "cdrgoat",
      "full_name": "stream-sec/cdrgoat",
      "owner": "stream-sec",
      "description": "Cloud Detection & Response GOAT is a scenario-driven, intentionally vulnerable framework designed to help defenders validate detection pipelines, practice SOC workflows and train analysts on realistic cloud attack paths - all in a safe, reproducible environment with no impact on production.",
      "url": "https://github.com/stream-sec/cdrgoat",
      "clone_url": "https://github.com/stream-sec/cdrgoat.git",
      "ssh_url": "git@github.com:stream-sec/cdrgoat.git",
      "homepage": null,
      "created_at": "2025-09-18T07:52:48Z",
      "updated_at": "2025-09-18T13:03:46Z",
      "pushed_at": "2025-09-18T07:55:01Z"
    },
    "stats": {
      "stars": 13,
      "forks": 0,
      "watchers": 13,
      "open_issues": 0,
      "size": 1396
    },
    "tech_info": {
      "language": "Shell",
      "languages": {
        "Shell": 131148,
        "HCL": 42540
      },
      "license": "BSD 3-Clause \"New\" or \"Revised\" License",
      "topics": []
    },
    "content": {
      "readme": "![CDRGoat](./assets/CDRGoat.png)\n\n\nCloud adoption has reshaped the enterprise attack surface, where adversaries can chain misconfigurations, excessive permissions, and runtime blind spots into full compromises.  \n**Cloud Detection & Response GOAT** is a scenario-driven, intentionally vulnerable framework designed to help defenders validate detection pipelines, practice SOC workflows and train analysts on realistic cloud attack paths - all in a safe, reproducible environment with no impact on production.  \n\nCDR GOAT enables:\n- **Advanced simulations** - Misconfigurations combined with live attacker behavior (privilege escalation, credential theft, lateral movement).  \n- **Detection & response validation** - Confirm alerts fire, understand context, and rehearse investigation workflows under realistic pressure.  \n- **SOC readiness** - Train analysts on real signals instead of abstract examples.  \n- **Purple teaming** - Run adversary emulation while measuring blue team effectiveness in real time.  \n\n&nbsp;\n\n## ⚠️ Warning\n- Do **not** deploy to production\n- Use only isolated sandbox/test accounts\n- Expect cloud usage costs while resources are running\n- Always destroy resources after finishing a scenario\n\n&nbsp;\n\n## ✨ Features\n- **Scenario‑driven attack paths** - Reproducible simulations of real‑world adversary tactics in cloud environments (IAM abuse, SSRF, privilege escalation, data exfiltration, etc.).\n- **Safe to run** - Resources are provisioned in isolated test accounts with minimal blast radius.\n- **Automated Attack Script** - A fully automated script to execute attacks end-to-end, reducing manual steps and ensuring repeatable outcomes.\n\n&nbsp;\n\n## 🚀 Getting Started\n\n#### 🧩 Prerequisites\n- Terraform ver. 1.5 or above\n- AWS account (sandbox recommended, do not run in production)  \n- AWS CLI configured with appropriate credentials  \n- jq utility for parsing JSON output  \n\n#### ⚙️ Install Dependencies\nmacOS\n```bash\nbrew install terraform awscli jq\n```\nLinux\n```bash\nsudo apt update && sudo apt install -y terraform awscli jq\n```\n\n#### 🗂️ Simulation Scenarios\nThe simulation scenarios are organized by folder under `scenarios/`.  \nEach folder includes:  \n- A **Terraform plan** to provision the environment for the scenario.  \n- An **attack script** that automates the attack path, allowing defenders to focus on detection and response.  \n\nNavigate into a scenario folder to run Terraform and execute the attack script as described below.\n\n#### 🏗️ Deploy\nBefore deploying, download the provided Terraform configuration and attack script to the machine where you will run the attack steps.\n\nUse the provided Terraform configuration to deploy the full lab environment.\n\nAt the end of the deployment Terraform will display output values (such as the public IP of the target instance). Save these details, you will need them when running the attack script.\nSome of this information might be sensitive and thus reducted by Terraform.\nIn order to reveal specific output we can use `terraform output` command.\n\nFor example to get output value of `leaked_user_secret_access_key` we can execute following:\n\n```bash\nterraform output leaked_user_access_key_id\n```\n\n⚠️ When a scenario’s initial step targets a public IP, add the public IP (or CIDR) of the machine that will run the attack script to the environment whitelist via terraform apply so the script can reach the target and complete any required interactions. See example\n\n```bash\nterraform init\nterraform apply -var='attack_whitelist=[\"87.68.140.7/32\",\"203.0.113.0/24\"]' -auto-approve\n```\n\n#### 🎯 Attack Execution\nSince our focus is on the defender’s perspective, each scenario includes a **fully automated attack script**. Instead of manually typing commands, the script replays the attack path so you can observe detections and signals.\nYou may be prompted to provide inputs (e.g., your external IP). These are always displayed at the end of the Terraform deployment.\n\n```bash\nchmod +x attack.sh\n./attack.sh\n```\n\n\n![attack](./assets/attack.png)\n\n#### 🧹 Clean Up\nWhen you are finished, destroy all resources to avoid ongoing costs. This will tear down the entire lab environment including all compute, networking, and IAM components created during deployment.\n\n```bash\nterraform destroy -var='attack_whitelist=[]' -auto-approve\n```\n\n&nbsp;\n\n## 📖 Usage Guide\nTerraform commands you’ll use most often:  \n\n```bash\nterraform init      # prepare the working directory\nterraform apply     # deploy a scenario\nterraform destroy   # clean up resources\n```\n\n&nbsp;\n***\n&nbsp;\n\n## Contributing\nWe welcome contributions! You can submit pull requests for:  \n- New scenarios  \n- Bug fixes  \n- Documentation improvements\n  \n&nbsp;\n## 💰 Cost\nEach scenario is designed with minimal cloud resources to reduce expenses and limit blast radius.  \nHowever, costs may still accrue while environments are running. To avoid unnecessary charges, always shut down and destroy the environment when you are finished.\n\n&nbsp;\n\n## 👥 Contributors\n- P",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-19T00:54:44.818818"
  },
  {
    "basic_info": {
      "name": "LiquidGlass-JetpackCompose",
      "full_name": "ardakazanci/LiquidGlass-JetpackCompose",
      "owner": "ardakazanci",
      "description": null,
      "url": "https://github.com/ardakazanci/LiquidGlass-JetpackCompose",
      "clone_url": "https://github.com/ardakazanci/LiquidGlass-JetpackCompose.git",
      "ssh_url": "git@github.com:ardakazanci/LiquidGlass-JetpackCompose.git",
      "homepage": null,
      "created_at": "2025-09-18T15:22:51Z",
      "updated_at": "2025-09-18T21:58:57Z",
      "pushed_at": "2025-09-18T15:28:38Z"
    },
    "stats": {
      "stars": 12,
      "forks": 0,
      "watchers": 12,
      "open_issues": 0,
      "size": 585
    },
    "tech_info": {
      "language": "Kotlin",
      "languages": {
        "Kotlin": 12328
      },
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "https://www.linkedin.com/posts/ardakazanci_jetpackcompose-androiddevelopment-androidprogramming-activity-7374464292516945920-jHLU?utm_source=share&utm_medium=member_desktop&rcm=ACoAAB2vx2QBVowkGS1YKnq1EhEAH-eJwQf_4T8\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-19T00:54:46.145639"
  },
  {
    "basic_info": {
      "name": "Magic-Door-Godot-4.5-release-page",
      "full_name": "lukky-nl/Magic-Door-Godot-4.5-release-page",
      "owner": "lukky-nl",
      "description": "Magic door seen on the Godot 4.5 release page",
      "url": "https://github.com/lukky-nl/Magic-Door-Godot-4.5-release-page",
      "clone_url": "https://github.com/lukky-nl/Magic-Door-Godot-4.5-release-page.git",
      "ssh_url": "git@github.com:lukky-nl/Magic-Door-Godot-4.5-release-page.git",
      "homepage": null,
      "created_at": "2025-09-18T15:07:18Z",
      "updated_at": "2025-09-18T23:49:48Z",
      "pushed_at": "2025-09-18T15:07:20Z"
    },
    "stats": {
      "stars": 12,
      "forks": 1,
      "watchers": 12,
      "open_issues": 0,
      "size": 24
    },
    "tech_info": {
      "language": null,
      "languages": {},
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-19T00:54:47.458516"
  },
  {
    "basic_info": {
      "name": "moyu_yolov8",
      "full_name": "danghb/moyu_yolov8",
      "owner": "danghb",
      "description": "AI编程的yolo摄像头摸鱼小工具，防止背后凉凉的，不知道有没有人。",
      "url": "https://github.com/danghb/moyu_yolov8",
      "clone_url": "https://github.com/danghb/moyu_yolov8.git",
      "ssh_url": "git@github.com:danghb/moyu_yolov8.git",
      "homepage": null,
      "created_at": "2025-09-18T05:06:44Z",
      "updated_at": "2025-09-18T17:30:56Z",
      "pushed_at": "2025-09-18T07:57:55Z"
    },
    "stats": {
      "stars": 11,
      "forks": 0,
      "watchers": 11,
      "open_issues": 0,
      "size": 5844
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 30021,
        "Shell": 373
      },
      "license": null,
      "topics": []
    },
    "content": {
      "readme": "# YOLO摸鱼后视镜项目\n\n为了解决摸鱼时总担心后面有没有人的问题，找哈基米（gemini）开发了一个基于python的摸鱼后视镜项目，实时告诉你摄像头里看到几个人。\n\n还能用桌面小浮窗看到具体人在哪里。让你做到摸鱼心里有底。2333333\n\n![image](https://github.com/danghb/moyu_yolov8/blob/master/pic/%E4%BD%BF%E7%94%A8%E7%A4%BA%E4%BE%8B.jpg)\n\n## 🚀 快速开始（推荐新手）\n\n### 一键启动简化版本\n```powershell\n# 安装依赖   我在python 3.13.3上可以运行，其他版本没试过\npip install -r requirements.txt\n\n# 直接运行简化版\npython single_yolo_mirror.py\n```\n\n**简化版特点：**\n- ✅ 单文件运行，无需复杂配置\n- ✅ CPU模式，兼容性好\n- ✅ 即装即用，适合快速体验\n- ⚠️ 性能较低，检测速度慢\n\n**性能提升：** 如果追求更高性能，可以自行研究WSL环境配置或使用AI助手修改代码启用GPU加速，详见下方WSL+CUDA版本说明。\n\n---\n\n## 📁 文件结构\n\n```\nyolo后视镜/\n├── WSL+win/              # GPU 加速版本（需要自行解决环境问题）\n│   ├── api_server.py     # WSL 端 FastAPI 服务器\n│   ├── windows_client.py # Windows 端摄像头采集和转发服务\n│   ├── yolo.py          # 客户端程序（托盘 + 显示）\n│   └── start_server.sh  # WSL 端启动脚本\n├── simple_yolo_mirror.py # 简化版单文件程序（CPU模式，新手推荐）\n├── requirements.txt      # Python 依赖包列表\n├── yolov8n.pt           # YOLOv8 预训练模型文件\n└── README.md            # 项目说明文档\n```\n\n## 🎮 程序说明\n\n### 系统托盘功能\n\n- **托盘图标** - 显示当前检测到的人数\n- **右键菜单**：\n  - 显示浮窗 - 显示可拖拽的人数浮窗\n  - 隐藏浮窗 - 隐藏人数浮窗\n  - 显示视频窗口 - 显示实时视频预览（带检测框）\n  - 退出 - 关闭程序\n\n### 浮窗操作\n\n- **拖拽移动** - 点击并拖拽浮窗到任意位置\n- **半透明显示** - 不影响其他应用的使用\n- **实时更新** - 显示最新的人数统计\n\n### 视频预览窗口\n\n- **实时画面** - 显示摄像头采集的实时画面\n- **检测框显示** - 红色矩形框标出检测到的人员\n- **拖拽移动** - 可拖拽到任意位置\n- **右键关闭** - 右键点击关闭窗口\n\n## 🏗️ 系统架构\n\n```\n┌─────────────────┐    HTTP API    ┌─────────────────┐    TCP Socket    ┌─────────────────┐\n│   WSL/Ubuntu    │ ◄──────────── │   Windows 端    │ ◄──────────────► │    客户端       │\n│                 │               │                 │                  │                 │\n│ • YOLOv8 模型   │               │ • 摄像头采集    │                  │ • 系统托盘      │\n│ • FastAPI 服务  │               │ • 图像处理      │                  │ • 浮窗显示      │\n│ • GPU 加速      │               │ • TCP 服务器    │                  │ • 视频预览      │\n└─────────────────┘               └─────────────────┘                  └─────────────────┘\n```\n## ⚙️ 配置说明\n\n### 1. WSL + CUDA 环境配置\n\n**注意：** `WSL+win` 文件夹中的代码是为了使用 GPU 加速而设计的。WSL 环境下安装 CUDA 更加便捷，能够充分利用 GPU 性能进行 YOLO 推理加速。\n\n- **推荐用户：** 熟悉 CUDA 环境配置的用户\n- **安装建议：** 可以结合 AI 助手（如 ChatGPT、Claude 等）来协助安装和配置 WSL + CUDA 环境\n- **性能优势：** GPU 加速可显著提升检测速度和响应性能\n\n### 2. 网络配置\n\n在 `WSL+win/windows_client.py` 中修改 WSL IP 地址：\n\n```python\nWSL_IP = \"172.26.8.96\"  # 替换为你的 WSL IP 地址\n```\n\n在 `WSL+win/yolo.py` 中修改服务器 IP 地址：\n\n```python\nSERVER_IP = \"192.168.233.1\"  # 替换为你的 Windows IP 地址\n```\n\n\n## 🚀 启动步骤\n\n### 1. 启动 WSL 端 YOLO API 服务\n\n```bash\ncd WSL+win\nchmod +x start_server.sh\n./start_server.sh\n```\n\n### 2. 启动 Windows 端服务\n\n```powershell\n# 在 WSL+win 目录中执行\ncd WSL+win\npython windows_client.py\n```\n\n### 3. 启动客户端\n\n```powershell\n# 在 WSL+win 目录中执行\ncd WSL+win\npython yolo.py\n```\n\n\n",
      "default_branch": "master"
    },
    "fetched_at": "2025-09-19T00:54:48.779901"
  },
  {
    "basic_info": {
      "name": "geobots.xyz",
      "full_name": "hvoking/geobots.xyz",
      "owner": "hvoking",
      "description": "Open source Mapbox data visualization tool",
      "url": "https://github.com/hvoking/geobots.xyz",
      "clone_url": "https://github.com/hvoking/geobots.xyz.git",
      "ssh_url": "git@github.com:hvoking/geobots.xyz.git",
      "homepage": "https://geobots.xyz/",
      "created_at": "2025-09-18T17:03:46Z",
      "updated_at": "2025-09-18T23:09:16Z",
      "pushed_at": "2025-09-18T17:05:24Z"
    },
    "stats": {
      "stars": 11,
      "forks": 2,
      "watchers": 11,
      "open_issues": 0,
      "size": 2464
    },
    "tech_info": {
      "language": "TypeScript",
      "languages": {
        "TypeScript": 74543,
        "SCSS": 10583,
        "HTML": 472
      },
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "# GeoBots.xyz - Geolocation Intelligence Embedded into Maps\n\nAn interactive web map showing snippets of the world at any location, with adjustable \"boundaries\" and the ability to toggle between a circle and actual walking distances using Mapbox isochrone API.\n\nThis project is open-source. All the code and interface is created from scratch with the fewest external libraries possible, and the objective is to be a base for AI applications.\n\n\n---\n\n## 🌐 Live Demo\n\n**Try it now:** [https://geobots.xyz/](https://geobots.xyz/)\n\n---\n\n## ✨ Key Features\n\n* **Global Support**: Explore worldwide.\n* **Interactive Map**: Simply click anywhere to instantly find the layers you want.\n* **Adjustable Boundaries**: Easily choose to display between circle and walking distances.\n* **Ultra-High Performance**: Optimized React application with minimal dependencies.\n* **Modern Frontend**: Built with React 19, TypeScript, and Sass.\n\n---\n\n---\n\n## 🚀 Getting Started\n\nThese instructions will get you a copy of the project up and running on your local machine for development and testing purposes.\n\n### Prerequisites\n\n* **Node**\n\n### Installation\n\n1.  **Clone the repository:**\n    ```bash\n    git clone [https://github.com/ugeom/geobots.xyz.git](https://github.com/ugeom/geobots.xyz.git)\n    cd geobots\n    ```\n\n2.  **Install dependencies:**\n    ```bash\n    yarn install\n    ```\n\n3.  **Set up environment variables:**\n    ```bash\n    cp .env.example .env\n    ```\n    Then add your Mapbox API key to the `.env` file (see Setup section below).\n\n4.  **Start the development server:**\n    ```bash\n    yarn start\n    ```\n    Visit `http://localhost:3000`\n\n---\n\n## 🔑 Mapbox API Setup\n\n1. **Create a Mapbox account** at [mapbox.com](https://account.mapbox.com/auth/signup/)\n2. **Get your access token** from [your account page](https://account.mapbox.com/access-tokens/)\n3. **Add the token to your `.env` file:**\n   ```\n   REACT_APP_MAPBOX_TOKEN=your_actual_token_here\n   ```\n\n## 📂 Project Structure\n* `src/app/` - Main application components\n    * `panel/` - Left sidebar with logo and menu sections\n    * `map/` - Core mapping interface with boundary tools and markers\n    * `tools/` - Map interaction tools (cursor, location, search)\n    * `views/` - Different view modes (agents, basemaps, features)\n* `src/context/` - React Context for global state management\n* `src/utils/` - Utility functions for geometry, mapping, and UI components\n\n---\n\n## 🤝 Contributing\n\nContributions are welcome! If you have suggestions for improvements, new features, or bug fixes, please open an issue or submit a pull request.\n\n---\n\n## 👨‍💻 Created By\n\n**Gustavo Gonzalez - Urban Geometry** - [ugeom.com](https://ugeom.com)  \nArchitect and Urbanist [Urban Geometry] (https://ugeom.com)",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-19T00:54:50.119409"
  },
  {
    "basic_info": {
      "name": "AD-DINOv3",
      "full_name": "Kaisor-Yuan/AD-DINOv3",
      "owner": "Kaisor-Yuan",
      "description": "AD-DINOv3: Enhancing DINOv3 for Zero-Shot Anomaly Detection with Anomaly-Aware Calibration",
      "url": "https://github.com/Kaisor-Yuan/AD-DINOv3",
      "clone_url": "https://github.com/Kaisor-Yuan/AD-DINOv3.git",
      "ssh_url": "git@github.com:Kaisor-Yuan/AD-DINOv3.git",
      "homepage": "",
      "created_at": "2025-09-18T01:53:52Z",
      "updated_at": "2025-09-18T08:29:24Z",
      "pushed_at": "2025-09-18T02:04:48Z"
    },
    "stats": {
      "stars": 11,
      "forks": 0,
      "watchers": 11,
      "open_issues": 0,
      "size": 2395
    },
    "tech_info": {
      "language": null,
      "languages": {},
      "license": null,
      "topics": []
    },
    "content": {
      "readme": "# AD-DINOv3\n\n[AD-DINOv3: Enhancing DINOv3 for Zero-Shot Anomaly Detection with Anomaly-Aware Calibration](https://arxiv.org/abs/2509.14084)\n\nWe present AD-DINOv3, the first framework to adapt DINOv3 for zero-shot anomaly detection. By introducing lightweight adapters for domain alignment and an Anomaly-Aware Calibration Module (AACM) that enables interactive calibration between the CLS token and patch tokens, AD-DINOv3 achieves stronger anomaly-aware representations. Extensive experiments on eight industrial and medical benchmarks demonstrate that our method consistently matches or surpasses state-of-the-art approaches, establishing AD-DINOv3 as a powerful and general ZSAD framework.\n\n![](AD-DINOv3.png)\n\nThe code is currently being organized and will be released soon. Stay tuned!\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-19T00:54:51.443440"
  }
]