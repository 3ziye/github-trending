[
  {
    "basic_info": {
      "name": "nanobot",
      "full_name": "HKUDS/nanobot",
      "owner": "HKUDS",
      "description": "\"üêà nanobot: The Ultra-Lightweight OpenClaw\"",
      "url": "https://github.com/HKUDS/nanobot",
      "clone_url": "https://github.com/HKUDS/nanobot.git",
      "ssh_url": "git@github.com:HKUDS/nanobot.git",
      "homepage": "",
      "created_at": "2026-02-01T07:16:15Z",
      "updated_at": "2026-02-15T03:37:38Z",
      "pushed_at": "2026-02-14T10:02:20Z"
    },
    "stats": {
      "stars": 18958,
      "forks": 2786,
      "watchers": 18958,
      "open_issues": 416,
      "size": 32988
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 341969,
        "TypeScript": 8773,
        "Shell": 7009,
        "JavaScript": 1316,
        "Dockerfile": 1294
      },
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "<div align=\"center\">\n  <img src=\"nanobot_logo.png\" alt=\"nanobot\" width=\"500\">\n  <h1>nanobot: Ultra-Lightweight Personal AI Assistant</h1>\n  <p>\n    <a href=\"https://pypi.org/project/nanobot-ai/\"><img src=\"https://img.shields.io/pypi/v/nanobot-ai\" alt=\"PyPI\"></a>\n    <a href=\"https://pepy.tech/project/nanobot-ai\"><img src=\"https://static.pepy.tech/badge/nanobot-ai\" alt=\"Downloads\"></a>\n    <img src=\"https://img.shields.io/badge/python-‚â•3.11-blue\" alt=\"Python\">\n    <img src=\"https://img.shields.io/badge/license-MIT-green\" alt=\"License\">\n    <a href=\"./COMMUNICATION.md\"><img src=\"https://img.shields.io/badge/Feishu-Group-E9DBFC?style=flat&logo=feishu&logoColor=white\" alt=\"Feishu\"></a>\n    <a href=\"./COMMUNICATION.md\"><img src=\"https://img.shields.io/badge/WeChat-Group-C5EAB4?style=flat&logo=wechat&logoColor=white\" alt=\"WeChat\"></a>\n    <a href=\"https://discord.gg/MnCvHqpUGB\"><img src=\"https://img.shields.io/badge/Discord-Community-5865F2?style=flat&logo=discord&logoColor=white\" alt=\"Discord\"></a>\n  </p>\n</div>\n\nüêà **nanobot** is an **ultra-lightweight** personal AI assistant inspired by [OpenClaw](https://github.com/openclaw/openclaw) \n\n‚ö°Ô∏è Delivers core agent functionality in just **~4,000** lines of code ‚Äî **99% smaller** than Clawdbot's 430k+ lines.\n\nüìè Real-time line count: **3,536 lines** (run `bash core_agent_lines.sh` to verify anytime)\n\n## üì¢ News\n\n- **2026-02-13** üéâ Released v0.1.3.post7 ‚Äî includes security hardening and multiple improvements. All users are recommended to upgrade to the latest version. See [release notes](https://github.com/HKUDS/nanobot/releases/tag/v0.1.3.post7) for more details.\n- **2026-02-12** üß† Redesigned memory system ‚Äî Less code, more reliable. Join the [discussion](https://github.com/HKUDS/nanobot/discussions/566) about it!\n- **2026-02-10** üéâ Released v0.1.3.post6 with improvements! Check the updates [notes](https://github.com/HKUDS/nanobot/releases/tag/v0.1.3.post6) and our [roadmap](https://github.com/HKUDS/nanobot/discussions/431).\n- **2026-02-09** üí¨ Added Slack, Email, and QQ support ‚Äî nanobot now supports multiple chat platforms!\n- **2026-02-08** üîß Refactored Providers‚Äîadding a new LLM provider now takes just 2 simple steps! Check [here](#providers).\n- **2026-02-07** üöÄ Released v0.1.3.post5 with Qwen support & several key improvements! Check [here](https://github.com/HKUDS/nanobot/releases/tag/v0.1.3.post5) for details.\n- **2026-02-06** ‚ú® Added Moonshot/Kimi provider, Discord integration, and enhanced security hardening!\n- **2026-02-05** ‚ú® Added Feishu channel, DeepSeek provider, and enhanced scheduled tasks support!\n- **2026-02-04** üöÄ Released v0.1.3.post4 with multi-provider & Docker support! Check [here](https://github.com/HKUDS/nanobot/releases/tag/v0.1.3.post4) for details.\n- **2026-02-03** ‚ö° Integrated vLLM for local LLM support and improved natural language task scheduling!\n- **2026-02-02** üéâ nanobot officially launched! Welcome to try üêà nanobot!\n\n## Key Features of nanobot:\n\nü™∂ **Ultra-Lightweight**: Just ~4,000 lines of core agent code ‚Äî 99% smaller than Clawdbot.\n\nüî¨ **Research-Ready**: Clean, readable code that's easy to understand, modify, and extend for research.\n\n‚ö°Ô∏è **Lightning Fast**: Minimal footprint means faster startup, lower resource usage, and quicker iterations.\n\nüíé **Easy-to-Use**: One-click to deploy and you're ready to go.\n\n## üèóÔ∏è Architecture\n\n<p align=\"center\">\n  <img src=\"nanobot_arch.png\" alt=\"nanobot architecture\" width=\"800\">\n</p>\n\n## ‚ú® Features\n\n<table align=\"center\">\n  <tr align=\"center\">\n    <th><p align=\"center\">üìà 24/7 Real-Time Market Analysis</p></th>\n    <th><p align=\"center\">üöÄ Full-Stack Software Engineer</p></th>\n    <th><p align=\"center\">üìÖ Smart Daily Routine Manager</p></th>\n    <th><p align=\"center\">üìö Personal Knowledge Assistant</p></th>\n  </tr>\n  <tr>\n    <td align=\"center\"><p align=\"center\"><img src=\"case/search.gif\" width=\"180\" height=\"400\"></p></td>\n    <td align=\"center\"><p align=\"center\"><img src=\"case/code.gif\" width=\"180\" height=\"400\"></p></td>\n    <td align=\"center\"><p align=\"center\"><img src=\"case/scedule.gif\" width=\"180\" height=\"400\"></p></td>\n    <td align=\"center\"><p align=\"center\"><img src=\"case/memory.gif\" width=\"180\" height=\"400\"></p></td>\n  </tr>\n  <tr>\n    <td align=\"center\">Discovery ‚Ä¢ Insights ‚Ä¢ Trends</td>\n    <td align=\"center\">Develop ‚Ä¢ Deploy ‚Ä¢ Scale</td>\n    <td align=\"center\">Schedule ‚Ä¢ Automate ‚Ä¢ Organize</td>\n    <td align=\"center\">Learn ‚Ä¢ Memory ‚Ä¢ Reasoning</td>\n  </tr>\n</table>\n\n## üì¶ Install\n\n**Install from source** (latest features, recommended for development)\n\n```bash\ngit clone https://github.com/HKUDS/nanobot.git\ncd nanobot\npip install -e .\n```\n\n**Install with [uv](https://github.com/astral-sh/uv)** (stable, fast)\n\n```bash\nuv tool install nanobot-ai\n```\n\n**Install from PyPI** (stable)\n\n```bash\npip install nanobot-ai\n```\n\n## üöÄ Quick Start\n\n> [!TIP]\n> Set your API key in `~/.nanobot/config.json`.\n> Get API keys: [OpenRouter](https://openrouter.ai/keys) (Global) ¬∑ [Brave Search](https://brave.com/search/a",
      "default_branch": "main"
    },
    "fetched_at": "2026-02-15T03:37:45.203646"
  },
  {
    "basic_info": {
      "name": "Qwen3-TTS",
      "full_name": "QwenLM/Qwen3-TTS",
      "owner": "QwenLM",
      "description": "Qwen3-TTS is an open-source series of TTS models developed by the Qwen team at Alibaba Cloud, supporting stable, expressive, and streaming speech generation, free-form voice design, and vivid voice cloning.",
      "url": "https://github.com/QwenLM/Qwen3-TTS",
      "clone_url": "https://github.com/QwenLM/Qwen3-TTS.git",
      "ssh_url": "git@github.com:QwenLM/Qwen3-TTS.git",
      "homepage": null,
      "created_at": "2026-01-21T06:41:32Z",
      "updated_at": "2026-02-15T03:33:15Z",
      "pushed_at": "2026-02-06T04:11:37Z"
    },
    "stats": {
      "stars": 7691,
      "forks": 963,
      "watchers": 7691,
      "open_issues": 62,
      "size": 6947
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 402779
      },
      "license": "Apache License 2.0",
      "topics": []
    },
    "content": {
      "readme": "# Qwen3-TTS\n\n<br>\n\n<p align=\"center\">\n    <img src=\"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-TTS-Repo/qwen3_tts_logo.png\" width=\"400\"/>\n<p>\n\n<p align=\"center\">\n&nbsp&nbspü§ó <a href=\"https://huggingface.co/collections/Qwen/qwen3-tts\">Hugging Face</a>&nbsp&nbsp | &nbsp&nbspü§ñ <a href=\"https://modelscope.cn/collections/Qwen/Qwen3-TTS\">ModelScope</a>&nbsp&nbsp | &nbsp&nbspüìë <a href=\"https://qwen.ai/blog?id=qwen3tts-0115\">Blog</a>&nbsp&nbsp | &nbsp&nbspüìë <a href=\"https://arxiv.org/abs/2601.15621\">Paper</a>&nbsp&nbsp\n<br>\nüñ•Ô∏è <a href=\"https://huggingface.co/spaces/Qwen/Qwen3-TTS\">Hugging Face Demo</a>&nbsp&nbsp | &nbsp&nbsp üñ•Ô∏è <a href=\"https://modelscope.cn/studios/Qwen/Qwen3-TTS\">ModelScope Demo</a>&nbsp&nbsp | &nbsp&nbspüí¨ <a href=\"https://github.com/QwenLM/Qwen/blob/main/assets/wechat.png\">WeChat (ÂæÆ‰ø°)</a>&nbsp&nbsp | &nbsp&nbspü´® <a href=\"https://discord.gg/CV4E9rpNSD\">Discord</a>&nbsp&nbsp | &nbsp&nbspüìë <a href=\"https://help.aliyun.com/zh/model-studio/qwen-tts-realtime\">API</a>\n\n</p>\n\nWe release **Qwen3-TTS**, a series of powerful speech generation capabilities developed by Qwen, offering comprehensive support for voice clone, voice design, ultra-high-quality human-like speech generation, and natural language-based voice control. It provides developers and users with the most extensive set of speech generation features available.\n\n\n## News\n* 2026.1.22: üéâüéâüéâ We have released [Qwen3-TTS](https://huggingface.co/collections/Qwen/qwen3-tts) series (0.6B/1.7B) based on Qwen3-TTS-Tokenizer-12Hz. Please check our [blog](https://qwen.ai/blog?id=qwen3tts-0115)!\n\n## Contents <!-- omit in toc -->\n\n- [Overview](#overview)\n  - [Introduction](#introduction)\n  - [Model Architecture](#model-architecture)\n  - [Released Models Description and Download](#released-models-description-and-download)\n- [Quickstart](#quickstart)\n  - [Environment Setup](#environment-setup)\n  - [Python Package Usage](#python-package-usage)\n    - [Custom Voice Generation](#custom-voice-generate)\n    - [Voice Design](#voice-design)\n    - [Voice Clone](#voice-clone)\n    - [Voice Design then Clone](#voice-design-then-clone)\n    - [Tokenizer Encode and Decode](#tokenizer-encode-and-decode)\n  - [Launch Local Web UI Demo](#launch-local-web-ui-demo)\n  - [DashScope API Usage](#dashscope-api-usage)\n- [vLLM Usage](#vllm-usage)\n- [Fine Tuning](#fine-tuning)\n- [Evaluation](#evaluation)\n- [Citation](#citation)\n\n## Overview\n### Introduction\n\n<p align=\"center\">\n    <img src=\"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-TTS-Repo/qwen3_tts_introduction.png\" width=\"90%\"/>\n<p>\n\nQwen3-TTS covers 10 major languages (Chinese, English, Japanese, Korean, German, French, Russian, Portuguese, Spanish, and Italian) as well as multiple dialectal voice profiles to meet global application needs. In addition, the models feature strong contextual understanding, enabling adaptive control of tone, speaking rate, and emotional expression based on instructions and text semantics, and they show markedly improved robustness to noisy input text. Key features:\n\n* **Powerful Speech Representation**: Powered by the self-developed Qwen3-TTS-Tokenizer-12Hz, it achieves efficient acoustic compression and high-dimensional semantic modeling of speech signals. It fully preserves paralinguistic information and acoustic environmental features, enabling high-speed, high-fidelity speech reconstruction through a lightweight non-DiT architecture.\n* **Universal End-to-End Architecture**: Utilizing a discrete multi-codebook LM architecture, it realizes full-information end-to-end speech modeling. This completely bypasses the information bottlenecks and cascading errors inherent in traditional LM+DiT schemes, significantly enhancing the model‚Äôs versatility, generation efficiency, and performance ceiling.\n* **Extreme Low-Latency Streaming Generation**: Based on the innovative Dual-Track hybrid streaming generation architecture, a single model supports both streaming and non-streaming generation. It can output the first audio packet immediately after a single character is input, with end-to-end synthesis latency as low as 97ms, meeting the rigorous demands of real-time interactive scenarios.\n* **Intelligent Text Understanding and Voice Control**: Supports speech generation driven by natural language instructions, allowing for flexible control over multi-dimensional acoustic attributes such as timbre, emotion, and prosody. By deeply integrating text semantic understanding, the model adaptively adjusts tone, rhythm, and emotional expression, achieving lifelike ‚Äúwhat you imagine is what you hear‚Äù output.\n\n\n### Model Architecture\n\n<p align=\"center\">\n    <img src=\"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-TTS-Repo/overview.png\" width=\"80%\"/>\n<p>\n\n### Released Models Description and Download\n\nBelow is an introduction and download information for the Qwen3-TTS models that have already been released. Other models mentioned in the technical report will be released in the near future. Please se",
      "default_branch": "main"
    },
    "fetched_at": "2026-02-15T03:37:46.317499"
  },
  {
    "basic_info": {
      "name": "knowledge-work-plugins",
      "full_name": "anthropics/knowledge-work-plugins",
      "owner": "anthropics",
      "description": "Open source repository of plugins primarily intended for knowledge workers to use in Claude Cowork",
      "url": "https://github.com/anthropics/knowledge-work-plugins",
      "clone_url": "https://github.com/anthropics/knowledge-work-plugins.git",
      "ssh_url": "git@github.com:anthropics/knowledge-work-plugins.git",
      "homepage": null,
      "created_at": "2026-01-23T20:11:54Z",
      "updated_at": "2026-02-15T02:30:15Z",
      "pushed_at": "2026-02-13T20:24:19Z"
    },
    "stats": {
      "stars": 7323,
      "forks": 717,
      "watchers": 7323,
      "open_issues": 41,
      "size": 640
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 310600,
        "HTML": 97651
      },
      "license": "Apache License 2.0",
      "topics": []
    },
    "content": {
      "readme": "# Knowledge Work Plugins\n\nPlugins that turn Claude into a specialist for your role, team, and company. Built for [Claude Cowork](https://claude.com/product/cowork), also compatible with [Claude Code](https://claude.com/product/claude-code).\n\n## Why Plugins\n\nCowork lets you set the goal and Claude delivers finished, professional work. Plugins let you go further: tell Claude how you like work done, which tools and data to pull from, how to handle critical workflows, and what slash commands to expose ‚Äî so your team gets better and more consistent outcomes.\n\nEach plugin bundles the skills, connectors, slash commands, and sub-agents for a specific job function. Out of the box, they give Claude a strong starting point for helping anyone in that role. The real power comes when you customize them for your company ‚Äî your tools, your terminology, your processes ‚Äî so Claude works like it was built for your team.\n\n## Plugin Marketplace\n\nWe're open-sourcing 11 plugins built and inspired by our own work:\n\n| Plugin | How it helps | Connectors |\n|--------|-------------|------------|\n| **[productivity](./productivity)** | Manage tasks, calendars, daily workflows, and personal context so you spend less time repeating yourself. | Slack, Notion, Asana, Linear, Jira, Monday, ClickUp, Microsoft 365 |\n| **[sales](./sales)** | Research prospects, prep for calls, review your pipeline, draft outreach, and build competitive battlecards. | Slack, HubSpot, Close, Clay, ZoomInfo, Notion, Jira, Fireflies, Microsoft 365 |\n| **[customer-support](./customer-support)** | Triage tickets, draft responses, package escalations, research customer context, and turn resolved issues into knowledge base articles. | Slack, Intercom, HubSpot, Guru, Jira, Notion, Microsoft 365 |\n| **[product-management](./product-management)** | Write specs, plan roadmaps, synthesize user research, keep stakeholders updated, and track the competitive landscape. | Slack, Linear, Asana, Monday, ClickUp, Jira, Notion, Figma, Amplitude, Pendo, Intercom, Fireflies |\n| **[marketing](./marketing)** | Draft content, plan campaigns, enforce brand voice, brief on competitors, and report on performance across channels. | Slack, Canva, Figma, HubSpot, Amplitude, Notion, Ahrefs, SimilarWeb, Klaviyo |\n| **[legal](./legal)** | Review contracts, triage NDAs, navigate compliance, assess risk, prep for meetings, and draft templated responses. | Slack, Box, Egnyte, Jira, Microsoft 365 |\n| **[finance](./finance)** | Prep journal entries, reconcile accounts, generate financial statements, analyze variances, manage close, and support audits. | Snowflake, Databricks, BigQuery, Slack, Microsoft 365 |\n| **[data](./data)** | Query, visualize, and interpret datasets ‚Äî write SQL, run statistical analysis, build dashboards, and validate your work before sharing. | Snowflake, Databricks, BigQuery, Hex, Amplitude, Jira |\n| **[enterprise-search](./enterprise-search)** | Find anything across email, chat, docs, and wikis ‚Äî one query across all your company's tools. | Slack, Notion, Guru, Jira, Asana, Microsoft 365 |\n| **[bio-research](./bio-research)** | Connect to preclinical research tools and databases (literature search, genomics analysis, target prioritization) to accelerate early-stage life sciences R&D. | PubMed, BioRender, bioRxiv, ClinicalTrials.gov, ChEMBL, Synapse, Wiley, Owkin, Open Targets, Benchling |\n| **[cowork-plugin-management](./cowork-plugin-management)** | Create new plugins or customize existing ones for your organization's specific tools and workflows. | ‚Äî |\n\nInstall these directly from Cowork, browse the full collection here on GitHub, or build your own.\n\n## Getting Started\n\n### Cowork\n\nInstall plugins from [claude.com/plugins](https://claude.com/plugins/).\n\n### Claude Code\n\n```bash\n# Add the marketplace first\nclaude plugin marketplace add anthropics/knowledge-work-plugins\n\n# Then install a specific plugin\nclaude plugin install sales@knowledge-work-plugins\n```\n\nOnce installed, plugins activate automatically. Skills fire when relevant, and slash commands are available in your session (e.g., `/sales:call-prep`, `/data:write-query`).\n\n## How Plugins Work\n\nEvery plugin follows the same structure:\n\n```\nplugin-name/\n‚îú‚îÄ‚îÄ .claude-plugin/plugin.json   # Manifest\n‚îú‚îÄ‚îÄ .mcp.json                    # Tool connections\n‚îú‚îÄ‚îÄ commands/                    # Slash commands you invoke explicitly\n‚îî‚îÄ‚îÄ skills/                      # Domain knowledge Claude draws on automatically\n```\n\n- **Skills** encode the domain expertise, best practices, and step-by-step workflows Claude needs to give you useful help. Claude draws on them automatically when relevant.\n- **Commands** are explicit actions you trigger (e.g., `/finance:reconciliation`, `/product-management:write-spec`).\n- **Connectors** wire Claude to the external tools your role depends on ‚Äî CRMs, project trackers, data warehouses, design tools, and more ‚Äî via [MCP servers](https://modelcontextprotocol.io/).\n\nEvery component is file-based ‚Äî markdown and ",
      "default_branch": "main"
    },
    "fetched_at": "2026-02-15T03:37:47.441489"
  },
  {
    "basic_info": {
      "name": "original_performance_takehome",
      "full_name": "anthropics/original_performance_takehome",
      "owner": "anthropics",
      "description": "Anthropic's original performance take-home, now open for you to try!",
      "url": "https://github.com/anthropics/original_performance_takehome",
      "clone_url": "https://github.com/anthropics/original_performance_takehome.git",
      "ssh_url": "git@github.com:anthropics/original_performance_takehome.git",
      "homepage": null,
      "created_at": "2026-01-19T19:16:04Z",
      "updated_at": "2026-02-15T01:32:29Z",
      "pushed_at": "2026-01-22T01:11:08Z"
    },
    "stats": {
      "stars": 3448,
      "forks": 758,
      "watchers": 3448,
      "open_issues": 13,
      "size": 8
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 57361,
        "HTML": 4758
      },
      "license": null,
      "topics": []
    },
    "content": {
      "readme": "# Anthropic's Original Performance Take-Home\n\nThis repo contains a version of Anthropic's original performance take-home, before Claude Opus 4.5 started doing better than humans given only 2 hours.\n\nThe original take-home was a 4-hour one that starts close to the contents of this repo, after Claude Opus 4 beat most humans at that, it was updated to a 2-hour one which started with code which achieved 18532 cycles (7.97x faster than this repo starts you). This repo is based on the newer take-home which has a few more instructions and comes with better debugging tools, but has the starter code reverted to the slowest baseline. After Claude Opus 4.5 we started using a different base for our time-limited take-homes.\n\nNow you can try to beat Claude Opus 4.5 given unlimited time!\n\n## Performance benchmarks \n\nMeasured in clock cycles from the simulated machine. All of these numbers are for models doing the 2 hour version which started at 18532 cycles:\n\n- **2164 cycles**: Claude Opus 4 after many hours in the test-time compute harness\n- **1790 cycles**: Claude Opus 4.5 in a casual Claude Code session, approximately matching the best human performance in 2 hours\n- **1579 cycles**: Claude Opus 4.5 after 2 hours in our test-time compute harness\n- **1548 cycles**: Claude Sonnet 4.5 after many more than 2 hours of test-time compute\n- **1487 cycles**: Claude Opus 4.5 after 11.5 hours in the harness\n- **1363 cycles**: Claude Opus 4.5 in an improved test time compute harness\n- **??? cycles**: Best human performance ever is substantially better than the above, but we won't say how much.\n\nWhile it's no longer a good time-limited test, you can still use this test to get us excited about hiring you! If you optimize below 1487 cycles, beating Claude Opus 4.5's best performance at launch, email us at performance-recruiting@anthropic.com with your code (and ideally a resume) so we can be appropriately impressed, especially if you get near the best solution we've seen. New model releases may change what threshold impresses us though, and no guarantees that we keep this readme updated with the latest on that.\n\nRun `python tests/submission_tests.py` to see which thresholds you pass.\n\n## Warning: LLMs can cheat\n\nNone of the solutions we received on the first day post-release below 1300 cycles were valid solutions. In each case, a language model modified the tests to make the problem easier.\n\nIf you use an AI agent, we recommend instructing it not to change the `tests/` folder and to use `tests/submission_tests.py` for verification.\n\nPlease run the following commands to validate your submission, and mention that you did so when submitting:\n```\n# This should be empty, the tests folder must be unchanged\ngit diff origin/main tests/\n# You should pass some of these tests and use the cycle count this prints\npython tests/submission_tests.py\n```\n\nAn example of this kind of hack is a model noticing that `problem.py` has multicore support, implementing multicore as an optimization, noticing there's no speedup and \"debugging\" that `N_CORES = 1` and \"fixing\" the core count so they get a speedup. Multicore is disabled intentionally in this version.\n",
      "default_branch": "main"
    },
    "fetched_at": "2026-02-15T03:37:48.567016"
  },
  {
    "basic_info": {
      "name": "lingbot-world",
      "full_name": "Robbyant/lingbot-world",
      "owner": "Robbyant",
      "description": "Advancing Open-source World Models",
      "url": "https://github.com/Robbyant/lingbot-world",
      "clone_url": "https://github.com/Robbyant/lingbot-world.git",
      "ssh_url": "git@github.com:Robbyant/lingbot-world.git",
      "homepage": "https://technology.robbyant.com/lingbot-world",
      "created_at": "2026-01-28T04:52:50Z",
      "updated_at": "2026-02-15T03:11:14Z",
      "pushed_at": "2026-02-02T11:46:45Z"
    },
    "stats": {
      "stars": 2857,
      "forks": 229,
      "watchers": 2857,
      "open_issues": 18,
      "size": 47043
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 594764
      },
      "license": "Apache License 2.0",
      "topics": [
        "aigc",
        "image-to-video",
        "lingbot-world",
        "video-generation",
        "world-models"
      ]
    },
    "content": {
      "readme": "<div align=\"center\">\n  <img src=\"assets/teaser.png\">\n\n<h1>LingBot-World: Advancing Open-source World Models</h1>\n\nRobbyant Team\n\n</div>\n\n\n<div align=\"center\">\n\n[![Page](https://img.shields.io/badge/%F0%9F%8C%90%20Project%20Page-Demo-00bfff)](https://technology.robbyant.com/lingbot-world)\n[![Tech Report](https://img.shields.io/static/v1?label=Paper&message=PDF&color=red&logo=arxiv)](https://arxiv.org/abs/2601.20540)\n[![Model](https://img.shields.io/static/v1?label=%F0%9F%A4%97%20Model&message=HuggingFace&color=yellow)](https://huggingface.co/robbyant/lingbot-world-base-cam)\n[![Model](https://img.shields.io/static/v1?label=%F0%9F%A4%96%20Model&message=ModelScope&color=purple)](https://www.modelscope.cn/models/Robbyant/lingbot-world-base-cam)\n[![License](https://img.shields.io/badge/License-Apache--2.0-green)](LICENSE.txt)\n\n\n</div>\n\n-----\n\nWe are excited to introduce **LingBot-World**, an open-sourced world simulator stemming from video generation. Positioned\nas a top-tier world model, LingBot-World offers the following features. \n- **High-Fidelity & Diverse Environments**: It maintains high fidelity and robust dynamics in a broad spectrum of environments, including realism, scientific contexts, cartoon styles, and beyond. \n- **Long-Term Memory & Consistency**: It enables a minute-level horizon while preserving contextual consistency over time, which is also known as long-term memory. \n- **Real-Time Interactivity & Open Access**: It supports real-time interactivity, achieving a latency of under 1 second when producing 16 frames per second. We provide public access to the code and model in an effort to narrow the divide between open-source and closed-source technologies. We believe our release will empower the community with practical applications across areas like content creation, gaming, and robot learning.\n\n## üé¨ Video Demo\n<div align=\"center\">\n  <video src=\"https://github.com/user-attachments/assets/ea4a7a8d-5d9e-4ccf-96e7-02f93797116e\" width=\"100%\" poster=\"\"> </video>\n</div>\n\n## üî• News\n- Jan 29, 2026: üéâ We release the technical report, code, and models for LingBot-World.\n\n<!-- ## üîñ Introduction of LingBot-World\nWe present **LingBot-World**, an **open-sourced** world simulator stemming from video generation. Positioned\nas a top-tier world model, LingBot-World offers the following features. \n- It maintains high fidelity and robust dynamics in a broad spectrum of environments, including realism, scientific contexts, cartoon styles, and beyond. \n- It enables a minute-level horizon while preserving contextual consistency over time, which is also known as **long-term memory**. \n- It supports real-time interactivity, achieving a latency of under 1 second when producing 16 frames per second. We provide public access to the code and model in an effort to narrow the divide between open-source and closed-source technologies. We believe our release will empower the community with practical applications across areas like content creation, gaming, and robot learning. -->\n\n## ‚öôÔ∏è Quick Start\nThis codebase is built upon [Wan2.2](https://github.com/Wan-Video/Wan2.2). Please refer to their documentation for installation instructions.\n### Installation\nClone the repo:\n```sh\ngit clone https://github.com/robbyant/lingbot-world.git\ncd lingbot-world\n```\nInstall dependencies:\n```sh\n# Ensure torch >= 2.4.0\npip install -r requirements.txt\n```\nInstall [`flash_attn`](https://github.com/Dao-AILab/flash-attention):\n```sh\npip install flash-attn --no-build-isolation\n```\n### Model Download\n\n| Model | Control Signals | Resolution | Download Links |\n| :---  | :--- | :--- | :--- |\n| **LingBot-World-Base (Cam)** | Camera Poses | 480P & 720P | ü§ó [HuggingFace](https://huggingface.co/robbyant/lingbot-world-base-cam) ü§ñ [ModelScope](https://www.modelscope.cn/models/Robbyant/lingbot-world-base-cam) |\n| **LingBot-World-Base (Act)** | Actions | - | *To be released* |\n| **LingBot-World-Fast**       |    -    | - | *To be released* |\n\nDownload models using huggingface-cli:\n```sh\npip install \"huggingface_hub[cli]\"\nhuggingface-cli download robbyant/lingbot-world-base-cam --local-dir ./lingbot-world-base-cam\n```\nDownload models using modelscope-cli:\n ```sh\npip install modelscope\nmodelscope download robbyant/lingbot-world-base-cam --local_dir ./lingbot-world-base-cam\n```\n### Inference\nBefore running inference, you need to prepare:\n- Input image\n- Text prompt\n- Control signals (optional, can be generated from a video using [ViPE](https://github.com/nv-tlabs/vipe))\n  - `intrinsics.npy`: Shape `[num_frames, 4]`, where the 4 values represent `[fx, fy, cx, cy]`\n  - `poses.npy`: Shape `[num_frames, 4, 4]`, where each `[4, 4]` represents a transformation matrix in OpenCV coordinates\n\n- 480P:\n``` sh\ntorchrun --nproc_per_node=8 generate.py --task i2v-A14B --size 480*832 --ckpt_dir lingbot-world-base-cam --image examples/00/image.jpg --action_path examples/00 --dit_fsdp --t5_fsdp --ulysses_size 8 --frame_num 161 --prompt \"The video presents a soaring journey through a",
      "default_branch": "main"
    },
    "fetched_at": "2026-02-15T03:37:49.664555"
  },
  {
    "basic_info": {
      "name": "last30days-skill",
      "full_name": "mvanhorn/last30days-skill",
      "owner": "mvanhorn",
      "description": "Claude Code skill that researches any topic across Reddit + X from the last 30 days, then writes copy-paste-ready prompts",
      "url": "https://github.com/mvanhorn/last30days-skill",
      "clone_url": "https://github.com/mvanhorn/last30days-skill.git",
      "ssh_url": "git@github.com:mvanhorn/last30days-skill.git",
      "homepage": null,
      "created_at": "2026-01-23T20:37:37Z",
      "updated_at": "2026-02-15T03:31:16Z",
      "pushed_at": "2026-02-07T19:48:11Z"
    },
    "stats": {
      "stars": 2612,
      "forks": 304,
      "watchers": 2612,
      "open_issues": 9,
      "size": 14216
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 189820
      },
      "license": null,
      "topics": [
        "ai-prompts",
        "claude",
        "claude-code",
        "reddit",
        "twitter"
      ]
    },
    "content": {
      "readme": "# /last30days v2\n\n**The AI world reinvents itself every month. This Claude Code skill keeps you current.** /last30days researches your topic across Reddit, X, and the web from the last 30 days, finds what the community is actually upvoting and sharing, and writes you a prompt that works today, not six months ago. Whether it's Ralph Wiggum loops, Suno music prompts, or the latest Midjourney techniques, you'll prompt like someone who's been paying attention.\n\n**New in V2:** Dramatically better search results. Smarter query construction finds posts that V1 missed entirely, and a new two-phase search automatically discovers key @handles and subreddits from initial results, then drills deeper. Also: free X search via [Bird CLI](https://github.com/steipete/bird) (no xAI key needed), `--days=N` for flexible lookback, and automatic model fallback. [Full changelog below.](#whats-new-in-v2)\n\n**The tradeoff:** V2 finds way more content but takes longer ‚Äî typically 2-8 minutes depending on how niche your topic is. The old V1 was faster but regularly missed results (like returning 0 X posts on trending topics). We think the depth is worth the wait, but if you'd use a faster \"quick mode\" that trades some depth for speed, let us know: [@mvanhorn](https://x.com/mvanhorn) / [@slashlast30days](https://x.com/slashlast30days).\n\n**Best for prompt research**: discover what prompting techniques actually work for any tool (ChatGPT, Midjourney, Claude, Figma AI, etc.) by learning from real community discussions and best practices.\n\n**But also great for anything trending**: music, culture, news, product recommendations, viral trends, or any question where \"what are people saying right now?\" matters.\n\n## Installation\n\n```bash\n# Clone the repo\ngit clone https://github.com/mvanhorn/last30days-skill.git ~/.claude/skills/last30days\n\n# Add your API keys\nmkdir -p ~/.config/last30days\ncat > ~/.config/last30days/.env << 'EOF'\nOPENAI_API_KEY=sk-...\nXAI_API_KEY=xai-...       # optional if using Bird CLI\nEOF\nchmod 600 ~/.config/last30days/.env\n```\n\n### Optional: Bird CLI for free X search\n\n[Bird CLI](https://github.com/steipete/bird) lets you search X without an xAI API key. If installed and authenticated, /last30days uses it automatically.\n\n```bash\nnpm install -g @steipete/bird\nbird login\n```\n\nBird is free and doesn't require an xAI key. If both Bird and an xAI key are available, Bird is preferred.\n\n## Usage\n\n```\n/last30days [topic]\n/last30days [topic] for [tool]\n```\n\nExamples:\n- `/last30days prompting techniques for ChatGPT for legal questions`\n- `/last30days iOS app mockups for Nano Banana Pro`\n- `/last30days What are the best rap songs lately`\n- `/last30days remotion animations for Claude Code`\n\n## What It Does\n\n1. **Researches** - Scans Reddit and X for discussions from the last 30 days\n2. **Synthesizes** - Identifies patterns, best practices, and what actually works\n3. **Delivers** - Either writes copy-paste-ready prompts for your target tool, or gives you a curated expert-level answer\n\n### Use it for:\n- **Prompt research** - \"What prompting techniques work for legal questions in ChatGPT?\"\n- **Tool best practices** - \"How are people using Remotion with Claude Code?\"\n- **Trend discovery** - \"What are the best rap songs right now?\"\n- **Product research** - \"What do people think of the new M4 MacBook?\"\n- **Viral content** - \"What's the dog-as-human trend on ChatGPT?\"\n\n---\n\n## Example: Nano Banana Pro Prompting (Image Generation)\n\n**Query:** `/last30days nano banana pro prompting`\n\n**Research Output:**\n> JSON prompting is the dominant technique ‚Äî The X community overwhelmingly uses structured JSON prompts rather than plain text. Creators like @Xmira_belle, @Cicily_aura, @RubenSalvo_, and @mahivisuals all share prompts in JSON format with nested objects for metadata, subject descriptions, lighting, camera specs, and scene composition. @realdigitaldao made a static ad \"in less than 5 min\" using a simple JSON prompt.\n>\n> Prompt adherence is Nano Banana Pro's killer advantage ‚Äî JSON prompts hit 92% precision for color accuracy, lighting ratios, and composition vs ~68% for natural language prompts, per fofr.ai. @artingent directly compared it to GPT Image 1.5 and noted Nano Banana Pro has higher prompt adherence ‚Äî \"GPT image 1.5 ignored the Top Down instruction.\"\n>\n> Stop writing \"tag soup\" ‚Äî act like a Creative Director ‚Äî The biggest shift is away from comma-separated keyword lists toward natural language descriptions with clear structure. The ICS Framework (Image type, Content, Style) and the 5-element formula (subject, composition, action, setting, style) are the two dominant approaches, per r/nanobanana2pro.\n\n**Key patterns discovered:**\n1. JSON > plain text ‚Äî Structure prompts as JSON objects with keys for subject, composition, lighting, camera, style ‚Äî per @Xmira_belle, @Cicily_aura\n2. The 5-element formula ‚Äî Subject + Composition + Action + Setting + Style ‚Äî per r/nanobanana2pro\n3. Separate subjects into distinct objects ‚Äî Multi-character s",
      "default_branch": "main"
    },
    "fetched_at": "2026-02-15T03:37:50.777962"
  },
  {
    "basic_info": {
      "name": "quip-protocol",
      "full_name": "QuipNetwork/quip-protocol",
      "owner": "QuipNetwork",
      "description": "experimental quip protocol network node",
      "url": "https://github.com/QuipNetwork/quip-protocol",
      "clone_url": "https://github.com/QuipNetwork/quip-protocol.git",
      "ssh_url": "git@github.com:QuipNetwork/quip-protocol.git",
      "homepage": null,
      "created_at": "2026-01-16T05:39:29Z",
      "updated_at": "2026-02-15T02:47:15Z",
      "pushed_at": "2026-02-05T02:33:01Z"
    },
    "stats": {
      "stars": 2487,
      "forks": 50,
      "watchers": 2487,
      "open_issues": 1,
      "size": 17109
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 4292055,
        "TypeScript": 313681,
        "Shell": 151867,
        "Cuda": 22581,
        "Metal": 21416,
        "CSS": 13632,
        "HTML": 319
      },
      "license": "GNU Affero General Public License v3.0",
      "topics": []
    },
    "content": {
      "readme": "# Quip Network Experimental Node\n\n> **WARNING: This is experimental demonstration software provided without warranty of any kind. It is not intended for production use. Use at your own risk.**\n\nThis project implements a quantum blockchain using quantum annealing for proof-of-work consensus. It features competitive mining between quantum computers (QPU) and classical simulated annealing (SA) with a dynamic difficulty adjustment mechanism.\n\n## Overview\n\nThe blockchain demonstrates:\n\n- **Quantum Annealing PoW**: Using Ising model optimization as the mining puzzle\n- **Competitive Mining**: Multiple miners (QPU and SA) compete to mine blocks\n- **Multi-Miner Support**: Configure any number of QPU and SA miners\n- **Dynamic Difficulty**: Inverted difficulty mechanism that prevents miner monopolization\n- **Streak Rewards**: Consecutive wins increase block rewards\n- **Solution Diversity**: Requires multiple diverse solutions to prevent trivial mining\n- **Individual Miner Tracking**: Each miner has unique ID and performance stats\n\n## Current Scope\n\nThe current implementation:\n- **Quantum PoW only** - No transactions, accounts, or other typical blockchain features\n- **Demonstration signatures** - The signature system is not yet production-secure; it demonstrates the hybrid ECDSA + WOTS+ approach but requires proper integration\n\n## Roadmap\n\nWe plan to build a complete blockchain by forking an existing battle tested codebase to maximize development velocity.\n\n### Phase 1: Core Integration\n- Fork a battle-tested blockchain codebase\n- Integrate our quantum proof-of-work mechanism (Ising model optimization, difficulty adjustment, block time targets already defined)\n- Target: Testnet deployment\n\n### Phase 2: Signature System\n- Integrate our hybrid signature system: classical ECDSA combined with post-quantum WOTS+ signatures\n- Implement stateful signature management\n- Wire signatures into transaction processing and consensus\n\n### Phase 3: Subnet Architecture\n- Implement a subnet system with **objective, measurable metrics** for validation\n- Subnets will solve computational problems (scientific computing, cryptographic proofs, etc.) with verifiable results\n- Define subnet registration, validation mechanisms, and reward distribution\n\n### Phase 4: Smart Contracts\n- Add smart contract support via EVM compatibility (Solidity/Vyper) and/or Rust-based WebAssembly runtime\n- Later: Enable contracts to interact with subnet computational results\n\n### Open Technical Decisions\n1. Which blockchain codebase to fork?\n2. How to structure subnets for different computational problem types?\n3. How to validate objective metrics across the decentralized network?\n4. Performance targets (TPS, finality time, subnet throughput)?\n\n## Getting Started\n\nYou can run your own node using the \"latest\" release, see the README in the `docker` directory for instructions on how to run the node in a container.\n\n## Setup\n\n1. Create and activate a virtual environment (Python 3.10+):\n\n   ```bash\n   python3 -m venv .quip\n   source .quip/bin/activate  # Windows: .venv\\Scripts\\activate\n   ```\n\n2. Install the package in editable mode:\n\n   ```bash\n   pip install -U pip setuptools wheel\n   pip install -e .\n   ```\n\n   This will install all dependencies from pyproject.toml and register console scripts.\n\n3. Set up D-Wave API credentials (optional, for QPU access):\n   ```bash\n   echo \"DWAVE_API_KEY=your_api_key_here\" > .env\n   ```\n\n## Project Structure\n\n```\nquip-protocol/\n‚îú‚îÄ‚îÄ quip_cli.py                # Main CLI entry point\n‚îú‚îÄ‚îÄ blockchain_base.py         # Base classes for miners\n‚îú‚îÄ‚îÄ shared/                    # Core modules\n‚îÇ   ‚îú‚îÄ‚îÄ network_node.py       # P2P networking (QUIC protocol)\n‚îÇ   ‚îú‚îÄ‚îÄ node.py               # Node state management\n‚îÇ   ‚îú‚îÄ‚îÄ block.py              # Block and header dataclasses\n‚îÇ   ‚îú‚îÄ‚îÄ block_signer.py       # ECDSA + WOTS+ signatures\n‚îÇ   ‚îú‚îÄ‚îÄ quantum_proof_of_work.py  # Ising model PoW\n‚îÇ   ‚îú‚îÄ‚îÄ base_miner.py         # Abstract miner interface\n‚îÇ   ‚îî‚îÄ‚îÄ ...                   # Additional utilities\n‚îú‚îÄ‚îÄ CPU/                       # CPU-based miners\n‚îÇ   ‚îú‚îÄ‚îÄ sa_miner.py           # Simulated annealing miner\n‚îÇ   ‚îî‚îÄ‚îÄ sa_sampler.py         # SA sampler implementation\n‚îú‚îÄ‚îÄ GPU/                       # GPU-accelerated miners\n‚îÇ   ‚îú‚îÄ‚îÄ cuda_miner.py         # CUDA GPU miner\n‚îÇ   ‚îú‚îÄ‚îÄ metal_miner.py        # Apple Metal/MPS miner\n‚îÇ   ‚îî‚îÄ‚îÄ modal_miner.py        # Modal Labs cloud GPU\n‚îú‚îÄ‚îÄ QPU/                       # Quantum processor miners\n‚îÇ   ‚îú‚îÄ‚îÄ dwave_miner.py        # D-Wave QPU miner\n‚îÇ   ‚îî‚îÄ‚îÄ dwave_sampler.py      # D-Wave sampler wrapper\n‚îú‚îÄ‚îÄ docker/                    # Docker deployment files\n‚îú‚îÄ‚îÄ tests/                     # Test suite\n‚îú‚îÄ‚îÄ reference/                 # Reference implementation\n‚îî‚îÄ‚îÄ benchmarks/                # Performance benchmarks\n```\n\n## quip-network-node\n\nRun a single P2P node of a specific type. Subcommands: cpu, gpu, qpu.\n\n- Always enables competitive mode\n- Implies a single miner of that type (num-sa/num-gpu/num-qpu = 1)\n- Supports a top-level --con",
      "default_branch": "main"
    },
    "fetched_at": "2026-02-15T03:37:51.884312"
  },
  {
    "basic_info": {
      "name": "DeepSeek-OCR-2",
      "full_name": "deepseek-ai/DeepSeek-OCR-2",
      "owner": "deepseek-ai",
      "description": "Visual Causal Flow",
      "url": "https://github.com/deepseek-ai/DeepSeek-OCR-2",
      "clone_url": "https://github.com/deepseek-ai/DeepSeek-OCR-2.git",
      "ssh_url": "git@github.com:deepseek-ai/DeepSeek-OCR-2.git",
      "homepage": "",
      "created_at": "2026-01-27T03:05:42Z",
      "updated_at": "2026-02-14T15:21:58Z",
      "pushed_at": "2026-02-03T00:34:18Z"
    },
    "stats": {
      "stars": 2247,
      "forks": 172,
      "watchers": 2247,
      "open_issues": 44,
      "size": 1075
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 107443
      },
      "license": "Apache License 2.0",
      "topics": []
    },
    "content": {
      "readme": "<!-- markdownlint-disable first-line-h1 -->\n<!-- markdownlint-disable html -->\n<!-- markdownlint-disable no-duplicate-header -->\n\n\n<div align=\"center\">\n  <img src=\"assets/logo.svg\" width=\"60%\" alt=\"DeepSeek AI\" />\n</div>\n\n\n<hr>\n<div align=\"center\">\n  <a href=\"https://www.deepseek.com/\" target=\"_blank\">\n    <img alt=\"Homepage\" src=\"assets/badge.svg\" />\n  </a>\n  <a href=\"https://huggingface.co/deepseek-ai/DeepSeek-OCR-2\" target=\"_blank\">\n    <img alt=\"Hugging Face\" src=\"https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-DeepSeek%20AI-ffc107?color=ffc107&logoColor=white\" />\n  </a>\n\n</div>\n\n<div align=\"center\">\n\n  <a href=\"https://discord.gg/Tc7c45Zzu5\" target=\"_blank\">\n    <img alt=\"Discord\" src=\"https://img.shields.io/badge/Discord-DeepSeek%20AI-7289da?logo=discord&logoColor=white&color=7289da\" />\n  </a>\n  <a href=\"https://twitter.com/deepseek_ai\" target=\"_blank\">\n    <img alt=\"Twitter Follow\" src=\"https://img.shields.io/badge/Twitter-deepseek_ai-white?logo=x&logoColor=white\" />\n  </a>\n\n</div>\n\n\n\n<p align=\"center\">\n  <a href=\"https://huggingface.co/deepseek-ai/DeepSeek-OCR-2\"><b>üì• Model Download</b></a> |\n  <a href=\"https://github.com/deepseek-ai/DeepSeek-OCR-2/blob/main/DeepSeek_OCR2_paper.pdf\"><b>üìÑ Paper Link</b></a> |\n  <a href=\"https://arxiv.org/abs/2601.20552\"><b>üìÑ Arxiv Paper Link</b></a> |\n</p>\n\n<h2>\n<p align=\"center\">\n  <a href=\"\">DeepSeek-OCR 2: Visual Causal Flow</a>\n</p>\n</h2>\n\n<p align=\"center\">\n<img src=\"assets/fig1.png\" style=\"width: 600px\" align=center>\n</p>\n<p align=\"center\">\n<a href=\"\">Explore more human-like visual encoding.</a>       \n</p>\n\n\n## Contents\n- [Install](#install)\n- [vLLM Inference](#vllm-inference)\n- [Transformers Inference](#transformers-inference)\n  \n\n\n\n\n## Install\n>Our environment is cuda11.8+torch2.6.0.\n1. Clone this repository and navigate to the DeepSeek-OCR-2 folder\n```bash\ngit clone https://github.com/deepseek-ai/DeepSeek-OCR-2.git\n```\n2. Conda\n```Shell\nconda create -n deepseek-ocr2 python=3.12.9 -y\nconda activate deepseek-ocr2\n```\n3. Packages\n\n- download the vllm-0.8.5 [whl](https://github.com/vllm-project/vllm/releases/tag/v0.8.5) \n```Shell\npip install torch==2.6.0 torchvision==0.21.0 torchaudio==2.6.0 --index-url https://download.pytorch.org/whl/cu118\npip install vllm-0.8.5+cu118-cp38-abi3-manylinux1_x86_64.whl\npip install -r requirements.txt\npip install flash-attn==2.7.3 --no-build-isolation\n```\n**Note:** if you want vLLM and transformers codes to run in the same environment, you don't need to worry about this installation error like: vllm 0.8.5+cu118 requires transformers>=4.51.1\n\n## vLLM-Inference\n- VLLM:\n>**Note:** change the INPUT_PATH/OUTPUT_PATH and other settings in the DeepSeek-OCR2-master/DeepSeek-OCR2-vllm/config.py\n```Shell\ncd DeepSeek-OCR2-master/DeepSeek-OCR2-vllm\n```\n1. image: streaming output\n```Shell\npython run_dpsk_ocr2_image.py\n```\n2. pdf: concurrency (on-par speed with DeepSeek-OCR)\n```Shell\npython run_dpsk_ocr2_pdf.py\n```\n3. batch eval for benchmarks (i.e., OmniDocBench v1.5)\n```Shell\npython run_dpsk_ocr2_eval_batch.py\n```\n\n## Transformers-Inference\n- Transformers\n```python\nfrom transformers import AutoModel, AutoTokenizer\nimport torch\nimport os\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\nmodel_name = 'deepseek-ai/DeepSeek-OCR-2'\n\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\nmodel = AutoModel.from_pretrained(model_name, _attn_implementation='flash_attention_2', trust_remote_code=True, use_safetensors=True)\nmodel = model.eval().cuda().to(torch.bfloat16)\n\n# prompt = \"<image>\\nFree OCR. \"\nprompt = \"<image>\\n<|grounding|>Convert the document to markdown. \"\nimage_file = 'your_image.jpg'\noutput_path = 'your/output/dir'\n\nres = model.infer(tokenizer, prompt=prompt, image_file=image_file, output_path = output_path, base_size = 1024, image_size = 768, crop_mode=True, save_results = True)\n```\nor you can\n```Shell\ncd DeepSeek-OCR2-master/DeepSeek-OCR2-hf\npython run_dpsk_ocr2.py\n```\n## Support-Modes\n- Dynamic resolution\n  - Default: (0-6)√ó768√ó768 + 1√ó1024√ó1024 ‚Äî (0-6)√ó144 + 256 visual tokens ‚úÖ\n\n## Main Prompts\n```python\n# document: <image>\\n<|grounding|>Convert the document to markdown.\n# without layouts: <image>\\nFree OCR.\n```\n\n\n\n\n## Acknowledgement\n\nWe would like to thank [DeepSeek-OCR](https://github.com/deepseek-ai/DeepSeek-OCR/), [Vary](https://github.com/Ucas-HaoranWei/Vary/), [GOT-OCR2.0](https://github.com/Ucas-HaoranWei/GOT-OCR2.0/), [MinerU](https://github.com/opendatalab/MinerU), [PaddleOCR](https://github.com/PaddlePaddle/PaddleOCR) for their valuable models.\n\nWe also appreciate the benchmark [OmniDocBench](https://github.com/opendatalab/OmniDocBench).\n\n## Citation\n\n```bibtex\n@article{wei2025deepseek,\n  title={DeepSeek-OCR: Contexts Optical Compression},\n  author={Wei, Haoran and Sun, Yaofeng and Li, Yukun},\n  journal={arXiv preprint arXiv:2510.18234},\n  year={2025}\n}\n@article{wei2026deepseek,\n  title={DeepSeek-OCR 2: Visual Causal Flow},\n  author={Wei, Haoran and Sun, Yaofeng and Li, Yukun},\n  journal={arXiv pr",
      "default_branch": "main"
    },
    "fetched_at": "2026-02-15T03:37:53.050139"
  },
  {
    "basic_info": {
      "name": "Trellis",
      "full_name": "mindfold-ai/Trellis",
      "owner": "mindfold-ai",
      "description": "All-in-one AI framework & toolkit for Claude Code & Cursor",
      "url": "https://github.com/mindfold-ai/Trellis",
      "clone_url": "https://github.com/mindfold-ai/Trellis.git",
      "ssh_url": "git@github.com:mindfold-ai/Trellis.git",
      "homepage": "https://trytrellis.app",
      "created_at": "2026-01-26T11:49:10Z",
      "updated_at": "2026-02-15T02:28:47Z",
      "pushed_at": "2026-02-14T02:56:01Z"
    },
    "stats": {
      "stars": 2205,
      "forks": 108,
      "watchers": 2205,
      "open_issues": 0,
      "size": 6324
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 546174,
        "Shell": 317862,
        "TypeScript": 287528,
        "JavaScript": 74070
      },
      "license": "GNU Affero General Public License v3.0",
      "topics": [
        "ai-agent",
        "ai-coding",
        "claude-code",
        "cli",
        "cursor",
        "developer-tools",
        "typescript",
        "workflow"
      ]
    },
    "content": {
      "readme": "<!--<p align=\"center\">\n<img src=\"assets/meme.png\" alt=\"AI Coding Problems Meme\" />\n</p>-->\n\n<p align=\"center\">\n<picture>\n<source srcset=\"assets/trellis.png\" media=\"(prefers-color-scheme: dark)\">\n<source srcset=\"assets/trellis.png\" media=\"(prefers-color-scheme: light)\">\n<img src=\"assets/trellis.png\" alt=\"Trellis Logo\" width=\"500\" style=\"image-rendering: -webkit-optimize-contrast; image-rendering: crisp-edges;\">\n</picture>\n</p>\n\n<p align=\"center\">\n<strong>All-in-one AI framework & toolkit for Claude Code, Cursor, iFlow & Codex</strong><br/>\n<sub>Wild AI ships nothing.</sub>\n</p>\n\n<p align=\"center\">\n<a href=\"https://www.npmjs.com/package/@mindfoldhq/trellis\"><img src=\"https://img.shields.io/npm/v/@mindfoldhq/trellis.svg?style=flat-square&color=blue\" alt=\"npm version\" /></a>\n<a href=\"https://github.com/mindfold-ai/Trellis/blob/main/LICENSE\"><img src=\"https://img.shields.io/badge/license-FSL-green.svg?style=flat-square\" alt=\"license\" /></a>\n<a href=\"https://github.com/mindfold-ai/Trellis/stargazers\"><img src=\"https://img.shields.io/github/stars/mindfold-ai/Trellis?style=flat-square&color=yellow\" alt=\"stars\" /></a>\n<a href=\"https://discord.com/invite/tWcCZ3aRHc\"><img src=\"https://img.shields.io/badge/Discord-Join-7289DA?style=flat-square&logo=discord&logoColor=white\" alt=\"Discord\" /></a>\n</p>\n\n<p align=\"center\">\n<a href=\"#quick-start\">Quick Start</a> ‚Ä¢\n<a href=\"#why-trellis\">Why Trellis</a> ‚Ä¢\n<a href=\"#use-cases\">Use Cases</a> ‚Ä¢\n<a href=\"#how-it-works\">How It Works</a> ‚Ä¢\n<a href=\"#faq\">FAQ</a>\n</p>\n\n## Why Trellis?\n\n| Feature | Problem Solved |\n| --- | --- |\n| **Auto-Injection** | Required specs and workflows auto-inject into every conversation. Write once, apply forever |\n| **Auto-updated Spec Library** | Best practices live in auto-updated spec files. The more you use it, the better it gets |\n| **Parallel Sessions** | Run multiple agents in tandem - each in its own worktree |\n| **Team Sync** | Share specs across your team. One person's best practice benefits everyone |\n| **Session Persistence** | Work traces persist in your repo. AI remembers project context across sessions |\n\n## Quick Start\n\n```bash\n# 1. Install globally\nnpm install -g @mindfoldhq/trellis@latest\n\n# 2. Initialize in your project directory\ntrellis init -u your-name\n\n# Or include iFlow CLI support\ntrellis init --iflow -u your-name\n\n# Or include Codex skills support\ntrellis init --codex -u your-name\n\n# 3. Start Claude Code and begin working\n```\n\n> `your-name` becomes your identifier and creates a personal workspace at `.trellis/workspace/your-name/`\n\n<p align=\"center\">\n<img src=\"assets/info.png\" alt=\"Trellis Initialization Example\" />\n</p>\n\n## Use Cases\n\n### Educating Your AI\n\nWrite your specs in Markdown. Trellis injects them into every AI session ‚Äî no more repeating yourself.\n\n<p align=\"center\">\n<img src=\"assets/usecase1.png\" alt=\"Teaching AI - Teach Once, Apply Forever\" />\n</p>\n\nDefine your component guidelines, file structure rules, and patterns once. AI automatically applies them when creating new code ‚Äî using TypeScript with Props interface, following PascalCase naming, building functional components with hooks.\n\n### Ship in Parallel\n\nSpawn multiple Claude sessions in isolated worktrees with `/trellis:parallel`. Work on several features at once, merge when ready.\n\n<p align=\"center\">\n<img src=\"assets/usecase2.png\" alt=\"Parallel Work - Multiple features developing simultaneously\" />\n</p>\n\nWhile coding, each worker runs in its own worktree (physically isolated directory), no blocking, no interference. Review and merge completed features while others are still in progress.\n\n### Custom Workflows\n\nDefine custom skills & commands that prepare Claude for specific tasks and contexts.\n\n<p align=\"center\">\n<img src=\"assets/usecase3.png\" alt=\"Workflows - Custom commands for instant context loading\" />\n</p>\n\nCreate commands like `/trellis:before-frontend-dev` that load component guidelines, check recent changes, pull in test patterns, and review shared hooks‚Äîall with a single slash.\n\n## How It Works\n\n### Project Structure\n\n```\n.trellis/\n‚îú‚îÄ‚îÄ workflow.md              # Workflow guide (auto-injected on start)\n‚îú‚îÄ‚îÄ worktree.yaml            # Multi-agent config (for /trellis:parallel)\n‚îú‚îÄ‚îÄ spec/                    # Spec library\n‚îÇ   ‚îú‚îÄ‚îÄ frontend/            #   Frontend specs\n‚îÇ   ‚îú‚îÄ‚îÄ backend/             #   Backend specs\n‚îÇ   ‚îî‚îÄ‚îÄ guides/              #   Decision & analysis frameworks\n‚îú‚îÄ‚îÄ workspace/{name}/        # Personal journal\n‚îú‚îÄ‚îÄ tasks/                   # Task management (progress tracking & more)\n‚îî‚îÄ‚îÄ scripts/                 # Utilities\n\n.claude/\n‚îú‚îÄ‚îÄ settings.json            # Hook configuration\n‚îú‚îÄ‚îÄ agents/                  # Agent definitions\n‚îÇ   ‚îú‚îÄ‚îÄ dispatch.md          #   Dispatch Agent (pure routing, doesn't read specs)\n‚îÇ   ‚îú‚îÄ‚îÄ implement.md         #   Implement Agent\n‚îÇ   ‚îú‚îÄ‚îÄ check.md             #   Check Agent\n‚îÇ   ‚îî‚îÄ‚îÄ research.md          #   Research Agent\n‚îú‚îÄ‚îÄ commands/                # Slash commands\n‚îî‚îÄ‚îÄ hooks/                   # Hook scripts\n ",
      "default_branch": "main"
    },
    "fetched_at": "2026-02-15T03:37:54.149573"
  },
  {
    "basic_info": {
      "name": "dash",
      "full_name": "agno-agi/dash",
      "owner": "agno-agi",
      "description": "Self-learning data agent that grounds its answers in 6 layers of context. Inspired by OpenAI's in-house implementation.",
      "url": "https://github.com/agno-agi/dash",
      "clone_url": "https://github.com/agno-agi/dash.git",
      "ssh_url": "git@github.com:agno-agi/dash.git",
      "homepage": "",
      "created_at": "2026-01-30T13:54:17Z",
      "updated_at": "2026-02-15T03:09:14Z",
      "pushed_at": "2026-02-01T19:53:23Z"
    },
    "stats": {
      "stars": 1638,
      "forks": 179,
      "watchers": 1638,
      "open_issues": 5,
      "size": 229
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 52528,
        "Shell": 10886,
        "Dockerfile": 662
      },
      "license": "Apache License 2.0",
      "topics": []
    },
    "content": {
      "readme": "# Dash\n\nDash is a **self-learning data agent** that grounds its answers in **6 layers of context** and improves with every run.\n\nInspired by [OpenAI's in-house data agent](https://openai.com/index/inside-our-in-house-data-agent/).\n\n## Quick Start\n\n```sh\n# Clone this repo\ngit clone https://github.com/agno-agi/dash.git && cd dash\n# Add OPENAI_API_KEY by adding to .env file or export OPENAI_API_KEY=sk-***\ncp example.env .env\n\n# Start the application\ndocker compose up -d --build\n\n# Load sample data and knowledge\ndocker exec -it dash-api python -m dash.scripts.load_data\ndocker exec -it dash-api python -m dash.scripts.load_knowledge\n```\n\nConfirm dash is running by navigation to [http://localhost:8000/docs](http://localhost:8000/docs).\n\n## Connect to the Web UI\n\n1. Open [os.agno.com](https://os.agno.com) and login\n2. Add OS ‚Üí Local ‚Üí `http://localhost:8000`\n3. Click \"Connect\"\n\n**Try it** (sample F1 dataset):\n\n- Who won the most F1 World Championships?\n- How many races has Lewis Hamilton won?\n- Compare Ferrari vs Mercedes points 2015-2020\n\n## Why Text-to-SQL Breaks in Practice\n\nOur goal is simple: ask a question in english, get a correct, meaningful answer. But raw LLMs writing SQL hit a wall fast:\n\n- **Schemas lack meaning.**\n- **Types are misleading.**\n- **Tribal knowledge is missing.**\n- **No way to learn from mistakes.**\n- **Results generally lack interpretation.**\n\nThe root cause is missing context and missing memory.\n\nDash solves this with **6 layers of grounded context**, a **self-learning loop** that improves with every query, and a focus on **understanding your question** to deliver insights you can act on.\n\n## The Six Layers of Context\n\n| Layer | Purpose | Source |\n|------|--------|--------|\n| **Table Usage** | Schema, columns, relationships | `knowledge/tables/*.json` |\n| **Human Annotations** | Metrics, definitions, and business rules | `knowledge/business/*.json` |\n| **Query Patterns** | SQL that is known to work | `knowledge/queries/*.sql` |\n| **Institutional Knowledge** | Docs, wikis, external references | MCP (optional) |\n| **Learnings** | Error patterns and discovered fixes | Agno `Learning Machine` |\n| **Runtime Context** | Live schema changes | `introspect_schema` tool |\n\nThe agent retrieves relevant context at query time via hybrid search, then generates SQL grounded in patterns that already work.\n\n## The Self-Learning Loop\n\nDash improves without retraining or fine-tuning. We call this gpu-poor continuous learning.\n\nIt learns through two complementary systems:\n\n| System | Stores | How It Evolves |\n|------|--------|----------------|\n| **Knowledge** | Validated queries and business context | Curated by you + dash |\n| **Learnings** | Error patterns and fixes | Managed by `Learning Machine` automatically |\n\n```\nUser Question\n     ‚Üì\nRetrieve Knowledge + Learnings\n     ‚Üì\nReason about intent\n     ‚Üì\nGenerate grounded SQL\n     ‚Üì\nExecute and interpret\n     ‚Üì\n ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n ‚Üì         ‚Üì\nSuccess    Error\n ‚Üì         ‚Üì\n ‚Üì         Diagnose ‚Üí Fix ‚Üí Save Learning\n ‚Üì                           (never repeated)\n ‚Üì\nReturn insight\n ‚Üì\nOptionally save as Knowledge\n```\n\n**Knowledge** is curated‚Äîvalidated queries and business context you want the agent to build on.\n\n**Learnings** is discovered‚Äîpatterns the agent finds through trial and error. When a query fails because `position` is TEXT not INTEGER, the agent saves that gotcha. Next time, it knows.\n\n## Insights, Not Just Rows\n\nDash reasons about what makes an answer useful, not just technically correct.\n\n**Question:**\nWho won the most races in 2019?\n\n| Typical SQL Agent | Dash |\n|------------------|------|\n| `Hamilton: 11` | Lewis Hamilton dominated 2019 with **11 wins out of 21 races**, more than double Bottas‚Äôs 4 wins. This performance secured his sixth world championship. |\n\n## Deploy to Railway\n\n```sh\nrailway login\n\n./scripts/railway_up.sh\n```\n\n### Production Operations\n\n**Load data and knowledge:**\n```sh\nrailway run python -m dash.scripts.load_data\nrailway run python -m dash.scripts.load_knowledge\n```\n\n**View logs:**\n\n```sh\nrailway logs --service dash\n```\n\n**Run commands in production:**\n\n```sh\nrailway run python -m dash  # CLI mode\n```\n\n**Redeploy after changes:**\n\n```sh\nrailway up --service dash -d\n```\n\n**Open dashboard:**\n```sh\nrailway open\n```\n\n## Adding Knowledge\n\nDash works best when it understands how your organization talks about data.\n\n```\nknowledge/\n‚îú‚îÄ‚îÄ tables/      # Table meaning and caveats\n‚îú‚îÄ‚îÄ queries/     # Proven SQL patterns\n‚îî‚îÄ‚îÄ business/    # Metrics and language\n```\n\n### Table Metadata\n\n```\n{\n  \"table_name\": \"orders\",\n  \"table_description\": \"Customer orders with denormalized line items\",\n  \"use_cases\": [\"Revenue reporting\", \"Customer analytics\"],\n  \"data_quality_notes\": [\n    \"created_at is UTC\",\n    \"status values: pending, completed, refunded\",\n    \"amount stored in cents\"\n  ]\n}\n```\n\n### Query Patterns\n\n```\n-- <query name>monthly_revenue</query name>\n-- <query description>\n-- Monthly revenue calculation.\n-- Converts cents to dollars.\n-- Excludes ",
      "default_branch": "main"
    },
    "fetched_at": "2026-02-15T03:37:55.289966"
  },
  {
    "basic_info": {
      "name": "GLM-OCR",
      "full_name": "zai-org/GLM-OCR",
      "owner": "zai-org",
      "description": "GLM-OCR: Accurate √ó  Fast √ó Comprehensive",
      "url": "https://github.com/zai-org/GLM-OCR",
      "clone_url": "https://github.com/zai-org/GLM-OCR.git",
      "ssh_url": "git@github.com:zai-org/GLM-OCR.git",
      "homepage": "",
      "created_at": "2026-02-02T12:59:43Z",
      "updated_at": "2026-02-15T01:47:22Z",
      "pushed_at": "2026-02-12T13:12:21Z"
    },
    "stats": {
      "stars": 1559,
      "forks": 99,
      "watchers": 1559,
      "open_issues": 31,
      "size": 100609
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 455938,
        "TypeScript": 306509,
        "CSS": 10590,
        "Shell": 6033,
        "Dockerfile": 3009,
        "HTML": 555
      },
      "license": "Apache License 2.0",
      "topics": [
        "glm",
        "image2text",
        "ocr"
      ]
    },
    "content": {
      "readme": "## GLM-OCR\n\n<div align=\"center\">\n<img src=resources/logo.svg width=\"40%\"/>\n</div>\n<p align=\"center\">\n    üëã Join our <a href=\"resources/WECHAT.md\" target=\"_blank\">WeChat</a> and <a href=\"https://discord.gg/QR7SARHRxK\" target=\"_blank\">Discord</a> community\n    <br>\n    üìç Use GLM-OCR's <a href=\"https://docs.z.ai/guides/vlm/glm-ocr\" target=\"_blank\">API</a>\n</p>\n\n<div align=\"center\">\n  <a href=\"README_zh.md\">ÁÆÄ‰Ωì‰∏≠Êñá</a> | English\n</div>\n\n### Model Introduction\n\nGLM-OCR is a multimodal OCR model for complex document understanding, built on the GLM-V encoder‚Äìdecoder architecture. It introduces Multi-Token Prediction (MTP) loss and stable full-task reinforcement learning to improve training efficiency, recognition accuracy, and generalization. The model integrates the CogViT visual encoder pre-trained on large-scale image‚Äìtext data, a lightweight cross-modal connector with efficient token downsampling, and a GLM-0.5B language decoder. Combined with a two-stage pipeline of layout analysis and parallel recognition based on PP-DocLayout-V3, GLM-OCR delivers robust and high-quality OCR performance across diverse document layouts.\n\n**Key Features**\n\n- **State-of-the-Art Performance**: Achieves a score of 94.62 on OmniDocBench V1.5, ranking #1 overall, and delivers state-of-the-art results across major document understanding benchmarks, including formula recognition, table recognition, and information extraction.\n\n- **Optimized for Real-World Scenarios**: Designed and optimized for practical business use cases, maintaining robust performance on complex tables, code-heavy documents, seals, and other challenging real-world layouts.\n\n- **Efficient Inference**: With only 0.9B parameters, GLM-OCR supports deployment via vLLM, SGLang, and Ollama, significantly reducing inference latency and compute cost, making it ideal for high-concurrency services and edge deployments.\n\n- **Easy to Use**: Fully open-sourced and equipped with a comprehensive [SDK](https://github.com/zai-org/GLM-OCR) and inference toolchain, offering simple installation, one-line invocation, and smooth integration into existing production pipelines.\n\n### News & Updates\n\n- **[Coming Soon]** GLM-OCR Technical Report\n- **[2026.2.12]** Fine-tuning tutorial based on LLaMA-Factory is now available. See: [GLM-OCR Fine-tuning Guide](examples/finetune/README.md)\n\n### Download Model\n\n| Model   | Download Links                                                                                                              | Precision |\n| ------- | --------------------------------------------------------------------------------------------------------------------------- | --------- |\n| GLM-OCR | [ü§ó Hugging Face](https://huggingface.co/zai-org/GLM-OCR)<br> [ü§ñ ModelScope](https://modelscope.cn/models/ZhipuAI/GLM-OCR) | BF16      |\n\n## GLM-OCR SDK\n\nWe provide an SDK for using GLM-OCR more efficiently and conveniently.\n\n### Install SDK\n\n> [UV Installation](https://docs.astral.sh/uv/getting-started/installation/)\n\n```bash\n# Install from source\ngit clone https://github.com/zai-org/glm-ocr.git\ncd glm-ocr\nuv venv --python 3.12 --seed && source .venv/bin/activate\nuv pip install -e .\n```\n\n### Model Deployment\n\nTwo ways to use GLM-OCR:\n\n#### Option 1: Zhipu MaaS API (Recommended for Quick Start)\n\nUse the hosted cloud API ‚Äì no GPU needed. The cloud service runs the complete GLM-OCR pipeline internally, so the SDK simply forwards your request and returns the result.\n\n1. Get an API key from https://open.bigmodel.cn\n2. Configure `config.yaml`:\n\n```yaml\npipeline:\n  maas:\n    enabled: true # Enable MaaS mode\n    api_key: your-api-key # Required\n```\n\nThat's it! When `maas.enabled=true`, the SDK acts as a thin wrapper that:\n\n- Forwards your documents to the Zhipu cloud API\n- Returns the results directly (Markdown + JSON layout details)\n- No local processing, no GPU required\n\nInput note (MaaS): the upstream API accepts `file` as a URL or a `data:<mime>;base64,...` data URI.\nIf you have raw base64 without the `data:` prefix, wrap it as a data URI (recommended). The SDK will\nauto-wrap local file paths / bytes / raw base64 into a data URI when calling MaaS.\n\nAPI documentation: https://docs.bigmodel.cn/cn/guide/models/vlm/glm-ocr\n\n#### Option 2: Self-host with vLLM / SGLang\n\nDeploy the GLM-OCR model locally for full control. The SDK provides the complete pipeline: layout detection, parallel region OCR, and result formatting.\n\n##### Using vLLM\n\nInstall vLLM:\n\n```bash\nuv pip install -U vllm --torch-backend=auto --extra-index-url https://wheels.vllm.ai/nightly\n# Or use Docker\ndocker pull vllm/vllm-openai:nightly\n```\n\nLaunch the service:\n\n```bash\n# In docker container, uv may not be need for transformers install\nuv pip install git+https://github.com/huggingface/transformers.git\n\n# Run with MTP for better performance\nvllm serve zai-org/GLM-OCR --allowed-local-media-path / --port 8080 --speculative-config '{\"method\": \"mtp\", \"num_speculative_tokens\": 1}' --served-model-name glm-ocr\n```\n\n##### Using SGLang\n\nIn",
      "default_branch": "main"
    },
    "fetched_at": "2026-02-15T03:37:56.412190"
  },
  {
    "basic_info": {
      "name": "Qwen3-ASR",
      "full_name": "QwenLM/Qwen3-ASR",
      "owner": "QwenLM",
      "description": "Qwen3-ASR is an open-source series of ASR models developed by the Qwen team at Alibaba Cloud, supporting stable multilingual speech/music/song recognition, language detection and timestamp prediction.",
      "url": "https://github.com/QwenLM/Qwen3-ASR",
      "clone_url": "https://github.com/QwenLM/Qwen3-ASR.git",
      "ssh_url": "git@github.com:QwenLM/Qwen3-ASR.git",
      "homepage": null,
      "created_at": "2026-01-28T05:44:59Z",
      "updated_at": "2026-02-15T03:14:22Z",
      "pushed_at": "2026-01-30T03:24:24Z"
    },
    "stats": {
      "stars": 1514,
      "forks": 124,
      "watchers": 1514,
      "open_issues": 31,
      "size": 2336
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 231743
      },
      "license": "Apache License 2.0",
      "topics": []
    },
    "content": {
      "readme": "# Qwen3-ASR\n\n<br>\n\n<p align=\"center\">\n    <img src=\"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-ASR-Repo/logo.png\" width=\"400\"/>\n<p>\n\n<p align=\"center\">\n&nbsp&nbspü§ó <a href=\"https://huggingface.co/collections/Qwen/qwen3-asr\">Hugging Face</a>&nbsp&nbsp | &nbsp&nbspü§ñ <a href=\"https://modelscope.cn/collections/Qwen/Qwen3-ASR\">ModelScope</a>&nbsp&nbsp | &nbsp&nbspüìë <a href=\"https://qwen.ai/blog?id=qwen3asr\">Blog</a>&nbsp&nbsp | &nbsp&nbspüìë <a href=\"https://arxiv.org/abs/2601.21337\">Paper</a>&nbsp&nbsp\n<br>\nüñ•Ô∏è <a href=\"https://huggingface.co/spaces/Qwen/Qwen3-ASR\">Hugging Face Demo</a>&nbsp&nbsp | &nbsp&nbsp üñ•Ô∏è <a href=\"https://modelscope.cn/studios/Qwen/Qwen3-ASR\">ModelScope Demo</a>&nbsp&nbsp | &nbsp&nbspüí¨ <a href=\"https://github.com/QwenLM/Qwen/blob/main/assets/wechat.png\">WeChat (ÂæÆ‰ø°)</a>&nbsp&nbsp | &nbsp&nbspü´® <a href=\"https://discord.gg/CV4E9rpNSD\">Discord</a>&nbsp&nbsp | &nbsp&nbspüìë <a href=\"https://help.aliyun.com/zh/model-studio/qwen-speech-recognition\">API</a>\n\n</p>\n\nWe release **Qwen3-ASR**, a family that includes two powerful all-in-one speech recognition models that support language identification and ASR for 52 languages and dialects, as well as a novel non-autoregressive speech forced-alignment model that can align text‚Äìspeech pairs in 11 languages.\n\n\n## News\n* 2026.1.29: üéâüéâüéâ We have released the [Qwen3-ASR](https://huggingface.co/collections/Qwen/qwen3-asr) series (0.6B/1.7B) and the Qwen3-ForcedAligner-0.6B model. Please check out our [blog](https://qwen.ai/blog?id=qwen3asr)!\n\n\n## Contents <!-- omit in toc -->\n\n- [Overview](#overview)\n  - [Introduction](#introduction)\n  - [Model Architecture](#model-architecture)\n  - [Released Models Description and Download](#released-models-description-and-download)\n- [Quickstart](#quickstart)\n  - [Environment Setup](#environment-setup)\n  - [Python Package Usage](#python-package-usage)\n    - [Quick Inference](#quick-inference)\n    - [vLLM Backend](#vllm-backend)\n    - [Streaming Inference](#streaming-inference)\n    - [ForcedAligner Usage](#forcedaligner-usage)\n  - [DashScope API Usage](#dashscope-api-usage)\n- [Launch Local Web UI Demo](#launch-local-web-ui-demo)\n  - [Gradio Demo](#gradio-demo)\n  - [Streaming Demo](#streaming-demo)\n- [Deployment with vLLM](#deployment-with-vllm)\n- [Fine Tuning](#fine-tuning)\n- [Docker](#docker)\n- [Evaluation](#evaluation)\n- [Citation](#citation)\n\n\n## Overview\n\n### Introduction\n\n<p align=\"center\">\n    <img src=\"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-ASR-Repo/qwen3_asr_introduction.png\" width=\"90%\"/>\n<p>\n\nThe Qwen3-ASR family includes Qwen3-ASR-1.7B and Qwen3-ASR-0.6B, which support language identification and ASR for 52 languages and dialects. Both leverage large-scale speech training data and the strong audio understanding capability of their foundation model, Qwen3-Omni. Experiments show that the 1.7B version achieves state-of-the-art performance among open-source ASR models and is competitive with the strongest proprietary commercial APIs. Here are the main features:\n\n* **All-in-one**: Qwen3-ASR-1.7B and Qwen3-ASR-0.6B support language identification and speech recognition for 30 languages and 22 Chinese dialects, so as to English accents from multiple countries and regions.\n\n* **Excellent and Fast**: The Qwen3-ASR family ASR models maintains high-quality and robust recognition under complex acoustic environments and challenging text patterns. Qwen3-ASR-1.7B achieves strong performance on both open-sourced and internal benchmarks. While the 0.6B version achieves accuracy-efficient trade-off, it reaches 2000 times throughput at a concurrency of 128. They both achieve streaming / offline unified inference with single model and support transcribe long audio.\n\n* **Novel and strong forced alignment Solution**: We introduce Qwen3-ForcedAligner-0.6B, which supports timestamp prediction for arbitrary units within up to 5 minutes of speech in 11 languages. Evaluations show its timestamp accuracy surpasses E2E based forced-alignment models.\n\n* **Comprehensive inference toolkit**: In addition to open-sourcing the architectures and weights of the Qwen3-ASR series, we also release a powerful, full-featured inference framework that supports vLLM-based batch inference, asynchronous serving, streaming inference, timestamp prediction, and more.\n\n### Model Architecture\n\n<p align=\"center\">\n    <img src=\"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-ASR-Repo/overview.jpg\" width=\"100%\"/>\n<p>\n\n\n### Released Models Description and Download\n\nBelow is an introduction and download information for the Qwen3-ASR models. Please select and download the model that fits your needs.\n\n| Model | Supported Languages | Supported Dialects | Inference Mode | Audio Types |\n|---|---|---|---|---|\n| Qwen3-ASR-1.7B & Qwen3-ASR-0.6B | Chinese (zh), English (en), Cantonese (yue), Arabic (ar), German (de), French (fr), Spanish (es), Portuguese (pt), Indonesian (id), Italian (it), Korean (ko), Russian (ru), Thai (th), Vietnamese (vi), Japan",
      "default_branch": "main"
    },
    "fetched_at": "2026-02-15T03:37:57.540903"
  },
  {
    "basic_info": {
      "name": "Edit-Banana",
      "full_name": "BIT-DataLab/Edit-Banana",
      "owner": "BIT-DataLab",
      "description": "Edit Banana: A framework for converting statistical formats into editable.",
      "url": "https://github.com/BIT-DataLab/Edit-Banana",
      "clone_url": "https://github.com/BIT-DataLab/Edit-Banana.git",
      "ssh_url": "git@github.com:BIT-DataLab/Edit-Banana.git",
      "homepage": "https://editbanana.anxin6.cn/",
      "created_at": "2026-01-16T10:34:52Z",
      "updated_at": "2026-02-15T03:25:40Z",
      "pushed_at": "2026-02-14T03:47:58Z"
    },
    "stats": {
      "stars": 1487,
      "forks": 62,
      "watchers": 1487,
      "open_issues": 5,
      "size": 10516
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 450318
      },
      "license": null,
      "topics": [
        "ai",
        "data",
        "figure",
        "llm",
        "nanobanana",
        "open-source",
        "python",
        "pythonprogramming"
      ]
    },
    "content": {
      "readme": "<p align=\"center\">\n  <img src=\"/static/banana.jpg\" width=\"180\" alt=\"Edit Banana Logo\"/>\n</p>\n\n<h1 align=\"center\">üçå Edit Banana</h1>\n<h3 align=\"center\">Universal Content Re-Editor: Make the Uneditable, Editable</h3>\n\n<p align=\"center\">\nBreak free from static formats. Our platform empowers you to transform fixed content into fully manipulatable assets.\nPowered by SAM 3 and multimodal large models, it enables high-fidelity reconstruction that preserves the original diagram details and logical relationships.\n</p>\n\n<p align=\"center\">\n  <a href=\"https://www.python.org/\"><img src=\"https://img.shields.io/badge/Python-3.10+-3776AB?style=flat-square&logo=python&logoColor=white\" alt=\"Python\"/></a>\n  <a href=\"LICENSE\"><img src=\"https://img.shields.io/badge/License-Apache_2.0-2F80ED?style=flat-square&logo=apache&logoColor=white\" alt=\"License\"/></a>\n  <a href=\"https://developer.nvidia.com/cuda-downloads\"><img src=\"https://img.shields.io/badge/GPU-CUDA%20Recommended-76B900?style=flat-square&logo=nvidia\" alt=\"CUDA\"/></a>\n  <a href=\"#-join-wechat-group\"><img src=\"https://img.shields.io/badge/WeChat-Join%20Group-07C160?style=flat-square&logo=wechat&logoColor=white\" alt=\"WeChat\"/></a>\n  <a href=\"https://github.com/BIT-DataLab/Edit-Banana/stargazers\"><img src=\"https://img.shields.io/github/stars/BIT-DataLab/Edit-Banana?style=flat-square&logo=github\" alt=\"GitHub stars\"/></a>\n</p>\n\n---\n\n<h3 align=\"center\">Try It Now!</h3>\n<p align=\"center\">\n  <a href=\"https://editbanana.anxin6.cn/\">\n    <img src=\"https://img.shields.io/badge/üöÄ%20Try%20Online%20Demo-editbanana.anxin6.cn-FF6B6B?style=for-the-badge&logoColor=white\" alt=\"Try Online Demo\"/>\n  </a>\n</p>\n\n<p align=\"center\">\n  üëÜ <b>Click above or https://editbanana.anxin6.cn/ to try Edit Banana online!</b> Upload an image or pdf, get <b>editable DrawIO (XML) or PPTX</b> in seconds. \n  <b>Please note</b>: Our GitHub repository currently trails behind our web-based service. For the most up-to-date features and performance, we recommend using our web platform.\n</p>\n\n---\n\n## üì∏ Effect Demonstration\n### High-Definition Input-Output Comparison (3 Typical Scenarios)\nTo demonstrate the high-fidelity conversion effect, we provides one-to-one comparisons between 3 scenarios of \"original static formats\" and \"editable reconstruction results\". All elements can be individually dragged, styled, and modified.\n\n#### Scenario 1: Figures to Drawio(xml, svg, pptx)\n\n| Example No. | Original Static Diagram (Input ¬∑ Non-editable) | DrawIO Reconstruction Result (Output ¬∑ Fully Editable) |\n|--------------|-----------------------------------------------|--------------------------------------------------------|\n| Example 1: Basic Flowchart | <img src=\"/static/demo/original_1.jpg\" width=\"400\" alt=\"Original Diagram 1\" style=\"border: 1px solid #eee; border-radius: 4px;\"/> | <img src=\"/static/demo/recon_1.png\" width=\"400\" alt=\"Reconstruction Result 1\" style=\"border: 1px solid #eee; border-radius: 4px;\"/> |\n| Example 2: Multi-level Architecture Diagram | <img src=\"/static/demo/original_2.png\" width=\"400\" alt=\"Original Diagram 2\" style=\"border: 1px solid #eee; border-radius: 4px;\"/> | <img src=\"/static/demo/recon_2.png\" width=\"400\" alt=\"Reconstruction Result 2\" style=\"border: 1px solid #eee; border-radius: 4px;\"/> |\n| Example 3: Technical Schematic | <img src=\"/static/demo/original_3.jpg\" width=\"400\" alt=\"Original Diagram 3\" style=\"border: 1px solid #eee; border-radius: 4px;\"/> | <img src=\"/static/demo/recon_3.png\" width=\"400\" alt=\"Reconstruction Result 3\" style=\"border: 1px solid #eee; border-radius: 4px;\"/> |\n| Example 4: Scientific Formula Diagram | <img src=\"/static/demo/original_4.jpg\" width=\"400\" alt=\"Original Diagram 4\" style=\"border: 1px solid #eee; border-radius: 4px;\"/> | <img src=\"/static/demo/recon_4.png\" width=\"400\" alt=\"Reconstruction Result 4\" style=\"border: 1px solid #eee; border-radius: 4px;\"/> |\n\n#### Scenario 2: PDF to PPTX\n\n\n#### Scenario 3: Human in the Loop Modification\n\n> ‚ú® Conversion Highlights:\n> 1.  Preserves the layout logic, color matching, and element hierarchy of the original diagram\n> 2.  1:1 restoration of shape stroke/fill and arrow styles (dashed lines/thickness)\n> 3.  Accurate text recognition, supporting direct subsequent editing and format adjustment\n> 4.  All elements are independently selectable, supporting native DrawIO template replacement and layout optimization\n\n## Key Features\n\n*   **Advanced Segmentation**: Using our fine-tuned **SAM 3 (Segment Anything Model 3)** for segmentation of diagram elements.\n*   **Fixed Multi-Round VLM Scanning**: An extraction process guided by **Multimodal LLMs (Qwen-VL/GPT-4V)**.\n*   **High-Quality OCR**:\n    *   **Azure Document Intelligence** for precise text localization.\n    *   **Fallback Mechanism**: Automatically switches to VLM-based end-to-end OCR if Azure services are unreachable.\n    *   **Mistral Vision/MLLM** for correcting text and converting mathematical formulas to **LaTeX** ($\\int f(x) dx$).\n    *   **Crop-Guided Strategy**: Extr",
      "default_branch": "main"
    },
    "fetched_at": "2026-02-15T03:37:58.670337"
  },
  {
    "basic_info": {
      "name": "Youtube-clipper-skill",
      "full_name": "op7418/Youtube-clipper-skill",
      "owner": "op7418",
      "description": null,
      "url": "https://github.com/op7418/Youtube-clipper-skill",
      "clone_url": "https://github.com/op7418/Youtube-clipper-skill.git",
      "ssh_url": "git@github.com:op7418/Youtube-clipper-skill.git",
      "homepage": null,
      "created_at": "2026-01-22T03:28:37Z",
      "updated_at": "2026-02-15T02:25:29Z",
      "pushed_at": "2026-01-22T04:56:21Z"
    },
    "stats": {
      "stars": 1346,
      "forks": 216,
      "watchers": 1346,
      "open_issues": 7,
      "size": 56
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 57805,
        "Shell": 6975
      },
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "# YouTube Clipper Skill\n\n> AI-powered YouTube video clipper for Claude Code. Download videos, generate semantic chapters, clip segments, translate subtitles to bilingual format, and burn subtitles into videos.\n\n[![License](https://img.shields.io/badge/license-MIT-blue.svg)](LICENSE)\n[![Python](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)\n\nEnglish | [ÁÆÄ‰Ωì‰∏≠Êñá](README.zh-CN.md)\n\n[Features](#features) ‚Ä¢ [Installation](#installation) ‚Ä¢ [Usage](#usage) ‚Ä¢ [Requirements](#requirements) ‚Ä¢ [Configuration](#configuration) ‚Ä¢ [Troubleshooting](#troubleshooting)\n\n---\n\n## Features\n\n- **AI Semantic Analysis** - Generate fine-grained chapters (2-5 minutes each) by understanding video content, not just mechanical time splitting\n- **Precise Clipping** - Use FFmpeg to extract video segments with frame-accurate timing\n- **Bilingual Subtitles** - Batch translate subtitles to Chinese/English with 95% API call reduction\n- **Subtitle Burning** - Hardcode bilingual subtitles into videos with customizable styling\n- **Content Summarization** - Auto-generate social media content (Xiaohongshu, Douyin, WeChat)\n\n---\n\n## Installation\n\n### Option 1: npx skills (Recommended)\n\n```bash\nnpx skills add https://github.com/op7418/Youtube-clipper-skill\n```\n\nThis command will automatically install the skill to `~/.claude/skills/youtube-clipper/`.\n\n### Option 2: Manual Installation\n\n```bash\ngit clone https://github.com/op7418/Youtube-clipper-skill.git\ncd Youtube-clipper-skill\nbash install_as_skill.sh\n```\n\nThe install script will:\n- Copy files to `~/.claude/skills/youtube-clipper/`\n- Install Python dependencies (yt-dlp, pysrt, python-dotenv)\n- Check system dependencies (Python, yt-dlp, FFmpeg)\n- Create `.env` configuration file\n\n---\n\n## Requirements\n\n### System Dependencies\n\n| Dependency | Version | Purpose | Installation |\n|------------|---------|---------|--------------|\n| **Python** | 3.8+ | Script execution | [python.org](https://www.python.org/downloads/) |\n| **yt-dlp** | Latest | YouTube download | `brew install yt-dlp` (macOS)<br>`sudo apt install yt-dlp` (Ubuntu)<br>`pip install yt-dlp` (pip) |\n| **FFmpeg with libass** | Latest | Video processing & subtitle burning | `brew install ffmpeg-full` (macOS)<br>`sudo apt install ffmpeg libass-dev` (Ubuntu) |\n\n### Python Packages\n\nThese are automatically installed by the install script:\n- `yt-dlp` - YouTube downloader\n- `pysrt` - SRT subtitle parser\n- `python-dotenv` - Environment variable management\n\n### Important: FFmpeg libass Support\n\n**macOS users**: The standard `ffmpeg` package from Homebrew does NOT include libass support (required for subtitle burning). You must install `ffmpeg-full`:\n\n```bash\n# Remove standard ffmpeg (if installed)\nbrew uninstall ffmpeg\n\n# Install ffmpeg-full (includes libass)\nbrew install ffmpeg-full\n```\n\n**Verify libass support**:\n```bash\nffmpeg -filters 2>&1 | grep subtitles\n# Should output: subtitles    V->V  (...)\n```\n\n---\n\n## Usage\n\n### In Claude Code\n\nSimply tell Claude to clip a YouTube video:\n\n```\nClip this YouTube video: https://youtube.com/watch?v=VIDEO_ID\n```\n\nor\n\n```\nÂâ™ËæëËøô‰∏™ YouTube ËßÜÈ¢ëÔºöhttps://youtube.com/watch?v=VIDEO_ID\n```\n\n### Workflow\n\n1. **Environment Check** - Verifies yt-dlp, FFmpeg, and Python dependencies\n2. **Video Download** - Downloads video (up to 1080p) and English subtitles\n3. **AI Chapter Analysis** - Claude analyzes subtitles to generate semantic chapters (2-5 min each)\n4. **User Selection** - Choose which chapters to clip and processing options\n5. **Processing** - Clips video, translates subtitles, burns subtitles (if requested)\n6. **Output** - Organized files in `./youtube-clips/<timestamp>/`\n\n### Output Files\n\nFor each clipped chapter:\n\n```\n./youtube-clips/20260122_143022/\n‚îî‚îÄ‚îÄ Chapter_Title/\n    ‚îú‚îÄ‚îÄ Chapter_Title_clip.mp4              # Original clip (no subtitles)\n    ‚îú‚îÄ‚îÄ Chapter_Title_with_subtitles.mp4    # With burned subtitles\n    ‚îú‚îÄ‚îÄ Chapter_Title_bilingual.srt         # Bilingual subtitle file\n    ‚îî‚îÄ‚îÄ Chapter_Title_summary.md            # Social media content\n```\n\n---\n\n## Configuration\n\nThe skill uses environment variables for customization. Edit `~/.claude/skills/youtube-clipper/.env`:\n\n### Key Settings\n\n```bash\n# FFmpeg path (auto-detected if empty)\nFFMPEG_PATH=\n\n# Output directory (default: current working directory)\nOUTPUT_DIR=./youtube-clips\n\n# Video quality limit (720, 1080, 1440, 2160)\nMAX_VIDEO_HEIGHT=1080\n\n# Translation batch size (20-25 recommended)\nTRANSLATION_BATCH_SIZE=20\n\n# Target language for translation\nTARGET_LANGUAGE=‰∏≠Êñá\n\n# Target chapter duration in seconds (180-300 recommended)\nTARGET_CHAPTER_DURATION=180\n```\n\nFor full configuration options, see [.env.example](.env.example).\n\n---\n\n## Examples\n\n### Example 1: Extract highlights from a tech interview\n\n**Input**:\n```\nClip this video: https://youtube.com/watch?v=Ckt1cj0xjRM\n```\n\n**Output** (AI-generated chapters):\n```\n1. [00:00 - 03:15] AGI as an exponential curve, not a point in time\n2. [03:15 - 06:30] China's gap in AI development\n3",
      "default_branch": "main"
    },
    "fetched_at": "2026-02-15T03:37:59.772893"
  },
  {
    "basic_info": {
      "name": "ComfyUI-Qwen-TTS",
      "full_name": "flybirdxx/ComfyUI-Qwen-TTS",
      "owner": "flybirdxx",
      "description": "A Simple Implementation of Qwen3-TTS's ComfyUI",
      "url": "https://github.com/flybirdxx/ComfyUI-Qwen-TTS",
      "clone_url": "https://github.com/flybirdxx/ComfyUI-Qwen-TTS.git",
      "ssh_url": "git@github.com:flybirdxx/ComfyUI-Qwen-TTS.git",
      "homepage": null,
      "created_at": "2026-01-22T18:38:52Z",
      "updated_at": "2026-02-15T01:34:08Z",
      "pushed_at": "2026-02-05T07:36:24Z"
    },
    "stats": {
      "stars": 1032,
      "forks": 103,
      "watchers": 1032,
      "open_issues": 38,
      "size": 1274
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 495452
      },
      "license": null,
      "topics": []
    },
    "content": {
      "readme": "# ComfyUI-Qwen-TTS\r\n\r\nEnglish | [‰∏≠ÊñáÁâà](README_CN.md)\r\n\r\n![Nodes Screenshot](example/example.png)\r\n\r\nComfyUI custom nodes for speech synthesis, voice cloning, and voice design, based on the open-source **Qwen3-TTS** project by the Alibaba Qwen team.\r\n\r\n## üìã Changelog\r\n\r\n- **2026-02-04**: Feature Update: Added Global Pause Control (`QwenTTSConfigNode`) and `extra_model_paths.yaml` support ([update.md](doc/update.md))\r\n- **2026-01-29**: Feature Update: Support for loading custom fine-tuned models & speakers ([update.md](doc/update.md))\r\n  - *Note: Fine-tuning is currently experimental; zero-shot cloning is recommended for best results.*\r\n- **2026-01-27**: UI Optimization: Sleek LoadSpeaker UI; fixed PyTorch 2.6+ compatibility ([update.md](doc/update.md))\r\n- **2026-01-26**: Functional Update: New voice persistence system (SaveVoice / LoadSpeaker) ([update.md](doc/update.md))\r\n- **2026-01-24**: Added attention mechanism selection & model memory management features ([update.md](doc/update.md))\r\n- **2026-01-24**: Added generation parameters (top_p, top_k, temperature, repetition_penalty) to all TTS nodes ([update.md](doc/update.md))\r\n- **2026-01-23**: Dependency compatibility & Mac (MPS) support, New nodes: VoiceClonePromptNode, DialogueInferenceNode ([update.md](doc/update.md))\r\n\r\n## Online Workflows\r\n\r\n- **Qwen3-TTS Multi-Role Multi-Round Dialogue Generation Workflow**:\r\n  - [workflow](https://www.runninghub.ai/post/2014703508829769729/?inviteCode=rh-v1041)\r\n- **Qwen3-TTS 3-in-1 (Clone, Design, Custom) Workflow**:\r\n  - [workflow](https://www.runninghub.ai/post/2014962110224142337/?inviteCode=rh-v1041)\r\n\r\n## Key Features\r\n\r\n- üéµ **Speech Synthesis**: High-quality text-to-speech conversion.\r\n- üé≠ **Voice Cloning**: Zero-shot voice cloning from short reference audio.\r\n- üé® **Voice Design**: Create custom voice characteristics based on natural language descriptions.\r\n- üöÄ **Efficient Inference**: Supports both 12Hz and 25Hz speech tokenizer architectures.\r\n- üéØ **Multilingual**: Native support for 10 languages (Chinese, English, Japanese, Korean, German, French, Russian, Portuguese, Spanish, and Italian).\r\n- ‚ö° **Integrated Loading**: No separate loader nodes required; model loading is managed on-demand with global caching.\r\n- ‚è±Ô∏è **Ultra-Low Latency**: Supports high-fidelity speech reconstruction with low-latency streaming.\r\n- üß† **Attention Mechanism Selection**: Choose from multiple attention implementations (sage_attn, flash_attn, sdpa, eager) with auto-detection and graceful fallback.\r\n- üíæ **Memory Management**: Optional model unloading after generation to free GPU memory for users with limited VRAM.\r\n\r\n## Nodes List\r\n\r\n### 1. Qwen3-TTS Voice Design (`VoiceDesignNode`)\r\nGenerate unique voices based on text descriptions.\r\n- **Inputs**:\r\n  - `text`: Target text to synthesize.\r\n  - `instruct`: Description of the voice (e.g., \"A gentle female voice with a high pitch\").\r\n  - `model_choice`: Currently locked to **1.7B** for VoiceDesign features.\r\n  - `attention`: Attention mechanism (auto, sage_attn, flash_attn, sdpa, eager).\r\n  - `unload_model_after_generate`: Unload model from memory after generation to free GPU memory.\r\n- **Capabilities**: Best for creating \"imaginary\" voices or specific character archetypes.\r\n\r\n### 2. Qwen3-TTS Voice Clone (`VoiceCloneNode`)\r\nClone a voice from a reference audio clip.\r\n- **Inputs**:\r\n  - `ref_audio`: A short (5-15s) audio clip to clone.\r\n  - `ref_text`: Text spoken in the `ref_audio` (helps improve quality).\r\n  - `target_text`: The new text you want the cloned voice to say.\r\n  - `model_choice`: Choose between **0.6B** (fast) or **1.7B** (high quality).\r\n  - `attention`: Attention mechanism (auto, sage_attn, flash_attn, sdpa, eager).\r\n  - `unload_model_after_generate`: Unload model from memory after generation to free GPU memory.\r\n\r\n### 3. Qwen3-TTS Custom Voice (`CustomVoiceNode`)\r\nStandard TTS using preset speakers.\r\n- **Inputs**:\r\n  - `text`: Target text.\r\n  - `speaker`: Selection from preset voices (Aiden, Eric, Serena, etc.).\r\n  - `instruct`: Optional style instructions.\r\n  - `attention`: Attention mechanism (auto, sage_attn, flash_attn, sdpa, eager).\r\n  - `unload_model_after_generate`: Unload model from memory after generation to free GPU memory.\r\n\r\n### 4. Qwen3-TTS Role Bank (`RoleBankNode`) [New]\r\nCollect and manage multiple voice prompts for dialogue generation.\r\n- **Inputs**:\r\n  - Up to 8 roles, each with:\r\n    - `role_name_N`: Name of the role (e.g., \"Alice\", \"Bob\", \"Narrator\")\r\n    - `prompt_N`: Voice clone prompt from `VoiceClonePromptNode`\r\n- **Capabilities**: Create named voice registry for use in `DialogueInferenceNode`. Supports up to 8 different voices per bank.\r\n\r\n### 5. Qwen3-TTS Voice Clone Prompt (`VoiceClonePromptNode`) [New]\r\nExtract and reuse voice features from reference audio.\r\n- **Inputs**:\r\n  - `ref_audio`: A short (5-15s) audio clip to extract features from.\r\n  - `ref_text`: Text spoken in the `ref_audio` (highly recommended for better quality).\r\n  - `model_ch",
      "default_branch": "main"
    },
    "fetched_at": "2026-02-15T03:38:00.908890"
  },
  {
    "basic_info": {
      "name": "Khazix-Skills",
      "full_name": "KKKKhazix/Khazix-Skills",
      "owner": "KKKKhazix",
      "description": null,
      "url": "https://github.com/KKKKhazix/Khazix-Skills",
      "clone_url": "https://github.com/KKKKhazix/Khazix-Skills.git",
      "ssh_url": "git@github.com:KKKKhazix/Khazix-Skills.git",
      "homepage": null,
      "created_at": "2026-01-22T18:59:12Z",
      "updated_at": "2026-02-14T12:49:56Z",
      "pushed_at": "2026-01-22T19:16:42Z"
    },
    "stats": {
      "stars": 873,
      "forks": 133,
      "watchers": 873,
      "open_issues": 5,
      "size": 15
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 19931
      },
      "license": null,
      "topics": []
    },
    "content": {
      "readme": "# Khazix-Skills\n\nA collection of AI Skills for managing and evolving your skill library. These tools help you create, maintain, and continuously improve AI skills from GitHub repositories.\n\n## Skills Overview\n\n| Skill | Description |\n|-------|-------------|\n| [github-to-skills](./github-to-skills/) | Convert GitHub repos into AI skills automatically |\n| [skill-manager](./skill-manager/) | Manage skill lifecycle - check updates, list, delete |\n| [skill-evolution-manager](./skill-evolution-manager/) | Evolve skills based on user feedback and experience |\n\n---\n\n## github-to-skills\n\n**Automated factory for converting GitHub repositories into specialized AI skills.**\n\n### Features\n- Fetches repository metadata (README, latest commit hash)\n- Creates standardized skill directory structure\n- Generates `SKILL.md` with extended frontmatter for lifecycle management\n- Creates wrapper scripts for tool invocation\n\n### Usage\n```\n/github-to-skills <github_url>\n```\nOr: \"Package this repo into a skill: <url>\"\n\n### Example\n```\n/github-to-skills https://github.com/yt-dlp/yt-dlp\n```\n\n---\n\n## skill-manager\n\n**Lifecycle manager for GitHub-based skills.**\n\n### Features\n- **Audit**: Scan local skills folder for GitHub-based skills\n- **Check**: Compare local commit hashes against remote HEAD\n- **Report**: Generate status report (Stale/Current)\n- **Update**: Guided workflow for upgrading skills\n- **Inventory**: List all skills, delete unwanted ones\n\n### Usage\n```\n/skill-manager check     # Scan for updates\n/skill-manager list      # List all skills\n/skill-manager delete <name>  # Remove a skill\n```\n\n### Scripts\n| Script | Purpose |\n|--------|---------|\n| `scan_and_check.py` | Scan directories, parse frontmatter, check remote versions |\n| `update_helper.py` | Backup files before update |\n| `list_skills.py` | List installed skills with metadata |\n| `delete_skill.py` | Permanently remove a skill |\n\n---\n\n## skill-evolution-manager\n\n**Continuously improve skills based on user feedback and conversation insights.**\n\n### Core Concepts\n1. **Session Review**: Analyze skill performance after conversations\n2. **Experience Extraction**: Convert feedback into structured `evolution.json`\n3. **Smart Stitching**: Persist learned best practices into `SKILL.md`\n\n### Usage\n```\n/evolve\n```\nOr: \"Save this experience to the skill\"\n\n### Workflow\n1. **Review**: Agent analyzes what worked/didn't work\n2. **Extract**: Creates structured JSON with preferences, fixes, custom prompts\n3. **Persist**: Merges into `evolution.json`\n4. **Stitch**: Updates `SKILL.md` with learned best practices\n\n### Scripts\n| Script | Purpose |\n|--------|---------|\n| `merge_evolution.py` | Incrementally merge new experience data |\n| `smart_stitch.py` | Generate/update best practices section in SKILL.md |\n| `align_all.py` | Batch re-stitch all skills after updates |\n\n---\n\n## Installation\n\n1. Clone this repository:\n```bash\ngit clone https://github.com/KKKKhazix/Khazix-Skills.git\n```\n\n2. Copy desired skills to your Claude skills directory:\n```bash\n# Windows\ncopy /E Khazix-Skills\\github-to-skills %USERPROFILE%\\.claude\\skills\\\n\n# macOS/Linux\ncp -r Khazix-Skills/github-to-skills ~/.claude/skills/\n```\n\n3. Restart Claude to load the new skills.\n\n---\n\n## Requirements\n\n- Python 3.8+\n- Git (for checking remote repositories)\n- PyYAML (`pip install pyyaml`)\n\n---\n\n## How It Works\n\n```\n+------------------+     +----------------+     +------------------------+\n| github-to-skills | --> | skill-manager  | --> | skill-evolution-manager|\n+------------------+     +----------------+     +------------------------+\n        |                       |                         |\n    Create new             Maintain &                 Evolve &\n    skills from            update skills              improve based\n    GitHub repos                                      on feedback\n```\n\n**The Complete Skill Lifecycle:**\n1. **Create**: Use `github-to-skills` to wrap a GitHub repo as a skill\n2. **Maintain**: Use `skill-manager` to check for updates and upgrade\n3. **Evolve**: Use `skill-evolution-manager` to capture learnings and improve\n\n---\n\n## License\n\nMIT\n\n---\n\n## Contributing\n\nContributions are welcome! Feel free to:\n- Report issues\n- Submit pull requests\n- Share your own skills\n\n---\n\n## Author\n\n**KKKKhazix**\n\nIf you find these skills useful, consider giving this repo a star!\n",
      "default_branch": "main"
    },
    "fetched_at": "2026-02-15T03:38:02.048760"
  },
  {
    "basic_info": {
      "name": "lingbot-depth",
      "full_name": "Robbyant/lingbot-depth",
      "owner": "Robbyant",
      "description": "Masked Depth Modeling for Spatial Perception",
      "url": "https://github.com/Robbyant/lingbot-depth",
      "clone_url": "https://github.com/Robbyant/lingbot-depth.git",
      "ssh_url": "git@github.com:Robbyant/lingbot-depth.git",
      "homepage": "https://technology.robbyant.com/lingbot-depth",
      "created_at": "2026-01-25T14:14:29Z",
      "updated_at": "2026-02-14T20:18:05Z",
      "pushed_at": "2026-02-14T16:39:34Z"
    },
    "stats": {
      "stars": 865,
      "forks": 61,
      "watchers": 865,
      "open_issues": 7,
      "size": 31479
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 144257
      },
      "license": "Apache License 2.0",
      "topics": [
        "depth",
        "depth-camera",
        "masked-image-modeling"
      ]
    },
    "content": {
      "readme": "# LingBot-Depth: Masked Depth Modeling for Spatial Perception\n\n\n[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](LICENSE)\n[![Python 3.9+](https://img.shields.io/badge/python-3.9+-blue.svg)](https://www.python.org/downloads/)\n[![PyTorch 2.6+](https://img.shields.io/badge/pytorch-2.6+-green.svg)](https://pytorch.org/)\n\nüìÑ **[Technical Report](https://github.com/Robbyant/lingbot-depth/blob/main/tech-report.pdf)** |\nüìÑ **[arXiv](https://arxiv.org/abs/2601.17895)** |\nüåê **[Project Page](https://technology.robbyant.com/lingbot-depth)** |\nüíª **[Code](https://github.com/robbyant/lingbot-depth)** |\nü§ó **[Hugging Face](https://huggingface.co/collections/robbyant/lingbot-depth)** |\nü§ñ **[ModelScope](https://www.modelscope.cn/collections/Robbyant/LingBot-Depth)** ÔΩú\nü§ñ **[Video](https://www.bilibili.com/video/BV1oa6uBXEyh)**\n\n\n**LingBot-Depth** transforms incomplete and noisy depth sensor data into high-quality, metric-accurate 3D measurements. By jointly aligning RGB appearance and depth geometry in a unified latent space, our model serves as a powerful spatial perception foundation for robot learning and 3D vision applications.\n\n<p align=\"center\">\n  <img src=\"assets/teaser/teaser-crop.png\" width=\"100%\">\n</p>\n\nOur approach refines raw sensor depth into clean, complete measurements, enabling:\n- **Depth Completion & Refinement**: Fills missing regions with metric accuracy and improved quality\n- **Scene Reconstruction**: High-fidelity indoor mapping with a strong depth prior\n- **4D Point Tracking**: Accurate dynamic tracking in metric space for robot learning\n- **Dexterous Manipulation**: Robust grasping with precise geometric understanding\n\n## News\n\n- **[2026.02.15]** Upload LingBot-Depth-v0.5 which fixes the bug in previous version.\n\n## Artifacts Release\n\n### Model Zoo\n\nWe provide pretrained models for different scenarios:\n\n| Model | Hugging Face Model | ModelScope Model | Description |\n|-------|-----------|-----------|-------------|\n| LingBot-Depth-v0.5 | [robbyant/lingbot-depth-pretrain-vitl-14-v0.5](https://huggingface.co/robbyant/lingbot-depth-pretrain-vitl-14-v0.5/tree/main) | [robbyant/lingbot-depth-pretrain-vitl-14-v0.5](https://www.modelscope.cn/models/Robbyant/lingbot-depth-pretrain-vitl-14-v0.5)| ‚≠ê **Recommended!** General-purpose depth refinement and completion (fixes the bug in LingBot-Depth-v0.1)|\n| LingBot-Depth-v0.1 | [robbyant/lingbot-depth-pretrain-vitl-14](https://huggingface.co/robbyant/lingbot-depth-pretrain-vitl-14/tree/main) | [robbyant/lingbot-depth-pretrain-vitl-14](https://www.modelscope.cn/models/Robbyant/lingbot-depth-pretrain-vitl-14)| General-purpose depth refinement |\n| LingBot-Depth-DC | [robbyant/lingbot-depth-postrain-dc-vitl14](https://huggingface.co/robbyant/lingbot-depth-postrain-dc-vitl14/tree/main) | [robbyant/lingbot-depth-postrain-dc-vitl14](https://www.modelscope.cn/models/Robbyant/lingbot-depth-postrain-dc-vitl14)| Optimized for sparse depth completion |\n\n### Data Release (Coming Soon)\n- The curated 3M RGB-D dataset will be released upon completion of the necessary licensing and approval procedures. \n- Expected release: **mid-March 2026**.\n\n## Installation\n\n### Requirements\n\n‚Ä¢ Python ‚â• 3.9 ‚Ä¢ PyTorch ‚â• 2.0.0 ‚Ä¢ CUDA-capable GPU (recommended)\n\n### From source\n\n```bash\ngit clone https://github.com/robbyant/lingbot-depth\ncd lingbot-depth\n\n# Install the package (use 'python -m pip' to ensure correct environment)\nconda create -n lingbot-depth python=3.9\nconda activate lingbot-depth\npython -m pip install -e .\n```\n## Quick Start\n\n**Inference:**\n\n```python\nimport torch\nimport cv2\nimport numpy as np\nfrom mdm.model.v2 import MDMModel\n\n# Load model\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = MDMModel.from_pretrained('robbyant/lingbot-depth-pretrain-vitl-14-v0.5').to(device)\n\n# Load and prepare inputs\nimage = cv2.cvtColor(cv2.imread('examples/0/rgb.png'), cv2.COLOR_BGR2RGB)\nh, w = image.shape[:2]\nimage = torch.tensor(image / 255, dtype=torch.float32, device=device).permute(2, 0, 1)[None]\n\ndepth = cv2.imread('examples/0/raw_depth.png', cv2.IMREAD_UNCHANGED).astype(np.float32) / 1000.0\ndepth = torch.tensor(depth, dtype=torch.float32, device=device)[None]\n\nintrinsics = np.loadtxt('examples/0/intrinsics.txt')\nintrinsics[0] /= w  # Normalize fx and cx by width\nintrinsics[1] /= h  # Normalize fy and cy by height\nintrinsics = torch.tensor(intrinsics, dtype=torch.float32, device=device)[None]\n\n# Run inference\noutput = model.infer(\n    image,\n    depth_in=depth,\n    intrinsics=intrinsics)\n\ndepth_pred = output['depth']  # Refined depth map\npoints = output['points']      # 3D point cloud\n```\n\n**Run example:**\n\nThe model is automatically downloaded from Hugging Face on first run (no manual download needed):\n\n```bash\n# Basic usage - processes example 0\npython example.py\n\n# Use a different example (0-7 available)\npython example.py --example 1\n\n# Use depth completion optimized model\npython example.py --model robbyant/lingbot-depth-postrain-dc-vitl14-v0.5\n\n#",
      "default_branch": "main"
    },
    "fetched_at": "2026-02-15T03:38:03.170628"
  },
  {
    "basic_info": {
      "name": "lingbot-vla",
      "full_name": "Robbyant/lingbot-vla",
      "owner": "Robbyant",
      "description": "A Pragmatic VLA Foundation Model",
      "url": "https://github.com/Robbyant/lingbot-vla",
      "clone_url": "https://github.com/Robbyant/lingbot-vla.git",
      "ssh_url": "git@github.com:Robbyant/lingbot-vla.git",
      "homepage": "",
      "created_at": "2026-01-26T12:52:25Z",
      "updated_at": "2026-02-14T17:29:26Z",
      "pushed_at": "2026-02-05T13:13:09Z"
    },
    "stats": {
      "stars": 803,
      "forks": 60,
      "watchers": 803,
      "open_issues": 16,
      "size": 20944
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 962338,
        "Dockerfile": 1249,
        "Shell": 617,
        "Makefile": 350
      },
      "license": "Apache License 2.0",
      "topics": []
    },
    "content": {
      "readme": "<h1 align=\"center\">LingBot-VLA: A Pragmatic VLA Foundation Model</h1>\n\n<p align=\"center\">\n  <a href=\"assets/LingBot-VLA.pdf\"><img src=\"https://img.shields.io/static/v1?label=Paper&message=PDF&color=red&logo=arxiv\"></a>\n  <a href=\"https://technology.robbyant.com/lingbot-vla\"><img src=\"https://img.shields.io/badge/Project-Website-blue\"></a>\n  <a href=\"https://huggingface.co/collections/robbyant/lingbot-vla\"><img src=\"https://img.shields.io/static/v1?label=%F0%9F%A4%97%20Model&message=HuggingFace&color=yellow\"></a>\n  <a href=\"https://modelscope.cn/collections/Robbyant/LingBot-VLA\"><img src=\"https://img.shields.io/static/v1?label=%F0%9F%A4%96%20Model&message=ModelScope&color=purple\"></a>\n  <a href=\"LICENSE\"><img src=\"https://img.shields.io/badge/License-Apache--2.0-green\"></a>\n</p>\n\n<p align=\"center\">\n  <img src=\"assets/Teaser.png\" width=\"100%\">\n</p>\n\n## ü•≥ We are excited to introduce **LingBot-VLA**, a pragmatic Vision-Language-Action foundation model.\n\n**LingBot-VLA** has focused on **Pragmatic**:\n- **Large-scale Pre-training Data**: 20,000 hours of real-world\ndata from 9 popular dual-arm robot configurations.\n<p align=\"center\">\n  <img src=\"assets/scale_sr.png\" width=\"45%\" style=\"margin: 0 10px;\">\n  <img src=\"assets/scale_ps.png\" width=\"45%\" style=\"margin: 0 10px;\">\n</p>\n\n- **Strong Performance**: Achieve clear superiority over competitors on simulation and real-world benchmarks.\n- **Training Efficiency**: Represent a 1.5 ‚àº 2.8√ó (depending on the relied VLM base model) speedup over existing VLA-oriented codebases.\n\n## üöÄ News\n- **[2026-01-27]** LingBot-VLA Technical Report is available on Arxiv.\n- **[2026-01-27]** Weights and code released!\n\n\n---\n\n\n## üõ†Ô∏è Installation\nRequirements\n - Python 3.12.3\n - Pytorch 2.8.0\n - CUDA 12.8\n\n```bash\n# Install Lerobot\npip install torch==2.8.0 torchvision==0.23.0 torchaudio==2.8.0 --index-url https://download.pytorch.org/whl/cu128\nGIT_LFS_SKIP_SMUDGE=1 git clone https://github.com/huggingface/lerobot.git\ncd lerobot\ngit checkout 0cf864870cf29f4738d3ade893e6fd13fbd7cdb5\npip install -e .\n# Install flash attention\npip install /path/to/flash_attn-2.8.3+cu12torch2.8cxx11abiTRUE-cp312-cp312-linux_x86_64.whl\n\n# Clone the repository\ngit clone https://github.com/robbyant/lingbot-vla.git\ncd lingbot-vla/\ngit submodule update --remote --recursive\npip install -e .\npip install -r requirements.txt\n# Install LingBot-Depth dependency\ncd ./lingbotvla/models/vla/vision_models/lingbot-depth/\npip install -e . --no-deps\ncd ../MoGe\npip install -e .\n```\n\n---\n\n## üì¶ Model Download\nWe release LingBot-VLA pre-trained weights in two configurations: depth-free version and a depth-distillated version.\n- **Pretrained Checkpoints for Post-Training with and without depth**\n\n| Model Name | Huggingface | ModelScope | Description |\n| :--- | :---: | :---: | :---: |\n| LingBot-VLA-4B &nbsp; | [ü§ó lingbot-vla-4b](https://huggingface.co/robbyant/lingbot-vla-4b) | [ü§ñ lingbot-vla-4b](https://modelscope.cn/models/Robbyant/lingbot-vla-4b) | LingBot-VLA *w/o* Depth|\n| LingBot-VLA-4B-Depth | [ü§ó lingbot-vla-4b-depth](https://huggingface.co/robbyant/lingbot-vla-4b-depth) | [ü§ñ lingbot-vla-4b-depth](https://modelscope.cn/models/Robbyant/lingbot-vla-4b-depth) | LingBot-VLA *w/* Depth |\n\n\n\n\nTo train LingBot with our codebase, weights from [Qwen2.5-VL-3B-Instruct](https://huggingface.co/Qwen/Qwen2.5-VL-3B-Instruct), [MoGe-2-vitb-normal](https://huggingface.co/Ruicheng/moge-2-vitb-normal), and [LingBot-Depth](https://huggingface.co/robbyant/lingbot-depth-pretrain-vitl-14) also need to be prepared.\n- **Run Command**:\n```bash\npython3 scripts/download_hf_model.py --repo_id robbyant/lingbot-vla-4b --local_dir lingbot-vla-4b \n```\n---\n\n## üíª Post-Training Example\n\n- **Data Preparation**:\nPlease follow [RoboTwin2.0 Preparation](experiment/robotwin/README.md)\n\n- **Training Configuration**:\nWe provide the mixed post-training configuration in five RoboTwin 2.0 tasks (\"open_microwave\" \"click_bell\" \"stack_blocks_three\" \"place_shoe\" \"put_object_cabinet\").\n<details>\n<summary><b>Click to expand full YAML configuration</b></summary>\n\n```yaml\nmodel:\n  model_path: \"path/to/lingbot_vla_checkpoint\" # Path to pre-trained VLA foundation model (w/o or w depth)\n  tokenizer_path: \"path/to/Qwen2.5-VL-3B-Instruct\" \n  post_training: true            # Enable post-training/fine-tuning mode\n  adanorm_time: true\n  old_adanorm: true\n\ndata:\n  datasets_type: vla\n  data_name: robotwin_5_new      \n  train_path: \"path/to/lerobot_merged_data\" # merged data from 5 robotwin2.0 tasks\n  num_workers: 8\n  norm_type: bounds_99_woclip\n  norm_stats_file: assets/norm_stats/robotwin_50.json # file of normalization statistics\n\ntrain:\n  output_dir: \"path/to/output\"\n  loss_type: L1_fm               # we apply L1 flow-matching loss in robotwin2.0 finetuning\n  data_parallel_mode: fsdp2      # Use Fully Sharded Data Parallel (PyTorch FSDP2)\n  enable_full_shard: false       # Don't apply reshare after forward in FSDP2\n  module_fsdp_enable: true\n  use_compile: true              # Acceleration",
      "default_branch": "main"
    },
    "fetched_at": "2026-02-15T03:38:04.297942"
  },
  {
    "basic_info": {
      "name": "skill-scanner",
      "full_name": "cisco-ai-defense/skill-scanner",
      "owner": "cisco-ai-defense",
      "description": "Security Scanner for Agent Skills",
      "url": "https://github.com/cisco-ai-defense/skill-scanner",
      "clone_url": "https://github.com/cisco-ai-defense/skill-scanner.git",
      "ssh_url": "git@github.com:cisco-ai-defense/skill-scanner.git",
      "homepage": "https://blogs.cisco.com/ai/personal-ai-agents-like-moltbot-are-a-security-nightmare",
      "created_at": "2026-01-29T01:31:32Z",
      "updated_at": "2026-02-14T22:38:48Z",
      "pushed_at": "2026-02-09T21:29:10Z"
    },
    "stats": {
      "stars": 785,
      "forks": 91,
      "watchers": 785,
      "open_issues": 6,
      "size": 753
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 1016767,
        "YARA": 52717,
        "Shell": 4043
      },
      "license": "Other",
      "topics": [
        "agent",
        "agent-skills",
        "security"
      ]
    },
    "content": {
      "readme": "# Skill Scanner\n\n[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)\n[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)\n[![PyPI version](https://img.shields.io/pypi/v/cisco-ai-skill-scanner.svg)](https://pypi.org/project/cisco-ai-skill-scanner/)\n[![CI](https://github.com/cisco-ai-defense/skill-scanner/actions/workflows/python-tests.yml/badge.svg)](https://github.com/cisco-ai-defense/skill-scanner/actions/workflows/python-tests.yml)\n[![Discord](https://img.shields.io/badge/Discord-Join%20Us-7289da?logo=discord&logoColor=white)](https://discord.com/invite/nKWtDcXxtx)\n[![Cisco AI Defense](https://img.shields.io/badge/Cisco-AI%20Defense-049fd9?logo=cisco&logoColor=white)](https://www.cisco.com/site/us/en/products/security/ai-defense/index.html)\n[![AI Security Framework](https://img.shields.io/badge/AI%20Security-Framework-orange)](https://learn-cloudsecurity.cisco.com/ai-security-framework)\n\nA security scanner for AI Agent Skills that detects prompt injection, data exfiltration, and malicious code patterns. Combines **pattern-based detection** (YAML + YARA), **LLM-as-a-judge**, and **behavioral dataflow analysis** for comprehensive threat detection.\n\nSupports [OpenAI Codex Skills](https://openai.github.io/codex/) and [Cursor Agent Skills](https://docs.cursor.com/context/rules) formats following the [Agent Skills specification](https://agentskills.io).\n\n---\n\n## Highlights\n\n- **Multi-Engine Detection** - Static analysis, behavioral dataflow, LLM semantic analysis, and cloud-based scanning\n- **False Positive Filtering** - Meta-analyzer significantly reduces noise while preserving detection capability\n- **CI/CD Ready** - SARIF output for GitHub Code Scanning, exit codes for build failures\n- **Extensible** - Plugin architecture for custom analyzers\n\n**[Join the Cisco AI Discord](https://discord.com/invite/nKWtDcXxtx)** to discuss, share feedback, or connect with the team.\n\n---\n\n## Documentation\n\n| Guide | Description |\n|-------|-------------|\n| [Quick Start](docs/quickstart.md) | Get started in 5 minutes |\n| [Architecture](docs/architecture.md) | System design and components |\n| [Threat Taxonomy](docs/threat-taxonomy.md) | Complete AITech threat taxonomy with examples |\n| [LLM Analyzer](docs/llm-analyzer.md) | LLM configuration and usage |\n| [Meta-Analyzer](docs/meta-analyzer.md) | False positive filtering and prioritization |\n| [Behavioral Analyzer](docs/behavioral-analyzer.md) | Dataflow analysis details |\n| [API Reference](docs/api-server.md) | REST API documentation |\n| [Development Guide](docs/developing.md) | Contributing and development setup |\n\n---\n\n## Installation\n\n**Prerequisites:** Python 3.10+ and [uv](https://docs.astral.sh/uv/) (recommended) or pip\n\n```bash\n# Using uv (recommended)\nuv pip install cisco-ai-skill-scanner\n\n# Using pip\npip install cisco-ai-skill-scanner\n```\n\n<details>\n<summary><strong>Cloud Provider Extras</strong></summary>\n\n```bash\n# AWS Bedrock support\npip install cisco-ai-skill-scanner[bedrock]\n\n# Google Vertex AI support\npip install cisco-ai-skill-scanner[vertex]\n\n# Azure OpenAI support\npip install cisco-ai-skill-scanner[azure]\n\n# All cloud providers\npip install cisco-ai-skill-scanner[all]\n```\n\n</details>\n\n---\n\n## Quick Start\n\n### Environment Setup (Optional)\n\n```bash\n# For LLM analyzer and Meta-analyzer\nexport SKILL_SCANNER_LLM_API_KEY=\"your_api_key\"\nexport SKILL_SCANNER_LLM_MODEL=\"claude-3-5-sonnet-20241022\"\n\n# For VirusTotal binary scanning\nexport VIRUSTOTAL_API_KEY=\"your_virustotal_api_key\"\n\n# For Cisco AI Defense\nexport AI_DEFENSE_API_KEY=\"your_aidefense_api_key\"\n```\n\n### CLI Usage\n\n```bash\n# Scan a single skill (static analyzer only)\nskill-scanner scan /path/to/skill\n\n# Scan with behavioral analyzer (dataflow analysis)\nskill-scanner scan /path/to/skill --use-behavioral\n\n# Scan with all engines\nskill-scanner scan /path/to/skill --use-behavioral --use-llm --use-aidefense\n\n# Scan with meta-analyzer for false positive filtering\nskill-scanner scan /path/to/skill --use-llm --enable-meta\n\n# Scan multiple skills recursively\nskill-scanner scan-all /path/to/skills --recursive --use-behavioral\n\n# CI/CD: Fail build if threats found\nskill-scanner scan-all ./skills --fail-on-findings --format sarif --output results.sarif\n\n# Use custom YARA rules\nskill-scanner scan /path/to/skill --custom-rules /path/to/my-rules/\n\n# Disable specific noisy rules\nskill-scanner scan /path/to/skill --disable-rule YARA_script_injection --disable-rule MANIFEST_MISSING_LICENSE\n\n# Strict mode (more findings, higher FP rate)\nskill-scanner scan /path/to/skill --yara-mode strict\n\n# Permissive mode (fewer findings, may miss some threats)\nskill-scanner scan /path/to/skill --yara-mode permissive\n```\n\n### Python SDK\n\n```python\nfrom skill_scanner import SkillScanner\nfrom skill_scanner.core.analyzers import StaticAnalyzer, BehavioralAnalyzer\n\n# Create scanner with analyzers\nscanner = SkillScanner(analyzers=[\n ",
      "default_branch": "main"
    },
    "fetched_at": "2026-02-15T03:38:05.415005"
  },
  {
    "basic_info": {
      "name": "LuxTTS",
      "full_name": "ysharma3501/LuxTTS",
      "owner": "ysharma3501",
      "description": "A high-quality rapid TTS voice cloning model that reaches speeds of 150x realtime.",
      "url": "https://github.com/ysharma3501/LuxTTS",
      "clone_url": "https://github.com/ysharma3501/LuxTTS.git",
      "ssh_url": "git@github.com:ysharma3501/LuxTTS.git",
      "homepage": null,
      "created_at": "2026-01-23T20:12:03Z",
      "updated_at": "2026-02-15T03:22:19Z",
      "pushed_at": "2026-02-14T04:34:36Z"
    },
    "stats": {
      "stars": 762,
      "forks": 102,
      "watchers": 762,
      "open_issues": 14,
      "size": 87
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 658819
      },
      "license": "Apache License 2.0",
      "topics": []
    },
    "content": {
      "readme": "# LuxTTS\n<p align=\"center\">\n  <a href=\"https://huggingface.co/YatharthS/LuxTTS\">\n    <img src=\"https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Model-FFD21E\" alt=\"Hugging Face Model\">\n  </a>\n  &nbsp;\n  <a href=\"https://huggingface.co/spaces/YatharthS/LuxTTS\">\n    <img src=\"https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Space-blue\" alt=\"Hugging Face Space\">\n  </a>\n  &nbsp;\n  <a href=\"https://colab.research.google.com/drive/1cDaxtbSDLRmu6tRV_781Of_GSjHSo1Cu?usp=sharing\">\n    <img src=\"https://img.shields.io/badge/Colab-Notebook-F9AB00?logo=googlecolab&logoColor=white\" alt=\"Colab Notebook\">\n  </a>\n</p>\n\nLuxTTS is an lightweight zipvoice based text-to-speech model designed for high quality voice cloning and realistic generation at speeds exceeding 150x realtime.\n\nhttps://github.com/user-attachments/assets/a3b57152-8d97-43ce-bd99-26dc9a145c29\n\n\n### The main features are\n- Voice cloning: SOTA voice cloning on par with models 10x larger.\n- Clarity: Clear 48khz speech generation unlike most TTS models which are limited to 24khz.\n- Speed: Reaches speeds of 150x realtime on a single GPU and faster then realtime on CPU's as well.\n- Efficiency: Fits within 1gb vram meaning it can fit in any local gpu.\n\n## Usage\nYou can try it locally, colab, or spaces.\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1cDaxtbSDLRmu6tRV_781Of_GSjHSo1Cu?usp=sharing)\n[![Open in Spaces](https://huggingface.co/datasets/huggingface/badges/resolve/main/open-in-hf-spaces-sm.svg)](https://huggingface.co/spaces/YatharthS/LuxTTS)\n\n#### Simple installation:\n```\ngit clone https://github.com/ysharma3501/LuxTTS.git\ncd LuxTTS\npip install -r requirements.txt\n```\n\n#### Load model:\n```python\nfrom zipvoice.luxvoice import LuxTTS\n\n# load model on GPU\nlux_tts = LuxTTS('YatharthS/LuxTTS', device='cuda')\n\n# load model on CPU\n# lux_tts = LuxTTS('YatharthS/LuxTTS', device='cpu', threads=2)\n\n# load model on MPS for macs\n# lux_tts = LuxTTS('YatharthS/LuxTTS', device='mps')\n```\n\n#### Simple inference\n```python\nimport soundfile as sf\nfrom IPython.display import Audio\n\ntext = \"Hey, what's up? I'm feeling really great if you ask me honestly!\"\n\n## change this to your reference file path, can be wav/mp3\nprompt_audio = 'audio_file.wav'\n\n## encode audio(takes 10s to init because of librosa first time)\nencoded_prompt = lux_tts.encode_prompt(prompt_audio, rms=0.01)\n\n## generate speech\nfinal_wav = lux_tts.generate_speech(text, encoded_prompt, num_steps=4)\n\n## save audio\nfinal_wav = final_wav.numpy().squeeze()\nsf.write('output.wav', final_wav, 48000)\n\n## display speech\nif display is not None:\n  display(Audio(final_wav, rate=48000))\n```\n\n#### Inference with sampling params:\n```python\nimport soundfile as sf\nfrom IPython.display import Audio\n\ntext = \"Hey, what's up? I'm feeling really great if you ask me honestly!\"\n\n## change this to your reference file path, can be wav/mp3\nprompt_audio = 'audio_file.wav'\n\nrms = 0.01 ## higher makes it sound louder(0.01 or so recommended)\nt_shift = 0.9 ## sampling param, higher can sound better but worse WER\nnum_steps = 4 ## sampling param, higher sounds better but takes longer(3-4 is best for efficiency)\nspeed = 1.0 ## sampling param, controls speed of audio(lower=slower)\nreturn_smooth = False ## sampling param, makes it sound smoother possibly but less cleaner\nref_duration = 5 ## Setting it lower can speedup inference, set to 1000 if you find artifacts.\n\n## encode audio(takes 10s to init because of librosa first time)\nencoded_prompt = lux_tts.encode_prompt(prompt_audio, duration=ref_duration, rms=rms)\n\n## generate speech\nfinal_wav = lux_tts.generate_speech(text, encoded_prompt, num_steps=num_steps, t_shift=t_shift, speed=speed, return_smooth=return_smooth)\n\n## save audio\nfinal_wav = final_wav.numpy().squeeze()\nsf.write('output.wav', final_wav, 48000)\n\n## display speech\nif display is not None:\n  display(Audio(final_wav, rate=48000))\n```\n## Tips\n- Please use at minimum a 3 second audio file for voice cloning.\n- You can use return_smooth = True if you hear metallic sounds.\n- Lower t_shift for less possible pronunciation errors but worse quality and vice versa.\n\n  \n## Info\n\nQ: How is this different from ZipVoice?\n\nA: LuxTTS uses the same architecture but distilled to 4 steps with an improved sampling technique. It also uses a custom 48khz vocoder instead of the default 24khz version.\n\nQ: Can it be even faster?\n\nA: Yes, currently it uses float32. Float16 should be significantly faster(almost 2x).\n\n## Roadmap\n\n- [x] Release model and code\n- [x] Huggingface spaces demo\n- [x] Release MPS support (thanks to @builtbybasit)\n- [ ] Release LuxTTS v1.5\n- [ ] Release code for float16 inference\n\n## Acknowledgments\n\n- [ZipVoice](https://github.com/k2-fsa/ZipVoice) for their excellent code and model.\n- [Vocos](https://github.com/gemelo-ai/vocos.git) for their great vocoder.\n  \n## Final Notes\n\nThe model and code are licensed under the Apache-2.0 license. See LICENSE for de",
      "default_branch": "master"
    },
    "fetched_at": "2026-02-15T03:38:06.525238"
  }
]