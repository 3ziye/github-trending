[
  {
    "basic_info": {
      "name": "MiMo-Audio",
      "full_name": "XiaomiMiMo/MiMo-Audio",
      "owner": "XiaomiMiMo",
      "description": null,
      "url": "https://github.com/XiaomiMiMo/MiMo-Audio",
      "clone_url": "https://github.com/XiaomiMiMo/MiMo-Audio.git",
      "ssh_url": "git@github.com:XiaomiMiMo/MiMo-Audio.git",
      "homepage": null,
      "created_at": "2025-09-19T00:46:49Z",
      "updated_at": "2025-09-19T03:04:11Z",
      "pushed_at": "2025-09-19T01:08:55Z"
    },
    "stats": {
      "stars": 64,
      "forks": 5,
      "watchers": 64,
      "open_issues": 0,
      "size": 6024
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 223404
      },
      "license": "Apache License 2.0",
      "topics": []
    },
    "content": {
      "readme": "<div align=\"center\">\n  <picture>\n    <source srcset=\"https://github.com/XiaomiMiMo/MiMo-VL/raw/main/figures/Xiaomi_MiMo_darkmode.png?raw=true\" media=\"(prefers-color-scheme: dark)\">\n    <img src=\"https://github.com/XiaomiMiMo/MiMo-VL/raw/main/figures/Xiaomi_MiMo.png?raw=true\" width=\"60%\" alt=\"Xiaomi-MiMo\" />\n  </picture>\n</div>\n\n<h3 align=\"center\">\n  <b>\n    <span>‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ</span>\n    <br/>\n    MiMo Audio: Audio Language Models are Few-Shot Learners\n    <br/>\n    <span>‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ</span>\n    <br/>\n  </b>\n</h3>\n\n<br/>\n\n<div align=\"center\" style=\"line-height: 1;\">\n  |\n  <a href=\"https://huggingface.co/collections/XiaomiMiMo/mimo-audio-68cc7202692c27dae881cce0\" target=\"_blank\">ü§ó HuggingFace</a>\n  &nbsp;|\n  <a href=\"https://github.com/XiaomiMiMo/MiMo-Audio/blob/main/MiMo-Audio-Technical-Report.pdf\" target=\"_blank\">üìÑ Paper</a>\n  &nbsp;|\n  <a href=\"https://xiaomimimo.github.io/MiMo-Audio-Demo\" target=\"_blank\">üì∞ Blog</a>\n  &nbsp;|\n  <a href=\"https://huggingface.co/spaces/XiaomiMiMo/mimo_audio_chat\" target=\"_blank\">üî• Online Demo</a>\n  &nbsp;|\n  <a href=\"https://github.com/XiaomiMiMo/MiMo-Audio-Eval\" target=\"_blank\">üìä MiMo-Audio-Eval</a>\n  &nbsp;|\n\n  <br/>\n</div>\n\n<br/>\n\n## Introduction\n\nExisting audio language models typically rely on task-specific fine-tuning to accomplish particular audio tasks. In contrast, humans are able to generalize to new audio tasks with only a few examples or simple instructions. GPT-3 has shown that scaling next-token prediction pretraining enables strong generalization capabilities in text, and we believe this paradigm is equally applicable to the audio domain. By scaling MiMo-Audio's pretraining data to over one hundred million of hours, we observe the emergence of few-shot learning capabilities across a diverse set of audio tasks. We develop a systematic evaluation of these capabilities and find that MiMo-Audio-7B-Base achieves SOTA performance on both speech intelligence and audio understanding benchmarks among open-source models. Beyond standard metrics, MiMo-Audio-7B-Base generalizes to tasks absent from its training data, such as voice conversion, style transfer, and speech editing. MiMo-Audio-7B-Base also demonstrates powerful speech continuation capabilities, capable of generating highly realistic talk shows, recitations, livestreaming and debates. At the post-training stage, we curate a diverse instruction-tuning corpus and introduce thinking mechanisms into both audio understanding and generation. MiMo-Audio-7B-Instruct achieves open-source SOTA on audio understanding benchmarks, spoken dialogue benchmarks and instruct-TTS evaluations, approaching or surpassing closed-source models.\n\n\n![Results](assets/Results.png)\n\n\n\n## Architecture\n### MiMo-Audio-Tokenizer\nMiMo-Audio-Tokenizer is a 1.2B-parameter Transformer operating at 25 Hz. It employs an eight-layer RVQ stack to generate 200 tokens per second. By jointly optimizing semantic and reconstruction objectives, we train MiMo-Audio-Tokenizer from scratch on a 10-million-hour corpus, achieving superior reconstruction quality and facilitating downstream language modeling.\n\n![Tokenizer](assets/tokenizer.png)\n\nMiMo-Audio couples a patch encoder, an LLM, and a patch decoder to improve modeling efficiency for high-rate sequences and bridge the length mismatch between speech and text. The patch encoder aggregates four consecutive time steps of RVQ tokens into a single patch, downsampling the sequence to a 6.25 Hz representation for the LLM. The patch decoder autoregressively generates the full 25 Hz RVQ token sequence via a delayed-generation scheme.\n### MiMo-Audio\n![Arch](assets/architecture.png)\n\n##  Explore MiMo-Audio Now! üöÄüöÄüöÄ\n- üéß **Try the Hugging Face demo:** [MiMo-Audio Demo](https://huggingface.co/spaces/XiaomiMiMo/mimo_audio_chat)\n- üì∞ **Read the Official Blog:** [MiMo-Audio Blog](https://xiaomimimo.github.io/MiMo-Audio-Demo)\n- üìÑ **Dive into the Technical Report:** [MiMo-Audio Technical Report](https://github.com/XiaomiMiMo/MiMo-Audio/blob/main/MiMo-Audio-Technical-Report.pdf)\n\n\n## Model Download\n| Models   | ü§ó Hugging Face |\n|-------|-------|\n| MiMo-Audio-Tokenizer | [XiaomiMiMo/MiMo-Audio-Tokenizer](https://huggingface.co/XiaomiMiMo/MiMo-Audio-Tokenizer) |\n| MiMo-Audio-7B-Base | [XiaomiMiMo/MiMo-Audio-7B-Base](https://huggingface.co/XiaomiMiMo/MiMo-Audio-7B-Base) |\n| MiMo-Audio-7B-Instruct | [XiaomiMiMo/MiMo-Audio-7B-Instruct](https://huggingface.co/XiaomiMiMo/MiMo-Audio-7B-Instruct) |\n\n\n\n## Getting Started\n\nSpin up the MiMo-Audio demo in minutes with the built-in Gradio app.\n\n### Installation\n``` sh\ngit clone https://github.com/XiaomiMiMo/MiMo-Audio.git\ncd MiMo-Audio\npip install -e .\n```\n### Run the demo\n``` sh\npython run_mimo_audio.py\n```\n\nThis launches a local Gradio interface where you can try MiMo-Audio interactively.\n\n![Demo UI](assets/demo_ui.jpg)\n\nEnter the local paths for `MiMo-Audio-Tokenizer` and `MiMo-Audio-7B-Instruct`, then enjoy the full functionality ",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-19T03:07:44.559071"
  },
  {
    "basic_info": {
      "name": "ai-humanizer-api",
      "full_name": "HuzefaUsama25/ai-humanizer-api",
      "owner": "HuzefaUsama25",
      "description": "AI Humanizer API converts AI text into high quality undetectable human-like writing. Bypasses Turnitin, GPTZero, Originality.ai, CopyLeaks. Fast, natural, high quality output for students, marketers, and content creators",
      "url": "https://github.com/HuzefaUsama25/ai-humanizer-api",
      "clone_url": "https://github.com/HuzefaUsama25/ai-humanizer-api.git",
      "ssh_url": "git@github.com:HuzefaUsama25/ai-humanizer-api.git",
      "homepage": null,
      "created_at": "2025-09-18T08:21:03Z",
      "updated_at": "2025-09-18T20:11:30Z",
      "pushed_at": "2025-09-18T08:37:02Z"
    },
    "stats": {
      "stars": 40,
      "forks": 0,
      "watchers": 40,
      "open_issues": 0,
      "size": 4
    },
    "tech_info": {
      "language": null,
      "languages": {},
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "# AI Humanizer API\n\nThe **AI Humanizer API** is the **best and only truly effective AI humanizer API**. It transforms AI-generated text into **natural, human-like writing** that passes every major AI detector without losing meaning, tone, or quality. Unlike other tools that produce awkward or robotic rewrites, the AI Humanizer API generates fluent, authentic, and **indistinguishable human text**.\n\nThis repository provides examples, quick integration steps, and developer resources for the [WriteHybrid AI Humanizer API](https://writehybrid.com/api-docs).\n\n---\n\n## Table of Contents\n\n- [Why Use the AI Humanizer API](#why-use-the-ai-humanizer-api)  \n- [Key Features](#key-features)  \n- [Quick Start](#quick-start)  \n  - [Authentication](#authentication)  \n  - [Base URL](#base-url)  \n  - [Example Request (cURL)](#example-request-curl)  \n  - [Example Response](#example-response)  \n- [Parameters](#parameters)  \n- [Pricing](#pricing)  \n- [Who Is It For](#who-is-it-for)  \n- [Why Trust This API](#why-trust-this-api)  \n- [Get Started](#get-started-with-ai-humanizer-api)  \n\n---\n\n## Why Use the AI Humanizer API\n\nAI text often gets flagged by detectors such as **Turnitin, GPTZero, Originality.ai, Copyleaks, Sapling, and more**. Getting flagged can result in:\n\n- SEO penalties and lower Google rankings  \n- Academic or publishing rejection  \n- Damaged trust with clients, readers, or customers  \n\nThe **AI Humanizer API is the only solution that reliably bypasses every major detector** while preserving the original context and style. It is fast, scalable, and trusted by real users.\n\n---\n\n## Key Features\n\n- **Only Useful Humanizer API**: Every other tool falls short; this one works.  \n- **Detector-Proof**: Passes Turnitin, GPTZero, Originality.ai, Copyleaks, and more.  \n- **Transparent Billing**: 1 credit = 1 word.  \n- **High Quality**: Meaning and nuance are preserved.  \n- **Fast**: Humanize thousands of words in seconds.  \n- **Natural Output**: Fluent, human-like text every time.  \n- **Developer Friendly**: RESTful API with simple JSON responses.  \n- **Scalable**: From indie projects to enterprise-grade platforms.  \n\n---\n\n## Quick Start\n\n### Authentication\n\nAll requests require an API key. Add it to the `Authorization` header as a Bearer token.\n\n### Base URL\n\n[https://whbserver.com/api/v1](https://whbserver.com/api/v1)\n\n\n### Example Request (cURL)\n\n```bash\ncurl -X POST https://whbserver.com/api/v1/humanizer/ \\\n  -H \"Authorization: Bearer YOUR_API_KEY\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n        \"text\": \"AI-generated text goes here...\",\n        \"complexity\": \"medium\",\n        \"purpose\": \"general\"\n      }'\n````\n\n### Example Response\n\n```json\n{\n  \"success\": true,\n  \"humanized_text\": \"In today‚Äôs fast-paced world, technology continues to evolve at incredible speed...\",\n  \"original_length\": 245,\n  \"humanized_length\": 263,\n  \"processing_time\": 1.1,\n  \"detection_score\": 0.01,\n  \"credits_used\": 245\n}\n```\n\n---\n\n## Parameters\n\n| Field        | Type   | Description                                                      |\n| ------------ | ------ | ---------------------------------------------------------------- |\n| `text`       | string | The AI-generated text you want to humanize                       |\n| `complexity` | string | Output style: `simple`, `medium`, `complex`                      |\n| `purpose`    | string | Rewrite context: `general`, `seo`, `academic`, `marketing`, etc. |\n\n---\n\n## Pricing\n\n* **Starter** ‚Äì 20,000 credits/month (20,000 words), email support\n* **Pro** ‚Äì 60,000 credits/month (60,000 words), advanced tones, faster processing\n* **Agency** ‚Äì 150,000 credits/month (150,000 words), team access, priority support\n* **Enterprise** ‚Äì Custom high-volume solutions\n\n**Note:** 1 credit = 1 word.\n\nFull details: [writehybrid.com/api-docs](https://writehybrid.com/api-docs)\n\n---\n\n## Who Is It For?\n\n* **SEO Teams** ‚Äì Safely scale content that ranks without AI penalties.\n* **Students & Academics** ‚Äì Submit detection-proof work that reads as authentic.\n* **Content Agencies** ‚Äì Deliver client-ready copy at scale with no manual rewrites.\n* **SaaS Builders** ‚Äì Add humanization as a core feature inside your product.\n* **Freelancers** ‚Äì Ensure client work is never penalized for AI generation.\n\n---\n\n## Why Trust This API\n\n* **Proven**: Successfully bypasses every major AI detector.\n* **Transparent**: 1 credit = 1 word, no hidden rules.\n* **Reliable**: Built on robust, scalable infrastructure.\n* **Trusted**: Used daily by agencies, startups, and independent creators worldwide.\n\n---\n\n## Get Started with AI Humanizer API\n\nStop wasting time on tools that don‚Äôt work. The **AI Humanizer API is the only real solution** for creating undetectable, human-like text at scale.\n\nüëâ [**Get your API key today**](https://writehybrid.com/api-docs) and start humanizing AI text instantly.\n\n\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-19T03:07:45.673365"
  },
  {
    "basic_info": {
      "name": "RamTorch",
      "full_name": "lodestone-rock/RamTorch",
      "owner": "lodestone-rock",
      "description": "RAM is all you need",
      "url": "https://github.com/lodestone-rock/RamTorch",
      "clone_url": "https://github.com/lodestone-rock/RamTorch.git",
      "ssh_url": "git@github.com:lodestone-rock/RamTorch.git",
      "homepage": null,
      "created_at": "2025-09-18T12:16:49Z",
      "updated_at": "2025-09-19T02:49:56Z",
      "pushed_at": "2025-09-18T14:44:33Z"
    },
    "stats": {
      "stars": 36,
      "forks": 1,
      "watchers": 36,
      "open_issues": 0,
      "size": 14
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 9145
      },
      "license": "Apache License 2.0",
      "topics": []
    },
    "content": {
      "readme": "# RamTorch\n\n**RAM is All You Need** - A PyTorch library for memory-efficient deep learning that enables training and inference of large models that don't fit in GPU memory.\n\n## Overview\n\nRamTorch provides CPU-GPU hybrid implementations of neural network components that keep parameters in CPU memory and transfer them to GPU on-demand. This approach dramatically reduces GPU memory usage while maintaining computational efficiency through asynchronous CUDA streams and intelligent batching.\n\n## Key Features\n\n- **Memory-Efficient Linear Layers**: CPU-stored parameters with on-demand GPU transfer\n- **Asynchronous CUDA Streams**: Overlap computation with data transfer for minimal latency\n- **ZeRO-1 Optimizer Support**: Distributed optimizer state sharding across multiple GPUs\n- **Drop-in Replacement**: Compatible with existing PyTorch code\n- **Configurable Transfer Throttling**: Controllable memory usage\n\n## Installation\n\n```bash\npip install ramtorch\n```\n\nOr install from source:\n\n```bash\ngit clone https://github.com/lodestone-rock/RamTorch.git\ncd RamTorch\npip install -e .\n```\n\n## Quick Start\n\n### Basic Usage\n\nReplace `torch.nn.Linear` with `ramtorch.modules.Linear` for automatic memory optimization:\n\n```python\nimport torch\nimport ramtorch.modules as ram_modules\n\n# Standard PyTorch approach (high GPU memory usage)\n# linear = torch.nn.Linear(1000, 1000)\n\n# RamTorch approach (low GPU memory usage)\nlinear = ram_modules.Linear(1000, 1000, device=\"cuda\")\n\n# Use exactly like a normal PyTorch layer\nx = torch.randn(32, 1000, device=\"cuda\")\noutput = linear(x)  # Parameters automatically transferred from CPU to GPU\n```\n\n### Building Models\n\n```python\nimport torch.nn as nn\nimport ramtorch.modules as ram_modules\n\nclass MemoryEfficientModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layers = nn.Sequential(\n            ram_modules.Linear(1000, 2000),\n            nn.ReLU(),\n            ram_modules.Linear(2000, 2000),\n            nn.ReLU(),\n            ram_modules.Linear(2000, 100)\n        )\n    \n    def forward(self, x):\n        return self.layers(x)\n\nmodel = MemoryEfficientModel()\n```\n\n### ZeRO-1 Optimizer Sharding\n\nFor distributed training with optimizer state sharding:\n\n```python\nimport torch.distributed as dist\nfrom ramtorch.zero1 import create_zero_param_groups, broadcast_zero_params\n\n# Initialize distributed training\ndist.init_process_group(backend='nccl')\nmodel = YourModel()\nall_params = list(model.parameters())\nrank = dist.get_rank()\nworld_size = dist.get_world_size()\n\n# Create ZeRO-1 sharded optimizer\nparam_groups = [{'params': all_params, 'lr': 1e-3, 'weight_decay': 0.01}]\nsharded_groups, owner_ranks = create_zero_param_groups(param_groups, rank, world_size)\noptimizer = torch.optim.AdamW(sharded_groups)\n\n# Scheduler works normally with sharded optimizer\nscheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n\n# Training loop\nfor epoch in range(num_epochs):\n    for batch in dataloader:\n        # Forward/backward with gradient accumulation\n        for micro_batch in split_batch(batch):\n            loss = model(micro_batch)\n            loss.backward()\n\n        # All-reduce gradients across ranks (you need to implement this)\n        all_reduce_gradients(all_params)\n        \n        # Each rank updates only its owned parameters\n        optimizer.step()\n        \n        # Broadcast updated parameters from owners to all ranks\n        broadcast_zero_params(all_params, owner_ranks)\n        \n        model.zero_grad()\n        scheduler.step()\n```\n## Configuration\n\n### Environment Variables\n\n- `MAX_INFLIGHT`: Maximum number of concurrent CPU-to-GPU transfers (default: 2)\n  ```bash\n  export MAX_INFLIGHT=4  # Allow more concurrent transfers\n  ```\n\n### Transfer Stream Management\n\nRamTorch automatically manages CUDA streams for optimal performance. The library uses a dedicated transfer stream to overlap data movement with computation.\n\n## Performance Considerations\n\n### When to Use RamTorch\n\n**Best suited for:**\n- Large models that don't fit in GPU memory\n- Inference scenarios with memory constraints\n- Training with limited GPU memory but abundant CPU memory\n- Distributed training with many parameters\n\n**Less suitable for:**\n- Small models that fit comfortably in GPU memory\n- Scenarios where CPU-GPU bandwidth is the bottleneck\n- Real-time applications requiring minimal latency\n\n### Optimization Tips\n\n1. **Use Larger Batch Sizes**: Helps amortize transfer costs\n2. **Configure MAX_INFLIGHT**: Tune based on your GPU memory availability\n3. **Mixed Precision**: Combine with `torch.cuda.amp` for additional memory savings\n4. **Strategic Placement**: Use RamTorch layers for the largest components only\n\n## Architecture\n\n### CPU Bouncing Linear Layer\n\n\n1. Stores parameters on CPU memory (with `share_memory_()` for multiprocessing)\n2. Asynchronously transfers weights to GPU during forward pass\n3. Uses CUDA events for proper stream synchronization\n4. Automatically throttles transfers to preve",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-19T03:07:46.812607"
  },
  {
    "basic_info": {
      "name": "obex",
      "full_name": "dis0rder0x00/obex",
      "owner": "dis0rder0x00",
      "description": "Obex ‚Äì Blocking unwanted DLLs in user mode",
      "url": "https://github.com/dis0rder0x00/obex",
      "clone_url": "https://github.com/dis0rder0x00/obex.git",
      "ssh_url": "git@github.com:dis0rder0x00/obex.git",
      "homepage": null,
      "created_at": "2025-09-18T15:43:10Z",
      "updated_at": "2025-09-19T02:56:20Z",
      "pushed_at": "2025-09-18T15:47:17Z"
    },
    "stats": {
      "stars": 33,
      "forks": 4,
      "watchers": 33,
      "open_issues": 0,
      "size": 234
    },
    "tech_info": {
      "language": "C",
      "languages": {
        "C": 14552
      },
      "license": "BSD 3-Clause \"New\" or \"Revised\" License",
      "topics": []
    },
    "content": {
      "readme": "# Obex - DLL Blocking\n\n**Obex** is a PoC tool/technique that can be used to prevent unwanted modules (e.g., EDR or monitoring libraries) from being loaded into a newly started process during process initialization or at runtime.\n\n## Features\n- Spawns any process with arguments under debug control.\n- Blocks a configurable list of DLLs by name.\n- Works both for startup DLLs and dynamically loaded DLLs (`LoadLibrary*`).\n- Written in plain C with no external dependencies.\n\n## Usage\n```\nobex.exe \"<command with args>\" [dll1.dll,dll2.dll,...]\n```\n- If no DLL list is provided, a default blocklist is used (at the time of writing just `amsi.dll`).\n- DLL names are case-insensitive.\n\n## How Does It Work?\nBesides parsing cli arguments the PoC does the following (in a rough overview):\n![](./images/flow.png)\n\nFor deeper understanding check code (obviously) or contact me on discord or [twitter](https://x.com/dis0rder_0x00).\n## Screenshot\nThe screenshot shows `obex` spawning `powershell.exe` with the default blocklist (only `amsi.dll`).\nAdditionally you can see the spawned process‚Äôs module list to verify that `amsi.dll` was not loaded.\n\n![](./images/1.png)\n\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-19T03:07:47.938454"
  },
  {
    "basic_info": {
      "name": "awesome-a2a-hub",
      "full_name": "questflowai/awesome-a2a-hub",
      "owner": "questflowai",
      "description": "A curated list of awesome Agent2Agent (A2A) protocol agents, tools, and resources, with a focus on the https://a2a.build. This repository is supported by https://questflow.ai.",
      "url": "https://github.com/questflowai/awesome-a2a-hub",
      "clone_url": "https://github.com/questflowai/awesome-a2a-hub.git",
      "ssh_url": "git@github.com:questflowai/awesome-a2a-hub.git",
      "homepage": "",
      "created_at": "2025-09-18T03:54:09Z",
      "updated_at": "2025-09-19T03:05:44Z",
      "pushed_at": "2025-09-18T06:08:39Z"
    },
    "stats": {
      "stars": 28,
      "forks": 1,
      "watchers": 28,
      "open_issues": 0,
      "size": 13
    },
    "tech_info": {
      "language": null,
      "languages": {},
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "# Awesome A2A Hub [![Awesome](https://awesome.re/badge.svg)](https://awesome.re)\n\nA curated list of awesome Agent2Agent (A2A) protocol agents, tools, and resources, with a focus on the [A2A Hub](https://a2a.build). This repository is supported by [Questflow](https://questflow.ai).\n\n## What is A2A?\n\n[A2A](https://github.com/google/A2A) (Agent2Agent) is an open protocol created by Google that enables different AI agents to communicate and collaborate with each other using a standardized interface. The protocol allows agents to discover capabilities, submit tasks for execution, monitor task progress, and receive task results in a unified way.\n\n## What is A2A Hub?\n\nThe [A2A Hub](https://a2a.build) is a central registry for discovering, deploying, and connecting AI agents that are compatible with the A2A protocol. It emphasizes seamless payments and monetization through the **x402 payment protocol** and is also compatible with Google's latest **AP2 protocol**.\n\n## Featured Agents on A2A Hub\n\nHere is a list of agents available on the A2A Hub. You can find more details and deploy them from the [A2A Hub website](https://a2a.build/agents).\n\n| Agent Name | Description | Version |\n| :--- | :--- | :--- |\n| [coingecko(Pro)](https://a2a.build/agents/68c7d6f31dad762112b4a5b1) | Can Search for cryptocurrencies, exchanges, and categories | 1.0.2 |\n| [Spec Token Agent](https://a2a.build/agents/68c770531dad762112b47394) | Spec Token Agent | 1.0.4 |\n| [GoPlus](https://a2a.build/agents/68c76fb71dad762112b47315) | GoPlus | 1.0.0 |\n| [Sim Dune](https://a2a.build/agents/68c76f071dad762112b471f7) | Sim Dune | 1.0.1 |\n| [Alpha Vantage Stock Market](https://a2a.build/agents/68c26db13367242ab91229d5) | Alpha Vantage Inc. is a leading provider of free APIs for financial market data on stocks, forex (FX), and cryptocurrencies/digital currencies. | 1.0.0 |\n| [Amap Maps](https://a2a.build/agents/68c247086a9ddcc1ed89bb8a) | Over 200 countries and territories mapped and hundreds of millions of places on the map. | 1.0.1 |\n| [Perplexity AI-powered Answer Engine](https://a2a.build/agents/68c23e8b3367242ab9121139) | Perplexity is a free AI-powered answer engine that provides accurate, trusted, and real-time answers to any question. | 1.0.0 |\n| [ElevenLabs AI Voice Generator](https://a2a.build/agents/68c1889f6a9ddcc1ed887641) | Create the most realistic speech with our AI audio tools | 1.0.0 |\n| [exa search](https://a2a.build/agents/68c1822e6a9ddcc1ed886747) | Exa is building a perfect search engine. | 1.0.0 |\n| [Nano Banana Agent](https://a2a.build/agents/68b6d9de9d58b0668c774296) | Nano Banana is an AI-powered image editor that‚Äôs designed to let you transform photos using simple, everyday language. | 1.0.1 |\n| [BWEnews](https://a2a.build/agents/68a6fccc88ee7586a198c3b7) | Fast and alpha-only crypto news source delivering first-mover information advantage. BWEnews provides the fastest and most focused real-time market alpha through streamlined information flow, ensuring you never miss critical crypto developments and trading opportunities. | 1.0.0 |\n| [Gloria](https://a2a.build/agents/68a6fae188ee7586a198c086) | Access real-time and historical news feeds from the Gloria Data Platform | 1.0.1 |\n| [parallel](https://a2a.build/agents/68a6f7c888ee7586a198bbb7) | At Parallel, we are building for the web‚Äôs second user.\\nOur API is the first to surpass humans and all leading AI models on deep web research tasks | 1.0.1 |\n| [aixbt](https://a2a.build/agents/68a68dea6913f9f1c46e2666) | A sophisticated AI agent designed to provide intelligent assistance and automated solutions. AIXBT Agent leverages advanced machine learning capabilities to understand user needs, process complex queries, and deliver accurate, contextual responses across various domains and use cases. | 1.0.1 |\n| [Financial Datasets](https://a2a.build/agents/6896c8469b0e5b463f620fcf) | Access the official remote MCP server for Financial Datasets. | 1.0.0 |\n| [Financial Dataset](https://a2a.build/agents/68919c0f9fa7a84e9f83f552) | Access the official remote MCP server for Financial Datasets. | 1.0.0 |\n| [ACP Client Agent](https://a2a.build/agents/688057886817cae0ea3b7674) | ACP Client Agent | 1.0.1 |\n| [Virtuals Agent](https://a2a.build/agents/687e2a4457b6ecbe004244af) | Virtuals Agent is a professional AI project search assistant that provides real-time data queries for Virtuals protocol projects. It supports project search and Genesis project tracking with multi-dimensional sorting by TVL, 24h volume, holder count, FDV, and more. Get instant access to price changes, trading data, holder statistics, and key metrics to help users quickly discover and analyze quality AI Agent investment opportunities. | 1.0.2 |\n| [Twitter Agent](https://a2a.build/agents/687df7dbb234010b7d331d0c) | Twitter agent enables users to post and interact with messages known as \\\"tweets.\\\" Can write a tweet, listen to twitter user post new tweet, query or search user latest tweets, get tweet detail, listen to a speci",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-19T03:07:49.055082"
  },
  {
    "basic_info": {
      "name": "Telegram-Adding-Users",
      "full_name": "duongtsa/Telegram-Adding-Users",
      "owner": "duongtsa",
      "description": "An automation for the process of adding communitymembers to any target of your choice. Simple to use and up to date!",
      "url": "https://github.com/duongtsa/Telegram-Adding-Users",
      "clone_url": "https://github.com/duongtsa/Telegram-Adding-Users.git",
      "ssh_url": "git@github.com:duongtsa/Telegram-Adding-Users.git",
      "homepage": "",
      "created_at": "2025-09-18T11:54:18Z",
      "updated_at": "2025-09-18T16:13:09Z",
      "pushed_at": "2025-09-18T11:56:56Z"
    },
    "stats": {
      "stars": 23,
      "forks": 0,
      "watchers": 23,
      "open_issues": 0,
      "size": 19779
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 56
      },
      "license": null,
      "topics": [
        "adders",
        "memberss-scri",
        "scraper",
        "telegram",
        "telegram-add",
        "telegram-add-user",
        "telegram-add-users",
        "telegram-adding-members",
        "telegram-channel-clone",
        "telegram-copy-channel",
        "telegram-copy-members",
        "telegram-copy-users",
        "telegram-group-clone",
        "telegram-member-adder-2025",
        "telegram-member-adders-2025",
        "telegram-memberadder",
        "telegram-members-add",
        "telegram-members-adder-2025",
        "telegram-membersadder",
        "telegram-user-adders-2025"
      ]
    },
    "content": {
      "readme": "# Telegram-Adding-Users\nAn automation for the process of adding communitymembers to any target of your choice. Simple to use and up to date!\n\n# üìÅ GET IT HERE: https://shorturl.at/MdvDy\n# CONTACT FOR QUESTIONS: https://shorturl.at/qRqTq\n\n<img src='UI1.png' width='450'>\n\n- EXTRACT MEMBERS, MESSAGES, MEDIA, CHANNELS AND MORE! (EVEN HIDDEN MEMBERS)\n![](scrap.gif)\n- ADD MEMBERS TO YOUR GROUPS/CHANNELS AUTOMATICALLY!\n- FILTERING ONLY PREMIUM MEMBERS POSSIBLE! (OPTIONAL)\n![](add.gif)\n- AUTOMATICALLY FORWARD ANY POST/MESSAGE/MEDIA TO ANY TARGET!\n- MASSDM ANYONE ON TELEGRAM!\n![](mass.gif)\n- CLONE AND COPY ANY CHANNELS/GROUPS!\n![](copy.gif)\n- JOIN TO TARGETS WITH ALL OF YOUR ACCOUNTS AUTOMATICALLY!\n![](join.gif)\n- GET RID OF YOUR COMPETITION EASILY!\n- GROW YOUR AUDIENCE EASILY!\n- GROW YOUR VIEWS AUTOMATICALLY!\n![](view_post.gif)\n- VOTE ON ANY POLLS AUTOMATICALLY!\n- UNSPAM AND UNFREEZE YOUR ACCOUNTS EASILY!\n- REACT TO ANY POST AUTOMATICALLY WITH EMOJI'S!\n- MAKE BACKUPS!\n- NO CODING SKILLS REQUIRED!\n- PROXY SUPPORTED (OPTIONAL)\n- THE ONLY TG TOOL WHICH IS UPDATED TO 2025!\n- SUPPORT AND UPDATES FOR LIFETIME!\n- AND MUCH MORE!\n\nNEW FEATURES WILL BE IMPLEMENTED AT WISH!\nIf you have any questions, make sure to contact us.\n\n# üìÅ GET IT HERE: https://shorturl.at/MdvDy\n# CONTACT FOR QUESTIONS: https://shorturl.at/qRqTq",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-19T03:07:50.267697"
  },
  {
    "basic_info": {
      "name": "model-memory-calculator",
      "full_name": "KolosalAI/model-memory-calculator",
      "owner": "KolosalAI",
      "description": "Simple model memory requirements calculator for GGUF",
      "url": "https://github.com/KolosalAI/model-memory-calculator",
      "clone_url": "https://github.com/KolosalAI/model-memory-calculator.git",
      "ssh_url": "git@github.com:KolosalAI/model-memory-calculator.git",
      "homepage": null,
      "created_at": "2025-09-18T06:51:30Z",
      "updated_at": "2025-09-19T00:02:32Z",
      "pushed_at": "2025-09-18T23:59:26Z"
    },
    "stats": {
      "stars": 23,
      "forks": 2,
      "watchers": 23,
      "open_issues": 0,
      "size": 54
    },
    "tech_info": {
      "language": "HTML",
      "languages": {
        "HTML": 28776
      },
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "# GGUF Metadata Reader (Browser)\n\nA single-file, static web app to read GGUF model metadata directly in the browser and estimate memory usage (RAM/VRAM) for a chosen context window and KV cache quantization.\n\n- Works with remote URLs that support HTTP Range requests (e.g., many Hugging Face files)\n- Works with local `.gguf` files (drag-and-drop via file picker)\n- Detects sharded models (e.g., `-00001-of-00013`) and sums total size\n- No server required; everything runs client-side\n\n## Quick Start\n\nOption A: Open the file directly\n1. Open `index.html` in a modern browser (Chrome, Edge, Safari).\n2. Paste a GGUF URL or choose a local `.gguf` file and click the corresponding button.\n\nOption B: Serve locally (helps with some CORS setups)\n```bash\ncd path/to/model-memory-calculator\npython -m http.server 8000\n# Then open http://localhost:8000 in your browser\n```\n\n## Usage\n\n- GGUF URL: Paste a direct link to a `.gguf` (e.g., a Hugging Face ‚Äúresolve/main‚Äù URL). Many hosts allow partial download via HTTP Range.\n- Or choose a local GGUF file: Uses the browser‚Äôs File API; no upload leaves your machine.\n- Context size (tokens): Select a preset (e.g., 4K, 16K, 128K) or choose \"Custom‚Ä¶\" and enter any positive integer token length.\n- KV cache quantization: Choose how keys/values are stored in memory. Options show approximate bytes per value.\n- Verbose: Prints debug logs of what‚Äôs read and how size is determined.\n\nClick ‚ÄúRead URL‚Äù or ‚ÄúRead File‚Äù. If successful, you‚Äôll see:\n- Extracted params: `attention_heads`, `kv_heads`, `hidden_layers`, `hidden_size`, `split_count` (if present)\n- Memory estimate: model size + KV cache size at your chosen context/quantization\n\n## How It Works\n\n- GGUF parsing: Reads just enough of the GGUF header to extract:\n  - `.attention.head_count`\n  - `.attention.head_count_kv`\n  - `.block_count`\n  - `.embedding_length`\n  - `split.count` (if present)\n- Remote file size:\n  - Tries `HEAD` to get `Content-Length`.\n  - Falls back to a `Range: bytes=0-0` request and reads `Content-Range`.\n- Sharded models:\n  - Detects `-00001-of-000NN` style patterns in URLs or uses `split.count` metadata.\n  - Sums sizes across parts (remote) or estimates total from a single shard (local) when possible.\n- KV cache estimate:\n  - Uses a simplified formula: `bytes_per_value √ó hidden_size √ó hidden_layers √ó context_tokens`.\n  - Shows total as: `Model + KV` (MB/GB). Actual usage can vary by backend/implementation.\n\n## Notes & Limitations\n\n- GGUF versions: Supports GGUF v1‚Äìv3 headers for the fields listed above.\n- CORS & Range: Remote hosts must allow cross-origin requests and HTTP Range. If not, size detection may fail; download the file and use the local option instead.\n- Range ignored: Some servers respond `200` without honoring `Range`. The app avoids downloading the full body for size only; estimates can fail in this case.\n- Sanity limits: Very long strings/arrays in metadata are bounded to avoid huge reads.\n- Estimates only: KV cache math is intentionally simplified. Different runtimes store KV differently (e.g., layout, precision, per-head factors).\n\n## Troubleshooting\n\n- ‚ÄúFailed to read params.‚Äù\n  - The file may not be GGUF or uses unsupported/unexpected metadata. Try another file or update the URL.\n- ‚ÄúCould not determine file size or compute usage (CORS/Range?).‚Äù\n  - The remote host may block CORS or not report size via `HEAD`/`Range`. Try serving the page locally, a different host, or the local file picker.\n- Split detection issues\n  - Ensure URLs use a stable pattern (e.g., `-00001-of-000xx`) or that `split.count` is present in metadata.\n\n## Privacy\n\n- The local file option never uploads your file; parsing happens entirely in your browser.\n- For remote URLs, the app performs small range requests to read the header and determine file size. It aborts early once required metadata is read.\n\n## Development\n\n- No build step required. The app is a single page:\n  - `index.html` ‚Äî All logic and UI\n- Open in a browser or serve with any static server.\n\n## License\n\nThis project is licensed under the terms in `LICENSE`.\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-19T03:07:51.368021"
  },
  {
    "basic_info": {
      "name": "InternVLA-A1",
      "full_name": "InternRobotics/InternVLA-A1",
      "owner": "InternRobotics",
      "description": "InternVLA-A1: Unifying Understanding, Generation, and Action for Robotic Manipulation‚Äã",
      "url": "https://github.com/InternRobotics/InternVLA-A1",
      "clone_url": "https://github.com/InternRobotics/InternVLA-A1.git",
      "ssh_url": "git@github.com:InternRobotics/InternVLA-A1.git",
      "homepage": null,
      "created_at": "2025-09-18T01:00:55Z",
      "updated_at": "2025-09-19T02:55:56Z",
      "pushed_at": "2025-09-18T10:49:23Z"
    },
    "stats": {
      "stars": 21,
      "forks": 0,
      "watchers": 21,
      "open_issues": 0,
      "size": 437
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 517220
      },
      "license": null,
      "topics": []
    },
    "content": {
      "readme": "<div align=\"center\">\n\n# InternVLA-A1: Unifying Understanding, Generation, and Action for Robotic Manipulation‚Äã\n\n</div>\n\n---\n\nInternVLA-A1 is an end-to-end vision‚Äìlanguage‚Äìaction (VLA) framework unifing understanding, generation ,and action for robotic manipulation. It leverages predictive imagination of task evolution to guide execution, enabling enhanced manipulation in highly dynamic environments. \n\n## :fire: Highlights <a name=\"high\"></a>\n<img width=\"1000\" alt=\"seer\" src=\"assets/internvla_a1_framework.jpg\">\n\n- **Novel Model Archituecture**: A Mixture-of-Transformers architecture for unified understanding, generation, and action.\n- **Hybrid Synthetic-Real Data Corpus**: A hybrid synthetic-real manipulation dataset [InternData-A1](https://huggingface.co/datasets/InternRobotics/InternData-A1), integrating 5 heterogeneous robots, 15 skills, and 200+ scenes, emphasizing multi-robot collaboration under dynamic scenarios.\n- **Impressive Real-World performance**: InternVLA-A1 demonstrates strong effectiveness and generalization in highly dynamic scenarios involving dynamic grasping of conveyor belts and multi-robot collaboration.\n\n### üèÜ **Unified Understanding-Generation-Action Family**\n\n- **F1-VLA** (F1 is a prequel version of InternVLA-A1): [Paper](https://arxiv.org/abs/2509.06951) | [Code](https://github.com/InternRobotics/F1-VLA) | [Model](https://huggingface.co/InternRobotics/F1-VLA)\n- **InternVLA-A1**: [Code](https://github.com/InternRobotics/InternVLA-A1) | [Paper/Model (Scheduled for late September release)]()\n\n## ü§ñ Real-World Robot Demonstrations\n\n### **Package grabbing and flipping in conveyor belt**\n<div align=\"center\">\n    <video src=\"https://github.com/user-attachments/assets/c7d8989c-be14-428e-b498-d02dc1fc1475\"\n         controls autoplay muted playsinline loop width=\"720\"></video>\n  <p><em>The model handles dynamically shaped packages on conveyor belts, tracking and predicting their trajectories in real-time to achieve high-speed stable grasping, while adaptively flipping packages and identifying express information from delivery notes.</em></p>\n</div>\n\n\n### **Multi-robot collaboration on long-horizon tasks in dynamic environments**\n<div align=\"center\">\n      <video src=\"https://github.com/user-attachments/assets/c438ff8a-4536-45b3-9117-e210c36ba8a0\"\n         controls autoplay muted playsinline loop width=\"720\"></video>\n  <p><em>The model swiftly identifies, locates, and grips high-speed ingredients based on task demands, showcasing its adaptability in complex environments.</em></p>\n</div>\n\n\n## üöÄ Quick Start\n\n### **Prerequisites**\n- Python ‚â• 3.10\n- torch ‚â• 2.6.0\n- CUDA ‚â• 12.4\n\n### **Installation**\n```bash\n# Clone repository\ngit clone https://github.com/InternRobotics/InternVLA-A1.git\n\n# Create environment\nconda create -f internvla_a1 python==3.10\nconda activate internvla_a1\n\n# Install dependencies\npip install torch==2.6.0 torchvision==0.21.0 torchaudio==2.6.0 torchcodec==0.2.1 --index-url https://download.pytorch.org/whl/cu124\n\n# install other requirements\npip install -r requirements.txt\n\npip install numpy==1.26.4\n```\n\n## üìÑ License\n\nThis project is licensed under the MIT License.\n\n## üôè Acknowledgments\n\n- [Lerobot](https://github.com/huggingface/lerobot)\n- [InternVL](https://github.com/OpenGVLab/InternVL)\n- [COSMOS](https://github.com/nvidia-cosmos)\n- [Any4lerobot](https://github.com/Tavish9/any4lerobot/)\n- [VAR](https://github.com/FoundationVision/VAR)\n",
      "default_branch": "master"
    },
    "fetched_at": "2025-09-19T03:07:52.482683"
  },
  {
    "basic_info": {
      "name": "qb-phone-pro",
      "full_name": "QBCoreStore/qb-phone-pro",
      "owner": "QBCoreStore",
      "description": "QBCore Advanced Phone for FiveM üì± Inspired by iPhoe. Sleek UI, customizable themes, GPS, banking, calling, social apps & QBCore integration, free for the community! ",
      "url": "https://github.com/QBCoreStore/qb-phone-pro",
      "clone_url": "https://github.com/QBCoreStore/qb-phone-pro.git",
      "ssh_url": "git@github.com:QBCoreStore/qb-phone-pro.git",
      "homepage": "https://fivem-qbcore.com/",
      "created_at": "2025-09-18T09:23:28Z",
      "updated_at": "2025-09-19T02:25:41Z",
      "pushed_at": "2025-09-18T12:03:21Z"
    },
    "stats": {
      "stars": 21,
      "forks": 0,
      "watchers": 21,
      "open_issues": 0,
      "size": 33649
    },
    "tech_info": {
      "language": null,
      "languages": {},
      "license": null,
      "topics": [
        "fivem",
        "iphone",
        "nopixel",
        "qb-phone",
        "qb-scripts",
        "qbcore",
        "qbcore-framework",
        "qbcore-iphone",
        "qbcore-script",
        "qbcore-scripts",
        "qbcore-ui",
        "qbcorestore"
      ]
    },
    "content": {
      "readme": "## ‚ö° About Us  \nWe are the official **QBCore Store**, active since 2020.  \nWhile we release free scripts for the community, our main focus is **Premium Pre-Made All-in-One Servers**.  \nIf you want a **ready-to-use, optimized, and complete QBCore roleplay server** with 400+ premium resources, maps, UIs, and monthly updates ‚Äì check us out below:  \n\nüåç Website: [fivem-qbcore.com](https://fivem-qbcore.com)  \nüí¨ Discord: [discord.gg/qbcoreframework](https://discord.gg/qbcoreframework)  \n\n# üì± qb-phone pro version\n**QBCore Advanced Phone for FiveM ‚Äì Inspired by iPhone**  \n\nWelcome to the **QBCore Phone Pro Version** ‚Äì a modern, feature-packed smartphone system built for **FiveM roleplay servers**.  \nThis phone is designed with **sleek UI, customizable themes, GPS, banking, calling, social apps, and seamless QBCore integration** ‚Äì all free for the community!  \n\n---\n\n## ‚ú® Why Use This Phone?  \n- Full optimized design, works perfectly without bugs.  \n- Inspired by real modern smartphones for the **best RP experience**.  \n- Community-focused: we release **free scripts** regularly.  \n- **Open-source & customizable** for developers and server owners.  \n- Do not resell it. You Will Get a Copyright Strike.\n\n---\n\n## ‚≠ê Goal: 300+ Stars!  \nIf you enjoy this release, please support us by giving a ‚≠ê star on GitHub ‚Äì it motivates us to release more high-quality scripts for the community!  \n\n---\n---\n\n## üì∏ Preview  \n\n![Preview](https://files.fivemerr.com/images/08b49fe2-55ca-4553-b22a-9df31da5f714.png)  \n\n---\n\nüöÄ Enhance your FiveM server today with **qb-phone pro** ‚Äì Free for all, Premium for those who want the next level!  \n",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-19T03:07:53.592042"
  },
  {
    "basic_info": {
      "name": "MiMo-Audio-Tokenizer",
      "full_name": "XiaomiMiMo/MiMo-Audio-Tokenizer",
      "owner": "XiaomiMiMo",
      "description": "A unified tokenizer that is capable of both extracting semantic information and enabling high-fidelity audio reconstruction.",
      "url": "https://github.com/XiaomiMiMo/MiMo-Audio-Tokenizer",
      "clone_url": "https://github.com/XiaomiMiMo/MiMo-Audio-Tokenizer.git",
      "ssh_url": "git@github.com:XiaomiMiMo/MiMo-Audio-Tokenizer.git",
      "homepage": "",
      "created_at": "2025-09-18T16:02:05Z",
      "updated_at": "2025-09-19T03:07:20Z",
      "pushed_at": "2025-09-18T16:42:57Z"
    },
    "stats": {
      "stars": 19,
      "forks": 3,
      "watchers": 19,
      "open_issues": 0,
      "size": 2086
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 88623
      },
      "license": "Apache License 2.0",
      "topics": []
    },
    "content": {
      "readme": "<div align=\"center\">\n\n<img src=\"https://raw.githubusercontent.com/XiaomiMiMo/MiMo-VL/main/figures/Xiaomi_MiMo.png\" alt=\"Description\" width=\"25%\" />\n\n# MiMo-Audio-Tokenizer\n\n<img src=\"mimo_audio_tokenizer/assets/Tokenizer_01.png\" alt=\"Description\" width=\"90%\" />\n\n<p><em>A unified tokenizer that is capable of both extracting semantic information and enabling high-fidelity audio reconstruction.</em></p>\n\n</div>\n\n## Key Features\n\n- Scaled parameters and training data bootstrap the frontier of audio tokenization\n  - 1.2B pure transformer-based architecture to keep both efficiency and effectiveness\n  - trained from scratch over 11 million hours covering both audio reconstruction task and the audio-to-text (A2T) task\n\n- Unified representation enhance both cross-modal alignment and speech reconstruction quality\n  - jointly capture both semantic and acoustic information while further alleviates the semantic-acoustic representation conflict\n\n## Installation\n\n```sh\ngit clone https://github.com/XiaomiMiMo/MiMo-Audio-Tokenizer\ncd mimo-audio-tokenizer\n# Install base dependencies\npip install -e .\n# Install flash-attn\npip install -e \".[flash]\"\n```\n\n## Model Download\n\n```sh\n# you might need `sudo apt-get install git-lfs` before download this model\ngit clone https://huggingface.co/XiaomiMiMo/MiMo-Audio-Tokenizer\n```\n\n## Example Usage\n\n### 0. Quick start\n\n```py\nimport mimo_audio_tokenizer\n\n# one-line model init\ntokenizer = mimo_audio_tokenizer.load_model(\"path to your model\").bfloat16().cuda()  # FlashAttention only support fp16 and bf16 data type\n\n# preprocess\nmels = []\nwav_paths = [\"mimo_audio_tokenizer/assets/BAC009S0764W0121.wav\", \"mimo_audio_tokenizer/assets/BAC009S0764W0122.wav\", \"mimo_audio_tokenizer/assets/Áå™ÂÖ´Êàí_gt.wav\"]\nfor wav_path in wav_paths:\n    wav = mimo_audio_tokenizer.load_audio(wav_path, tokenizer.config.sampling_rate)\n    mels.append(mimo_audio_tokenizer.mel_spectrogram(wav, tokenizer.config))\nmels, mels_lens = mimo_audio_tokenizer.padding(mels)  # (batch_size, n_mels, seq_len), (batch_size,)\n\n# one-line encode\ncodes, codes_lens, _ = tokenizer.encode(mels.cuda(), mels_lens.cuda())  # (batch_size, max_len, num_quantizers), (batch_size,)\n\n# one-line decode\nwavs, wavs_lens, _ = tokenizer.decode(codes, codes_lens)  # (batch_size, 1, wav_len)\n\n# inspect results\nfor i in range(len(wav_paths)):\n    print(codes[i, :codes_lens[i].item()])\n\nfor i in range(len(wav_paths)):\n    torchaudio.save(f\"{i}.wav\", wavs[i, :, :wavs_lens[i].item()].float().cpu().detach(),\n                    tokenizer.config.sampling_rate, format='wav', encoding='PCM_S')\n\n```\n\n### 1. Distributed offline batch inference via command-line tools\n\n`mimo_audio_tokenizer` is built for distributed offline batch inference.\n\n```sh\n# 1 node 8 gpu, try to decrease `batch_size` if OOM\n# task choices:\n#   \"wav2token\": need `key` / `wav` / `quantized_tokens` available in data.jsonl\n#   \"token2wav\": need `key` / `quantized_tokens` / `reconstructed_wav` available in data.jsonl\n#   \"wav2token2wav\": need `key` / `wav` / `quantized_tokens` / `reconstructed_wav` available in data.jsonl\ntorchrun --nproc_per_node=8 --nnodes=1 \\\n     --rdzv_id=2025 --rdzv_backend=\"c10d\" --rdzv_endpoint=\"localhost:0\" \\\n    `which mimo_audio_tokenizer` \\\n        --model_path \"path to your model\" \\\n        --data_list \"path to your data.jsonl\" \\\n        --batch_size 64 \\\n        --num_workers 8 \\\n        --prefetch 16 \\\n        --num_quantizers 20 \\\n        --task \"wav2token2wav\"\n```\n\n### Example Data Format\n\nHere is example `data.jsonl`:\n\n```json\n{\"key\": \"uttid_1\", \"wav\": \"/mnt/data/audio/uttid_1.wav\", \"quantized_tokens\": \"/mnt/data/audio_reconstructed/uttid_1.json\", \"reconstructed_wav\": \"/mnt/data/audio_reconstructed/uttid_1.wav\"}\n...\n{\"key\": \"uttid_2\", \"wav\": \"/mnt/data/audio/uttid_2.wav\", \"quantized_tokens\": \"/mnt/data/audio_reconstructed/uttid_2.json\", \"reconstructed_wav\": \"/mnt/data/audio_reconstructed/uttid_2.wav\"}\n...\n```\n\n- `key` is the key of this sample.\n- `wav` is the original audio.\n- `quantized_tokens` is the json path to save quantized tokens (we highly recommend to pre-define the save path before running the script).\n- `reconstructed_wav` is the wav path to save reconstructed result (we highly recommend to pre-define the save path before running the script).\n\n### 2. Online speech code extraction\n\n`mimo_audio_tokenizer` can also be used in online code extraction to power the training of AudioLLM.\n\n<table>\n<tr>\n<th>Before (extract code offline)</th>\n<th>After (extract code online)</th>\n</tr>\n<tr>\n<td>\n<sub>\n\n```py\n\nclass AudioLLM(nn.Module):\n    ...\n    def __init__(self, ...):\n        ...\n\n    def forward(self, speech_codes: Tensor, text_ids: Tensor, ...):\n        ...\n```\n\n</sub>\n<td>\n<sub>\n\n```py\nimport mimo_audio_tokenizer\n\nclass AudioLLM(nn.Module):\n    ...\n    def __init__(self, ...):\n        ...\n        self.audio_tokenizer = mimo_audio_tokenizer.load_model(\"path to your model\")\n        self.audio_tokenizer.freeze()  # no need for gradient calculation\n        ...\n\n    de",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-19T03:07:54.703517"
  },
  {
    "basic_info": {
      "name": "contracts",
      "full_name": "forum-online-protocol/contracts",
      "owner": "forum-online-protocol",
      "description": null,
      "url": "https://github.com/forum-online-protocol/contracts",
      "clone_url": "https://github.com/forum-online-protocol/contracts.git",
      "ssh_url": "git@github.com:forum-online-protocol/contracts.git",
      "homepage": null,
      "created_at": "2025-09-18T06:42:33Z",
      "updated_at": "2025-09-18T17:53:56Z",
      "pushed_at": "2025-09-18T15:26:15Z"
    },
    "stats": {
      "stars": 18,
      "forks": 0,
      "watchers": 18,
      "open_issues": 0,
      "size": 42
    },
    "tech_info": {
      "language": "Solidity",
      "languages": {
        "Solidity": 77818,
        "JavaScript": 46947,
        "Circom": 772
      },
      "license": null,
      "topics": []
    },
    "content": {
      "readme": "# Votta E-Voting System\n\n![Votta](https://via.placeholder.com/150?text=Votta)\n\nA secure, transparent, and decentralized electronic voting system built on blockchain technology with zero-knowledge proofs.\n\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n\n## Table of Contents\n\n- [Overview](#overview)\n- [System Architecture](#system-architecture)\n- [Smart Contracts](#smart-contracts)\n- [Off-Chain Services](#off-chain-services)\n- [Installation](#installation)\n- [Usage Guide](#usage-guide)\n  - [Running the System](#running-the-system)\n  - [Configuration](#configuration)\n  - [Monitoring](#monitoring)\n- [Development](#development)\n  - [Testing](#testing)\n  - [Deployment](#deployment)\n- [Security](#security)\n- [FAQ](#faq)\n- [Contributing](#contributing)\n- [License](#license)\n\n## Overview\n\nVotta is a next-generation e-voting system designed to provide unparalleled security, transparency, and user privacy. Built on blockchain technology with zero-knowledge proofs, it enables secure and auditable voting while maintaining voter anonymity.\n\n### Key Features\n\n- **Multiple Voting Polls**: Support for running multiple elections simultaneously\n- **Credential-Based Voting**: Secure voter registration with cryptographic credentials\n- **Batch Processing**: Efficient aggregation of votes into verifiable batches\n- **Zero-Knowledge Proofs**: Privacy-preserving vote verification\n- **Challenge Mechanism**: Security monitoring and fraud detection\n- **Gas-Optimized**: Subsidized transaction costs for voters\n- **Decentralized Governance**: Protocol-level management of system components\n\n## System Architecture\n\nThe Votta e-voting system consists of several interconnected components:\n\n\n1. **Smart Contracts**: Core blockchain components handling voting logic, credential management, and batch processing\n2. **Aggregator Service**: Off-chain service that collects voting receipts and submits them as batches\n3. **Watch-Tower Service**: Security monitoring service that challenges fraudulent batches\n4. **ZK-Prover**: Zero-knowledge proof generation and verification service\n5. **User Interface**: Web and mobile interfaces for voter interaction\n\n## Smart Contracts\n\nThe system includes the following smart contracts:\n\n- **VottaProtocol**: Central management contract that coordinates all other components\n- **CredentialRegistry**: Manages voter credentials and verifies eligibility\n- **VotingBatch**: Processes batched votes and handles challenges\n- **PenaltyVault**: Holds bonds and manages slashing for malicious actors\n- **VotingFactory**: Creates and manages voting polls\n- **VotingPaymaster**: Covers gas costs for voting operations\n- **PlonkVerifier**: Verifies zero-knowledge proofs for batched votes\n- **AAValidate**: Validates user operations using account abstraction\n\n## Off-Chain Services\n\n### Aggregator Service\n\nThe Aggregator Service, written in Rust, is responsible for:\n\n- Collecting voting receipts from users\n- Building Merkle trees of receipt hashes\n- Generating zero-knowledge proofs\n- Submitting batches to the VotingBatch contract\n\n### Watch-Tower Service\n\nThe Watch-Tower Service, written in Go, performs security monitoring:\n\n- Monitors chain events for new batch submissions\n- Verifies batch validity and detects fraud\n- Challenges fraudulent batches with evidence\n- Acts as a security backstop for the system\n\n### ZK-Prover Integration\n\nThe ZK-Prover component:\n\n- Generates zero-knowledge proofs for vote batches\n- Ensures vote counts match the claimed receipts\n- Preserves privacy while enabling verification\n\n## Installation\n\n### Prerequisites\n\n- Node.js v16+ and npm\n- Go v1.18+\n- Rust v1.65+\n- Solidity v0.8.24+\n- Hardhat\n\n### Installing Dependencies\n\n#### Smart Contracts\n\n```bash\n# Install JavaScript dependencies\nnpm install\n\n# Install Solidity dependencies\nnpm install @openzeppelin/contracts @openzeppelin/contracts-upgradeable\n```\n\n#### Watch-Tower Service\n\n```bash\ncd services/watchtower\ngo mod download\n```\n\n#### Aggregator Service\n\n```bash\ncd services/aggregator\ncargo build --release\n```\n\n## Usage Guide\n\n### Running the System\n\n#### Option 1: Native Execution\n\n1. **Start Local Blockchain**\n\n```bash\nnpx hardhat node\n```\n\n2. **Deploy Smart Contracts**\n\n```bash\nnpx hardhat run scripts/deploy.js --network localhost\n```\n\n3. **Start Aggregator Service**\n\n```bash\ncd services/aggregator\ncargo run --release -- --config config.toml\n```\n\n4. **Start Watch-Tower Service**\n\n```bash\ncd services/watchtower\ngo run main.go\n```\n\n#### Option 2: Docker Containers\n\nWe provide Docker containers for both the Aggregator and Watch-Tower services to simplify deployment and ensure consistent environments.\n\n1. **Build and Start Services with Docker Compose**\n\n```bash\n# From the project root directory\ndocker-compose up -d\n```\n\nThis will start both services as defined in the `docker-compose.yml` file.\n\n2. **View Service Logs**\n\n```bash\n# View logs for all services\ndocker-compose logs -f\n\n# View logs for a specifi",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-19T03:07:55.818645"
  },
  {
    "basic_info": {
      "name": "Kali_Linux_MCP",
      "full_name": "i3T4AN/Kali_Linux_MCP",
      "owner": "i3T4AN",
      "description": "Two-component system bridging Kali Linux penetration testing tools with AI agents via MCP. Flask API server executes 10+ security tools (Nmap, SQLMap, Metasploit, etc.) while MCP client provides seamless AI integration for automated security workflows.",
      "url": "https://github.com/i3T4AN/Kali_Linux_MCP",
      "clone_url": "https://github.com/i3T4AN/Kali_Linux_MCP.git",
      "ssh_url": "git@github.com:i3T4AN/Kali_Linux_MCP.git",
      "homepage": "",
      "created_at": "2025-09-18T13:49:53Z",
      "updated_at": "2025-09-18T22:48:49Z",
      "pushed_at": "2025-09-18T17:31:10Z"
    },
    "stats": {
      "stars": 17,
      "forks": 0,
      "watchers": 17,
      "open_issues": 0,
      "size": 13
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 23338
      },
      "license": "MIT License",
      "topics": [
        "kali-linux",
        "mcp",
        "mcp-server",
        "pentesting",
        "security"
      ]
    },
    "content": {
      "readme": "# Kali_Linux_MCP\n\n## Overview\nKali_Linux_MCP exposes Kali tools through:\n- **Kali_Linux_Server.py**: Flask API wrapping tools like `nmap`, `gobuster`, `nikto`, `sqlmap`, `metasploit`, `hydra`, `john`, `wpscan`, `enum4linux`.  \n- **MCP_Server.py**: MCP bridge using FastMCP, forwarding requests from MCP clients to the API.\n\nUse it for **authorized labs, CTFs, HTB/THM machines**, or AI-assisted testing via MCP clients (Claude Desktop, 5ire, etc.).\n\n---\n\n## Requirements\n- Kali Linux (or Linux with tools installed in PATH).  \n- Python 3 with `flask`, `requests`, `mcp`.  \n- Install:  \n  pip install flask requests mcp\n\n---\n\n## Run\n\n1. Start API:  \n   python3 Kali_Linux_Server.py --port 5000  \n\n2. Health check:  \n   curl http://localhost:5000/health  \n\n3. Start MCP bridge:  \n   python3 MCP_Server.py --server http://localhost:5000 --timeout 300  \n\n---\n\n## API Endpoints\n- GET `/health` ‚Äî tool status.  \n- POST `/api/command` ‚Äî run any command.  \n- POST `/api/tools/<tool>` ‚Äî wrappers for nmap, gobuster, dirb, nikto, sqlmap, metasploit, hydra, john, wpscan, enum4linux.  \n\nEach requires JSON body with tool-specific args (`target`, `url`, etc.).\n\n---\n\n## MCP Tools\nBridge registers MCP tools: `nmap_scan`, `gobuster_scan`, `dirb_scan`, `nikto_scan`, `sqlmap_scan`, `metasploit_run`, `hydra_attack`, `john_crack`, `wpscan_analyze`, `enum4linux_scan`, plus `execute_command` and `check_health`.\n\n---\n\n## Example Usage\n- Nmap:  \n  curl -X POST http://localhost:5000/api/tools/nmap -H \"Content-Type: application/json\" -d '{\"target\":\"scanme.nmap.org\",\"additional_args\":\"-sV\"}'  \n\n- WPScan:  \n  curl -X POST http://localhost:5000/api/tools/wpscan -H \"Content-Type: application/json\" -d '{\"url\":\"https://example.com\",\"additional_args\":\"--enumerate u\"}'  \n\n- MCP Client:  \n  Add MCP config pointing `python3 /path/to/MCP_Server.py --server http://LINUX_IP:5000`.\n\n---\n\n## Scenarios\n- Recon: `nmap_scan` to map services.  \n- Web enum: `gobuster_scan` or `dirb_scan`.  \n- Vuln triage: `nikto_scan`, `sqlmap_scan`.  \n- WordPress checks: `wpscan_analyze`.  \n- SMB recon: `enum4linux_scan`.  \n- Credential tests (lab only): `hydra_attack`.\n- Exploit check: `metasploit_run`.  \n\n---\n\n## Notes\n- Supports AI-assisted workflows: models suggest and run commands.  \n- Works with Claude Desktop, 5ire MCP clients.  \n- Extendable: other forensic tools (Volatility, SleuthKit) possible.  \n\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-19T03:07:56.943903"
  },
  {
    "basic_info": {
      "name": "Book-video-generate",
      "full_name": "SheenHalo/Book-video-generate",
      "owner": "SheenHalo",
      "description": "‰∏Ä‰∏™Ëá™Âä®Âåñ‰π¶Á±çÊé®ÂπøËßÜÈ¢ëÁîüÊàêÂ∑•ÂÖ∑ÔºåÂèØ‰ª•Ê†πÊçÆ‰π¶ÂêçËá™Âä®ÁîüÊàêÂ∏¶ÊúâÈÖçÈü≥ÂíåÂ≠óÂπïÁöÑÁü≠ËßÜÈ¢ë„ÄÇ",
      "url": "https://github.com/SheenHalo/Book-video-generate",
      "clone_url": "https://github.com/SheenHalo/Book-video-generate.git",
      "ssh_url": "git@github.com:SheenHalo/Book-video-generate.git",
      "homepage": null,
      "created_at": "2025-09-18T07:37:45Z",
      "updated_at": "2025-09-19T02:37:46Z",
      "pushed_at": "2025-09-18T08:16:20Z"
    },
    "stats": {
      "stars": 15,
      "forks": 6,
      "watchers": 15,
      "open_issues": 0,
      "size": 50414
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 43289
      },
      "license": null,
      "topics": []
    },
    "content": {
      "readme": "# üìö Book Video Generator\n\n[![Python](https://img.shields.io/badge/Python-3.7%2B-blue.svg)](https://www.python.org/)\n[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)\n[![FFmpeg](https://img.shields.io/badge/FFmpeg-required-red.svg)](https://ffmpeg.org/)\n\n‰∏Ä‰∏™Ëá™Âä®Âåñ‰π¶Á±çÊé®ÂπøËßÜÈ¢ëÁîüÊàêÂ∑•ÂÖ∑ÔºåÂèØ‰ª•Ê†πÊçÆ‰π¶ÂêçËá™Âä®ÁîüÊàêÂ∏¶ÊúâÈÖçÈü≥ÂíåÂ≠óÂπïÁöÑÁü≠ËßÜÈ¢ë„ÄÇ\n\n## üñºÔ∏è ÊïàÊûúÈ¢ÑËßà\n\n[Á§∫‰æãËßÜÈ¢ë](https://github.com/user-attachments/assets/385a804c-904a-4aae-a595-58f9240a66b9)\n\n\n### ËßÜÈ¢ëÁâπÊÄß\nÁîüÊàêÁöÑËßÜÈ¢ëÂåÖÂê´Ôºö\n- üé¨ **Âä®ÊÄÅÂ∞ÅÈù¢Â±ïÁ§∫ÊïàÊûú** - ‰π¶Á±çÂ∞ÅÈù¢ÊªëÂä®Âä®ÁîªÔºå4ÁßíÁâáÂ§¥ÊïàÊûúÔºå‰π¶Á±çÂ∞ÅÈù¢Âú®`resource/covers/`‰∏≠ÈöèÊú∫Ëé∑Âèñ\n- üñºÔ∏è **ËÉåÊôØÂõæÁâáËá™Âä®ÂàáÊç¢** - ÊØè10ÁßíÂàáÊç¢ËÉåÊôØÔºåËê•ÈÄ†Ê∞õÂõ¥ÔºåËÉåÊôØÂõæÁâáÈöèÊú∫‰ªé`resource/backgrounds/`‰∏≠Ëé∑Âèñ\n- üìù **ÂêåÊ≠•Â≠óÂπïÊòæÁ§∫** - Ê†πÊçÆÈü≥È¢ëÊó∂ÈïøÁ≤æÂáÜÂêåÊ≠•ÔºåÂ∫ïÈÉ®Â±Ö‰∏≠ÊòæÁ§∫\n- üéµ **Â§öÈü≥ËΩ®Ê∑∑Âêà** - ÈÖçÈü≥ + ËÉåÊôØÈü≥‰πê + Èü≥ÊïàÔºåÈü≥ÈáèËá™Âä®Âπ≥Ë°°ÔºåËÉåÊôØÈü≥‰πêÈöèÊú∫‰ªé`resource/bgm/`‰∏≠Ëé∑Âèñ\n\n### ‰ΩøÁî®Âª∫ËÆÆ\n1. **È¶ñÊ¨°‰ΩøÁî®**: Âª∫ËÆÆÂÖà‰∏ãËΩΩÁ§∫‰æãËßÜÈ¢ëÊü•ÁúãÊïàÊûú\n2. **ÊµãËØïËøêË°å**: ‰ΩøÁî®ÁÆÄÂçïÁöÑ‰π¶Á±çÂêçÁß∞ËøõË°åÊµãËØï\n3. **ÂèÇÊï∞Ë∞ÉÊï¥**: Ê†πÊçÆÈúÄË¶ÅË∞ÉÊï¥ËßÜÈ¢ëÂèÇÊï∞ÂíåËØ≠Èü≥ÈÄâÊã©\n\n## üìã Á≥ªÁªüË¶ÅÊ±Ç\n\n- **Python**: 3.7+\n- **FFmpeg**: ÂøÖÈ°ªÂÆâË£ÖÂπ∂Ê∑ªÂä†Âà∞Á≥ªÁªüPATH\n- **Êìç‰ΩúÁ≥ªÁªü**: Windows / macOS / Linux\n\n## üöÄ Âø´ÈÄüÂºÄÂßã\n\n### 1. ÂÖãÈöÜÈ°πÁõÆ\n\n```bash\ngit clone https://github.com/SheenHalo/Book-video-generate.git\ncd Book-video-generate\n```\n\n### 2. ÂàõÂª∫ËôöÊãüÁéØÂ¢É\n\n**Windows:**\n```bash\npython -m venv venv\nvenv\\Scripts\\activate\n```\n\n**macOS/Linux:**\n```bash\npython3 -m venv venv\nsource venv/bin/activate\n```\n\n### 3. ÂÆâË£Ö‰æùËµñ\n\n```bash\npip install -r requirements.txt\n```\n\n### 4. Ê£ÄÊü•FFmpeg\n\nÈ°πÁõÆ‰æùËµñFFmpegËøõË°åËßÜÈ¢ëÂêàÊàêÔºåËØ∑Á°Æ‰øùÂ∑≤Ê≠£Á°ÆÂÆâË£ÖÔºö\n\n```bash\npython video_processor.py\n```\n\nÂ¶ÇÊûúÊòæÁ§∫\"ffmpeg ÂèØÁî®\"ÔºåÂàôÂÆâË£ÖÊàêÂäü„ÄÇÂ¶ÇÊûúÊòæÁ§∫\"ffmpeg ‰∏çÂèØÁî®\"ÔºåËØ∑Êåâ‰ª•‰∏ãÊ≠•È™§ÂÆâË£ÖÔºö\n\n#### Windows FFmpegÂÆâË£Ö\n1. ‰∏ãËΩΩFFmpeg: https://ffmpeg.org/download.html\n2. Ëß£ÂéãÂà∞ `C:\\ffmpeg`\n3. Ê∑ªÂä† `C:\\ffmpeg\\bin` Âà∞Á≥ªÁªüPATHÁéØÂ¢ÉÂèòÈáè\n4. ÈáçÂêØÂëΩ‰ª§Ë°åÂπ∂È™åËØÅÔºö`ffmpeg -version`\n\n#### macOS FFmpegÂÆâË£Ö\n```bash\nbrew install ffmpeg\n```\n\n#### Linux FFmpegÂÆâË£Ö\n```bash\n# Ubuntu/Debian\nsudo apt update && sudo apt install ffmpeg\n\n# CentOS/RHEL/Fedora\nsudo yum install ffmpeg\n```\n\n### 5. ÈÖçÁΩÆLLM API\nÊèê‰æõ‰∫Ü‰∏Ä‰∏™ÂÖçË¥πÁöÑLLM APIÊé•Âè£„ÄÇÂ¶ÇÊûúÂ§±Êïà‰∫ÜÔºåËØ∑Ëá™Ë°åÈÖçÁΩÆ„ÄÇ\nÁºñËæë `llm.py` Êñá‰ª∂ÔºåÈÖçÁΩÆ‰Ω†ÁöÑLLM API‰ø°ÊÅØÔºö\n\n```python\n# Âú®LLMClientÁ±ª‰∏≠‰øÆÊîπ\nself.api_url = \"‰Ω†ÁöÑAPIÂú∞ÂùÄ\"\nself.api_key = \"‰Ω†ÁöÑAPIÂØÜÈí•\"\n```\n\n### 6. ÂáÜÂ§áËµÑÊ∫êÊñá‰ª∂\n\nÁ°Æ‰øù‰ª•‰∏ãÁõÆÂΩïÂåÖÂê´ÂøÖË¶ÅÁöÑÊñá‰ª∂Ôºö\n\n```\nresource/\n‚îú‚îÄ‚îÄ backgrounds/    # ËÉåÊôØÂõæÁâá (jpg/png)\n‚îú‚îÄ‚îÄ bgm/           # ËÉåÊôØÈü≥‰πê (mp3)\n‚îú‚îÄ‚îÄ covers/        # ‰π¶Á±çÂ∞ÅÈù¢Â≠òÂÇ®‰ΩçÁΩÆ\n‚îú‚îÄ‚îÄ effects/       # Èü≥ÊïàÊñá‰ª∂ (mp3)\n‚îî‚îÄ‚îÄ fonts/         # Â≠ó‰ΩìÊñá‰ª∂ (ÂåÖÂê´msyh.ttc)\n```\n\n### 7. ËøêË°åÁ®ãÂ∫è\n\n```bash\npython main.py\n```\n\nÊåâÁÖßÊèêÁ§∫ËæìÂÖ•‰π¶ÂêçÔºåÁ®ãÂ∫èÂ∞ÜËá™Âä®ÁîüÊàêËßÜÈ¢ëÔºö\n\n```\nËØ∑ËæìÂÖ•‰π¶Âêç: Â∑¥Âà´Â°î\nÊ≠£Âú®Ëé∑Âèñ‰π¶Á±ç‰ø°ÊÅØ...\nÊ≠£Âú®ÁîüÊàêÊñáÊ°à...\nÊ≠£Âú®ÁîüÊàêËØ≠Èü≥...\nÊ≠£Âú®ÁîüÊàêËßÜÈ¢ë...\nÂºÄÂßãÂêàÊàêÈü≥ËßÜÈ¢ë...\nÊúÄÁªàËßÜÈ¢ëÂ∑≤‰øùÂ≠òÂà∞: appdata/Â∑¥Âà´Â°î/final_video.mp4\n```\n\n## üõ†Ô∏è È´òÁ∫ßÈÖçÁΩÆ\n\n### ‰øÆÊîπËØ≠Èü≥Á±ªÂûã\n\nÂú® `main.py` ‰∏≠‰øÆÊîπËØ≠Èü≥ÈÄâÊã©Ôºö\n\n```python\n# Êü•ÁúãÊâÄÊúâÂèØÁî®ËØ≠Èü≥\nprint(voice_dict.keys())\n\n# ÈÄâÊã©ÁâπÂÆöËØ≠Èü≥\nvoice = voice_dict.get(\"ÊôìÁßã-Â•≥\")\n```\n\n### Ëá™ÂÆö‰πâËßÜÈ¢ëÂèÇÊï∞\n\nÂú® `app.py` ÁöÑ `make_movie` ÂáΩÊï∞‰∏≠ÂèØ‰ª•Ë∞ÉÊï¥Ôºö\n- Â±èÂπïÂ∞∫ÂØ∏\n- Âä®ÁîªÊó∂Èïø\n- Èü≥ÈáèÂ§ßÂ∞è\n- ËÉåÊôØÂàáÊç¢Êó∂Èó¥\n\n### ÊîØÊåÅÁöÑËØ≠Èü≥ÂàóË°®\n\nÈ°πÁõÆÊîØÊåÅ43Áßç‰∏≠ÊñáËØ≠Èü≥Âèò‰ΩìÔºö\n\n| ËØ≠Èü≥ÂêçÁß∞ | Á±ªÂûã | ÁâπÁÇπ |\n|---------|------|------|\n| ÊôìÊôìÔºàÊ†áÂáÜÔºâ-Â•≥ | Ê†áÂáÜ | Ê∏©ÊöñÔºåÂÖ®Èù¢ÔºåÁîüÂä® |\n| ÊôìËæ∞ÔºàÊ†áÂáÜÔºâ-Â•≥ | Ê†áÂáÜ | ÂèãÂ•ΩÔºå‰ºëÈó≤Ôºå‰πêËßÇ |\n| ‰∫ëÂ≥∞-Áî∑ | Ê†áÂáÜ | Ëá™‰ø°ÔºåÁîüÂä®ÔºåÊÉÖÊÑü |\n| ÊôìÊôìÔºàÂ§öËØ≠Ë®ÄÔºâ-Â•≥ | Â§öËØ≠Ë®Ä | Ê∏©ÊöñÔºåÁîüÂä®ÔºåÊòé‰∫Æ |\n| ÊôìÈÄöÔºàÂê¥ËØ≠Ôºâ-Â•≥ | ÊñπË®Ä | Ê∏©ÊöñÔºåÂèãÂ•ΩÔºåËàíÁºì |\n| ÊôìÊïèÔºàÁ≤§ËØ≠Ôºâ-Â•≥ | ÊñπË®Ä | Êòé‰∫ÆÔºåÊ∏ÖÊô∞ÔºåËá™‰ø° |\n| ...Êõ¥Â§öËØ≠Èü≥ËØ¶ËßÅ‰ª£Á†Å | | |\n\n## üìÅ È°πÁõÆÁªìÊûÑ\n\n```\nBook-video-generate/\n‚îú‚îÄ‚îÄ main.py              # ‰∏ªÂÖ•Âè£Êñá‰ª∂\n‚îú‚îÄ‚îÄ app.py               # ËßÜÈ¢ëÁîüÊàêÊ†∏ÂøÉ\n‚îú‚îÄ‚îÄ spider.py            # Ë±ÜÁì£Áà¨Ëô´\n‚îú‚îÄ‚îÄ llm.py               # LLMÂÆ¢Êà∑Á´Ø\n‚îú‚îÄ‚îÄ tts_generator.py     # TTSÁîüÊàêÂô®\n‚îú‚îÄ‚îÄ video_processor.py   # ËßÜÈ¢ëÂ§ÑÁêÜÂ∑•ÂÖ∑\n‚îú‚îÄ‚îÄ requirements.txt     # ‰æùËµñÂàóË°®\n‚îú‚îÄ‚îÄ appdata/            # ÁîüÊàêÁöÑÊñá‰ª∂\n‚îî‚îÄ‚îÄ resource/           # ËµÑÊ∫êÊñá‰ª∂\n```\n\n## üìÑ ËÆ∏ÂèØËØÅ\n\nÊú¨È°πÁõÆÈááÁî®MITËÆ∏ÂèØËØÅ - ËØ¶ËßÅ [LICENSE](LICENSE) Êñá‰ª∂\n\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-19T03:07:58.049404"
  },
  {
    "basic_info": {
      "name": "Student-Success-Analysis",
      "full_name": "ankitsharma-tech/Student-Success-Analysis",
      "owner": "ankitsharma-tech",
      "description": "Student success analysis ‚Äì statisitical data analyisis AI project",
      "url": "https://github.com/ankitsharma-tech/Student-Success-Analysis",
      "clone_url": "https://github.com/ankitsharma-tech/Student-Success-Analysis.git",
      "ssh_url": "git@github.com:ankitsharma-tech/Student-Success-Analysis.git",
      "homepage": "",
      "created_at": "2025-09-18T17:49:39Z",
      "updated_at": "2025-09-18T18:16:26Z",
      "pushed_at": "2025-09-18T17:53:18Z"
    },
    "stats": {
      "stars": 15,
      "forks": 0,
      "watchers": 15,
      "open_issues": 0,
      "size": 12094
    },
    "tech_info": {
      "language": "HTML",
      "languages": {
        "HTML": 641646
      },
      "license": "MIT License",
      "topics": [
        "ai",
        "artificial-intelligence",
        "r",
        "statistical-analysis",
        "statistical-data-analysis",
        "statistics"
      ]
    },
    "content": {
      "readme": "# üéì Student Success Analysis\n\n## Final report: [report.pdf](./report.pdf)\n\n## üìñ Project Overview\n\nThe main goal of this project was to create a comprehensive report explaining concepts of **statistical data analysis** applied to an existing dataset.\n \n- The choice of statistical methods was flexible, as long as they were relevant and covered in the course curriculum.\n- The report included test cases, either from the recommended list provided by faculty or created by team members.\n- **R language** was used for data analysis and report generation.\n\nProject evaluation was based on:\n\n1. **Report quality**\n2. **Oral examination** testing knowledge of theoretical concepts (e.g., when to use a test, test assumptions, and method details).\n\n---\n\n## üìä Dataset\n\nThe dataset consists of survey responses and student grades in **mathematics** and **Portuguese** from two high schools.\n\nCollecting this type of data is essential for analyzing and improving the quality of the education system.  \nMore details: [pdfs/dataset_documentation.pdf](./pdfs/dataset_documentation.pdf)\n\n---\n\n## üìÇ Directory Structure\n\n| Directory                     | Description                                        |\n| ----------------------------- | -------------------------------------------------- |\n| [auditorne](./auditorne/)     | Reference to existing problems and their solutions |\n| [cheatsheets](./cheatsheets/) | Tidyverse cheat sheets in PDF format               |\n| [pdfs](./pdfs/)               | Dataset and project descriptions                   |\n| [src](./src/)                 | R Markdown source files and dataset                |\n\n---\n\n## ‚öôÔ∏è Installation\n\n### Windows\n\n- Install [RStudio](https://www.rstudio.com/products/rstudio/download/#download)\n- Install [R](https://cran.r-project.org/bin/windows/base/)\n\n### Linux\n\n- Install [RStudio](https://www.rstudio.com/products/rstudio/download/#download)\n- Install R and tidyverse dependencies:\n  ```bash\n  sudo apt-get install r-cran-curl r-cran-openssl r-cran-xml2 libxml2-dev\n  ```\n\n## üì¶ R Packages Setup\n\n1. Open **RStudio** ‚Üí Open Project ‚Üí `student-success-analysis.Rproj`\n2. The file `student-success-analysis.Rmd` should open automatically\n   - If not, navigate to it in the **Files** panel and double-click\n3. Run the first code chunk (`Ctrl + Shift + Enter`) containing the `library` functions\n4. A popup will prompt to install required packages ‚Üí click **Yes**\n5. Installation may take ~20 minutes\n\n---\n\n## üìù Notes\n\n**KS Test:**\n\n- If _p = 1_ ‚Üí data surely come from the same distribution\n- If _p = 0_ ‚Üí data come from different distributions\n\n## üìã To-Do\n\n- [ ] Spellcheck the report\n- [x] Write introduction to the problem\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-19T03:07:59.199511"
  },
  {
    "basic_info": {
      "name": "Time-Management-Tool",
      "full_name": "ankitsharma-tech/Time-Management-Tool",
      "owner": "ankitsharma-tech",
      "description": "An AI-powered time management tool built with Python to help users plan, prioritize, and optimize their daily tasks efficiently.",
      "url": "https://github.com/ankitsharma-tech/Time-Management-Tool",
      "clone_url": "https://github.com/ankitsharma-tech/Time-Management-Tool.git",
      "ssh_url": "git@github.com:ankitsharma-tech/Time-Management-Tool.git",
      "homepage": "",
      "created_at": "2025-09-18T17:44:29Z",
      "updated_at": "2025-09-18T18:16:28Z",
      "pushed_at": "2025-09-18T17:47:26Z"
    },
    "stats": {
      "stars": 15,
      "forks": 0,
      "watchers": 15,
      "open_issues": 0,
      "size": 498
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 7968,
        "Batchfile": 54
      },
      "license": "MIT License",
      "topics": [
        "ai-project",
        "ml",
        "ml-project",
        "project",
        "python",
        "python-project",
        "python3",
        "timemanagement"
      ]
    },
    "content": {
      "readme": "# Time Management Assistant Tool\n\nWelcome to the ultimate tool designed to streamline your tasks and manage your time efficiently. With its sleek, futuristic interface and cutting-edge functionality, TMA is here to revolutionize how you stay organized.\n\n## üöÄ Setup\n\n**Step 1: Clone the Repository**\n\n```bash\ngit clone https://github.com/ankitsharma-tech/Time-Management-Tool.git\n```\n\n**Step 2: Navigate to the Directory**\n\n```bash\ncd Time-Management-Tool\n```\n\n**Step 3: Navigate to the Directory**\n\n```bash\npip install -r requirements.txt\n```\n\n**Step 4: Open the Configuration File**\n\n- Open the `.bat` file in Notepad or your preferred text editor.\n\n**Step 5: Insert Your Main Script Path**\n\n- Copy the path to your `main.py` file.\n\n**Step 6: Update the BAT File**\n\n- Paste the copied path into the second line of the `.bat` file.\n\n## üåü Usage\n\nTo start managing your time like a pro:\n\n- Simply double-click the `.bat` file to launch the tool.\n- If you wan't to sheel the list use command\n\n```\nShow\n```\n\ninside the TMA - Tool\n\n- for Example Command type any thing wrong\n\n```\n                        ______ Available commands _____\n                      1. show - to show the Schedule file\n                      2. tell me - try like \"tell me to sleep at 09:00pm\"\n```\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-19T03:08:00.336128"
  },
  {
    "basic_info": {
      "name": "Student-Performance-Prediction",
      "full_name": "ankitsharma-tech/Student-Performance-Prediction",
      "owner": "ankitsharma-tech",
      "description": "This is a simple machine learning project using classifiers for predicting factors which affect student grades, using data from CSV file.",
      "url": "https://github.com/ankitsharma-tech/Student-Performance-Prediction",
      "clone_url": "https://github.com/ankitsharma-tech/Student-Performance-Prediction.git",
      "ssh_url": "git@github.com:ankitsharma-tech/Student-Performance-Prediction.git",
      "homepage": "",
      "created_at": "2025-09-18T17:48:17Z",
      "updated_at": "2025-09-18T18:16:27Z",
      "pushed_at": "2025-09-18T17:48:55Z"
    },
    "stats": {
      "stars": 15,
      "forks": 0,
      "watchers": 15,
      "open_issues": 0,
      "size": 10
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 10521
      },
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "# üéì Student Performance Prediction\nThis project demonstrates a simple machine learning approach to predict factors that influence student grades using a dataset stored in a CSV file.\n\n## üìä Project Overview\n\nThe dataset contains information about students from different nationalities and grade levels, along with key determining factors such as:\n\n- Number of hands raised\n- Number of attendances\n- Hours studied\n- And more\n\nThe goal of this project is to analyze these factors and predict their impact on student performance.\n\n## üß† Machine Learning Models\n\nSeveral classifiers and machine learning models have been implemented and compared to achieve the most accurate predictions of the factors affecting student marks.\n\n## üìà Visualizations\n\nTo better understand the results, visual aids have been generated, including:\n\n- Graphs for data insights\n- Confusion matrices for model evaluation\n\n---\n\nThis project provides a hands-on demonstration of applying machine learning techniques to an educational dataset, highlighting the relationship between study habits, engagement, and student success.\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-19T03:08:01.500159"
  },
  {
    "basic_info": {
      "name": "AI-Object-Detection",
      "full_name": "ankitsharma-tech/AI-Object-Detection",
      "owner": "ankitsharma-tech",
      "description": "A web AI object detection",
      "url": "https://github.com/ankitsharma-tech/AI-Object-Detection",
      "clone_url": "https://github.com/ankitsharma-tech/AI-Object-Detection.git",
      "ssh_url": "git@github.com:ankitsharma-tech/AI-Object-Detection.git",
      "homepage": "https://woody.pizza/tensorflow/object-detection/",
      "created_at": "2025-09-18T17:38:04Z",
      "updated_at": "2025-09-18T18:16:29Z",
      "pushed_at": "2025-09-18T17:42:09Z"
    },
    "stats": {
      "stars": 15,
      "forks": 0,
      "watchers": 15,
      "open_issues": 0,
      "size": 4
    },
    "tech_info": {
      "language": "JavaScript",
      "languages": {
        "JavaScript": 3145,
        "HTML": 1575,
        "CSS": 175
      },
      "license": "MIT License",
      "topics": [
        "ai",
        "artificial-intelligence",
        "artificial-intelligence-projects",
        "camera",
        "camera-detect",
        "css",
        "detection",
        "html",
        "javascript",
        "javascript-ai",
        "javascript-artificial-intelligence",
        "javascript-tensorflow",
        "ml5js",
        "object-detection",
        "object-detector",
        "tensorflow",
        "web"
      ]
    },
    "content": {
      "readme": "# ü§ñ AI Object Detection\n\n## üëã About this Project\nThis is a web-based AI object detection application that runs directly in your browser. It uses your device‚Äôs camera to detect objects in real time, making it easy and accessible without requiring any installation.\n\n---\n\n## ‚öôÔ∏è Features\n- ‚úÖ Toggle switch to enable or disable AI detection  \n- ‚úÖ Range slider to control frame rate  \n- ‚úÖ Real-time object detection through your camera  \n\n---\n\n## üñºÔ∏è Preview\n<a href=\"https://ibb.co/JCNgfJr\"><img src=\"https://i.ibb.co/3kwQDZS/preview-combined.jpg\" alt=\"preview-combined\" width=\"100%\"></a>\n\n---\n\n## üí™ Try It\nNot convinced yet? Try it out yourself here:  \nüëâ [Live Demo](https://woody.pizza/tensorflow/object-detection/)\n\n---\n\n## üåê Browser Support\nThe app works on most modern browsers. Below are the tested ones:\n\n### Desktop\n| Browser           | Supported |\n|-------------------|:---------:|\n| Firefox           | ‚úÖ |\n| Chrome            | ‚úÖ |\n| Edge              | ‚úÖ |\n| Internet Explorer | ‚ùå |\n\n### Mobile\n| Browser  | Supported |\n|----------|:---------:|\n| Firefox  | ‚úÖ |\n| Chrome   | ‚úÖ |\n\n---\n\n## ‚úåÔ∏è Credits\n- [Materialize](https://materializecss.com/) ‚Äì for UI components  \n- [ml5.js](https://ml5js.org/) ‚Äì for machine learning in the browser  \n",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-19T03:08:02.649447"
  },
  {
    "basic_info": {
      "name": "LiquidGlass-JetpackCompose",
      "full_name": "ardakazanci/LiquidGlass-JetpackCompose",
      "owner": "ardakazanci",
      "description": null,
      "url": "https://github.com/ardakazanci/LiquidGlass-JetpackCompose",
      "clone_url": "https://github.com/ardakazanci/LiquidGlass-JetpackCompose.git",
      "ssh_url": "git@github.com:ardakazanci/LiquidGlass-JetpackCompose.git",
      "homepage": null,
      "created_at": "2025-09-18T15:22:51Z",
      "updated_at": "2025-09-19T02:53:04Z",
      "pushed_at": "2025-09-18T15:28:38Z"
    },
    "stats": {
      "stars": 14,
      "forks": 0,
      "watchers": 14,
      "open_issues": 0,
      "size": 585
    },
    "tech_info": {
      "language": "Kotlin",
      "languages": {
        "Kotlin": 12328
      },
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "https://www.linkedin.com/posts/ardakazanci_jetpackcompose-androiddevelopment-androidprogramming-activity-7374464292516945920-jHLU?utm_source=share&utm_medium=member_desktop&rcm=ACoAAB2vx2QBVowkGS1YKnq1EhEAH-eJwQf_4T8\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-19T03:08:03.768079"
  },
  {
    "basic_info": {
      "name": "Magic-Door-Godot-4.5-release-page",
      "full_name": "lukky-nl/Magic-Door-Godot-4.5-release-page",
      "owner": "lukky-nl",
      "description": "Magic door seen on the Godot 4.5 release page",
      "url": "https://github.com/lukky-nl/Magic-Door-Godot-4.5-release-page",
      "clone_url": "https://github.com/lukky-nl/Magic-Door-Godot-4.5-release-page.git",
      "ssh_url": "git@github.com:lukky-nl/Magic-Door-Godot-4.5-release-page.git",
      "homepage": null,
      "created_at": "2025-09-18T15:07:18Z",
      "updated_at": "2025-09-19T02:37:49Z",
      "pushed_at": "2025-09-18T15:07:20Z"
    },
    "stats": {
      "stars": 14,
      "forks": 1,
      "watchers": 14,
      "open_issues": 0,
      "size": 24
    },
    "tech_info": {
      "language": null,
      "languages": {},
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-19T03:08:04.872415"
  },
  {
    "basic_info": {
      "name": "cdrgoat",
      "full_name": "stream-sec/cdrgoat",
      "owner": "stream-sec",
      "description": "Cloud Detection & Response GOAT is a scenario-driven, intentionally vulnerable framework designed to help defenders validate detection pipelines, practice SOC workflows and train analysts on realistic cloud attack paths - all in a safe, reproducible environment with no impact on production.",
      "url": "https://github.com/stream-sec/cdrgoat",
      "clone_url": "https://github.com/stream-sec/cdrgoat.git",
      "ssh_url": "git@github.com:stream-sec/cdrgoat.git",
      "homepage": null,
      "created_at": "2025-09-18T07:52:48Z",
      "updated_at": "2025-09-18T13:03:46Z",
      "pushed_at": "2025-09-18T07:55:01Z"
    },
    "stats": {
      "stars": 13,
      "forks": 0,
      "watchers": 13,
      "open_issues": 0,
      "size": 1396
    },
    "tech_info": {
      "language": "Shell",
      "languages": {
        "Shell": 131148,
        "HCL": 42540
      },
      "license": "BSD 3-Clause \"New\" or \"Revised\" License",
      "topics": []
    },
    "content": {
      "readme": "![CDRGoat](./assets/CDRGoat.png)\n\n\nCloud adoption has reshaped the enterprise attack surface, where adversaries can chain misconfigurations, excessive permissions, and runtime blind spots into full compromises.  \n**Cloud Detection & Response GOAT** is a scenario-driven, intentionally vulnerable framework designed to help defenders validate detection pipelines, practice SOC workflows and train analysts on realistic cloud attack paths - all in a safe, reproducible environment with no impact on production.  \n\nCDR GOAT enables:\n- **Advanced simulations** - Misconfigurations combined with live attacker behavior (privilege escalation, credential theft, lateral movement).  \n- **Detection & response validation** - Confirm alerts fire, understand context, and rehearse investigation workflows under realistic pressure.  \n- **SOC readiness** - Train analysts on real signals instead of abstract examples.  \n- **Purple teaming** - Run adversary emulation while measuring blue team effectiveness in real time.  \n\n&nbsp;\n\n## ‚ö†Ô∏è Warning\n- Do **not** deploy to production\n- Use only isolated sandbox/test accounts\n- Expect cloud usage costs while resources are running\n- Always destroy resources after finishing a scenario\n\n&nbsp;\n\n## ‚ú® Features\n- **Scenario‚Äëdriven attack paths** - Reproducible simulations of real‚Äëworld adversary tactics in cloud environments (IAM abuse, SSRF, privilege escalation, data exfiltration, etc.).\n- **Safe to run** - Resources are provisioned in isolated test accounts with minimal blast radius.\n- **Automated Attack Script** - A fully automated script to execute attacks end-to-end, reducing manual steps and ensuring repeatable outcomes.\n\n&nbsp;\n\n## üöÄ Getting Started\n\n#### üß© Prerequisites\n- Terraform ver. 1.5 or above\n- AWS account (sandbox recommended, do not run in production)  \n- AWS CLI configured with appropriate credentials  \n- jq utility for parsing JSON output  \n\n#### ‚öôÔ∏è Install Dependencies\nmacOS\n```bash\nbrew install terraform awscli jq\n```\nLinux\n```bash\nsudo apt update && sudo apt install -y terraform awscli jq\n```\n\n#### üóÇÔ∏è Simulation Scenarios\nThe simulation scenarios are organized by folder under `scenarios/`.  \nEach folder includes:  \n- A **Terraform plan** to provision the environment for the scenario.  \n- An **attack script** that automates the attack path, allowing defenders to focus on detection and response.  \n\nNavigate into a scenario folder to run Terraform and execute the attack script as described below.\n\n#### üèóÔ∏è Deploy\nBefore deploying, download the provided Terraform configuration and attack script to the machine where you will run the attack steps.\n\nUse the provided Terraform configuration to deploy the full lab environment.\n\nAt the end of the deployment Terraform will display output values (such as the public IP of the target instance). Save these details, you will need them when running the attack script.\nSome of this information might be sensitive and thus reducted by Terraform.\nIn order to reveal specific output we can use `terraform output` command.\n\nFor example to get output value of `leaked_user_secret_access_key` we can execute following:\n\n```bash\nterraform output leaked_user_access_key_id\n```\n\n‚ö†Ô∏è When a scenario‚Äôs initial step targets a public IP, add the public IP (or CIDR) of the machine that will run the attack script to the environment whitelist via terraform apply so the script can reach the target and complete any required interactions. See example\n\n```bash\nterraform init\nterraform apply -var='attack_whitelist=[\"87.68.140.7/32\",\"203.0.113.0/24\"]' -auto-approve\n```\n\n#### üéØ Attack Execution\nSince our focus is on the defender‚Äôs perspective, each scenario includes a **fully automated attack script**. Instead of manually typing commands, the script replays the attack path so you can observe detections and signals.\nYou may be prompted to provide inputs (e.g., your external IP). These are always displayed at the end of the Terraform deployment.\n\n```bash\nchmod +x attack.sh\n./attack.sh\n```\n\n\n![attack](./assets/attack.png)\n\n#### üßπ Clean Up\nWhen you are finished, destroy all resources to avoid ongoing costs. This will tear down the entire lab environment including all compute, networking, and IAM components created during deployment.\n\n```bash\nterraform destroy -var='attack_whitelist=[]' -auto-approve\n```\n\n&nbsp;\n\n## üìñ Usage Guide\nTerraform commands you‚Äôll use most often:  \n\n```bash\nterraform init      # prepare the working directory\nterraform apply     # deploy a scenario\nterraform destroy   # clean up resources\n```\n\n&nbsp;\n***\n&nbsp;\n\n## Contributing\nWe welcome contributions! You can submit pull requests for:  \n- New scenarios  \n- Bug fixes  \n- Documentation improvements\n  \n&nbsp;\n## üí∞ Cost\nEach scenario is designed with minimal cloud resources to reduce expenses and limit blast radius.  \nHowever, costs may still accrue while environments are running. To avoid unnecessary charges, always shut down and destroy the environment when you are finished.\n\n&nbsp;\n\n## üë• Contributors\n- P",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-19T03:08:06.001643"
  }
]