[
  {
    "basic_info": {
      "name": "GuitarPedal",
      "full_name": "torvalds/GuitarPedal",
      "owner": "torvalds",
      "description": null,
      "url": "https://github.com/torvalds/GuitarPedal",
      "clone_url": "https://github.com/torvalds/GuitarPedal.git",
      "ssh_url": "git@github.com:torvalds/GuitarPedal.git",
      "homepage": null,
      "created_at": "2025-09-17T01:01:29Z",
      "updated_at": "2025-09-18T09:02:53Z",
      "pushed_at": "2025-09-18T01:29:25Z"
    },
    "stats": {
      "stars": 388,
      "forks": 10,
      "watchers": 388,
      "open_issues": 2,
      "size": 1853
    },
    "tech_info": {
      "language": null,
      "languages": {},
      "license": "GNU General Public License v2.0",
      "topics": []
    },
    "content": {
      "readme": "## Random guitar pedal board design\n\n### Background\n\nThis is a personal toy project that has gone through several phases, but\nthe common theme has been that it makes absolutely no sense outside of\nthe very specific niche of \"Linus is trying to learn random things about\nelectronics\".\n\nSo keep that in mind: there is very little point to any of this to\nanybody else.  Don't expect some great useful guitar pedal experience.\n\nI call it my \"LEGO for adults\" hobby, because this got started when I\nwanted to extend my traditional after-Christmas activity (which was\nreceiving and building _actual_ LEGO kits, which has been a thing for me\nsince I was a wee tyke) with something else.\n\nSo for Christmas 2024, I got a new soldering iron and randomly started\ndoing guitar pedal kits.  And so over the next month or two, I built at\nleast two dozen kits, and had to literally look for victims to give them\naway to because I had no use for them myself.\n\n> [!NOTE]\n> Of all the kits I built, the ones I enjoyed the most were the Aion FX\n> ones, and if you are looking for a kit build of traditional analog\n> guitar pedals, I can heartily recommend them.\n>\n> The documentation, the customer service, the components, and the\n> enclosures were all top notch. See [\"Aion FX\"](https://aionfx.com/)\n\nAnyway, after building a lot of these traditional analog guitar pedal\nkits I decided I really wanted to actually understand what they did,\nbecause I really had very little experience with any analog circuits.\n\nWhile I've done some very limited electronics most of my life, almost\nall of it has been related to computers, so it's been either digital\nlogic or power supplies for them.\n\nAlso, I was looking for a different kind of soldering experience where\nthere was less snipping of legs of through-hole components.  I actually\nlike soldering SMT components, but that doesn't tend to be what those\nguitar pedal kits do.\n\nI had done some very limited PCB design with kicad a few years ago, so I\ndecided to just start learning more about analog circuits.  And then it\nkind of grew from that.\n\n### Electrical design\n\nThis is the \"fourth generation\" of my guitar pedal design journey, and\nis a new repository because the goal of the learning experience has\nevolved.\n\nWhat started out being about the analog circuits (and the power rails:\nthose were always a big thing) got to the point where I realized I\nreally want to do a mixed signal design: understanding what the circuits\ndo is one thing, re-creating some analog design from the 70s when you\ndon't actually care about the sound is another thing entirely.\n\nAlso, on the actual analog signal side, I started out using op-amps, but\nas I was attempting to learn how things actually worked, I had switched\nover to a \"discrete components only\" model, and this continues that\ntrend (except for the whole digital side, of course).\n\n> [!NOTE]\n> To me \"discrete components\" does include more optimized packages:\n> things like dual diodes or matched transistors, but not more complex\n> circuits like a op-amp (or a 555 timer or D Flip-flop or other classic\n> logic IC)\n\nAlso, because I don't typically *listen* to the end result, but look at\nit with a signal generator and an oscilloscope, I've grown to detest\npower supply noise.\n\nNot knowing what I was doing, quite a lot of my circuits have been very\nnoisy indeed, and have coupled in noise from the power supply into the\nsignal chain, and you can really see that on an oscilloscope even when\nit's not always audible.\n\nEven in op-amp designs, where the op-amp itself has a very high PSRR and\nisn't mixing power supply noise into the signal, my biasing circuits\nwere often not great, and so the op-amp would see not just the signal\nbut the power supply noise coming in through the DC biasing.\n\nAnd every time I tried a dual power rail (so that I could just keep the\nsignal ground-referenced), the noise from the switching ended up just\nalways noticeable, and the extra complexity was annoying when a lot of\neffects then didn't have any real use for the dual rail.\n\nFiltering obviously helps, but this is just a long-winded explanation\nfor why I ended up really appreciating the \"bias to ground\" JFET model\nfor the signal input side, and the common drain follower in particular.\n\nThat works with a single JFET (the MMBF5103 worked well for me), but my\nfavorite design so far is a dual-JFET LS844 with the second matched JFET\nused as a current sink.  It has basically infinite input impedance (and\ncould be DC coupled, although I do the coupling capacitor with resistor\nto ground) and gives a good output signal somewhere roughly in the\nmiddle of the single-supply 9V rail.\n\nSee [LS844 Application note](https://www.linearsystems.com/_files/ugd/7e8069_52b1022fbded45fab609459acb337629.pdf)\n\nWhy do I mention this in particular? Mainly because it's a great example\nof how completely *insane* my designs are.  That LS844 is used as a\nvoltage follower with a noticeable DC offset, and that single dual-JFET\nSOT-23-6 component is m",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-18T09:12:54.008326"
  },
  {
    "basic_info": {
      "name": "comfydeploy",
      "full_name": "comfy-deploy/comfydeploy",
      "owner": "comfy-deploy",
      "description": "ComfyDeployed",
      "url": "https://github.com/comfy-deploy/comfydeploy",
      "clone_url": "https://github.com/comfy-deploy/comfydeploy.git",
      "ssh_url": "git@github.com:comfy-deploy/comfydeploy.git",
      "homepage": null,
      "created_at": "2025-09-17T02:53:50Z",
      "updated_at": "2025-09-18T09:04:37Z",
      "pushed_at": "2025-09-17T15:03:11Z"
    },
    "stats": {
      "stars": 183,
      "forks": 27,
      "watchers": 183,
      "open_issues": 0,
      "size": 323
    },
    "tech_info": {
      "language": null,
      "languages": {},
      "license": "GNU General Public License v3.0",
      "topics": []
    },
    "content": {
      "readme": "# ComfyDeploy\n\nRe: Open-sourcing ComfyDeploy\n\nTL;DR: We are open-sourcing ComfyDeploy again, with full platform backend + frontend.\n\n[Discord](https://discord.gg/qtHUaVNRVM) | [Website](https://www.comfydeploy.com)\n\n---\n\n### How we got here?\n\nIn late 2023, I started ComfyDeploy as an open source project while I was working at my previous company. We had a problem deploying ComfyUI to our production server due to how complicated it is to get ComfyUI into a serverless environment.\n\nLittle did I know I was going to meet my co-founder Nick and embark on a journey I never dreamt of.\n\nI posted on Twitter about this little project that I was working on as an indie hacker, and it blew up overnight. I woke up to 100k impressions on the post. I put up my cal link and people started scheduling calls. I got to talk to a bunch of people across the globe, with people reaching out to help or potentially use ComfyDeploy.\n\nNick was one of the early contributors, and later we decided to apply to a bunch of accelerators together. It was early February, just a couple of weeks after we met. We applied to YC—we were unsure if we would even get in, but we still decided to quit our jobs, and the day after we quit, we got in.\n\nIt was insane to even think that we would have had a chance.\n\n### Life Changing Decision\n\nWe immediately said yes.\n\nWe got into YC with ComfyDeploy around 2.5K MRR.\n\nAround the same timeframe, ComfyOrg was introduced, Stability collapsed, and Flux just came out.\n\nWe kept building ComfyDeploy, for months, and kept things going.\n\nThe company was growing, but very slowly. We realized the biggest issue is that we are still really early, and it takes time for businesses and enterprises to really adopt such a niche tool. And we are not ComfyOrg.\n\n### Dynamic Changes\n\nMeanwhile, closed source models dropped, and a bunch of workflows that we knew became no longer useful.\n\nComing from a game developer background, I saw huge potential with ComfyUI at first, but I never would have imagined that one giant model could do exactly what you put into words, and you still need workflows to fine-tune and control the exact outputs. And ComfyDeploy did make it possible for teams to experiment with this.\n\nWe were just unsure about the future of the company. While we kept getting more and more business inquiries about ComfyUI, this is what really kept us going before giving up. It's a mixed signal.\n\nMeanwhile, my friends bootstrapped their mobile app to $500k a month. I was really stressed—were we going to live up to the aspirations that we set out, both personally and for the company?\n\n### Problems\n\nI feel immensely grateful for the customers that we were able to help, and this is something I feel forever privileged to experience—working with the best creative teams across the globe and brands that I look up to.\n\nBut we were stuck in the middle. First, we are not ComfyOrg; second, closed source models were doing things way better and slowly eating up the market.\n\nAnd it's inevitable that ComfyOrg will have some sort of cloud solution. And we do appreciate all the work ComfyOrg has put into making ComfyUI great again. We realized we would be competing in some ways especially in the cloud space, and we could never grow out of this, which constrained the company's growth at the same time.\n\nAs of today, ComfyDeploy is doing $29k MRR, and our last 30 days' revenue was $50k processed. Which is the highest we have ever got, but also the most depressing day I have ever had.\n\n### So what now?\n\nWe have been working for months on our pay-as-you-go tier—no longer requiring you to talk to us, just pay for the cloud resources you use. And also, going back to our roots: open-sourcing the ENTIRE CLOUD PLATFORM while continuing to support existing customers.\n\nOur current customers and creative teams are the people who kept us going for the last year, and I will be forever grateful. But this might just not be the field for us, and we respect Comfy and do not want to get in the way. And I think it's going to be really good for everyone.\n\n#### What does this mean for current customers: \n\nThe service will stay online and supported until the last standing user!\n\n#### What does this mean for future customers: \nYou can either go with the pay-as-you-go tier to get started, or self-host the entire platform. If ComfyOrg eventually builds something like ComfyDeploy, the official solution will be recommended by us!\n\n#### What does this mean for ComfyDeploy team: \n\nWe will continue supporting existing customers. At the same time we will start exploring new ideas. And this does not mean the end to us. But rather a fresh start. Stay tuned for what's coming next.\n\n#### What I will be doing for the next couple days\n1. Documentation and tutorials to set up ComfyDeploy\n2. Explore new problems that could resonate with us.\n\n### Credits\n\nA list of credits that got us here (doesn't go in order)\nComfy, Nick, Karrix, Jeff, Edgar, Ecjojo, Brad, Aashay and Semil, Jon, Choco",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-18T09:12:55.281276"
  },
  {
    "basic_info": {
      "name": "Gift-Buyer-Tg",
      "full_name": "GothemU/Gift-Buyer-Tg",
      "owner": "GothemU",
      "description": "Bot that searches and buys telegram gifts based on given criteria",
      "url": "https://github.com/GothemU/Gift-Buyer-Tg",
      "clone_url": "https://github.com/GothemU/Gift-Buyer-Tg.git",
      "ssh_url": "git@github.com:GothemU/Gift-Buyer-Tg.git",
      "homepage": "",
      "created_at": "2025-09-17T17:32:03Z",
      "updated_at": "2025-09-18T01:55:41Z",
      "pushed_at": "2025-09-17T17:37:02Z"
    },
    "stats": {
      "stars": 137,
      "forks": 22,
      "watchers": 137,
      "open_issues": 0,
      "size": 44
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 61661,
        "Batchfile": 536
      },
      "license": "Apache License 2.0",
      "topics": [
        "gift-buyer",
        "gift-buyer-tg",
        "telegram-gift",
        "telegram-gift-buyer"
      ]
    },
    "content": {
      "readme": "# Auto Telegram Gift Buyer Bot✈\n[![Static Badge](https://img.shields.io/badge/Telegram-Channel-Link?style=for-the-badge&logo=Telegram&logoColor=white&logoSize=auto&color=blue)](https://t.me/+pB6j65Kv7cdjZmU0)\n\n**A bot that searches for available gifts and purchases them based on given criteria. Supports integration with Telegram bot!**\n\n# Preview🖼\n\n<img width=\"666\" height=\"306\" alt=\"image\" src=\"https://github.com/user-attachments/assets/f69ae513-4a49-4678-9c46-0992dc9a2826\" />\n\n\n\n\n# Features✨\n- **Fully automatic scans for new gifts**\n- **Fully customizable settings**\n- **Telegram bot integration available**\n- **Incredible fast gift purchase(100+ gifts per second)**\n- **Continuously searches for gifts while the script is online.**\n- **(Check Tonnel gift prices right inside the script.**\n\nAnd more.... *in future*\n\n# Requirements\n- Python 3.10+🐍\n- Git 🟦\n- Pip 🟩\n- VPS for continuously operation(unnecessary)\n\n# Installation📩\n```shell\ngit clone https://github.com/GothemU/Gift-Buyer-Tg\ncd Gift-Buyer-Tg\nrun.bat\n```\n\n**OR**\n\n```shell\ngit clone https://github.com/GothemU/Gift-Buyer-Tg\ncd Gift-Buyer-Tg\npip install -r requirements.txt\npython main.py\n```\n\n# Configuration⚙\n**To configure the bot, edit the config.py file**✅\n\n| Settings | Description |\n|----------------------------|:-------------------------------------------------------------------------------------------------------------:|\n| **API_ID / API_HASH**      | Platform data from which to run the Telegram session (get it from my.telegram.org)                                   |       \n| **PHONE_NUMBER**               | Phone number of the account you want to use to buy gifts                                                                 |\n| **BOT_TOKEN**              |  Get Bot Token from [@BotFather](https://t.me/BotFather) to receive notifications and control the bot through telegram(unnecessary)                                                                               |\n| **CHAT_ID** | Bot chat ID, for telegram bot integration(unnecessary)                                                                       |\n| **NFT_GIFTS_ONLY** | Buys upgradable gifts only(put True or False)                                                                     |\n| **GIFT_MIN_PRICE** | Minimum price of the gift you want to buy (put 0 for no limit)                                                                      |\n| **GIFT_MAX_PRICE** | Maximum price of the gift you want to buy(put 0 for no limit)                                                    |\n| **MAX_GIFT_SUPPLY** | Highest supply of the gift, the script wont buy if number is higher than supply of the gift(put 0 for no limit)                                                                      |\n| **QUANTITY** | Quantity of purchasable gifts(If you enter 0, the script will continue buying until you run out of stars.)                                                                   |\n| **RECEPIENT** |Recepient of the gifts. Can be @channel or @user                                                                      |\n| **BUY_MULTIPLE_GIFTS** | Buys all available gifts that meet the criteria with priority to lower supply(True or False)                                                                      |\n| **HIDE_SENDER_NAME** | Hide name of the gift sender or not                                                                     |\n\n## Support🌟\n**Thanks for using my scripts!❤**\n\n- ***Don't forget to put stars, it supports me a lot⭐***\n\n- ***JOIN OUR TELEGRAM [CHAT](https://t.me/+9j5RcKMfT5s4M2Q0)***\n\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-18T09:12:56.519794"
  },
  {
    "basic_info": {
      "name": "pingoo",
      "full_name": "pingooio/pingoo",
      "owner": "pingooio",
      "description": "The fast and secure Load Balancer / API Gateway / Reverse Proxy with built-in service discovery, GeoIP, WAF, bot protection and much more - https://pingoo.io",
      "url": "https://github.com/pingooio/pingoo",
      "clone_url": "https://github.com/pingooio/pingoo.git",
      "ssh_url": "git@github.com:pingooio/pingoo.git",
      "homepage": "https://pingoo.io",
      "created_at": "2025-09-17T07:18:40Z",
      "updated_at": "2025-09-18T08:58:37Z",
      "pushed_at": "2025-09-18T07:41:14Z"
    },
    "stats": {
      "stars": 115,
      "forks": 4,
      "watchers": 115,
      "open_issues": 6,
      "size": 327
    },
    "tech_info": {
      "language": "Rust",
      "languages": {
        "Rust": 183548,
        "TypeScript": 7653,
        "Dockerfile": 6485,
        "Makefile": 2265,
        "Shell": 1620,
        "HTML": 892,
        "CSS": 620,
        "Vim Script": 19
      },
      "license": "MIT License",
      "topics": [
        "akamai",
        "anti-bot",
        "apache2",
        "api",
        "api-gateway",
        "captcha",
        "cloudflare",
        "fastly",
        "firewall",
        "haproxy",
        "load-balancer",
        "nginx",
        "pingoo",
        "proxy",
        "quic",
        "reverse-proxy",
        "rust",
        "security",
        "service-discovery",
        "waf"
      ]
    },
    "content": {
      "readme": "<p align=\"center\">\n  <a href=\"https://pingoo.io\" target=\"_blank\" rel=\"noopener\"><img alt=\"Pingoo logo\" src=\"https://pingoo.io/icon-256.png\" height=\"128\" /></a>\n  <h1 align=\"center\">Pingoo</h1>\n  <h3 align=\"center\">The fast and secure Load Balancer / API Gateway / Reverse Proxy with built-in service discovery, GeoIP, WAF, bot protection and much more</h3>\n  <h3 align=\"center\">\n    <a href=\"https://pingoo.io\">Documentation</a> | <a href=\"https://kerkour.com/announcing-pingoo\">Read the launch post</a>\n  </h3>\n</p>\n\nOpen Source load balancers and reverse proxies are stuck in the past century with a very slow pace of development and most of the important features reserved for \"Enterprise Editions\" which lead developers to use third-party cloud services, exposing their users' traffic to legal, security and reliability risks.\n\nPingoo is a modern Load Balancer / API Gateway / Reverse Proxy that run on your own servers and already have (or will have soon) all the features you expect from managed services and even more. All of that with a huge boost in performance and security thanks to reduced latency and, of course, Rust ;)\n\n* Service Discovery (Docker, DNS...)\n* Web Application Firewall (WAF)\n* Easy compliance because the data never leaves your servers\n* Bot protection and management\n* TCP proxying\n* Post-Quantum TLS\n* GeoIP (country, ASN)\n* Static sites\n* And much more\n\n> ⚠️ Pingoo is currently in beta, use with caution.\n\n## Quickstart\n\n```bash\n# You have a static site in the www folder\n$ ls www\nindex.html\n$ docker run --rm -ti --network host -v `pwd`/www:/var/wwww ghcr.io/pingooio/pingoo\n# Pingoo is now listenning on http://0.0.0.0:8080\n```\n\n## Documentation\n\nSee https://pingoo.io\n\n\n## Updates\n\n[Click Here](https://kerkour.com/blog) to visit the blog and [subscribe](https://kerkour.com/subscribe) by RSS or email to get weekly / monthly updates. No spam ever, only technical deep dives.\n\n\n## Contributing\n\nPlease open an issue to discuss your idea before submitting a Pull Request.\n\n\n## Support\n\nDo you have custom needs? Do you want your features to be prioritized? Are you under attack and need help? Do you need support for deploying and self-hosting Pingoo?\n\nFeel free to reach our team of experts to see how we can help: https://pingoo.io/contact\n\n\n## Security\n\nWe are committed to make Pingoo the most secure Load Balancer / Reverse Proxy in the universe and beyond. If you've found a security issue in Pingoo, we appreciate your help in disclosing it to us in a responsible manner by contacting us: https://pingoo.io/contact\n\n\n## License\n\nMIT. See `LICENSE.txt`\n\nForever Open Source. No Open Core or \"Enterprise Edition\".\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-18T09:12:57.752452"
  },
  {
    "basic_info": {
      "name": "gemini_icpc2025",
      "full_name": "google-deepmind/gemini_icpc2025",
      "owner": "google-deepmind",
      "description": "Gemini 2025 ICPC World Finals Code Submissions",
      "url": "https://github.com/google-deepmind/gemini_icpc2025",
      "clone_url": "https://github.com/google-deepmind/gemini_icpc2025.git",
      "ssh_url": "git@github.com:google-deepmind/gemini_icpc2025.git",
      "homepage": "",
      "created_at": "2025-09-17T10:57:24Z",
      "updated_at": "2025-09-18T09:09:08Z",
      "pushed_at": "2025-09-17T11:09:46Z"
    },
    "stats": {
      "stars": 72,
      "forks": 4,
      "watchers": 72,
      "open_issues": 0,
      "size": 27
    },
    "tech_info": {
      "language": "C++",
      "languages": {
        "C++": 54237
      },
      "license": "Apache License 2.0",
      "topics": []
    },
    "content": {
      "readme": "# Gemini ICPC 2025 Submissions\n\nThis repository contains the code submissions from an advanced version of\n[Gemini 2.5 Deep Think](https://blog.google/products/gemini/gemini-2-5-deep-think/)\nfor the 2025 International Collegiate Programming Contest World Finals.\n\n## License and disclaimer\n\nCopyright 2025 Google LLC\n\nAll software is licensed under the Apache License, Version 2.0 (Apache 2.0);\nyou may not use this file except in compliance with the Apache 2.0 license.\nYou may obtain a copy of the Apache 2.0 license at:\nhttps://www.apache.org/licenses/LICENSE-2.0\n\nAll other materials are licensed under the Creative Commons Attribution 4.0\nInternational License (CC-BY). You may obtain a copy of the CC-BY license at:\nhttps://creativecommons.org/licenses/by/4.0/legalcode\n\nUnless required by applicable law or agreed to in writing, all software and\nmaterials distributed here under the Apache 2.0 or CC-BY licenses are\ndistributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,\neither express or implied. See the licenses for the specific language governing\npermissions and limitations under those licenses.\n\nThis is not an official Google product.\n",
      "default_branch": "release"
    },
    "fetched_at": "2025-09-18T09:12:58.967201"
  },
  {
    "basic_info": {
      "name": "TradingView-PCapp",
      "full_name": "darklight797t6/TradingView-PCapp",
      "owner": "darklight797t6",
      "description": null,
      "url": "https://github.com/darklight797t6/TradingView-PCapp",
      "clone_url": "https://github.com/darklight797t6/TradingView-PCapp.git",
      "ssh_url": "git@github.com:darklight797t6/TradingView-PCapp.git",
      "homepage": null,
      "created_at": "2025-09-17T12:35:15Z",
      "updated_at": "2025-09-17T22:18:21Z",
      "pushed_at": "2025-09-17T12:48:01Z"
    },
    "stats": {
      "stars": 55,
      "forks": 0,
      "watchers": 55,
      "open_issues": 0,
      "size": 1463
    },
    "tech_info": {
      "language": null,
      "languages": {},
      "license": null,
      "topics": []
    },
    "content": {
      "readme": "\n# TradingView App on pc\n\nAccess realtime market prices and indicator values from TradingView!\n\n▌Features\n\n- [x] Access to TradingView's enhanced data feeds\n- [x] Efficiently backtest strategies and explore different settings\n- [x] Retrieve drawings from your charts\n- [x] Compatible with invite-only indicators\n- [x] Supports a large number of simultaneous indicators (subject to rate limits)\n- [x] Realtime data streaming\n- [x] Access TradingView's technical analysis data\n- [x] Replay mode + Simulated Replay mode (for free plans)\n- [x] Retrieve historical data for specific date ranges\n- [ ] TradingView socket server compatibility layer (alpha)\n- [ ] Interact with public chats\n- [ ] Access Screener top values\n- [ ] Get Hotlists\n- [ ] Get Calendar\n- IF YOU WANT A FEATURE, ASK ME!\n\n▌Potential Uses\n\n- Algorithmic trading\n- Custom Discord alerts\n- Advanced backtesting\n- Machine learning research\n- Free replay mode on certain timeframes, subject to limitations\n\n___\n\n▌Installation\n\nStable version:\n\nnpm i @mathieuc/tradingview\n\nLast version:\n\nnpm i github:Mathieu2301/TradingView-API\n\n▌Examples\n\nYou can find all the examples and snippets in ./examples folder.\n\n▌Before opening an issue\n\nPlease look at examples and previously resolved issues before opening a new one. I can't help everyone (especially for questions that are not library related but JavaScript related). Thank you for your understanding.\n___\n\n```\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-18T09:13:00.181229"
  },
  {
    "basic_info": {
      "name": "worldexplorer",
      "full_name": "mschneider456/worldexplorer",
      "owner": "mschneider456",
      "description": "[SIGGRAPH Asia 2025] WorldExplorer: Towards Generating Fully Navigable 3D Scenes",
      "url": "https://github.com/mschneider456/worldexplorer",
      "clone_url": "https://github.com/mschneider456/worldexplorer.git",
      "ssh_url": "git@github.com:mschneider456/worldexplorer.git",
      "homepage": "https://mschneider456.github.io/world-explorer/",
      "created_at": "2025-09-17T01:03:33Z",
      "updated_at": "2025-09-18T08:52:38Z",
      "pushed_at": "2025-09-18T08:17:31Z"
    },
    "stats": {
      "stars": 44,
      "forks": 3,
      "watchers": 44,
      "open_issues": 0,
      "size": 130402
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 1361565,
        "Shell": 3692
      },
      "license": "Other",
      "topics": [
        "3d-gaussian-splatting",
        "scene-generation",
        "video-diffusion"
      ]
    },
    "content": {
      "readme": "# [SIGGRAPH Asia 2025] WorldExplorer: Towards Generating Fully Navigable 3D Scenes\n\nWorldExplorer produces high-quality scenes that remain stable under large camera motion, enabling realistic and unrestricted exploration.\n\nThis is the official repository for the SIGGRAPH Asia 2025 paper \"WorldExplorer: Towards Generating Fully Navigable 3D Scenes\".\n\n[[arXiv](https://arxiv.org/abs/2506.01799)] [[Project Page](https://mschneider456.github.io/world-explorer/)] [[Video](https://youtu.be/N6NJsNyiv6I)]\n\n![Teaser](docs/teaser.jpg \"WorldExplorer\")\n\nIf you find WorldExplorer useful for your work please consider giving a star ⭐️ and citing:\n\n```\n@InProceedings{schneider_hoellein_2025_worldexplorer,\n    title={WorldExplorer: Towards Generating Fully Navigable 3D Scenes},\n    author={Schneider, Manuel-Andreas and H{\\\"o}llein, Lukas and Nie{\\ss}ner, Matthias},\n    journal={arXiv preprint arXiv:2506.01799},\n    year={2025}\n}\n```\n\n\n## Installation\n\nWe have tested the below instructions with PyTorch 2.8.0+cu128, CUDA 12.8, PyTorch3D 0.7.8. \n\n```\nconda env create -f environment.yml\nconda activate worldexplorer\n\ncd model/stable-virtual-camera\npip install -e .\ncd ../..\n\nCUDA_HOME=/usr/local/cuda-12.8 pip install \"git+https://github.com/facebookresearch/pytorch3d.git@stable\"\npip install diffusers[\"torch\"] transformers protobuf transformers[sentencepiece] easydict plyfile\npip install --upgrade pip setuptools && pip install git+https://github.com/nerfstudio-project/nerfstudio.git@50e0e3c\npip install -U xformers --index-url https://download.pytorch.org/whl/cu128\n```\n\nIf you encounter [this issue](https://github.com/nerfstudio-project/gsplat/issues/249) during 3DGS optimization, it's likely that your CUDA version is not consistent between terminal sessions (check through `nvcc --version`). You can ensure the CUDA version stays fixed by setting the PATH variable in your `~/.bashrc`. For CUDA 12.8, this can be done by running:\n\n```\necho 'export PATH=/usr/local/cuda-12.8/bin:$PATH' >> ~/.bashrc\n```\n\n### Download checkpoints\n\nDownload the pretrained model checkpoints:\n\n```\nwget -O model/Depth_Anything_V2/checkpoints/depth_anything_v2_metric_hypersim_vitl.pth \"https://huggingface.co/depth-anything/Depth-Anything-V2-Metric-Hypersim-Large/resolve/main/depth_anything_v2_metric_hypersim_vitl.pth?download=true\"\n\nwget -O model/Video-Depth-Anything/checkpoints/video_depth_anything_vits.pth \"https://huggingface.co/depth-anything/Video-Depth-Anything-Small/resolve/main/video_depth_anything_vits.pth?download=true\"\n``` \n\n### Authenticate with Hugging Face\n\nThe `FLUX.1-dev` model used for initial image generation is gated and requires authentication.\n\n1. Request access to the model on its Hugging Face page:\n   https://huggingface.co/black-forest-labs/FLUX.1-dev\n\n2. Log in through the terminal and add a read-only token: \n   ```bash\n   pip install -U \"huggingface_hub[cli]\"\n   hf auth login\n   ```\n\nThe weights for the model will automatically be downloaded and saved the first time you generate a scene.\n\n## Usage\n\nWorldExplorer provides a command-line interface through `worldexplorer.py` for generating scene scaffolds (panoramas with known camera extrinsics and intrinsics) and 3D scenes.\n\n### Quick Start\n\nGenerate a complete 3D scene from a text description:\n```bash\npython worldexplorer.py generate \"bioluminescent, gravity-defying, telepathic cosmic jellyfish hive\"\n```\n\nPlease note that full 3D scene generation takes about 6 to 7 hours as 32 videos are generated in the process. The simplest way to track your progress is by checking the output at `./scenes/[theme_name]_[translation_scaling_factor]_[timestamp]/img2trajvid`.\n\n### Available Commands\n\n#### 1. `generate` - Full Pipeline (Scaffold + 3D Expansion)\nGenerates panoramic images from text and expands them into navigable 3D scenes.\n\n```bash\n# Indoor scene with automatic selection (CLIP-based)\npython worldexplorer.py generate \"Modern Apartment\" --mode automatic\n\n# Indoor scene with manual selection (yiels best results!)\npython worldexplorer.py generate \"Cozy Library\" --mode manual\n\n# Indoor scene with fast mode\npython worldexplorer.py generate \"Rustic Farmhouse (Wood, Leather, Wool)\" --mode fast\n\n# Custom/outdoor scene (requires four separate prompts for each viewing direction)\npython worldexplorer.py generate --custom\n\n# Skip 3D expansion (scaffold only)\npython worldexplorer.py generate \"Beach House\" --skip-expansion\n```\n\n**Options:**\n- `--mode, -m`: Panorama scaffold generation mode - `fast` (single output), `automatic` (CLIP selection), or `manual` (human curation)\n- `--translation-scaling, -t`: Movement scale factor (default: 3.0). The higher the translation scaling factor, the further your trajectories will expand into the scene. For indoor scenes the recommended range is 2.0 to 7.0, and for outdoors scenes 8.0 to 20.0. \n- `--skip-expansion`: Generate scaffold only without 3D expansion\n- `--custom, -c`: Custom mode for outdoor scenes or custom indoor scenes\n- `--root-dir`: Directory containing ",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-18T09:13:01.447720"
  },
  {
    "basic_info": {
      "name": "ECE_F_CRT_PYTHON",
      "full_name": "gilshan-s/ECE_F_CRT_PYTHON",
      "owner": "gilshan-s",
      "description": "CODING",
      "url": "https://github.com/gilshan-s/ECE_F_CRT_PYTHON",
      "clone_url": "https://github.com/gilshan-s/ECE_F_CRT_PYTHON.git",
      "ssh_url": "git@github.com:gilshan-s/ECE_F_CRT_PYTHON.git",
      "homepage": null,
      "created_at": "2025-09-17T04:49:52Z",
      "updated_at": "2025-09-17T05:02:14Z",
      "pushed_at": "2025-09-17T04:49:52Z"
    },
    "stats": {
      "stars": 38,
      "forks": 41,
      "watchers": 38,
      "open_issues": 0,
      "size": 0
    },
    "tech_info": {
      "language": null,
      "languages": {},
      "license": null,
      "topics": []
    },
    "content": {
      "readme": "# ECE_F_CRT_PYTHON\nCODING\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-18T09:13:02.684715"
  },
  {
    "basic_info": {
      "name": "usgc-invoice",
      "full_name": "usgraphics/usgc-invoice",
      "owner": "usgraphics",
      "description": "Invoice LaTeX template",
      "url": "https://github.com/usgraphics/usgc-invoice",
      "clone_url": "https://github.com/usgraphics/usgc-invoice.git",
      "ssh_url": "git@github.com:usgraphics/usgc-invoice.git",
      "homepage": null,
      "created_at": "2025-09-17T17:37:28Z",
      "updated_at": "2025-09-18T09:07:30Z",
      "pushed_at": "2025-09-17T17:40:48Z"
    },
    "stats": {
      "stars": 37,
      "forks": 4,
      "watchers": 37,
      "open_issues": 2,
      "size": 5
    },
    "tech_info": {
      "language": "TeX",
      "languages": {
        "TeX": 2539
      },
      "license": "BSD 3-Clause \"New\" or \"Revised\" License",
      "topics": []
    },
    "content": {
      "readme": "# usgc-invoice\nLaTeX template used by U.S. Graphics Company. Licensed under BSD 3 clause license, see LICENSE.md. Credit to U.S. Graphics Company would be much appreciated :)\n\n<img width=\"640\" alt=\"screenshot-2025-09-17_19-38-40\" src=\"https://github.com/user-attachments/assets/74e11f1d-00c7-48b5-b9ed-c9773220a910\" />\n",
      "default_branch": "master"
    },
    "fetched_at": "2025-09-18T09:13:04.014023"
  },
  {
    "basic_info": {
      "name": "daedalus-keyboard",
      "full_name": "Perseus333/daedalus-keyboard",
      "owner": "Perseus333",
      "description": "An open-source, split, wireless ergonomic keyboard with trackpad and encoder",
      "url": "https://github.com/Perseus333/daedalus-keyboard",
      "clone_url": "https://github.com/Perseus333/daedalus-keyboard.git",
      "ssh_url": "git@github.com:Perseus333/daedalus-keyboard.git",
      "homepage": "https://perseuslynx.dev/projects/daedalus-kb",
      "created_at": "2025-09-17T09:29:38Z",
      "updated_at": "2025-09-18T08:40:02Z",
      "pushed_at": "2025-09-18T08:39:59Z"
    },
    "stats": {
      "stars": 30,
      "forks": 0,
      "watchers": 30,
      "open_issues": 0,
      "size": 98233
    },
    "tech_info": {
      "language": null,
      "languages": {},
      "license": "GNU Affero General Public License v3.0",
      "topics": [
        "ergonomic-keyboard",
        "keyboard",
        "mechanical-keyboard",
        "rotary-encoder",
        "split-keyboard",
        "trackpad",
        "wireless-keyboard",
        "zmk"
      ]
    },
    "content": {
      "readme": "# Daedalus Keyboard\n\nDaedalus is a compact open-source, split, wireless ergonomic keyboard with trackpad and encoder.\n\n![Image](/assets/main-img.jpg)\n\n## Daedalus Keyboard\n\nIt's a 36 key low-profile, split, wireless, ergonomic keyboard that includes a trackpad and a rotary encoder powered with ZMK on 2 Nice!Nano microcontrollers running on 2 110mAh batteries. It features a key splay inspired from the TOTEM, and a key layout modelled after my hand and inspired by the Corne. It's meant to be portable, silent and unobstrusive thanks to the Ambient Twighlight silent switches and the wireless connectivity.\n\n## Mission\n\nThe Daedalus Keyboard has been a hobby project that I worked on during the last year. It has been my first real hardware project that I've done, and I learned a lot throughout the way. Therefore, I aim for this project to be the most open and educational keyboard project possible for everyone.\n\nTherefore, here you will find everything needed to get started or to satiate your curiosity: all of the code, the schematics, the CAD files and even my notes during research.\n\nIf you're interested in building your own keyboard, then the [report](/docs/report.pdf) might interest you. It is a compreheensive document that describes the whole process of creation of the keyboard step by step, whilst giving some useful tips throughout and some valuable lessons learned at the end.\n\n## Repository Navigation\n\nThe repository is split in the following directories:\n\n- **Chassis**: Everything related to the chassis and components that are not hardware - CAD files, drawings and files ready for 3D printing or laser cutting right away\n- **Hardware**: Related to the PCB and electronic components - KiCAD schematic and PCB, and even gerber files to submit to a PCB manufacturer\n- **Firmware**: The code necessary to run the keyboard - Keybinds, ZMK config, etc.\n- **Docs**: The documentation to help you get started building this keyboard or any other - Build log, BOM, research, etc.\n\n## Current development\n\nCurrently I'm working on adding to this repository the following, in order:\n\n- Porting the CATIA files into an open source file format\n- A standalone step by step build guide (& BOM)\n- A contributing/editing guide\n- Publishing the Drawings\n\n## Contributing\n\nYou're welcome to contribute to this project in any way shape or form that is constructive.\n\n## Licence\n\nThis project is licenced differently for each domain, with the intention of promoting open source hardware and software:\n\n- Firmware and code: [GNU Affero General Public License v3 (AGPLv3)](./LICENSE-FIRMWARE)\n- Hardware: [CERN Open Hardware License v2 - Strongly Reciprocal](./LICENSE-HARDWARE)\n- Documentation and Media: [Creative Commons Attribution-ShareAlike 4.0](https://creativecommons.org/licenses/by-sa/4.0/)\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-18T09:13:05.226255"
  },
  {
    "basic_info": {
      "name": "1A-OneClick",
      "full_name": "AndresDevelopes/1A-OneClick",
      "owner": "AndresDevelopes",
      "description": "Edit and Have Fun With Your Pics Like Never Before. Start Free Now --> https://tinyurl.com/AIRemoveNextGen",
      "url": "https://github.com/AndresDevelopes/1A-OneClick",
      "clone_url": "https://github.com/AndresDevelopes/1A-OneClick.git",
      "ssh_url": "git@github.com:AndresDevelopes/1A-OneClick.git",
      "homepage": "",
      "created_at": "2025-09-17T13:38:23Z",
      "updated_at": "2025-09-18T09:02:34Z",
      "pushed_at": "2025-09-17T13:42:31Z"
    },
    "stats": {
      "stars": 25,
      "forks": 0,
      "watchers": 25,
      "open_issues": 0,
      "size": 0
    },
    "tech_info": {
      "language": null,
      "languages": {},
      "license": null,
      "topics": [
        "deepnude-download"
      ]
    },
    "content": {
      "readme": "# [See more with less...effort](https://tinyurl.com/AIRemoveNextGen)\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-18T09:13:06.427427"
  },
  {
    "basic_info": {
      "name": "homed-wiki",
      "full_name": "u236/homed-wiki",
      "owner": "u236",
      "description": null,
      "url": "https://github.com/u236/homed-wiki",
      "clone_url": "https://github.com/u236/homed-wiki.git",
      "ssh_url": "git@github.com:u236/homed-wiki.git",
      "homepage": null,
      "created_at": "2025-09-17T15:45:06Z",
      "updated_at": "2025-09-18T06:56:35Z",
      "pushed_at": "2025-09-18T06:56:32Z"
    },
    "stats": {
      "stars": 23,
      "forks": 1,
      "watchers": 23,
      "open_issues": 1,
      "size": 16336
    },
    "tech_info": {
      "language": null,
      "languages": {},
      "license": null,
      "topics": []
    },
    "content": {
      "readme": "![HOMEd Wiki](.github/logo.png)\n\n# HOMEd Wiki\n\nФайлы для сайта документации HOMEd:\\\nhttps://wiki.homed.dev\n",
      "default_branch": "master"
    },
    "fetched_at": "2025-09-18T09:13:07.654871"
  },
  {
    "basic_info": {
      "name": "autozero-gpt",
      "full_name": "ai-joon/autozero-gpt",
      "owner": "ai-joon",
      "description": null,
      "url": "https://github.com/ai-joon/autozero-gpt",
      "clone_url": "https://github.com/ai-joon/autozero-gpt.git",
      "ssh_url": "git@github.com:ai-joon/autozero-gpt.git",
      "homepage": null,
      "created_at": "2025-09-17T09:34:54Z",
      "updated_at": "2025-09-18T07:45:25Z",
      "pushed_at": "2025-09-18T01:14:59Z"
    },
    "stats": {
      "stars": 20,
      "forks": 2,
      "watchers": 20,
      "open_issues": 0,
      "size": 107177
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 1546291,
        "TypeScript": 591591,
        "Dart": 203562,
        "HCL": 27393,
        "C++": 23419,
        "CMake": 18862,
        "MDX": 12901,
        "CSS": 9233,
        "Smarty": 7518,
        "Dockerfile": 6417,
        "JavaScript": 5968,
        "Shell": 4513,
        "HTML": 3413,
        "Ruby": 2803,
        "PLpgSQL": 2443,
        "Jinja": 2398,
        "Swift": 2384,
        "C": 1425,
        "Batchfile": 478,
        "Kotlin": 140,
        "Objective-C": 38
      },
      "license": null,
      "topics": []
    },
    "content": {
      "readme": "# AutoGPT: Build, Deploy, and Run AI Agents\n\n**AutoGPT** is a powerful platform that allows you to create, deploy, and manage continuous AI agents that automate complex workflows. \n\n## Hosting Options \n   - Download to self-host\n   - [Join the Waitlist](https://bit.ly/3ZDijAI) for the cloud-hosted beta  \n\n## How to Setup for Self-Hosting\n> [!NOTE]\n> Setting up and hosting the AutoGPT Platform yourself is a technical process. \n> If you'd rather something that just works, we recommend [joining the waitlist](https://bit.ly/3ZDijAI) for the cloud-hosted beta.\n\nhttps://github.com/user-attachments/assets/d04273a5-b36a-4a37-818e-f631ce72d603\n\nThis tutorial assumes you have Docker, VSCode, git and npm installed.\n\n### 🧱 AutoGPT Frontend\n\nThe AutoGPT frontend is where users interact with our powerful AI automation platform. It offers multiple ways to engage with and leverage our AI agents. This is the interface where you'll bring your AI automation ideas to life:\n\n   **Agent Builder:** For those who want to customize, our intuitive, low-code interface allows you to design and configure your own AI agents. \n   \n   **Workflow Management:** Build, modify, and optimize your automation workflows with ease. You build your agent by connecting blocks, where each block     performs a single action.\n   \n   **Deployment Controls:** Manage the lifecycle of your agents, from testing to production.\n   \n   **Ready-to-Use Agents:** Don't want to build? Simply select from our library of pre-configured agents and put them to work immediately.\n   \n   **Agent Interaction:** Whether you've built your own or are using pre-configured agents, easily run and interact with them through our user-friendly      interface.\n\n   **Monitoring and Analytics:** Keep track of your agents' performance and gain insights to continually improve your automation processes.\n\n[Read this guide](https://docs.agpt.co/server/new_blocks/) to learn how to build your own custom blocks.\n\n### 💽 AutoGPT Server\n\nThe AutoGPT Server is the powerhouse of our platform This is where your agents run. Once deployed, agents can be triggered by external sources and can operate continuously. It contains all the essential components that make AutoGPT run smoothly.\n\n   **Source Code:** The core logic that drives our agents and automation processes.\n   \n   **Infrastructure:** Robust systems that ensure reliable and scalable performance.\n   \n   **Marketplace:** A comprehensive marketplace where you can find and deploy a wide range of pre-built agents.\n\n### 🐙 Example Agents\n\nHere are two examples of what you can do with AutoGPT:\n\n1. **Generate Viral Videos from Trending Topics**\n   - This agent reads topics on Reddit.\n   - It identifies trending topics.\n   - It then automatically creates a short-form video based on the content. \n\n2. **Identify Top Quotes from Videos for Social Media**\n   - This agent subscribes to your YouTube channel.\n   - When you post a new video, it transcribes it.\n   - It uses AI to identify the most impactful quotes to generate a summary.\n   - Then, it writes a post to automatically publish to your social media. \n\nThese examples show just a glimpse of what you can achieve with AutoGPT! You can create customized workflows to build agents for any use case.\n\n---\n",
      "default_branch": "master"
    },
    "fetched_at": "2025-09-18T09:13:08.939066"
  },
  {
    "basic_info": {
      "name": "lazycommit",
      "full_name": "KartikLabhshetwar/lazycommit",
      "owner": "KartikLabhshetwar",
      "description": "A CLI that writes your git commit messages for you with AI using Groq. Never write a commit message again.",
      "url": "https://github.com/KartikLabhshetwar/lazycommit",
      "clone_url": "https://github.com/KartikLabhshetwar/lazycommit.git",
      "ssh_url": "git@github.com:KartikLabhshetwar/lazycommit.git",
      "homepage": "https://www.npmjs.com/package/lazycommitt",
      "created_at": "2025-09-17T13:31:09Z",
      "updated_at": "2025-09-18T08:56:43Z",
      "pushed_at": "2025-09-18T08:57:05Z"
    },
    "stats": {
      "stars": 18,
      "forks": 1,
      "watchers": 18,
      "open_issues": 1,
      "size": 746
    },
    "tech_info": {
      "language": "TypeScript",
      "languages": {
        "TypeScript": 68070,
        "HTML": 6323,
        "Ruby": 614
      },
      "license": "Apache License 2.0",
      "topics": [
        "ai",
        "cli",
        "git",
        "github",
        "groq",
        "groq-api",
        "npm",
        "npm-package",
        "openai"
      ]
    },
    "content": {
      "readme": "<div align=\"center\">\n  <div>\n    <h1 align=\"center\">lazycommit</h1>\n<img width=\"2816\" height=\"1536\" alt=\"lazycommit\" src=\"https://github.com/user-attachments/assets/ee0419ef-2461-4b45-8509-973f3bb0f55c\" />\n\n  </div>\n\t<p>A CLI that writes your git commit messages for you with AI using Groq. Never write a commit message again.</p>\n\t<a href=\"https://www.npmjs.com/package/lazycommitz\"><img src=\"https://img.shields.io/npm/v/lazycommitt\" alt=\"Current version\"></a>\n\t<a href=\"https://github.com/KartikLabhshetwar/lazycommit\"><img src=\"https://img.shields.io/github/stars/KartikLabhshetwar/lazycommit\" alt=\"GitHub stars\"></a>\n\t<a href=\"https://github.com/KartikLabhshetwar/lazycommit/blob/main/LICENSE\"><img src=\"https://img.shields.io/npm/l/lazycommitt\" alt=\"License\"></a>\n</div>\n\n---\n\n## Setup\n\n> The minimum supported version of Node.js is v18. Check your Node.js version with `node --version`.\n\n1. Install _lazycommit_:\n\n   ```sh\n   npm install -g lazycommitt\n   ```\n\n### Install via Homebrew (macOS)\n\n```sh\nbrew install lazycommit\n```\n\nUpgrade:\n\n```sh\nbrew upgrade lazycommit\n```\n\n2. Retrieve your API key from [Groq Console](https://console.groq.com/keys)\n\n   > Note: If you haven't already, you'll have to create an account and get your API key.\n\n3. Set the key so lazycommit can use it:\n\n   ```sh\n   lazycommit config set GROQ_API_KEY=<your token>\n   ```\n\n   This will create a `.lazycommit` file in your home directory.\n\n### Upgrading\n\nCheck the installed version with:\n\n```\nlazycommit --version\n```\n\nIf it's not the [latest version](https://github.com/KartikLabhshetwar/lazycommit/releases/latest), run:\n\n```sh\nnpm update -g lazycommitt\n```\n\n## Usage\n\n### CLI mode\n\nYou can call `lazycommit` directly to generate a commit message for your staged changes:\n\n```sh\ngit add <files...>\nlazycommit\n```\n\n`lazycommit` passes down unknown flags to `git commit`, so you can pass in [`commit` flags](https://git-scm.com/docs/git-commit).\n\nFor example, you can stage all changes in tracked files as you commit:\n\n```sh\nlazycommit --all # or -a\n```\n\n> 👉 **Tip:** Use the `lzc` alias if `lazycommit` is too long for you.\n\n#### Generate multiple recommendations\n\nSometimes the recommended commit message isn't the best so you want it to generate a few to pick from. You can generate multiple commit messages at once by passing in the `--generate <i>` flag, where 'i' is the number of generated messages:\n\n```sh\nlazycommit --generate <i> # or -g <i>\n```\n\n> Warning: this uses more tokens, meaning it costs more.\n\n#### Generating Conventional Commits\n\nIf you'd like to generate [Conventional Commits](https://conventionalcommits.org/), you can use the `--type` flag followed by `conventional`. This will prompt `lazycommit` to format the commit message according to the Conventional Commits specification:\n\n```sh\nlazycommit --type conventional # or -t conventional\n```\n\nThis feature can be useful if your project follows the Conventional Commits standard or if you're using tools that rely on this commit format.\n\n#### Exclude files from analysis\n\nYou can exclude specific files from AI analysis using the `--exclude` flag:\n\n```sh\nlazycommit --exclude package-lock.json --exclude dist/\n```\n\n#### Handling large diffs\n\nFor large commits with many files, lazycommit automatically stays within API limits:\n\n- **Automatic detection**: Large diffs are detected\n- **Per-file splitting**: Diffs are split by file first\n- **Safe chunking**: Each file diff is chunked conservatively (default: 4000 tokens)\n- **Combination**: Results are combined into one concise message\n\n### Git hook\n\nYou can also integrate _lazycommit_ with Git via the [`prepare-commit-msg`](https://git-scm.com/docs/githooks#_prepare_commit_msg) hook. This lets you use Git like you normally would, and edit the commit message before committing.\n\n#### Install\n\nIn the Git repository you want to install the hook in:\n\n```sh\nlazycommit hook install\n```\n\n#### Uninstall\n\nIn the Git repository you want to uninstall the hook from:\n\n```sh\nlazycommit hook uninstall\n```\n\n#### Usage\n\n1. Stage your files and commit:\n\n   ```sh\n   git add <files...>\n   git commit # Only generates a message when it's not passed in\n   ```\n\n   > If you ever want to write your own message instead of generating one, you can simply pass one in: `git commit -m \"My message\"`\n\n2. Lazycommit will generate the commit message for you and pass it back to Git. Git will open it with the [configured editor](https://docs.github.com/en/get-started/getting-started-with-git/associating-text-editors-with-git) for you to review/edit it.\n\n3. Save and close the editor to commit!\n\n## Configuration\n\n### Reading a configuration value\n\nTo retrieve a configuration option, use the command:\n\n```sh\nlazycommit config get <key>\n```\n\nFor example, to retrieve the API key, you can use:\n\n```sh\nlazycommit config get GROQ_API_KEY\n```\n\nYou can also retrieve multiple configuration options at once by separating them with spaces:\n\n```sh\nlazycommit config get GROQ_API_KEY generate\n```\n\n### Setting a co",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-18T09:13:10.167369"
  },
  {
    "basic_info": {
      "name": "maisti_teste",
      "full_name": "Benevdx/maisti_teste",
      "owner": "Benevdx",
      "description": "repositorio teste para aulas do mais ti - ufs",
      "url": "https://github.com/Benevdx/maisti_teste",
      "clone_url": "https://github.com/Benevdx/maisti_teste.git",
      "ssh_url": "git@github.com:Benevdx/maisti_teste.git",
      "homepage": null,
      "created_at": "2025-09-17T13:25:43Z",
      "updated_at": "2025-09-17T14:41:49Z",
      "pushed_at": "2025-09-17T14:28:44Z"
    },
    "stats": {
      "stars": 18,
      "forks": 0,
      "watchers": 18,
      "open_issues": 0,
      "size": 13
    },
    "tech_info": {
      "language": "JavaScript",
      "languages": {
        "JavaScript": 4780
      },
      "license": null,
      "topics": []
    },
    "content": {
      "readme": "# maisti_teste\nrepositorio teste para aulas do mais ti - ufs\n\n\n# Adição de um teste\n## Head 2\n### Head 3\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-18T09:13:11.374935"
  },
  {
    "basic_info": {
      "name": "Trajectory",
      "full_name": "ShyShyFaceElephant/Trajectory",
      "owner": "ShyShyFaceElephant",
      "description": "基於腦年齡與失智症預測模型之長期腦部健康追蹤系統",
      "url": "https://github.com/ShyShyFaceElephant/Trajectory",
      "clone_url": "https://github.com/ShyShyFaceElephant/Trajectory.git",
      "ssh_url": "git@github.com:ShyShyFaceElephant/Trajectory.git",
      "homepage": null,
      "created_at": "2025-09-17T04:21:57Z",
      "updated_at": "2025-09-18T07:02:35Z",
      "pushed_at": "2025-09-17T06:56:59Z"
    },
    "stats": {
      "stars": 17,
      "forks": 2,
      "watchers": 17,
      "open_issues": 0,
      "size": 547654
    },
    "tech_info": {
      "language": "Jupyter Notebook",
      "languages": {
        "Jupyter Notebook": 2212291,
        "Dart": 118262,
        "Python": 62925,
        "C++": 24945,
        "CMake": 19803,
        "HTML": 14229,
        "Swift": 2069,
        "C": 1425,
        "Kotlin": 128,
        "Objective-C": 38
      },
      "license": null,
      "topics": []
    },
    "content": {
      "readme": "# 🧠 Trajectory 基於腦年齡與失智症預測模型之長期腦部健康追蹤系統\n\n## 👀系統簡介\n\n[![產品介紹影片：](images/demo.png)](https://youtu.be/9u6xG_67Q1s)\n\n> 產品介紹影片：https://youtu.be/9u6xG_67Q1s\n\nTrajectory是一套基於**腦年齡與失智症預測模型**的長期腦部健康追蹤系統，旨在滿足高齡化與少子化社會對精準醫療的需求。受試者之**腦部 MRI**影像輸入至本系統後，腦齡預測模型會提供腦齡值作為腦部健康狀態的量化指標。本系統適用於所有年齡層，不僅可作為兒童與青少年腦部成熟度的評估工具，亦能作為中老年族群認知退化的早期徵兆。\n\n我們將本系統命名為「Trajectory」，意為「軌跡」。它不僅是單純的腦齡預測模型之圖形化介面，更是一個包含**長期腦齡追蹤**、**AI失智症輔助診斷**與**關鍵腦區熱圖**的完整系統。本系統透過折線圖呈現歷次腦齡紀錄，追蹤個人成長軌跡。一般使用者可調閱歷次 MRI 影像紀錄，並查看其分析結果（包括腦齡、AI 失智症診斷與關鍵腦區熱圖）；醫師則具有上傳、編輯與查看患者資料等更高層次的管理者權限。\n\n---\n\n## 🏥應用場景\n\n![系統應用場景](images/SystemApplicationScenarios.png)\n> 圖1. Trajectory系統應用場景\n\n本系統使用流程如下：\n\n1. 受試者進行簡易智能狀態測驗（MMSE）（中低年齡群體不需參與）\n2. 受試者拍攝腦部MRI影像\n3. 資訊室人員將腦部MRI影像及認知測驗結果上傳至伺服器\n4. 伺服器運行腦齡預測模型與失智症診斷模型\n5. 伺服器將分析結果（包含：腦齡、AI失智症診斷、關鍵腦區熱圖）歸檔\n6. 醫師調用受試者資料並結合自身專業知識做出最終診斷\n7. 受試者在院內機台也可調閱分析結果與診斷紀錄\n\n---\n\n## ⛓️系統架構\n\n![系統架構](images/SystemFramework.png)\n> 圖2. Trajectory系統架構\n\n本系統採**前後端完全分離**設計，目的是讓使用者在不同終端設備皆能使用相同API存取資料。\n\n本系統欲在醫院中多種終端設備部屬，因此採用跨平台前端框架，考量到 MRI 影像涉及大量數據傳輸與顯示，故在我們多種跨平台前端框架中，選擇存取速度與偵率表現更卓越的Flutter。前端分為醫師介面與一般使用者介面，一般使用者介面提供使用者查閱歷次影像紀錄與腦部分析結果；醫師介面則提供管理者上傳影像、執行AI運算功能，並統一管理受試者。\n\n由於本系統的 AI 模型以 Python 撰寫，為提升運算效率並簡化跨語言介面維護，我們選擇以同樣基於 Python 的 FastAPI 作為後端框架。後端由伺服器負責接受API請求，並溝通資料庫及AI模型模組，彼此獨立運行，確保系統安全穩定。\n\n順帶一提，本系統除了提供使用者查看診斷結果外，希望能納入「健康指南」給予具體的治療或保健方針，但本團隊受限於專業知識，此功能暫時留在藍圖中。\n\n---\n\n## 🤖AI運算模組\n\n![AI運算模組](images/AiModule.png)\n> 圖3. Trajectory AI運算模組\n\n圖3為本系統後端的AI運算模組之架構。首先，腦部 MRI 影像會經過前處理，處理後的影像隨後會被送入兩個平行的運算流程：\n\n1. 腦齡預測：影像被輸入至 SFCN 模型並輸出腦齡預測值。\n2. 失智症分類：影像與其他輔助特徵（認知測驗分數、性別與年齡）一起輸入至 DenseNet 、 XGBoost 混合模型，輸出失智症分類結果。此失智症分類模型由本實驗室的前輩所研發。\n\n最後，利用**Grad-CAM 演算法**，根據模型輸出生成對應的**關鍵腦區熱圖**。\n\n---\n\n## 🧑‍💻Trajectory功能介紹\n\n![登入介面](images/login.png)\n> 圖4. 登入介面\n\n圖4頁面提供兩種登入方式：受試者由 「一般登入」通道登入，而管理者則由 「醫師登入」通道登入。\n\n![醫師介面—成員管理](images/memberManagement.png)\n> 圖5. 醫師介面—成員管理\n\n圖5頁面提供管理者一個列表，能查看所以有受試者，點選「選擇」按鈕跳轉至該受試者介面，調閱其腦部分析結果及歷次影像記錄。\n\n![醫師介面—上傳影像](images/uploadImage.png)\n> 圖6. 醫師介面—上傳影像\n\n圖6頁面提供資訊室人員圖形化介面上傳檢測資料，上傳過程分為三步驟：\n\n1. 依序填入身分證字號、拍攝日期、腦部MRI影像與認知測驗結果（中低年齡群體不需要填寫此欄位），點擊「建檔」，資料將傳遞至伺服器並在資料庫中初始化一筆影像紀錄，並自動填寫實際年齡。\n2. 點擊「AI計算」，伺服器將根據資料庫中的資料運行腦齡預測模型與失智症診斷模型（若未填入認知測驗結果，則不會運行失智症診斷模型），運算結束後將結果自動填入相對應欄位中。\n3. 點擊「儲存」，將預測結果更新置資料庫。\n\n![醫師介面—新增成員](images/addMember.png)\n> 圖7. 醫師介面—新增成員\n\n圖7頁面提供管理者新增受試者的圖形化介面，依序填入姓名、身分證字號、出生日期、性別與個人照，點擊「送出」，便可快速建立一般使用者帳號。\n\n![一般使用者介面—腦部分析](images/brainAnalysis.png)\n> 圖8. 一般使用者介面—腦部分析\n\n圖8頁面提供受試者（及管理者）查閱腦部分析結果，上方的窗格顯示最近一次AI失智症診斷結果，若未曾檢測則不顯示結果；下方的窗格則顯示歷次檢測的實際年齡與腦齡折線圖，受試者能直觀看見腦部發展軌跡。\n\n![一般使用者介面—影像紀錄a](images/recordsA.png)\n> 圖9(a). 一般使用者介面—影像紀錄\n\n![一般使用者介面—影像紀錄b](images/recordsB.png)\n> 圖9(b). 一般使用者介面—影像紀錄\n\n圖9頁面提供受試者（及管理者）查閱腦歷次影像紀錄細節，從上方的窗格（圖9(a)）可以選取欲查看的影像紀錄，點擊「選取」，則會在下方（圖9(b)）顯示該次MRI影像（可同時開啟多筆影像紀錄），使用者透果滑鼠滾輪可以檢視各方向腦部細節，勾選「顯示關鍵腦區」功能，MRI影像上會顯示其關鍵腦區熱圖輔助判讀。\n\n---\n\n## 🧑‍🌾開發團隊\n\n| 成員     | 負責工作 |\n|----------|--------------------------------------------------------------------------|\n| 顏少于   | 主持 Trajectory 開發、前端設計、前後端整合、腦齡預測模型串接 |\n| 王冠智   | 後端 API 設計、資料庫、整合 AI 模組、校外競賽文件 |\n| 徐睿淳   | MRI 切片儲存、失智症分類模型串接、介紹影片製作、系統功能規劃   |\n| 劉學諺   | 失智症分類模型開發 |\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-18T09:13:12.599747"
  },
  {
    "basic_info": {
      "name": "Trotski",
      "full_name": "iluxu/Trotski",
      "owner": "iluxu",
      "description": "Real-time AI Interview Assistant. Transcribes live audio with faster-whisper, detects questions, and generates answers using an LLM. Includes a multi-platform client and a web UI dashboard. Your personal interview co-pilot.",
      "url": "https://github.com/iluxu/Trotski",
      "clone_url": "https://github.com/iluxu/Trotski.git",
      "ssh_url": "git@github.com:iluxu/Trotski.git",
      "homepage": null,
      "created_at": "2025-09-17T07:07:42Z",
      "updated_at": "2025-09-18T06:05:20Z",
      "pushed_at": "2025-09-17T12:08:52Z"
    },
    "stats": {
      "stars": 16,
      "forks": 2,
      "watchers": 16,
      "open_issues": 0,
      "size": 59
    },
    "tech_info": {
      "language": "HTML",
      "languages": {
        "HTML": 42318,
        "Python": 35225
      },
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "# Trotski - Real-Time AI Interview Assistant\n\nThis project provides a high-performance, real-time audio transcription and AI-powered answering server. It uses faster-whisper for low-latency STT (Speech-to-Text) and an LLM (like GPT models) to intelligently detect questions from the transcript and generate relevant, in-character answers on the fly.\n\nThe system is composed of three main parts:\n\n- **The Server** (`optimized_stt_server_v3.py`): A WebSocket server that receives raw audio, transcribes it, analyzes the text for questions, and generates answers.\n- **The Client** (`stable_audio_client_multi_os.py`): A robust, multi-platform audio streaming client that captures microphone input using FFmpeg and streams it to the server.\n- **The UI** (`index.html`): A standalone, zero-dependency web interface that connects to the server to display the live transcript and Q&A panel.\n\n## ✨ Features\n\n- **Real-Time Transcription**: Low-latency audio transcription using faster-whisper\n- **Intelligent Question Detection**: An LLM-powered analyzer detects questions from the live transcript\n- **AI-Powered Answer Generation**: Generates context-aware, in-character answers for detected questions\n- **Standalone Web UI**: A feature-rich, single-file `index.html` dashboard to monitor the interview\n- **Multi-Platform Support**: The server and client run on Windows, macOS, and Linux\n- **Robust & Stable**: Includes automatic reconnection, backpressure handling, and stable connection parameters\n- **Highly Configurable**: Nearly every aspect can be configured via environment variables\n\n## 📋 Prerequisites\n\nBefore you begin, ensure you have the following installed:\n\n### Python 3.9+\n\n### FFmpeg\nRequired by the audio client to capture microphone audio.\n\n- **Windows**: Download from the [official website](https://ffmpeg.org/download.html) and add to PATH, or use Chocolatey (`choco install ffmpeg`)\n- **macOS**: Install via Homebrew: `brew install ffmpeg`\n- **Linux**: Install via your package manager: `sudo apt-get install ffmpeg` (Debian/Ubuntu)\n\n### NVIDIA GPU with CUDA (Recommended)\nFor significant performance gains with the Whisper model.\n\n- Install the latest [NVIDIA Driver](https://www.nvidia.com/drivers/)\n- Install the [CUDA Toolkit](https://developer.nvidia.com/cuda-toolkit) (v11.x is compatible)\n- Install [cuDNN](https://developer.nvidia.com/cudnn)\n\n### OpenAI API Key\nRequired for question detection and answer generation.\n\n## 🚀 Installation\n\n### 1. Clone the Repository\n\n```bash\ngit clone https://github.com/iluxu/Trotski.git\ncd Trotski\n```\n\n### 2. Create a Virtual Environment\n\n```bash\npython -m venv venv\n\n# On Windows\n.\\venv\\Scripts\\activate\n\n# On macOS/Linux\nsource venv/bin/activate\n```\n\n### 3. Install Python Dependencies\n\nCreate a `requirements.txt` file with the content specified below and run:\n\n```bash\npip install -r requirements.txt\n```\n\n**CPU-Only Note**: If you don't have an NVIDIA GPU, first install the CPU version of PyTorch:\n```bash\npip install torch --index-url https://download.pytorch.org/whl/cpu\n```\nThen run `pip install -r requirements.txt`.\n\n### 4. Set Up Environment Variables\n\nCreate a `.env` file by copying the example:\n\n```bash\n# On Windows\ncopy .env.example .env\n\n# On macOS/Linux\ncp .env.example .env\n```\n\nNow, edit the `.env` file and add your `OPENAI_API_KEY`. See the `.env.example` section for all options.\n\n## ⚙️ Usage\n\nThe process involves three steps: starting the server, opening the UI, and starting the audio client.\n\n### 1. Run the Server\n\nStart the server in a terminal. It will download the Whisper model on its first run.\n\n```bash\npython optimized_stt_server_v3.py\n```\n\nYou should see output indicating the server is ready:\n```\n🎤 Server ready on ws://127.0.0.1:8123/\n```\n\n### 2. Open the Web UI\n\nSimply open the `index.html` file in your web browser (e.g., Chrome, Firefox, Safari). No web server is needed. The page will automatically try to connect to the WebSocket server running on your local machine.\n\n### 3. Run the Audio Client\n\nThe client needs to know which microphone to use.\n\n#### Step A: Find Your Audio Device\n\nOpen a new terminal and run the client with the `--list-devices` flag:\n\n```bash\npython stable_audio_client_multi_os.py --list-devices\n```\n\nThis will show you a list of available microphones and the correct name to use for your operating system.\n\n#### Step B: Start Streaming\n\nNow, run the client with the device name you found.\n\n```bash\n# Example for Windows\npython stable_audio_client_multi_os.py --device \"Mixage stéréo (Realtek(R) Audio)\"\n\n# Example for macOS\npython stable_audio_client_multi_os.py --device \":0\"\n\n# Example for Linux\npython stable_audio_client_multi_os.py --device \"hw:0,0\"\n```\n\nThe client will connect to the server. Start speaking, and you will see the live transcript and Q&A appear in the `index.html` UI in your browser.\n\n## 🖥️ Web UI Features (index.html)\n\nThe web UI is a powerful dashboard for monitoring the interview in real-time.\n\n<!-- It's a good idea to add a screenshot of your ",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-18T09:13:13.863322"
  },
  {
    "basic_info": {
      "name": "Stencil-Buffer-Holographic-Display",
      "full_name": "lukky-nl/Stencil-Buffer-Holographic-Display",
      "owner": "lukky-nl",
      "description": null,
      "url": "https://github.com/lukky-nl/Stencil-Buffer-Holographic-Display",
      "clone_url": "https://github.com/lukky-nl/Stencil-Buffer-Holographic-Display.git",
      "ssh_url": "git@github.com:lukky-nl/Stencil-Buffer-Holographic-Display.git",
      "homepage": null,
      "created_at": "2025-09-17T11:06:03Z",
      "updated_at": "2025-09-18T03:56:33Z",
      "pushed_at": "2025-09-17T11:09:06Z"
    },
    "stats": {
      "stars": 15,
      "forks": 0,
      "watchers": 15,
      "open_issues": 0,
      "size": 306
    },
    "tech_info": {
      "language": null,
      "languages": {},
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-18T09:13:15.083040"
  },
  {
    "basic_info": {
      "name": "perceptron",
      "full_name": "perceptron-ai-inc/perceptron",
      "owner": "perceptron-ai-inc",
      "description": "The official Python SDK for the Perceptron API",
      "url": "https://github.com/perceptron-ai-inc/perceptron",
      "clone_url": "https://github.com/perceptron-ai-inc/perceptron.git",
      "ssh_url": "git@github.com:perceptron-ai-inc/perceptron.git",
      "homepage": "https://www.perceptron.inc",
      "created_at": "2025-09-17T02:25:49Z",
      "updated_at": "2025-09-18T06:18:54Z",
      "pushed_at": "2025-09-17T17:21:37Z"
    },
    "stats": {
      "stars": 14,
      "forks": 1,
      "watchers": 14,
      "open_issues": 0,
      "size": 305
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 271445
      },
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "# Perceptron SDK\n\nPython SDK and CLI for perceptive-language models. The SDK is provider-agnostic and lets you compose visual + language tasks, run them locally for inspection, or execute them via a configured provider. Choose a provider and optional model per call; keep your application code stable across model updates.\n\n---\n\n## Installation\n- Prerequisites: Python 3.10+, `pip` 23+ (or [`uv`](https://github.com/astral-sh/uv))\n\n```bash\npip install perceptron\n\n# Optional extras\npip install \"perceptron[torch]\"   # TensorStream helpers (requires PyTorch)\npip install \"perceptron[dev]\"     # Dev tools (ruff, pytest, pre-commit)\n```\n\nUsing `uv`:\n```bash\nuv pip install perceptron\nuv pip install \"perceptron[torch]\"\nuv pip install \"perceptron[dev]\"\n```\n\nThe CLI entry point `perceptron` is available after install.\n\n---\n\n## Configuration\nSet credentials and defaults via environment, programmatically, or the CLI. The SDK ships with a `fal` provider; you can add others by extending `perceptron.client._PROVIDER_CONFIG`.\n\n- `PERCEPTRON_PROVIDER`: provider identifier (default `fal`)\n- `PERCEPTRON_API_KEY`: API key for the selected provider\n- `PERCEPTRON_BASE_URL`: override provider base URL when needed\n- `FAL_KEY`: alternative env var used when `provider=fal`\n\nProgrammatic configuration:\n```python\nfrom perceptron import configure, config\n\nconfigure(provider=\"fal\", api_key=\"sk_live_...\", base_url=\"https://api.example/v1\")\n\nwith config(max_tokens=512):\n    ...  # temporary overrides inside the context\n```\n\nCLI helper:\n```bash\nperceptron config --provider fal --api-key sk_live_...\n```\n\nNo credentials? Helpers return compile-only payloads so you can inspect tasks without sending requests.\n\n---\n\n## Python Quickstart\n```python\nfrom perceptron import caption, detect\n\n# Caption an image (provider default model)\nresult = caption(\"/path/to/image.png\", style=\"concise\")\nprint(result.text)\n\n# Stream grounded detections; optionally select a specific model\nfor event in detect(\"local.png\", classes=[\"person\", \"forklift\"], model=\"perceptron\", stream=True):\n    if event[\"type\"] == \"text.delta\":\n        print(\"chunk\", event[\"chunk\"])\n    elif event[\"type\"] == \"points.delta\":\n        print(\"bbox\", event[\"points\"])\n    elif event[\"type\"] == \"final\":\n        print(\"final\", event[\"result\"][\"points\"])\n```\n\n### Few-shot detection from COCO\n```python\nfrom perceptron import detect_from_coco\n\nruns = detect_from_coco(\n    \"/datasets/demo\",\n    split=\"train\",\n    shots=4,                 # build balanced in-context examples automatically\n    classes=[\"defect\", \"ok\"],\n)\n\nfor sample in runs:\n    print(sample.image_path.name)\n    for box in sample.result.points or []:\n        print(\" -\", box.mention, box)\n```\n\n---\n\n## CLI Usage\nThe CLI mirrors the high-level helpers and supports directory batching (JSON summaries written alongside input folders).\n\n```bash\n# Generate captions\nperceptron caption image.jpg\nperceptron caption ./images --style detailed\n\n# OCR with a custom prompt\nperceptron ocr schematic.png --prompt \"Extract component labels\"\n\n# Batched detection (writes detections.json)\nperceptron detect ./frames --classes defect,warning\n\n# Grounded question answering\nperceptron question image.jpg \"What stands out?\" --expects box --format json\n```\n\nDirectory mode disables streaming and logs raw responses, plus per-file validation issues.\n\n---\n\n## High-Level APIs\n- `caption(image, *, style=\"concise\", stream=False, **kwargs)`\n- `ocr(image, *, prompt=None, stream=False, **kwargs)`\n- `detect(image, *, classes=None, examples=None, stream=False, **kwargs)`\n- `detect_from_coco(dataset_dir, *, split=None, classes=None, shots=0, limit=None, **kwargs)`\n\nNotes\n- Pass `model=\"...\"`, `provider=\"...\"`, `max_tokens=...`, etc., through `**kwargs` on any helper.\n- `detect_from_coco` discovers annotations, constructs balanced examples when `shots > 0`, and returns `CocoDetectResult` objects.\n- For advanced workflows, build tasks with the typed DSL (`text`, `system`, `image`, `point`, `box`, `polygon`, `collection`) and decorate with `@perceive` / `@async_perceive`.\n\n### Using the DSL with `@perceive`\n```python\nfrom perceptron import perceive, image, text\n\n@perceive(expects=\"box\")\ndef describe_landmark(path):\n    return image(path) + text(\"Highlight the main structures in one sentence.\")\n\nresult = describe_landmark(\"./landmark.jpg\")\nprint(result.text)\nfor box in result.points or []:\n    print(box.mention, box)\n\n# Inspect the compiled payload without executing the request\nprint(describe_landmark.inspect(\"./landmark.jpg\"))\n```\n\nSet `stream=True` in the decorator to receive incremental events (`text.delta`, `points.delta`, `final`). Swap `expects` to `text`, `point`, or `polygon` when you need alternate structures.\n\n---\n\n## Troubleshooting\n| Symptom | Likely Cause | Resolution |\n| --- | --- | --- |\n| Compile-only result (no text) | Missing provider credentials | Export `FAL_KEY` / `PERCEPTRON_API_KEY` or call `configure(...)` |\n| `stream_buffer_overflow` warning | Long streaming r",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-18T09:13:16.307120"
  },
  {
    "basic_info": {
      "name": "My-Eveny",
      "full_name": "shahinexy/My-Eveny",
      "owner": "shahinexy",
      "description": null,
      "url": "https://github.com/shahinexy/My-Eveny",
      "clone_url": "https://github.com/shahinexy/My-Eveny.git",
      "ssh_url": "git@github.com:shahinexy/My-Eveny.git",
      "homepage": null,
      "created_at": "2025-09-17T11:22:58Z",
      "updated_at": "2025-09-18T08:09:19Z",
      "pushed_at": "2025-09-17T11:25:16Z"
    },
    "stats": {
      "stars": 14,
      "forks": 0,
      "watchers": 14,
      "open_issues": 0,
      "size": 101
    },
    "tech_info": {
      "language": "TypeScript",
      "languages": {
        "TypeScript": 40456,
        "JavaScript": 682,
        "CSS": 113
      },
      "license": null,
      "topics": []
    },
    "content": {
      "readme": "This is a [Next.js](https://nextjs.org) project bootstrapped with [`create-next-app`](https://nextjs.org/docs/app/api-reference/cli/create-next-app).\n\n## Getting Started\n\nFirst, run the development server:\n\n```bash\nnpm run dev\n# or\nyarn dev\n# or\npnpm dev\n# or\nbun dev\n```\n\nOpen [http://localhost:3000](http://localhost:3000) with your browser to see the result.\n\nYou can start editing the page by modifying `app/page.tsx`. The page auto-updates as you edit the file.\n\nThis project uses [`next/font`](https://nextjs.org/docs/app/building-your-application/optimizing/fonts) to automatically optimize and load [Geist](https://vercel.com/font), a new font family for Vercel.\n\n## Learn More\n\nTo learn more about Next.js, take a look at the following resources:\n\n- [Next.js Documentation](https://nextjs.org/docs) - learn about Next.js features and API.\n- [Learn Next.js](https://nextjs.org/learn) - an interactive Next.js tutorial.\n\nYou can check out [the Next.js GitHub repository](https://github.com/vercel/next.js) - your feedback and contributions are welcome!\n\n## Deploy on Vercel\n\nThe easiest way to deploy your Next.js app is to use the [Vercel Platform](https://vercel.com/new?utm_medium=default-template&filter=next.js&utm_source=create-next-app&utm_campaign=create-next-app-readme) from the creators of Next.js.\n\nCheck out our [Next.js deployment documentation](https://nextjs.org/docs/app/building-your-application/deploying) for more details.\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-18T09:13:17.528499"
  }
]