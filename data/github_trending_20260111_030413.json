[
  {
    "basic_info": {
      "name": "DeepTutor",
      "full_name": "HKUDS/DeepTutor",
      "owner": "HKUDS",
      "description": "\"DeepTutor: AI-Powered Personalized Learning Assistant\"",
      "url": "https://github.com/HKUDS/DeepTutor",
      "clone_url": "https://github.com/HKUDS/DeepTutor.git",
      "ssh_url": "git@github.com:HKUDS/DeepTutor.git",
      "homepage": "https://hkuds.github.io/DeepTutor",
      "created_at": "2025-12-28T15:35:54Z",
      "updated_at": "2026-01-11T02:56:51Z",
      "pushed_at": "2026-01-11T02:09:18Z"
    },
    "stats": {
      "stars": 7595,
      "forks": 928,
      "watchers": 7595,
      "open_issues": 11,
      "size": 60302
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 1575162,
        "TypeScript": 917189,
        "Shell": 13838,
        "Dockerfile": 11231,
        "CSS": 8674,
        "JavaScript": 2572
      },
      "license": "GNU Affero General Public License v3.0",
      "topics": [
        "ai-agents",
        "ai-tutor",
        "deepresearch",
        "idea-generation",
        "interactive-learning",
        "knowledge-graph",
        "large-language-models",
        "multi-agent-systems",
        "rag"
      ]
    },
    "content": {
      "readme": "<div align=\"center\">\n\n<img src=\"assets/logo-ver2.png\" alt=\"DeepTutor Logo\" width=\"150\" style=\"border-radius: 15px;\">\n\n# DeepTutor: AI-Powered Personalized Learning Assistant\n\n[![Python](https://img.shields.io/badge/Python-3.10%2B-3776AB?style=flat-square&logo=python&logoColor=white)](https://www.python.org/downloads/)\n[![FastAPI](https://img.shields.io/badge/FastAPI-0.100%2B-009688?style=flat-square&logo=fastapi&logoColor=white)](https://fastapi.tiangolo.com/)\n[![React](https://img.shields.io/badge/React-19-61DAFB?style=flat-square&logo=react&logoColor=black)](https://react.dev/)\n[![Next.js](https://img.shields.io/badge/Next.js-16-000000?style=flat-square&logo=next.js&logoColor=white)](https://nextjs.org/)\n[![TailwindCSS](https://img.shields.io/badge/Tailwind-3.4-06B6D4?style=flat-square&logo=tailwindcss&logoColor=white)](https://tailwindcss.com/)\n[![License](https://img.shields.io/badge/License-AGPL--3.0-blue?style=flat-square)](LICENSE)\n\n<p align=\"center\">\n  <a href=\"https://discord.gg/zpP9cssj\"><img src=\"https://img.shields.io/badge/Discord-Join_Community-5865F2?style=for-the-badge&logo=discord&logoColor=white\" alt=\"Discord\"></a>\n  &nbsp;&nbsp;\n  <a href=\"./Communication.md\"><img src=\"https://img.shields.io/badge/Feishu-Join_Group-00D4AA?style=for-the-badge&logo=feishu&logoColor=white\" alt=\"Feishu\"></a>\n  &nbsp;&nbsp;\n  <a href=\"https://github.com/HKUDS/DeepTutor/issues/78\"><img src=\"https://img.shields.io/badge/WeChat-Join_Group-07C160?style=for-the-badge&logo=wechat&logoColor=white\" alt=\"WeChat\"></a>\n</p>\n\n\n\n[**Quick Start**](#quick-start) Â· [**Core Modules**](#core-modules) Â· [**FAQ**](#faq)\n\n[ğŸ‡¨ğŸ‡³ ä¸­æ–‡](assets/README/README_CN.md) Â· [ğŸ‡¯ğŸ‡µ æ—¥æœ¬èª](assets/README/README_JA.md) Â· [ğŸ‡ªğŸ‡¸ EspaÃ±ol](assets/README/README_ES.md) Â· [ğŸ‡«ğŸ‡· FranÃ§ais](assets/README/README_FR.md) Â· [ğŸ‡¸ğŸ‡¦ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©](assets/README/README_AR.md) Â· [ğŸ‡·ğŸ‡º Ğ ÑƒÑÑĞºĞ¸Ğ¹](assets/README/README_RU.md) Â· [ğŸ‡®ğŸ‡³ à¤¹à¤¿à¤¨à¥à¤¦à¥€](assets/README/README_HI.md) Â· [ğŸ‡µğŸ‡¹ PortuguÃªs](assets/README/README_PT.md)\n\n</div>\n\n<div align=\"center\">\n\nğŸ“š **Massive Document Knowledge Q&A** &nbsp;â€¢&nbsp; ğŸ¨ **Interactive Learning Visualization**<br>\nğŸ¯ **Knowledge Reinforcement** &nbsp;â€¢&nbsp; ğŸ” **Deep Research & Idea Generation**\n\n</div>\n\n---\n### ğŸ“° News\n\n> **[2026.1.1]** Join our [Discord Community](https://discord.gg/zpP9cssj) and [GitHub Discussions](https://github.com/HKUDS/DeepTutor/discussions) - shape the future of DeepTutor! ğŸ’¬\n\n> **[2025.12.30]** Visit our [Official Website](https://hkuds.github.io/DeepTutor/) for more details!\n\n> **[2025.12.29]** DeepTutor is now live! âœ¨\n\n### ğŸ“¦ Releases\n\n> **[2026.1.9]** Release [v0.4.1](https://github.com/HKUDS/DeepTutor/releases/tag/v0.4.1) with LLM Provider system overhaul, Question Generation robustness improvements, and codebase cleanup - Thanks to all the contributors!\n<details>\n<summary>History releases</summary>\n\n> **[2026.1.9]** Release [v0.4.0](https://github.com/HKUDS/DeepTutor/releases/tag/v0.4.0) with new code structure, multiple llm & embeddings support - Thanks to all the contributors!\n\n> **[2026.1.5]** [v0.3.0](https://github.com/HKUDS/DeepTutor/releases/tag/v0.3.0) - Unified PromptManager architecture, CI/CD automation & pre-built Docker images on GHCR\n\n> **[2026.1.2]** [v0.2.0](https://github.com/HKUDS/DeepTutor/releases/tag/v0.2.0) - Docker deployment, Next.js 16 & React 19 upgrade, WebSocket security & critical vulnerability fixes\n\n</details>\n\n---\n\n## Key Features of DeepTutor\n\n### ğŸ“š Massive Document Knowledge Q&A\nâ€¢ **Smart Knowledge Base**: Upload textbooks, research papers, technical manuals, and domain-specific documents. Build a comprehensive AI-powered knowledge repository for instant access.<br>\nâ€¢ **Multi-Agent Problem Solving**: Dual-loop reasoning architecture with RAG, web search, and code execution -- delivering step-by-step solutions with precise citations.\n\n### ğŸ¨ Interactive Learning Visualization\nâ€¢ **Knowledge Simplification & Explanations**: Transform complex concepts, knowledge, and algorithms into easy-to-understand visual aids, detailed step-by-step breakdowns, and engaging interactive demonstrations.<br>\nâ€¢ **Personalized Q&A**: Context-aware conversations that adapt to your learning progress, with interactive pages and session-based knowledge tracking.\n\n### ğŸ¯ Knowledge Reinforcement with Practice Exercise Generator\nâ€¢ **Intelligent Exercise Creation**: Generate targeted quizzes, practice problems, and customized assessments tailored to your current knowledge level and specific learning objectives.<br>\nâ€¢ **Authentic Exam Simulation**: Upload reference exams to generate practice questions that perfectly match the original style, format, and difficultyâ€”giving you realistic preparation for the actual test.\n\n### ğŸ” Deep Research & Idea Generation\nâ€¢ **Comprehensive Research & Literature Review**: Conduct in-depth topic exploration with systematic analysis. Identify patterns, connect related concepts across disciplines, and synthesize existing research findings.<br>\nâ€¢ **Novel Insight Discovery**: Generate structured learning materials ",
      "default_branch": "main"
    },
    "fetched_at": "2026-01-11T03:04:14.716883"
  },
  {
    "basic_info": {
      "name": "vibe-coding-cn",
      "full_name": "2025Emma/vibe-coding-cn",
      "owner": "2025Emma",
      "description": null,
      "url": "https://github.com/2025Emma/vibe-coding-cn",
      "clone_url": "https://github.com/2025Emma/vibe-coding-cn.git",
      "ssh_url": "git@github.com:2025Emma/vibe-coding-cn.git",
      "homepage": null,
      "created_at": "2025-12-17T00:10:23Z",
      "updated_at": "2026-01-11T03:03:23Z",
      "pushed_at": "2025-12-17T00:11:18Z"
    },
    "stats": {
      "stars": 7377,
      "forks": 841,
      "watchers": 7377,
      "open_issues": 0,
      "size": 28890
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 34941,
        "Shell": 28256,
        "Makefile": 842
      },
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "<!--\n-------------------------------------------------------------------------------\n  é¡¹ç›®å¤´éƒ¨åŒºåŸŸ (HEADER)\n-------------------------------------------------------------------------------\n-->\n<p align=\"center\">\n  <!-- å»ºè®®å°ºå¯¸: 1280x640pxã€‚å¯ä»¥ä½¿ç”¨ Canva, Figma æˆ– https://banners.beyondco.de/ ç­‰å·¥å…·åˆ¶ä½œ -->\n  <img src=\"https://github.com/tukuaiai.png\" alt=\"Vibe Coding æŒ‡å—\" width=\"80px\">\n</p>\n\n<div align=\"center\">\n\n# Vibe Coding æŒ‡å—\n\n**ä¸€ä¸ªé€šè¿‡ä¸ AI ç»“å¯¹ç¼–ç¨‹ï¼Œå°†æƒ³æ³•å˜ä¸ºç°å®çš„ç»ˆæå·¥ä½œç«™**\n\n---\n\n<!--\n  å¾½ç« åŒºåŸŸ (BADGES)\n-->\n<!-- é¡¹ç›®çŠ¶æ€å¾½ç«  -->\n<p>\n  <a href=\"https://github.com/tukuaiai/vibe-coding-cn/actions\"><img src=\"https://img.shields.io/github/actions/workflow/status/tukuaiai/vibe-coding-cn/main.yml?label=%E6%9E%84%E5%BB%BA%E7%8A%B6%E6%80%81&style=for-the-badge\" alt=\"æ„å»ºçŠ¶æ€\"></a>\n  <a href=\"https://github.com/tukuaiai/vibe-coding-cn/releases\"><img src=\"https://img.shields.io/github/v/release/tukuaiai/vibe-coding-cn?label=%E6%9C%80%E6%96%B0%E7%89%88%E6%9C%AC&style=for-the-badge\" alt=\"æœ€æ–°ç‰ˆæœ¬\"></a>\n  <a href=\"LICENSE\"><img src=\"https://img.shields.io/github/license/tukuaiai/vibe-coding-cn?label=%E8%AE%B8%E5%8F%AF%E8%AF%81&style=for-the-badge\" alt=\"è®¸å¯è¯\"></a>\n  <a href=\"https://github.com/tukuaiai/vibe-coding-cn\"><img src=\"https://img.shields.io/github/languages/top/tukuaiai/vibe-coding-cn?label=%E4%B8%BB%E8%A6%81%E8%AF%AD%E8%A8%80&style=for-the-badge\" alt=\"ä¸»è¦è¯­è¨€\"></a>\n  <a href=\"https://github.com/tukuaiai/vibe-coding-cn\"><img src=\"https://img.shields.io/github/languages/code-size/tukuaiai/vibe-coding-cn?label=%E4%BB%A3%E7%A0%81%E9%87%8F&style=for-the-badge\" alt=\"ä»£ç é‡\"></a>\n  <a href=\"https://github.com/tukuaiai/vibe-coding-cn/graphs/contributors\"><img src=\"https://img.shields.io/github/contributors/tukuaiai/vibe-coding-cn?label=%E8%B4%A1%E7%8C%AE%E8%80%85&style=for-the-badge\" alt=\"è´¡çŒ®è€…\"></a>\n  <a href=\"https://t.me/glue_coding\"><img src=\"https://img.shields.io/badge/èŠå¤©-Telegram-blue?style=for-the-badge&logo=telegram\" alt=\"äº¤æµç¾¤\"></a>\n</p>\n\n<!-- å¤šè¯­è¨€å…¥å£ -->\n<p>\n  <a href=\"./i18n/zh/README.md\"><img src=\"https://img.shields.io/badge/è¯­è¨€-ä¸­æ–‡-red?style=for-the-badge\" alt=\"ç®€ä½“ä¸­æ–‡\"></a>\n  <a href=\"./i18n/en/README.md\"><img src=\"https://img.shields.io/badge/è¯­è¨€-English-lightgrey?style=for-the-badge\" alt=\"English\"></a>\n  <a href=\"./i18n/he/\"><img src=\"https://img.shields.io/badge/è¯­è¨€-×¢×‘×¨×™×ª-navy?style=for-the-badge\" alt=\"Hebrew\"></a>\n  <a href=\"./i18n/ar/\"><img src=\"https://img.shields.io/badge/è¯­è¨€-Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©-brown?style=for-the-badge\" alt=\"Arabic\"></a>\n  <a href=\"./i18n/bn/\"><img src=\"https://img.shields.io/badge/è¯­è¨€-à¦¬à¦¾à¦‚à¦²à¦¾-orange?style=for-the-badge\" alt=\"Bengali\"></a>\n  <a href=\"./i18n/de/\"><img src=\"https://img.shields.io/badge/è¯­è¨€-Deutsch-black?style=for-the-badge\" alt=\"Deutsch\"></a>\n  <a href=\"./i18n/es/\"><img src=\"https://img.shields.io/badge/è¯­è¨€-EspaÃ±ol-yellow?style=for-the-badge\" alt=\"EspaÃ±ol\"></a>\n  <a href=\"./i18n/fa/\"><img src=\"https://img.shields.io/badge/è¯­è¨€-ÙØ§Ø±Ø³ÛŒ-purple?style=for-the-badge\" alt=\"Farsi\"></a>\n  <a href=\"./i18n/fr/\"><img src=\"https://img.shields.io/badge/è¯­è¨€-FranÃ§ais-blue?style=for-the-badge\" alt=\"FranÃ§ais\"></a>\n  <a href=\"./i18n/ha/\"><img src=\"https://img.shields.io/badge/è¯­è¨€-Hausa-darkgreen?style=for-the-badge\" alt=\"Hausa\"></a>\n  <a href=\"./i18n/hi/\"><img src=\"https://img.shields.io/badge/è¯­è¨€-à¤¹à¤¿à¤¨à¥à¤¦à¥€-darkorange?style=for-the-badge\" alt=\"Hindi\"></a>\n  <a href=\"./i18n/id/\"><img src=\"https://img.shields.io/badge/è¯­è¨€-Bahasa%20Indonesia-teal?style=for-the-badge\" alt=\"Bahasa Indonesia\"></a>\n  <a href=\"./i18n/it/\"><img src=\"https://img.shields.io/badge/è¯­è¨€-Italiano-green?style=for-the-badge\" alt=\"Italiano\"></a>\n  <a href=\"./i18n/ja/\"><img src=\"https://img.shields.io/badge/è¯­è¨€-æ—¥æœ¬èª-indigo?style=for-the-badge\" alt=\"æ—¥æœ¬èª\"></a>\n  <a href=\"./i18n/ko/\"><img src=\"https://img.shields.io/badge/è¯­è¨€-í•œêµ­ì–´-slateblue?style=for-the-badge\" alt=\"í•œêµ­ì–´\"></a>\n  <a href=\"./i18n/ms/\"><img src=\"https://img.shields.io/badge/è¯­è¨€-Bahasa%20Melayu-seagreen?style=for-the-badge\" alt=\"Bahasa Melayu\"></a>\n  <a href=\"./i18n/nl/\"><img src=\"https://img.shields.io/badge/è¯­è¨€-Nederlands-darkred?style=for-the-badge\" alt=\"Nederlands\"></a>\n  <a href=\"./i18n/pl/\"><img src=\"https://img.shields.io/badge/è¯­è¨€-Polski-crimson?style=for-the-badge\" alt=\"Polski\"></a>\n  <a href=\"./i18n/pt/\"><img src=\"https://img.shields.io/badge/è¯­è¨€-PortuguÃªs-darkslategray?style=for-the-badge\" alt=\"PortuguÃªs\"></a>\n  <a href=\"./i18n/ru/\"><img src=\"https://img.shields.io/badge/è¯­è¨€-Ğ ÑƒÑÑĞºĞ¸Ğ¹-steelblue?style=for-the-badge\" alt=\"Ğ ÑƒÑÑĞºĞ¸Ğ¹\"></a>\n  <a href=\"./i18n/sw/\"><img src=\"https://img.shields.io/badge/è¯­è¨€-Kiswahili-forestgreen?style=for-the-badge\" alt=\"Swahili\"></a>\n  <a href=\"./i18n/ta/\"><img src=\"https://img.shields.io/badge/è¯­è¨€-à®¤à®®à®¿à®´à¯-darkmagenta?style=for-the-badge\" alt=\"Tamil\"></a>\n  <a href=\"./i18n/th/\"><img src=\"https://img.shields.io/badge/è¯­è¨€-à¹„à¸—à¸¢-royalblue?style=for-the-badge\" alt=\"à¸ à¸²à¸©à¸²à¹„à¸—à¸¢\"></a>\n  <a href=\"./i18n/tr/\"><img src=\"https://img.shields.io/badge/è¯­è¨€-TÃ¼rkÃ§e-firebrick?style=for-the-badge\" alt=\"TÃ¼rkÃ§e\"></a>\n  <a href=\"./i18n/uk/\"><img src=\"https://img.shields.io/badge/è¯­è¨€-Ğ£ĞºÑ€Ğ°Ñ—Ğ½ÑÑŒĞºĞ°-cornflowerblue?style=for-the-badge\" alt=\"Ğ£ĞºÑ€Ğ°Ñ—Ğ½ÑÑŒĞºĞ°\"></a>\n  <a href=",
      "default_branch": "main"
    },
    "fetched_at": "2026-01-11T03:04:15.852643"
  },
  {
    "basic_info": {
      "name": "ml-sharp",
      "full_name": "apple/ml-sharp",
      "owner": "apple",
      "description": "Sharp Monocular View Synthesis in Less Than a Second",
      "url": "https://github.com/apple/ml-sharp",
      "clone_url": "https://github.com/apple/ml-sharp.git",
      "ssh_url": "git@github.com:apple/ml-sharp.git",
      "homepage": "https://apple.github.io/ml-sharp/",
      "created_at": "2025-12-12T03:46:09Z",
      "updated_at": "2026-01-11T00:44:16Z",
      "pushed_at": "2025-12-19T05:14:10Z"
    },
    "stats": {
      "stars": 6717,
      "forks": 443,
      "watchers": 6717,
      "open_issues": 60,
      "size": 189483
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 186219
      },
      "license": "Other",
      "topics": []
    },
    "content": {
      "readme": "# Sharp Monocular View Synthesis in Less Than a Second\n\n[![Project Page](https://img.shields.io/badge/Project-Page-green)](https://apple.github.io/ml-sharp/)\n[![arXiv](https://img.shields.io/badge/arXiv-2512.10685-b31b1b.svg)](https://arxiv.org/abs/2512.10685)\n\nThis software project accompanies the research paper: _Sharp Monocular View Synthesis in Less Than a Second_\nby _Lars Mescheder, Wei Dong, Shiwei Li, Xuyang Bai, Marcel Santos, Peiyun Hu, Bruno Lecouat, Mingmin Zhen, AmaÃ«l Delaunoy,\nTian Fang, Yanghai Tsin, Stephan Richter and Vladlen Koltun_.\n\n![](data/teaser.jpg)\n\nWe present SHARP, an approach to photorealistic view synthesis from a single image. Given a single photograph, SHARP regresses the parameters of a 3D Gaussian representation of the depicted scene. This is done in less than a second on a standard GPU via a single feedforward pass through a neural network. The 3D Gaussian representation produced by SHARP can then be rendered in real time, yielding high-resolution photorealistic images for nearby views. The representation is metric, with absolute scale, supporting metric camera movements. Experimental results demonstrate that SHARP delivers robust zero-shot generalization across datasets. It sets a new state of the art on multiple datasets, reducing LPIPS by 25â€“34% and DISTS by 21â€“43% versus the best prior model, while lowering the synthesis time by three orders of magnitude.\n\n## Getting started\n\nWe recommend to first create a python environment:\n\n```\nconda create -n sharp python=3.13\n```\n\nAfterwards, you can install the project using\n\n```\npip install -r requirements.txt\n```\n\nTo test the installation, run\n\n```\nsharp --help\n```\n\n## Using the CLI\n\nTo run prediction:\n\n```\nsharp predict -i /path/to/input/images -o /path/to/output/gaussians\n```\n\nThe model checkpoint will be downloaded automatically on first run and cached locally at `~/.cache/torch/hub/checkpoints/`.\n\nAlternatively, you can download the model directly:\n\n```\nwget https://ml-site.cdn-apple.com/models/sharp/sharp_2572gikvuh.pt\n```\n\nTo use a manually downloaded checkpoint, specify it with the `-c` flag:\n\n```\nsharp predict -i /path/to/input/images -o /path/to/output/gaussians -c sharp_2572gikvuh.pt\n```\n\nThe results will be 3D gaussian splats (3DGS) in the output folder. The 3DGS `.ply` files are compatible to various public 3DGS renderers. We follow the OpenCV coordinate convention (x right, y down, z forward). The 3DGS scene center is roughly at (0, 0, +z). When dealing with 3rdparty renderers, please scale and rotate to re-center the scene accordingly.\n\n### Rendering trajectories (CUDA GPU only)\n\nAdditionally you can render videos with a camera trajectory. While the gaussians prediction works for all CPU, CUDA, and MPS, rendering videos via the `--render` option currently requires a CUDA GPU. The gsplat renderer takes a while to initialize at the first launch.\n\n```\nsharp predict -i /path/to/input/images -o /path/to/output/gaussians --render\n\n# Or from the intermediate gaussians:\nsharp render -i /path/to/output/gaussians -o /path/to/output/renderings\n```\n\n## Evaluation\n\nPlease refer to the paper for both quantitative and qualitative evaluations.\nAdditionally, please check out this [qualitative examples page](https://apple.github.io/ml-sharp/) containing several video comparisons against related work.\n\n## Citation\n\nIf you find our work useful, please cite the following paper:\n\n```bibtex\n@inproceedings{Sharp2025:arxiv,\n  title      = {Sharp Monocular View Synthesis in Less Than a Second},\n  author     = {Lars Mescheder and Wei Dong and Shiwei Li and Xuyang Bai and Marcel Santos and Peiyun Hu and Bruno Lecouat and Mingmin Zhen and Ama\\\"{e}l Delaunoy and Tian Fang and Yanghai Tsin and Stephan R. Richter and Vladlen Koltun},\n  journal    = {arXiv preprint arXiv:2512.10685},\n  year       = {2025},\n  url        = {https://arxiv.org/abs/2512.10685},\n}\n```\n\n## Acknowledgements\n\nOur codebase is built using multiple opensource contributions, please see [ACKNOWLEDGEMENTS](ACKNOWLEDGEMENTS) for more details.\n\n## License\n\nPlease check out the repository [LICENSE](LICENSE) before using the provided code and\n[LICENSE_MODEL](LICENSE_MODEL) for the released models.\n",
      "default_branch": "main"
    },
    "fetched_at": "2026-01-11T03:04:17.011732"
  },
  {
    "basic_info": {
      "name": "Agent-Skills-for-Context-Engineering",
      "full_name": "muratcankoylan/Agent-Skills-for-Context-Engineering",
      "owner": "muratcankoylan",
      "description": "A comprehensive collection of Agent Skills for context engineering, multi-agent architectures, and production agent systems. Use when building, optimizing, or debugging agent systems that require effective context management.",
      "url": "https://github.com/muratcankoylan/Agent-Skills-for-Context-Engineering",
      "clone_url": "https://github.com/muratcankoylan/Agent-Skills-for-Context-Engineering.git",
      "ssh_url": "git@github.com:muratcankoylan/Agent-Skills-for-Context-Engineering.git",
      "homepage": null,
      "created_at": "2025-12-21T02:43:42Z",
      "updated_at": "2026-01-11T03:02:15Z",
      "pushed_at": "2026-01-07T17:32:32Z"
    },
    "stats": {
      "stars": 6332,
      "forks": 512,
      "watchers": 6332,
      "open_issues": 4,
      "size": 3401
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 155988
      },
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "# Agent Skills for Context Engineering\n\nA comprehensive, open collection of Agent Skills focused on context engineering principles for building production-grade AI agent systems. These skills teach the art and science of curating context to maximize agent effectiveness across any agent platform.\n\n## What is Context Engineering?\n\nContext engineering is the discipline of managing the language model's context window. Unlike prompt engineering, which focuses on crafting effective instructions, context engineering addresses the holistic curation of all information that enters the model's limited attention budget: system prompts, tool definitions, retrieved documents, message history, and tool outputs.\n\nThe fundamental challenge is that context windows are constrained not by raw token capacity but by attention mechanics. As context length increases, models exhibit predictable degradation patterns: the \"lost-in-the-middle\" phenomenon, U-shaped attention curves, and attention scarcity. Effective context engineering means finding the smallest possible set of high-signal tokens that maximize the likelihood of desired outcomes.\n\n## Skills Overview\n\n### Foundational Skills\n\nThese skills establish the foundational understanding required for all subsequent context engineering work.\n\n| Skill | Description |\n|-------|-------------|\n| [context-fundamentals](skills/context-fundamentals/) | Understand what context is, why it matters, and the anatomy of context in agent systems |\n| [context-degradation](skills/context-degradation/) | Recognize patterns of context failure: lost-in-middle, poisoning, distraction, and clash |\n| [context-compression](skills/context-compression/) | Design and evaluate compression strategies for long-running sessions |\n\n### Architectural Skills\n\nThese skills cover the patterns and structures for building effective agent systems.\n\n| Skill | Description |\n|-------|-------------|\n| [multi-agent-patterns](skills/multi-agent-patterns/) | Master orchestrator, peer-to-peer, and hierarchical multi-agent architectures |\n| [memory-systems](skills/memory-systems/) | Design short-term, long-term, and graph-based memory architectures |\n| [tool-design](skills/tool-design/) | Build tools that agents can use effectively |\n| [filesystem-context](skills/filesystem-context/) | **NEW** Use filesystems for dynamic context discovery, tool output offloading, and plan persistence |\n\n### Operational Skills\n\nThese skills address the ongoing operation and optimization of agent systems.\n\n| Skill | Description |\n|-------|-------------|\n| [context-optimization](skills/context-optimization/) | Apply compaction, masking, and caching strategies |\n| [evaluation](skills/evaluation/) | Build evaluation frameworks for agent systems |\n| [advanced-evaluation](skills/advanced-evaluation/) | Master LLM-as-a-Judge techniques: direct scoring, pairwise comparison, rubric generation, and bias mitigation |\n\n### Development Methodology\n\nThese skills cover the meta-level practices for building LLM-powered projects.\n\n| Skill | Description |\n|-------|-------------|\n| [project-development](skills/project-development/) | Design and build LLM projects from ideation through deployment, including task-model fit analysis, pipeline architecture, and structured output design |\n\n### Cognitive Architecture Skills\n\nThese skills cover formal cognitive modeling for rational agent systems.\n\n| Skill | Description |\n|-------|-------------|\n| [bdi-mental-states](skills/bdi-mental-states/) | **NEW** Transform external RDF context into agent mental states (beliefs, desires, intentions) using formal BDI ontology patterns for deliberative reasoning and explainability |\n\n## Design Philosophy\n\n### Progressive Disclosure\n\nEach skill is structured for efficient context use. At startup, agents load only skill names and descriptions. Full content loads only when a skill is activated for relevant tasks.\n\n### Platform Agnosticism\n\nThese skills focus on transferable principles rather than vendor-specific implementations. The patterns work across Claude Code, Cursor, and any agent platform that supports skills or allows custom instructions.\n\n### Conceptual Foundation with Practical Examples\n\nScripts and examples demonstrate concepts using Python pseudocode that works across environments without requiring specific dependency installations.\n\n## Usage\n\n### Usage with Claude Code\n\nThis repository is a **Claude Code Plugin Marketplace** containing context engineering skills that Claude automatically discovers and activates based on your task context.\n\n### Installation\n\n**Step 1: Add the Marketplace**\n\nRun this command in Claude Code to register this repository as a plugin source:\n\n```\n/plugin marketplace add muratcankoylan/Agent-Skills-for-Context-Engineering\n```\n\n**Step 2: Browse and Install**\n\nOption A - Browse available plugins:\n1. Select `Browse and install plugins`\n2. Select `context-engineering-marketplace`\n3. Choose a plugin (e.g., `context-engineering-fundamentals`, `agent-ar",
      "default_branch": "main"
    },
    "fetched_at": "2026-01-11T03:04:18.185250"
  },
  {
    "basic_info": {
      "name": "agentskills",
      "full_name": "agentskills/agentskills",
      "owner": "agentskills",
      "description": "Specification and documentation for Agent Skills",
      "url": "https://github.com/agentskills/agentskills",
      "clone_url": "https://github.com/agentskills/agentskills.git",
      "ssh_url": "git@github.com:agentskills/agentskills.git",
      "homepage": "https://agentskills.io",
      "created_at": "2025-12-16T15:47:19Z",
      "updated_at": "2026-01-11T02:58:17Z",
      "pushed_at": "2026-01-09T17:15:44Z"
    },
    "stats": {
      "stars": 5052,
      "forks": 258,
      "watchers": 5052,
      "open_issues": 42,
      "size": 120
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 29204,
        "Shell": 268
      },
      "license": null,
      "topics": []
    },
    "content": {
      "readme": "# Agent Skills\n\n[Agent Skills](https://agentskills.io) are a simple, open format for giving agents new capabilities and expertise.\n\nSkills are folders of instructions, scripts, and resources that agents can discover and use to perform better at specific tasks. Write once, use everywhere.\n\n## Getting Started\n\n- [Documentation](https://agentskills.io) - Guides and tutorials\n- [Specification](https://agentskills.io/specification) - Format details\n- [Example Skills](https://github.com/anthropics/skills) - See what's possible\n\nThis repo contains the specification, documentation, and reference SDK. Also see a list of example skills [here](https://github.com/anthropics/skills).\n\n## About\n\nAgent Skills is an open format maintained by [Anthropic](https://anthropic.com) and open to contributions from the community.",
      "default_branch": "main"
    },
    "fetched_at": "2026-01-11T03:04:19.377258"
  },
  {
    "basic_info": {
      "name": "sqlit",
      "full_name": "Maxteabag/sqlit",
      "owner": "Maxteabag",
      "description": "A user friendly TUI for SQL databases. Written in python. Supports SQL server, Mysql, PostreSQL, SQLite, Turso and more.",
      "url": "https://github.com/Maxteabag/sqlit",
      "clone_url": "https://github.com/Maxteabag/sqlit.git",
      "ssh_url": "git@github.com:Maxteabag/sqlit.git",
      "homepage": "",
      "created_at": "2025-12-13T00:10:36Z",
      "updated_at": "2026-01-11T01:35:20Z",
      "pushed_at": "2026-01-10T17:30:16Z"
    },
    "stats": {
      "stars": 2721,
      "forks": 64,
      "watchers": 2721,
      "open_issues": 5,
      "size": 25945
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 2765927,
        "Shell": 6977,
        "HCL": 6254,
        "CSS": 5650,
        "Nix": 2732,
        "Dockerfile": 278
      },
      "license": "MIT License",
      "topics": [
        "cockroachdb",
        "command-line-tool",
        "duckdb",
        "mariadb",
        "mysql",
        "oracle",
        "postgresql",
        "python",
        "sql",
        "sqlite",
        "ssh",
        "tui",
        "turso"
      ]
    },
    "content": {
      "readme": "<p align=\"center\">\n  <img src=\"assets/favorites/logo_sqlit.png\" alt=\"sqlit logo\" width=\"180\">\n</p>\n\n<h3 align=\"center\">The lazygit of SQL databases</h3>\n\n<p align=\"center\">\n  <em>Connect and query your database from your terminal in seconds.</em>\n</p>\n\n<p align=\"center\">\n  <a href=\"https://github.com/Maxteabag/sqlit/stargazers\"><img src=\"https://img.shields.io/github/stars/Maxteabag/sqlit?style=flat&color=yellow\" alt=\"GitHub Stars\"></a>\n  <img src=\"https://img.shields.io/badge/python-3.10+-blue.svg\" alt=\"Python\">\n  <img src=\"https://img.shields.io/badge/license-MIT-green.svg\" alt=\"License\">\n</p>\n\n<p align=\"center\">\n  <code>pipx install sqlit-tui</code>\n</p>\n\n<p align=\"center\">\n  <a href=\"https://www.buymeacoffee.com/PeterAdams\"><img src=\"https://img.shields.io/badge/Buy%20Me%20a%20Coffee-ffdd00?style=flat&logo=buy-me-a-coffee&logoColor=black\" alt=\"Buy Me a Coffee\"></a>\n</p>\n\n---\n\n### Connect\nSupports all major databases: SQL Server, PostgreSQL, MySQL, SQLite, MariaDB, FirebirdSQL, Oracle, DuckDB, CockroachDB, ClickHouse, Snowflake, Supabase, CloudFlare D1, Turso, Athena, BigQuery, RedShift, IBM Db2, SAP HANA, Teradata, Trino, Presto and Apache Flight SQL.\n\n![Database Providers](docs/demos/demo-providers.gif)\n\n### Query\nSyntax highlighting. History. Vim-style keybindings.\n\n![Query History](docs/demos/demo-history.gif)\n\n### Results\nLoad millions of rows. Inspect data, filter by content, fuzzy search.\n\n![Filter results](docs/demos/demo-filter/demo-filter.gif)\n\n### Docker Discovery\nAutomatically finds running database containers. Press 'Enter' to connect, sqlit figures out the details for you.\n\n![Docker Discovery](docs/demos/demo-docker-picker.gif)\n\n---\n\n## Features\n\n**Connection manager:** Save and switch connections without CLI args\n\n**Just run `sqlit`:** No CLI config needed, pick a connection and go\n\n**Multi-database support:** PostgreSQL, MySQL, SQLite, SQL Server, and 10+ more\n\n**Docker integration:** Auto-detect running database containers\n\n**Cloud CLI integration:** Easily browse and connect to your external databases through Azure, AWS and GCP CLI's\n\n**SSH tunnels:** Connect to remote databases securely with password or key auth\n\n**Secure credentials:** Passwords stored in your OS keyring\n\n**Vim-style editing:** Modal editing for terminal purists\n\n**Query history:** Searchable, per-connection history\n\n**Filter results:** Fuzzy search through millions of rows\n\n**Context-aware help:** Keybindings shown on screen\n\n**Browse databases:** Tables, views, procedures, indexes, triggers, sequences\n\n**Autocomplete:** Sophisticated SQL completion engine for tables, columns, and procedures\n\n**CLI mode:** Execute SQL from the command line\n\n**Themes:** Rose Pine, Tokyo Night, Nord, Gruvbox\n\n**Dependency wizard:** Auto-install missing drivers\n\n---\n\n## Motivation\n\nThroughout my career, the undesputed truth was that heavy GUI's like SSMS was the only respectable way to access a database. It didn't matter that I wasn't a DBA, or that I didn't need complex performance graphs. I was expected to install a gigabyte-heavy behemoth that took ages to launch all for the mere purpose of running a few queries to update and view a couple of rows.\n\nWhen I switched to Linux, I was suddenly unable to return to the devil I know, and I asked myself: _how do I access my data now?_\n\nThe popular answer was VS Code's SQL extension. But why should we developers launch a heavy Electron app designed for coding just to execute SQL?\n\nI had recently grown fond of Terminal UI's for their speed and keybinding focus. I looked for SQL TUIs, but the options were sparse. The ones I found lacked the user-friendliness and immediate \"pick-up-and-go\" nature of tools I loved, like lazygit, and I shortly returning to vscode sql extension.\n\nSomething wasn't right. I asked myself, why is it that running SQL queries can't be enjoyable? So I created sqlit.\n\nsqlit is for the developer who just wants to query their database with a user friendly UI without their RAM being eaten alive. It is a lightweight, beautiful, and keyboard-driven TUI designed to make accessing your data enjoyable, fast and easy like it should be-- all from inside your favorite terminal.\n\n---\n\n## Installation\n\n```bash\n# pipx (recommended)\npipx install sqlit-tui\n\n# uv\nuv tool install sqlit-tui\n\n# pip\npip install sqlit-tui\n\n# Arch Linux (AUR)\nyay -S sqlit\n\n# Nix (flake)\nnix run github:Maxteabag/sqlit\n```\n\n## Usage\n\n```bash\nsqlit\n```\n\nThe keybindings are shown at the bottom of the screen.\n\n### Try it without a database\n\nWant to explore the UI without connecting to a real database? Run with mock data:\n\n```bash\nsqlit --mock=sqlite-demo\n```\n\n### CLI\n\n```bash\n# Run a query\nsqlit query -c \"MyConnection\" -q \"SELECT * FROM Users\"\n\n# Output as CSV or JSON\nsqlit query -c \"MyConnection\" -q \"SELECT * FROM Users\" --format csv\nsqlit query -c \"MyConnection\" -f \"script.sql\" --format json\n\n# Create connections for different databases\nsqlit connections add mssql --name \"MySqlServer\" --server \"localhost\" --auth-ty",
      "default_branch": "main"
    },
    "fetched_at": "2026-01-11T03:04:20.504737"
  },
  {
    "basic_info": {
      "name": "Continuous-Claude-v3",
      "full_name": "parcadei/Continuous-Claude-v3",
      "owner": "parcadei",
      "description": "Context management for Claude Code. Hooks maintain state via ledgers and handoffs. MCP execution without context pollution. Agent orchestration with isolated context windows.",
      "url": "https://github.com/parcadei/Continuous-Claude-v3",
      "clone_url": "https://github.com/parcadei/Continuous-Claude-v3.git",
      "ssh_url": "git@github.com:parcadei/Continuous-Claude-v3.git",
      "homepage": "",
      "created_at": "2025-12-23T00:12:49Z",
      "updated_at": "2026-01-11T03:00:48Z",
      "pushed_at": "2026-01-10T16:56:50Z"
    },
    "stats": {
      "stars": 2147,
      "forks": 155,
      "watchers": 2147,
      "open_issues": 10,
      "size": 1577
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 1849690,
        "TypeScript": 687435,
        "Lean": 271555,
        "JavaScript": 68351,
        "Shell": 65247,
        "PLpgSQL": 8825
      },
      "license": "MIT License",
      "topics": [
        "agents",
        "claude-code",
        "claude-code-cli",
        "claude-code-hooks",
        "claude-code-mcp",
        "claude-code-skills",
        "claude-code-subagents",
        "claude-skills",
        "mcp"
      ]
    },
    "content": {
      "readme": "# Continuous Claude\n\n> A persistent, learning, multi-agent development environment built on Claude Code\n\n[![License: MIT](https://img.shields.io/badge/License-MIT-blue.svg)](LICENSE)\n[![Claude Code](https://img.shields.io/badge/Claude-Code-orange.svg)](https://claude.ai/code)\n[![Skills](https://img.shields.io/badge/Skills-109-green.svg)](#skills-system)\n[![Agents](https://img.shields.io/badge/Agents-32-purple.svg)](#agents-system)\n[![Hooks](https://img.shields.io/badge/Hooks-30-blue.svg)](#hooks-system)\n\n**Continuous Claude** transforms Claude Code into a continuously learning system that maintains context across sessions, orchestrates specialized agents, and eliminates wasting tokens through intelligent code analysis.\n\n## Table of Contents\n\n- [Why Continuous Claude?](#why-continuous-claude)\n- [Design Principles](#design-principles)\n- [How to Talk to Claude](#how-to-talk-to-claude)\n- [Quick Start](#quick-start)\n- [Architecture](#architecture)\n- [Core Systems](#core-systems)\n  - [Skills (109)](#skills-system)\n  - [Agents (32)](#agents-system)\n  - [Hooks (30)](#hooks-system)\n  - [TLDR Code Analysis](#tldr-code-analysis)\n  - [Memory System](#memory-system)\n  - [Continuity System](#continuity-system)\n  - [Math System](#math-system)\n- [Workflows](#workflows)\n- [Installation](#installation)\n- [Configuration](#configuration)\n- [Contributing](#contributing)\n- [License](#license)\n\n---\n\n## Why Continuous Claude?\n\nClaude Code has a **compaction problem**: when context fills up, the system compacts your conversation, losing nuanced understanding and decisions made during the session.\n\n**Continuous Claude solves this with:**\n\n| Problem | Solution |\n|---------|----------|\n| Context loss on compaction | YAML handoffs - more token-efficient transfer |\n| Starting fresh each session | Memory system recalls + daemon auto-extracts learnings |\n| Reading entire files burns tokens | 5-layer code analysis + semantic index |\n| Complex tasks need coordination | Meta-skills orchestrate agent workflows |\n| Repeating workflows manually | 109 skills with natural language triggers |\n\n**The mantra: Compound, don't compact.** Extract learnings automatically, then start fresh with full context.\n\n### Why \"Continuous\"? Why \"Compounding\"?\n\nThe name is a pun. **Continuous** because Claude maintains state across sessions. **Compounding** because each session makes the system smarterâ€”learnings accumulate like compound interest.\n\n---\n\n## Design Principles\n\nAn agent is five things: **Prompt + Tools + Context + Memory + Model**.\n\n| Component | What We Optimize |\n|-----------|------------------|\n| **Prompt** | Skills inject relevant context; hooks add system reminders |\n| **Tools** | TLDR reduces tokens; agents parallelize work |\n| **Context** | Not just *what* Claude knows, but *how* it's provided |\n| **Memory** | Daemon extracts learnings; recall surfaces them |\n| **Model** | Becomes swappable when the other four are solid |\n\n### Anti-Complexity\n\nWe resist plugin sprawl. Every MCP, subscription, and tool you add promises improvement but risks breaking context, tools, or prompts through clashes.\n\n**Our approach:**\n- **Time, not money** â€” No required paid services. Perplexity and NIA are optional, high-value-per-token.\n- **Learn, don't accumulate** â€” A system that learns handles edge cases better than one that collects plugins.\n- **Shift-left validation** â€” Hooks run pyright/ruff after edits, catching errors before tests.\n\nThe failure modes of complex systems are structurally invisible until they happen. A learning, context-efficient system doesn't prevent all failuresâ€”but it recovers and improves.\n\n---\n\n## How to Talk to Claude\n\n**You don't need to memorize slash commands.** Just describe what you want naturally.\n\n### The Skill Activation System\n\nWhen you send a message, a hook injects context that tells **Claude** which skills and agents are relevant. Claude infers from a rule-based system and decides which tools to use.\n\n```\n> \"Fix the login bug in auth.py\"\n\nğŸ¯ SKILL ACTIVATION CHECK\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n\nâš ï¸ CRITICAL SKILLS (REQUIRED):\n  â†’ create_handoff\n\nğŸ“š RECOMMENDED SKILLS:\n  â†’ fix\n  â†’ debug\n\nğŸ¤– RECOMMENDED AGENTS (token-efficient):\n  â†’ debug-agent\n  â†’ scout\n\nACTION: Use Skill tool BEFORE responding\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n```\n\n### Priority Levels\n\n| Level | Meaning |\n|-------|---------|\n| âš ï¸ **CRITICAL** | Must use (e.g., handoffs before ending session) |\n| ğŸ“š **RECOMMENDED** | Should use (e.g., workflow skills) |\n| ğŸ’¡ **SUGGESTED** | Consider using (e.g., optimization tools) |\n| ğŸ“Œ **OPTIONAL** | Nice to have (e.g., documentation helpers) |\n\n### Natural Language Examples\n\n| What You Say | What Activates |\n|--------------|----------------|\n| \"Fix the broken login\" | `/fix` workflow â†’ debug-agent, scout |\n| \"Build a user dashboard\" | `/build` workflow â†’ plan-agent, kraken |\n| \"I want to understand this codebase\" | `/explore` + scout agent |\n| \"What could go wrong with this plan?\" | `/premortem` |\n| \"Help me figure ",
      "default_branch": "main"
    },
    "fetched_at": "2026-01-11T03:04:21.642828"
  },
  {
    "basic_info": {
      "name": "LTX-2",
      "full_name": "Lightricks/LTX-2",
      "owner": "Lightricks",
      "description": "Official Python inference and LoRA trainer package for the LTX-2 audioâ€“video generative model.",
      "url": "https://github.com/Lightricks/LTX-2",
      "clone_url": "https://github.com/Lightricks/LTX-2.git",
      "ssh_url": "git@github.com:Lightricks/LTX-2.git",
      "homepage": "https://ltx.io/model/ltx-2",
      "created_at": "2026-01-03T13:16:29Z",
      "updated_at": "2026-01-11T03:00:20Z",
      "pushed_at": "2026-01-08T15:35:04Z"
    },
    "stats": {
      "stars": 1920,
      "forks": 205,
      "watchers": 1920,
      "open_issues": 29,
      "size": 413
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 787101
      },
      "license": "Other",
      "topics": [
        "generative-ai",
        "ltx",
        "ltx-2"
      ]
    },
    "content": {
      "readme": "# LTX-2\n\n[![Website](https://img.shields.io/badge/Website-LTX-181717?logo=google-chrome)](https://ltx.io)\n[![Model](https://img.shields.io/badge/HuggingFace-Model-orange?logo=huggingface)](https://huggingface.co/Lightricks/LTX-2)\n[![Demo](https://img.shields.io/badge/Demo-Try%20Now-brightgreen?logo=vercel)](https://app.ltx.studio/ltx-2-playground/i2v)\n[![Paper](https://img.shields.io/badge/Paper-PDF-EC1C24?logo=adobeacrobatreader&logoColor=white)](https://videos.ltx.io/LTX-2/grants/LTX_2_Technical_Report_compressed.pdf)\n[![Discord](https://img.shields.io/badge/Join-Discord-5865F2?logo=discord)](https://discord.gg/ltxplatform)\n\n**LTX-2** is the first DiT-based audio-video foundation model that contains all core capabilities of modern video generation in one model: synchronized audio and video, high fidelity, multiple performance modes, production-ready outputs, API access, and open access.\n\n<div align=\"center\">\n  <video src=\"https://github.com/user-attachments/assets/4414adc0-086c-43de-b367-9362eeb20228\" width=\"70%\" poster=\"\"> </video>\n</div>\n\n## ğŸš€ Quick Start\n\n```bash\n# Clone the repository\ngit clone https://github.com/Lightricks/LTX-2.git\ncd LTX-2\n\n# Set up the environment\nuv sync --frozen\nsource .venv/bin/activate\n```\n\n### Required Models\n\nDownload the following models from the [LTX-2 HuggingFace repository](https://huggingface.co/Lightricks/LTX-2):\n\n**LTX-2 Model Checkpoint** (choose and download one of the following)\n  * [`ltx-2-19b-dev-fp8.safetensors`](https://huggingface.co/Lightricks/LTX-2/blob/main/ltx-2-19b-dev-fp8.safetensors) - [Download](https://huggingface.co/Lightricks/LTX-2/resolve/main/ltx-2-19b-dev-fp8.safetensors)\n\n  * [`ltx-2-19b-dev.safetensors`](https://huggingface.co/Lightricks/LTX-2/blob/main/ltx-2-19b-dev.safetensors) - [Download](https://huggingface.co/Lightricks/LTX-2/resolve/main/ltx-2-19b-dev.safetensors)\n  * [`ltx-2-19b-distilled.safetensors`](https://huggingface.co/Lightricks/LTX-2/blob/main/ltx-2-19b-distilled.safetensors) - [Download](https://huggingface.co/Lightricks/LTX-2/resolve/main/ltx-2-19b-distilled.safetensors)\n  * [`ltx-2-19b-distilled-fp8.safetensors`](https://huggingface.co/Lightricks/LTX-2/blob/main/ltx-2-19b-distilled-fp8.safetensors) - [Download](https://huggingface.co/Lightricks/LTX-2/resolve/main/ltx-2-19b-distilled-fp8.safetensors)\n\n**Spatial Upscaler** - Required for current two-stage pipeline implementations in this repository\n  * [`ltx-2-spatial-upscaler-x2-1.0.safetensors`](https://huggingface.co/Lightricks/LTX-2/blob/main/ltx-2-spatial-upscaler-x2-1.0.safetensors) - [Download](https://huggingface.co/Lightricks/LTX-2/resolve/main/ltx-2-spatial-upscaler-x2-1.0.safetensors)\n\n**Temporal Upscaler** - Supported by the model and will be required for future pipeline implementations\n  * [`ltx-2-temporal-upscaler-x2-1.0.safetensors`](https://huggingface.co/Lightricks/LTX-2/blob/main/ltx-2-temporal-upscaler-x2-1.0.safetensors) - [Download](https://huggingface.co/Lightricks/LTX-2/resolve/main/ltx-2-temporal-upscaler-x2-1.0.safetensors)\n\n**Distilled LoRA** - Required for current two-stage pipeline implementations in this repository (except DistilledPipeline and ICLoraPipeline)\n  * [`ltx-2-19b-distilled-lora-384.safetensors`](https://huggingface.co/Lightricks/LTX-2/blob/main/ltx-2-19b-distilled-lora-384.safetensors) - [Download](https://huggingface.co/Lightricks/LTX-2/resolve/main/ltx-2-19b-distilled-lora-384.safetensors)\n\n**Gemma Text Encoder** (download all assets from the repository)\n  * [`Gemma 3`](https://huggingface.co/google/gemma-3-12b-it-qat-q4_0-unquantized/tree/main)\n\n**LoRAs**\n  * [`LTX-2-19b-IC-LoRA-Canny-Control`](https://huggingface.co/Lightricks/LTX-2-19b-IC-LoRA-Canny-Control) - [Download](https://huggingface.co/Lightricks/LTX-2-19b-IC-LoRA-Canny-Control/resolve/main/ltx-2-19b-ic-lora-canny-control.safetensors)\n  * [`LTX-2-19b-IC-LoRA-Depth-Control`](https://huggingface.co/Lightricks/LTX-2-19b-IC-LoRA-Depth-Control) - [Download](https://huggingface.co/Lightricks/LTX-2-19b-IC-LoRA-Depth-Control/resolve/main/ltx-2-19b-ic-lora-depth-control.safetensors)\n  * [`LTX-2-19b-IC-LoRA-Detailer`](https://huggingface.co/Lightricks/LTX-2-19b-IC-LoRA-Detailer) - [Download](https://huggingface.co/Lightricks/LTX-2-19b-IC-LoRA-Detailer/resolve/main/ltx-2-19b-ic-lora-detailer.safetensors)\n  * [`LTX-2-19b-IC-LoRA-Pose-Control`](https://huggingface.co/Lightricks/LTX-2-19b-IC-LoRA-Pose-Control) - [Download](https://huggingface.co/Lightricks/LTX-2-19b-IC-LoRA-Pose-Control/resolve/main/ltx-2-19b-ic-lora-pose-control.safetensors)\n  * [`LTX-2-19b-LoRA-Camera-Control-Dolly-In`](https://huggingface.co/Lightricks/LTX-2-19b-LoRA-Camera-Control-Dolly-In) - [Download](https://huggingface.co/Lightricks/LTX-2-19b-LoRA-Camera-Control-Dolly-In/resolve/main/ltx-2-19b-lora-camera-control-dolly-in.safetensors)\n  * [`LTX-2-19b-LoRA-Camera-Control-Dolly-Left`](https://huggingface.co/Lightricks/LTX-2-19b-LoRA-Camera-Control-Dolly-Left) - [Download](https://huggingface.co/Lightricks/LTX-2-19b",
      "default_branch": "main"
    },
    "fetched_at": "2026-01-11T03:04:22.786980"
  },
  {
    "basic_info": {
      "name": "HY-Motion-1.0",
      "full_name": "Tencent-Hunyuan/HY-Motion-1.0",
      "owner": "Tencent-Hunyuan",
      "description": "HY-Motion model for 3D character animation generation. ",
      "url": "https://github.com/Tencent-Hunyuan/HY-Motion-1.0",
      "clone_url": "https://github.com/Tencent-Hunyuan/HY-Motion-1.0.git",
      "ssh_url": "git@github.com:Tencent-Hunyuan/HY-Motion-1.0.git",
      "homepage": "https://hunyuan.tencent.com/motion",
      "created_at": "2025-12-29T11:09:18Z",
      "updated_at": "2026-01-11T01:08:27Z",
      "pushed_at": "2026-01-04T03:45:23Z"
    },
    "stats": {
      "stars": 1698,
      "forks": 118,
      "watchers": 1698,
      "open_issues": 11,
      "size": 20527
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 285533,
        "HTML": 40286
      },
      "license": "Other",
      "topics": []
    },
    "content": {
      "readme": "[ä¸­æ–‡é˜…è¯»](README_zh_cn.md)\n\n\n<p align=\"center\">\n  <img src=\"./assets/banner.png\" alt=\"Banner\" width=\"100%\">\n</p>\n\n<div align=\"center\">\n  <a href=\"https://hunyuan.tencent.com/motion\" target=\"_blank\">\n    <img src=\"https://img.shields.io/badge/Official%20Site-333399.svg?logo=homepage\" height=\"22px\" alt=\"Official Site\">\n  </a>\n  <a href=\"https://github.com/Tencent-Hunyuan/HY-Motion-1.0\" target=\"_blank\">\n    <img src=\"https://img.shields.io/badge/GitHub-Repo-181717?logo=github&logoColor=white\" height=\"22px\" alt=\"Github Repo\">\n  </a>\n  <a href=\"https://huggingface.co/spaces/tencent/HY-Motion-1.0\" target=\"_blank\">\n    <img src=\"https://img.shields.io/badge/%F0%9F%A4%97%20Demo-276cb4.svg\" height=\"22px\" alt=\"HuggingFace Space\">\n  </a>\n  <a href=\"https://huggingface.co/tencent/HY-Motion-1.0\" target=\"_blank\">\n    <img src=\"https://img.shields.io/badge/%F0%9F%A4%97%20Models-d96902.svg\" height=\"22px\" alt=\"HuggingFace Models\">\n  </a>\n  <a href=\"https://arxiv.org/pdf/2512.23464\" target=\"_blank\">\n    <img src=\"https://img.shields.io/badge/Report-b5212f.svg?logo=arxiv\" height=\"22px\" alt=\"ArXiv Report\">\n  </a>\n  <a href=\"https://x.com/TencentHunyuan\" target=\"_blank\">\n    <img src=\"https://img.shields.io/badge/Hunyuan-black.svg?logo=x\" height=\"22px\" alt=\"X (Twitter)\">\n  </a>\n</div>\n\n\n# HY-Motion 1.0: Scaling Flow Matching Models for 3D Motion Generation\n\n\n<p align=\"center\">\n  <img src=\"./assets/teaser.jpg\" alt=\"Teaser\" width=\"100%\">\n</p>\n\n\n## ğŸ”¥ News\n- **Dec 30, 2025**: ğŸ¤— We released the inference code and pretrained models of [HY-Motion 1.0](https://huggingface.co/tencent/HY-Motion-1.0). Please give it a try via our [HuggingFace Space](https://huggingface.co/spaces/tencent/HY-Motion-1.0) and our [Official Site](https://hunyuan.tencent.com/motion)!\n\n\n## **Introduction**\n\n**HY-Motion 1.0** is a series of text-to-3D human motion generation models based on Diffusion Transformer (DiT) and Flow Matching. It allows developers to generate skeleton-based 3D character animations from simple text prompts, which can be directly integrated into various 3D animation pipelines. This model series is the first to scale DiT-based text-to-motion models to the billion-parameter level, achieving significant improvements in instruction-following capabilities and motion quality over existing open-source models.\n\n### Key Features\n- **State-of-the-Art Performance**: Achieves state-of-the-art performance in both instruction-following capability and generated motion quality.\n\n- **Billion-Scale Models**: We are the first to successfully scale DiT-based models to the billion-parameter level for text-to-motion generation. This results in superior instruction understanding and following capabilities, outperforming comparable open-source models.\n\n- **Advanced Three-Stage Training**: Our models are trained using a comprehensive three-stage process:\n\n    - *Large-Scale Pre-training*: Trained on over 3,000 hours of diverse motion data to learn a broad motion prior.\n\n    - *High-Quality Fine-tuning*: Fine-tuned on 400 hours of curated, high-quality 3D motion data to enhance motion detail and smoothness.\n\n    - *Reinforcement Learning*: Utilizes Reinforcement Learning from human feedback and reward models to further refine instruction-following and motion naturalness.\n\n\n\n<p align=\"center\">\n  <img src=\"./assets/pipeline.png\" alt=\"System Overview\" width=\"100%\">\n</p>\n\n<p align=\"center\">\n  <img src=\"./assets/arch.png\" alt=\"Architecture\" width=\"100%\">\n</p>\n\n<p align=\"center\">\n  <img src=\"./assets/sotacomp.jpg\" alt=\"ComparisonSoTA\" width=\"100%\">\n</p>\n\n\n\n\n## ğŸ Model Zoo\n\n**HY-Motion 1.0 Series**\n\n| Model | Description | Date | Size | Huggingface | VRAM (min) |\n|:-------|:-------------|:------:|:------:|:-------------:|:-------------:|\n| **HY-Motion-1.0** | Standard Text2Motion Model | 2025-12-30 | 1.0B | [Download](https://huggingface.co/tencent/HY-Motion-1.0/tree/main/HY-Motion-1.0) | 26GB |\n| **HY-Motion-1.0-Lite** | Lightweight Text2Motion Model | 2025-12-30 | 0.46B | [Download](https://huggingface.co/tencent/HY-Motion-1.0/tree/main/HY-Motion-1.0-Lite) | 24GB |\n\n*Note*: To reduce GPU VRAM requirements, please use the following settings: `--num_seeds=1`, text prompt with less than 30 words, and motion length less than 5 seconds.  \n*Note*: This table does not includes GPU VRAM requirements for LLM-based prompt engineering feature. If you have sufficient VRAM to run HY-Motion-1.0 model but gradio fails with a VRAM-related error, Run the Gradio application with prompt engineering disabled by setting the environment variable like this: `DISABLE_PROMPT_ENGINEERING=True python3 gradio_app.py`\n\n## ğŸ¤— Get Started with HY-Motion 1.0\n\nHY-Motion 1.0 supports macOS, Windows, and Linux.\n\n\n- [Code Usage (CLI)](#code-usage-cli)\n- [Gradio App](#gradio-app)\n\n\n#### 1. Installation\n\nFirst, install PyTorch via the [official site](https://pytorch.org/). Then install the dependencies:\n\n```bash\ngit clone https://github.com/Tencent-Hunyuan/HY-Motion-1.0.git\ncd HY-Motion-1.0/\n# Make sure git-lf",
      "default_branch": "master"
    },
    "fetched_at": "2026-01-11T03:04:23.918436"
  },
  {
    "basic_info": {
      "name": "NitroGen",
      "full_name": "MineDojo/NitroGen",
      "owner": "MineDojo",
      "description": "A Foundation Model for Generalist Gaming Agents",
      "url": "https://github.com/MineDojo/NitroGen",
      "clone_url": "https://github.com/MineDojo/NitroGen.git",
      "ssh_url": "git@github.com:MineDojo/NitroGen.git",
      "homepage": "",
      "created_at": "2025-12-17T17:03:47Z",
      "updated_at": "2026-01-10T17:16:43Z",
      "pushed_at": "2026-01-07T15:15:42Z"
    },
    "stats": {
      "stars": 1571,
      "forks": 184,
      "watchers": 1571,
      "open_issues": 19,
      "size": 27889
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 117031
      },
      "license": "Other",
      "topics": []
    },
    "content": {
      "readme": "<img src=\"assets/github_banner.gif\" width=\"100%\" />\n\n<div align=\"center\">\n  <p style=\"font-size: 1.2em;\">\n    <a href=\"https://nitrogen.minedojo.org/\"><strong>Website</strong></a> | \n    <a href=\"https://huggingface.co/nvidia/NitroGen\"><strong>Model</strong></a> |\n    <a href=\"https://huggingface.co/datasets/nvidia/NitroGen\"><strong>Dataset</strong></a> |\n    <a href=\"https://arxiv.org/abs/2601.02427\"><strong>Paper</strong></a>\n  </p>\n</div>\n\n\n# NitroGen\n\nNitroGen is an open foundation model for generalist gaming agents. This multi-game model takes pixel input and predicts gamepad actions.\n\nNitroGen is trained through behavior cloning on the largest video-action gameplay dataset, assembled exclusively from internet videos. It can be adapted via post-training to unseen games.\n\n# Installation\n\n## Prerequisites\n\nWe **do not distribute game environments**, you must use your own copies of the games. This repository only supports running the agent on **Windows games**. You can serve the model from a Linux machine for inference, but the game ultimately has to run on Windows. We have tested on Windows 11 with Python â‰¥ 3.12.\n\n## Setup\n\nInstall this repo:\n```bash\ngit clone https://github.com/MineDojo/NitroGen.git\ncd NitroGen\npip install -e .\n```\n\nDownload NitroGen checkpoint from [HuggingFace](https://huggingface.co/nvidia/NitroGen):\n```bash\nhf download nvidia/NitroGen ng.pt\n```\n\n# Getting Started\n\nFirst, start an inference server for the model:\n```bash\npython scripts/serve.py <path_to_ng.pt>  \n```\n\nThen, run the agent on the game of your choice:\n```bash\npython scripts/play.py --process '<game_executable_name>.exe'\n```\n\nThe `--process` parameter must be the exact executable name of the game you want to play. You can find it by right-clicking on the game process in Windows Task Manager (Ctrl+Shift+Esc), and selecting `Properties`. The process name should be in the `General` tab and end with `.exe`.\n\n# Paper and Citation\n\nIf you find our work useful, please consider citing us!\n\n```bibtex\n@misc{magne2026nitrogen,\n      title={NitroGen: An Open Foundation Model for Generalist Gaming Agents}, \n      author={LoÃ¯c Magne and Anas Awadalla and Guanzhi Wang and Yinzhen Xu and Joshua Belofsky and Fengyuan Hu and Joohwan Kim and Ludwig Schmidt and Georgia Gkioxari and Jan Kautz and Yisong Yue and Yejin Choi and Yuke Zhu and Linxi \"Jim\" Fan},\n      year={2026},\n      eprint={2601.02427},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV},\n      url={https://arxiv.org/abs/2601.02427}, \n}\n```\n\n**Disclaimer**: This project is strictly for research purposes and is not an official NVIDIA product.\n",
      "default_branch": "main"
    },
    "fetched_at": "2026-01-11T03:04:25.080129"
  },
  {
    "basic_info": {
      "name": "Qwen-Image-Layered",
      "full_name": "QwenLM/Qwen-Image-Layered",
      "owner": "QwenLM",
      "description": "Qwen-Image-Layered: Layered Decomposition for Inherent Editablity",
      "url": "https://github.com/QwenLM/Qwen-Image-Layered",
      "clone_url": "https://github.com/QwenLM/Qwen-Image-Layered.git",
      "ssh_url": "git@github.com:QwenLM/Qwen-Image-Layered.git",
      "homepage": null,
      "created_at": "2025-12-18T08:54:50Z",
      "updated_at": "2026-01-11T02:54:51Z",
      "pushed_at": "2025-12-31T11:40:35Z"
    },
    "stats": {
      "stars": 1390,
      "forks": 109,
      "watchers": 1390,
      "open_issues": 17,
      "size": 23145
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 14946
      },
      "license": "Apache License 2.0",
      "topics": []
    },
    "content": {
      "readme": "<p align=\"center\">\n    <img src=\"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/layered/qwen-image-layered-logo.png\" width=\"800\"/>\n<p> \n<p align=\"center\">&nbsp&nbspğŸ¤— <a href=\"https://huggingface.co/Qwen/Qwen-Image-Layered\">HuggingFace</a>&nbsp&nbsp | &nbsp&nbspğŸ¤– <a href=\"https://modelscope.cn/models/Qwen/Qwen-Image-Layered\">ModelScope</a>&nbsp&nbsp | &nbsp&nbsp ğŸ“‘ <a href=\"https://arxiv.org/abs/2512.15603\">Research Paper</a> &nbsp&nbsp | &nbsp&nbsp ğŸ“‘ <a href=\"https://qwen.ai/blog?id=qwen-image-layered\">Blog</a> &nbsp&nbsp | &nbsp&nbsp ğŸ¤— <a href=\"https://huggingface.co/spaces/Qwen/Qwen-Image-Layered\">Demo</a> &nbsp&nbsp \n</p>\n\n<p align=\"center\">\n    <img src=\"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/layered/layered.JPG\" width=\"1024\"/>\n<p>\n\n## Introduction\nWe are excited to introduce **Qwen-Image-Layered**, a model capable of decomposing an image into multiple RGBA layers. This layered representation unlocks **inherent editability**: each layer can be independently manipulated without affecting other content. Meanwhile, such a layered representation naturally supports **high-fidelity elementary operations**-such as resizing, reposition, and recoloring. By physically isolating semantic or structural components into distinct layers, our approach enables high-fidelity and consistent editing.\n\n\n[![Qwen Image Layered](https://img.youtube.com/vi/OVhmiBrsziQ/0.jpg)](https://www.youtube.com/watch?v=OVhmiBrsziQ)\n\n\n## News\n- 2025.12.22: You can try Qwen-Image-Layered on [Huggingface Spaces](https://huggingface.co/spaces/Qwen/Qwen-Image-Layered) and [Modelscope Studio](https://modelscope.cn/studios/Qwen/Qwen-Image-Layered).\n- 2025.12.19: We released Qwen-Image-Layered weights! Check at [Huggingface](https://huggingface.co/Qwen/Qwen-Image-Layered) and [ModelScope](https://modelscope.cn/models/Qwen/Qwen-Image-Layered)!\n- 2025.12.19: We released Qwen-Image-Layered! Check our [Blog](https://qwen.ai/blog?id=qwen-image-layered) for more details!\n- 2025.12.18: We released our [Research Paper](https://arxiv.org/abs/2512.15603) on Arxiv!\n\n> [!NOTE]\n> - The text prompt is intended to describe the overall content of the input imageâ€”including elements that may be partially occluded (e.g., you may specify the text hidden behind a foreground object). It is not designed to control the semantic content of individual layers explicitly.\n> - The released weights are specifically fine-tuned for the image-to-multi-RGBA decomposition task. As a result, while the model supports text-conditioned inference, its performance on text-to-multi-RGBA generation is limited.\n\n## Quick Start\n\n1. Make sure your transformers>=4.51.3 (Supporting Qwen2.5-VL)\n\n2. Install the latest version of diffusers\n```\npip install git+https://github.com/huggingface/diffusers\npip install python-pptx\npip install psd-tools\n```\n\n\n```python\nfrom diffusers import QwenImageLayeredPipeline\nimport torch\nfrom PIL import Image\n\npipeline = QwenImageLayeredPipeline.from_pretrained(\"Qwen/Qwen-Image-Layered\")\npipeline = pipeline.to(\"cuda\", torch.bfloat16)\npipeline.set_progress_bar_config(disable=None)\n\nimage = Image.open(\"asserts/test_images/1.png\").convert(\"RGBA\")\ninputs = {\n    \"image\": image,\n    \"generator\": torch.Generator(device='cuda').manual_seed(777),\n    \"true_cfg_scale\": 4.0,\n    \"negative_prompt\": \" \",\n    \"num_inference_steps\": 50,\n    \"num_images_per_prompt\": 1,\n    \"layers\": 4,\n    \"resolution\": 640,      # Using different bucket (640, 1024) to determine the resolution. For this version, 640 is recommended\n    \"cfg_normalize\": True,  # Whether enable cfg normalization.\n    \"use_en_prompt\": True,  # Automatic caption language if user does not provide caption\n}\n\nwith torch.inference_mode():\n    output = pipeline(**inputs)\n    output_image = output.images[0]\n\nfor i, image in enumerate(output_image):\n    image.save(f\"{i}.png\")\n```\n\n## Deploy Qwen-Image-Layered\nThe following scripts will start a Gradio-based web interface where you can decompose an image and export the layers into pptx, zip, and psd files, where you can edit and move these layers flexibly.\n```bash\npython src/app.py\n```\n\nAfter decomposition, you may want to edit specific layers. The following scripts will launch a Gradio-based web interface where you can edit images with transparency using Qwen-Image-Edit.\n```bash\npython src/tool/edit_rgba_image.py\n```\n\nAfter editing the individual decomposed layers, you can use the following script to combine them into a new image. Remember to upload the layers in orderâ€”from the bottom layer to the top.\n```bash\npython src/tool/combine_layers.py\n```\n\n### vLLM-Omni\n[vLLM-Omni](https://github.com/vllm-project/vllm-omni) now supports Qwen-Image-Layered. See the [recipes](https://docs.vllm.ai/projects/recipes/en/latest/Qwen/Qwen-Image.html) for up-to-date details.\n\n## Showcase\n### Layered Decomposition in Application\nGiven an image, Qwen-Image-Layered can decompose it into several RGBA layers:\n![Example Image](https://qianwen-res.oss-cn-beijing.aliyunc",
      "default_branch": "main"
    },
    "fetched_at": "2026-01-11T03:04:26.324848"
  },
  {
    "basic_info": {
      "name": "IQuest-Coder-V1",
      "full_name": "IQuestLab/IQuest-Coder-V1",
      "owner": "IQuestLab",
      "description": null,
      "url": "https://github.com/IQuestLab/IQuest-Coder-V1",
      "clone_url": "https://github.com/IQuestLab/IQuest-Coder-V1.git",
      "ssh_url": "git@github.com:IQuestLab/IQuest-Coder-V1.git",
      "homepage": null,
      "created_at": "2025-12-31T06:42:57Z",
      "updated_at": "2026-01-11T02:34:58Z",
      "pushed_at": "2026-01-03T17:47:18Z"
    },
    "stats": {
      "stars": 1179,
      "forks": 75,
      "watchers": 1179,
      "open_issues": 15,
      "size": 28395
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 706343,
        "Shell": 47957
      },
      "license": "Other",
      "topics": []
    },
    "content": {
      "readme": "![Evaluation Results](./papers/iquest-coder-v1-logo.png)\n\n<p align=\"center\">\n  ğŸ“˜ <a href=\"https://iquestlab.github.io\">Blog</a >\n  &nbsp;â€¢&nbsp;\n  ğŸ“„ <a href=\"https://github.com/IQuestLab/IQuest-Coder-V1/blob/main/papers/IQuest_Coder_Technical_Report.pdf\">Technical Report</a >\n</p >\n\n# IQuest-Coder-V1 Model Family\n\n| Model | Link |\n|-------|------|\n| IQuest-Coder-V1-40B-Base-Stage1 | [ğŸ¤— Hugging Face](https://huggingface.co/IQuestLab/IQuest-Coder-V1-40B-Base-Stage1) |\n| IQuest-Coder-V1-40B-Base | [ğŸ¤— Hugging Face](https://huggingface.co/IQuestLab/IQuest-Coder-V1-40B-Base) |\n| IQuest-Coder-V1-40B-Instruct | [ğŸ¤— Hugging Face](https://huggingface.co/IQuestLab/IQuest-Coder-V1-40B-Instruct) |\n| IQuest-Coder-V1-40B-Loop-Instruct | [ğŸ¤— Hugging Face](https://huggingface.co/IQuestLab/IQuest-Coder-V1-40B-Loop-Instruct) |\n\n## Sampling Parameters:\nFor the IQuest-Coder-V1-Instruct: We suggest using Temperature=0.6, TopP=0.85, TopK=20.\n\n## IQuest-Coder-V1 Highlights\n\nIQuest-Coder-V1 is a new family of code large language models (LLMs) designed to advance autonomous software engineering and code intelligence. Built on the innovative code-flow multi-stage training paradigm, IQuest-Coder-V1 captures the dynamic evolution of software logic, delivering state-of-the-art performance across critical dimensions:\n\n- **State-of-the-Art Performance**: Achieves leading results on SWE-Bench Verified (76.2%), BigCodeBench (49.9%), LiveCodeBench v6 (81.1%), and other major coding benchmarks, surpassing competitive models across agentic software engineering, competitive programming, and complex tool use.\n- **Code-Flow Training Paradigm**: Moving beyond static code representations, our models learn from repository evolution patterns, commit transitions, and dynamic code transformations to understand real-world software development processes.\n- **Dual Specialization Paths**: Bifurcated post-training delivers two specialized variantsâ€”Thinking models (utilizing reasoning-driven RL for complex problem-solving) and Instruct models (optimized for general coding assistance and instruction-following).\n- **Efficient Architecture**: The IQuest-Coder-V1-Loop variant introduces a recurrent mechanism that optimizes the trade-off between model capacity and deployment footprint.\n- **Native Long Context**: All models natively support up to 128K tokens without requiring additional scaling techniques.\n\n## Model Overview\n\nThe IQuest-Coder-V1 series includes models ranging from 7B to 40B parameters, with both standard and Loop variants:\n\n| Model | Parameters | Layers | Hidden Size | Attention Heads (Q/KV) | Context Length |\n|-------|------------|--------|-------------|------------------------|----------------|\n| IQuest-Coder-V1-7B-Instruct | 7B | 14 | 5120 | 40/8 | 128K |\n| IQuest-Coder-V1-7B-Thinking | 7B | 14 | 5120 | 40/8 | 128K |\n| IQuest-Coder-V1-14B-Instruct | 14B | 28 | 5120 | 40/8 | 128K |\n| IQuest-Coder-V1-14B-Thinking | 14B | 28 | 5120 | 40/8 | 128K |\n| IQuest-Coder-V1-40B-Instruct | 40B | 80 | 5120 | 40/8 | 128K |\n| IQuest-Coder-V1-40B-Thinking | 40B | 80 | 5120 | 40/8 | 128K |\n| IQuest-Coder-V1-40B-Loop-Instruct | 40B | 80 (2 iterations) | 5120 | 40/8 | 128K |\n| IQuest-Coder-V1-40B-Loop-Thinking | 40B | 80 (2 iterations) | 5120 | 40/8 | 128K |\n\n**Architecture Features:**\n\n- Grouped Query Attention (GQA) for efficient inference\n- Native 128K context length support\n- Vocabulary size: 76,800 tokens\n- Loop variants use recurrent transformer design with shared parameters across two iterations\n\nFor more details, please refer to our Technical Report, GitHub.\n\n## Quickstart\n\nIQuest-Coder-V1 uses custom modeling code via Hugging Face's auto_map feature. We recommend using transformers>=4.52.4.\n\n### Basic Usage with Transformers\n\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_name = \"IQuest/IQuest-Coder-V1-40B-Instruct\"\n\n# Load the tokenizer and model\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype=\"auto\",\n    device_map=\"auto\"\n)\n\n# Prepare the input\nprompt = \"Write a Python function to calculate the Fibonacci sequence using dynamic programming.\"\nmessages = [\n    {\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n\n# Generate response\ngenerated_ids = model.generate(\n    **model_inputs,\n    max_new_tokens=8192\n)\ngenerated_ids = generated_ids[0][len(model_inputs.input_ids[0]):]\nresponse = tokenizer.decode(generated_ids, skip_special_tokens=True)\n\nprint(response)\n```\n\n### Using Thinking Models\n\nFor complex reasoning tasks, use the Thinking variant:\n\n```python\nmodel_name = \"IQuestLab/IQuest-Coder-V1-40B-Thinking\"\n\n# The Thinking model includes explicit reasoning traces\n# Use similar code as above, but expect longer, more detailed responses\n# with step-by-step problem decomposition\n```\n\n### Dep",
      "default_branch": "main"
    },
    "fetched_at": "2026-01-11T03:04:27.476165"
  },
  {
    "basic_info": {
      "name": "claude-workflow-v2",
      "full_name": "CloudAI-X/claude-workflow-v2",
      "owner": "CloudAI-X",
      "description": "Universal Claude Code workflow plugin with agents, skills, hooks, and commands",
      "url": "https://github.com/CloudAI-X/claude-workflow-v2",
      "clone_url": "https://github.com/CloudAI-X/claude-workflow-v2.git",
      "ssh_url": "git@github.com:CloudAI-X/claude-workflow-v2.git",
      "homepage": null,
      "created_at": "2026-01-01T02:20:41Z",
      "updated_at": "2026-01-11T01:54:26Z",
      "pushed_at": "2026-01-07T11:59:16Z"
    },
    "stats": {
      "stars": 1097,
      "forks": 166,
      "watchers": 1097,
      "open_issues": 0,
      "size": 137
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 17085,
        "Shell": 3568
      },
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "# project-starter\n\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n[![Claude Code](https://img.shields.io/badge/Claude%20Code-v1.0.33+-blue.svg)](https://code.claude.com)\n[![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg)](https://github.com/CloudAI-X/claude-workflow/pulls)\n\nA universal Claude Code workflow plugin with specialized agents, skills, hooks, and output styles for any software project.\n\n---\n\n## Quick Start\n\n### Option 1: CLI (Per-Session)\n\n```bash\n# Clone the plugin\ngit clone https://github.com/CloudAI-X/claude-workflow.git\n\n# Run Claude Code with the plugin\nclaude --plugin-dir ./claude-workflow\n```\n\n### Option 2: Agent SDK\n\n```typescript\nimport { query } from \"@anthropic-ai/claude-agent-sdk\";\n\nfor await (const message of query({\n  prompt: \"Hello\",\n  options: {\n    plugins: [{ type: \"local\", path: \"./claude-workflow\" }],\n  },\n})) {\n  // Plugin commands, agents, and skills are now available\n}\n```\n\n### Option 3: Install Permanently\n\n```bash\n# Install from marketplace (when available)\nclaude plugin install project-starter\n\n# Or install from local directory\nclaude plugin install ./claude-workflow\n```\n\n### Verify Installation\n\nAfter loading the plugin, verify it's working:\n\n```\n> /plugin\n```\n\nTab to **Installed** - you should see `project-starter` listed.\nTab to **Errors** - should be empty (no errors).\n\nThese commands become available:\n\n```\n/project-starter:architect    # Architecture-first mode\n/project-starter:rapid        # Ship fast mode\n/project-starter:commit       # Auto-generate commit message\n/project-starter:verify-changes  # Multi-agent verification\n```\n\n---\n\n## What's Included\n\n| Component    | Count | Description                                                              |\n| ------------ | ----- | ------------------------------------------------------------------------ |\n| **Agents**   | 7     | Specialized subagents for code review, debugging, security, etc.         |\n| **Commands** | 17    | Slash commands for workflows and output styles                           |\n| **Skills**   | 6     | Knowledge domains Claude uses autonomously                               |\n| **Hooks**    | 9     | Automation scripts for formatting, security, verification, notifications |\n\n---\n\n## Usage Examples\n\n### Commands in Action\n\n**Auto-commit your changes:**\n\n```\n> /project-starter:commit\n\nLooking at staged changes...\nâœ“ Created commit: feat(auth): add JWT refresh token endpoint\n```\n\n**Full git workflow:**\n\n```\n> /project-starter:commit-push-pr\n\nâœ“ Committed: feat: add user dashboard\nâœ“ Pushed to origin/feature/dashboard\nâœ“ Created PR #42: https://github.com/you/repo/pull/42\n```\n\n**Verify before shipping:**\n\n```\n> /project-starter:verify-changes\n\nSpawning verification agents...\nâ”œâ”€ build-validator: âœ“ Build passes\nâ”œâ”€ test-runner: âœ“ 42 tests pass\nâ”œâ”€ lint-checker: âš  2 warnings (non-blocking)\nâ””â”€ security-scanner: âœ“ No vulnerabilities\n\nReady to ship!\n```\n\n### Agents in Action\n\nAgents spawn automatically based on your request:\n\n**You say:** \"The login is broken, users get 401 errors\"\n\n```\n[debugger agent activated]\nâ†’ Checking auth middleware... found issue\nâ†’ Token validation uses wrong secret in production\nâ†’ Fix: Update AUTH_SECRET in .env.production\n```\n\n**You say:** \"Review my changes\"\n\n```\n[code-reviewer agent activated]\nâ†’ Analyzing 3 files changed...\nâœ“ Logic is correct\nâš  Missing null check on line 42\nâš  Consider adding rate limiting to this endpoint\n```\n\n**You say:** \"Add authentication to the API\"\n\n```\n[orchestrator agent activated]\nâ†’ Breaking down into subtasks:\n  1. Design auth schema (spawning architect)\n  2. Implement JWT middleware\n  3. Add login/register endpoints\n  4. Write tests (spawning test-architect)\n  5. Update API docs (spawning docs-writer)\n```\n\n### Skills in Action\n\nSkills provide domain knowledge automatically:\n\n**You ask:** \"How should I structure the payment service?\"\n\n```\n[designing-architecture skill applied]\nâ†’ Recommending hexagonal architecture\nâ†’ Payment providers as adapters\nâ†’ Core domain isolated from infrastructure\n```\n\n**You ask:** \"Make this endpoint faster\"\n\n```\n[optimizing-performance skill applied]\nâ†’ Adding database indexes\nâ†’ Implementing response caching\nâ†’ Using pagination for large results\n```\n\n### Hooks in Action\n\nHooks run automatically on events:\n\n**Security block (pre-edit):**\n\n```\nâ›” BLOCKED: Potential secret detected\n   File: src/config.ts, Line 5\n   Pattern: API key (sk-...)\n\n   Remove the secret and use environment variables.\n```\n\n**Auto-format (post-edit):**\n\n```\nâœ“ Formatted with prettier: src/components/Button.tsx\nâœ“ Formatted with black: scripts/deploy.py\n```\n\n**Desktop notifications:**\n\n```\nğŸ”” \"Claude needs input\" - when waiting for your response\nğŸ”” \"Task complete\" - when finished\n```\n\n---\n\n## Commands Reference\n\nAll commands use the format `/project-starter:<command>`.\n\n### Output Styles\n\n| Command                      | Mode                                          |\n| -----------------",
      "default_branch": "main"
    },
    "fetched_at": "2026-01-11T03:04:28.642444"
  },
  {
    "basic_info": {
      "name": "tally",
      "full_name": "davidfowl/tally",
      "owner": "davidfowl",
      "description": "Let agents classify your bank transactions.",
      "url": "https://github.com/davidfowl/tally",
      "clone_url": "https://github.com/davidfowl/tally.git",
      "ssh_url": "git@github.com:davidfowl/tally.git",
      "homepage": "http://tallyai.money",
      "created_at": "2025-12-26T18:43:19Z",
      "updated_at": "2026-01-10T23:31:39Z",
      "pushed_at": "2026-01-10T04:30:48Z"
    },
    "stats": {
      "stars": 938,
      "forks": 68,
      "watchers": 938,
      "open_issues": 10,
      "size": 3828
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 943787,
        "JavaScript": 94288,
        "CSS": 35333,
        "HTML": 15833,
        "PowerShell": 8204,
        "Shell": 5925
      },
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "# Tally\n\n**A local rule engine for transaction classification.** Pair it with an AI assistant to eliminate the manual work.\n\nWorks with Claude Code, Codex, Copilot, Cursor, or any command-line AI agent.\n\n## Install\n\n**Linux / macOS**\n```bash\ncurl -fsSL https://tallyai.money/install.sh | bash\n```\n\n**PowerShell**\n```powershell\nirm https://tallyai.money/install.ps1 | iex\n```\n\n## Quick Start\n\n```bash\ntally init ./my-budget      # Create budget folder\ncd my-budget\ntally workflow              # See next steps\n```\n\nTell your AI assistant: *\"Use tally to categorize my transactions\"*\n\n## Documentation\n\nFull documentation is available at **[tallyai.money](https://tallyai.money)**:\n\n- [Quick Start](https://tallyai.money/quickstart.html) - Get running in 5 minutes\n- [Guide](https://tallyai.money/guide.html) - Using Tally with AI assistants\n- [Reference](https://tallyai.money/reference.html) - merchants.rules and views.rules syntax\n- [Formats](https://tallyai.money/formats.html) - settings.yaml and CSV format strings\n\n## Commands\n\n| Command | Description |\n|---------|-------------|\n| `tally init` | Create a new budget folder |\n| `tally workflow` | Show context-aware next steps |\n| `tally run` | Generate HTML spending report |\n| `tally discover` | Find uncategorized transactions |\n| `tally explain` | Explain merchant classifications |\n| `tally inspect` | Analyze CSV structure |\n| `tally reference` | Show full syntax reference |\n\n## License\n\nMIT\n",
      "default_branch": "main"
    },
    "fetched_at": "2026-01-11T03:04:29.773723"
  },
  {
    "basic_info": {
      "name": "z80ai",
      "full_name": "HarryR/z80ai",
      "owner": "HarryR",
      "description": "Z80-Î¼LM is a 2-bit quantized language model small enough to run on an 8-bit Z80 processor. Train conversational models in Python, export them as CP/M .COM binaries, and chat with your vintage computer.",
      "url": "https://github.com/HarryR/z80ai",
      "clone_url": "https://github.com/HarryR/z80ai.git",
      "ssh_url": "git@github.com:HarryR/z80ai.git",
      "homepage": "",
      "created_at": "2025-12-21T14:26:48Z",
      "updated_at": "2026-01-10T11:09:28Z",
      "pushed_at": "2026-01-06T06:14:52Z"
    },
    "stats": {
      "stars": 932,
      "forks": 36,
      "watchers": 932,
      "open_issues": 5,
      "size": 217
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 87847
      },
      "license": null,
      "topics": [
        "chatbot",
        "code-golf",
        "cpm",
        "language-model",
        "machine-learning",
        "nlp",
        "quantization",
        "retro",
        "retrocomputing",
        "tinyml",
        "z80"
      ]
    },
    "content": {
      "readme": "# Z80-Î¼LM: A Retrocomputing Micro Language Model\n\nZ80-Î¼LM is a 'conversational AI' that generates short character-by-character sequences, with quantization-aware training (QAT) to run on a Z80 processor with 64kb of ram.\n\nThe root behind this project was the question: how small can we go while still having personality, and can it be trained or fine-tuned easily? With easy self-hosted distribution?\n\nThe answer is Yes! And a 40kb .com binary (including inference, weights & a chat-style UI) running on a 4MHz processor from 1976.\n\nIt won't pass the Turing test, but it might make you smile at the green screen.\n\nFor insight on how to best train your own model, see [TRAINING.md](TRAINING.md).\n\n## Examples\n\nTwo pre-built examples are included:\n\n### [tinychat](examples/tinychat/)\n\nA conversational chatbot trained on casual Q&A pairs. Responds to greetings, questions about itself, and general banter with terse personality-driven answers.\n\n```\n> hello\nHI\n> are you a robot\nYES\n> do you dream\nMAYBE\n```\n\n### [guess](examples/guess/)\n\nA 20 Questions game where the model knows a secret topic and answers YES/NO/MAYBE to your questions. Guess correctly to WIN.\n\n```\n> is it alive\nYES\n> is it big\nYES\n> does it have a trunk\nYES\n> is it grey\nMAYBE\n> elephant\nWIN\n```\n\nIncludes tools for generating training data with LLMs (Ollama or Claude API) and balancing class distributions.\n\n## Features\n\n- **Trigram hash encoding**: Input text is hashed into 128 buckets - typo-tolerant, word-order invariant\n- **2-bit weight quantization**: Each weight is {-2, -1, 0, +1}, packed 4 per byte\n- **16-bit integer inference**: All math uses Z80-native 16-bit signed arithmetic\n- **~40KB .COM file**: Fits in CP/M's Transient Program Area (TPA)\n- **Autoregressive generation**: Outputs text character-by-character\n- **No floating point**: Everything is integer math with fixed-point scaling\n- **Interactive chat mode**: Just run `CHAT` with no arguments\n\n## Interaction Style\n\nThe model doesn't understand you. But somehow, it *gets* you.\n\nYour input is hashed into 128 buckets via trigram encoding - an abstract \"tag cloud\" representation. The model responds to the *shape* of your input, not the exact words:\n\n```\n\"hello there\"  â†’  [bucket 23: 64, bucket 87: 32, ...]\n\"there hello\"  â†’  [bucket 23: 64, bucket 87: 32, ...]  (same!)\n\"helo ther\"    â†’  [bucket 23: 32, bucket 87: 32, ...]  (similar - typo tolerant)\n```\n\nThis is semantically powerful for short inputs, but there's a limit: longer or order-dependent sentences blur together as concepts compete for the same buckets. \"Open the door and turn on the lights\" will likely be too close to distinguish from \"turn on the door and open the lights.\"\n\n### Small Responses, Big Meaning\n\nA 1-2 word response can convey surprising nuance:\n\n- `OK` - acknowledged, neutral\n- `WHY?` - questioning your premise\n- `R U?` - casting existential doubt\n- `MAYBE` - genuine uncertainty\n- `AM I?` - reflecting the question back\n\nThis isn't necessarily a limitation - it's a different mode of interaction. The terse responses force you to infer meaning from context or ask probing direct yes/no questions to see if it understands or not (e.g. 'are you a bot', 'are you human', 'am i human' displays logically consistent memorized answers)\n\n### What It's Good At\n\n- Short, varied inputs with consistent categorized outputs\n- Fuzzy matching (typos, rephrasing, word order)\n- Personality through vocabulary choice\n- Running on constrained 8-bit hardware\n\n### What It's Not\n\n- A chatbot that generates novel sentences\n- Something that tracks multi-turn context deeply\n- A parser that understands grammar\n- Anything approaching general intelligence\n\nIt's small, but functional. And sometimes that's exactly what you need.\n\n## Architecture\n\n- **Input**: 128 query trigram buckets + 128 context buckets\n- **Hidden layers**: Configurable depth/width, e.g., 256 â†’ 192 â†’ 128\n- **Output**: One neuron per character in charset\n- **Activation**: ReLU between hidden layers\n\n### Quantization Constraints\n\nThe Z80 is an 8-bit CPU, but we use its 16-bit register pairs (HL, DE, BC) for activations and accumulators. Weights are packed 4-per-byte (2-bit each) and unpacked into 8-bit signed values for the multiply-accumulate.\n\nThe 16-bit accumulator gives us numerical stability (summing 256 inputs without overflow), but the model's expressiveness is still bottlenecked by the 2-bit weights, and naive training may overflow or act 'weirdly' without QAT.\n\n### Z80 Inner Loops\n\nThe core of inference is a tight multiply-accumulate loop. Weights are packed 4-per-byte:\n\n```z80\n; Unpack 2-bit weight from packed byte\nld a, (PACKED)      ; Get packed weights\nand 03h             ; Mask bottom 2 bits\nsub 2               ; Map 0,1,2,3 â†’ -2,-1,0,+1\nld (WEIGHT), a\n\n; Rotate for next weight\nld a, (PACKED)\nrrca\nrrca\nld (PACKED), a\n```\n\nThe multiply-accumulate handles the 4 possible weight values:\n\n```z80\nMULADD:\n    or a\n    jr z, DONE       ; weight=0: skip entirely\n    jp m, NEG        ; weight<0",
      "default_branch": "main"
    },
    "fetched_at": "2026-01-11T03:04:30.925325"
  },
  {
    "basic_info": {
      "name": "autocoder",
      "full_name": "leonvanzyl/autocoder",
      "owner": "leonvanzyl",
      "description": null,
      "url": "https://github.com/leonvanzyl/autocoder",
      "clone_url": "https://github.com/leonvanzyl/autocoder.git",
      "ssh_url": "git@github.com:leonvanzyl/autocoder.git",
      "homepage": null,
      "created_at": "2025-12-30T17:01:32Z",
      "updated_at": "2026-01-11T02:52:06Z",
      "pushed_at": "2026-01-10T18:27:25Z"
    },
    "stats": {
      "stars": 921,
      "forks": 193,
      "watchers": 921,
      "open_issues": 29,
      "size": 241
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 307524,
        "TypeScript": 226096,
        "CSS": 11000,
        "Shell": 5068,
        "Batchfile": 2255,
        "JavaScript": 734,
        "HTML": 694
      },
      "license": "Other",
      "topics": []
    },
    "content": {
      "readme": "# AutoCoder\n\n[![Buy Me A Coffee](https://img.shields.io/badge/Buy%20Me%20A%20Coffee-FFDD00?style=flat&logo=buy-me-a-coffee&logoColor=black)](https://www.buymeacoffee.com/leonvanzyl)\n\nA long-running autonomous coding agent powered by the Claude Agent SDK. This tool can build complete applications over multiple sessions using a two-agent pattern (initializer + coding agent). Includes a React-based UI for monitoring progress in real-time.\n\n## Video Tutorial\n\n[![Watch the tutorial](https://img.youtube.com/vi/lGWFlpffWk4/hqdefault.jpg)](https://youtu.be/lGWFlpffWk4)\n\n> **[Watch the setup and usage guide â†’](https://youtu.be/lGWFlpffWk4)**\n\n---\n\n## Prerequisites\n\n### Claude Code CLI (Required)\n\nThis project requires the Claude Code CLI to be installed. Install it using one of these methods:\n\n**macOS / Linux:**\n```bash\ncurl -fsSL https://claude.ai/install.sh | bash\n```\n\n**Windows (PowerShell):**\n```powershell\nirm https://claude.ai/install.ps1 | iex\n```\n\n### Authentication\n\nYou need one of the following:\n\n- **Claude Pro/Max Subscription** - Use `claude login` to authenticate (recommended)\n- **Anthropic API Key** - Pay-per-use from https://console.anthropic.com/\n\n---\n\n## Quick Start\n\n### Option 1: Web UI (Recommended)\n\n**Windows:**\n```cmd\nstart_ui.bat\n```\n\n**macOS / Linux:**\n```bash\n./start_ui.sh\n```\n\nThis launches the React-based web UI at `http://localhost:5173` with:\n- Project selection and creation\n- Kanban board view of features\n- Real-time agent output streaming\n- Start/pause/stop controls\n\n### Option 2: CLI Mode\n\n**Windows:**\n```cmd\nstart.bat\n```\n\n**macOS / Linux:**\n```bash\n./start.sh\n```\n\nThe start script will:\n1. Check if Claude CLI is installed\n2. Check if you're authenticated (prompt to run `claude login` if not)\n3. Create a Python virtual environment\n4. Install dependencies\n5. Launch the main menu\n\n### Creating or Continuing a Project\n\nYou'll see options to:\n- **Create new project** - Start a fresh project with AI-assisted spec generation\n- **Continue existing project** - Resume work on a previous project\n\nFor new projects, you can use the built-in `/create-spec` command to interactively create your app specification with Claude's help.\n\n---\n\n## How It Works\n\n### Two-Agent Pattern\n\n1. **Initializer Agent (First Session):** Reads your app specification, creates features in a SQLite database (`features.db`), sets up the project structure, and initializes git.\n\n2. **Coding Agent (Subsequent Sessions):** Picks up where the previous session left off, implements features one by one, and marks them as passing in the database.\n\n### Feature Management\n\nFeatures are stored in SQLite via SQLAlchemy and managed through an MCP server that exposes tools to the agent:\n- `feature_get_stats` - Progress statistics\n- `feature_get_next` - Get highest-priority pending feature\n- `feature_get_for_regression` - Random passing features for regression testing\n- `feature_mark_passing` - Mark feature complete\n- `feature_skip` - Move feature to end of queue\n- `feature_create_bulk` - Initialize all features (used by initializer)\n\n### Session Management\n\n- Each session runs with a fresh context window\n- Progress is persisted via SQLite database and git commits\n- The agent auto-continues between sessions (3 second delay)\n- Press `Ctrl+C` to pause; run the start script again to resume\n\n---\n\n## Important Timing Expectations\n\n> **Note: Building complete applications takes time!**\n\n- **First session (initialization):** The agent generates feature test cases. This takes several minutes and may appear to hang - this is normal.\n\n- **Subsequent sessions:** Each coding iteration can take **5-15 minutes** depending on complexity.\n\n- **Full app:** Building all features typically requires **many hours** of total runtime across multiple sessions.\n\n**Tip:** The feature count in the prompts determines scope. For faster demos, you can modify your app spec to target fewer features (e.g., 20-50 features for a quick demo).\n\n---\n\n## Project Structure\n\n```\nautonomous-coding/\nâ”œâ”€â”€ start.bat                 # Windows CLI start script\nâ”œâ”€â”€ start.sh                  # macOS/Linux CLI start script\nâ”œâ”€â”€ start_ui.bat              # Windows Web UI start script\nâ”œâ”€â”€ start_ui.sh               # macOS/Linux Web UI start script\nâ”œâ”€â”€ start.py                  # CLI menu and project management\nâ”œâ”€â”€ start_ui.py               # Web UI backend (FastAPI server launcher)\nâ”œâ”€â”€ autonomous_agent_demo.py  # Agent entry point\nâ”œâ”€â”€ agent.py                  # Agent session logic\nâ”œâ”€â”€ client.py                 # Claude SDK client configuration\nâ”œâ”€â”€ security.py               # Bash command allowlist and validation\nâ”œâ”€â”€ progress.py               # Progress tracking utilities\nâ”œâ”€â”€ prompts.py                # Prompt loading utilities\nâ”œâ”€â”€ api/\nâ”‚   â””â”€â”€ database.py           # SQLAlchemy models (Feature table)\nâ”œâ”€â”€ mcp_server/\nâ”‚   â””â”€â”€ feature_mcp.py        # MCP server for feature management tools\nâ”œâ”€â”€ server/\nâ”‚   â”œâ”€â”€ main.py               # FastAPI REST API server\nâ”‚   â”œâ”€â”€ websocket.py          # WebSoc",
      "default_branch": "master"
    },
    "fetched_at": "2026-01-11T03:04:32.047570"
  },
  {
    "basic_info": {
      "name": "clash-ip-checker",
      "full_name": "tombcato/clash-ip-checker",
      "owner": "tombcato",
      "description": "ğŸ¤–Automated node IP risk checking and tagging tool for Clash (é€‚ç”¨äº Clash ä»£ç†çš„è‡ªåŠ¨åŒ–èŠ‚ç‚¹ IP é£é™©æ£€æŸ¥å¹¶é‡å‘½åæ ‡è®°å·¥å…·)",
      "url": "https://github.com/tombcato/clash-ip-checker",
      "clone_url": "https://github.com/tombcato/clash-ip-checker.git",
      "ssh_url": "git@github.com:tombcato/clash-ip-checker.git",
      "homepage": "https://tombcato.github.io/clash-ip-checker/",
      "created_at": "2025-12-12T17:41:11Z",
      "updated_at": "2026-01-10T17:33:40Z",
      "pushed_at": "2026-01-10T17:41:06Z"
    },
    "stats": {
      "stars": 884,
      "forks": 57,
      "watchers": 884,
      "open_issues": 1,
      "size": 2331
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 46341,
        "JavaScript": 15315,
        "HTML": 11740,
        "CSS": 8198
      },
      "license": "GNU General Public License v3.0",
      "topics": [
        "automation",
        "clash",
        "clash-verge",
        "ippure",
        "iprisk",
        "playwright",
        "python"
      ]
    },
    "content": {
      "readme": "# ğŸš€ Clash Node IP CHECKER\n\n[ä¸­æ–‡](README.md) | [English](README_EN.md) | [å®˜ç½‘](https://tombcato.github.io/clash-ip-checker/) | [Dockeréƒ¨ç½²](https://github.com/tombcato/clash-ip-checker/tree/docker)\n\n![Python](https://img.shields.io/badge/Python-3.10%2B-blue)\n[![Twitter](https://img.shields.io/badge/Twitter-%40hibearss-1DA1F2?style=flat&logo=twitter&logoColor=white)](https://x.com/hibearss)\n[![zread](https://img.shields.io/badge/Ask_Zread-_.svg?style=flat&color=00b0aa&labelColor=000000&logo=data%3Aimage%2Fsvg%2Bxml%3Bbase64%2CPHN2ZyB3aWR0aD0iMTYiIGhlaWdodD0iMTYiIHZpZXdCb3g9IjAgMCAxNiAxNiIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KPHBhdGggZD0iTTQuOTYxNTYgMS42MDAxSDIuMjQxNTZDMS44ODgxIDEuNjAwMSAxLjYwMTU2IDEuODg2NjQgMS42MDE1NiAyLjI0MDFWNC45NjAxQzEuNjAxNTYgNS4zMTM1NiAxLjg4ODEgNS42MDAxIDIuMjQxNTYgNS42MDAxSDQuOTYxNTZDNS4zMTUwMiA1LjYwMDEgNS42MDE1NiA1LjMxMzU2IDUuNjAxNTYgNC45NjAxVjIuMjQwMUM1LjYwMTU2IDEuODg2NjQgNS4zMTUwMiAxLjYwMDEgNC45NjE1NiAxLjYwMDFaIiBmaWxsPSIjZmZmIi8%2BCjxwYXRoIGQ9Ik00Ljk2MTU2IDEwLjM5OTlIMi4yNDE1NkMxLjg4ODEgMTAuMzk5OSAxLjYwMTU2IDEwLjY4NjQgMS42MDE1NiAxMS4wMzk5VjEzLjc1OTlDMS42MDE1NiAxNC4xMTM0IDEuODg4MSAxNC4zOTk5IDIuMjQxNTYgMTQuMzk5OUg0Ljk2MTU2QzUuMzE1MDIgMTQuMzk9OSA1LjYwMTU2IDE0LjExMzQgNS42MDE1NiAxMy43NTk5VjExLjAzOTlDNS42MDE1NiAxMC42ODY0IDUuMzE1MDIgMTAuMzk5OSA0Ljk2MTU2IDEwLjM5OTlaIiBmaWxsPSIjZmZmIi8%2BCjxwYXRoIGQ9Ik0xMy43NTg0IDEuNjAwMUgxMS4wMzg0QzEwLjY4NSAxLjYwMDEgMTAuMzk4NCAxLjg4NjY0IDEwLjM5ODQgMi4yNDAxVjQuOTYwMUMxMC4zOTg0IDUuMzEzNTYgMTAuNjg1IDUuNjAwMSAxMS4wMzg0IDUuNjAwMUgxMy43NTg0QzE0LjExMTkgNS42MDAxIDE0LjM5ODQgNS4zMTM1NiAxNC4zOTk4IDQuOTYwMVYyLjI0MDFDMTQuMzk4NCAxLjg4NjY0IDE0LjExMTkgMS42MDAxIDEzLjc1ODQgMS42MDAxWiIgZmlsbD0iI2ZmZiIvPgo8cGF0aCBkPSJNNCAxMkwxMiA0TDQgMTJaIiBmaWxsPSIjZmZmIi8%2BCjxwYXRoIGQ9Ik00IDEyTDEyIDQiIHN0cm9rZT0iI2ZmZiIgc3Ryb2tlLXdpZHRoPSIxLjUiIHN0cm9rZS1saW5lY2FwPSJyb3VuZCIvPgo8L3N2Zz4K&logoColor=ffffff)](https://zread.ai/tombcato/clash-ip-checker)\n\n\n\n\nä¸€ä¸ªé’ˆå¯¹ **Clash** (åŠå…¼å®¹æ ¸å¿ƒ) çš„è‡ªåŠ¨åŒ–èŠ‚ç‚¹å·¥å…·ã€‚å®ƒä¼šè‡ªåŠ¨éå†ä½ çš„ä»£ç†èŠ‚ç‚¹ï¼Œé€šè¿‡ [IPPure](https://ippure.com/)æˆ–è€…[Ping0](https://ping0.cc/) æ£€æµ‹ IP çº¯å‡€åº¦å’Œç›¸å…³å±æ€§ï¼Œå¹¶é‡å‘½åèŠ‚ç‚¹ï¼Œæ·»åŠ å®ç”¨çš„æŒ‡æ ‡ï¼ˆIP çº¯å‡€åº¦ã€Bot æ¯”ä¾‹(æˆ–å…±äº«äººæ•°)ã€IPå±æ€§/IPæ¥æºçŠ¶æ€ï¼‰`ã€ğŸŸ¢ğŸŸ¡ ä½å®…|åŸç”Ÿã€‘`ã€‚\næ•ˆæœå±•ç¤ºï¼š\n![å›¾ç‰‡æè¿°](assets/clash-node-checked.png)\nWebå¯è§†åŒ–é…ç½®æ£€æµ‹ï¼š\n![alt text](assets/clash-web-check.png)\n## ğŸ“… æ›´æ–°æ—¥å¿— (Changelog)\n\n### v2.0.0 (2025-01-11)\n- **Web UI**: å…¨æ–°æ¨å‡º Web å¯è§†åŒ–ç•Œé¢ï¼Œæ“ä½œæ›´ä¾¿æ·ã€‚\n- **å¤šæºæ£€æµ‹**: æ–°å¢ `Ping0` æ£€æµ‹æº æ”¯æŒå…±äº«äººæ•°ï¼Œä¸ `ippure` äº’è¡¥ï¼Œå¹¶è®¾ä¸ºé»˜è®¤ï¼ˆé€Ÿåº¦ä¸ä¿¡æ¯é‡å¹³è¡¡æ›´ä½³ï¼‰ã€‚\n- **æ™ºèƒ½é™çº§**: æ–°å¢ `Fallback` æœºåˆ¶ï¼Œä¾‹å¦‚ï¼šPing0 å¤±è´¥æ—¶è‡ªåŠ¨åˆ‡æ¢è‡³ IPPureã€‚\n- **æé€Ÿé»˜è®¤**: æé€Ÿæ¨¡å¼ (`fast_mode`) é»˜è®¤å¼€å¯ï¼Œå¤§å¹…æå‡æ‰¹é‡æ£€æµ‹æ•ˆç‡ã€‚\n- **å•ç‚¹é‡æµ‹**: Web ç•Œé¢æ”¯æŒå¯¹å•ä¸ªèŠ‚ç‚¹è¿›è¡Œé‡æ–°æ£€æµ‹ï¼Œæ–¹ä¾¿å¤æ ¸ã€‚\n- **å¯¼å‡ºå¢å¼º**: æ”¯æŒæ£€æµ‹ç»“æœçš„å®æ—¶é¢„è§ˆã€ç¼–è¾‘å’Œä¸€é”®å¯¼å‡ºï¼Œä¸€é”®å¯¼å…¥Clash\n- **ä½“éªŒä¼˜åŒ–**: è‡ªåŠ¨æ¸…ç† IP ç¼“å­˜ï¼Œé˜²æ­¢ç»“æœæ®‹ç•™ï¼›ä¼˜åŒ–äº†ç«¯å£æ£€æµ‹å’Œå†²çªå¤„ç†ã€‚\n\n\n## âœ¨ åŠŸèƒ½ç‰¹ç‚¹\n\n- **ğŸ–¥ï¸ Web å¯è§†åŒ–ç•Œé¢ (æ–°!)**: æä¾›ç°ä»£åŒ–çš„ Web ç•Œé¢ï¼Œæ”¯æŒå¯è§†åŒ–é…ç½®ï¼Œæ£€æµ‹å¯æŸ¥çœ‹å®æ—¶è¿›åº¦æ˜¾ç¤ºã€å•ç‚¹é‡æµ‹ã€ç»“æœç¼–è¾‘å’Œå¯¼å‡ºé¢„è§ˆï¼Œæ”¯æŒä¸€é”®è·³è½¬å¯¼å…¥Clashã€‚\n- **æé€Ÿæ¨¡å¼**: é»˜è®¤ **å¼€å¯**ï¼Œé€šè¿‡ IPPure API æˆ–è€… Ping0 ç›´æ¥æ£€æµ‹ï¼Œé€Ÿåº¦æ¯”æµè§ˆå™¨æ¨¡å¼æ›´å¿«ï¼å¯åœ¨config.yamlä¸­è®¾ç½®`fast_mode = False`å…³é—­ã€‚\n- **æé€Ÿæ¨¡å¼å¤šæ•°æ®æºæ”¯æŒ**: æ”¯æŒ `ping0` (é»˜è®¤) å’Œ `ippure` ä¸¤ç§æ£€æµ‹æºï¼Œæ”¯æŒè‡ªåŠ¨é™çº§ (Fallback) æœºåˆ¶ï¼Œå½“ ping0 å¤±è´¥æ—¶è‡ªåŠ¨åˆ‡æ¢åˆ° ippureã€‚**ippureç¼ºå°‘ Bot æ¯”ä¾‹åˆ†æï¼ŒPing0æœ‰å…±äº«äººæ•°æ•°æ®**\n- **è‡ªåŠ¨åˆ‡æ¢**: è‡ªåŠ¨éå†å¹¶åˆ‡æ¢ä½ çš„ Clash ä»£ç†èŠ‚ç‚¹ã€‚\n- **æ·±åº¦ IP åˆ†æ**: æ£€æµ‹ IP çº¯å‡€åº¦åˆ†æ•°ã€Bot æ¯”ä¾‹ã€IP å±æ€§ (åŸç”Ÿ/æœºæˆ¿) ä»¥åŠå½’å±åœ°ã€‚\n- **é«˜æ‹ŸçœŸæ£€æµ‹ (å¯é€‰)**: åœ¨æµè§ˆå™¨æ¨¡å¼ä¸‹ä½¿ç”¨ **Playwright** è¿›è¡Œé«˜æ‹ŸçœŸæ£€æµ‹ï¼ŒåŒ…å« Bot æ¯”ä¾‹åˆ†æã€‚æ”¯æŒæ— å¤´æ¨¡å¼ (Headless) é…ç½®ã€‚\n- **æ™ºèƒ½è¿‡æ»¤**: è‡ªåŠ¨è·³è¿‡æ— æ•ˆèŠ‚ç‚¹ (å¦‚ \"åˆ°æœŸ\", \"æµé‡é‡ç½®\", \"å®˜ç½‘\" ç­‰)ã€‚\n- **é…ç½®æ³¨å…¥**: ç”Ÿæˆä¸€ä¸ªæ–°çš„ Clash é…ç½®æ–‡ä»¶ (`_checked.yaml`)ï¼Œåœ¨èŠ‚ç‚¹åç§°åè¿½åŠ  Emoji å’ŒçŠ¶æ€ä¿¡æ¯ã€‚\n- **å¼ºåˆ¶å…¨å±€æ¨¡å¼**: ä¸´æ—¶å°† Clash å¼ºåˆ¶åˆ‡æ¢ä¸ºå…¨å±€æ¨¡å¼ä»¥ç¡®ä¿æµ‹è¯•å‡†ç¡®æ€§ã€‚\n\n##  âš¡æ–°å¢Dockeréƒ¨ç½² [è¯¦æƒ…è§Dockeråˆ†æ”¯](https://github.com/tombcato/clash-ip-checker/tree/docker)\nç›¸å¯¹äºä¸»åˆ†æ”¯è€Œè¨€ï¼ŒDockeréƒ¨ç½²åä»£ç†åˆ‡æ¢ä¸å½±å“æœ¬åœ°ç½‘ç»œï¼ˆéƒ¨ç½²NASæˆ–è€…äº‘æœåŠ¡å™¨ï¼‰ï¼Œä¸”èƒ½ç›´æ¥è¾“å…¥è®¢é˜…é“¾æ¥è¾“å‡ºæ–°è®¢é˜…é“¾æ¥ï¼Œæ²¡æœ‰ç¹ççš„ä½¿ç”¨æ­¥éª¤ï¼Œ**ä¸€é”®æ›¿æ¢è®¢é˜…urlæ£€æµ‹ï¼**\n**äº‘éƒ¨ç½²Demoåœ°å€ï¼šhttps://tombcat.space/ipcheck** \n\n## ğŸ› ï¸ å‰ç½®è¦æ±‚\n\n- **Python 3.10+**\n- **Clash Verge** (æˆ–å…¶ä»–å¼€å¯äº† External Controller çš„ Clash å®¢æˆ·ç«¯)\n\n## ğŸ“¦ å®‰è£…è¯´æ˜\n\n1.  **å…‹éš†ä»“åº“**\n    ```bash\n    git clone git@github.com:tombcato/clash-ip-checker.git\n    cd clash-ip-checker\n    ```\n\n2.  **å®‰è£…ä¾èµ–**\n    ```bash\n    pip install -r requirements.txt\n    # éæé€Ÿæ¨¡å¼éœ€è¦\n    playwright install chromium\n    # å¦‚æœ install chromium è¿è¡Œå¤±è´¥è¯´æ˜ playwright æ²¡æ·»åŠ ç¯å¢ƒå˜é‡ï¼Œå¯ä»¥ç”¨ï¼š\n    # python -m playwright install chromium\n    ```\n\n3.  **å¯åŠ¨ Web ç•Œé¢ (æ–°ç‰ˆæ¨è)**\n    ```bash\n    python web.py\n    ```\n    è®¿é—® http://127.0.0.1:8080 å³å¯ä½¿ç”¨å›¾å½¢åŒ–ç•Œé¢è¿›è¡Œé…ç½®å’Œæ£€æµ‹ã€‚\n    \n4.  **å‘½ä»¤è¡Œæ¨¡å¼ (æ—§ç‰ˆ)**\n    - ä¿®æ”¹ `config.yaml.example` åˆ é™¤åç¼€é‡å‘½åä¸º `config.yaml`ã€‚\n    - ç¼–è¾‘ `config.yaml` å¡«å…¥é…ç½®ï¼ˆWeb ç•Œé¢ä¸­ä¹Ÿå¯ç›´æ¥è®¾ç½®ï¼‰ï¼š\n        - `yaml_path`: ä½ çš„ Clash é…ç½®æ–‡ä»¶ (**.yaml**) çš„ç»å¯¹è·¯å¾„ã€‚\n        - `clash_api_secret`: ä½ çš„ API å¯†é’¥ (å¦‚æœæœ‰çš„è¯)ã€‚\n        - `fast_mode`: âš¡ æ˜¯å¦ä½¿ç”¨æé€Ÿæ¨¡å¼ (True/False)ã€‚\n        - `source`: æ£€æµ‹æºï¼Œå¯é€‰ `ping0` æˆ– `ippure` (é»˜è®¤ ping0)ã€‚\n        - `fallback`: æ˜¯å¦å¼€å¯è‡ªåŠ¨é™çº§ (True/False)ã€‚\n    - è¿è¡Œ `python clash_automator.py`ã€‚\n\n## Web ç•Œé¢ä½¿ç”¨æ–¹æ³• (æ–°ç‰ˆæ¨è)\n1. æ‰“å¼€ä½ çš„ Clash å®¢æˆ·ç«¯ (ä¾‹å¦‚ Clash Verge) å°†å½“å‰clashæ­£åœ¨è¿è¡Œçš„è®¢é˜…é…ç½®æ–‡ä»¶åˆ‡æ¢ä¸ºä½ æƒ³è¦æµ‹è¯•çš„è®¢é˜…ï¼Œç‚¹å‡»è·å–YAMLæºä»£ç ï¼Œç„¶åå¤åˆ¶ç²˜è´´è¿›[webç•Œé¢](http://127.0.0.1:8080)çš„yamlæ¡†ä¸­\n    ![alt text](assets/clash-open-yaml-code.png)\n2. ç¡®ä¿Clashä¸­ External Controller (å¤–éƒ¨æ§åˆ¶) å·²åœ¨è®¾ç½®ä¸­å¼€å¯ï¼Œå¯†ç éšä¾¿è®¾ç½®, ç„¶åå†Webç•Œé¢ä¸­é…ç½®\n![alt text](assets/clash-controller.png)\n3. ä½¿ç”¨é»˜è®¤é…ç½®ç›´æ¥ç‚¹å‡»å¼€å§‹å³å¯ï¼Œæ£€æµ‹å®Œæˆåä¼šå¯ä¸€é”®é¢„è§ˆå¯¼å…¥Clash\n![alt text](assets/clash-web-check.png)\n\n## å‘½ä»¤è¡Œæ¨¡å¼ä½¿ç”¨æ–¹æ³•ï¼ˆæ—§ç‰ˆï¼‰\n\n1.  æ‰“å¼€ä½ çš„ Clash å®¢æˆ·ç«¯ (ä¾‹å¦‚ Clash Verge) å°†å½“å‰clashæ­£åœ¨è¿è¡Œçš„è®¢é˜…é…ç½®æ–‡ä»¶åˆ‡æ¢ä¸ºä½ æƒ³è¦æµ‹è¯•çš„è®¢é˜…ï¼Œ ç„¶åè·å–è¯¥é…ç½®æ–‡ä»¶çš„yamlæ–‡ä»¶ç»å¯¹è·¯å¾„, åœ¨config.yamlä¸­é…ç½®yaml_path.\n    å³é”®é…ç½®æ–‡ä»¶é€‰æ‹©æ‰“å¼€æ–‡ä»¶\n    ![](assets/clash-open-yaml.png",
      "default_branch": "main"
    },
    "fetched_at": "2026-01-11T03:04:33.208503"
  },
  {
    "basic_info": {
      "name": "rlm",
      "full_name": "alexzhang13/rlm",
      "owner": "alexzhang13",
      "description": "General plug-and-play inference library for Recursive Language Models (RLMs), supporting various sandboxes.",
      "url": "https://github.com/alexzhang13/rlm",
      "clone_url": "https://github.com/alexzhang13/rlm.git",
      "ssh_url": "git@github.com:alexzhang13/rlm.git",
      "homepage": "https://arxiv.org/abs/2512.24601v1",
      "created_at": "2025-12-20T23:12:43Z",
      "updated_at": "2026-01-11T02:40:00Z",
      "pushed_at": "2026-01-10T06:19:48Z"
    },
    "stats": {
      "stars": 871,
      "forks": 147,
      "watchers": 871,
      "open_issues": 21,
      "size": 2823
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 239068,
        "Makefile": 1613
      },
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "\n---\n\n<h1 align=\"center\" style=\"font-size:2.8em\">\n<span>Recursive Language Models (<span style=\"color:orange\">RLM</span>s)</span>\n</h1>\n\n<p align=\"center\" style=\"font-size:1.3em\">\n  <a href=\"https://arxiv.org/abs/2512.24601\">Full Paper</a> â€¢\n  <a href=\"https://alexzhang13.github.io/blog/2025/rlm/\">Blogpost</a> â€¢\n  <a href=\"https://alexzhang13.github.io/rlm/\">Documentation</a> â€¢\n  <a href=\"https://github.com/alexzhang13/rlm-minimal\">RLM Minimal</a>\n</p>\n\n<p align=\"center\">\n  <a href=\"https://github.com/alexzhang13/rlm/actions/workflows/style.yml\">\n    <img src=\"https://github.com/alexzhang13/rlm/actions/workflows/style.yml/badge.svg\" alt=\"Style\" />\n  </a>\n  <a href=\"https://github.com/alexzhang13/rlm/actions/workflows/test.yml\">\n    <img src=\"https://github.com/alexzhang13/rlm/actions/workflows/test.yml/badge.svg\" alt=\"Test\" />\n  </a>\n</p>\n\n<p align=\"center\">\n  <a href=\"https://arxiv.org/abs/2512.24601\">\n    <img src=\"media/paper_preview.png\" alt=\"Paper Preview\" width=\"300\"/>\n  </a>\n</p>\n\n## Overview\nRecursive Language Models (RLMs) are a task-agnostic inference paradigm for language models (LMs) to handle near-infinite length contexts by enabling the LM to *programmatically* examine, decompose, and recursively call itself over its input. RLMs replace the canonical `llm.completion(prompt, model)` call with a `rlm.completion(prompt, model)` call. RLMs offload the context as a variable in a REPL environment that the LM can interact with and launch sub-LM calls inside of.\n\nThis repository provides an extensible inference engine for using RLMs around standard API-based and local LLMs. The initial experiments and idea were proposed in a [blogpost](https://alexzhang13.github.io/blog/2025/rlm/) in 2025, with expanded results in an [arXiv preprint](https://arxiv.org/abs/2512.24601).\n\n> [!NOTE]\n> This repository contains inference code for RLMs with support for various sandbox environments. Open-source contributions are welcome. This repository is maintained by the authors of the paper from the MIT OASYS lab.\n\n<!-- ## Installation\n```\npip install rlm\n```\nTo install the latest from `main`:\n```\npip install git+https://github.com/alexzhang13/rlm.git\n```\n``` -->\n\n## Quick Setup\nSet up the dependencies with `uv` (or your virtual environment of choice):\n```bash\ncurl -LsSf https://astral.sh/uv/install.sh | sh\nuv init && uv venv --python 3.12  # change version as needed\nuv pip install -e .\n```\n\nThis project includes a `Makefile` to simplify common tasks.\n\n- `make install`: Install base dependencies.\n- `make check`: Run linter, formatter, and tests.\n\nTo run a quick test, the following will run an RLM query with the OpenAI client using your environment variable `OPENAI_API_KEY` (feel free to change this). This will generate console output as well as a log which you can use with the visualizer to explore the trajectories.\n```bash\nmake quickstart\n```\n\nThe default RLM client uses a REPL environment that runs on the host process through Python `exec` calls. It uses the same virtual environment as the host process (i.e. it will have access to the same dependencies), but with some limitations in its available global modules. As an example, we can call RLM completions using GPT-5-nano:\n```python\nfrom rlm import RLM\n\nrlm = RLM(\n    backend=\"openai\",\n    backend_kwargs={\"model_name\": \"gpt-5-nano\"},\n    verbose=True,  # For printing to console with rich, disabled by default.\n)\n\nprint(rlm.completion(\"Print me the first 100 powers of two, each on a newline.\").response)\n```\n\n## REPL Environments\nWe support two types of REPL environments -- isolated, and non-isolated. Non-isolated environments (default) run code execution on the same machine as the RLM (e.g. through `exec`), which is pretty reasonable for some local low-risk tasks, like simple benchmarking, but can be problematic if the prompts or tool calls can interact with malicious users. Fully isolated environments used Cloud-based sandboxes (e.g. Prime Sandboxes, [Modal Sandboxes](https://modal.com/docs/guide/sandboxes)) to run code generated by the RLM, ensuring completely isolation from the host process. Environments can be added, but we natively support the following: `local` (default), `modal`, `prime`.\n\n```python\nrlm = RLM(\n    environment=\"...\", # \"local\", \"docker\", \"modal\", \"prime\"\n    environment_kwargs={...},\n)\n```\n\n### Local Environments\nThe default `local` environment `LocalREPL` runs in the same process as the RLM itself, with specified global and local namespaces for minimal security. Using this REPL is generally safe, but should not be used for production settings. It also shares the same virtual environment (e.g. Conda or uv) as the host process.\n\n#### Docker <img src=\"https://github.com/docker.png\" alt=\"Docker\" height=\"20\" style=\"vertical-align: middle;\"/> (*requires [Docker installed](https://docs.docker.com/desktop/setup/install/)*)\nWe also support a Docker-based environment called `DockerREPL` that launches the REPL environment as a Docker image. By default, we use ",
      "default_branch": "main"
    },
    "fetched_at": "2026-01-11T03:04:34.346062"
  },
  {
    "basic_info": {
      "name": "JSAnalyzer",
      "full_name": "jenish-sojitra/JSAnalyzer",
      "owner": "jenish-sojitra",
      "description": null,
      "url": "https://github.com/jenish-sojitra/JSAnalyzer",
      "clone_url": "https://github.com/jenish-sojitra/JSAnalyzer.git",
      "ssh_url": "git@github.com:jenish-sojitra/JSAnalyzer.git",
      "homepage": null,
      "created_at": "2026-01-03T23:33:27Z",
      "updated_at": "2026-01-11T01:23:23Z",
      "pushed_at": "2026-01-06T16:59:40Z"
    },
    "stats": {
      "stars": 794,
      "forks": 135,
      "watchers": 794,
      "open_issues": 3,
      "size": 41
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 28246
      },
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "# JS Analyzer - Burp Suite Extension by Jensec (https://x.com/_jensec)\n\nA powerful Burp Suite extension for JavaScript static analysis. Extracts API endpoints, URLs, secrets, and email addresses from JavaScript files with intelligent noise filtering. The goal is reduce noise as much as possible to ensure the accuracy\n\n![Burp Suite](https://img.shields.io/badge/Burp%20Suite-Extension-orange)\n![Python](https://img.shields.io/badge/Python-Jython%202.7-blue)\n![License](https://img.shields.io/badge/License-MIT-green)\n\n## Features\n\n- **Endpoint Detection** - Finds API paths, REST endpoints, OAuth URLs, admin routes\n- **URL Extraction** - Extracts full URLs including cloud storage (AWS S3, Azure, GCP)\n- **Secret Scanning** - Detects API keys, tokens, credentials (AWS, Stripe, GitHub, Slack, JWT, etc.)\n- **Email Extraction** - Finds email addresses in JS code\n- **File Detection** - Detects references to sensitive files (.sql, .csv, .bak, .env, .pdf, etc.)\n- **Smart Filtering** - Removes noise from XML namespaces, module imports, build artifacts\n- **Source Tracking** - Shows which JS file each finding came from\n- **Live Search** - Filter results in real-time\n- **Copy Function** - Copy individual or all findings to clipboard\n- **JSON Export** - Export all findings to JSON file\n\n## Installation\n\n1. Download [Jython standalone JAR](https://www.jython.org/download)\n2. In Burp Suite: `Extensions > Extensions-Settings > Python Environment`\n3. Set the Jython JAR path\n4. `Extensions > Installed > Add`\n5. Select `Python` and browse to `js_analyzer.py`\n\n## Usage\n\n1. **Browse** websites with your browser proxied through Burp Suite\n2. **Right-click** on any raw(s) containing JS response in (either of following tabs):\n   - Proxy > HTTP history\n   - Target > Site map\n   - Repeater\n3. Select **\"Analyze JS with JS Analyzer\"**\n4. Check the **JS Analyzer** tab for results\n\nYou can select multiple requests from HTTP history or Dashboard and send it all together to JS Analayzer.\n\n## What It Detects\n\n### Endpoints\n| Pattern | Example |\n|---------|---------|\n| API paths | `/api/v1/users`, `/api/v2/auth` |\n| REST endpoints | `/rest/data`, `/graphql` |\n| OAuth/Auth | `/oauth2/token`, `/auth/login`, `/callback` |\n| Admin routes | `/admin`, `/dashboard`, `/internal` |\n| Well-known | `/.well-known/openid-configuration` |\n\n### Secrets\n| Type | Pattern |\n|------|---------|\n| AWS Access Key | `AKIA[0-9A-Z]{16}` |\n| Google API Key | `AIza[0-9A-Za-z\\-_]{35}` |\n| Stripe Live Key | `sk_live_[0-9a-zA-Z]{24,}` |\n| GitHub PAT | `ghp_[0-9a-zA-Z]{36}` |\n| Slack Token | `xox[baprs]-...` |\n| JWT | `eyJ...` |\n| Private Keys | `-----BEGIN PRIVATE KEY-----` |\n| Database URLs | `mongodb://`, `postgres://`, `mysql://` |\n\n#Note: Feel free to fork and add more secrets detections as required. \n\n### Noise Filtering\nThe extension automatically filters out:\n- XML namespaces (`schemas.openxmlformats.org`, `www.w3.org`)\n- Module imports (`./`, `../`, `@angular/`, etc.)\n- PDF internal paths (`/Type`, `/Font`, `/Filter`)\n- Excel/XML paths (`xl/`, `docProps/`, `worksheets/`)\n- Locale files (`en.js`, `fr-ca.js`)\n- Crypto library internals (`sha.js`, `aes`, `bn.js`)\n\n### Files\nDetects references to sensitive file types:\n| Category | Extensions |\n|----------|------------|\n| Data | `.sql`, `.csv`, `.xlsx`, `.json`, `.xml`, `.yaml` |\n| Config | `.env`, `.conf`, `.ini`, `.cfg`, `.config` |\n| Backup | `.bak`, `.backup`, `.old`, `.orig` |\n| Certs | `.key`, `.pem`, `.crt`, `.p12`, `.pfx` |\n| Docs | `.pdf`, `.doc`, `.docx` |\n| Archives | `.zip`, `.tar`, `.gz` |\n| Scripts | `.sh`, `.bat`, `.ps1`, `.py` |\n\n## Standalone Engine\n\nFor use in your own Python projects or APIs:\n\n```python\nfrom js_analyzer_engine import JSAnalyzerEngine\n\nengine = JSAnalyzerEngine()\nresults = engine.analyze(javascript_content)\n\nprint(results[\"endpoints\"])  # ['/api/v1/users', ...]\nprint(results[\"urls\"])       # ['https://api.example.com', ...]\nprint(results[\"secrets\"])    # [{'type': 'AWS Key', 'value': '...', 'masked': '...'}, ...]\nprint(results[\"emails\"])     # ['admin@company.com', ...]\n```\n\n### Flask API Example\n\n```python\nfrom flask import Flask, request, jsonify\nfrom js_analyzer_engine import JSAnalyzerEngine\n\napp = Flask(__name__)\nengine = JSAnalyzerEngine()\n\n@app.route('/analyze', methods=['POST'])\ndef analyze():\n    content = request.json.get('content', '')\n    results = engine.analyze(content)\n    return jsonify(results)\n\nif __name__ == '__main__':\n    app.run(port=5000)\n```\n\n## File Structure\n\n```\nJSextension/\nâ”œâ”€â”€ js_analyzer.py          # Main Burp extension entry point\nâ”œâ”€â”€ ui/\nâ”‚   â”œâ”€â”€ __init__.py\nâ”‚   â””â”€â”€ results_panel.py    # Burp UI panel\nâ”œâ”€â”€ README.md\nâ””â”€â”€ LICENSE\n```\n\n## Contributing\n\nContributions are welcome! Feel free to:\n- Add new secret patterns\n- Improve noise filtering\n- Add new endpoint patterns\n- Report bugs or issues\n\n## License\n\nMIT License - see [LICENSE](LICENSE) file.\n\n## Credits\n\nInspired by: \n- [LinkFinder](https://github.com/GerbenJavado/LinkFinder) - Endpoint detection regex\n",
      "default_branch": "main"
    },
    "fetched_at": "2026-01-11T03:04:35.496357"
  },
  {
    "basic_info": {
      "name": "smtp-tunnel-proxy",
      "full_name": "x011/smtp-tunnel-proxy",
      "owner": "x011",
      "description": "A high-speed covert tunnel that disguises TCP traffic as SMTP email communication to bypass Deep Packet Inspection (DPI) firewalls.",
      "url": "https://github.com/x011/smtp-tunnel-proxy",
      "clone_url": "https://github.com/x011/smtp-tunnel-proxy.git",
      "ssh_url": "git@github.com:x011/smtp-tunnel-proxy.git",
      "homepage": "",
      "created_at": "2026-01-06T19:38:32Z",
      "updated_at": "2026-01-11T03:03:43Z",
      "pushed_at": "2026-01-07T08:24:27Z"
    },
    "stats": {
      "stars": 770,
      "forks": 62,
      "watchers": 770,
      "open_issues": 1,
      "size": 277
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 93900,
        "Shell": 15165
      },
      "license": "GNU General Public License v3.0",
      "topics": [
        "censorship-circumvention",
        "censorship-resistance",
        "covert-tunnel",
        "deep-packet-inspection",
        "encryption",
        "evasion",
        "network-security",
        "proxy",
        "python",
        "smtp",
        "socks5",
        "socks5-proxy",
        "socks5-server",
        "tls",
        "tunnel"
      ]
    },
    "content": {
      "readme": "# ğŸ“§ SMTP Tunnel Proxy\n\n> **A high-speed covert tunnel that disguises TCP traffic as SMTP email communication to bypass Deep Packet Inspection (DPI) firewalls.**\n\n```\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ Application â”‚â”€â”€â”€â”€â”€â–¶â”‚   Client    â”‚â”€â”€â”€â”€â”€â–¶â”‚   Server    â”‚â”€â”€â”€â”€â”€â–¶â”‚  Internet    â”‚\nâ”‚  (Browser)  â”‚ TCP  â”‚ SOCKS5:1080 â”‚ SMTP â”‚  Port 587   â”‚ TCP  â”‚              â”‚\nâ”‚             â”‚â—€â”€â”€â”€â”€â”€â”‚             â”‚â—€â”€â”€â”€â”€â”€â”‚             â”‚â—€â”€â”€â”€â”€â”€â”‚              â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                            â”‚                    â”‚\n                            â”‚   Looks like       â”‚\n                            â”‚   Email Traffic    â”‚\n                            â–¼                    â–¼\n                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n                     â”‚     DPI Firewall               â”‚\n                     â”‚  âœ… Sees: Normal SMTP Session  â”‚\n                     â”‚  âŒ Cannot see: Tunnel Data    â”‚\n                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n```\n\n---\n\n## ğŸ¯ Features\n\n| Feature | Description |\n|---------|-------------|\n| ğŸ”’ **TLS Encryption** | All traffic encrypted with TLS 1.2+ after STARTTLS |\n| ğŸ­ **DPI Evasion** | Initial handshake mimics real SMTP servers (Postfix) |\n| âš¡ **High Speed** | Binary streaming protocol after handshake - minimal overhead |\n| ğŸ‘¥ **Multi-User** | Per-user secrets, IP whitelists, and logging settings |\n| ğŸ”‘ **Authentication** | Per-user pre-shared keys with HMAC-SHA256 |\n| ğŸŒ **SOCKS5 Proxy** | Standard proxy interface - works with any application |\n| ğŸ“¡ **Multiplexing** | Multiple connections over single tunnel |\n| ğŸ›¡ï¸ **IP Whitelist** | Per-user access control by IP address/CIDR |\n| ğŸ“¦ **Easy Install** | One-liner server installation with systemd service |\n| ğŸ **Client Packages** | Auto-generated ZIP files for each user |\n| ğŸ”„ **Auto-Reconnect** | Client automatically reconnects on connection loss |\n\n> ğŸ“š For in-depth technical details, protocol specifications, and security analysis, see [TECHNICAL.md](TECHNICAL.md).\n\n---\n\n## âš¡ Quick Start\n\n### ğŸ“‹ Prerequisites\n\n- **Server**: Linux VPS with Python 3.8+, port 587 open\n- **Client**: Windows/macOS/Linux with Python 3.8+\n- **Domain name**: Required for TLS certificate verification (free options: [DuckDNS](https://www.duckdns.org), [No-IP](https://www.noip.com), [FreeDNS](https://freedns.afraid.org))\n\n---\n\n## ğŸš€ Server Setup (VPS)\n\n### Step 1ï¸âƒ£: Get a Domain Name\n\nGet a free domain pointing to your VPS:\n- ğŸ¦† **[DuckDNS](https://www.duckdns.org)** - Recommended, simple and free\n- ğŸŒ **[No-IP](https://www.noip.com)** - Free tier available\n- ğŸ†“ **[FreeDNS](https://freedns.afraid.org)** - Many domain options\n\nExample: `myserver.duckdns.org` â†’ `203.0.113.50` (your VPS IP)\n\n### Step 2ï¸âƒ£: Run the Installer\n\n```bash\ncurl -sSL https://raw.githubusercontent.com/x011/smtp-tunnel-proxy/main/install.sh | sudo bash\n```\n\nThe installer will:\n1. ğŸ“¥ Download and install everything\n2. â“ Ask for your domain name\n3. ğŸ” Generate TLS certificates automatically\n4. ğŸ‘¤ Offer to create your first user\n5. ğŸ”¥ Configure firewall\n6. ğŸš€ Start the service\n\n**That's it!** Your server is ready.\n\n### â• Add More Users Later\n\n```bash\nsmtp-tunnel-adduser bob      # Add user + generate client ZIP\nsmtp-tunnel-listusers        # List all users\nsmtp-tunnel-deluser bob      # Remove a user\n```\n\n### ğŸ”„ Update Server\n\n```bash\nsmtp-tunnel-update           # Updates code, preserves config/certs/users\n```\n\n---\n\n## ğŸ’» Client Setup\n\n### Option A: Easy Way (Recommended)\n\n1. Get your `username.zip` file from the server admin\n2. Extract the ZIP file\n3. Run the launcher:\n\n| Platform | How to Run |\n|----------|------------|\n| ğŸªŸ **Windows** | Double-click `start.bat` |\n| ğŸ§ **Linux** | Run `./start.sh` |\n| ğŸ **macOS** | Run `./start.sh` |\n\nThe launcher will automatically install dependencies and start the client.\n\nâœ… You should see:\n```\nSMTP Tunnel Proxy Client\nUser: alice\n\n[INFO] Starting SMTP Tunnel...\n[INFO] SOCKS5 proxy will be available at 127.0.0.1:1080\n\nConnecting to myserver.duckdns.org:587\nConnected - binary mode active\nSOCKS5 proxy on 127.0.0.1:1080\n```\n\n### Option B: Manual Way\n\n```bash\ncd alice\npip install -r requirements.txt\npython client.py\n```\n\n### Option C: Custom Configuration\n\n```bash\n# Download files\nscp root@myserver.duckdns.org:/etc/smtp-tunnel/ca.crt .\n\n# Create config.yaml:\ncat > config.yaml << EOF\nclient:\n  server_host: \"myserver.duckdns.org\"\n  server_port: 587\n  socks_port: 1080\n  username: \"alice\"\n  secret: \"your-secret-from-admin\"\n  ca_cert: \"ca.crt\"\nEOF\n\n# Run client\npython client.py -c config.yaml\n```\n\n---\n\n## ğŸ“– Usage\n\n### ğŸŒ Configure Your Applications\n\nSet SOCKS5 proxy to: `127.0.0.1:1080`\n\n#### ğŸ¦Š Firefox\n1. Settings â†’ Network Settings â†’ Settings\n2. Manual proxy configuration\n3. SOCKS Host: `127.0.0.1`, Port: `1080`\n4. Select SOCKS v5\n5. âœ… Check \"Proxy DNS when using SOCKS v5\"\n\n#### ğŸŒ Chrome\n1. Install \"Proxy SwitchyOmega\" extension\n2. Create profile with SOCKS5: `127.0.0.1:1080`\n\n#### ğŸªŸ Wind",
      "default_branch": "main"
    },
    "fetched_at": "2026-01-11T03:04:36.630830"
  }
]