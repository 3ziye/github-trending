[
  {
    "basic_info": {
      "name": "omnilingual-asr",
      "full_name": "facebookresearch/omnilingual-asr",
      "owner": "facebookresearch",
      "description": "Omnilingual ASR Open-Source Multilingual SpeechRecognition for 1600+ Languages",
      "url": "https://github.com/facebookresearch/omnilingual-asr",
      "clone_url": "https://github.com/facebookresearch/omnilingual-asr.git",
      "ssh_url": "git@github.com:facebookresearch/omnilingual-asr.git",
      "homepage": null,
      "created_at": "2025-11-06T22:38:00Z",
      "updated_at": "2025-11-23T02:21:30Z",
      "pushed_at": "2025-11-19T18:07:58Z"
    },
    "stats": {
      "stars": 2179,
      "forks": 179,
      "watchers": 2179,
      "open_issues": 16,
      "size": 1026
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 295799
      },
      "license": "Other",
      "topics": []
    },
    "content": {
      "readme": "<div align=\"center\">\n  <img src=\"./omniASR_header.jpg\" alt=\"Header image with a collage of on-the-ground photos from the transcription gathering efforts in Pakistan and Liberia.\" width=\"100%\" />\n  <p><i>Photographs captured during corpus creation efforts in Pakistan and Liberia.</i></p>\n</div>\n\n# Omnilingual ASR: Open-Source Multilingual Speech Recognition for 1600+ Languages\n\nOmnilingual ASR is an open-source speech recognition system supporting over 1,600 languages â€” including hundreds never previously covered by any ASR technology. Designed for broad accessibility, it enables new languages to be added with just a few paired examples without requiring specialized expertise or large datasets. By combining scalable zero-shot learning with a flexible model family, Omnilingual ASR aims to make speech technology more inclusive and adaptable for communities and researchers worldwide.\n\n* [Huggingface Demo](https://huggingface.co/spaces/facebook/omniasr-transcriptions)\n* [Huggingface Dataset](https://huggingface.co/datasets/facebook/omnilingual-asr-corpus)\n* [Paper](https://ai.meta.com/research/publications/omnilingual-asr-open-source-multilingual-speech-recognition-for-1600-languages/)\n* [Blogpost](http://ai.meta.com/blog/omnilingual-asr-advancing-automatic-speech-recognition)\n\n<div align=\"center\">\n  <img src=\"./result_table.png\" alt=\"Performance results table\" width=\"100%\" />\n  <p><i>Our 7B-LLM-ASR system achieves state-of-the-art performance across 1,600+ languages, with character error rates (CER) below 10 for 78% of those languages.</i></p>\n</div>\n\n\n## Documentation\n\n### Quick Start\n- **[Installation & Basic Usage](#installation)** - Setup and first transcription\n- **[Inference Pipeline](src/omnilingual_asr/models/inference/README.md)** - Comprehensive transcription guide with batch processing, language conditioning, and context examples\n- **[Supported Languages](#supported-languages)** - View the complete list of 1600+ supported languages\n\n\n### Models & Architecture\n- **[Model Specifications](#model-architectures)** - Available models, parameters, and memory requirements\n- **[Architecture Overview](src/omnilingual_asr/models/README.md)** - Technical details on W2V, CTC, and LLM model families\n- **[Asset Management](src/omnilingual_asr/cards/README.md)** - Configuration system for models, tokenizers, and datasets\n\n### Training & Data Pipeline\n- **[Data Preparation](workflows/dataprep/README.md)** - End-to-end guide for multilingual dataset preparation, HuggingFace integration, and parquet processing\n- **[Training Recipes](workflows/recipes/wav2vec2/asr/README.md)** - Pre-configured workflows for CTC and LLM model training\n\n---\n\n## Installation\n\nThe models were developed using [fairseq2](https://github.com/facebookresearch/fairseq2), a research-focused sequence modeling toolkit. While we provide a **reference** inference pipeline that works across platforms, audio support requires [libsndfile](https://github.com/facebookresearch/fairseq2?tab=readme-ov-file#system-dependencies) (Mac: `brew install libsndfile`; Windows may need an additional [setup](https://github.com/facebookresearch/fairseq2?tab=readme-ov-file#installing-on-windows)).\n\n```bash\n# using pip\npip install omnilingual-asr\n\n# using uv\nuv add omnilingual-asr\n```\n\n## Inference\n\n```python\nfrom omnilingual_asr.models.inference.pipeline import ASRInferencePipeline\n\npipeline = ASRInferencePipeline(model_card=\"omniASR_LLM_7B\")\n\naudio_files = [\"/path/to/eng_audio1.flac\", \"/path/to/deu_audio2.wav\"]\nlang = [\"eng_Latn\", \"deu_Latn\"]\ntranscriptions = pipeline.transcribe(audio_files, lang=lang, batch_size=2)\n```\n\nMore details on running specific models can be found in the [src/omnilingual_asr/models/inference](/src/omnilingual_asr/models/inference/README.md) directory.\n\n> **âš ï¸ Important:** Currently only audio files shorter than 40 seconds are accepted for inference. We plan to add support for transcribing unlimited-length audio files shortly.\n\n### Supported Languages\n\nTo view the full list of 1600+ supported languages, you can access the language list [programmatically](/src/omnilingual_asr/models/wav2vec2_llama/lang_ids.py):\n\n```python\nfrom omnilingual_asr.models.wav2vec2_llama.lang_ids import supported_langs\n\n# Print all supported languages\nprint(f\"Total supported languages: {len(supported_langs)}\")\nprint(supported_langs)\n\n# Check if a specific language is supported\nif \"eng_Latn\" in supported_langs:\n    print(\"English (Latin script) is supported!\")\n```\n\nLanguages follow the format `{language_code}_{script}`, for example `eng_Latn` - English (Latin script), `cmn_Hans` - Mandarin Chinese (Simplified), ...\n\n### Using the HuggingFace Dataset ğŸ¤—\n\nWe provide a large-scale multilingual speech dataset on HuggingFace under CC-BY-4.0 License: [`facebook/omnilingual-asr-corpus`](https://huggingface.co/datasets/facebook/omnilingual-asr-corpus).\nThis dataset can be directly used with our inference pipeline for evaluation or testing:\n\n```bash\npip install \"omnilingual-asr[dat",
      "default_branch": "main"
    },
    "fetched_at": "2025-11-23T02:53:21.553363"
  },
  {
    "basic_info": {
      "name": "reader3",
      "full_name": "karpathy/reader3",
      "owner": "karpathy",
      "description": "Quick illustration of how one can easily read books together with LLMs. It's great and I highly recommend it.",
      "url": "https://github.com/karpathy/reader3",
      "clone_url": "https://github.com/karpathy/reader3.git",
      "ssh_url": "git@github.com:karpathy/reader3.git",
      "homepage": null,
      "created_at": "2025-11-18T02:37:00Z",
      "updated_at": "2025-11-23T02:47:11Z",
      "pushed_at": "2025-11-18T02:37:51Z"
    },
    "stats": {
      "stars": 1813,
      "forks": 196,
      "watchers": 1813,
      "open_issues": 8,
      "size": 271
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 13925,
        "HTML": 8921
      },
      "license": null,
      "topics": []
    },
    "content": {
      "readme": "# reader 3\n\n![reader3](reader3.png)\n\nA lightweight, self-hosted EPUB reader that lets you read through EPUB books one chapter at a time. This makes it very easy to copy paste the contents of a chapter to an LLM, to read along. Basically - get epub books (e.g. [Project Gutenberg](https://www.gutenberg.org/) has many), open them up in this reader, copy paste text around to your favorite LLM, and read together and along.\n\nThis project was 90% vibe coded just to illustrate how one can very easily [read books together with LLMs](https://x.com/karpathy/status/1990577951671509438). I'm not going to support it in any way, it's provided here as is for other people's inspiration and I don't intend to improve it. Code is ephemeral now and libraries are over, ask your LLM to change it in whatever way you like.\n\n## Usage\n\nThe project uses [uv](https://docs.astral.sh/uv/). So for example, download [Dracula EPUB3](https://www.gutenberg.org/ebooks/345) to this directory as `dracula.epub`, then:\n\n```bash\nuv run reader3.py dracula.epub\n```\n\nThis creates the directory `dracula_data`, which registers the book to your local library. We can then run the server:\n\n```bash\nuv run server.py\n```\n\nAnd visit [localhost:8123](http://localhost:8123/) to see your current Library. You can easily add more books, or delete them from your library by deleting the folder. It's not supposed to be complicated or complex.\n\n## License\n\nMIT",
      "default_branch": "master"
    },
    "fetched_at": "2025-11-23T02:53:22.818451"
  },
  {
    "basic_info": {
      "name": "Emu3.5",
      "full_name": "baaivision/Emu3.5",
      "owner": "baaivision",
      "description": "Native Multimodal Models are World Learners",
      "url": "https://github.com/baaivision/Emu3.5",
      "clone_url": "https://github.com/baaivision/Emu3.5.git",
      "ssh_url": "git@github.com:baaivision/Emu3.5.git",
      "homepage": "",
      "created_at": "2025-10-29T13:40:19Z",
      "updated_at": "2025-11-22T23:27:07Z",
      "pushed_at": "2025-11-19T08:25:09Z"
    },
    "stats": {
      "stars": 1271,
      "forks": 44,
      "watchers": 1271,
      "open_issues": 23,
      "size": 23845
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 269009
      },
      "license": "Apache License 2.0",
      "topics": []
    },
    "content": {
      "readme": "<div align='center'>\n<h1>Emu3.5: Native Multimodal Models are World Learners</h1>\n\nEmu3.5 Team, BAAI\n\n[Project Page](https://emu.world/) | [ğŸ¤—HF Models](https://huggingface.co/collections/BAAI/emu35) | [Paper](https://arxiv.org/pdf/2510.26583)\n</div>\n\n\n<div align='center'>\n<img src=\"./assets/arch.png\" class=\"interpolation-image\" alt=\"arch.\" height=\"100%\" width=\"100%\" />\n</div>\n\n\n<div align='center'>\n<img src=\"./assets/co.png\" class=\"interpolation-image\" alt=\"arch.\" height=\"90%\" width=\"90%\" />\n</div>\n\n\n|  ğŸ”¹ | **Core Concept**                         | **Description**                                                                                                                            |\n| :-: | :--------------------------------------- | :----------------------------------------------------------------------------------------------------------------------------------------- |\n|  ğŸ§  | **Unified World Modeling**               | Predicts the **next state jointly across vision and language**, enabling coherent **world modeling** and **generation**.              |\n|  ğŸ§© | **End-to-End Pretraining**               | Trained with a **unified next-token prediction** objective over **interleaved visionâ€“language sequences**.                                 |\n|  ğŸ“š | **Over 10T+ Multimodal Tokens**               | Pre-trained on **over 10 trillion interleaved tokens** from **video frames** and **transcripts**, capturing **spatiotemporal structure**.       |\n|  ğŸ”„ | **Native Multimodal I/O**                | Processes and generates **interleaved visualâ€“text sequences** without **modality adapters** or **task-specific heads**.                    |\n|  ğŸ¯ | **RL Post-Training**                     | Large-scale **reinforcement learning** enhances **reasoning**, **compositionality**, and **generation quality**.                           |\n|  âš¡  | **Discrete Diffusion Adaptation (DiDA)** | Converts **sequential decoding â†’ bidirectional parallel prediction**, achieving **â‰ˆ20Ã— faster inference without performance loss**.      |\n| ğŸ–¼ï¸ | **Versatile Generation**                 | Excels in **long-horizon visionâ€“language generation**, **any-to-image (X2I)** synthesis, and **text-rich image creation**.                 |\n|  ğŸŒ | **Generalizable World Modeling**         | Enables **spatiotemporally consistent world exploration**, and **open-world embodied manipulation** across diverse scenarios.          |\n|  ğŸ† | **Performance Benchmark**                | Matches **Gemini 2.5 Flash Image (Nano Banana)** on **image generation/editing**, and **outperforms** on **interleaved generation tasks**. |\n\n\n\n## ğŸ”¥ News\n\n- **2025-11-19 Â· ğŸš€ vLLM Offline Inference Released** â€” Meet `inference_vllm.py` with a new cond/uncond batch scheduler, delivering **4â€“5Ã— faster end-to-end generation** on vLLM 0.11.0 across Emu3.5 tasks. Jump to [#Run Inference with vLLM](#run-inference-with-vllm) for setup guidance and see PR [#47](https://github.com/baaivision/Emu3.5/pull/47) for full details.\n- **2025-11-17 Â· ğŸ›ï¸ Gradio Demo (Transformers Backend)** â€” Introduced `gradio_demo_image.py` and `gradio_demo_interleave.py` presets for the standard Transformers runtime, providing turnkey T2I/X2I and interleaved generation experiences with streaming output. Try the commands in [#Gradio Demo](#3-gradio-demo) to launch both UIs locally.\n\n## Table of Contents\n\n1. [Model & Weights](#1-model--weights)\n2. [Quick Start](#2-quick-start)\n3. [Gradio Demo](#3-gradio-demo)\n4. [Schedule](#4-schedule)\n5. [Citation](#5-citation)\n\n## 1. Model & Weights\n\n| Model name               | HF Weight |\n| ------------------------ | --------- |\n| Emu3.5               | [ğŸ¤— HF link](https://huggingface.co/BAAI/Emu3.5/tree/main) |\n| Emu3.5-Image                | [ğŸ¤— HF link](https://huggingface.co/BAAI/Emu3.5-Image/tree/main) |\n| Emu3.5-VisionTokenizer     | [ğŸ¤— HF link](https://huggingface.co/BAAI/Emu3.5-VisionTokenizer/tree/main) |\n\n\n*Note:*  \n- **Emu3.5** supports general-purpose multimodal predictions, including interleaved image-text generation and single-image generation (T2I/X2I) tasks.\n- **Emu3.5-Image** is a model focused on T2I/X2I tasks for best performance on these scenarios.\n- Both models are pure next-token predictors without DiDA acceleration (each image may take several minutes to generate).  \n- âš¡ **Stay tuned for DiDA-accelerated weights.**\n\n> ğŸ’¡ **Usage tip:**  \n> For **interleaved image-text generation**, use **Emu3.5**.  \n> For **single-image generation** (T2I and X2I), use **Emu3.5-Image** for the best quality.\n\n\n\n## 2. Quick Start\n\n### Environment Setup\n\n```bash\n# Requires Python 3.12 or higher.\ngit clone https://github.com/baaivision/Emu3.5\ncd Emu3.5\npip install -r requirements/transformers.txt\npip install flash_attn==2.8.3 --no-build-isolation\n```\n### Configuration\n\nEdit `configs/config.py` to set:\n\n- Paths: `model_path`, `vq_path`\n- Task template: `task_type in {t2i, x2i, howto, story, explore, vla}`\n- Input image: `use_image` (True to provide reference images, controls <|IMAGE|> tok",
      "default_branch": "main"
    },
    "fetched_at": "2025-11-23T02:53:24.141311"
  },
  {
    "basic_info": {
      "name": "xiaomi-miloco",
      "full_name": "XiaoMi/xiaomi-miloco",
      "owner": "XiaoMi",
      "description": "Xiaomi Miloco",
      "url": "https://github.com/XiaoMi/xiaomi-miloco",
      "clone_url": "https://github.com/XiaoMi/xiaomi-miloco.git",
      "ssh_url": "git@github.com:XiaoMi/xiaomi-miloco.git",
      "homepage": null,
      "created_at": "2025-11-06T13:01:59Z",
      "updated_at": "2025-11-23T02:00:16Z",
      "pushed_at": "2025-11-22T09:44:22Z"
    },
    "stats": {
      "stars": 1269,
      "forks": 73,
      "watchers": 1269,
      "open_issues": 56,
      "size": 13401
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 1054483,
        "JavaScript": 506375,
        "Shell": 86782,
        "C++": 83167,
        "Less": 71704,
        "HTML": 53752,
        "CSS": 9737,
        "Dockerfile": 5505,
        "C": 2948,
        "CMake": 1813
      },
      "license": "Other",
      "topics": []
    },
    "content": {
      "readme": "# Xiaomi Miloco\n\n**Xiaomi Local Copilot** is a future exploration solution for smart homes. Using Xiaomi Home cameras as the source of visual information and a self-developed LLM as its core, it connects all IoT devices throughout the house. Based on the development paradigm of LLM, it enables users to define various family needs and rules in natural language, achieving broader and more creative smart device integration.\n\n<div align=\"center\">\n\nEnglish | [ç®€ä½“ä¸­æ–‡](README_zh_Hans.md)\n\n</div>\n\n## News\n\n- [2025-11] Xiaomi Miloco Framework Open Source\n\n## Key Features\n\n1. New Interaction Paradigm: Based on the development paradigm of LLM, rule-setting and complex device command control can be completed through natural language interaction.\n2. New Use for Visual Data: Using camera data streams as a source of perceptual information, the LLM is used to analyze various home scene events contained in the visual data to respond to user queries.\n3. On-Device LLM: The home scene tasks are split into two stages: planning and visual understanding. It provides Xiaomi's self-developed on-device model to realize on-device video understanding and ensure family privacy and security.\n4. Xiaomi Home Ecosystem: It connects with the Xiaomi Home ecosystem, supports the retrieval and execution of Mi Home devices and scenes, and supports sending customized content for Xiao Home notifications.\n\n    <img src=\"assets/images/ai_center.jpg\" width=\"60%\" />\n\n## Quick Start\n\n### System Requirements\n\n- **Hardware Requirements**\n```Plain Text\nCPU: x64 architecture\nGraphics Card: NVIDIA 30 series and above, 8GB VRAM minimum (recommended 12GB and above)\nStorage: Recommended 16GB or more available space (for local model storage)\n```\n\n- **Software Requirements**\n```Plain Text\nOperating System:\n  - Linux: x64 architecture, recommended Ubuntu 22.04 and above LTS versions\n  - Windows: x64 architecture, recommended Windows 10 and above, requires WSL2 support\n  - macOS: Not currently supported\nDocker: Version 20.10 and above, requires docker compose support\nNVIDIA Driver: NVIDIA driver with CUDA support\nNVIDIA Container Toolkit: For Docker GPU support\n```\n\n### Install\n\n> **Note**: Please ensure your system meets the above hardware and software requirements. Windows systems need to enter the WSL environment.\n\n**Install with Docker**  \nOne-click installation via command line\n```bash\nbash -c \"$(wget -qO- https://xiaomi-miloco.cnbj1.mi-fds.com/xiaomi-miloco/install.sh)\"\n```\nOr download the source code first, then execute the one-click installation script:\n```bash\ngit clone https://github.com/XiaoMi/xiaomi-miloco.git\n\nbash scripts/install.sh\n```\nFor detailed installation steps, please refer to the [Docker Deployment Documentation](docs/environment-setup.md).\n\n**Install with source code**  \nFor source code installation steps, please refer to the [Development Guide](docs/development/developer-setup.md).\n\n## Usage Documentation\n\nPlease refer to the [Usage Documentation](docs/usage/README.md).\n\n## Contributing\n\nPlease refer to the [Contributing Guide](CONTRIBUTING.md).\n\n## License\n\nFor license details, please see [LICENSE.md](LICENSE.md).\n\n**Important Notice**: This project is limited to non-commercial use only. Without written authorization from Xiaomi Corporation, this project may not be used for developing applications, web services, or other forms of software.\n\n## Security Issues\n\nIf you discover potential security issues in this project, or believe you may have found a security issue, please notify the [Miloco Team](xiaomi-miloco@xiaomi.com) via our vulnerability reporting email. Please do not create public GitHub Issues.\n\n## Contact Us\n\n### Issue Reporting\n\nFor issue reporting, please participate through the following methods:\n- Submit a [GitHub Issue](https://github.com/XiaoMi/xiaomi-miloco/issues/new/)\n\n### Technical Discussion\n\n- GitHub [Discussions](https://github.com/XiaoMi/xiaomi-miloco/discussions/)\n- Project Discussion Group (WeChat):\n\n  <img src=\"assets/images/miloco_wechat_6.jpg\" width=\"30%\" />  <img src=\"assets/images/miloco_wechat_group_12.jpeg\" width=\"30%\" />\n\n\n### Join Us\n\nThe **Xiaomi Miloco** team is hiring. Send your resume to `xiaomi-miloco@xiaomi.com`, and it will be delivered directly to the project lead.\n\n## Acknowledgments\n\nThank you to the original team members who worked hard for Milocoï¼šzhaoyã€yangyongjieã€xxã€Changyuã€yykã€junhuiã€éƒ­å…´å®ã€47ã€afeiã€‚\n\nYour passion and talent are the fundamental driving force behind Miloco's continuous innovation and progress.\n\nSpecial thanks to:\n- The [llama.cpp](https://github.com/ggml-org/llama.cpp) open source project for providing inference backend capabilities\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-11-23T02:53:25.443820"
  },
  {
    "basic_info": {
      "name": "crypto-trading-open",
      "full_name": "cryptocj520/crypto-trading-open",
      "owner": "cryptocj520",
      "description": "crypto-trading-open",
      "url": "https://github.com/cryptocj520/crypto-trading-open",
      "clone_url": "https://github.com/cryptocj520/crypto-trading-open.git",
      "ssh_url": "git@github.com:cryptocj520/crypto-trading-open.git",
      "homepage": null,
      "created_at": "2025-11-11T12:00:02Z",
      "updated_at": "2025-11-22T09:12:16Z",
      "pushed_at": "2025-11-11T12:03:28Z"
    },
    "stats": {
      "stars": 1229,
      "forks": 674,
      "watchers": 1229,
      "open_issues": 12,
      "size": 997
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 2825921,
        "Shell": 37998
      },
      "license": null,
      "topics": []
    },
    "content": {
      "readme": "# å¤šäº¤æ˜“æ‰€ç­–ç•¥è‡ªåŠ¨åŒ–ç³»ç»Ÿ\n\n**Multi-Exchange Strategy Automation System**\n\n## ğŸ¯ é¡¹ç›®ç®€ä»‹\n\nè¿™æ˜¯ä¸€ä¸ªä¼ä¸šçº§çš„å¤šäº¤æ˜“æ‰€åŠ å¯†è´§å¸è‡ªåŠ¨åŒ–äº¤æ˜“ç³»ç»Ÿï¼Œæä¾›é«˜æ€§èƒ½ã€é«˜å¯é æ€§çš„ç½‘æ ¼äº¤æ˜“ã€åˆ·é‡äº¤æ˜“ã€å¥—åˆ©ç›‘æ§å’Œå¸‚åœºç›‘æ§åŠŸèƒ½ã€‚ç³»ç»Ÿé‡‡ç”¨ä¸¥æ ¼çš„åˆ†å±‚æ¶æ„è®¾è®¡ï¼Œæ”¯æŒ Hyperliquidã€Backpackã€Lighterã€Binanceã€OKXã€EdgeX ç­‰å¤šä¸ªäº¤æ˜“æ‰€çš„å®Œæ•´é€‚é…ã€‚\n\n## ğŸ—ï¸ æ ¸å¿ƒç³»ç»Ÿæ¶æ„\n\n### ç³»ç»Ÿç»„ä»¶\n\n```\nå¤šäº¤æ˜“æ‰€ç­–ç•¥è‡ªåŠ¨åŒ–ç³»ç»Ÿ\nâ”œâ”€â”€ ğŸ“Š ç½‘æ ¼äº¤æ˜“ç³»ç»Ÿ (Grid Trading)\nâ”‚   â”œâ”€â”€ æ™®é€šç½‘æ ¼              # å›ºå®šä»·æ ¼åŒºé—´ç½‘æ ¼\nâ”‚   â”œâ”€â”€ é©¬ä¸ç½‘æ ¼              # é©¬ä¸æ ¼å°”é€’å¢ç­–ç•¥\nâ”‚   â”œâ”€â”€ ä»·æ ¼ç§»åŠ¨ç½‘æ ¼          # åŠ¨æ€è·Ÿéšä»·æ ¼\nâ”‚   â”œâ”€â”€ å‰¥å¤´çš®æ¨¡å¼            # å¿«é€Ÿæ­¢æŸç­–ç•¥\nâ”‚   â”œâ”€â”€ æ™ºèƒ½å‰¥å¤´çš®            # å¤šæ¬¡æ·±è·Œæ£€æµ‹\nâ”‚   â”œâ”€â”€ æœ¬é‡‘ä¿æŠ¤æ¨¡å¼          # è‡ªåŠ¨æ­¢æŸä¿æŠ¤\nâ”‚   â”œâ”€â”€ æ­¢ç›ˆæ¨¡å¼              # åˆ°è¾¾ç›®æ ‡è‡ªåŠ¨å¹³ä»“\nâ”‚   â””â”€â”€ ç°è´§é¢„ç•™ç®¡ç†          # ç°è´§å¸ç§é¢„ç•™\nâ”œâ”€â”€ ğŸ” ç½‘æ ¼æ³¢åŠ¨ç‡æ‰«æå™¨ (Grid Volatility Scanner)\nâ”‚   â”œâ”€â”€ è™šæ‹Ÿç½‘æ ¼æ¨¡æ‹Ÿ          # æ— éœ€å®é™…ä¸‹å•çš„æ¨¡æ‹Ÿç½‘æ ¼\nâ”‚   â”œâ”€â”€ å®æ—¶APRè®¡ç®—           # å‡†ç¡®é¢„æµ‹å¹´åŒ–æ”¶ç›Šç‡\nâ”‚   â”œâ”€â”€ ä»£å¸æ’è¡Œæ¦œ            # æŒ‰æ³¢åŠ¨ç‡å’ŒAPRæ’åº\nâ”‚   â”œâ”€â”€ æ™ºèƒ½è¯„çº§ç³»ç»Ÿ          # S/A/B/C/Dç­‰çº§è¯„ä¼°\nâ”‚   â””â”€â”€ ç»ˆç«¯ UI              # Rich å®æ—¶ç›‘æ§ç•Œé¢\nâ”œâ”€â”€ ğŸ’¹ åˆ·é‡äº¤æ˜“ç³»ç»Ÿ (Volume Maker)\nâ”‚   â”œâ”€â”€ æŒ‚å•æ¨¡å¼              # é™ä»·å•åˆ·é‡ï¼ˆBackpackï¼‰\nâ”‚   â””â”€â”€ å¸‚ä»·æ¨¡å¼              # å¸‚ä»·å•å¿«é€Ÿåˆ·é‡ï¼ˆLighterï¼‰\nâ”œâ”€â”€ ğŸ”„ å¥—åˆ©ç›‘æ§ç³»ç»Ÿ (Arbitrage Monitor)\nâ”‚   â”œâ”€â”€ ä»·æ ¼ç›‘æ§              # å®æ—¶ä»·æ ¼å·®ç›‘æ§\nâ”‚   â”œâ”€â”€ èµ„é‡‘è´¹ç‡ç›‘æ§          # è·¨äº¤æ˜“æ‰€è´¹ç‡å·®å¼‚\nâ”‚   â”œâ”€â”€ å¥—åˆ©æœºä¼šè¯†åˆ«          # ä»·å·®å’Œè´¹ç‡å¥—åˆ©\nâ”‚   â”œâ”€â”€ ç»ˆç«¯ UI              # Rich å®æ—¶ç›‘æ§ç•Œé¢\nâ”‚   â””â”€â”€ äº¤æ˜“å¯¹è‡ªåŠ¨å‘ç°        # å¤šäº¤æ˜“æ‰€äº¤æ˜“å¯¹åŒ¹é…\nâ”œâ”€â”€ ğŸ”” ä»·æ ¼æé†’ç³»ç»Ÿ (Price Alert)\nâ”‚   â”œâ”€â”€ ä»·æ ¼çªç ´ç›‘æ§          # ä»·æ ¼è§¦åŠç›®æ ‡æé†’\nâ”‚   â”œâ”€â”€ å¤šäº¤æ˜“æ‰€æ”¯æŒ          # æ”¯æŒæ‰€æœ‰æ¥å…¥çš„äº¤æ˜“æ‰€\nâ”‚   â”œâ”€â”€ ç»ˆç«¯ UI              # å®æ—¶ä»·æ ¼æ˜¾ç¤º\nâ”‚   â””â”€â”€ å£°éŸ³æé†’              # çªç ´æ—¶å£°éŸ³é€šçŸ¥\nâ”œâ”€â”€ ğŸ”— äº¤æ˜“æ‰€é€‚é…å±‚ (Exchange Adapters)\nâ”‚   â”œâ”€â”€ Hyperliquid é€‚é…å™¨    # æ°¸ç»­åˆçº¦ + ç°è´§\nâ”‚   â”œâ”€â”€ Backpack é€‚é…å™¨       # æ°¸ç»­åˆçº¦\nâ”‚   â”œâ”€â”€ Lighter é€‚é…å™¨        # æ°¸ç»­åˆçº¦ï¼ˆä½æ‰‹ç»­è´¹ï¼‰\nâ”‚   â”œâ”€â”€ Binance é€‚é…å™¨        # ç°è´§ + æ°¸ç»­åˆçº¦\nâ”‚   â”œâ”€â”€ OKX é€‚é…å™¨            # ç°è´§ + æ°¸ç»­åˆçº¦\nâ”‚   â”œâ”€â”€ EdgeX é€‚é…å™¨          # æ°¸ç»­åˆçº¦\nâ”‚   â””â”€â”€ ç»Ÿä¸€æ¥å£æ ‡å‡†          # æ ‡å‡†åŒ– API æ¥å£\nâ””â”€â”€ ğŸ›ï¸ åŸºç¡€è®¾æ–½å±‚ (Infrastructure)\n    â”œâ”€â”€ ä¾èµ–æ³¨å…¥å®¹å™¨          # DI å®¹å™¨ç®¡ç†\n    â”œâ”€â”€ äº‹ä»¶ç³»ç»Ÿ              # äº‹ä»¶é©±åŠ¨æ¶æ„\n    â”œâ”€â”€ æ—¥å¿—ç³»ç»Ÿ              # ç»“æ„åŒ–æ—¥å¿—\n    â”œâ”€â”€ é…ç½®ç®¡ç†              # YAML é…ç½®ç³»ç»Ÿ\n    â””â”€â”€ æ•°æ®èšåˆå™¨            # å¤šäº¤æ˜“æ‰€æ•°æ®èšåˆ\n```\n\n## ğŸš€ å¿«é€Ÿå¼€å§‹\n\n### ç³»ç»Ÿè¦æ±‚\n\n- Python 3.8+\n- æ”¯æŒçš„æ“ä½œç³»ç»Ÿï¼šLinuxã€macOSã€Windows\n- å¯é€‰ï¼štmuxï¼ˆç”¨äºå¤šè¿›ç¨‹ç®¡ç†ï¼‰\n\n### å®‰è£…ä¾èµ–\n\n```bash\n# å®‰è£… Python ä¾èµ–\npip install -r requirements.txt\n```\n\n### é…ç½® API å¯†é’¥\n\nåœ¨ `config/exchanges/` ç›®å½•ä¸‹é…ç½®å¯¹åº”äº¤æ˜“æ‰€çš„ API å¯†é’¥ï¼š\n\n```bash\nconfig/exchanges/\nâ”œâ”€â”€ hyperliquid_config.yaml   # Hyperliquid é…ç½®\nâ”œâ”€â”€ backpack_config.yaml       # Backpack é…ç½®\nâ”œâ”€â”€ lighter_config.yaml        # Lighter é…ç½®\nâ”œâ”€â”€ binance_config.yaml        # Binance é…ç½®\nâ”œâ”€â”€ okx_config.yaml            # OKX é…ç½®\nâ””â”€â”€ edgex_config.yaml          # EdgeX é…ç½®\n```\n\n### å¿«é€Ÿå¯åŠ¨å„ç³»ç»Ÿ\n\n#### ç½‘æ ¼äº¤æ˜“ç³»ç»Ÿ\n```bash\npython3 run_grid_trading.py config/grid/lighter-long-perp-btc.yaml\n```\n\n#### åˆ·é‡äº¤æ˜“ç³»ç»Ÿï¼ˆBackpackæŒ‚å•æ¨¡å¼ï¼‰\n```bash\npython3 run_volume_maker.py config/volume_maker/backpack_btc_volume_maker.yaml\n```\n\n#### åˆ·é‡äº¤æ˜“ç³»ç»Ÿï¼ˆLighterå¸‚ä»·æ¨¡å¼ï¼‰\n```bash\npython3 run_lighter_volume_maker.py config/volume_maker/lighter_volume_maker.yaml\n```\n\n#### å¥—åˆ©ç›‘æ§ç³»ç»Ÿ\n```bash\npython3 run_arbitrage_monitor.py\n```\n\n#### ä»·æ ¼æé†’ç³»ç»Ÿ\n```bash\npython3 run_price_alert.py config/price_alert/binance_alert.yaml\n```\n\n#### ç½‘æ ¼æ³¢åŠ¨ç‡æ‰«æå™¨\n```bash\npython3 grid_volatility_scanner/run_scanner.py\n```\n\n## ğŸ“‹ æ ¸å¿ƒåŠŸèƒ½è¯¦è§£\n\n### 1ï¸âƒ£ ç½‘æ ¼äº¤æ˜“ç³»ç»Ÿ\n\n#### åŠŸèƒ½ç‰¹æ€§\n\n- **å¤šç§ç½‘æ ¼æ¨¡å¼**ï¼šæ™®é€šç½‘æ ¼ã€é©¬ä¸ç½‘æ ¼ã€ä»·æ ¼ç§»åŠ¨ç½‘æ ¼\n- **æ™ºèƒ½ç­–ç•¥**ï¼šå‰¥å¤´çš®ã€æ™ºèƒ½å‰¥å¤´çš®ã€æœ¬é‡‘ä¿æŠ¤ã€æ­¢ç›ˆæ¨¡å¼\n- **å¥åº·æ£€æŸ¥**ï¼šè‡ªåŠ¨è®¢å•æ ¡éªŒå’Œä¿®å¤æœºåˆ¶\n- **ç»ˆç«¯ UI**ï¼šå®æ—¶ç›‘æ§ç•Œé¢ï¼Œæ˜¾ç¤ºæŒä»“ã€ç›ˆäºã€ç½‘æ ¼çŠ¶æ€\n- **ç°è´§æ”¯æŒ**ï¼šç°è´§é¢„ç•™ç®¡ç†ï¼ˆè‡ªåŠ¨ç»´æŒå¸ç§ä½™é¢ï¼‰\n- **å¤šäº¤æ˜“æ‰€**ï¼šæ”¯æŒ Hyperliquidã€Backpackã€Lighter\n\n#### é…ç½®æ–‡ä»¶ä½ç½®\n\n```\nconfig/grid/\nâ”œâ”€â”€ lighter_btc_perp_long.yaml              # Lighter BTC åšå¤š\nâ”œâ”€â”€ lighter_btc_perp_short.yaml             # Lighter BTC åšç©º\nâ”œâ”€â”€ hyperliquid_btc_perp_long.yaml          # Hyperliquid BTC åšå¤š\nâ”œâ”€â”€ hyperliquid_btc_perp_short.yaml         # Hyperliquid BTC åšç©º\nâ”œâ”€â”€ hyperliquid_btc_spot_long.yaml          # Hyperliquid ç°è´§åšå¤š\nâ”œâ”€â”€ backpack_capital_protection_long_btc.yaml   # Backpack BTC æœ¬é‡‘ä¿æŠ¤\nâ”œâ”€â”€ backpack_capital_protection_long_eth.yaml   # Backpack ETH æœ¬é‡‘ä¿æŠ¤\nâ”œâ”€â”€ backpack_capital_protection_long_sol.yaml   # Backpack SOL æœ¬é‡‘ä¿æŠ¤\nâ”œâ”€â”€ backpack_capital_protection_long_bnb.yaml   # Backpack BNB æœ¬é‡‘ä¿æŠ¤\nâ””â”€â”€ backpack_capital_protection_long_hype.yaml  # Backpack HYPE æœ¬é‡‘ä¿æŠ¤\n```\n\n#### å¯åŠ¨æ–¹å¼\n\n```bash\n# æ–¹å¼1ï¼šç›´æ¥å¯åŠ¨ï¼ˆæ¨èï¼‰\npython3 run_grid_trading.py config/grid/lighter_btc_perp_long.yaml\npython3 run_grid_trading.py config/grid/lighter_eth_perp_long.yaml\n\n# æ–¹å¼2ï¼šDEBUG æ¨¡å¼å¯åŠ¨ï¼ˆæŸ¥çœ‹è¯¦ç»†æ—¥å¿—ï¼‰\npython3 run_grid_trading.py config/grid/lighter_btc_perp_long.yaml --debug\n\n# æ–¹å¼3ï¼šä½¿ç”¨ Shell è„šæœ¬æ‰¹é‡å¯åŠ¨ï¼ˆtmuxï¼‰\n./scripts/start_all_grids.sh\n```\n\n#### æ ¸å¿ƒæ–‡ä»¶\n\n| æ–‡ä»¶è·¯å¾„ | è¯´æ˜ |\n|---------|------|\n| `run_grid_trading.py` | ç½‘æ ¼äº¤æ˜“ç³»ç»Ÿä¸»å¯åŠ¨è„šæœ¬ |\n| `core/services/grid/coordinator/grid_coordinator.py` | ç½‘æ ¼ç³»ç»Ÿåè°ƒå™¨ï¼ˆæ ¸å¿ƒé€»è¾‘ï¼‰ |\n| `core/services/grid/implementations/grid_engine_impl.py` | ç½‘æ ¼æ‰§è¡Œå¼•æ“ |\n| `core/services/grid/implementations/grid_strategy_impl.py` | ç½‘æ ¼ç­–ç•¥å®ç° |\n| `core/services/grid/implementations/position_tracker_impl.py` | æŒä»“è·Ÿè¸ªå™¨ |\n| `core/services/grid/implementations/order_health_checker.py` | è®¢å•å¥åº·æ£€æŸ¥å™¨ |\n| `core/services/grid/scalping/scalping_manager.py` | å‰¥å¤´çš®ç®¡ç†å™¨ |\n| `core/services/grid/scalping/smart_scalping_tracker.py` | æ™ºèƒ½å‰¥å¤´çš®è¿½è¸ªå™¨ |\n| `core/services/grid/capital_protection/capital_protection_manager.py` | æœ¬é‡‘ä¿æŠ¤ç®¡ç†å™¨ |\n| `core/services/grid/terminal_ui.py` | ç»ˆç«¯ UI ç•Œé¢ |\n\n### 2ï¸âƒ£ åˆ·é‡äº¤æ˜“ç³»ç»Ÿ\n\n#### åŠŸèƒ½ç‰¹æ€§\n\n- **åŒäº¤æ˜“æ¨¡å¼**ï¼šæŒ‚å•æ¨¡å¼ï¼ˆBackpackï¼‰ã€å¸‚ä»·æ¨¡å¼ï¼ˆLighterï¼‰\n- **ä¿¡å·æºæ”¯æŒ**ï¼šBackpack REST APIã€Hyperliquid WebSocket\n- **æ™ºèƒ½åˆ¤æ–­**ï¼šä¹°å–å•æ•°é‡å¯¹æ¯”ã€ä»·æ ¼å˜åŠ¨ç›‘æ§\n- **å®æ—¶ç»Ÿè®¡**ï¼šæˆäº¤é‡ã€",
      "default_branch": "main"
    },
    "fetched_at": "2025-11-23T02:53:26.735914"
  },
  {
    "basic_info": {
      "name": "LongCat-Video",
      "full_name": "meituan-longcat/LongCat-Video",
      "owner": "meituan-longcat",
      "description": null,
      "url": "https://github.com/meituan-longcat/LongCat-Video",
      "clone_url": "https://github.com/meituan-longcat/LongCat-Video.git",
      "ssh_url": "git@github.com:meituan-longcat/LongCat-Video.git",
      "homepage": null,
      "created_at": "2025-10-25T06:49:49Z",
      "updated_at": "2025-11-23T00:56:28Z",
      "pushed_at": "2025-11-04T10:07:37Z"
    },
    "stats": {
      "stars": 1196,
      "forks": 119,
      "watchers": 1196,
      "open_issues": 12,
      "size": 1300750
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 328127
      },
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "# LongCat-Video\n\n<div align=\"center\">\n  <img src=\"assets/longcat-video_logo.svg\" width=\"45%\" alt=\"LongCat-Video\" />\n</div>\n<hr>\n\n<div align=\"center\" style=\"line-height: 1;\">\n  <a href='https://meituan-longcat.github.io/LongCat-Video/'><img src='https://img.shields.io/badge/Project-Page-green'></a>\n  <a href='https://arxiv.org/abs/2510.22200'><img src='https://img.shields.io/badge/Technique-Report-red'></a>\n  <a href='https://huggingface.co/meituan-longcat/LongCat-Video'><img src='https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Model-blue'></a>\n</div>\n\n<div align=\"center\" style=\"line-height: 1;\">\n  <a href='https://github.com/meituan-longcat/LongCat-Flash-Chat/blob/main/figures/wechat_official_accounts.png'><img src='https://img.shields.io/badge/WeChat-LongCat-brightgreen?logo=wechat&logoColor=white'></a>  \n  <a href='https://x.com/Meituan_LongCat'><img src='https://img.shields.io/badge/Twitter-LongCat-white?logo=x&logoColor=white'></a>\n</div>\n\n<div align=\"center\" style=\"line-height: 1;\">\n  <a href='LICENSE'><img src='https://img.shields.io/badge/License-MIT-f5de53?&color=f5de53'></a>\n</div>\n\n## Model Introduction\nWe introduce LongCat-Video, a foundational video generation model with 13.6B parameters, delivering strong performance across *Text-to-Video*, *Image-to-Video*, and *Video-Continuation* generation tasks. It particularly excels in efficient and high-quality long video generation, representing our first step toward world models.\n\n### Key Features\n- ğŸŒŸ **Unified architecture for multiple tasks**: LongCat-Video unifies *Text-to-Video*, *Image-to-Video*, and *Video-Continuation* tasks within a single video generation framework. It natively supports all these tasks with a single model and consistently delivers strong performance across each individual task.\n- ğŸŒŸ **Long video generation**: LongCat-Video is natively pretrained on *Video-Continuation* tasks, enabling it to produce minutes-long videos without color drifting or quality degradation.\n- ğŸŒŸ **Efficient inference**: LongCat-Video generates $720p$, $30fps$ videos within minutes by employing a coarse-to-fine generation strategy along both the temporal and spatial axes. Block Sparse Attention further enhances efficiency, particularly at high resolutions\n- ğŸŒŸ **Strong performance with multi-reward RLHF**: Powered by multi-reward Group Relative Policy Optimization (GRPO), comprehensive evaluations on both internal and public benchmarks demonstrate that LongCat-Video achieves performance comparable to leading open-source video generation models as well as the latest commercial solutions.\n\nFor more detail, please refer to the comprehensive [***LongCat-Video Technical Report***](https://arxiv.org/abs/2510.22200).\n\n## ğŸ¥ Teaser Video\n\n<div align=\"center\">\n  <video src=\"https://github.com/user-attachments/assets/00fa63f0-9c4e-461a-a79e-c662ad596d7d\" width=\"2264\" height=\"384\"> </video>\n</div>\n\n## Quick Start\n\n### Installation\n\nClone the repo:\n\n```shell\ngit clone --single-branch --branch main https://github.com/meituan-longcat/LongCat-Video\ncd LongCat-Video\n```\n\nInstall dependencies:\n\n```shell\n# create conda environment\nconda create -n longcat-video python=3.10\nconda activate longcat-video\n\n# install torch (configure according to your CUDA version)\npip install torch==2.6.0+cu124 torchvision==0.21.0+cu124 torchaudio==2.6.0 --index-url https://download.pytorch.org/whl/cu124\n\n# install flash-attn-2\npip install ninja \npip install psutil \npip install packaging \npip install flash_attn==2.7.4.post1\n\n# install other requirements\npip install -r requirements.txt\n```\n\nFlashAttention-2 is enabled in the model config by default; you can also change the model config (\"./weights/LongCat-Video/dit/config.json\") to use FlashAttention-3 or xformers once installed.\n\n### Model Download\n\n| Models | Download Link |\n| --- | --- |\n| LongCat-Video | ğŸ¤— [Huggingface](https://huggingface.co/meituan-longcat/LongCat-Video) |\n\nDownload models using huggingface-cli:\n```shell\npip install \"huggingface_hub[cli]\"\nhuggingface-cli download meituan-longcat/LongCat-Video --local-dir ./weights/LongCat-Video\n```\n\n### Run Text-to-Video\n\n```shell\n# Single-GPU inference\ntorchrun run_demo_text_to_video.py --checkpoint_dir=./weights/LongCat-Video --enable_compile\n\n# Multi-GPU inference\ntorchrun --nproc_per_node=2 run_demo_text_to_video.py --context_parallel_size=2 --checkpoint_dir=./weights/LongCat-Video --enable_compile\n```\n\n### Run Image-to-Video\n\n```shell\n# Single-GPU inference\ntorchrun run_demo_image_to_video.py --checkpoint_dir=./weights/LongCat-Video --enable_compile\n\n# Multi-GPU inference\ntorchrun --nproc_per_node=2 run_demo_image_to_video.py --context_parallel_size=2 --checkpoint_dir=./weights/LongCat-Video --enable_compile\n```\n\n### Run Video-Continuation\n\n```shell\n# Single-GPU inference\ntorchrun run_demo_video_continuation.py --checkpoint_dir=./weights/LongCat-Video --enable_compile\n\n# Multi-GPU inference\ntorchrun --nproc_per_node=2 run_demo_video_continuation.py --context_parallel_size=2 -",
      "default_branch": "main"
    },
    "fetched_at": "2025-11-23T02:53:28.069312"
  },
  {
    "basic_info": {
      "name": "JiT",
      "full_name": "LTH14/JiT",
      "owner": "LTH14",
      "description": "PyTorch implementation of JiT https://arxiv.org/abs/2511.13720",
      "url": "https://github.com/LTH14/JiT",
      "clone_url": "https://github.com/LTH14/JiT.git",
      "ssh_url": "git@github.com:LTH14/JiT.git",
      "homepage": "",
      "created_at": "2025-11-10T22:37:40Z",
      "updated_at": "2025-11-23T02:33:55Z",
      "pushed_at": "2025-11-18T03:24:51Z"
    },
    "stats": {
      "stars": 1081,
      "forks": 40,
      "watchers": 1081,
      "open_issues": 12,
      "size": 67601
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 56577
      },
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "## Just image Transformer (JiT) for Pixel-space Diffusion\n\n[![arXiv](https://img.shields.io/badge/arXiv%20paper-2511.13720-b31b1b.svg)](https://arxiv.org/abs/2511.13720)&nbsp;\n\n<p align=\"center\">\n  <img src=\"demo/visual.jpg\" width=\"100%\">\n</p>\n\n\nThis is a PyTorch/GPU re-implementation of the paper [Back to Basics: Let Denoising Generative Models Denoise](https://arxiv.org/abs/2511.13720):\n\n```\n@article{li2025jit,\n  title={Back to Basics: Let Denoising Generative Models Denoise},\n  author={Li, Tianhong and He, Kaiming},\n  journal={arXiv preprint arXiv:2511.13720},\n  year={2025}\n}\n```\n\nJiT adopts a minimalist and self-contained design for pixel-level high-resolution image diffusion. \nThe original implementation was in JAX+TPU. This re-implementation is in PyTorch+GPU.\n\n<p align=\"center\">\n  <img src=\"demo/jit.jpg\" width=\"40%\">\n</p>\n\n### Dataset\nDownload [ImageNet](http://image-net.org/download) dataset, and place it in your `IMAGENET_PATH`.\n\n### Installation\n\nDownload the code:\n```\ngit clone https://github.com/LTH14/JiT.git\ncd JiT\n```\n\nA suitable [conda](https://conda.io/) environment named `jit` can be created and activated with:\n\n```\nconda env create -f environment.yaml\nconda activate jit\n```\n\nIf you get ```undefined symbol: iJIT_NotifyEvent``` when importing ```torch```, simply\n```\npip uninstall torch\npip install torch==2.5.1 --index-url https://download.pytorch.org/whl/cu124\n```\nCheck this [issue](https://github.com/conda/conda/issues/13812#issuecomment-2071445372) for more details.\n\n### Training\nThe below training scripts have been tested on 8 H200 GPUs.\n\nExample script for training JiT-B/16 on ImageNet 256x256 for 600 epochs:\n```\ntorchrun --nproc_per_node=8 --nnodes=1 --node_rank=0 \\\nmain_jit.py \\\n--model JiT-B/16 \\\n--proj_dropout 0.0 \\\n--P_mean -0.8 --P_std 0.8 \\\n--img_size 256 --noise_scale 1.0 \\\n--batch_size 128 --blr 5e-5 \\\n--epochs 600 --warmup_epochs 5 \\\n--gen_bsz 128 --num_images 50000 --cfg 2.9 --interval_min 0.1 --interval_max 1.0 \\\n--output_dir ${OUTPUT_DIR} --resume ${OUTPUT_DIR} \\\n--data_path ${IMAGENET_PATH} --online_eval\n```\n\nExample script for training JiT-B/32 on ImageNet 512x512 for 600 epochs:\n```\ntorchrun --nproc_per_node=8 --nnodes=1 --node_rank=0 \\\nmain_jit.py \\\n--model JiT-B/32 \\\n--proj_dropout 0.0 \\\n--P_mean -0.8 --P_std 0.8 \\\n--img_size 512 --noise_scale 2.0 \\\n--batch_size 128 --blr 5e-5 \\\n--epochs 600 --warmup_epochs 5 \\\n--gen_bsz 128 --num_images 50000 --cfg 2.9 --interval_min 0.1 --interval_max 1.0 \\\n--output_dir ${OUTPUT_DIR} --resume ${OUTPUT_DIR} \\\n--data_path ${IMAGENET_PATH} --online_eval\n```\n\nExample script for training JiT-H/16 on ImageNet 256x256 for 600 epochs:\n```\ntorchrun --nproc_per_node=8 --nnodes=1 --node_rank=0 \\\nmain_jit.py \\\n--model JiT-H/16 \\\n--proj_dropout 0.2 \\\n--P_mean -0.8 --P_std 0.8 \\\n--img_size 256 --noise_scale 1.0 \\\n--batch_size 128 --blr 5e-5 \\\n--epochs 600 --warmup_epochs 5 \\\n--gen_bsz 128 --num_images 50000 --cfg 2.2 --interval_min 0.1 --interval_max 1.0 \\\n--output_dir ${OUTPUT_DIR} --resume ${OUTPUT_DIR} \\\n--data_path ${IMAGENET_PATH} --online_eval\n```\n\n### Evaluation\n\nEvaluate a trained JiT:\n```\ntorchrun --nproc_per_node=8 --nnodes=1 --node_rank=0 \\\nmain_jit.py \\\n--model JiT-B/16 \\\n--img_size 256 --noise_scale 1.0 \\\n--gen_bsz 128 --num_images 50000 --cfg 2.9 --interval_min 0.1 --interval_max 1.0 \\\n--output_dir ${CKPT_DIR} --resume ${CKPT_DIR} \\\n--data_path ${IMAGENET_PATH} --evaluate_gen\n```\n\nWe use a customized [```torch-fidelity```](https://github.com/LTH14/torch-fidelity)\nto evaluate FID and IS against a reference image folder or statistics. You can use ```prepare_ref.py```\nto prepare the reference image folder, or directly use our pre-computed reference stats\nunder ```fid_stats```.\n\n### Acknowledgements\n\nWe thank Google TPU Research Cloud (TRC) for granting us access to TPUs, and the MIT\nORCD Seed Fund Grants for supporting GPU resources.\n\n### Contact\n\nIf you have any questions, feel free to contact me through email (tianhong@mit.edu). Enjoy!\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-11-23T02:53:29.341452"
  },
  {
    "basic_info": {
      "name": "karpathy",
      "full_name": "K-Dense-AI/karpathy",
      "owner": "K-Dense-AI",
      "description": "An agentic Machine Learning Engineer",
      "url": "https://github.com/K-Dense-AI/karpathy",
      "clone_url": "https://github.com/K-Dense-AI/karpathy.git",
      "ssh_url": "git@github.com:K-Dense-AI/karpathy.git",
      "homepage": "https://k-dense.ai",
      "created_at": "2025-11-16T22:39:26Z",
      "updated_at": "2025-11-23T02:52:12Z",
      "pushed_at": "2025-11-20T17:55:09Z"
    },
    "stats": {
      "stars": 941,
      "forks": 95,
      "watchers": 941,
      "open_issues": 1,
      "size": 20
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 10658
      },
      "license": "MIT License",
      "topics": [
        "agentic-ai",
        "automl",
        "machine-learning"
      ]
    },
    "content": {
      "readme": "# Karpathy\n\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n[![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg)](https://github.com/K-Dense-AI/karpathy/pulls)\n\nAn agentic Machine Learning Engineer that trains state-of-the-art ML models using Claude Code SDK and Google ADK. This is a very simple implemenation demonstraing the power of Claude Scientific Skills for machine learning.\n\n## Prerequisites\n\n- Python 3.13 or higher\n- [uv](https://github.com/astral-sh/uv) package manager\n- Claude Code installed and authenticated (see [installation guide](https://www.claude.com/product/claude-code))\n\n## Setup\n\n### 1. Clone the Repository\n\n```bash\ngit clone https://github.com/K-Dense-AI/karpathy.git\ncd karpathy\n```\n\n### 2. Install Dependencies\n\nInstall dependencies using `uv`:\n\n```bash\nuv sync\n```\n\n### 3. Environment Variables\n\nCreate a `.env` file in the `karpathy` directory with your API keys:\n\n```bash\nOPENROUTER_API_KEY=your_openrouter_api_key_here\nAGENT_MODEL=your_model_name_here\n```\n\nThe `OPENROUTER_API_KEY` is required for the agent to function properly.\n\nThis is the same environment variable that will be copied to the `sandbox` directory so the agents can use any API keys you provide here.\n\n## Quick Start\n\nRun the startup script to set up the sandbox and start the ADK web interface:\n\n```bash\npython start.py\n```\n\nThis automatically:\n1. Creates a `sandbox` directory with scientific skills from Claude Scientific Skills\n2. Sets up a Python virtual environment with ML packages (PyTorch, transformers, scikit-learn, etc.)\n3. Copies your `.env` file to the sandbox\n4. Starts the ADK web interface\n5. Navigate to **http://localhost:8000** in your browser\n6. Select `karpathy` in the top left under 'Select an agent'\n7. All outputs will be in the `sandbox` directory so continue to monitor that as you converse with the agent\n\n**Note:** Any files you want the agent to use (datasets, scripts, etc.) should be manually added to the `sandbox` directory.\n\n## Community\n\nJoin our K-Dense Slack community to connect with other users, share ideas, and get support:\n\n**[Join K-Dense Slack Community](https://join.slack.com/t/k-densecommunity/shared_invite/zt-3iajtyls1-EwmkwIZk0g_o74311Tkf5g)**\n\n## Claude Scientific Skills\n\nThis repository is designed to work with the **[Claude Scientific Skills](https://github.com/K-Dense-AI/claude-scientific-skills)** collection of ready-to-use scientific tools and workflows ([link](https://github.com/K-Dense-AI/claude-scientific-skills)). The `start.py` setup script creates a `sandbox` that includes scientific skills from this collection so the `karpathy` agent can leverage specialized ML libraries and scientific workflows. For full details on the skills themselves, see the upstream repositoryâ€™s README and documentation [here](https://github.com/K-Dense-AI/claude-scientific-skills).\n\n## Manual Usage\n\nTo set up the sandbox without starting the web interface:\n\n```bash\npython -m karpathy.utils\n```\n\n**Note:** Any files you want the agent to use (datasets, scripts, etc.) should be manually added to the `sandbox` directory.\n\nTo run the ADK web interface manually:\n\n```bash\nadk web\n```\n\nThen navigate to **http://localhost:8000** in your browser.\n\n## Enhanced ML Capabilities\n\nIf you want substantially more powerful ML capabilities through a multi-agentic system, sign up for [www.k-dense.ai](https://www.k-dense.ai). Currently in closed beta, launching publicly in December 2025.\n\n## Upcoming Features\n\n- **Modal sandbox integration** - Choose any type of compute you want\n- **K-Dense Web features** - We might make some features from K-Dense Web available here based on interest\n\n## Star History\n\n[![Star History Chart](https://api.star-history.com/svg?repos=K-Dense-AI/karpathy&type=date&legend=top-left)](https://www.star-history.com/#K-Dense-AI/karpathy&type=date&legend=top-left)\n\n## Disclaimer\n\nThis project is **not** endorsed by or affiliated with Andrej Karpathy. The name is used as a tribute and out of deep respect for his contributions to AI and technical leadership.",
      "default_branch": "main"
    },
    "fetched_at": "2025-11-23T02:53:30.633464"
  },
  {
    "basic_info": {
      "name": "MuMuAINovel",
      "full_name": "xiamuceer-j/MuMuAINovel",
      "owner": "xiamuceer-j",
      "description": "ä¸€æ¬¾åŸºäº AI çš„æ™ºèƒ½å°è¯´åˆ›ä½œåŠ©æ‰‹ï¼Œå¸®åŠ©ä½ è½»æ¾åˆ›ä½œç²¾å½©æ•…äº‹",
      "url": "https://github.com/xiamuceer-j/MuMuAINovel",
      "clone_url": "https://github.com/xiamuceer-j/MuMuAINovel.git",
      "ssh_url": "git@github.com:xiamuceer-j/MuMuAINovel.git",
      "homepage": "https://mumuverse.space:1566/",
      "created_at": "2025-10-30T01:24:24Z",
      "updated_at": "2025-11-22T21:25:50Z",
      "pushed_at": "2025-11-22T10:23:42Z"
    },
    "stats": {
      "stars": 692,
      "forks": 140,
      "watchers": 692,
      "open_issues": 5,
      "size": 4476
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 999120,
        "TypeScript": 623969,
        "CSS": 2496,
        "Dockerfile": 2371,
        "JavaScript": 621,
        "HTML": 367
      },
      "license": "GNU General Public License v3.0",
      "topics": []
    },
    "content": {
      "readme": "# MuMuAINovel ğŸ“šâœ¨\n\n<div align=\"center\">\n\n![Version](https://img.shields.io/badge/version-1.0.0-blue.svg)\n![Python](https://img.shields.io/badge/python-3.11-blue.svg)\n![FastAPI](https://img.shields.io/badge/FastAPI-0.109.0-green.svg)\n![React](https://img.shields.io/badge/react-18.3.1-blue.svg)\n![License](https://img.shields.io/badge/license-GPL%20v3-blue.svg)\n\n**åŸºäº AI çš„æ™ºèƒ½å°è¯´åˆ›ä½œåŠ©æ‰‹**\n\n[ç‰¹æ€§](#-ç‰¹æ€§) â€¢ [å¿«é€Ÿå¼€å§‹](#-å¿«é€Ÿå¼€å§‹) â€¢ [é…ç½®è¯´æ˜](#%EF%B8%8F-é…ç½®è¯´æ˜) â€¢ [é¡¹ç›®ç»“æ„](#-é¡¹ç›®ç»“æ„)\n\n</div>\n\n---\n\n## âœ¨ ç‰¹æ€§\n\n- ğŸ¤– **å¤š AI æ¨¡å‹** - æ”¯æŒ OpenAIã€Geminiã€Claude ç­‰ä¸»æµæ¨¡å‹\n- ğŸ“ **æ™ºèƒ½å‘å¯¼** - AI è‡ªåŠ¨ç”Ÿæˆå¤§çº²ã€è§’è‰²å’Œä¸–ç•Œè§‚\n- ğŸ‘¥ **è§’è‰²ç®¡ç†** - äººç‰©å…³ç³»ã€ç»„ç»‡æ¶æ„å¯è§†åŒ–ç®¡ç†\n- ğŸ“– **ç« èŠ‚ç¼–è¾‘** - æ”¯æŒåˆ›å»ºã€ç¼–è¾‘ã€é‡æ–°ç”Ÿæˆå’Œæ¶¦è‰²\n- ğŸŒ **ä¸–ç•Œè§‚è®¾å®š** - æ„å»ºå®Œæ•´çš„æ•…äº‹èƒŒæ™¯\n- ğŸ” **å¤šç§ç™»å½•** - LinuxDO OAuth æˆ–æœ¬åœ°è´¦æˆ·ç™»å½•\n- ğŸ’¾ **PostgreSQL** - ç”Ÿäº§çº§æ•°æ®åº“ï¼Œå¤šç”¨æˆ·æ•°æ®éš”ç¦»\n- ğŸ³ **Docker éƒ¨ç½²** - ä¸€é”®å¯åŠ¨ï¼Œå¼€ç®±å³ç”¨\n\n## ğŸ“‹ TODO List\n\n- [x] **çµæ„Ÿæ¨¡å¼** - åˆ›ä½œçµæ„Ÿå’Œç‚¹å­ç”Ÿæˆ\n- [x] **è‡ªå®šä¹‰å†™ä½œé£æ ¼** - æ”¯æŒè‡ªå®šä¹‰ AI å†™ä½œé£æ ¼\n- [x] **æ•°æ®å¯¼å…¥å¯¼å‡º** - é¡¹ç›®æ•°æ®çš„å¯¼å…¥å¯¼å‡º\n- [ ] **Prompt è°ƒæ•´ç•Œé¢** - å¯è§†åŒ–ç¼–è¾‘ Prompt æ¨¡æ¿\n- [x] **ç« èŠ‚å­—æ•°é™åˆ¶** - ç”¨æˆ·å¯è®¾ç½®ç”Ÿæˆå­—æ•°\n- [ ] **è®¾å®šè¿½æº¯ä¸çŸ›ç›¾æ£€æµ‹** - è‡ªåŠ¨æ£€æµ‹è®¾å®šå†²çª\n- [ ] **æ€ç»´é“¾ä¸ç« èŠ‚å…³ç³»å›¾è°±** - å¯è§†åŒ–ç« èŠ‚é€»è¾‘å…³ç³»\n- [x] **æ ¹æ®åˆ†æä¸€é”®é‡å†™** - æ ¹æ®åˆ†æå»ºè®®é‡æ–°ç”Ÿæˆ\n- [x] **Linux DO è‡ªåŠ¨åˆ›å»ºè´¦å·** - OAuth ç™»å½•è‡ªåŠ¨ç”Ÿæˆè´¦å·\n\n> ğŸ’¡ æ¬¢è¿æäº¤ Issue æˆ– Pull Requestï¼\n\n## ğŸš€ å¿«é€Ÿå¼€å§‹\n\n### å‰ç½®è¦æ±‚\n\n- Docker å’Œ Docker Compose\n- è‡³å°‘ä¸€ä¸ª AI æœåŠ¡çš„ API Keyï¼ˆOpenAI/Gemini/Claudeï¼‰\n\n### Docker Compose éƒ¨ç½²ï¼ˆæ¨èï¼‰\n\n```bash\n# 1. å…‹éš†é¡¹ç›®\ngit clone https://github.com/xiamuceer-j/MuMuAINovel.git\ncd MuMuAINovel\n\n# 2. é…ç½®ç¯å¢ƒå˜é‡ï¼ˆå¿…éœ€ï¼‰\ncp backend/.env.example .env\n# ç¼–è¾‘ .env æ–‡ä»¶ï¼Œå¡«å…¥å¿…è¦é…ç½®ï¼ˆAPI Keyã€æ•°æ®åº“å¯†ç ç­‰ï¼‰\n\n# 3. ç¡®ä¿æ–‡ä»¶å‡†å¤‡å®Œæ•´\n# âš ï¸ é‡è¦ï¼šç¡®ä¿ä»¥ä¸‹æ–‡ä»¶å­˜åœ¨\n# - .envï¼ˆé…ç½®æ–‡ä»¶ï¼Œå¿…éœ€æŒ‚è½½åˆ°å®¹å™¨ï¼‰\n# - backend/scripts/init_postgres.sqlï¼ˆæ•°æ®åº“åˆå§‹åŒ–è„šæœ¬ï¼‰\n\n# 4. å¯åŠ¨æœåŠ¡\ndocker-compose up -d\n\n# 5. è®¿é—®åº”ç”¨\n# æ‰“å¼€æµè§ˆå™¨è®¿é—® http://localhost:8000\n```\n\n> **ğŸ“Œ æ³¨æ„äº‹é¡¹**\n>\n> 1. **`.env` æ–‡ä»¶æŒ‚è½½**: `docker-compose.yml` ä¼šè‡ªåŠ¨å°† `.env` æŒ‚è½½åˆ°å®¹å™¨ï¼Œç¡®ä¿æ–‡ä»¶å­˜åœ¨\n> 2. **æ•°æ®åº“åˆå§‹åŒ–**: `init_postgres.sql` ä¼šåœ¨é¦–æ¬¡å¯åŠ¨æ—¶è‡ªåŠ¨æ‰§è¡Œï¼Œå®‰è£…å¿…è¦çš„PostgreSQLæ‰©å±•\n> 3. **è‡ªè¡Œæ„å»º**: å¦‚éœ€ä»æºç æ„å»ºï¼Œè¯·å…ˆä¸‹è½½ embedding æ¨¡å‹æ–‡ä»¶ï¼ˆ[åŠ ç¾¤è·å–](frontend/public/qq.jpg)ï¼‰\n\n### ä½¿ç”¨ Docker Hub é•œåƒï¼ˆæ¨èæ–°æ‰‹ï¼‰\n\n```bash\n# 1. æ‹‰å–æœ€æ–°é•œåƒï¼ˆå·²åŒ…å«æ¨¡å‹æ–‡ä»¶ï¼‰\ndocker pull mumujie/mumuainovel:latest\n\n# 2. é…ç½® .env æ–‡ä»¶\ncp backend/.env.example .env\n# ç¼–è¾‘ .env å¡«å…¥é…ç½®\n\n# 3. å¯åŠ¨æœåŠ¡\ndocker-compose up -d\n\n# 4. æŸ¥çœ‹æ—¥å¿—\ndocker-compose logs -f\n\n# 5. æ›´æ–°åˆ°æœ€æ–°ç‰ˆæœ¬\ndocker-compose pull\ndocker-compose up -d\n```\n\n> **ğŸ’¡ æç¤º**: Docker Hub é•œåƒå·²åŒ…å«æ‰€æœ‰ä¾èµ–å’Œæ¨¡å‹æ–‡ä»¶ï¼Œæ— éœ€é¢å¤–ä¸‹è½½\n\n### æœ¬åœ°å¼€å‘ / ä»æºç æ„å»º\n\n#### å‰ç½®å‡†å¤‡\n\n```bash\n# âš ï¸ é‡è¦ï¼šå¦‚æœä»æºç æ„å»ºï¼Œéœ€è¦å…ˆä¸‹è½½ embedding æ¨¡å‹æ–‡ä»¶\n# æ¨¡å‹æ–‡ä»¶è¾ƒå¤§ï¼ˆçº¦ 400MBï¼‰ï¼Œéœ€æ”¾ç½®åˆ°ä»¥ä¸‹ç›®å½•ï¼š\n# backend/embedding/models--sentence-transformers--paraphrase-multilingual-MiniLM-L12-v2/\n#\n# ğŸ“¥ è·å–æ–¹å¼ï¼š\n# - åŠ å…¥é¡¹ç›® QQ ç¾¤æˆ– Linux DO è®¨è®ºåŒºè·å–ä¸‹è½½é“¾æ¥\n# - ç¾¤å·ï¼šè§é¡¹ç›®ä¸»é¡µ\n# - Linux DOï¼šhttps://linux.do/t/topic/1100112\n```\n\n#### åç«¯\n\n```bash\ncd backend\npython -m venv .venv\nsource .venv/bin/activate  # Windows: .venv\\Scripts\\activate\npip install -r requirements.txt\n\n# é…ç½® .env æ–‡ä»¶\ncp .env.example .env\n# ç¼–è¾‘ .env å¡«å…¥å¿…è¦é…ç½®\n\n# å¯åŠ¨ PostgreSQLï¼ˆå¯ä½¿ç”¨ Dockerï¼‰\ndocker run -d --name postgres \\\n  -e POSTGRES_PASSWORD=your_password \\\n  -e POSTGRES_DB=mumuai_novel \\\n  -p 5432:5432 \\\n  postgres:18-alpine\n\n# å¯åŠ¨åç«¯\npython -m uvicorn app.main:app --host 0.0.0.0 --port 8000 --reload\n```\n\n#### å‰ç«¯\n\n```bash\ncd frontend\nnpm install\nnpm run dev  # å¼€å‘æ¨¡å¼\nnpm run build  # ç”Ÿäº§æ„å»º\n```\n\n## âš™ï¸ é…ç½®è¯´æ˜\n\n### å¿…éœ€é…ç½®\n\nåˆ›å»º `.env` æ–‡ä»¶ï¼š\n\n```bash\n# PostgreSQL æ•°æ®åº“ï¼ˆå¿…éœ€ï¼‰\nDATABASE_URL=postgresql+asyncpg://mumuai:your_password@postgres:5432/mumuai_novel\nPOSTGRES_PASSWORD=your_secure_password\n\n# AI æœåŠ¡ï¼ˆè‡³å°‘é…ç½®ä¸€ä¸ªï¼‰\nOPENAI_API_KEY=your_openai_key\nOPENAI_BASE_URL=https://api.openai.com/v1\nDEFAULT_AI_PROVIDER=openai\nDEFAULT_MODEL=gpt-4o-mini\n\n# æœ¬åœ°è´¦æˆ·ç™»å½•\nLOCAL_AUTH_ENABLED=true\nLOCAL_AUTH_USERNAME=admin\nLOCAL_AUTH_PASSWORD=your_password\n```\n\n### å¯é€‰é…ç½®\n\n```bash\n# Gemini\nGEMINI_API_KEY=your_gemini_key\n\n# Claude\nANTHROPIC_API_KEY=your_claude_key\n\n# LinuxDO OAuth\nLINUXDO_CLIENT_ID=your_client_id\nLINUXDO_CLIENT_SECRET=your_client_secret\nLINUXDO_REDIRECT_URI=http://localhost:8000/api/auth/callback\n\n# PostgreSQL è¿æ¥æ± ï¼ˆé«˜å¹¶å‘ä¼˜åŒ–ï¼‰\nDATABASE_POOL_SIZE=30\nDATABASE_MAX_OVERFLOW=20\n```\n\n### ä¸­è½¬ API é…ç½®\n\næ”¯æŒæ‰€æœ‰ OpenAI å…¼å®¹æ ¼å¼çš„ä¸­è½¬æœåŠ¡ï¼š\n\n```bash\n# New API ç¤ºä¾‹\nOPENAI_API_KEY=sk-xxxxxxxx\nOPENAI_BASE_URL=https://api.new-api.com/v1\n\n# å…¶ä»–ä¸­è½¬æœåŠ¡\nOPENAI_BASE_URL=https://your-proxy-service.com/v1\n```\n\n## ğŸ³ Docker éƒ¨ç½²è¯¦æƒ…\n\n### æœåŠ¡æ¶æ„\n\n- **postgres**: PostgreSQL 18 æ•°æ®åº“\n  - ç«¯å£: 5432\n  - æ•°æ®æŒä¹…åŒ–: `postgres_data` volume\n  - åˆå§‹åŒ–è„šæœ¬: `backend/scripts/init_postgres.sql`ï¼ˆè‡ªåŠ¨æŒ‚è½½ï¼‰\n  - ä¼˜åŒ–é…ç½®: æ”¯æŒ 80-150 å¹¶å‘ç”¨æˆ·\n\n- **mumuainovel**: ä¸»åº”ç”¨æœåŠ¡\n  - ç«¯å£: 8000\n  - æ—¥å¿—ç›®å½•: `./logs`\n  - é…ç½®æŒ‚è½½: `.env` æ–‡ä»¶\n  - è‡ªåŠ¨ç­‰å¾…æ•°æ®åº“å°±ç»ª\n  - å¥åº·æ£€æŸ¥: æ¯ 30 ç§’æ£€æµ‹ä¸€æ¬¡\n\n### é‡è¦æ–‡ä»¶è¯´æ˜\n\n| æ–‡ä»¶ | è¯´æ˜ | æ˜¯å¦å¿…éœ€ |\n|------|------|---------|\n| `.env` | ç¯å¢ƒé…ç½®ï¼ˆAPI Keyã€æ•°æ®åº“å¯†ç ç­‰ï¼‰ | âœ… å¿…éœ€ |\n| `docker-compose.yml` | æœåŠ¡ç¼–æ’é…ç½® | âœ… å¿…éœ€ |\n| `backend/scripts/init_postgres.sql` | PostgreSQL æ‰©å±•å®‰è£…è„šæœ¬ | âœ… è‡ªåŠ¨æŒ‚è½½ |\n| `backend/embedding/models--*/` | Embedding æ¨¡å‹æ–‡ä»¶ | âš ï¸ è‡ªå»ºéœ€è¦ |\n\n> **æ³¨æ„**: ä½¿ç”¨ Docker Hub é•œåƒæ—¶ï¼Œæ¨¡å‹æ–‡ä»¶å·²åŒ…å«åœ¨é•œåƒä¸­ï¼Œæ— éœ€é¢å¤–ä¸‹è½½\n\n### å¸¸ç”¨å‘½ä»¤\n\n```bash\n# å¯åŠ¨æœåŠ¡\ndocker-compose up -d\n\n# æŸ¥çœ‹çŠ¶æ€\ndocker-compose ps\n\n# æŸ¥çœ‹æ—¥å¿—\ndocker-compose logs -f\n\n# åœæ­¢æœåŠ¡\ndocker-compose down\n\n# é‡å¯æœåŠ¡\ndocker-compose restart\n\n# æŸ¥çœ‹èµ„æºä½¿ç”¨\ndocker stats\n```\n\n### æ•°æ®æŒä¹…åŒ–\n\n- `./postgres_data` - PostgreSQL æ•°æ®åº“æ–‡ä»¶\n- `./logs` - åº”ç”¨æ—¥å¿—æ–‡ä»¶\n\n### ç«¯å£é…ç½®\n\nä¿®æ”¹ `docker-compose.yml` ä¸­çš„ç«¯å£æ˜ å°„ï¼š\n\n```yaml\nports:\n  - \"8800:8000\"  # å®¿ä¸»æœº:å®¹å™¨\n```\n\n## ğŸ“ é¡¹ç›®ç»“æ„\n\n```\nMuMuAINovel/\nâ”œâ”€â”€ backend/                 # åç«¯æœåŠ¡\nâ”‚   â”œâ”€â”€ app/\nâ”‚   â”‚   â”œâ”€â”€ api/            # API è·¯ç”±\nâ”‚   ",
      "default_branch": "main"
    },
    "fetched_at": "2025-11-23T02:53:31.945748"
  },
  {
    "basic_info": {
      "name": "codeassist",
      "full_name": "gensyn-ai/codeassist",
      "owner": "gensyn-ai",
      "description": "A completely private and local AI coding assistant, developed by Gensyn. It helps you practice programming problems and train a novel assistant to help you code.",
      "url": "https://github.com/gensyn-ai/codeassist",
      "clone_url": "https://github.com/gensyn-ai/codeassist.git",
      "ssh_url": "git@github.com:gensyn-ai/codeassist.git",
      "homepage": null,
      "created_at": "2025-10-31T14:03:09Z",
      "updated_at": "2025-11-22T10:26:35Z",
      "pushed_at": "2025-11-13T14:23:42Z"
    },
    "stats": {
      "stars": 671,
      "forks": 115,
      "watchers": 671,
      "open_issues": 13,
      "size": 23696
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 500590,
        "TypeScript": 265570,
        "CSS": 6326,
        "Dockerfile": 4578,
        "JavaScript": 2247,
        "Nix": 842,
        "Shell": 157
      },
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "\n# CodeAssist - AI Programming Assistant\n\nCodeAssist is a completely private and local AI coding assistant, developed by Gensyn. It helps you practice programming problems and train a novel assistant to help you code.\n\nUnlike typical code assistants, CodeAssist writes directly in your editor as you work. Every keystroke - whether you type, fix, delete, or leave its output untouched - becomes a learning signal. Over time, it adapts to your habits and style, acting more like an apprentice learning from your craft than a tool following commands.\n\n[Docs](https://docs.gensyn.ai/testnet/codeassist) | [Tutorial](https://docs.gensyn.ai/testnet/codeassist/using-codeassist) | [Leaderboard](https://dashboard.gensyn.ai/?application=CodeAssist)\n\n# Installation\n\nGet started with installing CodeAssist.\n\n## Docker\n\nInstall [Docker](https://docs.docker.com/engine/install/) on your system, according to the instructions for your machine.\n\n## Python\n\nPython is required to run the main script that handles your environment. We require a version no older than 3.10.\n\n## UV\n\nUV is required to manage the dependencies of the main script. It can be installed with the following steps:\n\n### MacOS\n\n```bash\nbrew install uv\n```\n\n### Linux (or alternate MacOS install, for those without Brew)\n\n```bash\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n```\n\n# Downloading the Code\n\nTo download the code, simply clone the repository:\n\n```bash\ngit clone https://github.com/gensyn-ai/codeassist.git\ncd codeassist\n```\n\n# Running\n\nTo run CodeAssist, simply execute the following command:\n\n```bash\nuv run run.py\n```\n\n## HuggingFace Token\n\nTo start CodeAssist, you will need to have a HuggingFace token. Follow [these instructions](https://huggingface.co/docs/hub/en/security-tokens) and generate a token with `Write` access.\n\n## Web UI\n\nAfter the script is running, your browser should open automatically but if it doesn't, open a window and go to [localhost:3000](http://localhost:3000) to open CodeAssist.\n\nWhen the web UI loads, you'll see a login modal where you can log in with email (which sends a one-time passcode) or with Google. After logging in for the first time, your local credentials will be stored in `persistent-data/auth/userKeyMap.json`.\n\nOnce logged in, you can select Easy, Medium, or Hard problems from the sidebar. CodeAssist will begin recording an episode. Every click, keystroke, edit, or deletion is logged as training feedback.\n\nWhen you stop typing, CodeAssist takes initiative. It writes directly into your file without any pop-ups or confirmations. Whether you accept, modify, or remove its edits, each interaction contributes to the modelâ€™s understanding of your preferences.\n\n### Tips & Tricks\n\n- Use `Shift+Space` or click the Pause Assistant button to temporarily stop the assistant. The first keystroke after pausing will unpause it.\n- Keep your cursor near the section you're working on, as CodeAssist inserts code relative to your cursor position.\n- When the assistant produces a \"No-Op\" (does nothing), it's waiting for you. This is intentional and signals it's your turn to act.\n\n## Training\n\nCodeAssist continuously records your interactions while the web UI is running. To complete an episode and train your model, press `Ctrl+C` in the terminal where CodeAssist is running.\n\nYou do not need to successfully solve a LeetCode problem to train the model. You can stop recording the episode by leaving the CodeAssist web UI, returning to the terminal CodeAssist is running in, and using the `ctrl+c` command to start training.\n\nDuring training, CodeAssist will:\n- Compare your edits to the assistant's actions\n- Calculate rewards and penalties based on your interactions\n- Update your local model checkpoint\n- Store new model weights under `persistent-data/trainer/models`\n- Upload your trained model to Hugging Face (if a valid token is provided)\n\nAfter training completes (which takes a few minutes depending on your system), you can restart CodeAssist to use your updated model trained on your most recent episode.\n\n### Best Practices\n\n- **Be patient**: The assistant watches your typing and timing. Working too quickly or aggressively correcting can neutralize training efficacy.\n- **Treat it as a collaborator**: Let it naturally interject code and keep useful code around briefly before editing or removing.\n- **Don't delete everything instantly**: If you delete everything it writes right away, you're teaching it to stop acting altogether.\n- **Record multiple varied problems**: Diversify its learning signals by working on different problems.\n- **Expect gradual improvement**: Early episodes may feel inconsistent. Improvement becomes clearer after 4-5 episodes of training.\n\n# Troubleshooting\n\n## Exception: Container <container-name> is unhealthy.\n\nThis occurs when a container fails to boot. You can view the logs by running `docker logs <container-name>`. Please review and upload the logs when creating a new issue.\n\n## Error connecting to Docker daemon\n\n```\n2025-09-04 ",
      "default_branch": "main"
    },
    "fetched_at": "2025-11-23T02:53:33.199532"
  },
  {
    "basic_info": {
      "name": "Step-Audio-EditX",
      "full_name": "stepfun-ai/Step-Audio-EditX",
      "owner": "stepfun-ai",
      "description": "A powerful 3B-parameter, LLM-based Reinforcement Learning audio edit model excels at editing emotion, speaking style, and paralinguistics, and features robust zero-shot text-to-speech",
      "url": "https://github.com/stepfun-ai/Step-Audio-EditX",
      "clone_url": "https://github.com/stepfun-ai/Step-Audio-EditX.git",
      "ssh_url": "git@github.com:stepfun-ai/Step-Audio-EditX.git",
      "homepage": "",
      "created_at": "2025-10-29T11:54:17Z",
      "updated_at": "2025-11-22T18:55:48Z",
      "pushed_at": "2025-11-22T07:33:40Z"
    },
    "stats": {
      "stars": 671,
      "forks": 43,
      "watchers": 671,
      "open_issues": 21,
      "size": 8496
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 2565728,
        "Cuda": 19692,
        "C": 6731,
        "C++": 4155,
        "Dockerfile": 1346
      },
      "license": "Apache License 2.0",
      "topics": [
        "audio-editing",
        "cross-lingual",
        "emotion-control",
        "paralinguistics",
        "reinforcement-learning",
        "speaking-style",
        "style-control",
        "text-to-speech",
        "tts",
        "voice-cloning",
        "zero-shot-tts"
      ]
    },
    "content": {
      "readme": "# Step-Audio-EditX\n<p align=\"center\">\n  <img src=\"assets/logo.png\"  height=100>\n</p>\n\n<div align=\"center\">\n    <a href=\"https://stepaudiollm.github.io/step-audio-editx/\"><img src=\"https://img.shields.io/static/v1?label=Demo%20Page&message=Web&color=green\"></a> &ensp;\n  <a href=\"https://arxiv.org/abs/2511.03601\"><img src=\"https://img.shields.io/static/v1?label=Tech%20Report&message=Arxiv&color=red\"></a> &ensp;\n  <a href=\"https://huggingface.co/stepfun-ai/Step-Audio-EditX\"><img src=\"https://img.shields.io/static/v1?label=Step-Audio-EditX&message=HuggingFace&color=yellow\"></a> &ensp;\n    <a href=\"https://modelscope.cn/models/stepfun-ai/Step-Audio-EditX\"><img src=\"https://img.shields.io/static/v1?label=Step-Audio-EditX&message=ModelScope&color=blue\"></a> &ensp;\n  <a href=\"https://huggingface.co/spaces/stepfun-ai/Step-Audio-EditX\"><img src=\"https://img.shields.io/static/v1?label=Space%20Playground&message=HuggingFace&color=yellow\"></a> &ensp;\n</div>\n\n## ğŸ”¥ğŸ”¥ğŸ”¥ News!!\n* Nov 19, 2025: âš™ï¸ We release a **new version** of our model, which **supports polyphonic pronunciation control** and improves the performance of emotion, speaking style, and paralinguistic editing.\n* Nov 12, 2025: ğŸ“¦ We release the **optimized inference code** and **model weights** of **Step-Audio-EditX** ([HuggingFace](https://huggingface.co/stepfun-ai/Step-Audio-EditX);  [ModelScope](https://modelscope.cn/models/stepfun-ai/Step-Audio-EditX)) and **Step-Audio-Tokenizer**([HuggingFace](https://huggingface.co/stepfun-ai/Step-Audio-Tokenizer);  [ModelScope](https://modelscope.cn/models/stepfun-ai/Step-Audio-Tokenizer))\n* Nov 07, 2025: âœ¨ [Demo Page](https://stepaudiollm.github.io/step-audio-editx/) ; ğŸ®  [HF Space Playground](https://huggingface.co/spaces/stepfun-ai/Step-Audio-EditX)\n* Nov 06, 2025: ğŸ‘‹ We release the technical report of [Step-Audio-EditX](https://arxiv.org/abs/2511.03601).\n\n## Introduction\nWe are open-sourcing Step-Audio-EditX, a powerful **3B-parameter** LLM-based **Reinforcement Learning** audio model specialized in expressive and iterative audio editing. It excels at editing emotion, speaking style, and paralinguistics, and also features robust zero-shot text-to-speech (TTS) capabilities. \n\n## ğŸ“‘ Open-source Plan\n- [x] Inference Code\n- [x] Online demo (Gradio)\n- [ ] Step-Audio-Edit-Benchmark\n- [x] Model Checkpoints\n  - [x] Step-Audio-Tokenizer\n  - [x] Step-Audio-EditX\n  - [ ] Step-Audio-EditX-Int4\n- [ ] Training Code\n  - [ ] SFT training\n  - [ ] PPO training\n- [ ] â³ Feature Support Plan\n  - [ ] Editing\n    - [x] Polyphone pronunciation control\n    - [ ] More paralinguistic tags ([Cough, Crying, Stress, etc.])\n    - [ ] Filler word removal\n  - [ ] Other Languages\n    - [ ] Japanese, Korean, Arabic, French, Russian, Spanish, etc.\n  \n## Features\n- **Zero-Shot TTS**\n  - Excellent zero-shot TTS cloning for Mandarin, English, Sichuanese, and Cantonese.\n  - To use a dialect, just add a **[Sichuanese]** or **[Cantonese]** tag before your text.\n  - ğŸ”¥ Polyphone pronunciation control, all you need to do is replace the polyphonic characters with pinyin.\n    - **[æˆ‘ä¹Ÿæƒ³è¿‡è¿‡è¿‡å„¿è¿‡è¿‡çš„ç”Ÿæ´»]** -> **[æˆ‘ä¹Ÿæƒ³guo4guo4guo1å„¿guo4guo4çš„ç”Ÿæ´»]**\n \n    \n- **Emotion and Speaking Style Editing**\n  - Remarkably effective iterative control over emotions and styles, supporting **dozens** of options for editing.\n    - Emotion Editing : [ *Angry*, *Happy*, *Sad*, *Excited*, *Fearful*, *Surprised*, *Disgusted*, etc. ]\n    - Speaking Style Editing: [ *Act_coy*, *Older*, *Child*, *Whisper*, *Serious*, *Generous*, *Exaggerated*, etc.]\n    - Editing with more emotion and more speaking styles is on the way. **Get Ready!** ğŸš€\n    \n\n- **Paralinguistic Editing**\n  -  Precise control over 10 types of paralinguistic features for more natural, human-like, and expressive synthetic audio.\n  - Supporting Tags:\n    - [ *Breathing*, *Laughter*, *Suprise-oh*, *Confirmation-en*, *Uhm*, *Suprise-ah*, *Suprise-wa*, *Sigh*, *Question-ei*, *Dissatisfaction-hnn* ]\n\n- **Available Tags**\n<table>\n  <tr>\n    <td rowspan=\"8\" style=\"vertical-align: middle; text-align:center;\" align=\"center\">emotion</td>\n    <td align=\"center\"><b>happy</b></td>\n    <td align=\"center\">Expressing happiness</td>\n    <td align=\"center\"><b>angry</b></td>\n    <td align=\"center\">Expressing anger</td>\n  </tr>\n  <tr>\n    <td align=\"center\"><b>sad</b></td>\n    <td align=\"center\">Expressing sadness</td>\n    <td align=\"center\"><b>fear</b></td>\n    <td align=\"center\">Expressing fear</td>\n  </tr>\n  <tr>\n    <td align=\"center\"><b>surprised</b></td>\n    <td align=\"center\">Expressing surprise</td>\n    <td align=\"center\"><b>confusion</b></td>\n    <td align=\"center\">Expressing confusion</td>\n  </tr>\n  <tr>\n    <td align=\"center\"><b>empathy</b></td>\n    <td align=\"center\">Expressing empathy and understanding</td>\n    <td align=\"center\"><b>embarrass</b></td>\n    <td align=\"center\">Expressing embarrassment</td>\n  </tr>\n  <tr>\n    <td align=\"center\"><b>excited</b></td>\n    <td align=\"center\">Expressing excitement and enthusiasm</td>\n    <td align=\"center\"><b>d",
      "default_branch": "main"
    },
    "fetched_at": "2025-11-23T02:53:34.502230"
  },
  {
    "basic_info": {
      "name": "Mini-Agent",
      "full_name": "MiniMax-AI/Mini-Agent",
      "owner": "MiniMax-AI",
      "description": "A minimal yet professional single agent demo project that showcases the core execution pipeline and production-grade features of agents.",
      "url": "https://github.com/MiniMax-AI/Mini-Agent",
      "clone_url": "https://github.com/MiniMax-AI/Mini-Agent.git",
      "ssh_url": "git@github.com:MiniMax-AI/Mini-Agent.git",
      "homepage": "https://www.minimax.io/",
      "created_at": "2025-10-31T03:56:27Z",
      "updated_at": "2025-11-22T23:42:17Z",
      "pushed_at": "2025-11-18T10:26:15Z"
    },
    "stats": {
      "stars": 668,
      "forks": 90,
      "watchers": 668,
      "open_issues": 6,
      "size": 4282
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 810622,
        "JavaScript": 45621,
        "HTML": 20844,
        "Shell": 15390,
        "PowerShell": 5180
      },
      "license": "MIT License",
      "topics": [
        "agent",
        "llm",
        "minimax"
      ]
    },
    "content": {
      "readme": "# Mini Agent\n\nEnglish | [ä¸­æ–‡](./README_CN.md)\n\n**Mini Agent** is a minimal yet professional demo project that showcases the best practices for building agents with the MiniMax M2 model. Leveraging an Anthropic-compatible API, it fully supports interleaved thinking to unlock M2's powerful reasoning capabilities for long, complex tasks.\n\nThis project comes packed with features designed for a robust and intelligent agent development experience:\n\n*   âœ… **Full Agent Execution Loop**: A complete and reliable foundation with a basic toolset for file system and shell operations.\n*   âœ… **Persistent Memory**: An active **Session Note Tool** ensures the agent retains key information across multiple sessions.\n*   âœ… **Intelligent Context Management**: Automatically summarizes conversation history to handle contexts up to a configurable token limit, enabling infinitely long tasks.\n*   âœ… **Claude Skills Integration**: Comes with 15 professional skills for documents, design, testing, and development.\n*   âœ… **MCP Tool Integration**: Natively supports MCP for tools like knowledge graph access and web search.\n*   âœ… **Comprehensive Logging**: Detailed logs for every request, response, and tool execution for easy debugging.\n*   âœ… **Clean & Simple Design**: A beautiful CLI and a codebase that is easy to understand, making it the perfect starting point for building advanced agents.\n\n## Table of Contents\n\n- [Mini Agent](#mini-agent)\n  - [Table of Contents](#table-of-contents)\n  - [Quick Start](#quick-start)\n    - [1. Get API Key](#1-get-api-key)\n    - [2. Choose Your Usage Mode](#2-choose-your-usage-mode)\n      - [ğŸš€ Quick Start Mode (Recommended for Beginners)](#-quick-start-mode-recommended-for-beginners)\n      - [ğŸ”§ Development Mode](#-development-mode)\n  - [ACP \\& Zed Editor Integration(optional)](#acp--zed-editor-integrationoptional)\n  - [Usage Examples](#usage-examples)\n    - [Task Execution](#task-execution)\n    - [Using a Claude Skill (e.g., PDF Generation)](#using-a-claude-skill-eg-pdf-generation)\n    - [Web Search \\& Summarization (MCP Tool)](#web-search--summarization-mcp-tool)\n  - [Testing](#testing)\n    - [Quick Run](#quick-run)\n    - [Test Coverage](#test-coverage)\n  - [Troubleshooting](#troubleshooting)\n    - [SSL Certificate Error](#ssl-certificate-error)\n    - [Module Not Found Error](#module-not-found-error)\n  - [Related Documentation](#related-documentation)\n  - [Contributing](#contributing)\n  - [License](#license)\n  - [References](#references)\n\n## Quick Start\n\n### 1. Get API Key\n\nMiniMax provides both global and China platforms. Choose based on your network environment:\n\n| Version    | Platform                                                       | API Base                   |\n| ---------- | -------------------------------------------------------------- | -------------------------- |\n| **Global** | [https://platform.minimax.io](https://platform.minimax.io)     | `https://api.minimax.io`   |\n| **China**  | [https://platform.minimaxi.com](https://platform.minimaxi.com) | `https://api.minimaxi.com` |\n\n**Steps to get API Key:**\n1. Visit the corresponding platform to register and login\n2. Go to **Account Management > API Keys**\n3. Click **\"Create New Key\"**\n4. Copy and save it securely (key is only shown once)\n\n> ğŸ’¡ **Tip**: Remember the API Base address corresponding to your chosen platform, you'll need it for configuration\n\n### 2. Choose Your Usage Mode\n\n**Prerequisites: Install uv**\n\nBoth usage modes require uv. If you don't have it installed:\n\n```bash\n# macOS/Linux/WSL\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n\n# Windows (PowerShell)\npython -m pip install --user pipx\npython -m pipx ensurepath\n# Restart PowerShell after installation\n\n# After installation, restart your terminal or run:\nsource ~/.bashrc  # or ~/.zshrc (macOS/Linux)\n```\n\nWe offer two usage modes - choose based on your needs:\n\n#### ğŸš€ Quick Start Mode (Recommended for Beginners)\n\nPerfect for users who want to quickly try Mini Agent without cloning the repository or modifying code.\n\n**Installation:**\n\n```bash\n# 1. Install directly from GitHub\nuv tool install git+https://github.com/MiniMax-AI/Mini-Agent.git\n\n# 2. Run setup script (automatically creates config files)\n# macOS/Linux:\ncurl -fsSL https://raw.githubusercontent.com/MiniMax-AI/Mini-Agent/main/scripts/setup-config.sh | bash\n\n# Windows (PowerShell):\nInvoke-WebRequest -Uri \"https://raw.githubusercontent.com/MiniMax-AI/Mini-Agent/main/scripts/setup-config.ps1\" -OutFile \"$env:TEMP\\setup-config.ps1\"\npowershell -ExecutionPolicy Bypass -File \"$env:TEMP\\setup-config.ps1\"\n```\n\n> ğŸ’¡ **Tip**: If you want to develop locally or modify code, use \"Development Mode\" below\n\n**Configuration:**\n\nThe setup script creates config files in `~/.mini-agent/config/`. Edit the config file:\n\n```bash\n# Edit config file\nnano ~/.mini-agent/config/config.yaml\n```\n\nFill in your API Key and corresponding API Base:\n\n```yaml\napi_key: \"YOUR_API_KEY_HERE\"          # API Key from step 1\napi_base: \"https://api.minimax.io\"  # Global\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-11-23T02:53:35.815095"
  },
  {
    "basic_info": {
      "name": "intelligent-audit-system",
      "full_name": "Ricky-7-Yan/intelligent-audit-system",
      "owner": "Ricky-7-Yan",
      "description": "ğŸ¤– AutoAudit--æ™ºèƒ½å®¡è®¡å†³ç­–ç³»ç»Ÿ Python FastAPI License  åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„æ™ºèƒ½å®¡è®¡å¹³å° | é›†æˆçŸ¥è¯†å›¾è°±ã€RAGã€å¼ºåŒ–å­¦ä¹ ç­‰å‰æ²¿æŠ€æœ¯  åŠŸèƒ½ç‰¹æ€§ â€¢ å¿«é€Ÿå¼€å§‹ â€¢ æŠ€æœ¯æ¶æ„ â€¢ æ–‡æ¡£  ğŸ“‹ é¡¹ç›®ç®€ä»‹ æ™ºèƒ½å®¡è®¡å†³ç­–ç³»ç»Ÿæ˜¯ä¸€ä¸ªåŸºäºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ™ºèƒ½å®¡è®¡å¹³å°ï¼Œé›†æˆäº†çŸ¥è¯†å›¾è°±ã€RAGæ£€ç´¢å¢å¼ºç”Ÿæˆã€å¼ºåŒ–å­¦ä¹ ç­‰å‰æ²¿æŠ€æœ¯ï¼Œä¸ºå®¡è®¡å·¥ä½œæä¾›æ™ºèƒ½åŒ–æ”¯æŒã€‚  ğŸ¯ æ ¸å¿ƒä»·å€¼: çªç ´ä¼ ç»Ÿå®¡è®¡å·¥å…·å±€é™ï¼Œæ”¯æŒå¤æ‚ä¸šåŠ¡é€»è¾‘çš„æ·±åº¦æ¨ç†",
      "url": "https://github.com/Ricky-7-Yan/intelligent-audit-system",
      "clone_url": "https://github.com/Ricky-7-Yan/intelligent-audit-system.git",
      "ssh_url": "git@github.com:Ricky-7-Yan/intelligent-audit-system.git",
      "homepage": "",
      "created_at": "2025-10-28T16:14:23Z",
      "updated_at": "2025-11-23T02:52:31Z",
      "pushed_at": "2025-10-30T10:20:31Z"
    },
    "stats": {
      "stars": 646,
      "forks": 54,
      "watchers": 646,
      "open_issues": 1,
      "size": 123
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 131202,
        "HTML": 57179,
        "Shell": 2162,
        "Batchfile": 2075
      },
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "# ğŸ¤– AutoAudit--æ™ºèƒ½å®¡è®¡å†³ç­–ç³»ç»Ÿ\r\n\r\n<div align=\"center\">\r\n\r\n![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)\r\n![FastAPI](https://img.shields.io/badge/FastAPI-0.104+-green.svg)\r\n![License](https://img.shields.io/badge/License-MIT-yellow.svg)\r\n\r\n**åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„æ™ºèƒ½å®¡è®¡å¹³å° | é›†æˆçŸ¥è¯†å›¾è°±ã€RAGã€å¼ºåŒ–å­¦ä¹ ç­‰å‰æ²¿æŠ€æœ¯**\r\n\r\n[åŠŸèƒ½ç‰¹æ€§](#-åŠŸèƒ½ç‰¹æ€§) â€¢ [å¿«é€Ÿå¼€å§‹](#-å¿«é€Ÿå¼€å§‹) â€¢ [æŠ€æœ¯æ¶æ„](#-æŠ€æœ¯æ¶æ„) â€¢ [æ–‡æ¡£](Project_Summary.md)\r\n\r\n</div>\r\n\r\n---\r\n\r\n## ğŸ“‹ é¡¹ç›®ç®€ä»‹\r\n\r\nAutoAuditæ˜¯ä¸€ä¸ªåŸºäºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ™ºèƒ½å®¡è®¡å¹³å°ï¼Œé›†æˆäº†çŸ¥è¯†å›¾è°±ã€RAGæ£€ç´¢å¢å¼ºç”Ÿæˆã€å¼ºåŒ–å­¦ä¹ ç­‰å‰æ²¿æŠ€æœ¯ï¼Œä¸ºå®¡è®¡å·¥ä½œæä¾›æ™ºèƒ½åŒ–æ”¯æŒã€‚\r\n\r\n> ğŸ¯ **æ ¸å¿ƒä»·å€¼**: çªç ´ä¼ ç»Ÿå®¡è®¡å·¥å…·å±€é™ï¼Œæ”¯æŒå¤æ‚ä¸šåŠ¡é€»è¾‘çš„æ·±åº¦æ¨ç†\r\n\r\n---\r\n\r\n## âœ¨ åŠŸèƒ½ç‰¹æ€§\r\n\r\n### ğŸ¤– æ™ºèƒ½å¯¹è¯Agent\r\n- âœ… åŸºäºLangChainçš„å¤šè½®å¯¹è¯å¼å®¡è®¡å†³ç­–\r\n- âœ… ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„è¿ç»­å¯¹è¯\r\n- âœ… ä¸“ä¸šæ¨ç†å’Œå®¡è®¡å»ºè®®\r\n\r\n### ğŸ—ºï¸ çŸ¥è¯†å›¾è°±\r\n- âœ… æ•´åˆCOBITã€ISO27001ã€SOXç­‰å®¡è®¡æ ‡å‡†\r\n- âœ… å®ä½“è¯†åˆ«å’Œå…³ç³»æŠ½å–\r\n- âœ… åŠ¨æ€çŸ¥è¯†æ›´æ–°\r\n\r\n### ğŸ” Agentic RAGç³»ç»Ÿ\r\n- âœ… æ™ºèƒ½æ£€ç´¢å¢å¼ºç”Ÿæˆ\r\n- âœ… å‘é‡æ•°æ®åº“æ”¯æŒ\r\n- âœ… è¯­ä¹‰æœç´¢å’Œç­”æ¡ˆç”Ÿæˆ\r\n\r\n### âš ï¸ é£é™©è¯„ä¼°\r\n- âœ… è‡ªåŠ¨è¯†åˆ«å’Œè¯„ä¼°å®¡è®¡é£é™©\r\n- âœ… é£é™©ç­‰çº§è¯„åˆ†\r\n- âœ… å®æ—¶é£é™©ç›‘æ§\r\n\r\n### âœ… åˆè§„æ£€æŸ¥\r\n- âœ… å¯¹ç…§æ ‡å‡†è¿›è¡Œåˆè§„æ€§æ£€æŸ¥\r\n- âœ… å¤šæ ‡å‡†æ”¯æŒï¼ˆCOBITã€ISO27001ã€SOXï¼‰\r\n- âœ… æ•´æ”¹å»ºè®®ç”Ÿæˆ\r\n\r\n### ğŸ“ æ¨¡å‹è®­ç»ƒ\r\n- âœ… æ”¯æŒSFTã€RLHFç­‰è®­ç»ƒæ–¹æ³•\r\n- âœ… LoRAé«˜æ•ˆå¾®è°ƒ\r\n- âœ… Benchmarkæµ‹è¯„ç³»ç»Ÿ\r\n\r\n---\r\n\r\n## ğŸš€ å¿«é€Ÿå¼€å§‹\r\n\r\n### ğŸ“¦ ç¯å¢ƒè¦æ±‚\r\n\r\n- Python 3.8+ ğŸ\r\n- MySQL 8.0+ ğŸ—„ï¸\r\n- Neo4j 5.0+ ğŸ•¸ï¸\r\n- 8GB+ RAM ğŸ’¾\r\n\r\n### ğŸƒ ä¸€é”®å¯åŠ¨ï¼ˆæ¨èï¼‰\r\n\r\n#### Windows ç”¨æˆ·\r\n```bash\r\n# åŒå‡»è¿è¡Œ\r\nstart.bat\r\n```\r\n\r\n#### Linux/Mac ç”¨æˆ·\r\n```bash\r\n# èµ‹äºˆæ‰§è¡Œæƒé™\r\nchmod +x start.sh\r\n\r\n# è¿è¡Œå¯åŠ¨è„šæœ¬\r\n./start.sh\r\n```\r\n\r\n### ğŸ“ æ‰‹åŠ¨å®‰è£…\r\n\r\n#### 1. å…‹éš†é¡¹ç›®\r\n```bash\r\ngit clone <repository-url>\r\ncd intelligent-audit-system\r\n```\r\n\r\n#### 2. åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ\r\n```bash\r\npython -m venv venv\r\nsource venv/bin/activate  # Linux/Mac\r\n# æˆ–\r\nvenv\\Scripts\\activate  # Windows\r\n```\r\n\r\n#### 3. å®‰è£…ä¾èµ–\r\n```bash\r\npip install -r requirements.txt\r\n```\r\n\r\n#### 4. å®‰è£…spaCyæ¨¡å‹\r\n```bash\r\npython -m spacy download en_core_web_sm\r\n```\r\n\r\n#### 5. é…ç½®ç¯å¢ƒå˜é‡\r\n```bash\r\ncp config.env.example config.env\r\n# ç¼–è¾‘config.envï¼Œå¡«å…¥ç›¸åº”é…ç½®\r\n```\r\n\r\n#### 6. åˆå§‹åŒ–æ•°æ®åº“\r\n```bash\r\n# åˆå§‹åŒ–MySQL\r\npython database/init_db.py\r\n\r\n# åˆå§‹åŒ–Neo4j\r\npython knowledge_graph/neo4j_init.py\r\n```\r\n\r\n#### 7. å¯åŠ¨ç³»ç»Ÿ\r\n```bash\r\npython web/main.py\r\n```\r\n\r\n### ğŸŒ è®¿é—®ç³»ç»Ÿ\r\n\r\næ‰“å¼€æµè§ˆå™¨è®¿é—®: **http://localhost:8000**\r\n\r\n---\r\n\r\n## ğŸ® åŠŸèƒ½æ¨¡å—\r\n\r\n### ğŸ’¬ æ™ºèƒ½å¯¹è¯\r\n- ğŸ¯ è®¿é—® `/chat` \r\n- ğŸ’¡ ä¸AIå®¡è®¡åŠ©æ‰‹è¿›è¡Œä¸“ä¸šå¯¹è¯\r\n- ğŸ”„ å¤šè½®å¯¹è¯æ”¯æŒ\r\n\r\n### ğŸ” å®¡è®¡åˆ†æ\r\n- ğŸ¯ è®¿é—® `/audit`\r\n- âš¡ è‡ªåŠ¨é£é™©è¯„ä¼°å’Œåˆè§„æ£€æŸ¥\r\n- ğŸ“Š å¯è§†åŒ–ç»“æœå±•ç¤º\r\n\r\n### ğŸ“š çŸ¥è¯†ç®¡ç†\r\n- ğŸ¯ è®¿é—® `/knowledge`\r\n- ğŸ“– æ„å»ºå’Œç»´æŠ¤å®¡è®¡çŸ¥è¯†åº“\r\n- ğŸ—ºï¸ çŸ¥è¯†å›¾è°±å¯è§†åŒ–\r\n\r\n### ğŸ“ æ¨¡å‹è®­ç»ƒ\r\n- ğŸ¯ è®¿é—® `/training`\r\n- ğŸ‹ï¸ SFTå’ŒRLHFè®­ç»ƒ\r\n- ğŸ“ˆ æ€§èƒ½è¯„ä¼°å’Œå¯¹æ¯”\r\n\r\n---\r\n\r\n## ğŸ“¡ APIæ¥å£\r\n\r\n### ğŸ’¬ èŠå¤©API\r\n```http\r\nPOST /api/chat\r\nContent-Type: application/json\r\n\r\n{\r\n    \"message\": \"è¯·å¯¹ERPç³»ç»Ÿè¿›è¡Œå®‰å…¨å®¡è®¡\",\r\n    \"session_id\": \"optional_session_id\"\r\n}\r\n```\r\n\r\n### ğŸ” å®¡è®¡API\r\n```http\r\nPOST /api/audit\r\nContent-Type: application/json\r\n\r\n{\r\n    \"audit_item\": \"ERPç³»ç»Ÿ\",\r\n    \"audit_type\": \"å®‰å…¨å®¡è®¡\",\r\n    \"standard_type\": \"COBIT\",\r\n    \"risk_level\": \"é«˜\"\r\n}\r\n```\r\n\r\n---\r\n\r\n## ğŸ“ é¡¹ç›®ç»“æ„\r\n\r\n```\r\nintelligent-audit-system/\r\nâ”œâ”€â”€ agents/                 # æ™ºèƒ½Agentæ¨¡å—\r\nâ”‚   â””â”€â”€ audit_agent.py\r\nâ”œâ”€â”€ knowledge_graph/        # çŸ¥è¯†å›¾è°±æ¨¡å—\r\nâ”‚   â”œâ”€â”€ builder.py\r\nâ”‚   â””â”€â”€ neo4j_init.py\r\nâ”œâ”€â”€ rag/                    # RAGç³»ç»Ÿæ¨¡å—\r\nâ”‚   â””â”€â”€ agentic_rag.py\r\nâ”œâ”€â”€ training/               # è®­ç»ƒæ¨¡å—\r\nâ”‚   â””â”€â”€ training_pipeline.py\r\nâ”œâ”€â”€ web/                    # Webç•Œé¢\r\nâ”‚   â””â”€â”€ main.py\r\nâ”œâ”€â”€ templates/              # HTMLæ¨¡æ¿\r\nâ”‚   â”œâ”€â”€ index.html         # ä¸»é¡µ\r\nâ”‚   â”œâ”€â”€ chat.html          # èŠå¤©é¡µé¢\r\nâ”‚   â”œâ”€â”€ audit.html         # å®¡è®¡åˆ†æ\r\nâ”‚   â”œâ”€â”€ knowledge.html     # çŸ¥è¯†ç®¡ç†\r\nâ”‚   â””â”€â”€ training.html      # æ¨¡å‹è®­ç»ƒ\r\nâ”œâ”€â”€ database/               # æ•°æ®åº“æ¨¡å—\r\nâ”‚   â””â”€â”€ init_db.py\r\nâ”œâ”€â”€ config.py               # é…ç½®æ–‡ä»¶\r\nâ”œâ”€â”€ requirements.txt        # ä¾èµ–åˆ—è¡¨\r\nâ””â”€â”€ start.py                # å¯åŠ¨è„šæœ¬\r\n```\r\n\r\n---\r\n\r\n## âš™ï¸ é…ç½®è¯´æ˜\r\n\r\n### ğŸ”§ ç¯å¢ƒå˜é‡\r\n\r\nç¼–è¾‘ `config.env` æ–‡ä»¶ï¼š\r\n\r\n```env\r\n# MySQLé…ç½®\r\nMYSQL_HOST=localhost\r\nMYSQL_PORT=3306\r\nMYSQL_USER=your-username\r\nMYSQL_PASSWORD=your-password\r\nMYSQL_DATABASE=audit_system\r\n\r\n# Neo4jé…ç½®\r\nNEO4J_URI=bolt://localhost:7687\r\nNEO4J_USER=your-username\r\nNEO4J_PASSWORD=your-password\r\n\r\n# LLM APIé…ç½®\r\nQWEN_API_KEY=your_api_key\r\nQWEN_BASE_URL=https://dashscope.aliyuncs.com/compatible-mode/v1\r\n```\r\n\r\n---\r\n\r\n## ğŸ“¥ æ¨¡å‹ä¸‹è½½\r\n\r\nå¦‚æœéœ€è¦ä½¿ç”¨RAGåŠŸèƒ½ï¼Œéœ€è¦ä¸‹è½½Sentence Transformersæ¨¡å‹ï¼š\r\n\r\n### ğŸŒŸ ä½¿ç”¨é•œåƒç«™ï¼ˆæ¨èï¼‰\r\n\r\n```bash\r\n# Windows PowerShell\r\n$env:HF_ENDPOINT=\"https://hf-mirror.com\"\r\n\r\n# Linux/Mac\r\nexport HF_ENDPOINT=https://hf-mirror.com\r\n\r\n# å¯åŠ¨ç³»ç»Ÿï¼Œä¼šè‡ªåŠ¨ä¸‹è½½\r\npython web/main.py\r\n```\r\n\r\nè¯¦ç»†è¯´æ˜è¯·æŸ¥çœ‹ï¼š[ğŸ“¥ ä¸‹è½½æ¨¡å‹è¯´æ˜.md](ä¸‹è½½æ¨¡å‹è¯´æ˜.md)\r\n\r\n---\r\n\r\n## â“ å¸¸è§é—®é¢˜\r\n\r\n### â“ æ¨¡å‹ä¸‹è½½å¤±è´¥ï¼Ÿ\r\n- âœ… ä½¿ç”¨é•œåƒç«™: è®¾ç½® `HF_ENDPOINT=https://hf-mirror.com`\r\n- âœ… æˆ–æ‰‹åŠ¨ä¸‹è½½æ¨¡å‹åˆ° `./models/` ç›®å½•\r\n\r\n### â“ æ•°æ®åº“è¿æ¥å¤±è´¥ï¼Ÿ\r\n- âœ… ç¡®ä¿MySQLå’ŒNeo4jæœåŠ¡å·²å¯åŠ¨\r\n- âœ… æ£€æŸ¥é…ç½®ä¿¡æ¯æ˜¯å¦æ­£ç¡®\r\n\r\n### â“ APIè°ƒç”¨å¤±è´¥ï¼Ÿ\r\n- âœ… æ£€æŸ¥APIå¯†é’¥æ˜¯å¦æ­£ç¡®\r\n- âœ… éªŒè¯ç½‘ç»œè¿æ¥\r\n\r\næ›´å¤šé—®é¢˜è¯·æŸ¥çœ‹ï¼š[ğŸ“š PyCharmé…ç½®æŒ‡å—](PyCharm_Setup_Guide.md)\r\n\r\n---\r\n\r\n## ğŸ“Š æ€§èƒ½æŒ‡æ ‡\r\n\r\n- ğŸ¯ **å‡†ç¡®ç‡**: 85%+ çš„å®¡è®¡å»ºè®®å‡†ç¡®æ€§\r\n- âš¡ **å“åº”æ—¶é—´**: <3ç§’\r\n- ğŸ‘¥ **å¹¶å‘æ”¯æŒ**: 100+ å¹¶å‘ç”¨æˆ·\r\n- ğŸ’ª **å¯ç”¨æ€§**: 99.9% ç³»ç»Ÿå¯ç”¨æ€§\r\n\r\n---\r\n\r\n## ğŸ¤ è´¡çŒ®æŒ‡å—\r\n\r\næ¬¢è¿è´¡çŒ®ä»£ç ï¼è¯·éµå¾ªä»¥ä¸‹æ­¥éª¤ï¼š\r\n\r\n1. ğŸ´ Fork é¡¹ç›®\r\n2. ğŸŒ¿ åˆ›å»ºç‰¹æ€§åˆ†æ”¯ (`git checkout -b feature/AmazingFeature`)\r\n3. ğŸ’¾ æäº¤æ›´æ”¹ (`git commit -m 'Add some AmazingFeature'`)\r\n4. ğŸ“¤ æ¨é€åˆ°åˆ†æ”¯ (`git push origin feature/AmazingFeature`)\r\n5. ğŸ”„ å¼€å¯ Pull Request\r\n\r\n---\r\n\r\n## ğŸ“„ è®¸å¯è¯\r\n\r\næœ¬é¡¹ç›®é‡‡ç”¨ **MIT License** - æŸ¥çœ‹ [LICENSE](LICENSE) æ–‡ä»¶äº†è§£è¯¦æƒ…\r\n\r\n---\r\n\r\n## ğŸ‘¥ è”ç³»æˆ‘ä»¬\r\n\r\nå¦‚æœ‰é—®é¢˜æˆ–å»ºè®®ï¼Œè¯·æäº¤ [Issue](https://github.com/your-username/intelligent-audit-system/issues)\r\n\r\n---\r\n\r\n<div align=\"center\">\r\n\r\n**â­ å¦‚æœè¿™ä¸ªé¡¹ç›®å¯¹ä½ æœ‰å¸®åŠ©ï¼Œè¯·ç»™å®ƒä¸€ä¸ªæ˜Ÿæ˜Ÿï¼**\r\n\r\nMade with â¤ï¸ by AutoAudit Team\r\n\r\n</div>\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-11-23T02:53:37.085223"
  },
  {
    "basic_info": {
      "name": "lejepa",
      "full_name": "rbalestr-lab/lejepa",
      "owner": "rbalestr-lab",
      "description": null,
      "url": "https://github.com/rbalestr-lab/lejepa",
      "clone_url": "https://github.com/rbalestr-lab/lejepa.git",
      "ssh_url": "git@github.com:rbalestr-lab/lejepa.git",
      "homepage": null,
      "created_at": "2025-11-11T14:29:00Z",
      "updated_at": "2025-11-23T00:16:25Z",
      "pushed_at": "2025-11-20T16:38:29Z"
    },
    "stats": {
      "stars": 620,
      "forks": 45,
      "watchers": 620,
      "open_issues": 6,
      "size": 4034
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 274017
      },
      "license": "Other",
      "topics": []
    },
    "content": {
      "readme": "# LeJEPA\n**Lean Joint-Embedding Predictive Architecture (LeJEPA): Provable and Scalable Self-Supervised Learning Without the Heuristics**\n[GitHub Repository](https://github.com/rbalestr-lab/lejepa)  \n[arXiv:2511.08544](https://arxiv.org/abs/2511.08544)\n---\n\nRush to our [minimal working example](MINIMAL.md) to see a full-fledge working example (ViT, inet).\n\n## Demo\n\n<img src=\"eval/output1.gif\" controls width=\"400\">\n<img src=\"eval/output2.gif\" controls width=\"400\">\n<img src=\"eval/output3.gif\" controls width=\"400\">\n<table>\n  <tr>\n    <td><img src=\"eval/n01818515_919_original.png\" width=\"200\"/></td>\n    <td><img src=\"eval/n01818515_919_pca.png\" width=\"200\"/></td>\n  </tr>\n  <tr>\n    <td><img src=\"eval/n01818515_14304_original.png\" width=\"200\"/></td>\n    <td><img src=\"eval/n01818515_14304_pca.png\" width=\"200\"/></td>\n  </tr>\n</table>\n\n| shots | model                  | params | pretrain | epochs | DTD      | aircr.   | cars     | cifar10  | cifar100 | flowers102 | food     | pets     | avg.    |\n|-------|------------------------|--------|----------|--------|----------|----------|----------|----------|----------|------------|----------|----------|---------|\n| 1     | LeJEPA ViT-L           | 304M   | IN-1K    | 100    | **33.21**| 9.37     | 3.40     | 51.65    | 27.01    | 48.53      | 17.14    | 46.11    | 29.55   |\n| 1     | LeJEPA ConvNeXtV2-H    | 660M   | IN-1K    | 100    | 32.15    | 8.07     | 4.28     | 50.95    | **31.48**| **48.74**  | **17.95**| **58.98**| **31.58**|\n| 1     | I-JEPA ViT-H           | 632M   | IN-1K    | 300    | 27.71    | **9.86** | **4.33** | **56.52**| 30.58    | 44.69      | 14.53    | 53.38    | 30.20   |\n| 10    | LeJEPA ViT-L           | 304M   | IN-1K    | 100    | **64.72**| **35.25**| 22.25    | 85.15    | 59.77    | **92.53**  | **50.90**| 77.00    | **60.95**|\n| 10    | LeJEPA ConvNeXtV2-H    | 660M   | IN-1K    | 100    | 61.84    | 30.67    | **24.46**| 85.74    | 63.29    | 91.78      | 49.32    | 78.53    | 60.70   |\n| 10    | I-JEPA ViT-H           | 632M   | IN-1K    | 300    | 57.68    | 33.82    | 21.96    | **88.77**| **66.42**| 88.24      | 43.97    | **83.23**| 60.51   |\n| all   | LeJEPA ViT-L           | 304M   | IN-1K    | 100    | **78.30**| 57.01    | **57.28**| 96.50    | 83.71    | **91.21**  | **82.05**| 89.74    | **79.48**|\n| all   | LeJEPA ConvNeXtV2-H    | 660M   | IN-1K    | 100    | 76.60    | 52.99    | 54.88    | 96.15    | 81.34    | 91.11      | 77.64    | 89.76    | 77.56   |\n| all   | I-JEPA ViT-H           | 632M   | IN-1K    | 300    | 73.32    | **56.61**| 54.47    | **97.54**| **86.42**| 86.47      | 81.02    | **92.11**| 78.50   |\n\n## Overview\nLeJEPA is a lean, scalable, and theoretically grounded framework for self-supervised representation learning, based on Joint-Embedding Predictive Architectures (JEPAs). LeJEPA introduces **Sketched Isotropic Gaussian Regularization (SIGReg)**, a novel objective that constrains learned embeddings to an optimal isotropic Gaussian distribution, minimizing downstream prediction risk.\n**Key Features:**\n- Single trade-off hyperparameter\n- Linear time and memory complexity\n- Stable training across architectures and domains\n- Heuristics-free implementation (no stop-gradient, teacherâ€“student, or schedulers)\n- Distributed training-friendly codebase (~50 lines of core code)\n- State-of-the-art results across 10+ datasets and 60+ architectures\n---\n\n## GOTO hyperparameters\n\n\nOur data augmentation strategy follows a multi-crop approach inspired by DINO, where we generate multiple views of each image at different scales to encourage the model to learn both global semantic information and local fine-grained features.\n\n### Data augmentation and views\n\nEach training image is augmented to produce **2 global views** and **6 local views** with different spatial scales but the same set of color and geometric transformations:\n| **Global Views** | **Local Views** |\n|------------------|-----------------|\n| **RandomResizedCrop**<br>- Resolution: 224x224<br>- Scale: (0.3, 1.0)<br>- Covers 30-100% of the image | **RandomResizedCrop**<br>- Resolution: 98x98<br>- Scale: (0.05, 0.3)<br>- Covers 5-30% of the image |\n| **RandomHorizontalFlip** (p=0.5) | **RandomHorizontalFlip** (p=0.5) |\n| **ColorJitter** (p=0.8)<br>- Brightness: 0.4<br>- Contrast: 0.4<br>- Saturation: 0.2<br>- Hue: 0.1 | **ColorJitter** (p=0.8)<br>- Brightness: 0.4<br>- Contrast: 0.4<br>- Saturation: 0.2<br>- Hue: 0.1 |\n| **RandomGrayscale** (p=0.2) | **RandomGrayscale** (p=0.2) |\n| **GaussianBlur** (p=0.5) | **GaussianBlur** (p=0.5) |\n| **RandomSolarize** (p=0.2, threshold=128) | **RandomSolarize** (p=0.2, threshold=128) |\n| **Normalization** (mean, std) | **Normalization** (mean, std) |\n\n\nThe key difference between global and local views is the **cropping scale**: global views capture larger portions of the image to learn high-level semantics, while local views focus on smaller regions to learn fine-grained local patterns. All other augmentations are applied iden",
      "default_branch": "main"
    },
    "fetched_at": "2025-11-23T02:53:38.359053"
  },
  {
    "basic_info": {
      "name": "calm",
      "full_name": "shaochenze/calm",
      "owner": "shaochenze",
      "description": "Official implementation of \"Continuous Autoregressive Language Models\"",
      "url": "https://github.com/shaochenze/calm",
      "clone_url": "https://github.com/shaochenze/calm.git",
      "ssh_url": "git@github.com:shaochenze/calm.git",
      "homepage": null,
      "created_at": "2025-10-30T15:45:14Z",
      "updated_at": "2025-11-23T02:25:02Z",
      "pushed_at": "2025-11-10T04:22:00Z"
    },
    "stats": {
      "stars": 606,
      "forks": 74,
      "watchers": 606,
      "open_issues": 3,
      "size": 4494
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 208745,
        "Shell": 8517
      },
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "# CALM: Continuous Autoregressive Language Models\n[![arXiv](https://img.shields.io/badge/arXiv-2510.27688-b31b1b.svg?logo=arXiv)](https://arxiv.org/abs/2510.27688)\n[![code](https://img.shields.io/badge/Github-Code-keygen.svg?logo=github)](https://github.com/shaochenze/calm)\n[![model](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging_Face-Model-blue.svg)](https://huggingface.co/collections/cccczshao/calm)\n[![Blog Post](https://img.shields.io/badge/Blog%20Post-Read%20Here-blue)](https://shaochenze.github.io/blog/2025/CALM)\n\n## Overview\n<p align=\"center\">\n  <img src=\"overview.png\" width=\"700\">\n  <br>\n  <em></em>\n</p>\n\nModern Large Language Models (LLMs) are constrained by a fundamental bottleneck: they generate text one token at a time. **CALM (Continuous Autoregressive Language Models)** confronts this challenge by introducing a paradigm shift in language modeling. Instead of predicting one discrete token at a time, CALM learns to predict a single **continuous vector** that represents an entire chunk of **K** tokens. \n\nThis is achieved through a two-stage process:\n1.  **A high-fidelity autoencoder** learns to compress K tokens into a single vector and reconstruct them with near-perfect accuracy.\n2.  **A continuous-domain language model** then performs autoregressive prediction in this vector space.\n\nAn in-depth explanation of CALM is available in [this blog](https://shaochenze.github.io/blog/2025/CALM).\n\n### Key Features\n\n*   ğŸš€ **Ultra-Efficient by Design:** Dramatically improves training and inference efficiency by reducing the number of autoregressive steps by a factor of K. \n\n*   ğŸ’¡ **A New Scaling Axis:** Introduces a new scaling dimension for LLMsâ€”**semantic bandwidth (K)**. Instead of just scaling parameters and data, you can now scale the amount of information processed in a single step.\n\n*   ğŸ› ï¸ **A Comprehensive Likelihood-Free Toolkit:** Operating in a continuous domain requires new tools. This repository provides the full suite of algorithms that make CALM possible:\n    *   **A Robust Autoencoder** to learn high-fidelity continuous representations of token chunks.\n    *   **Energy-Based Training**, a principled and likelihood-free method for generative modeling.\n    *   **BrierLM**, a new metric for calibrated, likelihood-free evaluation of language models.\n    *   **Temperature Sampling** for controlled, high-quality text generation using only a black-box sampler.\n\n## Getting Started\n\n1.  Clone the Repository\n```bash\ngit clone https://github.com/shaochenze/calm.git\ncd calm\n```\n\n2.  Install Dependencies\n```bash\npip install -r requirements.txt\n```\n\n3.  Prepare the Training Data\n\nRun the following script to download and process the [pile-uncopyrighted](https://huggingface.co/datasets/monology/pile-uncopyrighted) dataset for training.\n```bash\nbash data/get_data.sh\n```\nThe dataset is large. Please ensure you have at least **2.5TB** of free disk space.\n\n## Training\n\nTo replicate the results for CALM with `K=4`, follow these steps. The training process is divided into two main stages: train the autoencoder and then train the CALM language model.\n\n### 1. Train the Autoencoder\n\nFirst, train the autoencoder on approximately 15B tokens of data. This model learns the mapping between token chunks and their continuous vector representations.\n\n```bash\nbash train/train_autoencoder.sh\n```\n\n<details>\n<summary>Click to see the full training script</summary>\n\n```bash\n#!/bin/bash\n\nWORK_PATH=/path/to/the/code\nCHECKPOINT_PATH=${WORK_PATH}/checkpoints/autoencoder\nTOKENIZER_PATH=${WORK_PATH}/llama3_tokenizer\nDATASET_TRAIN=${WORK_PATH}/pile-uncopyrighted/train/00.text.jsonl,${WORK_PATH}/pile-uncopyrighted/train/01.text.jsonl\nDATASET_VALID=${WORK_PATH}/data/wikitext_document_level-test.json\n\ntorchrun --nnodes 1 --node_rank 0 --nproc_per_node 8 \\\n    -m train.train_autoencoder \\\n    --tokenizer_name $TOKENIZER_PATH \\\n    --config_overrides \"latent_size=128,num_encoder_layers=2,num_decoder_layers=2,patch_size=4\" \\\n    --train_file $DATASET_TRAIN \\\n    --validation_file $DATASET_VALID \\\n    --keep_linebreaks True \\\n    --weight_decay 0.1 \\\n    --warmup_steps 1000 \\\n    --block_size 2048 \\\n    --adam_beta1 0.9 \\\n    --adam_beta2 0.95 \\\n    --max_grad_norm 1.0 \\\n    --streaming \\\n    --seed 1 \\\n    --per_device_train_batch_size 8 \\\n    --per_device_eval_batch_size 1 \\\n    --gradient_accumulation_steps 4 \\\n    --num_train_epochs 1 \\\n    --max_steps 30000 \\\n    --save_strategy \"steps\" \\\n    --save_steps 10000 \\\n    --evaluation_strategy \"steps\" \\\n    --eval_steps 1000 \\\n    --learning_rate 3e-4 \\\n    --lr_scheduler_type \"constant\" \\\n    --logging_steps 100 \\\n    --do_train \\\n    --do_eval \\\n    --save_safetensors False \\\n    --output_dir $CHECKPOINT_PATH \\\n    --overwrite_output_dir \\\n    --bf16 True\n```\n</details>\n\n### 2. Train the CALM Language Model\n\nOnce the autoencoder is trained, you can train the CALM model on the remaining data using our proposed **energy loss**. During evaluation steps, the **BrierLM score** is computed t",
      "default_branch": "main"
    },
    "fetched_at": "2025-11-23T02:53:39.644979"
  },
  {
    "basic_info": {
      "name": "Video-Materials-AutoGEN-Workstation",
      "full_name": "Norsico/Video-Materials-AutoGEN-Workstation",
      "owner": "Norsico",
      "description": "ä¸€ä¸ªé›†å†…å®¹ç­–åˆ’ã€AIæ–‡æ¡ˆè‡ªåŠ¨ç”Ÿæˆã€TTS æ‰¹é‡è‡ªåŠ¨é…éŸ³ã€(AI)å›¾ç‰‡ç´ æåˆæˆã€ASRè‡ªåŠ¨æå–è¯­è¨€å­—å¹•è„šæœ¬ã€AIè‡ªç”±åˆ›ä½œäºä¸€ä½“çš„(çŸ­è§†é¢‘)ç”Ÿæˆå·¥ä½œç«™ã€‚æ–¹ä¾¿ç®¡ç†æ¯æœŸçš„è§†é¢‘é¡¹ç›®ã€‚",
      "url": "https://github.com/Norsico/Video-Materials-AutoGEN-Workstation",
      "clone_url": "https://github.com/Norsico/Video-Materials-AutoGEN-Workstation.git",
      "ssh_url": "git@github.com:Norsico/Video-Materials-AutoGEN-Workstation.git",
      "homepage": "",
      "created_at": "2025-11-18T13:04:59Z",
      "updated_at": "2025-11-23T02:48:34Z",
      "pushed_at": "2025-11-21T14:36:25Z"
    },
    "stats": {
      "stars": 602,
      "forks": 131,
      "watchers": 602,
      "open_issues": 1,
      "size": 58390
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 19833252,
        "JavaScript": 144399,
        "HTML": 45750,
        "CSS": 28271,
        "Batchfile": 215
      },
      "license": null,
      "topics": []
    },
    "content": {
      "readme": "# Video Material GEN Workstation\n\nä¸€ä¸ªé›†å†…å®¹ç­–åˆ’ã€AIæ–‡æ¡ˆè‡ªåŠ¨ç”Ÿæˆã€TTS æ‰¹é‡è‡ªåŠ¨é…éŸ³ã€(AI)å›¾ç‰‡ç´ æåˆæˆã€ASRè‡ªåŠ¨æå–è¯­è¨€å­—å¹•è„šæœ¬ã€AIè‡ªç”±åˆ›ä½œäºä¸€ä½“çš„(çŸ­è§†é¢‘)ç”Ÿæˆå·¥ä½œç«™ã€‚æ–¹ä¾¿ç®¡ç†æ¯æœŸçš„è§†é¢‘é¡¹ç›®ã€‚\n\n## åŠŸèƒ½é€Ÿè§ˆ\n\n- æ”¯æŒæŒ‰æ¨¡æ¿æ‰¹é‡ç”Ÿæˆè§†é¢‘é¡¹ç›®ï¼Œè„šæœ¬ã€å›¾ç‰‡ç´ æ(AI)ã€å­—å¹•å’ŒéŸ³é¢‘ä¸€é”®é½å¤‡ã€‚\n- Gemini + TTSåˆæˆï¼Œæ—¢èƒ½æ”¹å†™è„šæœ¬åˆèƒ½ç›´æ¥è¾“å‡º(å¸¦æƒ…ç»ªçš„)é…éŸ³ã€‚\n- å›¾æ–‡åˆ†è½¨ç®¡ç†ï¼Œå¯åœ¨å‰ç«¯éšæ—¶æ›¿æ¢å›¾ç‰‡ã€å­—å¹•æˆ–éŸ³é¢‘å¹¶é¢„è§ˆç»“æœã€‚\n\n## æ•°æ®å±•ç¤º\n\n![æŠ–éŸ³æŠ•æ”¾æ•°æ®](img/æ•°æ®.png)\n\n## å‰ç«¯ç•Œé¢\n\n![ç•Œé¢ 1](img/1.png)\n![ç•Œé¢ 2](img/2.png)\n![ç•Œé¢ 3](img/3.png)\n![ç•Œé¢ 4](img/4.png)\n![ç•Œé¢ 5](img/5.png)\n![ç•Œé¢ 6](img/6.png)\n\n## å¿«é€Ÿä¸Šæ‰‹\n\n1. å¤åˆ¶ `env.example.yaml` ä¸º `env.yaml`ï¼Œå¡«å…¥è‡ªå·±çš„ Gemini Keyã€Base URLã€æ¨¡å‹ã€TTS Key ä¸æç¤ºè¯ç­‰é…ç½®ï¼Œå¦åˆ™æ— æ³•è°ƒç”¨æ¥å£ã€‚\n2. ï¼ˆå¯é€‰ï¼‰åœ¨ `env.yaml` ä¸­è®¾ç½® `Default-Project-Root`ï¼Œç”¨äºå­˜æ”¾è‡ªåŠ¨ç”Ÿæˆçš„è„šæœ¬ã€éŸ³é¢‘ä¸å›¾ç‰‡æ–‡ä»¶ã€‚\n3. å®‰è£…ä¾èµ–ï¼š`npm install`ã€‚\n4. å¯åŠ¨æœåŠ¡ï¼š`npm start` æˆ–ç›´æ¥åŒå‡» `start.bat`ï¼Œé»˜è®¤è®¿é—®åœ°å€ä¸º `http://localhost:8765`ã€‚\n\n## åŠŸèƒ½ä»‹ç»\n\n1. **é¡¹ç›®æ€»è§ˆ**ï¼šä»¥å¡ç‰‡å½¢å¼ç®¡ç†æ‰¹é‡é¡¹ç›®ï¼Œæ˜¾ç¤ºè¾“å‡ºç›®å½•ã€åˆ›å»ºæ—¶é—´åŠåˆ é™¤åŠ¨ä½œï¼Œä¾¿äºå¿«é€Ÿå®šä½ã€‚\n2. **æ–‡æ¡ˆç”Ÿæˆ**ï¼šç»“æ„åŒ–å±•ç¤ºåœºæ™¯è„šæœ¬ï¼Œå¯å¤åˆ¶å•æ¡æˆ–æ•´æ®µæ–‡æ¡ˆï¼Œå·¦ä¾§å‹¾é€‰è”åŠ¨å³ä¾§æç¤ºè¯ã€‚\n3. **å­—å¹•è·å–**: éœ€é…åˆæˆ‘çš„å¦ä¸€ä¸ªé¡¹ç›®(n8n-http-tools): å¼€æºåœ°å€:[n8n-http-tools](https://github.com/Norsico/n8n-http-tools)\n4. **TTS åˆæˆ**ï¼šæ”¯æŒå•æ¡ä¸æ‰¹é‡ä¸¤ç§æ¨¡å¼ï¼Œè¾“å…¥åˆæˆæ–‡æœ¬ä¸æƒ…æ„Ÿæç¤ºå³å¯ç”Ÿæˆè¯­éŸ³ã€‚\n5. **å›¾ç‰‡ç”Ÿæˆ**ï¼šé›†ä¸­ç®¡ç†è§’è‰²æè¿°ã€åœºæ™¯æè¿°ç­‰æç¤ºè¯ï¼Œå‹¾é€‰åå³å¯æ‰¹é‡å¤åˆ¶åˆ°ç»˜å›¾ä»»åŠ¡ã€‚\n6. **ç«‹ç»˜/èƒŒæ™¯ç­‰ç”Ÿæˆ**ï¼šæä¾›æç¤ºè¯è¾“å…¥ã€å‚è€ƒå›¾ä¸Šä¼ ã€å®½é«˜æ¯”è®¾ç½®ä¸å†å²è®°å½•ï¼Œæ–¹ä¾¿éšæ—¶å¤ç”¨ç´ æã€‚\n7. **é€†å‘æ¥å£å®ç°ASRè‡ªåŠ¨æå–å‰ªè¾‘éœ€è¦çš„å­—å¹•æ–‡ä»¶**ï¼šåœ¨TTSåˆæˆç•Œé¢ä¸‹æ–¹ï¼Œæœ‰â€œå­—å¹•ç”Ÿæˆâ€åŠŸèƒ½ï¼Œç‚¹å‡»ä¸‹æ–¹çš„æŒ‰é’®å¯ä»¥æ‰“å¼€å­—å¹•ç”Ÿæˆå·¥å…·ã€‚æ­¤éƒ¨åˆ†ä»£ç ç”±å…¶å®ƒä½œè€…å¼€æºã€‚\n8. **å¸¸ç”¨æç¤ºè¯ä¸è‡ªç”±åˆ›ä½œ**ï¼šæ”¶è—é«˜é¢‘æç¤ºè¯å¹¶ä¸€é”®å¤åˆ¶ï¼ŒåŒæ—¶æä¾›è‡ªç”±åˆ›ä½œé¢æ¿è¿›è¡Œè‡ªå®šä¹‰ç»˜åˆ¶ã€‚\n\n### å…¶å®ƒåŠŸèƒ½æˆ‘å°±æ‡’å¾—ä¸€ä¸ªä¸€ä¸ªå†™äº†ï¼Œå…·ä½“æœ‰å•¥è‡ªå·±å¯ä»¥éƒ¨ç½²ä¸€ä¸‹å»ç©ï¼Œæ³¨æ„æ–‡æ¡ˆç”Ÿæˆè¿™é‡Œéœ€è¦é…åˆn8næ¥æ“ä½œï¼Œä¹‹å‰å†™çš„n8næ–‡ä»¶æ‰¾ä¸åˆ°äº†ï¼Œæ‰€ä»¥è¿™éƒ¨åˆ†å…¶å®å¯ä»¥å¿½ç•¥ï¼Œä¸»è¦å°±æ˜¯ä¸€ä¸ªç”¨äºç”Ÿæˆæ–‡æ¡ˆçš„è„šæœ¬AIæç¤ºè¯ä»¥åŠæˆ‘ä¸»é¡µå¦ä¸€ä¸ªä»“åº“ä¸­æœ‰çš„ä¸€ä¸ªå¼€æºçš„Bç«™è§†é¢‘å­—å¹•æå–å™¨ï¼ˆå½“ç„¶ç½‘ä¸Šä¹Ÿæœ‰ï¼‰ï¼ˆå‚è€ƒåˆ«äººé«˜æ’­æ”¾çš„è§†é¢‘è‡ªå·±å­¦èµ·æ¥ä¹Ÿä¼šå¿«å¾ˆå¤šï¼‰\n\n## æ¥ä¸‹æ¥å¦‚ä½•å¥½å¥½åˆ©ç”¨è¿™ä¸ªé¡¹ç›®è¿˜æ˜¯å¾—é è‡ªå·±ã€‚\n### å› ä¸ºä¸»è¦è¿˜æ˜¯åå‘ç®¡ç†ç”¨çš„ï¼ˆç®€å•æ¥è®²å°±æ˜¯åŠŸèƒ½ä¸ä¼šæœ‰ä½ æƒ³è±¡çš„é‚£ä¹ˆå®ç”¨ï¼‰ï¼Œè§†é¢‘å†…å®¹å¦‚ä½•å®šä¹‰ï¼Œå¦‚ä½•æ‰“é€ çˆ†æ¬¾è¿˜æ˜¯éœ€è¦åŠ¨è„‘å­ã€‚å½“ç„¶æœ¬é¡¹ç›®é‡Œé¢ä½¿ç”¨å›¾åƒç¼–è¾‘æ¨¡å‹çš„æ˜¯NanoBananaï¼Œæœ¬åœ°éƒ¨ç½²çš„AIStudioçš„åå‘ä»£ç†çš„æ¥å£ï¼Œç”¨æ¥ç”Ÿå›¾ç„¶åç»™Soraä¹Ÿæ˜¯ä¸é”™çš„ï¼Œèµ·ç æµ‹è¯•ä¸‹æ¥æ¯”è¾ƒç¨³å®šã€‚\n\n## Star History\n\n[![Star History Chart](https://api.star-history.com/svg?repos=Norsico/Video-Materials-AutoGEN-Workstation&type=date&legend=bottom-right)](https://www.star-history.com/#Norsico/Video-Materials-AutoGEN-Workstation&type=date&legend=bottom-right)\n\n## å…è´£å£°æ˜\n\n### é¡¹ç›®ä»…å…±å‚è€ƒäº¤æµå­¦ä¹ ä½¿ç”¨ï¼Œä¸å¯¹ä»»ä½•ä½¿ç”¨è€…äº§ç”Ÿçš„é—®é¢˜è´Ÿè´£\n\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-11-23T02:53:40.962149"
  },
  {
    "basic_info": {
      "name": "EverMemOS",
      "full_name": "EverMind-AI/EverMemOS",
      "owner": "EverMind-AI",
      "description": "EverMemOS is an open-source, enterprise-grade intelligent memory system. Our mission is to build AI memory that never forgets, making every conversation built on previous understanding.",
      "url": "https://github.com/EverMind-AI/EverMemOS",
      "clone_url": "https://github.com/EverMind-AI/EverMemOS.git",
      "ssh_url": "git@github.com:EverMind-AI/EverMemOS.git",
      "homepage": "https://everm.ai/",
      "created_at": "2025-10-28T15:09:32Z",
      "updated_at": "2025-11-23T00:02:17Z",
      "pushed_at": "2025-11-18T14:43:06Z"
    },
    "stats": {
      "stars": 579,
      "forks": 45,
      "watchers": 579,
      "open_issues": 8,
      "size": 29358
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 2808263,
        "Dockerfile": 404
      },
      "license": "Apache License 2.0",
      "topics": []
    },
    "content": {
      "readme": "<div align=\"center\">\n\n<h1>\n  <img src=\"figs/logo.png\" alt=\"EverMemOS Logo\" height=\"40\" style=\"vertical-align: middle; margin-right: 12px;\"/>\n  EverMemOS\n</h1>\n\n<p>\n  <a href=\"https://everm.ai/\" target=\"_blank\">\n    <img alt=\"Website\" src=\"https://img.shields.io/badge/Website-everm.ai-4A90E2?style=flat-square&logo=link&logoColor=white\" />\n  </a>\n</p>\n\n<p><strong>Let every interaction be driven by understanding.</strong> Â· Enterprise-Grade Intelligent Memory System</p>\n\n<p>\n  <img alt=\"Python\" src=\"https://img.shields.io/badge/Python-3.10+-0084FF?style=flat-square&logo=python&logoColor=white\" />\n  <img alt=\"License\" src=\"https://img.shields.io/badge/License-Apache%202.0-00B894?style=flat-square&logo=apache&logoColor=white\" />\n  <img alt=\"Docker\" src=\"https://img.shields.io/badge/Docker-Supported-4A90E2?style=flat-square&logo=docker&logoColor=white\" />\n  <img alt=\"FastAPI\" src=\"https://img.shields.io/badge/FastAPI-Latest-26A69A?style=flat-square&logo=fastapi&logoColor=white\" />\n  <img alt=\"MongoDB\" src=\"https://img.shields.io/badge/MongoDB-7.0+-00C853?style=flat-square&logo=mongodb&logoColor=white\" />\n  <img alt=\"Elasticsearch\" src=\"https://img.shields.io/badge/Elasticsearch-8.x-0084FF?style=flat-square&logo=elasticsearch&logoColor=white\" />\n  <img alt=\"Milvus\" src=\"https://img.shields.io/badge/Milvus-2.4+-00A3E0?style=flat-square\" />\n  <img alt=\"Redis\" src=\"https://img.shields.io/badge/Redis-7.x-26A69A?style=flat-square&logo=redis&logoColor=white\" />\n   <a href=\"https://github.com/EverMind-AI/EverMemOS/releases\">\n    <img alt=\"Release\" src=\"https://img.shields.io/badge/release-v1.0.0-4A90E2?style=flat-square\" />\n  </a>\n</p>\n\n<p>\n  <a href=\"README.md\">English</a> | <a href=\"README_zh.md\">ç®€ä½“ä¸­æ–‡</a>\n</p>\n\n</div>\n\n---\n\n> ğŸ’¬ **More than memory â€” it's foresight.**\n\n**EverMemOS** is a forward-thinking **intelligent system**.  \nWhile traditional AI memory serves merely as a \"look-back\" database, EverMemOS enables AI not only to \"remember\" what happened, but also to \"understand\" the meaning behind these memories and use them to guide current actions and decisions. In the EverMemOS demo tools, you can see how EverMemOS extracts important information from your history, and then remembers your preferences, habits, and history during conversations, just like a **friend** who truly knows you.\nOn the **LoCoMo** benchmark, our approach built upon EverMemOS achieved a reasoning accuracy of **92.3%** (evaluated by LLM-Judge), outperforming comparable methods in our evaluation.\n\n---\n\n## ğŸ“¢ Latest Updates\n\n<table>\n<tr>\n<td width=\"100%\" style=\"border: none;\">\n\n**[2025-11-02] ğŸ‰ ğŸ‰ ğŸ‰ EverMemOS v1.0.0 Released!**\n\n- âœ¨ **Stable Version**: AI Memory System officially open sourced  \n- ğŸ“š **Complete Documentation**: Quick start guide and comprehensive API documentation  \n- ğŸ“ˆ **Benchmark Testing**: LoCoMo dataset benchmark evaluation pipeline\n- ğŸ–¥ï¸ **Demo Tools**: Get started quickly with easy-to-use demos\n\n</td>\n</tr>\n</table>\n\n---\n\n## ğŸ¯ Core Vision  \nBuild AI memory that never forgets, making every conversation built on previous understanding.\n\n---\n\n## ğŸ’¡ Unique Advantages\n\n<table>\n  <tr>\n    <td width=\"33%\" valign=\"top\">\n      <h3>ğŸ”— Coherent Narrative</h3>\n      <p><strong>Beyond \"fragments,\" connecting \"stories\"</strong>: Automatically linking conversation pieces to build clear thematic context, enabling AI to \"truly understand.\"</p>\n      <blockquote>\n        When facing multi-threaded conversations, it naturally distinguishes between \"Project A progress discussion\" and \"Team B strategy planning,\" maintaining coherent contextual logic within each theme.<br/><br/>\n        From scattered phrases to complete narratives, AI no longer just \"understands one sentence\" but \"understands the whole story.\"\n      </blockquote>\n    </td>\n    <td width=\"33%\" valign=\"top\">\n      <h3>ğŸ§  Evidence-Based Perception</h3>\n      <p><strong>Beyond \"retrieval,\" intelligent \"perception\"</strong>: Proactively capturing deep connections between memories and tasks, enabling AI to \"think thoroughly\" at critical moments.</p>\n      <blockquote>\n        Imagine: When a user asks for \"food recommendations,\" the AI proactively recalls \"you had dental surgery two days ago\" as a key piece of information, automatically adjusting suggestions to avoid unsuitable options.<br/><br/>\n        This is <strong>Contextual Awareness</strong> â€” enabling AI thinking to be truly built on understanding rather than isolated responses.\n      </blockquote>\n    </td>\n    <td width=\"33%\" valign=\"top\">\n      <h3>ğŸ’¾ Living Profiles</h3>\n      <p><strong>Beyond \"records,\" dynamic \"growth\"</strong>: Real-time user profile updates that get to know you better with each conversation, enabling AI to \"recognize you authentically.\"</p>\n      <blockquote>\n        Every interaction subtly updates the AI's understanding of you â€” preferences, style, and focus points all continuously evolve.<br/><br/>\n        As interactions deepen, it doesn't just \"remember what you said,\" but is \"learning who you are.\"\n      ",
      "default_branch": "main"
    },
    "fetched_at": "2025-11-23T02:53:42.281383"
  },
  {
    "basic_info": {
      "name": "AgentEvolver",
      "full_name": "modelscope/AgentEvolver",
      "owner": "modelscope",
      "description": "AgentEvolver: Towards Efficient Self-Evolving Agent System",
      "url": "https://github.com/modelscope/AgentEvolver",
      "clone_url": "https://github.com/modelscope/AgentEvolver.git",
      "ssh_url": "git@github.com:modelscope/AgentEvolver.git",
      "homepage": "https://modelscope.github.io/AgentEvolver/",
      "created_at": "2025-11-13T08:09:51Z",
      "updated_at": "2025-11-23T02:25:49Z",
      "pushed_at": "2025-11-21T08:43:42Z"
    },
    "stats": {
      "stars": 546,
      "forks": 58,
      "watchers": 546,
      "open_issues": 8,
      "size": 15550
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 975095,
        "Shell": 11430,
        "Dockerfile": 4664
      },
      "license": "Apache License 2.0",
      "topics": [
        "agent",
        "agent-system",
        "llm",
        "reinforcement-learning",
        "self-evolving"
      ]
    },
    "content": {
      "readme": "<p align=\"center\">\n <img src=\"docs/img/logo.png\" alt=\"AgentEvolver Logo\" width=\"70%\">\n</p>\n<h2 align=\"center\">AgentEvolver: Towards Efficient Self-Evolving Agent System</h2>\n\n<!-- --- -->\n\n<p align=\"center\">\n  <!-- <a href=\"https://arxiv.org/abs/0000\"><img src=\"https://img.shields.io/badge/cs.MA-0000-B31C1C?logo=arxiv&logoColor=B31C1C\" alt=\"arxiv\"/></a> -->\n  <a href=\"https://www.python.org/\"><img src=\"https://img.shields.io/badge/python-3.11+-blue\" alt=\"Python Version\"></a>\n  <a href=\"./LICENSE\"><img src=\"https://img.shields.io/badge/license-Apache--2.0-black\" alt=\"License\"></a>\n  <a href=\"https://modelscope.github.io/AgentEvolver/\"><img src=\"https://img.shields.io/badge/docs-online-blue?logo=markdown\" alt=\"Documentation\"></a>\n  <a href=\"https://arxiv.org/abs/2511.10395\"><img src=\"https://img.shields.io/badge/arXiv-2511.10395-b31b1b.svg\" alt=\"arXiv\"></a>\n  <a href=\"https://deepwiki.com/modelscope/AgentEvolver\"><img src=\"https://deepwiki.com/badge.svg\" alt=\"deepwiki\"></a>\n  <a href=\"https://github.com/modelscope/AgentEvolver\"><img src=\"https://img.shields.io/github/stars/modelscope/AgentEvolver?style=social\" alt=\"GitHub Stars\"></a>\n</p>\n\n\n<!-- <p align=\"center\">\n  <strong>AgentEvolver: An Efficient Self-Evolving Agent System</strong><br>\n</p> -->\n\n**AgentEvolver** is an end-to-end, self-evolving training framework that unifies self-questioning, self-navigating, and self-attributing into a cohesive system. It empowers agents to autonomously\nimprove their capabilities, aiming for efficient, cost-effective, and continuous capability evolution.\n\n\n## ğŸ“° News\n\n- **[2025-11]** ğŸ“„ [The AgentEvolver Technical Report is now available](https://arxiv.org/abs/2511.10395), detailing the frameworkâ€™s architecture, methodology, and key findings.\n- **[2025-11]** ğŸ§© AgentEvolver v1 has been released now!\n\n\n## âœ¨ Why AgentEvolver\n\n\n\nğŸ§  AgentEvolver provides three **Self-Evolving Mechanisms** from Environment to Policy:\n\n- **Automatic Task Generation (Self-Questioning)** â€“ Explore the environment and autonomously create diverse tasks, eliminating costly manual dataset construction.\n- **Experience-guided Exploration (Self-Navigating)** â€“ Summarize and reuse cross-task experience, guiding higher-quality rollouts and improving exploration efficiency.\n- **Attribution-based Credit Assignment (Self-Attributing)** â€“ Process long trajectories to uncover the causal contribution of intermediate steps, enabling fine-grained and efficient policy optimization.\n\n<p align=\"center\">\n <img src=\"docs/img/flowchart.png\" alt=\"AgentEvolver Flowchart\" width=\"80%\">\n</p>\n\n\n\n\n## ğŸ”§ Architecture Design\nAgentEvolver adopts a service-oriented dataflow architecture, seamlessly integrating environment sandboxes, LLMs, and experience management into modular services.\n\n<p align=\"center\">\n <img src=\"docs/img/system.png\" alt=\"system framework\" width=\"80%\">\n</p>\n\n\n- **Environment Compatibility** â€“ Standardized interfaces for seamless integration with a wide range of external environments and tool APIs.\n- **Flexible Context Manager** â€“ Built-in utilities for managing multi-turn contexts and complex interaction logic, supporting diverse deployment scenarios.\n- **Modular & Extensible Architecture** â€“ Decoupled components allow easy customization, secondary development, and future algorithm upgrades.\n\n\n## ğŸŒŸ Benchmark Performance\n\nPerformance comparison on the AppWorld and BFCL-v3 benchmarks. AgentEvolver achieves superior results while using substantially fewer parameters than larger baseline models.\n\n<p align=\"center\">\n <img src=\"docs/img/performance.png\" alt=\"Benchmark Performance\" width=\"80%\">\n</p>\n\nPerformance on two benchmarks. Columns show avg@8 and best@8 for each benchmark, plus their averages (Avg.). All values are in percent (%). **Bolded numbers** highlight the best results.\n\n| **Model** | **Params** | **AppWorld** | | **BFCL v3** | | **Avg.** | |\n|:---|:---:|:---:|:---:|:---:|:---:|:---:|:---:|\n| | | avg@8 | best@8 | avg@8 | best@8 | avg@8 | best@8 |\n| Qwen2.5-7B | 7B | 1.8 | 5.6 | 29.8 | 42.4 | 15.8 | 24.0 |\n| +Questioning | 7B | 23.2 | 40.3 | 49.0 | 60.6 | 36.1 | 50.5 |\n| +Questioning&Navigating | 7B | 26.3 | 43.1 | 53.3 | 61.0 | 39.8 | 52.1 |\n| +Questioning&Attributing | 7B | 25.7 | 43.7 | 56.8 | 65.3 | 41.3 | 54.5 |\n| **AgentEvolver (overall)** | **7B** | **32.4** | **51.2** | **57.9** | **69.0** | **45.2** | **60.1** |\n| | | | | | | | |\n| Qwen2.5-14B | 14B | 18.0 | 31.4 | 41.6 | 54.1 | 29.8 | 42.8 |\n| +Questioning | 14B | 44.3 | 65.5 | 60.3 | 72.1 | 52.3 | 68.8 |\n| +Questioning&Navigating | 14B | 45.4 | 65.3 | 62.8 | 74.5 | 54.1 | 69.9 |\n| +Questioning&Attributing | 14B | 47.8 | 65.6 | 64.9 | 76.3 | 56.4 | 71.0 |\n| **AgentEvolver (overall)** | **14B** | **48.7** | **69.4** | **66.5** | **76.7** | **57.6** | **73.1** |\n\n\n## ğŸš€ Quick Start\n### Step 1. Basic Dependency Installation\n\nMake sure you have **conda** and **cuda toolkit** installed.\n\nThen, set up the training environment by running the script\n\n```bash\nbash install.sh\n```\n\n\n### Step 2. Setup Env-Serv",
      "default_branch": "main"
    },
    "fetched_at": "2025-11-23T02:53:43.554985"
  },
  {
    "basic_info": {
      "name": "ok-duet-night-abyss",
      "full_name": "BnanZ0/ok-duet-night-abyss",
      "owner": "BnanZ0",
      "description": "äºŒé‡èºæ—‹ åå°å‰¯æœ¬æŒ‚æœº Automation for Duet Night Abyss",
      "url": "https://github.com/BnanZ0/ok-duet-night-abyss",
      "clone_url": "https://github.com/BnanZ0/ok-duet-night-abyss.git",
      "ssh_url": "git@github.com:BnanZ0/ok-duet-night-abyss.git",
      "homepage": "",
      "created_at": "2025-11-06T14:56:40Z",
      "updated_at": "2025-11-22T11:16:51Z",
      "pushed_at": "2025-11-22T11:17:11Z"
    },
    "stats": {
      "stars": 545,
      "forks": 40,
      "watchers": 545,
      "open_issues": 2,
      "size": 42020
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 229039,
        "PowerShell": 419
      },
      "license": "GNU Affero General Public License v3.0",
      "topics": [
        "dna",
        "duet-night-abyss",
        "duet-night-abyss-hack",
        "duet-night-abyss-software",
        "ok-dna",
        "okdna"
      ]
    },
    "content": {
      "readme": "[English](README_en.md) | [ç®€ä½“ä¸­æ–‡](README.md)\n\n<div align=\"center\">\n  <img src=\"icons/icon.png\" alt=\"icon\" width=\"200\"><br>\n  <h1>ok-dna</h1>\n  <p>ä¸€æ¬¾åŸºäºå›¾åƒè¯†åˆ«çš„ã€ŠäºŒé‡èºæ—‹ã€‹è‡ªåŠ¨åŒ–å·¥å…·ï¼Œæ”¯æŒåå°è¿è¡Œã€‚</p>\n  <p>åŸºäº <a href=\"https://github.com/ok-oldking/ok-script\">ok-script</a> æ¡†æ¶å¼€å‘ã€‚</p>\n  \n  <p>\n    <img src=\"https://img.shields.io/badge/platform-Windows-blue\" alt=\"å¹³å°\">\n    <img src=\"https://img.shields.io/badge/python-3.12+-skyblue\" alt=\"Pythonç‰ˆæœ¬\">\n    <img src=\"https://img.shields.io/github/downloads/BnanZ0/ok-duet-night-abyss/total\" alt=\"æ€»ä¸‹è½½é‡\">\n    <img src=\"https://img.shields.io/github/v/release/BnanZ0/ok-duet-night-abyss\" alt=\"æœ€æ–°ç‰ˆæœ¬\">\n    <a href=\"https://discord.gg/vVyCatEBgA\"><img alt=\"Discord\" src=\"https://img.shields.io/discord/296598043787132928?color=5865f2&label=%20Discord\"></a>\n  </p>\n</div>\n\n## âš ï¸ å…è´£å£°æ˜\n\næœ¬è½¯ä»¶ä¸ºå¼€æºã€å…è´¹çš„å¤–éƒ¨å·¥å…·ï¼Œä»…ä¾›å­¦ä¹ å’Œäº¤æµä½¿ç”¨ï¼Œæ—¨åœ¨é€šè¿‡æ¨¡æ‹Ÿæ“ä½œç®€åŒ–ã€ŠäºŒé‡èºæ—‹ã€‹çš„æ¸¸æˆç©æ³•ã€‚\n\n-   **å·¥ä½œåŸç†**ï¼šç¨‹åºä»…é€šè¿‡è¯†åˆ«ç°æœ‰ç”¨æˆ·ç•Œé¢ä¸æ¸¸æˆè¿›è¡Œäº¤äº’ï¼Œä¸ä¿®æ”¹ä»»ä½•æ¸¸æˆæ–‡ä»¶æˆ–ä»£ç ã€‚\n-   **ä½¿ç”¨ç›®çš„**ï¼šæ—¨åœ¨ä¸ºç”¨æˆ·æä¾›ä¾¿åˆ©ï¼Œæ— æ„ç ´åæ¸¸æˆå¹³è¡¡æˆ–æä¾›ä»»ä½•ä¸å…¬å¹³ä¼˜åŠ¿ã€‚\n-   **æ³•å¾‹è´£ä»»**ï¼šä½¿ç”¨æœ¬è½¯ä»¶äº§ç”Ÿçš„æ‰€æœ‰é—®é¢˜åŠåæœï¼Œå‡ä¸æœ¬é¡¹ç›®åŠå¼€å‘è€…å›¢é˜Ÿæ— å…³ã€‚å¼€å‘è€…å›¢é˜Ÿæ‹¥æœ‰å¯¹æœ¬é¡¹ç›®çš„æœ€ç»ˆè§£é‡Šæƒã€‚\n-   **å•†ä¸šè¡Œä¸º**ï¼šè‹¥æ‚¨é‡åˆ°å•†å®¶ä½¿ç”¨æœ¬è½¯ä»¶è¿›è¡Œä»£ç»ƒå¹¶æ”¶è´¹ï¼Œæ­¤è¡Œä¸ºå¯èƒ½æ¶‰åŠè®¾å¤‡ä¸æ—¶é—´æˆæœ¬ï¼Œä¸æœ¬è½¯ä»¶æœ¬èº«æ— å…³ã€‚\n\n> **è¯·æ³¨æ„ï¼šæ ¹æ®[ã€ŠäºŒé‡èºæ—‹ã€‹å…¬å¹³æ¸¸æˆå®£è¨€](https://dna.yingxiong.com/#/news/list?id=14453&type=2523)ï¼š**\n>\n> > \"ä¸¥ç¦ä½¿ç”¨ä»»ä½•å¤–æŒ‚ã€ç¬¬ä¸‰æ–¹å·¥å…·ä»¥åŠå…¶ä»–ç ´åæ¸¸æˆå…¬å¹³æ€§çš„è¡Œä¸ºã€‚\"\n> > \"ä¸€ç»æ ¸å®ï¼Œè¿è¥å›¢é˜Ÿå°†æ ¹æ®æƒ…èŠ‚ä¸¥é‡ç¨‹åº¦å’Œæ¬¡æ•°ï¼Œé‡‡å–æ‰£é™¤è¿è§„æ”¶ç›Šã€å†»ç»“æˆ–æ°¸ä¹…å°ç¦æ¸¸æˆè´¦å·ç­‰æªæ–½ï¼Œä»¥ç»´æŠ¤ç©å®¶çš„å…¬å¹³æƒç›Šã€‚\"\n>\n> **æ‚¨åº”å……åˆ†äº†è§£å¹¶è‡ªæ„¿æ‰¿æ‹…ä½¿ç”¨æœ¬å·¥å…·å¯èƒ½å¸¦æ¥çš„æ‰€æœ‰é£é™©ã€‚**\n\n<details>\n<summary><strong>Disclaimer in English</strong></summary>\n\nThis software is an open-source, free external tool intended for learning and exchange purposes only. It is designed to automate the gameplay of *Duet Night Abyss* by interacting with the game solely through the existing user interface and in compliance with relevant laws and regulations. The package is intended to provide a simplified way for users to interact with the game and is not meant to disrupt the game balance or provide any unfair advantage. This package does not modify any game files or game code in any way.\n\nAll issues and consequences arising from the use of this software are not related to this project or its development team. The development team reserves the final right of interpretation for this project. If you encounter vendors using this software for services and charging a fee, this may cover their costs for equipment and time; any resulting problems or consequences are not associated with this software.\n</details>\n\n## âœ¨ ä¸»è¦åŠŸèƒ½\n\n<img width=\"100%\" alt=\"åŠŸèƒ½æ¼”ç¤º\" src=\"https://github.com/user-attachments/assets/cb7f145b-b304-4a0e-891a-3b6d5acff65d\" />\n\n*   **å‰¯æœ¬è‡ªåŠ¨æŒ‚æœº**\n    *   æ”¯æŒå…¨è‡ªåŠ¨ä¸åŠè‡ªåŠ¨æ¨¡å¼\n    *   è‡ªåŠ¨è¿æˆ˜\n    *   å…¼å®¹å¤–éƒ¨ç§»åŠ¨é€»è¾‘ (Mod)\n*   **è‡ªåŠ¨é’“é±¼** (æ ¸å¿ƒé€»è¾‘åŸä½œè€…: Bç«™ @æ— æ•Œå¤§èœœç“œ)\n*   **å¿«é€Ÿç§»åŠ¨**\n    *   è‡ªåŠ¨ç©¿å¼•å…±é¸£\n*   **è‡ªåŠ¨èŠ±åºå¼“è“„åŠ›**\n*   **åå°è¿è¡Œ**\n    *   æ”¯æŒ PC ç‰ˆæ¸¸æˆåœ¨åå°è¿è¡Œæ—¶è¿›è¡Œè‡ªåŠ¨åŒ–æ“ä½œ\n\n## ğŸ–¥ï¸ è¿è¡Œç¯å¢ƒä¸å…¼å®¹æ€§\n\n*   **æ“ä½œç³»ç»Ÿ**ï¼šWindows\n*   **æ¸¸æˆåˆ†è¾¨ç‡**ï¼š1600x900 æˆ–æ›´é«˜ï¼ˆæ¨è 16:9 å®½é«˜æ¯”ï¼‰\n*   **æ¸¸æˆè¯­è¨€**ï¼šç®€ä½“ä¸­æ–‡ / English\n\n## ğŸš€ å®‰è£…æŒ‡å—\n\n### æ–¹å¼ä¸€ï¼šä½¿ç”¨å®‰è£…åŒ… (æ¨è)\n\næ­¤æ–¹æ³•é€‚åˆç»å¤§å¤šæ•°ç”¨æˆ·ï¼Œç®€å•å¿«æ·ï¼Œå¹¶æ”¯æŒè‡ªåŠ¨æ›´æ–°ã€‚\n\n1.  å‰å¾€ [**Releases**](https://github.com/BnanZ0/ok-duet-night-abyss/releases) é¡µé¢ã€‚\n2.  ä¸‹è½½æœ€æ–°çš„ `ok-dna-win32-China-setup.exe` æ–‡ä»¶ã€‚\n3.  åŒå‡»è¿è¡Œå®‰è£…ç¨‹åºï¼ŒæŒ‰æç¤ºå®Œæˆå®‰è£…å³å¯ã€‚\n\n### æ–¹å¼äºŒï¼šä»æºç è¿è¡Œ (é€‚åˆå¼€å‘è€…)\n\næ­¤æ–¹æ³•éœ€è¦æ‚¨å…·å¤‡ Python ç¯å¢ƒï¼Œé€‚åˆå¸Œæœ›è¿›è¡ŒäºŒæ¬¡å¼€å‘æˆ–è°ƒè¯•çš„ç”¨æˆ·ã€‚\n\n1.  **ç¯å¢ƒè¦æ±‚**ï¼šç¡®ä¿å·²å®‰è£… **Python 3.12** æˆ–æ›´é«˜ç‰ˆæœ¬ã€‚\n2.  **å…‹éš†ä»“åº“**ï¼š\n    ```bash\n    git clone https://github.com/BnanZ0/ok-duet-night-abyss.git\n    cd ok-duet-night-abyss\n    ```\n3.  **å®‰è£…ä¾èµ–**ï¼š\n    ```bash\n    pip install -r requirements.txt --upgrade\n    ```\n    *æç¤ºï¼šæ¯æ¬¡æ›´æ–°ä»£ç åï¼Œå»ºè®®é‡æ–°è¿è¡Œæ­¤å‘½ä»¤ä»¥ç¡®ä¿ä¾èµ–åº“ä¸ºæœ€æ–°ç‰ˆæœ¬ã€‚*\n4.  **è¿è¡Œç¨‹åº**ï¼š\n    ```bash\n    # è¿è¡Œæ­£å¼ç‰ˆ\n    python main.py\n    \n    # è¿è¡Œè°ƒè¯•ç‰ˆ (ä¼šè¾“å‡ºæ›´è¯¦ç»†çš„æ—¥å¿—)\n    python main_debug.py\n    ```\n\n## ğŸ“– ä½¿ç”¨æŒ‡å—ä¸ FAQ\n\nä¸ºç¡®ä¿ç¨‹åºç¨³å®šè¿è¡Œï¼Œè¯·åœ¨ä½¿ç”¨å‰ä»”ç»†é˜…è¯»ä»¥ä¸‹é…ç½®è¦æ±‚å’Œå¸¸è§é—®é¢˜è§£ç­”ã€‚\n\n### ä¸€ã€ ä½¿ç”¨å‰é…ç½® (å¿…è¯»)\n\nåœ¨å¯åŠ¨è‡ªåŠ¨åŒ–å‰ï¼Œè¯·åŠ¡å¿…æ£€æŸ¥å¹¶ç¡®è®¤ä»¥ä¸‹è®¾ç½®ï¼š\n\n*   **å›¾å½¢è®¾ç½®**\n    *   **æ˜¾å¡æ»¤é•œ**ï¼š**å…³é—­** æ‰€æœ‰æ˜¾å¡æ»¤è¡Œå’Œé”åŒ–æ•ˆæœï¼ˆå¦‚ NVIDIA Freestyle, AMD FidelityFXï¼‰ã€‚\n    *   **æ¸¸æˆäº®åº¦**ï¼šä½¿ç”¨æ¸¸æˆ **é»˜è®¤äº®åº¦**ã€‚\n    *   **æ¸¸æˆUIç¼©æ”¾**ï¼šä½¿ç”¨æ¸¸æˆ **é»˜è®¤ç¼©æ”¾100%**ã€‚\n*   **åˆ†è¾¨ç‡**\n    *   æ¨èä½¿ç”¨ **1600x900** æˆ–ä»¥ä¸Šçš„ä¸»æµåˆ†è¾¨ç‡ã€‚\n*   **æŒ‰é”®è®¾ç½®**\n    *   è¯·åŠ¡å¿…ä½¿ç”¨æ¸¸æˆ **é»˜è®¤** æŒ‰é”®ç»‘å®šã€‚\n*   **ç¬¬ä¸‰æ–¹è½¯ä»¶**\n    *   å…³é—­ä»»ä½•åœ¨æ¸¸æˆç”»é¢ä¸Šæ˜¾ç¤ºä¿¡æ¯çš„æ‚¬æµ®çª—ï¼Œå¦‚ MSI Afterburner (å°é£æœº) çš„ **å¸§ç‡æ˜¾ç¤º**ã€‚\n*   **çª—å£ä¸ç³»ç»ŸçŠ¶æ€**\n    *   **é¼ æ ‡å¹²æ‰°**ï¼šå½“æ¸¸æˆçª—å£å¤„äº **å‰å°** æ—¶ï¼Œè¯·å‹¿ç§»åŠ¨é¼ æ ‡ï¼Œå¦åˆ™ä¼šå¹²æ‰°ç¨‹åºçš„æ¨¡æ‹Ÿç‚¹å‡»ã€‚\n    *   **çª—å£çŠ¶æ€**ï¼šæ¸¸æˆçª—å£å¯ä»¥ç½®äºåå°ï¼Œä½† **ä¸å¯æœ€å°åŒ–**ã€‚\n    *   **ç³»ç»ŸçŠ¶æ€**ï¼šè¯·å‹¿è®©ç”µè„‘ **ç†„å±** æˆ– **é”å±**ï¼Œå¦åˆ™å°†å¯¼è‡´ç¨‹åºä¸­æ–­ã€‚\n\n### äºŒã€ å¿«é€Ÿä¸Šæ‰‹\n\n1.  è¿›å…¥æ‚¨æƒ³è¦è‡ªåŠ¨åŒ–çš„å…³å¡æˆ–åœºæ™¯ã€‚\n2.  åœ¨ç¨‹åºç•Œé¢ä¸Šç‚¹å‡» **â€œå¼€å§‹â€** æŒ‰é’®å³å¯ã€‚\n\n### ä¸‰ã€ å®‰è£…å¤–éƒ¨é€»è¾‘ (Mod)\n\næ‚¨å¯ä»¥å®‰è£…ç¤¾åŒºå¼€å‘çš„å¤–éƒ¨é€»è¾‘æ¨¡å—æ¥æ‰©å±•ç¨‹åºåŠŸèƒ½ã€‚\n\n1.  åœ¨ç¨‹åºä¸»é¡µï¼Œç‚¹å‡» **â€œå®‰è£…ç›®å½•â€** æŒ‰é’®æ‰“å¼€ç¨‹åºæ–‡ä»¶å¤¹ã€‚\n2.  å°†ä¸‹è½½çš„ Mod æ–‡ä»¶æ”¾å…¥ `mod` æ–‡ä»¶å¤¹å†…ã€‚\n3.  é‡å¯ç¨‹åºå³å¯åŠ è½½ã€‚\n\n### å››ã€ å¸¸è§é—®é¢˜è§£ç­” (FAQ)\n\n**Q1: è§’è‰²ç§»åŠ¨æ—¶ç»å¸¸æ’å¢™ï¼Œæˆ–è€…æ— æ³•å‡†ç¡®èµ°åˆ°ç›®æ ‡ç‚¹ï¼Ÿ**\n\n*   **åŸå› **ï¼šæ¸¸æˆå¼•æ“çš„ç§»åŠ¨é€Ÿåº¦ä¸å¸§ç‡ (FPS) å¼ºç›¸å…³ã€‚\n*   **è§£å†³æ–¹æ¡ˆ**ï¼š\n    1.  **è°ƒæ•´æ¸¸æˆå¸§ç‡**ï¼šåœ¨æ¸¸æˆè®¾ç½®ä¸­ï¼Œä¾æ¬¡å°è¯•å°†å¸§ç‡ä¸Šé™è®¾ä¸º **60 FPS** / **120 FPS** / **æ— é™åˆ¶**ï¼Œæ‰¾åˆ°è¡¨ç°æœ€ç¨³å®šçš„ä¸€æ¡£ã€‚\n    2.  **è°ƒæ•´æŒ‰é”®æ—¶é•¿**ï¼šåœ¨å¯¹åº”ä»»åŠ¡æˆ– Mod çš„è®¾ç½®ä¸­ï¼Œå¾®è°ƒ **æŒ‰é”®æ—¶é•¿** å‚æ•°ã€‚\n    3.  **ç­‰å¾…å®˜æ–¹ä¼˜åŒ–**ï¼šæ­¤é—®é¢˜å¯èƒ½éœ€è¦ç­‰å¾…æ¸¸æˆå®˜æ–¹åç»­æ›´æ–°ä¿®å¤ã€‚\n\n**Q2: æˆ‘å®‰è£…çš„ Mod æ²¡æœ‰ç”Ÿæ•ˆï¼Œæˆ–è€…è¯†åˆ«ä¸æ­£ç¡®ï¼Ÿ**\n\n*   **åŸå› **ï¼šMod å†…ç½®çš„å›¾åƒè¯†åˆ«ç´ æå¯èƒ½æ— æ³•é€‚é…æ‰€æœ‰åˆ†è¾¨ç‡ã€‚\n*   **è§£å†³æ–¹æ¡ˆ**ï¼š\n    1.  **åˆ‡æ¢åˆ†è¾¨ç‡**ï¼šå°è¯•æ›´æ¢ä¸€ä¸ªå¸¸è§åˆ†è¾¨ç‡ï¼ˆå¦‚ 1920x1080 æˆ– 1600x900ï¼‰ã€‚\n    2.  **æ‰‹åŠ¨æ›´æ–°ç´ æ**ï¼šå¦‚æœæ‚¨äº†è§£ Mod åˆ¶ä½œï¼Œå¯ä»¥ä¸ºæ‚¨å½“å‰çš„åˆ†è¾¨ç‡é‡æ–°å½•åˆ¶è¯†å›¾æ‰€éœ€çš„æˆªå›¾ã€‚\n\n**Q3: ç¨‹åºå¡åœ¨ç»“ç®—æˆ–å¤ä½ç•Œé¢ï¼Œä¸å†ç»§ç»­æ‰§è¡Œï¼Ÿ**\n\n*   **åŸå› **ï¼šå¾ˆå¯èƒ½æ˜¯æ— æ„çš„é¼ æ ‡ç§»åŠ¨å¹²æ‰°äº†ç¨‹åºçš„å›¾åƒè¯†åˆ«ã€‚\n*   **è§£å†³æ–¹æ¡ˆ**ï¼š\n    1.  åœ¨ç¨‹åºå·¦ä¸‹è§’ç‚¹å‡» **â€œè®¾ç½®â€**ã€‚\n    2.  åˆ‡æ¢åˆ° **â€œæŒ‚æœºè®¾ç½®â€** é€‰é¡¹å¡ã€‚\n    3.  å‹¾é€‰å¹¶å¯ç”¨ **â€œé˜²æ­¢é¼ æ ‡å¹²æ‰°â€** åŠŸèƒ½ã€‚\n\n### äº”ã€ é—®é¢˜åé¦ˆ\n\nå¦‚æœä»¥ä¸Šæ–¹æ³•æœªèƒ½è§£å†³æ‚¨çš„é—®é¢˜ï¼Œæ¬¢è¿é€šè¿‡ [**Issues**](https://github.com/BnanZ0/ok-duet-night-abyss/issues) å‘æˆ‘ä»¬åé¦ˆã€‚ä¸ºå¸®åŠ©æˆ‘ä»¬å¿«é€Ÿå®šä½é—®é¢˜ï¼Œè¯·åœ¨æäº¤æ—¶æä¾›ä»¥ä¸‹ä¿¡æ¯ï¼š\n\n*   **é—®é¢˜æˆªå›¾**ï¼šæ¸…æ™°å±•ç¤ºå¼‚å¸¸ç•Œé¢æˆ–é”™è¯¯æç¤ºã€‚\n*   **æ—¥å¿—æ–‡ä»¶**ï¼šé™„ä¸Šç¨‹åºç›®å½•ä¸‹çš„ `",
      "default_branch": "master"
    },
    "fetched_at": "2025-11-23T02:53:44.848226"
  },
  {
    "basic_info": {
      "name": "acemcp",
      "full_name": "qy527145/acemcp",
      "owner": "qy527145",
      "description": "ä¸€ä¸ªå°†ACE(Augment Context Engine) åšæˆMCPçš„é¡¹ç›®",
      "url": "https://github.com/qy527145/acemcp",
      "clone_url": "https://github.com/qy527145/acemcp.git",
      "ssh_url": "git@github.com:qy527145/acemcp.git",
      "homepage": null,
      "created_at": "2025-11-06T10:13:39Z",
      "updated_at": "2025-11-22T10:27:53Z",
      "pushed_at": "2025-11-19T07:05:57Z"
    },
    "stats": {
      "stars": 541,
      "forks": 97,
      "watchers": 541,
      "open_issues": 2,
      "size": 125
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 53780,
        "HTML": 32891
      },
      "license": "Apache License 2.0",
      "topics": []
    },
    "content": {
      "readme": "[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/qy527145-acemcp-badge.png)](https://mseep.ai/app/qy527145-acemcp)\n\nç®€ä½“ä¸­æ–‡ | [English](./README_EN.md)\n\n# Acemcp\n\nä»£ç åº“ç´¢å¼•å’Œè¯­ä¹‰æœç´¢çš„ MCP æœåŠ¡å™¨ã€‚\n\n<a href=\"https://glama.ai/mcp/servers/@qy527145/acemcp\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/@qy527145/acemcp/badge\" alt=\"Acemcp MCP server\" />\n</a>\n\n## å®‰è£…\n\n### ä½œä¸ºå·¥å…·å®‰è£…ï¼ˆæ¨èï¼‰\n\n```bash\n# å®‰è£…åˆ°ç³»ç»Ÿ\nuv tool install acemcp\n\n# æˆ–ä¸´æ—¶è¿è¡Œï¼ˆæ— éœ€å®‰è£…ï¼‰\nuvx acemcp\n```\n\n### å¼€å‘å®‰è£…\n\n```bash\n# å…‹éš†ä»“åº“\ngit clone https://github.com/qy527145/acemcp.git\ncd acemcp\n\n# å®‰è£…ä¾èµ–\nuv sync\n\n# è¿è¡Œ\nuv run acemcp\n```\n\n## é…ç½®\n\né…ç½®æ–‡ä»¶ä¼šåœ¨é¦–æ¬¡è¿è¡Œæ—¶è‡ªåŠ¨åˆ›å»ºåœ¨ `~/.acemcp/settings.toml`ï¼ŒåŒ…å«é»˜è®¤å€¼ã€‚\n\nç¼–è¾‘ `~/.acemcp/settings.toml` è¿›è¡Œé…ç½®ï¼š\n```toml\nBATCH_SIZE = 10\nMAX_LINES_PER_BLOB = 800\nBASE_URL = \"https://your-api-endpoint.com\"\nTOKEN = \"your-bearer-token-here\"\nTEXT_EXTENSIONS = [\".py\", \".js\", \".ts\", ...]\nEXCLUDE_PATTERNS = [\".venv\", \"node_modules\", \".git\", \"__pycache__\", \"*.pyc\", ...]\n```\n\n**é…ç½®é€‰é¡¹ï¼š**\n- `BATCH_SIZE`: æ¯æ‰¹ä¸Šä¼ çš„æ–‡ä»¶æ•°é‡ï¼ˆé»˜è®¤ï¼š10ï¼‰\n- `MAX_LINES_PER_BLOB`: å¤§æ–‡ä»¶åˆ†å‰²å‰çš„æœ€å¤§è¡Œæ•°ï¼ˆé»˜è®¤ï¼š800ï¼‰\n- `BASE_URL`: API ç«¯ç‚¹ URL\n- `TOKEN`: è®¤è¯ä»¤ç‰Œ\n- `TEXT_EXTENSIONS`: è¦ç´¢å¼•çš„æ–‡ä»¶æ‰©å±•ååˆ—è¡¨\n- `EXCLUDE_PATTERNS`: è¦æ’é™¤çš„æ¨¡å¼åˆ—è¡¨ï¼ˆæ”¯æŒé€šé…ç¬¦å¦‚ `*.pyc`ï¼‰\n\næ‚¨è¿˜å¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼é…ç½®ï¼š\n- **å‘½ä»¤è¡Œå‚æ•°**ï¼ˆæœ€é«˜ä¼˜å…ˆçº§ï¼‰ï¼š`--base-url`ã€`--token`\n- **Web ç®¡ç†ç•Œé¢**ï¼ˆæ›´æ–°ç”¨æˆ·é…ç½®æ–‡ä»¶ï¼‰\n- **ç¯å¢ƒå˜é‡**ï¼ˆä½¿ç”¨ `ACEMCP_` å‰ç¼€ï¼‰\n\n## MCP é…ç½®\n\nå°†ä»¥ä¸‹å†…å®¹æ·»åŠ åˆ°æ‚¨çš„ MCP å®¢æˆ·ç«¯é…ç½®ä¸­ï¼ˆä¾‹å¦‚ Claude Desktopï¼‰ï¼š\n\n### åŸºç¡€é…ç½®\n\n```json\n{\n  \"mcpServers\": {\n    \"acemcp\": {\n      \"command\": \"uvx\",\n      \"args\": [\n        \"acemcp\"\n      ]\n    }\n  }\n}\n```\n\n\n**å¯ç”¨çš„å‘½ä»¤è¡Œå‚æ•°ï¼š**\n- `--base-url`: è¦†ç›– BASE_URL é…ç½®\n- `--token`: è¦†ç›– TOKEN é…ç½®\n- `--web-port`: åœ¨æŒ‡å®šç«¯å£å¯ç”¨ Web ç®¡ç†ç•Œé¢ï¼ˆä¾‹å¦‚ 8080ï¼‰\n\n### å¯ç”¨ Web ç®¡ç†ç•Œé¢çš„é…ç½®\n\nè¦å¯ç”¨ Web ç®¡ç†ç•Œé¢ï¼Œæ·»åŠ  `--web-port` å‚æ•°ï¼š\n\n```json\n{\n  \"mcpServers\": {\n    \"acemcp\": {\n      \"command\": \"uvx\",\n      \"args\": [\n        \"acemcp\",\n        \"--web-port\",\n        \"8888\"\n      ]\n    }\n  }\n}\n```\n\nç„¶åè®¿é—®ç®¡ç†ç•Œé¢ï¼š`http://localhost:8888`\n\n**Web ç®¡ç†åŠŸèƒ½ï¼š**\n- **é…ç½®ç®¡ç†**ï¼šæŸ¥çœ‹å’Œç¼–è¾‘æœåŠ¡å™¨é…ç½®ï¼ˆBASE_URLã€TOKENã€BATCH_SIZEã€MAX_LINES_PER_BLOBã€TEXT_EXTENSIONSï¼‰\n- **å®æ—¶æ—¥å¿—**ï¼šé€šè¿‡ WebSocket è¿æ¥å®æ—¶ç›‘æ§æœåŠ¡å™¨æ—¥å¿—ï¼Œå…·æœ‰æ™ºèƒ½é‡è¿åŠŸèƒ½\n  - æŒ‡æ•°é€€é¿é‡è¿ç­–ç•¥ï¼ˆ1ç§’ â†’ 1.5ç§’ â†’ 2.25ç§’ ... æœ€å¤§ 30ç§’ï¼‰\n  - æœ€å¤š 10 æ¬¡é‡è¿å°è¯•ï¼Œé˜²æ­¢æ— é™å¾ªç¯\n  - ç½‘ç»œæ•…éšœæ—¶è‡ªåŠ¨é‡è¿\n  - å‡å°‘æ—¥å¿—å™ªéŸ³ï¼ˆWebSocket è¿æ¥è®°å½•åœ¨ DEBUG çº§åˆ«ï¼‰\n- **å·¥å…·è°ƒè¯•å™¨**ï¼šç›´æ¥ä» Web ç•Œé¢æµ‹è¯•å’Œè°ƒè¯• MCP å·¥å…·\n  - æµ‹è¯• `search_context` å·¥å…·ï¼Œè¾“å…¥é¡¹ç›®è·¯å¾„å’ŒæŸ¥è¯¢\n  - æŸ¥çœ‹æ ¼å¼åŒ–çš„ç»“æœå’Œé”™è¯¯æ¶ˆæ¯\n\n## å·¥å…·\n\n### search_context\n\nåŸºäºæŸ¥è¯¢æœç´¢ç›¸å…³çš„ä»£ç ä¸Šä¸‹æ–‡ã€‚æ­¤å·¥å…·åœ¨æœç´¢å‰**è‡ªåŠ¨æ‰§è¡Œå¢é‡ç´¢å¼•**ï¼Œç¡®ä¿ç»“æœå§‹ç»ˆæ˜¯æœ€æ–°çš„ã€‚å®ƒåœ¨æ‚¨çš„ä»£ç åº“ä¸­æ‰§è¡Œ**è¯­ä¹‰æœç´¢**ï¼Œå¹¶è¿”å›æ ¼å¼åŒ–çš„æ–‡æœ¬ç‰‡æ®µï¼Œæ˜¾ç¤ºç›¸å…³ä»£ç çš„ä½ç½®ã€‚\n\n**æ ¸å¿ƒç‰¹æ€§ï¼š**\n- **è‡ªåŠ¨å¢é‡ç´¢å¼•**ï¼šæ¯æ¬¡æœç´¢å‰ï¼Œå·¥å…·è‡ªåŠ¨ä»…ç´¢å¼•æ–°æ–‡ä»¶æˆ–ä¿®æ”¹è¿‡çš„æ–‡ä»¶ï¼Œè·³è¿‡æœªæ›´æ”¹çš„æ–‡ä»¶ä»¥æé«˜æ•ˆç‡\n- **æ— éœ€æ‰‹åŠ¨ç´¢å¼•**ï¼šæ‚¨æ— éœ€æ‰‹åŠ¨ç´¢å¼•é¡¹ç›® - åªéœ€æœç´¢ï¼Œå·¥å…·ä¼šè‡ªåŠ¨å¤„ç†ç´¢å¼•\n- **å§‹ç»ˆä¿æŒæœ€æ–°**ï¼šæœç´¢ç»“æœåæ˜ ä»£ç åº“çš„å½“å‰çŠ¶æ€\n- **å¤šç¼–ç æ”¯æŒ**ï¼šè‡ªåŠ¨æ£€æµ‹å’Œå¤„ç†å¤šç§æ–‡ä»¶ç¼–ç ï¼ˆUTF-8ã€GBKã€GB2312ã€Latin-1ï¼‰\n- **.gitignore é›†æˆ**ï¼šç´¢å¼•é¡¹ç›®æ—¶è‡ªåŠ¨éµå®ˆ `.gitignore` æ¨¡å¼\n\n**å‚æ•°ï¼š**\n- `project_root_path`ï¼ˆå­—ç¬¦ä¸²ï¼‰ï¼šé¡¹ç›®æ ¹ç›®å½•çš„ç»å¯¹è·¯å¾„\n  - **é‡è¦**ï¼šå³ä½¿åœ¨ Windows ä¸Šä¹Ÿä½¿ç”¨æ­£æ–œæ ï¼ˆ`/`ï¼‰ä½œä¸ºè·¯å¾„åˆ†éš”ç¬¦\n  - Windows ç¤ºä¾‹ï¼š`C:/Users/username/projects/myproject`\n  - Linux/Mac ç¤ºä¾‹ï¼š`/home/username/projects/myproject`\n- `query`ï¼ˆå­—ç¬¦ä¸²ï¼‰ï¼šç”¨äºæŸ¥æ‰¾ç›¸å…³ä»£ç ä¸Šä¸‹æ–‡çš„è‡ªç„¶è¯­è¨€æœç´¢æŸ¥è¯¢\n  - ä½¿ç”¨ä¸æ‚¨è¦æŸ¥æ‰¾çš„å†…å®¹ç›¸å…³çš„æè¿°æ€§å…³é”®è¯\n  - å·¥å…·æ‰§è¡Œè¯­ä¹‰åŒ¹é…ï¼Œè€Œä¸ä»…ä»…æ˜¯å…³é”®è¯æœç´¢\n  - è¿”å›å¸¦æœ‰æ–‡ä»¶è·¯å¾„å’Œè¡Œå·çš„ä»£ç ç‰‡æ®µ\n\n**è¿”å›å†…å®¹ï¼š**\n- ä¸æ‚¨çš„æŸ¥è¯¢åŒ¹é…çš„æ–‡ä»¶ä¸­çš„æ ¼å¼åŒ–æ–‡æœ¬ç‰‡æ®µ\n- æ¯ä¸ªç‰‡æ®µçš„æ–‡ä»¶è·¯å¾„å’Œè¡Œå·\n- ç›¸å…³ä»£ç éƒ¨åˆ†å‘¨å›´çš„ä¸Šä¸‹æ–‡\n- æŒ‰ç›¸å…³æ€§æ’åºçš„å¤šä¸ªç»“æœ\n\n**æŸ¥è¯¢ç¤ºä¾‹ï¼š**\n\n1. **æŸ¥æ‰¾é…ç½®ä»£ç ï¼š**\n   ```json\n   {\n     \"project_root_path\": \"C:/Users/username/projects/myproject\",\n     \"query\": \"æ—¥å¿—é…ç½® è®¾ç½® åˆå§‹åŒ– logger\"\n   }\n   ```\n   è¿”å›ï¼šä¸æ—¥å¿—è®¾ç½®ã€logger åˆå§‹åŒ–å’Œé…ç½®ç›¸å…³çš„ä»£ç \n\n2. **æŸ¥æ‰¾è®¤è¯é€»è¾‘ï¼š**\n   ```json\n   {\n     \"project_root_path\": \"C:/Users/username/projects/myproject\",\n     \"query\": \"ç”¨æˆ·è®¤è¯ ç™»å½• å¯†ç éªŒè¯\"\n   }\n   ```\n   è¿”å›ï¼šè®¤è¯å¤„ç†å™¨ã€ç™»å½•å‡½æ•°ã€å¯†ç éªŒè¯ä»£ç \n\n3. **æŸ¥æ‰¾æ•°æ®åº“ä»£ç ï¼š**\n   ```json\n   {\n     \"project_root_path\": \"C:/Users/username/projects/myproject\",\n     \"query\": \"æ•°æ®åº“è¿æ¥æ±  åˆå§‹åŒ–\"\n   }\n   ```\n   è¿”å›ï¼šæ•°æ®åº“è¿æ¥è®¾ç½®ã€è¿æ¥æ± é…ç½®ã€åˆå§‹åŒ–ä»£ç \n\n4. **æŸ¥æ‰¾é”™è¯¯å¤„ç†ï¼š**\n   ```json\n   {\n     \"project_root_path\": \"C:/Users/username/projects/myproject\",\n     \"query\": \"é”™è¯¯å¤„ç† å¼‚å¸¸ try catch\"\n   }\n   ```\n   è¿”å›ï¼šé”™è¯¯å¤„ç†æ¨¡å¼ã€å¼‚å¸¸å¤„ç†å™¨ã€try-catch å—\n\n5. **æŸ¥æ‰¾ API ç«¯ç‚¹ï¼š**\n   ```json\n   {\n     \"project_root_path\": \"C:/Users/username/projects/myproject\",\n     \"query\": \"API ç«¯ç‚¹ è·¯ç”± HTTP å¤„ç†å™¨\"\n   }\n   ```\n   è¿”å›ï¼šAPI è·¯ç”±å®šä¹‰ã€HTTP å¤„ç†å™¨ã€ç«¯ç‚¹å®ç°\n\n**è·å¾—æ›´å¥½ç»“æœçš„æŠ€å·§ï¼š**\n- ä½¿ç”¨å¤šä¸ªç›¸å…³å…³é”®è¯ï¼ˆä¾‹å¦‚ï¼Œ\"æ—¥å¿—é…ç½®è®¾ç½®\"è€Œä¸ä»…ä»…æ˜¯\"æ—¥å¿—\"ï¼‰\n- åŒ…å«æ‚¨è¦æŸ¥æ‰¾çš„ç‰¹å®šæŠ€æœ¯æœ¯è¯­\n- æè¿°åŠŸèƒ½è€Œä¸æ˜¯ç¡®åˆ‡çš„å˜é‡å\n- å¦‚æœç¬¬ä¸€æ¬¡æŸ¥è¯¢æ²¡æœ‰è¿”å›æ‚¨éœ€è¦çš„å†…å®¹ï¼Œå°è¯•ä¸åŒçš„æªè¾\n\n**ç´¢å¼•ç‰¹æ€§ï¼š**\n- **å¢é‡ç´¢å¼•**ï¼šä»…ä¸Šä¼ æ–°æ–‡ä»¶æˆ–ä¿®æ”¹è¿‡çš„æ–‡ä»¶ï¼Œè·³è¿‡æœªæ›´æ”¹çš„æ–‡ä»¶\n- **åŸºäºå“ˆå¸Œçš„å»é‡**ï¼šé€šè¿‡è·¯å¾„ + å†…å®¹çš„ SHA-256 å“ˆå¸Œè¯†åˆ«æ–‡ä»¶\n- **è‡ªåŠ¨é‡è¯•**ï¼šç½‘ç»œè¯·æ±‚è‡ªåŠ¨é‡è¯•æœ€å¤š 3 æ¬¡ï¼Œé‡‡ç”¨æŒ‡æ•°é€€é¿ï¼ˆ1ç§’ã€2ç§’ã€4ç§’ï¼‰\n- **æ‰¹æ¬¡å¼¹æ€§**ï¼šå¦‚æœæ‰¹æ¬¡ä¸Šä¼ åœ¨é‡è¯•åå¤±è´¥ï¼Œå·¥å…·ä¼šç»§ç»­å¤„ç†ä¸‹ä¸€æ‰¹æ¬¡\n- **æ–‡ä»¶åˆ†å‰²**ï¼šå¤§æ–‡ä»¶è‡ªåŠ¨åˆ†å‰²ä¸ºå¤šä¸ªå—ï¼ˆé»˜è®¤ï¼šæ¯å— 800 è¡Œï¼‰\n- **æ’é™¤æ¨¡å¼**ï¼šè‡ªåŠ¨è·³è¿‡è™šæ‹Ÿç¯å¢ƒã€node_modulesã€.gitã€æ„å»ºäº§ç‰©ç­‰\n- **å¤šç¼–ç æ”¯æŒ**ï¼šè‡ªåŠ¨æ£€æµ‹æ–‡ä»¶ç¼–ç ï¼ˆUTF-8ã€GBKã€GB2312ã€Latin-1ï¼‰ï¼Œå¹¶åœ¨å¤±è´¥æ—¶å›é€€åˆ° UTF-8 é”™è¯¯å¤„ç†\n- **.gitignore é›†æˆ**ï¼šè‡ªåŠ¨ä»é¡¹ç›®æ ¹ç›®å½•åŠ è½½å¹¶éµå®ˆ `.gitignore` æ¨¡å¼ï¼Œä¸é…ç½®çš„æ’é™¤æ¨¡å¼ç»“åˆä½¿ç”¨\n\n**æœç´¢ç‰¹æ€§ï¼š**\n- **è‡ªåŠ¨é‡è¯•**ï¼šæœç´¢è¯·æ±‚è‡ªåŠ¨é‡è¯•æœ€å¤š 3 æ¬¡ï¼Œé‡‡ç”¨æŒ‡æ•°é€€é¿ï¼ˆ2ç§’ã€4ç§’ã€8ç§’ï¼‰\n- **ä¼˜é›…é™çº§**ï¼šå¦‚æœæ‰€æœ‰é‡è¯•åæœç´¢å¤±è´¥ï¼Œè¿”å›æ¸…æ™°çš„é”™è¯¯æ¶ˆæ¯\n- **è¶…æ—¶å¤„ç†**ï¼šä½¿ç”¨ 60 ç§’è¶…æ—¶æ¥å¤„ç†é•¿æ—¶é—´è¿è¡Œçš„æœç´¢\n- **ç©ºç»“æœå¤„ç†**ï¼šå¦‚æœæœªæ‰¾åˆ°ç›¸å…³ä»£ç ï¼Œè¿”å›æœ‰ç”¨çš„æ¶ˆæ¯\n\n**é»˜è®¤æ’é™¤æ¨¡å¼ï¼š**\n```\n.venv, venv, .env, env, node_modules, .git, .svn, .hg, __pycache__,\n.pytest_cache, .mypy_cache, .tox, .eggs, *.egg-info, dist, build,\n.idea, .vscode, .DS_Store, *.pyc, *.pyo, *.pyd, .Python,\npip-log.txt, pip-delete-this-directory.txt, .coverage, htmlcov,\n.gradle, target, bin, obj\n```\næ¨¡å¼æ”¯æŒé€šé…ç¬¦ï¼ˆ`*`ã€`?`ï¼‰ï¼Œå¹¶åŒ¹é…ç›®å½•/æ–‡ä»¶åæˆ–è·¯å¾„ã€‚\n\n**æ³¨æ„ï¼š** å¦‚æœé¡¹ç›®æ ¹ç›®å½•å­˜åœ¨ `.gitignore` æ–‡ä»¶ï¼Œå…¶æ¨¡å¼å°†è‡ªåŠ¨åŠ è½½å¹¶ä¸é…ç½®çš„æ’é™¤æ¨¡å¼ç»“åˆä½¿ç”¨ã€‚`.gitignore` æ¨¡å¼éµå¾ª Git çš„æ ‡å‡† wildmatch è¯­æ³•ã€‚\n\n## é«˜çº§ç‰¹æ€§\n\n### å¤šç¼–ç æ–‡ä»¶æ”¯æŒ\n\nAcemcp è‡ªåŠ¨æ£€æµ‹å’Œå¤„ç†ä¸åŒå­—ç¬¦ç¼–ç çš„æ–‡ä»¶ï¼Œé€‚ç”¨äºå›½é™…åŒ–é¡¹ç›®ï¼š\n\n- **è‡ªåŠ¨æ£€æµ‹**ï¼šæŒ‰é¡ºåºå°è¯•å¤šç§ç¼–ç ï¼šUTF-8 â†’ GBK â†’ GB2312 â†’ Latin-1\n- **å›é€€å¤„ç†**ï¼šå¦‚æœæ‰€æœ‰ç¼–ç éƒ½å¤±è´¥ï¼Œä½¿ç”¨ UTF-8 é”™è¯¯å¤„ç†ä»¥é˜²æ­¢å´©æºƒ\n- **æ—¥å¿—è®°å½•**ï¼šè®°å½•æ¯ä¸ªæ–‡ä»¶æˆåŠŸä½¿ç”¨çš„ç¼–ç ï¼ˆDEBUG çº§",
      "default_branch": "main"
    },
    "fetched_at": "2025-11-23T02:53:46.141047"
  }
]