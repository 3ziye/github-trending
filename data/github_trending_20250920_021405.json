[
  {
    "basic_info": {
      "name": "spec-kit",
      "full_name": "github/spec-kit",
      "owner": "github",
      "description": "ğŸ’« Toolkit to help you get started with Spec-Driven Development",
      "url": "https://github.com/github/spec-kit",
      "clone_url": "https://github.com/github/spec-kit.git",
      "ssh_url": "git@github.com:github/spec-kit.git",
      "homepage": "",
      "created_at": "2025-08-21T22:54:31Z",
      "updated_at": "2025-09-20T02:10:39Z",
      "pushed_at": "2025-09-20T01:24:35Z"
    },
    "stats": {
      "stars": 22541,
      "forks": 1805,
      "watchers": 22541,
      "open_issues": 186,
      "size": 2603
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 42852,
        "PowerShell": 14700,
        "Shell": 13469
      },
      "license": "MIT License",
      "topics": [
        "ai",
        "copilot",
        "development",
        "engineering",
        "prd",
        "spec",
        "spec-driven"
      ]
    },
    "content": {
      "readme": "<div align=\"center\">\n    <img src=\"./media/logo_small.webp\"/>\n    <h1>ğŸŒ± Spec Kit</h1>\n    <h3><em>Build high-quality software faster.</em></h3>\n</div>\n\n<p align=\"center\">\n    <strong>An effort to allow organizations to focus on product scenarios rather than writing undifferentiated code with the help of Spec-Driven Development.</strong>\n</p>\n\n[![Release](https://github.com/github/spec-kit/actions/workflows/release.yml/badge.svg)](https://github.com/github/spec-kit/actions/workflows/release.yml)\n\n---\n\n## Table of Contents\n\n- [ğŸ¤” What is Spec-Driven Development?](#-what-is-spec-driven-development)\n- [âš¡ Get started](#-get-started)\n- [ğŸ“½ï¸ Video Overview](#ï¸-video-overview)\n- [ğŸ”§ Specify CLI Reference](#-specify-cli-reference)\n- [ğŸ“š Core philosophy](#-core-philosophy)\n- [ğŸŒŸ Development phases](#-development-phases)\n- [ğŸ¯ Experimental goals](#-experimental-goals)\n- [ğŸ”§ Prerequisites](#-prerequisites)\n- [ğŸ“– Learn more](#-learn-more)\n- [ğŸ“‹ Detailed process](#-detailed-process)\n- [ğŸ” Troubleshooting](#-troubleshooting)\n- [ğŸ‘¥ Maintainers](#-maintainers)\n- [ğŸ’¬ Support](#-support)\n- [ğŸ™ Acknowledgements](#-acknowledgements)\n- [ğŸ“„ License](#-license)\n\n## ğŸ¤” What is Spec-Driven Development?\n\nSpec-Driven Development **flips the script** on traditional software development. For decades, code has been king â€” specifications were just scaffolding we built and discarded once the \"real work\" of coding began. Spec-Driven Development changes this: **specifications become executable**, directly generating working implementations rather than just guiding them.\n\n## âš¡ Get started\n\n### 1. Install Specify\n\nInitialize your project depending on the coding agent you're using:\n\n```bash\nuvx --from git+https://github.com/github/spec-kit.git specify init <PROJECT_NAME>\n```\n\n### 2. Establish project principles\n\nUse the **`/constitution`** command to create your project's governing principles and development guidelines that will guide all subsequent development.\n\n```bash\n/constitution Create principles focused on code quality, testing standards, user experience consistency, and performance requirements\n```\n\n### 3. Create the spec\n\nUse the **`/specify`** command to describe what you want to build. Focus on the **what** and **why**, not the tech stack.\n\n```bash\n/specify Build an application that can help me organize my photos in separate photo albums. Albums are grouped by date and can be re-organized by dragging and dropping on the main page. Albums are never in other nested albums. Within each album, photos are previewed in a tile-like interface.\n```\n\n### 4. Create a technical implementation plan\n\nUse the **`/plan`** command to provide your tech stack and architecture choices.\n\n```bash\n/plan The application uses Vite with minimal number of libraries. Use vanilla HTML, CSS, and JavaScript as much as possible. Images are not uploaded anywhere and metadata is stored in a local SQLite database.\n```\n\n### 5. Break down into tasks\n\nUse **`/tasks`** to create an actionable task list from your implementation plan.\n\n```bash\n/tasks\n```\n\n### 6. Execute implementation\n\nUse **`/implement`** to execute all tasks and build your feature according to the plan.\n\n```bash\n/implement\n```\n\nFor detailed step-by-step instructions, see our [comprehensive guide](./spec-driven.md).\n\n## ğŸ“½ï¸ Video Overview\n\nWant to see Spec Kit in action? Watch our [video overview](https://www.youtube.com/watch?v=a9eR1xsfvHg&pp=0gcJCckJAYcqIYzv)!\n\n[![Spec Kit video header](/media/spec-kit-video-header.jpg)](https://www.youtube.com/watch?v=a9eR1xsfvHg&pp=0gcJCckJAYcqIYzv)\n\n## ğŸ”§ Specify CLI Reference\n\nThe `specify` command supports the following options:\n\n### Commands\n\n| Command     | Description                                                    |\n|-------------|----------------------------------------------------------------|\n| `init`      | Initialize a new Specify project from the latest template      |\n| `check`     | Check for installed tools (`git`, `claude`, `gemini`, `code`/`code-insiders`, `cursor-agent`, `windsurf`, `qwen`, `opencode`) |\n\n### `specify init` Arguments & Options\n\n| Argument/Option        | Type     | Description                                                                  |\n|------------------------|----------|------------------------------------------------------------------------------|\n| `<project-name>`       | Argument | Name for your new project directory (optional if using `--here`)            |\n| `--ai`                 | Option   | AI assistant to use: `claude`, `gemini`, `copilot`, `cursor`, `qwen`, `opencode`, or `windsurf` |\n| `--script`             | Option   | Script variant to use: `sh` (bash/zsh) or `ps` (PowerShell)                 |\n| `--ignore-agent-tools` | Flag     | Skip checks for AI agent tools like Claude Code                             |\n| `--no-git`             | Flag     | Skip git repository initialization                                          |\n| `--here`               | Flag     | Initialize project in the current directory instead of creating a",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-20T02:14:06.024436"
  },
  {
    "basic_info": {
      "name": "Awesome-Nano-Banana-images",
      "full_name": "PicoTrex/Awesome-Nano-Banana-images",
      "owner": "PicoTrex",
      "description": "A curated collection of fun and creative examples generated with Nano BananağŸŒ, Gemini-2.5-flash-image based model. We also release Nano-consistent-150K openly to support the community's development of image generation and unified models(click to website to see our blog)",
      "url": "https://github.com/PicoTrex/Awesome-Nano-Banana-images",
      "clone_url": "https://github.com/PicoTrex/Awesome-Nano-Banana-images.git",
      "ssh_url": "git@github.com:PicoTrex/Awesome-Nano-Banana-images.git",
      "homepage": "https://picotrex.github.io/Awesome-Nano-Banana-images/",
      "created_at": "2025-08-28T17:03:09Z",
      "updated_at": "2025-09-20T01:54:05Z",
      "pushed_at": "2025-09-18T16:32:53Z"
    },
    "stats": {
      "stars": 9695,
      "forks": 988,
      "watchers": 9695,
      "open_issues": 12,
      "size": 150241
    },
    "tech_info": {
      "language": null,
      "languages": {},
      "license": "Apache License 2.0",
      "topics": [
        "awesome",
        "gemini-2-5-flash-image",
        "nano-banana"
      ]
    },
    "content": {
      "readme": "<div align=\"center\">\n\n<img src=\"images/logo.jpg\"  alt=\"è¾“å…¥å›¾ç‰‡\"> \n\n[![License: CC BY 4.0](https://img.shields.io/badge/License-CC_BY_4.0-lightgrey.svg)](LICENSE) \n[![ç®€ä½“ä¸­æ–‡](https://img.shields.io/badge/ç®€ä½“ä¸­æ–‡-ç‚¹å‡»æŸ¥çœ‹-orange)](README.md)\n[![English](https://img.shields.io/badge/English-Click_to_View-yellow)](README_en.md)\n[![Japanese](https://img.shields.io/badge/æ—¥æœ¬èª-ã‚¯ãƒªãƒƒã‚¯ã—ã¦è¡¨ç¤º-green)](README_ja.md)\n[![Korean](https://img.shields.io/badge/í•œêµ­ì–´-ëˆŒëŸ¬ì„œ_ë³´ê¸°-blue)](README_kr.md)\n[![Turkish](https://img.shields.io/badge/TÃ¼rkÃ§e-GÃ¶rÃ¼ntÃ¼lemek_iÃ§in_TÄ±klayÄ±n-red)](README_tr.md)\n\n</div>\n\n> [!NOTE]\n> æˆ‘ä»¬æå‡º Nano-consistent-150kâ€”â€”é¦–ä¸ªåŸºäº Nano-Banana æ„å»ºã€è§„æ¨¡è¶…è¿‡ 150k çš„é«˜è´¨é‡æ•°æ®é›†ï¼Œä¸“ä¸ºåœ¨å¤šæ ·è€Œå¤æ‚çš„ç¼–è¾‘åœºæ™¯ä¸­ä¿æŒäººç‰©èº«ä»½ä¸€è‡´æ€§è€Œè®¾è®¡ã€‚å…¶ä¸€å¤§ç‰¹ç‚¹æ˜¯å“è¶Šçš„èº«ä»½ä¸€è‡´æ€§ï¼šé’ˆå¯¹åŒä¸€äººåƒï¼Œæˆ‘ä»¬åœ¨å¤šç§ä»»åŠ¡ä¸æŒ‡ä»¤ä¸‹æä¾›äº† 35 ç§ä»¥ä¸Šä¸åŒçš„ç¼–è¾‘ç»“æœã€‚ä»¥ä¸€è‡´çš„äººç‰©èº«ä»½ä¸ºé”šç‚¹ï¼Œè¯¥æ•°æ®é›†ä½¿å¾—å›´ç»•åŒä¸€ä¸»ä½“åœ¨å¤šç§ç¼–è¾‘ä»»åŠ¡ã€æŒ‡ä»¤ä¸æ¨¡æ€ä¹‹é—´æ— ç¼è¡”æ¥çš„äº¤é”™ï¼ˆinterleavedï¼‰æ•°æ®æ„å»ºæˆä¸ºå¯èƒ½ã€‚\n<a href='https://picotrex.github.io/Awesome-Nano-Banana-images/'><img src='https://img.shields.io/badge/ğŸŒ Website-Blog-orange' height=\"25\"></a>\n<a href='https://huggingface.co/datasets/Yejy53/Nano-consistent-150k'><img src='https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Dataset-yellow' height=\"25\"></a>\n\n## ğŸŒ Introduction\n\næ¬¢è¿æ¥åˆ° Nano-banana ç²¾é€‰å›¾ç‰‡åº“ï¼ğŸ¤— \n\n**æˆ‘ä»¬æ”¶é›†äº†Nano-bananaåœ¨å„ä¸ªä»»åŠ¡åœºæ™¯ä¸‹ç”Ÿæˆçš„ä»¤äººæƒŠè‰³çš„å›¾ç‰‡å’Œæç¤ºè¯**ï¼Œå…¨æ–¹ä½å±•ç¤ºGoogleåœ¨å›¾åƒç”Ÿæˆä¸ç¼–è¾‘çš„æ— é™å¯èƒ½ã€‚å¸Œæœ›èƒ½å¸®åŠ©ä½ æ›´å¥½åœ°äº†è§£Nano-bananaã€‚å¿«ä¸€èµ·è§£é” Nano-banana çš„å¤šå›¾èåˆä¸åˆ›æ„ç¼–è¾‘åŠ›é‡å§ï¼âœ¨\n\nè¿™äº›æ¡ˆä¾‹ä¸»è¦æ¥æºäºTwitter/ X ğŸ¦ã€å°çº¢ä¹¦ğŸ“•ç­‰è‡ªåª’ä½“å¹³å°ã€‚\n\nå–œæ¬¢å°±ç‚¹ â­ Star æ”¶è—èµ·æ¥å§ï¼\n\n## ğŸ“° News\n\n- **2025å¹´9æœˆ18æ—¥ï¼š** æˆ‘ä»¬å‘å¸ƒäº† [**Nano-consistent-150k**](https://picotrex.github.io/Awesome-Nano-Banana-images/) æ•°æ®é›†\n- **2025å¹´9æœˆ16æ—¥ï¼š** 4ï¸âƒ£ ç¬¬å››æ¬¡ä»“åº“æ›´æ–°\n- **2025å¹´9æœˆ9æ—¥ï¼š** 3ï¸âƒ£ ç¬¬ä¸‰æ¬¡ä»“åº“æ›´æ–°\n- **2025å¹´9æœˆ3æ—¥ï¼š** 2ï¸âƒ£ ç¬¬äºŒæ¬¡ä»“åº“æ›´æ–°\n- **2025å¹´8æœˆ28æ—¥ï¼š** ğŸ‰ 1ï¸âƒ£ ${\\color{red} ç¬¬ä¸€æ¬¡\\ Awesome-Nano-Banana-images \\ æ›´æ–°!}$\n\n## ğŸ“‘ Menu\n\n- [ğŸŒ Introduction](#-introduction)\n- [ğŸ“° News](#-news)\n- [ğŸ“‘ Menu](#-menu)\n- [ğŸ–¼ï¸ ä¾‹å­](#ï¸-ä¾‹å­)\n  - [ä¾‹ 1: æ’ç”»å˜æ‰‹åŠï¼ˆby @ZHO\\_ZHO\\_ZHOï¼‰](#ä¾‹-1-æ’ç”»å˜æ‰‹åŠby-zho_zho_zho)\n  - [ä¾‹ 2: æ ¹æ®åœ°å›¾ç®­å¤´ç”Ÿæˆåœ°é¢è§†è§’å›¾ç‰‡ï¼ˆby @tokuminï¼‰](#ä¾‹-2-æ ¹æ®åœ°å›¾ç®­å¤´ç”Ÿæˆåœ°é¢è§†è§’å›¾ç‰‡by-tokumin)\n  - [ä¾‹ 3: çœŸå®ä¸–ç•Œçš„ARä¿¡æ¯åŒ–ï¼ˆby @bilawalsidhuï¼‰](#ä¾‹-3-çœŸå®ä¸–ç•Œçš„arä¿¡æ¯åŒ–by-bilawalsidhu)\n  - [ä¾‹ 4: åˆ†ç¦»å‡º3Då»ºç­‘/åˆ¶ä½œç­‰è·æ¨¡å‹ï¼ˆby @Zieeettï¼‰](#ä¾‹-4-åˆ†ç¦»å‡º3då»ºç­‘åˆ¶ä½œç­‰è·æ¨¡å‹by-zieeett)\n  - [ä¾‹ 5: ä¸åŒæ—¶ä»£è‡ªå·±çš„ç…§ç‰‡ï¼ˆby @AmirMushichï¼‰](#ä¾‹-5-ä¸åŒæ—¶ä»£è‡ªå·±çš„ç…§ç‰‡by-amirmushich)\n  - [ä¾‹ 6: å¤šå‚è€ƒå›¾åƒç”Ÿæˆï¼ˆby @MrDavids1ï¼‰](#ä¾‹-6-å¤šå‚è€ƒå›¾åƒç”Ÿæˆby-mrdavids1)\n  - [ä¾‹ 7: è‡ªåŠ¨ä¿®å›¾ï¼ˆby @op7418ï¼‰](#ä¾‹-7-è‡ªåŠ¨ä¿®å›¾by-op7418)\n  - [ä¾‹ 8: æ‰‹ç»˜å›¾æ§åˆ¶å¤šè§’è‰²å§¿æ€ï¼ˆby @op7418ï¼‰](#ä¾‹-8-æ‰‹ç»˜å›¾æ§åˆ¶å¤šè§’è‰²å§¿æ€by-op7418)\n  - [ä¾‹ 9: è·¨è§†è§’å›¾åƒç”Ÿæˆï¼ˆby @op7418ï¼‰](#ä¾‹-9-è·¨è§†è§’å›¾åƒç”Ÿæˆby-op7418)\n  - [ä¾‹ 10: å®šåˆ¶äººç‰©è´´çº¸ï¼ˆby @op7418ï¼‰](#ä¾‹-10-å®šåˆ¶äººç‰©è´´çº¸by-op7418)\n  - [ä¾‹ 11: åŠ¨æ¼«è½¬çœŸäººCoserï¼ˆby @ZHO\\_ZHO\\_ZHOï¼‰](#ä¾‹-11-åŠ¨æ¼«è½¬çœŸäººcoserby-zho_zho_zho)\n  - [ä¾‹ 12: ç”Ÿæˆè§’è‰²è®¾å®šï¼ˆby @ZHO\\_ZHO\\_ZHOï¼‰](#ä¾‹-12-ç”Ÿæˆè§’è‰²è®¾å®šby-zho_zho_zho)\n  - [ä¾‹ 13: è‰²å¡çº¿ç¨¿ä¸Šè‰²ï¼ˆby @ZHO\\_ZHO\\_ZHOï¼‰](#ä¾‹-13-è‰²å¡çº¿ç¨¿ä¸Šè‰²by-zho_zho_zho)\n  - [ä¾‹ 14: æ–‡ç« ä¿¡æ¯å›¾ï¼ˆby @é»„å»ºåŒå­¦ï¼‰](#ä¾‹-14-æ–‡ç« ä¿¡æ¯å›¾by-é»„å»ºåŒå­¦)\n  - [ä¾‹ 15: æ›´æ¢å¤šç§å‘å‹ï¼ˆby @balconychyï¼‰](#ä¾‹-15-æ›´æ¢å¤šç§å‘å‹by-balconychy)\n  - [ä¾‹ 16: æ¨¡å‹æ ‡æ³¨è®²è§£å›¾ï¼ˆby @berryxia\\_aiï¼‰](#ä¾‹-16-æ¨¡å‹æ ‡æ³¨è®²è§£å›¾by-berryxia_ai)\n  - [ä¾‹ 17: å®šåˆ¶å¤§ç†çŸ³é›•å¡‘ï¼ˆby @umesh\\_aiï¼‰](#ä¾‹-17-å®šåˆ¶å¤§ç†çŸ³é›•å¡‘by-umesh_ai)\n  - [ä¾‹ 18: æ ¹æ®é£Ÿæåšèœï¼ˆby @Gdgtifyï¼‰](#ä¾‹-18-æ ¹æ®é£Ÿæåšèœby-gdgtify)\n  - [ä¾‹ 19: æ•°å­¦é¢˜æ¨ç†ï¼ˆby @Gorden Sunï¼‰](#ä¾‹-19-æ•°å­¦é¢˜æ¨ç†by-gorden-sun)\n  - [ä¾‹ 20: æ—§ç…§ç‰‡ä¸Šè‰²ï¼ˆby @GeminiAppï¼‰](#ä¾‹-20-æ—§ç…§ç‰‡ä¸Šè‰²by-geminiapp)\n  - [ä¾‹ 21: OOTDç©¿æ­ï¼ˆby @302.AIï¼‰](#ä¾‹-21-ootdç©¿æ­by-302ai)\n  - [ä¾‹ 22: äººç‰©æ¢è¡£ï¼ˆby @skiranoï¼‰](#ä¾‹-22-äººç‰©æ¢è¡£by-skirano)\n  - [ä¾‹ 23: å¤šè§†å›¾ç»“æœç”Ÿæˆï¼ˆby @Error\\_HTTP\\_404ï¼‰](#ä¾‹-23-å¤šè§†å›¾ç»“æœç”Ÿæˆby-error_http_404)\n  - [ä¾‹ 24: ç”µå½±åˆ†é•œï¼ˆby @GeminiAppï¼‰](#ä¾‹-24-ç”µå½±åˆ†é•œby-geminiapp)\n  - [ä¾‹ 25: äººç‰©å§¿åŠ¿ä¿®æ”¹ï¼ˆby @arrakis\\_aiï¼‰](#ä¾‹-25-äººç‰©å§¿åŠ¿ä¿®æ”¹by-arrakis_ai)\n  - [ä¾‹ 26: çº¿ç¨¿å›¾ç”Ÿæˆå›¾åƒï¼ˆby @ZHO\\_ZHO\\_ZHOï¼‰](#ä¾‹-26-çº¿ç¨¿å›¾ç”Ÿæˆå›¾åƒby-zho_zho_zho)\n  - [ä¾‹ 27: ä¸ºå›¾åƒæ·»åŠ æ°´å°ï¼ˆby @AiMacheteï¼‰](#ä¾‹-27-ä¸ºå›¾åƒæ·»åŠ æ°´å°by-aimachete)\n  - [ä¾‹ 28: çŸ¥è¯†æ¨ç†ç”Ÿæˆå›¾åƒï¼ˆby @icreatelifeï¼‰](#ä¾‹-28-çŸ¥è¯†æ¨ç†ç”Ÿæˆå›¾åƒby-icreatelife)\n  - [ä¾‹ 29: çº¢ç¬”æ‰¹æ³¨ï¼ˆby @AiMacheteï¼‰](#ä¾‹-29-çº¢ç¬”æ‰¹æ³¨by-aimachete)\n  - [ä¾‹ 30: çˆ†ç‚¸çš„é£Ÿç‰©ï¼ˆby @icreatelifeï¼‰](#ä¾‹-30-çˆ†ç‚¸çš„é£Ÿç‰©by-icreatelife)\n  - [ä¾‹ 31: åˆ¶ä½œæ¼«ç”»ä¹¦ï¼ˆby @icreatelifeï¼‰](#ä¾‹-31-åˆ¶ä½œæ¼«ç”»ä¹¦by-icreatelife)\n  - [ä¾‹ 32: åŠ¨ä½œäººå¶ï¼ˆby @icreatelifeï¼‰](#ä¾‹-32-åŠ¨ä½œäººå¶by-icreatelife)\n  - [ä¾‹ 33: åœ°å›¾ç”Ÿæˆç­‰è·å»ºç­‘ï¼ˆby @demishassabisï¼‰](#ä¾‹-33-åœ°å›¾ç”Ÿæˆç­‰è·å»ºç­‘by-demishassabis)\n  - [ä¾‹ 34: å‚è€ƒå›¾æ§åˆ¶äººç‰©è¡¨æƒ…ï¼ˆby @ZHO\\_ZHO\\_ZHOï¼‰](#ä¾‹-34-å‚è€ƒå›¾æ§åˆ¶äººç‰©è¡¨æƒ…by-zho_zho_zho)\n  - [ä¾‹ 35: æ’ç”»ç»˜ç”»è¿‡ç¨‹å››æ ¼ï¼ˆby @ZHO\\_ZHO\\_ZHOï¼‰](#ä¾‹-35-æ’ç”»ç»˜ç”»è¿‡ç¨‹å››æ ¼by-zho_zho_zho)\n  - [ä¾‹ 36: è™šæ‹Ÿè¯•å¦†ï¼ˆby @ZHO\\_ZHO\\_ZHOï¼‰](#ä¾‹-36-è™šæ‹Ÿè¯•å¦†by-zho_zho_zho)\n  - [ä¾‹ 37: å¦†é¢åˆ†æï¼ˆby @ZHO\\_ZHO\\_ZHOï¼‰](#ä¾‹-37-å¦†é¢åˆ†æby-zho_zho_zho)\n  - [ä¾‹ 38: Googleåœ°å›¾è§†è§’ä¸‹çš„ä¸­åœŸä¸–ç•Œï¼ˆby @TechHalloï¼‰](#ä¾‹-38-googleåœ°å›¾è§†è§’ä¸‹çš„ä¸­åœŸä¸–ç•Œby-techhallo)\n  - [ä¾‹ 39: å°åˆ·æ’ç”»ç”Ÿæˆï¼ˆby @Umeshï¼‰](#ä¾‹-39-å°åˆ·æ’ç”»ç”Ÿæˆby-umesh)\n  - [ä¾‹ 40: è¶…å¤šäººç‰©å§¿åŠ¿ç”Ÿæˆï¼ˆby @tapehead\\_Labï¼‰](#ä¾‹-40-è¶…å¤šäººç‰©å§¿åŠ¿ç”Ÿæˆby-tapehead_lab)\n  - [ä¾‹ 41: ç‰©å“åŒ…è£…ç”Ÿæˆï¼ˆby @ZHO\\_ZHO\\_ZHOï¼‰](#ä¾‹-41-ç‰©å“åŒ…è£…ç”Ÿæˆby-zho_zho_zho)\n  - [ä¾‹ 42: å åŠ æ»¤é•œ/æè´¨ï¼ˆby @ZHO\\_ZHO\\_ZHOï¼‰](#ä¾‹-42-å åŠ æ»¤é•œæè´¨by-zho_zho_zho)\n  - [ä¾‹ 43: æ§åˆ¶äººç‰©è„¸å‹ï¼ˆby @ZHO\\_ZHO\\_ZHOï¼‰](#ä¾‹-43-æ§åˆ¶äººç‰©è„¸å‹by-zho_zho_zho)\n  - [ä¾‹ 44: å…‰å½±æ§åˆ¶ï¼ˆby @ZHO\\_ZHO\\_ZHOï¼‰](#ä¾‹-44-å…‰å½±æ§åˆ¶by-zho_zho_zho)\n  - [ä¾‹ 45: ä¹é«˜ç©å…·å°äººï¼ˆby @ZHO\\_ZHO\\_ZHOï¼‰](#ä¾‹-45-ä¹é«˜ç©å…·å°äººby-zho_zho_zho)\n  - [ä¾‹ 46: é«˜è¾¾æ¨¡å‹å°äººï¼ˆby @ZHO\\_ZHO\\_ZHOï¼‰](#ä¾‹-46-é«˜è¾¾æ¨¡å‹å°äººby-zho_zho_zho)\n  - [ä¾‹ 47: ç¡¬ä»¶æ‹†è§£å›¾ï¼ˆby @AIimaginedï¼‰](#ä¾‹-47-ç¡¬ä»¶æ‹†è§£å›¾by-aiimagined)\n  - [ä¾‹ 48: é£Ÿç‰©å¡è·¯é‡Œæ ‡æ³¨ï¼ˆby @icreatelifeï¼‰](#ä¾‹-48-é£Ÿç‰©å¡è·¯é‡Œæ ‡æ³¨by-icreatelife)\n  - [ä¾‹ 49: æå–ä¿¡æ¯å¹¶æ”¾ç½®é€æ˜å›¾å±‚ï¼ˆby @nglprzï¼‰](#ä¾‹-49-æå–ä¿¡æ¯å¹¶æ”¾ç½®é€æ˜å›¾å±‚by-nglprz)\n  - [ä¾‹ 50: å›¾åƒå¤–æ‰©ä¿®å¤ï¼ˆby @bwabbageï¼‰](#ä¾‹-50-å›¾åƒå¤–æ‰©ä¿®å¤by-bwabbage)\n  - [ä¾‹ 51: å¤è€åœ°å›¾ç”Ÿæˆå¤ä»£åœºæ™¯ï¼ˆby @levelsioï¼‰](#ä¾‹-51-å¤è€åœ°å›¾ç”Ÿæˆå¤ä»£åœºæ™¯by-levelsio)\n  - [ä¾‹ 52: æ—¶å°šæœè£…æ‹¼è´´ç”»ï¼ˆby @tetumemoï¼‰](#ä¾‹-52-æ—¶å°šæœè£…æ‹¼è´´ç”»by-tetume",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-20T02:14:07.141055"
  },
  {
    "basic_info": {
      "name": "VibeVoice",
      "full_name": "microsoft/VibeVoice",
      "owner": "microsoft",
      "description": "Frontier Open-Source Text-to-Speech",
      "url": "https://github.com/microsoft/VibeVoice",
      "clone_url": "https://github.com/microsoft/VibeVoice.git",
      "ssh_url": "git@github.com:microsoft/VibeVoice.git",
      "homepage": "",
      "created_at": "2025-08-25T13:24:01Z",
      "updated_at": "2025-09-20T01:40:27Z",
      "pushed_at": "2025-09-05T15:02:35Z"
    },
    "stats": {
      "stars": 9034,
      "forks": 1056,
      "watchers": 9034,
      "open_issues": 25,
      "size": 55066
    },
    "tech_info": {
      "language": null,
      "languages": {},
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "<div align=\"center\">\n\n## ğŸ™ï¸ VibeVoice: A Frontier Long Conversational Text-to-Speech Model\n[![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=microsoft)](https://microsoft.github.io/VibeVoice)\n[![Hugging Face](https://img.shields.io/badge/HuggingFace-Collection-orange?logo=huggingface)](https://huggingface.co/collections/microsoft/vibevoice-68a2ef24a875c44be47b034f)\n[![Technical Report](https://img.shields.io/badge/Technical-Report-red?logo=adobeacrobatreader)](https://arxiv.org/pdf/2508.19205)\n\n\n</div>\n\n\n<div align=\"center\">\n<picture>\n  <source media=\"(prefers-color-scheme: dark)\" srcset=\"Figures/VibeVoice_logo_white.png\">\n  <img src=\"Figures/VibeVoice_logo.png\" alt=\"VibeVoice Logo\" width=\"300\">\n</picture>\n</div>\n\n\n**2025-09-05: VibeVoice is an open-source research framework intended to advance collaboration in the speech synthesis community. After release, we discovered instances where the tool was used in ways inconsistent with the stated intent. Since responsible use of AI is one of Microsoftâ€™s guiding principles, we have disabled this repo until we are confident that out-of-scope use is no longer possible.**\n\n<br>\n\nVibeVoice is a novel framework designed for generating **expressive**, **long-form**, **multi-speaker** conversational audio, such as podcasts, from text. It addresses significant challenges in traditional Text-to-Speech (TTS) systems, particularly in scalability, speaker consistency, and natural turn-taking.\n\nA core innovation of VibeVoice is its use of continuous speech tokenizers (Acoustic and Semantic) operating at an ultra-low frame rate of 7.5 Hz. These tokenizers efficiently preserve audio fidelity while significantly boosting computational efficiency for processing long sequences. VibeVoice employs a [next-token diffusion](https://arxiv.org/abs/2412.08635) framework, leveraging a Large Language Model (LLM) to understand textual context and dialogue flow, and a diffusion head to generate high-fidelity acoustic details.\n\nThe model can synthesize speech up to **90 minutes** long with up to **4 distinct speakers**, surpassing the typical 1-2 speaker limits of many prior models. \n\n\n<p align=\"left\">\n  <img src=\"Figures/MOS-preference.png\" alt=\"MOS Preference Results\" height=\"260px\">\n  <img src=\"Figures/VibeVoice.jpg\" alt=\"VibeVoice Overview\" height=\"250px\" style=\"margin-right: 10px;\">\n</p>\n\n\n### ğŸµ Demo Examples\n\n\n**Video Demo**\n\nWe produced this video with [Wan2.2](https://github.com/Wan-Video/Wan2.2). We sincerely appreciate the Wan-Video team for their great work.\n\n**English**\n<div align=\"center\">\n\nhttps://github.com/user-attachments/assets/0967027c-141e-4909-bec8-091558b1b784\n\n</div>\n\n\n**Chinese**\n<div align=\"center\">\n\nhttps://github.com/user-attachments/assets/322280b7-3093-4c67-86e3-10be4746c88f\n\n</div>\n\n**Cross-Lingual**\n<div align=\"center\">\n\nhttps://github.com/user-attachments/assets/838d8ad9-a201-4dde-bb45-8cd3f59ce722\n\n</div>\n\n**Spontaneous Singing**\n<div align=\"center\">\n\nhttps://github.com/user-attachments/assets/6f27a8a5-0c60-4f57-87f3-7dea2e11c730\n\n</div>\n\n\n**Long Conversation with 4 people**\n<div align=\"center\">\n\nhttps://github.com/user-attachments/assets/a357c4b6-9768-495c-a576-1618f6275727\n\n</div>\n\nFor more examples, see the [Project Page](https://microsoft.github.io/VibeVoice).\n\n\n\n## Risks and limitations\n\nWhile efforts have been made to optimize it through various techniques, it may still produce outputs that are unexpected, biased, or inaccurate. VibeVoice inherits any biases, errors, or omissions produced by its base model (specifically, Qwen2.5 1.5b in this release).\nPotential for Deepfakes and Disinformation: High-quality synthetic speech can be misused to create convincing fake audio content for impersonation, fraud, or spreading disinformation. Users must ensure transcripts are reliable, check content accuracy, and avoid using generated content in misleading ways. Users are expected to use the generated content and to deploy the models in a lawful manner, in full compliance with all applicable laws and regulations in the relevant jurisdictions. It is best practice to disclose the use of AI when sharing AI-generated content.\n\nEnglish and Chinese only: Transcripts in languages other than English or Chinese may result in unexpected audio outputs.\n\nNon-Speech Audio: The model focuses solely on speech synthesis and does not handle background noise, music, or other sound effects.\n\nOverlapping Speech: The current model does not explicitly model or generate overlapping speech segments in conversations.\n\nWe do not recommend using VibeVoice in commercial or real-world applications without further testing and development. This model is intended for research and development purposes only. Please use responsibly.\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-20T02:14:08.262130"
  },
  {
    "basic_info": {
      "name": "term.everything",
      "full_name": "mmulet/term.everything",
      "owner": "mmulet",
      "description": "Run any GUI app in the terminalâ—",
      "url": "https://github.com/mmulet/term.everything",
      "clone_url": "https://github.com/mmulet/term.everything.git",
      "ssh_url": "git@github.com:mmulet/term.everything.git",
      "homepage": "",
      "created_at": "2025-09-07T02:52:48Z",
      "updated_at": "2025-09-20T02:10:08Z",
      "pushed_at": "2025-09-18T04:30:07Z"
    },
    "stats": {
      "stars": 5668,
      "forks": 118,
      "watchers": 5668,
      "open_issues": 16,
      "size": 53541
    },
    "tech_info": {
      "language": "TypeScript",
      "languages": {
        "TypeScript": 219331,
        "C++": 39626,
        "JavaScript": 2607,
        "Meson": 2286,
        "Shell": 1291,
        "C": 314
      },
      "license": "GNU Affero General Public License v3.0",
      "topics": [
        "alacritty",
        "cli",
        "foss",
        "iterm2",
        "kitty",
        "linux",
        "ssh",
        "terminal",
        "wayland",
        "wayland-compositor"
      ]
    },
    "content": {
      "readme": "\n\n\n<table>\n  <tr>\n    <td valign=\"middle\">\n      <img width=\"128\" height=\"128\" alt=\"icon2\" src=\"./resources/icon.png\" />\n    </td>\n    <td><h1>Term.Everythingâ—</h1></td>\n    <td><a href=\"https://github.com/mmulet/term.everything/releases\">Download the beta test now!</a></td>\n    <td><a href=\"./resources/HowIDidIt.md\">HowIDidIt.md</a></td>\n  </tr>\n  <tr>\n    <td></td>\n    <td>Works on both x11 and Wayland host systems.</td>\n    <td></td>\n    <td></td>\n  </tr>\n</table>\n\n## Run every GUI app in the terminal!\n\n![warp_into_terminal0001-0195](./resources/graphics/warp_in_2.gif)\n\n## Even over ssh!\nBehold as I play a [video game in a font](https://github.com/mmulet/font-game-engine) in a web browser in a terminal transmitted over ssh (with one hand tied behind my back)!\n\n![ssh_example](./resources/graphics/ssh_example.gif)\n\n### Read about how it works!\nCheck out [HowIDidIt.md](./resources/HowIDidIt.md)\n\n## More Examples\nThe quality of the window is limited to the number of rows and columns in your\nterminal. If you increase the resolution (ctrl - in alacritty, check your\nterminal) the quality will go up, (but performance may go down).\n\nHere I open up the Wing It! movie, and increase the quality until I get both\na good frame rate and resolution:\n\n![increase resolution](./resources/graphics/show_increase_res.gif)\n\n----------------\n\nIf your terminal supports images (like [kitty](https://sw.kovidgoyal.net/kitty/)\nor [iterm2](https://iterm2.com/)) you can render windows at full resolution\n(performance may degrade).\n\nIn this example, on my mac, I open iTerm2 ssh into ubuntu and open firefox\nat full resolution:\n\n![full_resultion](resources/graphics/full_resultion.gif)\n\n------------\n\nI feel like every single day I hear about another terminal file viewer. I say, stop making terminal file viewers because you can just use the file viewer you already have! In your terminal!\n\n![file_manager](./resources/graphics/file_manager.gif)\n\n-------------\n\nTerminal in a terminal in a terminal in a terminal in a terminal.... it's terminals all the way down.\n![terminal_in_terminal](./resources/graphics/terminal_in_terminal.gif)\n\n-------------\nWith only a small amount hacking, it can run Doom (shareware episode)!\n\n![Doom](./resources/graphics/doom.gif)\n------\nRun an entire Desktop in your terminal!\n[@ismail-yilmaz](https://github.com/ismail-yilmaz) is running Firefox, on [KDE Neon](https://neon.kde.org) in a [VM](https://gitlab.gnome.org/GNOME/gnome-boxes) on [Bobcat](https://github.com/ismail-yilmaz/Bobcat)\n![Desktop in VM](./resources/graphics/desktop_in_vm.gif)\n\nAnd this isn't even full resolution! Checkout the [full vid in in the discussions](https://github.com/mmulet/term.everything/discussions/16#discussioncomment-14390137)\n\n## About\n`term.everythingâ—` is a Linux CLI program to run GUI windows in your terminal. Specifically, `term.everythingâ—` is a built-from-scratch [Wayland](https://wiki.archlinux.org/title/Wayland) compositor that outputs to a terminal rather than your monitor.\n\n>Don't know what Wayland is or just want to know more about how this works? Then, head over to [HowIDidIt.md](./resources/HowIDidIt.md) where I will explain how everything works in detail.\n\n## Try it out!\n[Download the beta test now!](https://github.com/mmulet/term.everything/releases)\n\n## Roadmap\n1. [x] Term some things <--- This is where we are at\n  - Some apps or (even most apps) may fail to launch or even crash! Please create [an issue]( https://github.com/mmulet/term.everything/issues) if you have problems.\n2. [ ] Term most things\n3. [ ] Term everythingâ—\n\n## Help and Usage\nCheck out the [help file here](./resources/help.md) for a usage guide on how to use `term.everythingâ—`\n\n## Contributing\nterm.everythingâ— is written in developer friendly [Typescript](https://www.typescriptlang.org/) using the [bun](https://bun.com/) engine, with a just a smidge of C++.\nSee [./Contributing.md](./Contributing.md).\n\n## Legal:\n\nterm.everythingâ— copyright 2025 Late for Dinner Studios, LLC\n---\nFontemon copyright 2021 Late for Dinner Studios, LLC\n---\nWing It! movie is licensed under the Creative Commons Attribution 4.0 license\n[Wing it licensing page](https://studio.blender.org/projects/wing-it/pages/licensing/)\nAttribution:\n(CC) Blender Foundation | studio.blender.org\n---\nDoom shareware episode is copyright 1993 id Software\n---\n\n## Bonus:\nThis is Gwerm the Term Worm.\n\n![this is gwern](./resources/graphics/this_is_gwern.gif)\n\nHe is doing okay. Thanks for asking.\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-20T02:14:09.381028"
  },
  {
    "basic_info": {
      "name": "LidAngleSensor",
      "full_name": "samhenrigold/LidAngleSensor",
      "owner": "samhenrigold",
      "description": "tfw when you when your lid when uhh angle your lid sensor",
      "url": "https://github.com/samhenrigold/LidAngleSensor",
      "clone_url": "https://github.com/samhenrigold/LidAngleSensor.git",
      "ssh_url": "git@github.com:samhenrigold/LidAngleSensor.git",
      "homepage": "https://samhenri.gold",
      "created_at": "2025-09-06T19:07:20Z",
      "updated_at": "2025-09-20T00:29:05Z",
      "pushed_at": "2025-09-08T21:30:26Z"
    },
    "stats": {
      "stars": 3262,
      "forks": 122,
      "watchers": 3262,
      "open_issues": 31,
      "size": 1486
    },
    "tech_info": {
      "language": "Objective-C",
      "languages": {
        "Objective-C": 49586
      },
      "license": "Apache License 2.0",
      "topics": []
    },
    "content": {
      "readme": "# Lid Angle Sensor\n\nHi, Iâ€™m Sam Gold. Did you know that you have ~rights~ a lid angle sensor in your MacBook? [The ~Constitution~ human interface device utility says you do.](https://youtu.be/wqnHtGgVAUE?t=21)\n\nThis is a little utility that shows the angle from the sensor and, optionally, plays a wooden door creaking sound if you adjust it reeaaaaaal slowly.\n\n## FAQ\n\n**What is a lid angle sensor?**\n\nDespite what the name would have you believe, it is a sensor that detects the angle of the lid.\n\n**Which devices have a lid angle sensor?**\n\nIt was introduced with the 2019 16-inch MacBook Pro. If your laptop is newer, you probably have it. [People have reported](https://github.com/samhenrigold/LidAngleSensor/issues/13) that it **does not work on M1 devices**, I have not yet figured out a fix.\n\n**My laptop should have it, why doesn't it show up?**\n\nI've only tested this on my M4 MacBook Pro and have hard-coded it to look for a specific sensor. If that doesn't work, try running [this script](https://gist.github.com/samhenrigold/42b5a92d1ee8aaf2b840be34bff28591) and report the output in [an issue](https://github.com/samhenrigold/LidAngleSensor/issues/new/choose).\n\nKnown problematic models:\n\n- M1 MacBook Air\n- M1 MacBook Pro\n\n**Can I use this on my iMac?**\n\n~~Not yet tested. Feel free to slam your computer into your desk and make a PR with your results.~~\n\n[It totally works](https://github.com/samhenrigold/LidAngleSensor/issues/33). If it doesn't work for you, try slamming your computer harder?\n\n**Why?**\n\nA lot of free time. I'm open to full-time work in NYC or remote. I'm a designer/design-engineer. https://samhenri.gold\n\n**No I mean like why does my laptop need to know the exact angle of its lid?**\n\nOh. I don't know.\n\n**Can I contribute?**\n\nI guess.\n\n**Why does it say it's by Lisa?**\n\nI signed up for my developer account when I was a kid, used my mom's name, and now it's stuck that way forever and I can't change it. That's life.\n\n**How come the audio feels kind of...weird?**\n\nI'm bad at audio.\n\n**Where did the sound effect come from?**\n\nLEGO Batman 3: Beyond Gotham. But you knew that already.\n\n**Can I turn off the sound?**\n\nYes, never click \"Start Audio\". But this energy isn't encouraged.\n\n## Building\n\nAccording to [this issue](https://github.com/samhenrigold/LidAngleSensor/issues/12), building requires having Xcode installed. I've only tested this on Xcode 26. YMMV.\n\n## Installation\n\nVia Homebrew:\n\n```shell\nbrew install lidanglesensor\n```\n\n## Related projects\n\n- [Python library that taps into this sensor](https://github.com/tcsenpai/pybooklid)\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-20T02:14:10.504269"
  },
  {
    "basic_info": {
      "name": "growchief",
      "full_name": "growchief/growchief",
      "owner": "growchief",
      "description": "The Ultimate all-in social media automation (outreach) tool ğŸ¤–",
      "url": "https://github.com/growchief/growchief",
      "clone_url": "https://github.com/growchief/growchief.git",
      "ssh_url": "git@github.com:growchief/growchief.git",
      "homepage": "https://growchief.com",
      "created_at": "2025-08-21T08:06:06Z",
      "updated_at": "2025-09-19T21:13:44Z",
      "pushed_at": "2025-09-09T16:29:45Z"
    },
    "stats": {
      "stars": 3051,
      "forks": 198,
      "watchers": 3051,
      "open_issues": 0,
      "size": 851
    },
    "tech_info": {
      "language": "TypeScript",
      "languages": {
        "TypeScript": 834458,
        "JavaScript": 7990,
        "CSS": 3660,
        "Shell": 517,
        "HTML": 506
      },
      "license": "GNU Affero General Public License v3.0",
      "topics": [
        "automation",
        "n8n",
        "nestjs",
        "nodejs",
        "outreach",
        "social-media",
        "temporal"
      ]
    },
    "content": {
      "readme": "<p align=\"center\">\n  <a href=\"https://growchief.com/\" target=\"_blank\">\n  <picture>\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://github.com/user-attachments/assets/1ba8233b-856e-448e-899b-5f9445e65d85\">\n    <img alt=\"GrowChief Logo\" src=\"https://github.com/user-attachments/assets/50401d55-d870-478a-a5c4-ef6b474e6ccc\" width=\"280\"/>\n  </picture>\n  </a>\n</p>\n\n<p align=\"center\">\n<a href=\"https://opensource.org/license/agpl-v3\">\n  <img src=\"https://img.shields.io/badge/License-AGPL%203.0-blue.svg\" alt=\"License\">\n</a>\n</p>\n\n<div align=\"center\">\n  <strong>\n  <h2>Your ultimate social media automation tool (outreach tool)</h2><br />\n  <a href=\"https://growchief.com\">GrowChief</a>: An alternative to: Phantom Buster, Expandi, Zopto, LinkedIn Helper, Meet Alfred, etc.<br /><br />\n  </strong>\n  GrowChief is an API based tool to automate your social media accounts such as<br />sending connection requests and follow-up messages. Perfect for n8n / Make / Zapier users\n</div>\n\n<div class=\"flex\" align=\"center\">\n  <br />\n  <img alt=\"Linkedin\" src=\"https://postiz.com/svgs/socials/Linkedin.svg\" width=\"32\">\n  <img alt=\"X\" src=\"https://postiz.com/svgs/socials/X.svg\" width=\"32\">\n</div>\n<p align=\"center\">\n  <br />\n  <a href=\"https://docs.growchief.com\" rel=\"dofollow\"><strong>Explore the docs Â»</strong></a>\n  <br />\n</p>\n<p align=\"center\">\n  <a href=\"https://www.npmjs.com/package/n8n-nodes-growchief\">N8N node</a>\n  Â·\n  <a href=\"https://platform.growchief.com\">Register</a>\n  Â·\n  <a href=\"https://discord.growchief.com\">Join Our Discord (devs only)</a>\n</p>\n\n## âœ¨ Features\n\n| ![Image 1](https://github.com/user-attachments/assets/492ffc23-98ff-4d1b-a812-34debc0d2161) | ![Image 2](https://github.com/user-attachments/assets/1dd33597-dc87-45a7-8380-f31c102c3687) |\n| ------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------- |\n| ![Image 3](https://github.com/user-attachments/assets/ba7d377a-8ede-424e-9c5e-6f7741d97f81) | ![Image 4](https://github.com/user-attachments/assets/17e903c8-b32c-4f6c-b565-dc30f240f069) |\n\n---\n\n## Introduction\n\nGrowchief is an open-source social media automation tool (aka social scraper). It allows you to create a workflow (step-by-step) for interacting with different accounts on social media, such as sending a connection request, following up with a message, and so on.\nWe do not encourage spam (perfect for API/n8n automations).\n\n## Why is GrowChief so good?\n\n* It takes care of concurrency â€” even if you create 10 workflows with the same account and trigger all of them at the same time, it will make an action every 10 minutes, never having multiple scrapings happening at the same time.\n\n* Enrichment waterful â€” when you don't provide the account URL but other parameters like email, we use multiple provider to figure out the profile URL.\n\n* It takes care of your working hours â€” you can use the API to keep adding leads to your workflows, but they will only be processed during working hours.\n\n* Proxies are allowed â€” you can add your own proxies or create one using proxy providers to keep you safe.\n\n* Human-like automation â€” GrowChief uses natural mouse movements and clicks on different parts of the screen. It never triggers clicks by `\"document.querySelector('x').click()\"`.\n\n* It uses [Playwright](https://github.com/microsoft/playwright) together with [Patchright](https://github.com/Kaliiiiiiiiii-Vinyzu/patchright) for maximum stealthiness.\n\n* It uses a special technology to authenticate your accounts â€” you never need to put your username and password directly into the system.\n\n* It always runs in headful â€” our Docker image is already built with `xvfb` for real human automation.\n\n## Things you should know\n\n* Social media automation is a common practice in businesses, from small ones to enterprises. Yes, even the biggest companies in the world do it.\n  However, it violates the terms of service of the platforms and can result in a ban. Use it at your own risk and connect only with leads you know.\n\n* GrowChief Docker can work great without scale, as every time you start automation it opens a Chrome browser (perfect for 1â€“2 accounts).\n  However, once you scale, you need a smarter system (remote browser) with an option to scale, as Chrome consumes a lot of memory. For that, you can use GrowChief Cloud.\n\n\n## Tech Stack\n\n* PNPM (Monorepo)\n* React (Vite)\n* NestJS (Backend, Workers)\n* Prisma (Default to PostgreSQL)\n* Temporal (Orchestrator)\n\n## QuickStart / Installation\n\nView https://docs.growchief.com\n\n## Sponsorship\n\nThis can be very valuable for Proxies / Lead enrichment companies, feel free to check our sponsorship page.\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-20T02:14:11.621870"
  },
  {
    "basic_info": {
      "name": "youtu-agent",
      "full_name": "TencentCloudADP/youtu-agent",
      "owner": "TencentCloudADP",
      "description": "A simple yet powerful agent framework that delivers with open-source models",
      "url": "https://github.com/TencentCloudADP/youtu-agent",
      "clone_url": "https://github.com/TencentCloudADP/youtu-agent.git",
      "ssh_url": "git@github.com:TencentCloudADP/youtu-agent.git",
      "homepage": "https://tencentcloudadp.github.io/youtu-agent/",
      "created_at": "2025-08-21T07:58:13Z",
      "updated_at": "2025-09-19T22:31:31Z",
      "pushed_at": "2025-09-19T13:43:41Z"
    },
    "stats": {
      "stars": 2934,
      "forks": 280,
      "watchers": 2934,
      "open_issues": 25,
      "size": 10171
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 485594,
        "TypeScript": 209405,
        "CSS": 50866,
        "Jinja": 12981,
        "JavaScript": 1085,
        "Dockerfile": 871,
        "Makefile": 786,
        "HTML": 716,
        "Shell": 219
      },
      "license": "Other",
      "topics": [
        "agent-framework",
        "agents",
        "openai-agents",
        "python"
      ]
    },
    "content": {
      "readme": "# <img src=\"docs/assets/logo.svg\" alt=\"Youtu-agent Logo\" height=\"24px\"> Youtu-Agent: A simple yet powerful agent framework that delivers with open-source models\n\n<div align=\"center\">\n<a href=\"https://tencentcloudadp.github.io/youtu-agent/\"><img src=https://img.shields.io/badge/ğŸ“–-Documentation-blue.svg></a>\n<!-- <a href=https://arxiv.org/abs/2502.14345><img src=https://img.shields.io/badge/arXiv-2502.14345-b31b1b.svg></a> -->\n<a href=https://github.com/TencentCloudADP/youtu-agent><img src=https://img.shields.io/badge/GitHub-Tencent-blue.svg></a>\n<a href=https://deepwiki.com/TencentCloudADP/youtu-agent><img src=https://img.shields.io/badge/DeepWiki-Tencent-blue.svg></a>\n</div>\n\n<p align=\"center\">\n| <a href=\"README_ZH.md\"><b>ä¸­æ–‡</b></a>\n| <a href=\"README_JA.md\"><b>æ—¥æœ¬èª</b></a>\n| <a href=\"#-benchmark-performance\"><b>ğŸŒŸ Performance</b></a> \n| <a href=\"#-examples\"><b>ğŸ’¡ Examples</b> </a> \n| <a href=\"#-features\"><b>âœ¨ Features</b> </a> \n| <a href=\"#-getting-started\"><b>ğŸš€ Getting Started</b> </a> \n| <a href=\"https://discord.gg/svwuqgUx\"><b>ğŸ“¢ Join Community</b> </a> \n</p>\n\n\n`Youtu-Agent` is a flexible, high-performance framework for building, running, and evaluating autonomous agents. Beyond topping the benchmarks, this framework delivers powerful agent capabilities, e.g. data analysis, file processing, and deep research, all with open-source models.\n\n<img src=\"docs/assets/mascot.png\" alt=\"Youtu-agent Logo\" width=\"200\" align=\"left\" style=\"margin-right:20px;\">\n\nKey highlights:\n- **Verified performance**: Achieved 71.47% on WebWalkerQA (pass@1) and 72.8% on GAIA (text-only subset, pass@1), using purely `DeepSeek-V3` series models (without Claude or GPT), establishing a strong open-source starting point.\n- **Open-source friendly & cost-aware**: Optimized for accessible, low-cost deployment without reliance on closed models.\n- **Practical use cases**: Out-of-the-box support for tasks like CSV analysis, literature review, personal file organization, and podcast and video generation (coming soon).\n- **Flexible architecture**: Built on [openai-agents](https://github.com/openai/openai-agents-python), with extensible support for diverse model APIs (form `DeepSeek` to `gpt-oss`), tool integrations, and framework implementations.\n- **Automation & simplicity**: YAML-based configs, auto agent generation, and streamlined setup reduce manual overhead.\n\n## ğŸ—ï¸ News\n\n- ğŸ“º [2025-09-09] We hosted a live sharing the design philosophy and basic usage of `Youtu-Agent`. [[video](https://www.bilibili.com/video/BV1mypqz4EvS)] [[documentation](https://doc.weixin.qq.com/doc/w3_AcMATAZtAPICNLgt3CbnxRWaYWnW4)].\n- ğŸ [2025-09-02] [Tencent Cloud International](https://www.tencentcloud.com/) offers new users of the DeepSeek API **3 million free tokens** (**Sep 1 â€“ Oct 31, 2025**). [Try it out](https://www.tencentcloud.com/document/product/1255/70381) for free if you want to use DeepSeek models in `Youtu-Agent`! For enterprise agent solutions, also check out [Agent Development Platform](https://adp.tencentcloud.com) (ADP).\n- ğŸ“º [2025-08-28] We hosted a live sharing updates about DeepSeek-V3.1 and how to use it in the `Youtu-Agent` framework. [[video](https://www.bilibili.com/video/BV1XwayzrETi/)] [[documentation](https://doc.weixin.qq.com/doc/w3_AcMATAZtAPICNvcLaY5FvTOuo7MwF)].\n\n## ğŸŒŸ Benchmark Performance\n\n`Youtu-Agent` is built on open-source models and lightweight tools, demonstrating strong results on challenging deep search and tool use benchmarks.\n\n- **[WebWalkerQA](https://huggingface.co/datasets/callanwu/WebWalkerQA)**: Achieved 60.71% accuracy with `DeepSeek-V3-0324`ï¼Œ using new released `DeepSeek-V3.1` can further improve to 71.47%, setting a new SOTA performance.\n- **[GAIA](https://gaia-benchmark-leaderboard.hf.space/)**: Achieved 72.8% pass@1 on the [text-only validation subset](https://github.com/sunnynexus/WebThinker/blob/main/data/GAIA/dev.json) using `DeepSeek-V3-0324` (including models used within tools). We are actively extending evaluation to the full GAIA benchmark with multimodal tools, and will release the trajectories in the near future. Stay tuned! âœ¨\n\n![WebWalkerQA](docs/assets/images/benchmark_webwalkerqa.png)\n\n## ğŸ’¡ Examples\n\nClick on the images to view detailed videos.\n\n<table>\n  <tr>\n    <td style=\"border: 1px solid black; padding: 10px; width: 50%; vertical-align: top;\">\n      <strong>Data Analysis</strong><br>Analyzes a CSV file and generates an HTML report.\n    </td>\n    <td style=\"border: 1px solid black; padding: 10px; width: 50%; vertical-align: top;\">\n      <strong>File Management</strong><br>Renames and categorizes local files for the user.\n    </td>\n  </tr>\n  <tr>\n    <td style=\"border: 1px solid black; padding: 10px; width: 50%; vertical-align: top;\">\n      <video src=\"https://github.com/user-attachments/assets/60193435-b89d-47d3-8153-5799d6ff2920\" \n             poster=\"https://img.youtube.com/vi/r9we4m1cB6M/sddefault.jpg\" \n             controls muted preload=\"metadata\" \n             width=\"100%\" height=\"300\"\n             ",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-20T02:14:12.753866"
  },
  {
    "basic_info": {
      "name": "shimmy",
      "full_name": "Michael-A-Kuykendall/shimmy",
      "owner": "Michael-A-Kuykendall",
      "description": "âš¡ Python-free Rust inference server â€” OpenAI-API compatible. GGUF + SafeTensors, hot model swap, auto-discovery, single binary. FREE now, FREE forever.",
      "url": "https://github.com/Michael-A-Kuykendall/shimmy",
      "clone_url": "https://github.com/Michael-A-Kuykendall/shimmy.git",
      "ssh_url": "git@github.com:Michael-A-Kuykendall/shimmy.git",
      "homepage": "",
      "created_at": "2025-08-28T22:55:46Z",
      "updated_at": "2025-09-20T01:46:55Z",
      "pushed_at": "2025-09-20T01:29:48Z"
    },
    "stats": {
      "stars": 2440,
      "forks": 166,
      "watchers": 2440,
      "open_issues": 1,
      "size": 213258
    },
    "tech_info": {
      "language": "Rust",
      "languages": {
        "Rust": 536594,
        "C": 158306,
        "C++": 77564,
        "Shell": 46795,
        "TOML": 18128,
        "Python": 14810,
        "TypeScript": 10203,
        "JavaScript": 7526,
        "YAML": 6387,
        "Dockerfile": 4809,
        "Batchfile": 4747,
        "Ruby": 931
      },
      "license": "MIT License",
      "topics": [
        "api-server",
        "command-line-tool",
        "developer-tools",
        "gguf",
        "huggingface",
        "huggingface-models",
        "huggingface-transformers",
        "inference-server",
        "llama",
        "llamacpp",
        "llm-inference",
        "local-ai",
        "lora",
        "machine-learning",
        "ollama-api",
        "openai-compatible",
        "rust",
        "rust-crate",
        "transformers"
      ]
    },
    "content": {
      "readme": "<div align=\"center\">\r\n  <img src=\"assets/shimmy-logo.png\" alt=\"Shimmy Logo\" width=\"300\" height=\"auto\" />\r\n  \r\n  # The Privacy-First Alternative to Ollama\r\n  \r\n  ### ğŸ”’ Local AI Without the Lock-in ğŸš€\r\n\r\n  [![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\r\n  [![Security](https://img.shields.io/badge/Security-Audited-green)](https://github.com/Michael-A-Kuykendall/shimmy/security)\r\n  [![Crates.io](https://img.shields.io/crates/v/shimmy.svg)](https://crates.io/crates/shimmy)\r\n  [![Downloads](https://img.shields.io/crates/d/shimmy.svg)](https://crates.io/crates/shimmy)\r\n  [![Rust](https://img.shields.io/badge/rust-stable-brightgreen.svg)](https://rustup.rs/)\r\n  [![GitHub Stars](https://img.shields.io/github/stars/Michael-A-Kuykendall/shimmy?style=social)](https://github.com/Michael-A-Kuykendall/shimmy/stargazers)\r\n  \r\n  [![ğŸ’ Sponsor this project](https://img.shields.io/badge/ğŸ’_Sponsor_this_project-ea4aaa?style=for-the-badge&logo=github&logoColor=white)](https://github.com/sponsors/Michael-A-Kuykendall)\r\n</div>\r\n\r\n**Shimmy will be free forever.** No asterisks. No \"free for now.\" No pivot to paid.\r\n\r\n### ğŸ’ Support Shimmy's Growth\r\n\r\nğŸš€ **If Shimmy helps you, consider [sponsoring](https://github.com/sponsors/Michael-A-Kuykendall) â€” 100% of support goes to keeping it free forever.**\r\n\r\n- **$5/month**: Coffee tier â˜• - Eternal gratitude + sponsor badge  \r\n- **$25/month**: Bug prioritizer ğŸ› - Priority support + name in [SPONSORS.md](SPONSORS.md)\r\n- **$100/month**: Corporate backer ğŸ¢ - Logo placement + monthly office hours  \r\n- **$500/month**: Infrastructure partner ğŸš€ - Direct support + roadmap input\r\n\r\n[**ğŸ¯ Become a Sponsor**](https://github.com/sponsors/Michael-A-Kuykendall) | See our amazing [sponsors](SPONSORS.md) ğŸ™\r\n\r\n---\r\n\r\n## Drop-in OpenAI API Replacement for Local LLMs\r\n\r\nShimmy is a **5.1MB single-binary** that provides **100% OpenAI-compatible endpoints** for GGUF models. Point your existing AI tools to Shimmy and they just work â€” locally, privately, and free.\r\n\r\n## ğŸ¤” What are you building with Shimmy?\r\n\r\n**New developer tools and specifications included!** Whether you're forking Shimmy for your application or integrating it as a service, we now provide:\r\n\r\n- **ğŸ”§ Integration Templates**: Copy-paste guidance for embedding Shimmy in your projects\r\n- **ğŸ“‹ Development Specifications**: GitHub Spec-Kit methodology for planning Shimmy-based features\r\n- **ğŸ›¡ï¸ Architectural Guarantees**: Constitutional principles ensuring Shimmy stays reliable and lightweight\r\n- **ğŸ“– Complete Documentation**: Everything you need to build on Shimmy's foundation\r\n\r\n**Building something cool with Shimmy?** These tools help you do it systematically and reliably.\r\n\r\n### ğŸš€ **GitHub Spec-Kit Integration**\r\nShimmy now includes [GitHub's brand-new Spec-Kit methodology](https://github.com/github/spec-kit) â€“ specification-driven development that just launched in September 2025! Get professional-grade development workflows:\r\n\r\n- **ğŸ—ï¸ Systematic Development**: `/specify` â†’ `/plan` â†’ `/tasks` â†’ implement\r\n- **ğŸ¤– AI-Native Workflow**: Works with Claude Code, GitHub Copilot, and other AI assistants  \r\n- **ğŸ“‹ Professional Templates**: Complete specification and planning frameworks\r\n- **ğŸ›¡ï¸ Constitutional Protection**: Built-in governance and architectural validation\r\n\r\n[**ğŸ“– Complete Developer Guide â†’**](DEVELOPERS.md) â€¢ [**ğŸ› ï¸ Learn GitHub Spec-Kit â†’**](https://github.com/github/spec-kit)\r\n\r\n### Try it in 30 seconds\r\n\r\n```bash\r\n# 1) Install + run\r\ncargo install shimmy --features huggingface\r\nshimmy serve &\r\n\r\n# 2) See models and pick one\r\nshimmy list\r\n\r\n# 3) Smoke test the OpenAI API\r\ncurl -s http://127.0.0.1:11435/v1/chat/completions \\\r\n  -H 'Content-Type: application/json' \\\r\n  -d '{\r\n        \"model\":\"REPLACE_WITH_MODEL_FROM_list\",\r\n        \"messages\":[{\"role\":\"user\",\"content\":\"Say hi in 5 words.\"}],\r\n        \"max_tokens\":32\r\n      }' | jq -r '.choices[0].message.content'\r\n```\r\n\r\n## ğŸš€ Works with Your Existing Tools\r\n\r\n**No code changes needed** - just change the API endpoint:\r\n\r\n- **VSCode Extensions**: Point to `http://localhost:11435`\r\n- **Cursor Editor**: Built-in OpenAI compatibility  \r\n- **Continue.dev**: Drop-in model provider\r\n- **Any OpenAI client**: Python, Node.js, curl, etc.\r\n\r\n### Use with OpenAI SDKs\r\n\r\n- Node.js (openai v4)\r\n\r\n```ts\r\nimport OpenAI from \"openai\";\r\n\r\nconst openai = new OpenAI({\r\n  baseURL: \"http://127.0.0.1:11435/v1\",\r\n  apiKey: \"sk-local\", // placeholder, Shimmy ignores it\r\n});\r\n\r\nconst resp = await openai.chat.completions.create({\r\n  model: \"REPLACE_WITH_MODEL\",\r\n  messages: [{ role: \"user\", content: \"Say hi in 5 words.\" }],\r\n  max_tokens: 32,\r\n});\r\n\r\nconsole.log(resp.choices[0].message?.content);\r\n```\r\n\r\n- Python (openai>=1.0.0)\r\n\r\n```python\r\nfrom openai import OpenAI\r\n\r\nclient = OpenAI(base_url=\"http://127.0.0.1:11435/v1\", api_key=\"sk-local\")\r\n\r\nresp = client.chat.completions.create(\r\n    model=\"REPLACE_WITH_MODEL\",\r\n    messages=[{\"role\": \"user\", \"content\": \"Say hi i",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-20T02:14:13.867646"
  },
  {
    "basic_info": {
      "name": "dionysus",
      "full_name": "pewdiepie-archdaemon/dionysus",
      "owner": "pewdiepie-archdaemon",
      "description": "laptop ",
      "url": "https://github.com/pewdiepie-archdaemon/dionysus",
      "clone_url": "https://github.com/pewdiepie-archdaemon/dionysus.git",
      "ssh_url": "git@github.com:pewdiepie-archdaemon/dionysus.git",
      "homepage": null,
      "created_at": "2025-08-27T22:09:28Z",
      "updated_at": "2025-09-19T20:02:19Z",
      "pushed_at": "2025-09-01T06:43:55Z"
    },
    "stats": {
      "stars": 2437,
      "forks": 81,
      "watchers": 2437,
      "open_issues": 73,
      "size": 25034
    },
    "tech_info": {
      "language": "Shell",
      "languages": {
        "Shell": 44000,
        "SCSS": 10593,
        "GLSL": 8416,
        "Python": 5585,
        "CSS": 4914
      },
      "license": null,
      "topics": []
    },
    "content": {
      "readme": "",
      "default_branch": "dionysus"
    },
    "fetched_at": "2025-09-20T02:14:14.974092"
  },
  {
    "basic_info": {
      "name": "RustGPT",
      "full_name": "tekaratzas/RustGPT",
      "owner": "tekaratzas",
      "description": "An transformer based LLM. Written completely in Rust",
      "url": "https://github.com/tekaratzas/RustGPT",
      "clone_url": "https://github.com/tekaratzas/RustGPT.git",
      "ssh_url": "git@github.com:tekaratzas/RustGPT.git",
      "homepage": null,
      "created_at": "2025-09-13T22:05:55Z",
      "updated_at": "2025-09-20T01:43:02Z",
      "pushed_at": "2025-09-17T14:27:56Z"
    },
    "stats": {
      "stars": 2408,
      "forks": 182,
      "watchers": 2408,
      "open_issues": 4,
      "size": 91
    },
    "tech_info": {
      "language": "Rust",
      "languages": {
        "Rust": 66254
      },
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "# ğŸ¦€ Rust LLM from Scratch\n\nhttps://github.com/user-attachments/assets/ec4a4100-b03a-4b3c-a7d6-806ea54ed4ed\n\nA complete **Large Language Model implementation in pure Rust** with no external ML frameworks. Built from the ground up using only `ndarray` for matrix operations.\n\n## ğŸš€ What This Is\n\nThis project demonstrates how to build a transformer-based language model from scratch in Rust, including:\n- **Pre-training** on factual text completion\n- **Instruction tuning** for conversational AI\n- **Interactive chat mode** for testing\n- **Full backpropagation** with gradient clipping\n- **Modular architecture** with clean separation of concerns\n\n## âŒ What This Isn't\n\nThis is not a production grade LLM. It is so far away from the larger models.\n\nThis is just a toy project that demonstrates how these models work under the hood.\n\n## ğŸ” Key Files to Explore\n\nStart with these two core files to understand the implementation:\n\n- **[`src/main.rs`](src/main.rs)** - Training pipeline, data preparation, and interactive mode\n- **[`src/llm.rs`](src/llm.rs)** - Core LLM implementation with forward/backward passes and training logic\n\n## ğŸ—ï¸ Architecture\n\nThe model uses a **transformer-based architecture** with the following components:\n\n```\nInput Text â†’ Tokenization â†’ Embeddings â†’ Transformer Blocks â†’ Output Projection â†’ Predictions\n```\n\n### Project Structure\n\n```\nsrc/\nâ”œâ”€â”€ main.rs              # ğŸ¯ Training pipeline and interactive mode\nâ”œâ”€â”€ llm.rs               # ğŸ§  Core LLM implementation and training logic\nâ”œâ”€â”€ lib.rs               # ğŸ“š Library exports and constants\nâ”œâ”€â”€ transformer.rs       # ğŸ”„ Transformer block (attention + feed-forward)\nâ”œâ”€â”€ self_attention.rs    # ğŸ‘€ Multi-head self-attention mechanism  \nâ”œâ”€â”€ feed_forward.rs      # âš¡ Position-wise feed-forward networks\nâ”œâ”€â”€ embeddings.rs        # ğŸ“Š Token embedding layer\nâ”œâ”€â”€ output_projection.rs # ğŸ° Final linear layer for vocabulary predictions\nâ”œâ”€â”€ vocab.rs            # ğŸ“ Vocabulary management and tokenization\nâ”œâ”€â”€ layer_norm.rs       # ğŸ§® Layer normalization\nâ””â”€â”€ adam.rs             # ğŸƒ Adam optimizer implementation\n\ntests/\nâ”œâ”€â”€ llm_test.rs         # Tests for core LLM functionality\nâ”œâ”€â”€ transformer_test.rs # Tests for transformer blocks\nâ”œâ”€â”€ self_attention_test.rs # Tests for attention mechanisms\nâ”œâ”€â”€ feed_forward_test.rs # Tests for feed-forward layers\nâ”œâ”€â”€ embeddings_test.rs  # Tests for embedding layers\nâ”œâ”€â”€ vocab_test.rs       # Tests for vocabulary handling\nâ”œâ”€â”€ adam_test.rs        # Tests for optimizer\nâ””â”€â”€ output_projection_test.rs # Tests for output layer\n```\n\n## ğŸ§ª What The Model Learns\n\nThe implementation includes two training phases:\n\n1. **Pre-training**: Learns basic world knowledge from factual statements\n   - \"The sun rises in the east and sets in the west\"\n   - \"Water flows downhill due to gravity\"\n   - \"Mountains are tall and rocky formations\"\n\n2. **Instruction Tuning**: Learns conversational patterns\n   - \"User: How do mountains form? Assistant: Mountains are formed through tectonic forces...\"\n   - Handles greetings, explanations, and follow-up questions\n\n## ğŸš€ Quick Start\n\n```bash\n# Clone and run\ngit clone https://github.com/tekaratzas/RustGPT.git \ncd RustGPT\ncargo run\n\n# The model will:\n# 1. Build vocabulary from training data\n# 2. Pre-train on factual statements (100 epochs)  \n# 3. Instruction-tune on conversational data (100 epochs)\n# 4. Enter interactive mode for testing\n```\n\n## ğŸ® Interactive Mode\n\nAfter training, test the model interactively:\n\n```\nEnter prompt: How do mountains form?\nModel output: Mountains are formed through tectonic forces or volcanism over long geological time periods\n\nEnter prompt: What causes rain?\nModel output: Rain is caused by water vapor in clouds condensing into droplets that become too heavy to remain airborne\n```\n\n## ğŸ§® Technical Implementation\n\n### Model Configuration\n- **Vocabulary Size**: Dynamic (built from training data)\n- **Embedding Dimension**: 128\n- **Hidden Dimension**: 256  \n- **Max Sequence Length**: 80 tokens\n- **Architecture**: 3 Transformer blocks + embeddings + output projection\n\n### Training Details\n- **Optimizer**: Adam with gradient clipping\n- **Pre-training LR**: 0.0005 (100 epochs)\n- **Instruction Tuning LR**: 0.0001 (100 epochs)\n- **Loss Function**: Cross-entropy loss\n- **Gradient Clipping**: L2 norm capped at 5.0\n\n### Key Features\n- **Custom tokenization** with punctuation handling\n- **Greedy decoding** for text generation\n- **Gradient clipping** for training stability\n- **Modular layer system** with clean interfaces\n- **Comprehensive test coverage** for all components\n\n## ğŸ”§ Development\n\n```bash\n# Run all tests\ncargo test\n\n# Test specific components\ncargo test --test llm_test\ncargo test --test transformer_test\ncargo test --test self_attention_test\n\n# Build optimized version\ncargo build --release\n\n# Run with verbose output\ncargo test -- --nocapture\n```\n\n## ğŸ§  Learning Resources\n\nThis implementation demonstrates key ML concepts:\n- **Transformer architecture** (attention, feed-forward, layer norm)\n- **Backpropagation** through ne",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-20T02:14:16.097977"
  },
  {
    "basic_info": {
      "name": "ZHO-nano-banana-Creation",
      "full_name": "ZHO-ZHO-ZHO/ZHO-nano-banana-Creation",
      "owner": "ZHO-ZHO-ZHO",
      "description": "æˆ‘çš„ nano-banana åˆ›æ„ç©æ³•å¤§åˆé›†ï¼  æŒç»­æ›´æ–°ä¸­ï¼",
      "url": "https://github.com/ZHO-ZHO-ZHO/ZHO-nano-banana-Creation",
      "clone_url": "https://github.com/ZHO-ZHO-ZHO/ZHO-nano-banana-Creation.git",
      "ssh_url": "git@github.com:ZHO-ZHO-ZHO/ZHO-nano-banana-Creation.git",
      "homepage": null,
      "created_at": "2025-08-28T13:09:00Z",
      "updated_at": "2025-09-20T02:12:51Z",
      "pushed_at": "2025-09-18T22:04:39Z"
    },
    "stats": {
      "stars": 2377,
      "forks": 233,
      "watchers": 2377,
      "open_issues": 3,
      "size": 85
    },
    "tech_info": {
      "language": null,
      "languages": {},
      "license": "GNU General Public License v3.0",
      "topics": []
    },
    "content": {
      "readme": "<img width=\"1152\" height=\"1536\" alt=\"Mask groupå°é¢\" src=\"https://github.com/user-attachments/assets/48d727e7-7998-4d2a-aa37-59beb41ac23e\" />\n\n\n<div align=\"center\">\n   \n# Nano-Banana Creation ZHO\n   \næˆ‘çš„ Nano-Banana åŸåˆ›ç©æ³•å¤§åˆé›† | [å®Œæ•´å¸–](https://x.com/ZHO_ZHO_ZHO/status/1961073677681983926) | My Nano-Banana Creation collection\n\nï¼ï¼ï¼æ³¨æ„æ ‡æ˜å‡ºå¤„å“¦ï¼ï¼ï¼\n\n\n\n<img width=\"1032\" height=\"1373\" alt=\"Group 320\" src=\"https://github.com/user-attachments/assets/8f6ec719-1097-43f5-b56f-ef68123e2203\" />\n\n\n\n<div align=\"left\">\n\n\n# ï¼ˆ1ï¼‰ç›®å½•ï¼ˆ46é¡¹ï¼‰\n\n- [1ï¸âƒ£ å‡ºåœˆ/ç«çš„ç”¨æ³•ï¼šå›¾ç‰‡å˜æ‰‹åŠ/æ‰‹åŠè§†é¢‘](#1ï¸âƒ£-å‡ºåœˆç«çš„ç”¨æ³•å›¾ç‰‡å˜æ‰‹åŠæ‰‹åŠè§†é¢‘)\n- [2ï¸âƒ£ åäºº/æŒ‡å®šäººç‰©ï¼ˆä¸Šä¼ å›¾ç‰‡ï¼‰è¶…å†™å®ç…§ç‰‡çº§ç”Ÿæˆ](#2ï¸âƒ£-åäººæŒ‡å®šäººç‰©ä¸Šä¼ å›¾ç‰‡è¶…å†™å®ç…§ç‰‡çº§ç”Ÿæˆ)\n- [3ï¸âƒ£ æŒ‡å®šäººç‰©çŸ­è§†é¢‘ï¼šäººåƒç‰¹å¾ä¿æŒ + åˆ‡æ¢è§†è§’ + veo3 é¦–å°¾å¸§](#3ï¸âƒ£-æŒ‡å®šäººç‰©çŸ­è§†é¢‘äººåƒç‰¹å¾ä¿æŒ--åˆ‡æ¢è§†è§’--veo3-é¦–å°¾å¸§)\n- [4ï¸âƒ£ å»ºç­‘å›¾è½¬æ¨¡å‹/å»ºæ¨¡](#4ï¸âƒ£-å»ºç­‘å›¾è½¬æ¨¡å‹å»ºæ¨¡)\n- [5ï¸âƒ£ è¿ç»­ç¼–è¾‘ + ç‰©ä½“ç»„åˆ + èƒŒæ™¯è®¾è®¡](#5ï¸âƒ£-è¿ç»­ç¼–è¾‘--ç‰©ä½“ç»„åˆ--èƒŒæ™¯è®¾è®¡)\n- [6ï¸âƒ£ é«˜æ¸…ä¿®å¤](#6ï¸âƒ£-é«˜æ¸…ä¿®å¤)\n- [7ï¸âƒ£ ç‰©ä½“ç»„åˆ/ç‰ˆæœ¬å¯¹æ¯”](#7ï¸âƒ£-ç‰©ä½“ç»„åˆç‰ˆæœ¬å¯¹æ¯”)\n- [8ï¸âƒ£ å•†å“å¹¿å‘ŠçŸ­ç‰‡ï¼šæŒ‡å®šäººç‰© + å•†å“](#8ï¸âƒ£-å•†å“å¹¿å‘ŠçŸ­ç‰‡æŒ‡å®šäººç‰©--å•†å“)\n- [9ï¸âƒ£ äººç¾¤ä¸­åˆ†ç¦»æŒ‡å®šæ¨¡ç³Šäººç‰© + é«˜æ¸…ç”Ÿæˆ](#9ï¸âƒ£-äººç¾¤ä¸­åˆ†ç¦»æŒ‡å®šæ¨¡ç³Šäººç‰©--é«˜æ¸…ç”Ÿæˆ)\n- [1ï¸âƒ£0ï¸âƒ£ å›¾è½¬çº¿ç¨¿ + è‰²å¡ä¸Šè‰²](#1ï¸âƒ£0ï¸âƒ£-å›¾è½¬çº¿ç¨¿--è‰²å¡ä¸Šè‰²)\n- [1ï¸âƒ£1ï¸âƒ£ ä¸€å¥è¯ç”Ÿæˆä¸€å¥—è§’è‰²è®¾å®š/æ•…äº‹ä¹¦](#1ï¸âƒ£1ï¸âƒ£-ä¸€å¥è¯ç”Ÿæˆä¸€å¥—è§’è‰²è®¾å®šæ•…äº‹ä¹¦)\n- [1ï¸âƒ£2ï¸âƒ£ è™šå®ç»“åˆ/è·¨æ¬¡å…ƒï¼šæ’ç”»äººç‰©æ¢åº—](#1ï¸âƒ£2ï¸âƒ£-è™šå®ç»“åˆè·¨æ¬¡å…ƒæ’ç”»äººç‰©æ¢åº—)\n- [1ï¸âƒ£3ï¸âƒ£ æŒ‡å®šäººç‰© + è®¾è®¡ å®æ™¯ä½“éªŒ/æ•ˆæœå›¾](#1ï¸âƒ£3ï¸âƒ£-æŒ‡å®šäººç‰©--è®¾è®¡-å®æ™¯ä½“éªŒæ•ˆæœå›¾)\n- [1ï¸âƒ£4ï¸âƒ£ ç²¾å‡†æ›¿æ¢è§†é¢‘äººç‰©](#1ï¸âƒ£4ï¸âƒ£-ç²¾å‡†æ›¿æ¢è§†é¢‘äººç‰©)\n- [1ï¸âƒ£5ï¸âƒ£ åŠ¨æ¼«è½¬çœŸäººï¼ˆæ¥è¿‘ 1:1 è¿˜åŸï¼‰](#1ï¸âƒ£5ï¸âƒ£-åŠ¨æ¼«è½¬çœŸäººæ¥è¿‘-11-è¿˜åŸ)\n- [1ï¸âƒ£6ï¸âƒ£ é«˜è´¨é‡æ‘„å½±ï¼šæŒ‡å®šäººç‰© + é«˜è´¨é‡å§¿åŠ¿å‚è€ƒ](#1ï¸âƒ£6ï¸âƒ£-é«˜è´¨é‡æ‘„å½±æŒ‡å®šäººç‰©--é«˜è´¨é‡å§¿åŠ¿å‚è€ƒ)\n- [1ï¸âƒ£7ï¸âƒ£ å›¾ç‰‡è½¬äººå¶ç©å…·](#1ï¸âƒ£7ï¸âƒ£-å›¾ç‰‡è½¬äººå¶ç©å…·)\n- [1ï¸âƒ£8ï¸âƒ£ å›¾ç‰‡è½¬-funko-pop-å…¬ä»”](#1ï¸âƒ£8ï¸âƒ£-å›¾ç‰‡è½¬-funko-pop-å…¬ä»”)\n- [1ï¸âƒ£9ï¸âƒ£ å›¾ç‰‡è½¬ä¹é«˜](#1ï¸âƒ£9ï¸âƒ£-å›¾ç‰‡è½¬ä¹é«˜)\n- [2ï¸âƒ£0ï¸âƒ£ å›¾ç‰‡è½¬é’ˆç»‡ç©å¶](#2ï¸âƒ£0ï¸âƒ£-å›¾ç‰‡è½¬é’ˆç»‡ç©å¶)\n- [2ï¸âƒ£1ï¸âƒ£ å›¾ç‰‡è½¬èŠ­æ¯”å¨ƒå¨ƒ](#2ï¸âƒ£1ï¸âƒ£-å›¾ç‰‡è½¬èŠ­æ¯”å¨ƒå¨ƒ)\n- [2ï¸âƒ£2ï¸âƒ£ ä¸‡ç‰©å˜é«˜è¾¾](#2ï¸âƒ£2ï¸âƒ£-ä¸‡ç‰©å˜é«˜è¾¾)\n- [2ï¸âƒ£3ï¸âƒ£ èµ›åšç”Ÿå¨ƒï¼Ÿï¼ä¸¤å¼ äººè„¸ç”Ÿæˆå­©å­è„¸](#2ï¸âƒ£3ï¸âƒ£-èµ›åšç”Ÿå¨ƒä¸¤å¼ äººè„¸ç”Ÿæˆå­©å­è„¸)\n- [2ï¸âƒ£4ï¸âƒ£ äº§å“è®¾è®¡å›¾è½¬çœŸå®æ•ˆæœ/æ¸²æŸ“](#2ï¸âƒ£4ï¸âƒ£-äº§å“è®¾è®¡å›¾è½¬çœŸå®æ•ˆæœæ¸²æŸ“)\n- [2ï¸âƒ£5ï¸âƒ£ éšæ‰‹æ‹ç§’å˜ä¸“ä¸šæ‘„å½±å¤§ç‰‡ï¼Ÿï¼nano-banana-æ‹¯æ•‘ä½ çš„åºŸç‰‡](#2ï¸âƒ£5ï¸âƒ£-éšæ‰‹æ‹ç§’å˜ä¸“ä¸šæ‘„å½±å¤§ç‰‡nano-banana-æ‹¯æ•‘ä½ çš„åºŸç‰‡)\n- [2ï¸âƒ£6ï¸âƒ£ å…‰å½±å‚è€ƒ](#2ï¸âƒ£6ï¸âƒ£-å…‰å½±å‚è€ƒ)\n- [2ï¸âƒ£7ï¸âƒ£ ä½¿ç”¨å…‰å½±äººå¶åšæ‰“å…‰å‚è€ƒ](#2ï¸âƒ£7ï¸âƒ£-ä½¿ç”¨å…‰å½±äººå¶åšæ‰“å…‰å‚è€ƒ)\n- [2ï¸âƒ£8ï¸âƒ£ ç”Ÿæˆç»˜ç”»/æ¸²æŸ“è¿‡ç¨‹å››å®«æ ¼](#2ï¸âƒ£8ï¸âƒ£-ç”Ÿæˆç»˜ç”»æ¸²æŸ“è¿‡ç¨‹å››å®«æ ¼)\n- [2ï¸âƒ£9ï¸âƒ£ ä¸€å¥è¯-ç…§ç‰‡å˜æ’ç”»-è¿˜é™„å¸¦ç»˜ç”»è¿‡ç¨‹](#2ï¸âƒ£9ï¸âƒ£-ä¸€å¥è¯-ç…§ç‰‡å˜æ’ç”»-è¿˜é™„å¸¦ç»˜ç”»è¿‡ç¨‹)\n- [3ï¸âƒ£0ï¸âƒ£ è„¸å‹å‚è€ƒ/æ§åˆ¶ï¼Œç§’å˜å¡é€šå½¢è±¡](#3ï¸âƒ£0ï¸âƒ£-è„¸å‹å‚è€ƒæ§åˆ¶ç§’å˜å¡é€šå½¢è±¡)\n- [3ï¸âƒ£1ï¸âƒ£ ä¸€å¥å’’è¯­ä»»ä½•é£æ ¼å˜å†™å®](#3ï¸âƒ£1ï¸âƒ£-ä¸€å¥å’’è¯­ä»»ä½•é£æ ¼å˜å†™å®)\n- [3ï¸âƒ£2ï¸âƒ£ æ›²é¢å±è´´å›¾](#3ï¸âƒ£2ï¸âƒ£-æ›²é¢å±è´´å›¾)\n- [3ï¸âƒ£3ï¸âƒ£ ç›´æ¥ä¸ºå›¾ç‰‡ä¸­çš„æ›²é¢å¤§å±ç”Ÿæˆ-æŒ‡å®šçš„-è£¸çœ¼-3d-å†…å®¹](#3ï¸âƒ£3ï¸âƒ£-ç›´æ¥ä¸ºå›¾ç‰‡ä¸­çš„æ›²é¢å¤§å±ç”Ÿæˆ-æŒ‡å®šçš„-è£¸çœ¼-3d-å†…å®¹)\n- [3ï¸âƒ£4ï¸âƒ£ ä»»æ„å›¾ç‰‡ï¼ˆæ˜æ˜Ÿ/åŠ¨æ¼«ï¼‰å˜æŒ‚ä»¶æŒ‚åœ¨è‡ªå·±/å¥³æœ‹å‹åŒ…åŒ…ä¸Š](#3ï¸âƒ£4ï¸âƒ£-ä»»æ„å›¾ç‰‡æ˜æ˜ŸåŠ¨æ¼«å˜æŒ‚ä»¶æŒ‚åœ¨è‡ªå·±å¥³æœ‹å‹åŒ…åŒ…ä¸Š)\n- [3ï¸âƒ£5ï¸âƒ£ å åŠ æŒ‡å®š-æè´¨è´¨æ„Ÿ/æ•ˆæœ](#3ï¸âƒ£5ï¸âƒ£-å åŠ æŒ‡å®š-æè´¨è´¨æ„Ÿæ•ˆæœ)\n- [3ï¸âƒ£6ï¸âƒ£ ç…§ç‰‡å˜å¨ƒå¨ƒ](#3ï¸âƒ£6ï¸âƒ£-ç…§ç‰‡å˜å¨ƒå¨ƒ)\n- [3ï¸âƒ£7ï¸âƒ£ äº§å“åŒ…è£…è´´åˆ](#3ï¸âƒ£7ï¸âƒ£-äº§å“åŒ…è£…è´´åˆ)\n- [3ï¸âƒ£8ï¸âƒ£ æŠŠæŒ‡å®šå›¾ç‰‡è´´åœ¨å¤§é˜¶æ¢¯ä¸Š](#3ï¸âƒ£8ï¸âƒ£-æŠŠæŒ‡å®šå›¾ç‰‡è´´åœ¨å¤§é˜¶æ¢¯ä¸Š)\n- [3ï¸âƒ£9ï¸âƒ£ è™šæ‹Ÿè¯•å¦†åŒ–æŒ‡å®šå¦†é¢](#3ï¸âƒ£9ï¸âƒ£-è™šæ‹Ÿè¯•å¦†åŒ–æŒ‡å®šå¦†é¢)\n- [4ï¸âƒ£0ï¸âƒ£ å¦†é¢åˆ†æ-+-ä¼˜åŒ–å»ºè®®](#4ï¸âƒ£0ï¸âƒ£-å¦†é¢åˆ†æ--ä¼˜åŒ–å»ºè®®)\n- [4ï¸âƒ£1ï¸âƒ£ å·¥ä¸šè®¾è®¡-æ‰‹ç»˜-ç§’å˜-å®æ™¯æ•ˆæœ](#4ï¸âƒ£1ï¸âƒ£-å·¥ä¸šè®¾è®¡-æ‰‹ç»˜-ç§’å˜-å®æ™¯æ•ˆæœ)\n- [4ï¸âƒ£2ï¸âƒ£ å·¥ä¸šè®¾è®¡å¥—å›¾é©¬å…‹ç¬”ã€æ°´å½©ã€åˆ†æå›¾ã€æ¸²æŸ“å›¾](#4ï¸âƒ£2ï¸âƒ£-å·¥ä¸šè®¾è®¡å¥—å›¾é©¬å…‹ç¬”æ°´å½©åˆ†æå›¾æ¸²æŸ“å›¾)\n- [4ï¸âƒ£3ï¸âƒ£ è¡¨æƒ…å‡†ç¡®å‚è€ƒåŠ¨æ¼«ã€çœŸäººéƒ½æ²¡é—®é¢˜](#4ï¸âƒ£3ï¸âƒ£-è¡¨æƒ…å‡†ç¡®å‚è€ƒåŠ¨æ¼«çœŸäººéƒ½æ²¡é—®é¢˜)\n- [4ï¸âƒ£4ï¸âƒ£ åŠ¨ç‰©æ‹Ÿäººè¡¨æƒ…](#4ï¸âƒ£4ï¸âƒ£-åŠ¨ç‰©æ‹Ÿäººè¡¨æƒ…)\n- [4ï¸âƒ£5ï¸âƒ£ ç»ç¾å¡ç‰‡è®¾è®¡](#4ï¸âƒ£5ï¸âƒ£-ç»ç¾å¡ç‰‡è®¾è®¡)\n- [4ï¸âƒ£6ï¸âƒ£ å¤šäººç‰©æ’ç”»é›†](#4ï¸âƒ£6ï¸âƒ£-å¤šäººç‰©æ’ç”»é›†)\n\n\n# ï¼ˆ2ï¼‰åœ¨çº¿ä½“éªŒ/æœ¬åœ°éƒ¨ç½²ï¼ˆå¼€æºï¼‰ï¼šğŸŒ ä¸‰ä»¶å¥—\n\n\n- çª—å£å¼ï¼š[1ï¸âƒ£ Nano Bananaryï½œé¦™è•‰è¶…å¸‚](#1ï¸âƒ£-nano-bananaryé¦™è•‰è¶…å¸‚)\n\n- ç™½æ¿/ç”»å¸ƒå¼ï¼š[2ï¸âƒ£ BananaPodï½œé¦™è•‰é“ºå­](#2ï¸âƒ£-bananapodé¦™è•‰é“ºå­)\n\n- èŠ‚ç‚¹/å·¥ä½œæµå¼ï¼š[3ï¸âƒ£ BananaFlowï½œé¦™è•‰å·¥å‚](#3ï¸âƒ£-bananaflowé¦™è•‰å·¥å‚)\n\n\n\n## 1ï¸âƒ£ Nano Bananaryï½œé¦™è•‰è¶…å¸‚\n\n<img width=\"1251\" height=\"2051\" alt=\"Group 336\" src=\"https://github.com/user-attachments/assets/51a70ae5-9f94-4dc3-88ca-8ef688d1ee5c\" />\n\n\næ‰€æœ‰ç©æ³•å·²é™†ç»­ä¸Šçº¿æˆ‘è‡ªå·±çš„å¼€æº APPï¼šNano Bananaryï½œé¦™è•‰è¶…å¸‚ï¼Œæ— éœ€æç¤ºè¯ï¼Œä¸æ»‘è¡”æ¥ï¼Œå·²ä¸Šçº¿ è§†é¢‘ç”Ÿæˆ åŠŸèƒ½\n\n\nä¸€é”®ç›´è¾¾ï¼š[Nano Bananaryï½œé¦™è•‰è¶…å¸‚](https://github.com/ZHO-ZHO-ZHO/Nano-Bananary)\n\n\n\n## 2ï¸âƒ£ BananaPodï½œé¦™è•‰é“ºå­\n\n\n<img width=\"2261\" height=\"1410\" alt=\"screenshot-20250909-020322\" src=\"https://github.com/user-attachments/assets/9997b969-c773-4f7b-9c0b-dfeed5cf9e71\" />\n\nå„ç§ç©æ³•å·²å†…ç½®ï¼Œæ”¯æŒæ‰‹ç»˜ç”Ÿå›¾ï¼Œæ”¯æŒå¤šå›¾æ¡†é€‰ï¼Œè½»æ¾æ„å»ºåˆ›æ„ç”»æ¿\n\nä¸€é”®ç›´è¾¾ï¼š[BananaPodï½œé¦™è•‰é“ºå­](https://github.com/ZHO-ZHO-ZHO/BananaPod)\n\n\n\n## 3ï¸âƒ£ BananaFlowï½œé¦™è•‰å·¥å‚\n\n<img width=\"2667\" height=\"3518\" alt=\"Group 357\" src=\"https://github.com/user-attachments/assets/a934c87c-90a6-4628-8866-428c07eb434d\" />\n\nNanoBanana + Veo çš„å¼€æºå·¥ä½œæµåˆ›æ„å¹³å°ï¼Œå·¥ä½œæµ + çª—å£ åŒæ¨¡å¼çºµäº«ä¸æ»‘ï¼Œå„ç§ç©æ³•å·²å†…ç½®ï¼Œè½»æ¾æ„å»ºåˆ›æ„å·¥ä½œæµ\n\nä¸€é”®ç›´è¾¾ï¼š[BananaFlowï½œé¦™è•‰å·¥å‚](https://github.com/ZHO-ZHO-ZHO/BananaFlow-ZHO)\n\n\n\n# ï¼ˆ3ï¼‰åŸåˆ›ç©æ³• + æç¤ºè¯\n\n## 1ï¸âƒ£ å‡ºåœˆ/ç«çš„ç”¨æ³•ï¼šå›¾ç‰‡å˜æ‰‹åŠ/æ‰‹åŠè§†é¢‘\n\nhttps://github.com/user-attachments/assets/6440b1eb-4ecb-4d77-8dba-63c6d61c1ad3\n\n1ï¼‰å˜æ‰‹åŠ Promptï¼š\n\n```\nturn this photo into a character figure. Behind it, place a box with the characterâ€™s image printed on it, and a computer showing the Blender modeling process on its screen. In front of the box, add a round plastic base with the character figure standing on it. set the scene indoors if possible\n```\n\n2ï¼‰å˜è§†é¢‘ Promptï¼ˆVeo3ï¼‰ï¼š\n\n```\nA pair of hands picks up the figurine and examines it closely\n```\n\n<details>\n<summary>æˆ‘çš„åŸå¸–</summary>\n\n  \n[å›¾ç‰‡å˜æ‰‹åŠ](https://x.com/ZHO_ZHO_ZHO/status/1958539464994959715)\n\n[å›¾ç‰‡å˜æ‰‹åŠä¼˜åŒ–ç‰ˆ](https://x.com/ZHO_ZHO_ZHO/status/1959524430520135749)\n\n[æ‰‹åŠå˜è§†é¢‘](https://x.com/ZHO_ZHO_ZHO/status/1958550998815023573)\n\n\n</details>\n\n\n\n## 2ï¸âƒ£ åäºº/æŒ‡å®šäººç‰©ï¼ˆä¸Šä¼ å›¾ç‰‡ï¼‰è¶…å†™å®ç…§ç‰‡çº§ç”Ÿæˆ\n\n\n<img width=\"894\" height=\"900\" alt=\"image\" src=\"https://github.com/user-attachments/assets/0a0fd607-66c4-4993-bd10-fe3f1b734ef0\" />\n\n\nPromptï¼š\n\n```\nç”»é¢é‡‡ç”¨ä¸­æ™¯è¿‘ä¹åŠèº«çš„æ„å›¾ï¼Œé•œå¤´ä¸äººç‰©å‡ ä¹å¹³è§†ï¼Œä½†é€è§†æ„Ÿå¼ºçƒˆï¼Œä½†å› ä¸ºä¸»ä½“å¾®å¾®å‰å€¾ï¼Œè§†è§‰ä¸Šäº§ç”Ÿä¸€ç§ç•¥å¸¦ä¿¯è§†æ„Ÿçš„å‹ç¼©æ•ˆæœï¼Œè®©è§‚è€…ä¸æ¨¡ç‰¹ä¹‹é—´çš„è·ç¦»æ˜¾å¾—äº²å¯†è€Œç›´æ¥ã€‚äººç‰©å¾®å¾®æŠ¬å¤´å†²å‘é•œå¤´ï¼Œæœ‰ç§æ‹½å§çš„æ„Ÿè§‰ã€‚é—ªå…‰ç¯ä»æ­£é¢åå·¦ä¸Šæ–¹æ‰“æ¥ï¼Œåˆ¶é€ å‡ºç¡¬æœ—çš„é«˜å…‰ä¸æ·±é‡é˜´å½±â€”â€”å¢¨é•œé•œç‰‡ä¸Šæœ‰æ˜æ˜¾é«˜å…‰åå°„ï¼Œäººç‰©åæ–¹å¢™é¢å‡ºç°æ·¡æ·¡çš„æŠ•å½±ï¼Œæ•´ä½“å‘ˆç°å…¸å‹çš„â€œç›´é—ªâ€è´¨æ„Ÿï¼šé¢—ç²’æ„Ÿè½»å¾®ï¼Œå¯è§èƒ¶ç‰‡é£æ ¼æˆ–é«˜æ„Ÿå…‰åº¦æ•°ç æ‹æ‘„çš„ç²—ç²çº¹ç†ã€‚äººç‰©é¢éƒ¨ç¨ç¨æœ‰ç‚¹è¿‡åº¦æ›å…‰\n\nè‰²å½©åŸºè°ƒä»¥ä½é¥±å’Œçš„ä¸­æ€§è‰²ä¸ºä¸»ï¼šé»‘è‰²çš„å®½è‚©ä¸ç»’è¥¿è£…å¤–å¥—å æ®ç”»é¢æœ€å¤§é¢ç§¯ï¼Œé¢æ–™ä¼¼ä¹æ˜¯ä¸ç»’ï¼Œå¸¦æœ‰ç»†è…»çš„ç»’é¢çº¹è·¯ï¼›å†…æ­çš„é»‘è‰²ä¸ç»¸åŠå¸¦ï¼ˆæ¬è¿ä¸æ ‡å‡ºå¤„æ­»å¦ˆï¼‰ã€‚æ¨¡ç‰¹ä¸‹è£…æ˜¯é»‘è‰²è¶…çŸ­è£™å’Œè–„é€è´¨æ„Ÿçš„è¿è£¤è¢œã€‚èƒŒæ™¯å·¦ä¾§å †æ”¾çš„å¢¨ç»¿è‰²ä¸å’–è‰²æ£’çƒå¸½ã€å³ä¾§å æ”¾çš„äº®è“è‰²å¤¹å…‹åŠé»‘è‰²å¤´ç›”ç­‰æ‚ç‰©ï¼Œåœ¨æŸ”å’Œé˜´",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-20T02:14:17.204473"
  },
  {
    "basic_info": {
      "name": "KatelyaTV",
      "full_name": "katelya77/KatelyaTV",
      "owner": "katelya77",
      "description": "ä¸€ä¸ªåŸºäºMoonTV(LunaTV)äºŒæ¬¡å¼€å‘çš„é¡¹ç›®ã€‚",
      "url": "https://github.com/katelya77/KatelyaTV",
      "clone_url": "https://github.com/katelya77/KatelyaTV.git",
      "ssh_url": "git@github.com:katelya77/KatelyaTV.git",
      "homepage": "https://tv.katelya.eu.org",
      "created_at": "2025-08-27T06:04:23Z",
      "updated_at": "2025-09-20T00:53:05Z",
      "pushed_at": "2025-09-05T08:21:49Z"
    },
    "stats": {
      "stars": 1807,
      "forks": 2323,
      "watchers": 1807,
      "open_issues": 48,
      "size": 29311
    },
    "tech_info": {
      "language": "TypeScript",
      "languages": {
        "TypeScript": 661598,
        "JavaScript": 58289,
        "CSS": 39181,
        "Dockerfile": 2444,
        "Shell": 328
      },
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "<div align=\"center\">\n  <img src=\"public/logo.png\" alt=\"KatelyaTV Logo\" width=\"128\" />\n\n  <h1>KatelyaTV</h1>\n  <p><strong>è·¨å¹³å° Â· èšåˆæœç´¢ Â· å³å¼€å³ç”¨ Â· è‡ªæ‰˜ç®¡å½±è§†èšåˆæ’­æ”¾å™¨</strong></p>\n  <p>åŸºäº <code>Next.js 14</code> Â· <code>TypeScript</code> Â· <code>Tailwind CSS</code> Â· å¤šæºèšåˆ / æ’­æ”¾è®°å½• / æ”¶è—åŒæ­¥ / è·³è¿‡ç‰‡å¤´ç‰‡å°¾ / PWA</p>\n  \n  <p>\n    <a href=\"#-å¿«é€Ÿå¼€å§‹\">ğŸš€ å¿«é€Ÿå¼€å§‹</a> Â·\n    <a href=\"#-åŠŸèƒ½ç‰¹æ€§\">âœ¨ åŠŸèƒ½ç‰¹æ€§</a> Â·\n    <a href=\"#-éƒ¨ç½²æ–¹æ¡ˆ\">ğŸ“‹ éƒ¨ç½²æ–¹æ¡ˆ</a> Â·\n    <a href=\"#-é…ç½®è¯´æ˜\">âš™ï¸ é…ç½®è¯´æ˜</a>\n  </p>\n</div>\n\n---\n\n## ğŸ“° é¡¹ç›®å£°æ˜\n\næœ¬é¡¹ç›®è‡ªã€ŒMoonTVã€æ¼”è¿›è€Œæ¥ï¼Œä¸ºå…¶äºŒåˆ›/ç»§æ‰¿ç‰ˆæœ¬ï¼ŒæŒç»­ç»´æŠ¤ä¸æ”¹è¿›åŠŸèƒ½ä¸ä½“éªŒã€‚ä¿ç•™å¹¶è‡´è°¢åŸä½œè€…ä¸ç¤¾åŒºè´¡çŒ®è€…ã€‚\n\n> **ğŸ”” é‡è¦å˜æ›´**ï¼šåº”ç”¨æˆ·ç¤¾åŒºå»ºè®®ï¼Œä¸ºç¡®ä¿é¡¹ç›®é•¿æœŸç¨³å®šè¿è¡Œå’Œåˆè§„æ€§ï¼Œå†…ç½®è§†é¢‘æºå·²ç§»é™¤ã€‚ç°éœ€è¦ç”¨æˆ·è‡ªè¡Œé…ç½®èµ„æºç«™ä»¥ä½¿ç”¨å®Œæ•´åŠŸèƒ½ã€‚æˆ‘ä»¬æä¾›äº†ç»è¿‡æµ‹è¯•çš„æ¨èé…ç½®æ–‡ä»¶ï¼Œè®©æ‚¨å¿«é€Ÿä¸Šæ‰‹ä½¿ç”¨ã€‚\n\n---\n\n## âœ¨ åŠŸèƒ½ç‰¹æ€§\n\n### ğŸ¬ æ ¸å¿ƒåŠŸèƒ½\n\n- **ğŸ” èšåˆæœç´¢**ï¼šæ•´åˆå¤šä¸ªå½±è§†èµ„æºç«™ï¼Œä¸€é”®æœç´¢å…¨ç½‘å†…å®¹\n- **ğŸ“º é«˜æ¸…æ’­æ”¾**ï¼šåŸºäº ArtPlayer çš„å¼ºå¤§æ’­æ”¾å™¨ï¼Œæ”¯æŒå¤šç§æ ¼å¼\n- **â­ï¸ æ™ºèƒ½è·³è¿‡**ï¼šè‡ªåŠ¨æ£€æµ‹å¹¶è·³è¿‡ç‰‡å¤´ç‰‡å°¾ï¼Œæ‰‹åŠ¨è®¾ç½®è·³è¿‡æ—¶é—´æ®µ\n- **ğŸ¯ æ–­ç‚¹ç»­æ’­**ï¼šè‡ªåŠ¨è®°å½•æ’­æ”¾è¿›åº¦ï¼Œè·¨è®¾å¤‡åŒæ­¥è§‚çœ‹ä½ç½®\n- **ğŸ“± å“åº”å¼è®¾è®¡**ï¼šå®Œç¾é€‚é…æ‰‹æœºã€å¹³æ¿ã€ç”µè„‘å„ç§å±å¹•\n\n### ğŸ’¾ æ•°æ®ç®¡ç†\n\n- **â­ æ”¶è—åŠŸèƒ½**ï¼šæ”¶è—å–œæ¬¢çš„å½±è§†ä½œå“ï¼Œæ”¯æŒè·¨è®¾å¤‡åŒæ­¥\n- **ğŸ“– æ’­æ”¾å†å²**ï¼šè‡ªåŠ¨è®°å½•è§‚çœ‹å†å²ï¼Œå¿«é€Ÿæ‰¾å›çœ‹è¿‡çš„å†…å®¹\n- **ğŸ‘¥ å¤šç”¨æˆ·æ”¯æŒ**ï¼šç‹¬ç«‹çš„ç”¨æˆ·ç³»ç»Ÿï¼Œæ¯ä¸ªç”¨æˆ·ç‹¬äº«ä¸ªäººæ•°æ®\n- **ğŸ”„ æ•°æ®åŒæ­¥**ï¼šæ”¯æŒå¤šç§å­˜å‚¨åç«¯ï¼ˆLocalStorageã€Redisã€D1ã€Upstashï¼‰\n- **ğŸ”’ å†…å®¹è¿‡æ»¤**ï¼šæ™ºèƒ½æˆäººå†…å®¹è¿‡æ»¤ç³»ç»Ÿï¼Œé»˜è®¤å¼€å¯å®‰å…¨ä¿æŠ¤\n\n### ğŸš€ éƒ¨ç½²ç‰¹æ€§\n\n- **ğŸ³ Docker ä¸€é”®éƒ¨ç½²**ï¼šæä¾›å®Œæ•´çš„ Docker é•œåƒï¼Œå¼€ç®±å³ç”¨\n- **â˜ï¸ å¤šå¹³å°æ”¯æŒ**ï¼šVercelã€Dockerã€Cloudflare Pages å…¨å…¼å®¹\n- **ğŸ”§ çµæ´»é…ç½®**ï¼šæ”¯æŒè‡ªå®šä¹‰èµ„æºç«™ã€ä»£ç†è®¾ç½®ã€ä¸»é¢˜é…ç½®\n- **ğŸ“± PWA æ”¯æŒ**ï¼šå¯å®‰è£…ä¸ºæ¡Œé¢/æ‰‹æœºåº”ç”¨\n- **ğŸ“º TVBox å…¼å®¹**ï¼šæ”¯æŒ TVBox é…ç½®æ¥å£\n\n---\n\n## ğŸš€ å¿«é€Ÿå¼€å§‹\n\n### ğŸ’¡ æ–¹æ¡ˆé€‰æ‹©æŒ‡å—\n\n| ä½¿ç”¨åœºæ™¯     | æ¨èæ–¹æ¡ˆ         | å­˜å‚¨ç±»å‹     | æˆäººå†…å®¹è¿‡æ»¤ | å¤šç”¨æˆ· | éƒ¨ç½²éš¾åº¦ |\n| ------------ | ---------------- | ------------ | ------------ | ------ | -------- |\n| **ä¸ªäººä½¿ç”¨** | Docker å•å®¹å™¨    | localstorage | âŒ           | âŒ     | â­       |\n| **å®¶åº­ä½¿ç”¨** | Docker + Redis   | redis        | âœ…           | âœ…     | â­â­     |\n| **å…è´¹éƒ¨ç½²** | Vercel + Upstash | upstash      | âœ…           | âœ…     | â­â­â­   |\n| **ç”Ÿäº§ç¯å¢ƒ** | Docker + Kvrocks | kvrocks      | âœ…           | âœ…     | â­â­     |\n| **å…¨çƒåŠ é€Ÿ** | Cloudflare Pages | d1           | âœ…           | âœ…     | â­â­â­â­ |\n\n> ğŸ’¡ **é‡è¦æç¤º**ï¼šæˆäººå†…å®¹è¿‡æ»¤åŠŸèƒ½éœ€è¦æ•°æ®åº“å­˜å‚¨æ”¯æŒï¼Œä¸æ”¯æŒ `localstorage` æ–¹å¼\n\n---\n\n## ğŸ“‹ éƒ¨ç½²æ–¹æ¡ˆ\n\n### æ–¹æ¡ˆä¸€ï¼šDocker å•å®¹å™¨ï¼ˆæœ€ç®€å•ï¼‰\n\n**ç‰¹ç‚¹**ï¼š5 åˆ†é’Ÿéƒ¨ç½²ï¼Œä¸ªäººä½¿ç”¨ï¼Œæ— å¤šç”¨æˆ·åŠŸèƒ½\n\n```bash\ndocker run -d \\\n  --name katelyatv \\\n  -p 3000:3000 \\\n  -e PASSWORD=your_password \\\n  --restart unless-stopped \\\n  ghcr.io/katelya77/katelyatv:latest\n```\n\n**æŒ‚è½½è‡ªå®šä¹‰é…ç½®**ï¼ˆå¯é€‰ï¼‰ï¼š\n\n```bash\ndocker run -d \\\n  --name katelyatv \\\n  -p 3000:3000 \\\n  -e PASSWORD=your_password \\\n  -v $(pwd)/config.json:/app/config.json:ro \\\n  --restart unless-stopped \\\n  ghcr.io/katelya77/katelyatv:latest\n```\n\n### æ–¹æ¡ˆäºŒï¼šDocker + Redisï¼ˆæ¨èå®¶åº­ä½¿ç”¨ï¼‰\n\n**ç‰¹ç‚¹**ï¼šå®Œæ•´åŠŸèƒ½ï¼Œå¤šç”¨æˆ·æ”¯æŒï¼Œæˆäººå†…å®¹è¿‡æ»¤\n\n```bash\n# 1. ä¸‹è½½é…ç½®æ–‡ä»¶\ncurl -O https://raw.githubusercontent.com/katelya77/KatelyaTV/main/docker-compose.redis.yml\ncurl -O https://raw.githubusercontent.com/katelya77/KatelyaTV/main/.env.redis.example\n\n# 2. é…ç½®ç¯å¢ƒå˜é‡\ncp .env.redis.example .env\n```\n\n**ç¼–è¾‘ .env æ–‡ä»¶**ï¼š\n\n```bash\n# ç®¡ç†å‘˜è´¦å·ï¼ˆå¿…å¡«ï¼‰\nUSERNAME=admin\nPASSWORD=your_secure_password\n\n# å­˜å‚¨é…ç½®\nNEXT_PUBLIC_STORAGE_TYPE=redis\nREDIS_URL=redis://katelyatv-redis:6379\n\n# åŠŸèƒ½å¼€å…³\nNEXT_PUBLIC_ENABLE_REGISTER=true\n```\n\n```bash\n# 3. å¯åŠ¨æœåŠ¡\ndocker compose -f docker-compose.redis.yml up -d\n```\n\n### æ–¹æ¡ˆä¸‰ï¼šDocker + Kvrocksï¼ˆç”Ÿäº§ç¯å¢ƒï¼‰\n\n**ç‰¹ç‚¹**ï¼šæé«˜å¯é æ€§ï¼Œæ•°æ®æŒä¹…åŒ–åˆ°ç£ç›˜ï¼ŒèŠ‚çœå†…å­˜\n\n```bash\n# 1. ä¸‹è½½é…ç½®æ–‡ä»¶\ncurl -O https://raw.githubusercontent.com/katelya77/KatelyaTV/main/docker-compose.kvrocks.yml\ncurl -O https://raw.githubusercontent.com/katelya77/KatelyaTV/main/.env.kvrocks.example\n\n# 2. é…ç½®ç¯å¢ƒå˜é‡\ncp .env.kvrocks.example .env\n```\n\n**ç¼–è¾‘ .env æ–‡ä»¶**ï¼š\n\n```bash\n# ç®¡ç†å‘˜è´¦å·ï¼ˆå¿…å¡«ï¼Œå¦åˆ™æ— æ³•ç™»å½•ï¼‰\nUSERNAME=admin\nPASSWORD=your_secure_password\n\n# å­˜å‚¨é…ç½®\nNEXT_PUBLIC_STORAGE_TYPE=kvrocks\nKVROCKS_URL=redis://kvrocks:6666\n\n# åŠŸèƒ½å¼€å…³\nNEXT_PUBLIC_ENABLE_REGISTER=true\n```\n\n```bash\n# 3. å¯åŠ¨æœåŠ¡\ndocker compose -f docker-compose.kvrocks.yml up -d\n```\n\n### æ–¹æ¡ˆå››ï¼šVercel + Upstashï¼ˆå…è´¹æ¨èï¼‰\n\n**ç‰¹ç‚¹**ï¼šå®Œå…¨å…è´¹ï¼Œè‡ªåŠ¨ HTTPSï¼Œå…¨çƒ CDN\n\n#### åŸºç¡€éƒ¨ç½²\n\n1. **Fork é¡¹ç›®** â†’ [GitHub ä»“åº“](https://github.com/katelya77/KatelyaTV)\n2. **éƒ¨ç½²åˆ° Vercel**ï¼š\n   - ç™»å½• [Vercel](https://vercel.com/)\n   - å¯¼å…¥åˆš Fork çš„ä»“åº“\n   - æ·»åŠ ç¯å¢ƒå˜é‡ï¼š`PASSWORD=your_password`\n   - ç‚¹å‡» Deploy\n\n#### å¤šç”¨æˆ·é…ç½®\n\n3. **åˆ›å»º Upstash æ•°æ®åº“**ï¼š\n\n   - è®¿é—® [Upstash](https://upstash.com/)\n   - åˆ›å»ºå…è´¹ Redis æ•°æ®åº“\n   - è·å– `UPSTASH_URL` å’Œ `UPSTASH_TOKEN`\n\n4. **æ·»åŠ ç¯å¢ƒå˜é‡**ï¼š\n\n```bash\n# å­˜å‚¨é…ç½®\nNEXT_PUBLIC_STORAGE_TYPE=upstash\nUPSTASH_URL=https://xxx.upstash.io\nUPSTASH_TOKEN=your_token\n\n# ç®¡ç†å‘˜è´¦å·\nUSERNAME=admin\nPASSWORD=your_password\n\n# åŠŸèƒ½å¼€å…³\nNEXT_PUBLIC_ENABLE_REGISTER=true\n```\n\n5. **é‡æ–°éƒ¨ç½²** â†’ Vercel Dashboard â†’ Redeploy\n\n### æ–¹æ¡ˆäº”ï¼šCloudflare Pages + D1ï¼ˆå…¨çƒåŠ é€Ÿï¼‰\n\n**ç‰¹ç‚¹**ï¼šå…¨çƒ CDNï¼Œæ— é™å¸¦å®½ï¼Œå…è´¹ SSL\n\n#### å¿«é€Ÿéƒ¨ç½²\n\n1. **Fork é¡¹ç›®** â†’ [GitHub ä»“åº“](https://github.com/katelya77/KatelyaTV)\n2. **åˆ›å»º Pages é¡¹ç›®**ï¼š\n\n   - ç™»å½• [Cloudflare Dashboard](https://dash.cloudflare.com/)\n   - Pages â†’ Connect to Git â†’ é€‰æ‹©ä»“åº“\n   - æ„å»ºè®¾ç½®ï¼š\n     ```\n     Build command: pnpm install --frozen-lockfile && pnpm run pages:build\n     Build output directory: .vercel/output/static\n     ```\n   - å…¼å®¹æ€§æ ‡å¿—ï¼š`nodejs_compat`\n\n3. **ç¯å¢ƒå˜é‡é…ç½®**ï¼š\n\n```bash\n# ç®¡ç†å‘˜è´¦å·\nUSERNAME=admin\nPASSWORD=your_password\n\n# å­˜å‚¨é…ç½®\nNEXT_PUBLIC_STORAGE_TYPE=d1\n\n# åŠŸèƒ½å¼€å…³\nNEXT_PUBLIC_ENABLE_REGISTER=true\n```\n\n4. **åˆ›å»º D1 æ•°æ®åº“**ï¼ˆå¤šç”¨æˆ·æ”¯æŒï¼‰ï¼š\n\n```bash\n# å®‰è£…Wrangler CLI\nnpm install -g wrangler\nwrangler auth login\n\n# åˆ›å»ºæ•°æ®åº“\nwrangler d1 create katelyatv-db\n# âš ï¸ é‡è¦ï¼šç¡®ä¿åœ¨é¡¹ç›®æ ¹ç›®å½•ä¸‹è¿è¡Œæ­¤å‘½ä»¤\n# å¦‚æœé‡åˆ°æ–‡ä»¶è·¯å¾„é”™è¯¯ï¼Œè¯·å‚è€ƒ D1_MIGRATION.md æ’æŸ¥æŒ‡å—\nwrangler d1 execute katelyatv-db --file=./scripts/d1-init.sql\n```\n\n5. **é…ç½®æ•°æ®åº“ç»‘å®š** â†’ åœ¨ `wr",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-20T02:14:18.307416"
  },
  {
    "basic_info": {
      "name": "linear-algebra",
      "full_name": "little-book-of/linear-algebra",
      "owner": "little-book-of",
      "description": "There is hardly any theory which is more elementary than linear algebra, in spite of the fact that generations of professors and textbook writers have obscured its simplicity by preposterous calculations with matrices. â€”Jean Dieudonne",
      "url": "https://github.com/little-book-of/linear-algebra",
      "clone_url": "https://github.com/little-book-of/linear-algebra.git",
      "ssh_url": "git@github.com:little-book-of/linear-algebra.git",
      "homepage": "",
      "created_at": "2025-09-02T03:28:30Z",
      "updated_at": "2025-09-19T19:28:03Z",
      "pushed_at": "2025-09-04T07:55:49Z"
    },
    "stats": {
      "stars": 1671,
      "forks": 52,
      "watchers": 1671,
      "open_issues": 3,
      "size": 1159
    },
    "tech_info": {
      "language": "TeX",
      "languages": {
        "TeX": 159618
      },
      "license": "Other",
      "topics": []
    },
    "content": {
      "readme": "# The Little Book of Linear Algebra\n\nA concise, beginner-friendly introduction to the core ideas of linear algebra.\n\n## Formats\n\n- [Download PDF](book.pdf) â€“ print-ready version\n- [Download EPUB](book.epub) â€“ e-reader friendly\n- [View LaTeX](book.tex) â€“ Latex source\n\n# Chapter 1. Vectors\n\n## 1.1 Scalars and Vectors\n\nA scalar is a single numerical quantity, most often taken from the real numbers, denoted by $\\mathbb{R}$. Scalars are\nthe fundamental building blocks of arithmetic: they can be added, subtracted, multiplied, and, except in the case of\nzero, divided. In linear algebra, scalars play the role of coefficients, scaling factors, and entries of larger\nstructures such as vectors and matrices. They provide the weights by which more complex objects are measured and\ncombined. A vector is an ordered collection of scalars, arranged either in a row or a column. When the scalars are real\nnumbers, the vector is said to belong to *real* $n$-dimensional space, written\n\n$$\n\\mathbb{R}^n = \\{ (x_1, x_2, \\dots, x_n) \\mid x_i \\in \\mathbb{R} \\}.\n$$\n\nAn element of $\\mathbb{R}^n$ is called a vector of dimension $n$ or an *n*-vector. The number $n$ is called the\ndimension of the vector space. Thus $\\mathbb{R}^2$ is the space of all ordered pairs of real numbers, $\\mathbb{R}^3$ the\nspace of all ordered triples, and so on.\n\nExample 1.1.1.\n\n- A 2-dimensional vector: $(3, -1) \\in \\mathbb{R}^2$.\n- A 3-dimensional vector: $(2, 0, 5) \\in \\mathbb{R}^3$.\n- A 1-dimensional vector: $(7) \\in \\mathbb{R}^1$, which corresponds to the scalar \\$7$ itself.\n\nVectors are often written vertically in column form, which emphasizes their role in matrix multiplication:\n\n$$\n\\mathbf{v} = \\begin{bmatrix}\n2 \\\\\n0 \\\\\n5 \\end{bmatrix} \\in \\mathbb{R}^3.\n$$\n\nThe vertical layout makes the structure clearer when we consider linear combinations or multiply matrices by vectors.\n\n### Geometric Interpretation\n\nIn $\\mathbb{R}^2$, a vector $(x_1, x_2)$ can be visualized as an arrow starting at the origin $(0,0)$ and ending at the\npoint $(x_1, x_2)$. Its length corresponds to the distance from the origin, and its orientation gives a direction in the\nplane. In $\\mathbb{R}^3$, the same picture extends into three dimensions: a vector is an arrow from the origin\nto $(x_1, x_2, x_3)$. Beyond three dimensions, direct visualization is no longer possible, but the algebraic rules of\nvectors remain identical. Even though we cannot draw a vector in $\\mathbb{R}^{10}$, it behaves under addition, scaling,\nand transformation exactly as a 2- or 3-dimensional vector does. This abstract point of view is what allows linear\nalgebra to apply to data science, physics, and machine learning, where data often lives in very high-dimensional spaces.\nThus a vector may be regarded in three complementary ways:\n\n1. As a point in space, described by its coordinates.\n2. As a displacement or arrow, described by a direction and a length.\n3. As an abstract element of a vector space, whose properties follow algebraic rules independent of geometry.\n\n### Notation\n\n- Vectors are written in boldface lowercase letters: $\\mathbf{v}, \\mathbf{w}, \\mathbf{x}$.\n- The *i*-th entry of a vector $\\mathbf{v}$ is written $v_i$, where indices begin at 1.\n- The set of all *n*-dimensional vectors over $\\mathbb{R}$ is denoted $\\mathbb{R}^n$.\n- Column vectors will be the default form unless otherwise stated.\n\n### Why begin here?\n\nScalars and vectors form the atoms of linear algebra. Every structure we will build-vector spaces, linear\ntransformations, matrices, eigenvalues-relies on the basic notions of number and ordered collection of numbers. Once\nvectors are understood, we can define operations such as addition and scalar multiplication, then generalize to\nsubspaces, bases, and coordinate systems. Eventually, this framework grows into the full theory of linear algebra, with\npowerful applications to geometry, computation, and data.\n\n### Exercises 1.1\n\n1. Write three different vectors in $\\mathbb{R}^2$ and sketch them as arrows from the origin. Identify their coordinates\n   explicitly.\n2. Give an example of a vector in $\\mathbb{R}^4$. Can you visualize it directly? Explain why high-dimensional\n   visualization is challenging.\n3. Let $\\mathbf{v} = (4, -3, 2)$. Write $\\mathbf{v}$ in column form and state $v_1, v_2, v_3$.\n4. In what sense is the set $\\mathbb{R}^1$ both a line and a vector space? Illustrate with examples.\n5. Consider the vector $\\mathbf{u} = (1,1,\\dots,1) \\in \\mathbb{R}^n$. What is special about this vector when $n$ is\n   large? What might it represent in applications?\n\n## 1.2 Vector Addition and Scalar Multiplication\n\nVectors in linear algebra are not static objects; their power comes from the operations we can perform on them. Two\nfundamental operations define the structure of vector spaces: addition and scalar multiplication. These operations\nsatisfy simple but far-reaching rules that underpin the entire subject.\n\n### Vector Addition\n\nGiven two vectors of the same dimension, their sum is obtained by adding",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-20T02:14:19.453950"
  },
  {
    "basic_info": {
      "name": "port-kill",
      "full_name": "kagehq/port-kill",
      "owner": "kagehq",
      "description": "Port Kill helps you find and free ports blocking your dev work.",
      "url": "https://github.com/kagehq/port-kill",
      "clone_url": "https://github.com/kagehq/port-kill.git",
      "ssh_url": "git@github.com:kagehq/port-kill.git",
      "homepage": "https://portkill.com",
      "created_at": "2025-08-24T02:58:20Z",
      "updated_at": "2025-09-20T01:10:18Z",
      "pushed_at": "2025-09-19T16:09:36Z"
    },
    "stats": {
      "stars": 1495,
      "forks": 35,
      "watchers": 1495,
      "open_issues": 1,
      "size": 12767
    },
    "tech_info": {
      "language": "Rust",
      "languages": {
        "Rust": 336933,
        "Vue": 156718,
        "TypeScript": 92260,
        "Shell": 23719,
        "JavaScript": 12794,
        "Batchfile": 10726,
        "CSS": 2100
      },
      "license": "Other",
      "topics": []
    },
    "content": {
      "readme": "# Port Kill\n\nPort Kill helps you find and free ports blocking your dev work. It works on macOS, Linux, and Windows, locally or over SSH with a simple CLI, status bar and an optional dashboard.\n\n![Port Kill Status Bar Icon](image-short.png)\n\n## Community & Support\n\nJoin our Discord community for discussions, support, and updates:\n\n[![Discord](https://img.shields.io/badge/Discord-Join%20our%20community-7289DA?style=for-the-badge&logo=discord&logoColor=white)](https://discord.gg/KqdBcqRk5E)\n\n\n## Install\n\n```bash\n# macOS/Linux (when releases are available)\ncurl -fsSL https://raw.githubusercontent.com/kagehq/port-kill/main/install-release.sh | bash\n\n# Windows (PowerShell)\nInvoke-WebRequest -Uri 'https://raw.githubusercontent.com/kagehq/port-kill/main/install-release.bat' -OutFile 'install-release.bat'; .\\install-release.bat\n\n# From source\ngit clone https://github.com/kagehq/port-kill.git && cd port-kill && ./install.sh --all\n```\n\n## Quick start (CLI)\n\n```bash\n# See what's using common dev ports\n./target/release/port-kill-console --console --ports 3000,8000,8080\n\n# Scan port ranges (new!)\n./target/release/port-kill-console --console --json --ports '6000-9999'\n\n# Mixed individual ports and ranges\n./target/release/port-kill-console --console --ports '3000,6000-6002,8000'\n\n# Free up the usual suspects\n./target/release/port-kill-console --reset\n\n# Remote over SSH\n./target/release/port-kill-console --remote user@host --ports 3000,8000\n\n# Guard mode (watch + auto-resolve)\n./target/release/port-kill-console --guard-mode --auto-resolve\n\n# Security audit (JSON)\n./target/release/port-kill-console --audit --json\n\n# Endpoint monitoring (send data to external endpoint)\n./target/release/port-kill-console --monitor-endpoint https://api.company.com/port-status\n```\n\n## Dashboard (optional)\n\n![Port Kill Dashboard](dashboard/assets/img/portkill-dashboard.png)\n\n```bash\ncd dashboard\nnpm install\nnpm run dev   # http://localhost:3000\n```\n\n## MCP (use Port Kill from Cursor, Claude etc.)\n\nAdd `npx -y 'https://gitpkg.vercel.app/kagehq/port-kill/mcp?main'` to your MCP config.\n\nFor example for Cursor add to `.cursor/mcp.json`:\n```\n{\n   \"mcpServers\": {\n      \"port-kill-mcp\": {\n         \"command\": \"npx\",\n         \"args\": [\"-y\", \"https://gitpkg.vercel.app/kagehq/port-kill/mcp?main\"]\n      }\n   }\n}\n```\n\nNotes:\n- The server shells out to `./target/release/port-kill-console` or `port-kill-console` if it is on the PATH. If yours lives elsewhere, set `PORT_KILL_BIN=/absolute/path/to/port-kill-console`. e.g.\n```\n{\n   \"mcpServers\": {\n      \"port-kill-mcp\": {\n         \"command\": \"npx\",\n         \"args\": [\"-y\", \"https://gitpkg.vercel.app/kagehq/port-kill/mcp?main\"],\n         \"env\": {\n            \"PORT_KILL_BIN\": \"/absolute/path/to/port-kill-console\"\n         }\n      }\n   }\n}\n```\n\nSee [mcp/README.md](mcp/README.md) for more information on port-kill-mcp including how to install from source.\n\n## Features\n\n- Realâ€‘time process detection on specific ports or ranges\n- Oneâ€‘shot cleanup: `--reset`\n- Smart filtering and ignore lists\n- Port Guard Mode (watch/reserve/autoâ€‘resolve)\n- Security Audit Mode (suspicious ports, risk score, JSON)\n- Remote Mode over SSH\n- Works with Docker; console mode works everywhere\n\n## Common flags\n\n```bash\n--ports 3000,8000,8080          # specific ports\n--ports '6000-9999'             # port ranges (new!)\n--ports '3000,6000-6002,8000'   # mixed individual ports and ranges\n--start-port 3000 --end-port 9000\n--ignore-ports 5353,5000,7000\n--ignore-processes Chrome,rapportd\n--guard-mode --auto-resolve\n--audit --json\n--remote user@server\n```\n\n\n### Manual Installation\n\n1. Clone the repository:\n```bash\ngit clone <repository-url>\ncd port-kill\n```\n\n2. Install and build (recommended):\n```bash\n./install.sh\n```\n\nOr manually:\n```bash\n./build-macos.sh\n./run.sh\n```\n\n### Linux Installation\n\n1. Clone the repository:\n```bash\ngit clone <repository-url>\ncd port-kill\n```\n\n2. Install required packages:\n```bash\n# Ubuntu/Debian\nsudo apt-get install libatk1.0-dev libgdk-pixbuf2.0-dev libgtk-3-dev libappindicator3-dev\n\n# Fedora/RHEL\nsudo dnf install atk-devel gdk-pixbuf2-devel gtk3-devel libappindicator-gtk3-devel\n\n# Arch Linux\nsudo pacman -S atk gdk-pixbuf2 gtk3 libappindicator-gtk3\n```\n\n3. Install and build (recommended):\n```bash\n./install.sh\n```\n\nOr manually:\n```bash\n./build-linux.sh\n./run-linux.sh\n```\n\n### Windows Installation\n\n1. Clone the repository:\n```bash\ngit clone <repository-url>\ncd port-kill\n```\n\n2. Install Rust (if not already installed):\n```bash\n# Download and run rustup-init.exe from https://rustup.rs/\n```\n\n3. Install and build (recommended):\n```bash\n./install.sh\n```\n\nOr manually:\n```bash\nbuild-windows.bat\nrun-windows.bat\n```\n\n## Usage\n\n### Basic Usage\n\n**Platform-Specific Run Scripts:**\n- **macOS**: Use `./run.sh` \n- **Linux**: Use `./run-linux.sh`\n- **Windows**: Use `run-windows.bat`\n\n1. **Start the Application**: Run the appropriate script for your platform with default settings (ports 2000-6000)\n2. **Monitor Status**: Check the status bar fo",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-20T02:14:20.634133"
  },
  {
    "basic_info": {
      "name": "vimmaster",
      "full_name": "renzorlive/vimmaster",
      "owner": "renzorlive",
      "description": "VIM Master: in-browser game that teaches core Vim motions and editing commands through short, focused levels. ",
      "url": "https://github.com/renzorlive/vimmaster",
      "clone_url": "https://github.com/renzorlive/vimmaster.git",
      "ssh_url": "git@github.com:renzorlive/vimmaster.git",
      "homepage": "https://vim-master.com",
      "created_at": "2025-08-27T14:53:51Z",
      "updated_at": "2025-09-19T09:56:25Z",
      "pushed_at": "2025-09-02T18:11:17Z"
    },
    "stats": {
      "stars": 1322,
      "forks": 54,
      "watchers": 1322,
      "open_issues": 5,
      "size": 16027
    },
    "tech_info": {
      "language": "JavaScript",
      "languages": {
        "JavaScript": 204630,
        "HTML": 28685,
        "CSS": 8117
      },
      "license": "MIT License",
      "topics": [
        "coding",
        "education",
        "game",
        "indie-game",
        "vim"
      ]
    },
    "content": {
      "readme": "# VIM Master\n\nVIM Master -- in-browser game that teaches core Vim motions and editing commands through short, focused levels. \n\n## Try the Online Demo\n[![Demo Online](https://img.shields.io/badge/demo-online-brightgreen?logo=github&style=for-the-badge)](https://renzorlive.github.io/vimmaster/)\n\n> ğŸ’¡ **Tip:** For the best experience, use a desktop/laptop (full keyboard support).\n\n## Screenshot\n![VIM Master Screenshot](images/vm.gif)\n\n## Features\n- **Normal/Insert modes** with an on-screen status bar\n- **Command log** showing your keystrokes\n- **16 Progressive Levels** that validate your action outcomes (not just keystrokes)\n- **Complete Vim Support**: `h j k l`, `w b e`, `gg G`, `0 $`, `x`, `dd`, `dw`, `yy`, `p`, `i`, `a`, `o/O`, `cw`, `D`, `r`, ex-commands `:q`, `:wq`\n- **Numeric counts** for motions/operators (e.g., `3w`, `2dd`, `5x`, `5G`)\n- **Undo/redo support** (`u`, `Ctrl+r`)\n- **Vim-style search**: `/` and `?`, with `n`/`N` navigation and match highlighting\n- **Challenge Mode**: Fast-paced timed challenges to test your Vim skills\n- **Cheat Mode**: Interactive command reference with instant practice sessions\n- **Progress Management**: Auto-save, export/import codes, and progress tracking\n- **Badge System**: Earn visual badges as you learn (Beginner, Search Master)\n- **Profile Page**: Beautiful showcase of achievements with social media sharing\n- **Canvas-Based Achievement Cards**: Generate downloadable and shareable images\n- **Modular Architecture**: Clean, maintainable codebase for easy development\n\n## Recent UI/UX Improvements\n- **Streamlined Layout**: ASCII logo at top, title under text editor, buttons above instructions\n- **Compact Achievements**: Achievements container positioned before instructions for better flow\n- **Collapsible Progress Management**: Click to expand/collapse progress information, reducing UI clutter\n- **Responsive Design**: Optimized layout for better focus on gameplay elements\n- **Auto-Focus Editor**: Editor automatically focuses when lessons start for seamless UX\n- **Challenge Points Integration**: Challenge points now properly tracked and displayed in progress summary\n- **Enhanced Cheat Mode**: All cheat mode lessons now work with proper auto-focus and completion tracking\n\n## Latest Bug Fixes & Improvements\n- **Fixed \"undefined challenge points\"**: Challenge points now properly display in progress summary\n- **Enhanced Cheat Mode**: All practice lessons now auto-focus and validate completion correctly\n- **Improved Challenge Mode**: Better validation and scoring system for challenge tasks\n- **Auto-Focus UX**: Editor automatically focuses when starting lessons for better user experience\n- **Progress System**: Robust error handling and fallback values for all progress data\n\n## Progress Management System\nVIM Master features a comprehensive client-side progress tracking system that works entirely in your browser:\n\n### **Features**\n- **Auto-save**: Progress automatically saved every 5 seconds and after earning badges\n- **Export/Import Codes**: Generate compact Base64-encoded progress codes for backup and sharing\n- **Local Storage**: Progress persists between browser sessions\n- **Progress Summary**: Real-time display of current level, badges earned, and commands practiced\n- **Clear Progress**: Reset all progress with a single click\n\n### **How It Works**\n1. **Export Progress**: Click \"Export Progress\" to generate a shareable code\n2. **Import Progress**: Paste a code and click \"Import Progress\" to restore your game state\n3. **Progress Codes**: Compact, shareable strings containing all your achievements and progress\n4. **Privacy First**: All data stays on your device - no accounts or backend required\n\n### **Progress Data Tracked**\n- Current level and challenge mode status\n- Earned badges (Beginner, Search Master, etc.)\n- Commands practiced during gameplay\n- Challenge points earned from challenge mode\n- Timestamp of last save\n\n## Profile Page & Social Sharing\nShowcase your VIM mastery journey with a beautiful profile page and share achievements on social media.\n\n### **Profile Page Features**\n- **Beautiful Achievement Cards**: Eye-catching cards for each earned badge\n- **Progress Overview**: Visual representation of your learning journey with circular progress indicators\n- **ASCII Logo Integration**: The iconic VIM Master logo prominently displayed\n- **Social Media Integration**: Share achievements on Twitter, Facebook, and other platforms\n- **Progress Code Management**: Copy and share your progress codes easily\n- **GitHub Integration**: Links to view source code and contribute to the project\n\n### **Canvas-Based Achievement Cards**\n- **Dynamic Image Generation**: Create custom achievement cards using HTML5 Canvas\n- **Downloadable Images**: Save achievement cards as PNG files for sharing\n- **Social Media Ready**: Optimized dimensions and styling for social platforms\n- **Custom Branding**: Features the VIM Master ASCII logo and your progress data\n- **Professional Design**: Beautiful blue/pu",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-20T02:14:21.752958"
  },
  {
    "basic_info": {
      "name": "semantic-router",
      "full_name": "vllm-project/semantic-router",
      "owner": "vllm-project",
      "description": "Intelligent Mixture-of-Models Router for Efficient LLM Inference",
      "url": "https://github.com/vllm-project/semantic-router",
      "clone_url": "https://github.com/vllm-project/semantic-router.git",
      "ssh_url": "git@github.com:vllm-project/semantic-router.git",
      "homepage": "https://vllm-semantic-router.com",
      "created_at": "2025-08-26T21:49:50Z",
      "updated_at": "2025-09-20T01:01:30Z",
      "pushed_at": "2025-09-19T21:13:54Z"
    },
    "stats": {
      "stars": 1265,
      "forks": 122,
      "watchers": 1265,
      "open_issues": 56,
      "size": 5556
    },
    "tech_info": {
      "language": "Go",
      "languages": {
        "Go": 667927,
        "Python": 644711,
        "Rust": 189496,
        "JavaScript": 85364,
        "CSS": 63646,
        "Shell": 57681,
        "Makefile": 23181,
        "Dockerfile": 1176
      },
      "license": "Apache License 2.0",
      "topics": [
        "ai-gateway",
        "bert-classification",
        "envoy-ext-proc",
        "envoyproxy",
        "fine-tuning",
        "golang",
        "huggingface-candle",
        "huggingface-transformers",
        "kubernetes",
        "llm-tool-call",
        "mixture-of-models",
        "pii-detection",
        "prompt-guard",
        "python",
        "rust",
        "semantic-router",
        "vllm"
      ]
    },
    "content": {
      "readme": "<div align=\"center\">\n\n<img src=\"website/static/img/repo.png\" alt=\"vLLM Semantic Router\" width=\"80%\"/>\n\n[![Documentation](https://img.shields.io/badge/docs-read%20the%20docs-blue)](https://vllm-semantic-router.com)\n[![Hugging Face](https://img.shields.io/badge/ğŸ¤—%20Hugging%20Face-Community-yellow)](https://huggingface.co/LLM-Semantic-Router)\n[![License](https://img.shields.io/badge/license-Apache%202.0-blue.svg)](LICENSE)\n[![Crates.io](https://img.shields.io/crates/v/candle-semantic-router.svg)](https://crates.io/crates/candle-semantic-router)\n![Test And Build](https://github.com/vllm-project/semantic-router/workflows/Test%20And%20Build/badge.svg)\n\n**ğŸ“š [Complete Documentation](https://vllm-semantic-router.com) | ğŸš€ [Quick Start](https://vllm-semantic-router.com/docs/getting-started/installation) | ğŸ“£ [Blog](https://vllm-semantic-router.com/blog/) | ğŸ“– [API Reference](https://vllm-semantic-router.com/docs/api/router/)**\n\n![code](./website/static/img/code.png)\n\n</div>\n\n## Innovations âœ¨\n\n![architecture](./website/static/img/architecture.png)\n\n### Intelligent Routing ğŸ§ \n\n#### Auto-Reasoning and Auto-Selection of Models\n\nAn **Mixture-of-Models** (MoM) router that intelligently directs OpenAI API requests to the most suitable models from a defined pool based on **Semantic Understanding** of the request's intent (Complexity, Task, Tools).\n\nThis is achieved using BERT classification. Conceptually similar to Mixture-of-Experts (MoE) which lives *within* a model, this system selects the best *entire model* for the nature of the task.\n\nAs such, the overall inference accuracy is improved by using a pool of models that are better suited for different types of tasks:\n\n![Model Accuracy](./website/static/img/category_accuracies.png)\n\nThe screenshot below shows the LLM Router dashboard in Grafana.\n\n![LLM Router Dashboard](./website/static/img/grafana_screenshot.png)\n\nThe router is implemented in two ways: \n\n- Golang (with Rust FFI based on the [candle](https://github.com/huggingface/candle) rust ML framework)\n- Python\nBenchmarking will be conducted to determine the best implementation.\n\n#### Auto-Selection of Tools\n\nSelect the tools to use based on the prompt, avoiding the use of tools that are not relevant to the prompt so as to reduce the number of prompt tokens and improve tool selection accuracy by the LLM.\n\n### Enterprise Security ğŸ”’\n\n#### PII detection\n\nDetect PII in the prompt, avoiding sending PII to the LLM so as to protect the privacy of the user.\n\n#### Prompt guard\n\nDetect if the prompt is a jailbreak prompt, avoiding sending jailbreak prompts to the LLM so as to prevent the LLM from misbehaving.\n\n### Similarity Caching âš¡ï¸\n\nCache the semantic representation of the prompt so as to reduce the number of prompt tokens and improve the overall inference latency.\n\n## Documentation ğŸ“–\n\nFor comprehensive documentation including detailed setup instructions, architecture guides, and API references, visit:\n\n**ğŸ‘‰ [Complete Documentation at Read the Docs](https://vllm-semantic-router.com/)**\n\nThe documentation includes:\n\n- **[Installation Guide](https://vllm-semantic-router.com/docs/getting-started/installation/)** - Complete setup instructions\n- **[System Architecture](https://vllm-semantic-router.com/docs/architecture/system-architecture/)** - Technical deep dive\n- **[Model Training](https://vllm-semantic-router.com/docs/training/training-overview/)** - How classification models work\n- **[API Reference](https://vllm-semantic-router.com/docs/api/router/)** - Complete API documentation\n\n## Community ğŸ‘‹\n\nFor questions, feedback, or to contribute, please join `#semantic-router` channel in vLLM Slack.\n\n## Citation\n\nIf you find Semantic Router helpful in your research or projects, please consider citing it:\n\n```\n@misc{semanticrouter2025,\n  title={vLLM Semantic Router},\n  author={vLLM Semantic Router Team},\n  year={2025},\n  howpublished={\\url{https://github.com/vllm-project/semantic-router}},\n}\n```\n\n## Star History ğŸ”¥\n\nWe opened the project at Aug 31, 2025. We love open source  and collaboration â¤ï¸\n\n[![Star History Chart](https://api.star-history.com/svg?repos=vllm-project/semantic-router&type=Date)](https://www.star-history.com/#vllm-project/semantic-router&Date)\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-20T02:14:22.851886"
  },
  {
    "basic_info": {
      "name": "multi-agent-coding-system",
      "full_name": "Danau5tin/multi-agent-coding-system",
      "owner": "Danau5tin",
      "description": "Reached #13 on Stanford's Terminal Bench leaderboard. Orchestrator, explorer & coder agents working together with intelligent context sharing.",
      "url": "https://github.com/Danau5tin/multi-agent-coding-system",
      "clone_url": "https://github.com/Danau5tin/multi-agent-coding-system.git",
      "ssh_url": "git@github.com:Danau5tin/multi-agent-coding-system.git",
      "homepage": "",
      "created_at": "2025-08-30T19:23:45Z",
      "updated_at": "2025-09-20T01:16:31Z",
      "pushed_at": "2025-09-07T16:01:23Z"
    },
    "stats": {
      "stars": 1183,
      "forks": 145,
      "watchers": 1183,
      "open_issues": 2,
      "size": 849
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 139397,
        "Shell": 483
      },
      "license": null,
      "topics": []
    },
    "content": {
      "readme": "# ğŸ¤“ Orchestrator: A multi-agent AI coder. Reached #13 on Stanford's TerminalBench. Open sourced!\n\nTL;DR:\n- Over the weekend, quite unexpectedly, I made a multi-agent AI system that places slightly higher than Claude Code on Stanford's TerminalBench leaderboard (13th place).\n- This AI system consists of an orchestration agent that dispatches multiple explorer and coder agents to do all the work.\n- The orchestrator explicitly defines what knowledge artifacts subagents must return, then reuses and synthesises these artifacts across future tasks - creating compound intelligence where each action builds meaningfully on previous discoveries.\n\n![Orchestrator with claude-sonnet-4 on standford's terminal bench](./readme_imgs/orchestrator-sonnet-4-stanford-terminal-bench-leaderboard.png)\n\n## How the System Works\n\n![System architecture overview](readme_imgs/orch_agent_sys_arch.png)\n\nThe orchestrator acts as the brain of the operation - it receives the user's task but never touches code directly. Instead, it:\n\n1. **Analyses** the task and breaks it into focused subtasks\n2. **Dispatches** explorer agents to understand the system\n3. **Delegates** implementation work to coder agents with precise instructions\n4. **Verifies** all changes through additional explorer agents\n5. **Maintains** the context store with all discovered knowledge\n\nThe orchestrator's lack of direct code access forces proper delegation and verification patterns, leading to more strategic solutions.\n\nFor a full breakdown of this project's code structure, see [here](./PROJECT_STRUCTURE.md)\n\n\n## ğŸ“ˆ Evaluation Results\n\n### Performance on TerminalBench\n\n[Terminal bench](https://www.tbench.ai/) is a brilliant benchmark created by Stanford and [Laude Institute](https://www.laude.org/) to quantify agents' ability to complete complex tasks in the terminal. My Orchestrator system achieved **13th place** on the leaderboard, demonstrating competitive performance against leading AI coding assistants.\n\nI ran the Orchestrator evaluations with both Claude-4-Sonnet and also Qwen3-Coder-480B-A35B:\n\n![Performance comparison chart](readme_imgs/perf_chart.png)\n![Orchestrator with qwen-3-coder on standford's terminal bench](./readme_imgs/orchestrator-qwen-3-coder-stanford-terminal-bench-leaderboard.png)\n\nThis image shows Qwen-3-Coder performance on the benchmark. The screenshot towards the top of this README shows Sonnet-4 performance.\n\n### Cost & Efficiency\n\nOne of the most striking results is the amount of tokens used by Sonnet-4 as opposed to Qwen3-Coder.\n\nThe below table shows the total tokens (input and output included) processed across the TerminalBench evaluation run (5 attempts at 80 tasks = 400 trajectories).\n\n| Model | Success Rate | Total Evaluation Cost | Token Usage |\n|-------|--------------|------------|-------------|\n| **Claude Sonnet-4** | 37.0% | $263.56* | 93.2M tokens |\n| **Qwen-3-Coder** | 19.7% | $217.83 | 14.7M tokens |\n\n*Claude Sonnet-4 costs reflect heavy caching usage, reducing actual API costs\n\n\n## ğŸ¤– The Agents\n\nWhile all agents use the same underlying LLM, each operates with its own context window, specialised system message, and distinct toolset. This creates functionally different agents optimised for their specific roles.\n\n### ğŸ¯ Orchestrator Agent\n[System message](./src/agents/system_msgs/md_files/orchestrator_sys_msg_v0.1.md)\n**Role:** Strategic coordinator and persistent intelligence layer  \n**Capabilities:** Task decomposition, context management, subagent delegation  \n**Tools:** Task creation, subagent launching, context store management  \n**Restrictions:** Cannot read or modify code directly - operates purely at architectural level  \n\nThe orchestrator maintains the complete picture across all tasks, tracking discoveries and progress. It crafts precise task descriptions that explicitly specify what contexts subagents should return, ensuring focused and valuable information gathering.\n\n**Trust Calibration Strategy:**  \nThe orchestrator employs adaptive delegation based on task complexity:\n- **Low Complexity Tasks**: Grants extremely high autonomy to the coder agent for simple modifications and bug fixes\n- **Medium/Large Tasks**: Maintains strong trust but uses iterative decomposition - breaking complex problems into atomic, verifiable steps\n- **Verification Philosophy**: Uses explorer agents liberally to verify progress, especially when tasks involve critical functionality\n\n\n### ğŸ” Explorer Agent \n[System message](./src/agents/system_msgs/md_files/explorer_sys_msg_v0.1.md) \n**Role:** Read-only investigation and verification specialist  \n**Capabilities:** System exploration, code analysis, test execution, verification  \n**Tools:** File reading, search operations (grep/glob), bash commands, temporary script creation  \n**Restrictions:** Cannot modify existing files - strictly read-only operations  \n\nExplorers gather intelligence about the codebase, verify implementations, and discover system behaviors. They create knowledge artifacts that eliminat",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-20T02:14:23.974333"
  },
  {
    "basic_info": {
      "name": "Super-Mario-Bros.-Remastered-Public",
      "full_name": "JHDev2006/Super-Mario-Bros.-Remastered-Public",
      "owner": "JHDev2006",
      "description": null,
      "url": "https://github.com/JHDev2006/Super-Mario-Bros.-Remastered-Public",
      "clone_url": "https://github.com/JHDev2006/Super-Mario-Bros.-Remastered-Public.git",
      "ssh_url": "git@github.com:JHDev2006/Super-Mario-Bros.-Remastered-Public.git",
      "homepage": null,
      "created_at": "2025-09-13T15:29:58Z",
      "updated_at": "2025-09-20T02:08:57Z",
      "pushed_at": "2025-09-19T19:59:31Z"
    },
    "stats": {
      "stars": 1147,
      "forks": 137,
      "watchers": 1147,
      "open_issues": 158,
      "size": 51559
    },
    "tech_info": {
      "language": "GDScript",
      "languages": {
        "GDScript": 937173,
        "GAP": 36590,
        "C#": 10724,
        "GDShader": 2322
      },
      "license": "GNU General Public License v3.0",
      "topics": []
    },
    "content": {
      "readme": "# Super Mario Bros Remastered\nA Remake / Celebration of the original 'Super Mario Bros.' games. Features new levels, custom modes, new characters, alongside a full level editor / custom level system!\n\n<img width=\"3840\" height=\"2160\" alt=\"SMB1R_BANNER_printable\" src=\"https://github.com/user-attachments/assets/ed0e97a8-614a-44e2-b69f-2654fca6196c\" />\n\n### Art by [@krystalphantasm.bsky.social](https://bsky.app/profile/krystalphantasm.bsky.social/post/3lvgmgvjeks2f)\n\n### Download: https://github.com/JHDev2006/Super-Mario-Bros.-Remastered-Public/releases\n\n# Requires an original SMB1 NES ROM to play! None of the original assets are contained in the source code, unless it was originally made by us!\n\n# This does NOT act as a replacement for the original Super Mario Bros. games. Super Mario Bros. & Super Mario Bros.: The Lost Levels, can be played now on Nintendo Switch, through Nintendo Switch Online\n\n## Features\n- Super Mario Bros., Super Mario Bros.: The Lost Levels, Super Mario Bros. Special and All Night Nippon: Super Mario Bros. Fully recreated from the ground up!\n- Improved physics / level design\n- Resource Packs! Fully customize how the game looks and sounds.\n- Custom Characters - Add in your own characters to use in game.\n- Fully Open Source!\n- Level Share Square Partnered\n\n## Downloading\n1. Go to the 'Releases' page\n2. Look for the latest version\n3. Download the .zip for your OS\n4. Extract and run\n5. Enjoy!\n\n## Importing for editing\n1. Download the source\n2. Download Godot 4.5 beta 3\n3. Import the project\n4. Enjoy!\n\n## Contributing\nYou are more than welcome to contribute any fixes / improvements you'd like, simply open a pull request, and I'll review it ASAP!\n\n## System Requirements\n\nPlease refer to the Godot engine requirements for minimum and recommended hardware specifications.\n\n[Minimum Requirements](https://docs.godotengine.org/en/latest/about/system_requirements.html#desktop-or-laptop-pc-minimum)\n\n[Recommended Requirements](https://docs.godotengine.org/en/latest/about/system_requirements.html#id3)\n\n\n## Issues\nWhen opening an issue, please keep it to one report, per post, and try and be as helpful as possible, when telling me what has occured, so that its as easy to fix as possible.\nPlease do not open issues, for feature requests, suggestions, or opinions. BUG REPORTS ONLY\n\n## Known Issues\nThere are a couple known issues, mainly due to being built off of Godot, and these issues existing in the engine itself.\n- Steam deck controls do not work natively, you can circumvent this by setting up controller bindings to emulate keys instead, apologies.\n- Physics are weird, when interacting with corners + the camera barrier\n- Drop shadows jitter when playing with \"Smooth Rendering\"\n- Several entities jitter at times.\n- Blocks + coins, respawn when reloading resource packs\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-20T02:14:25.094706"
  },
  {
    "basic_info": {
      "name": "HunyuanWorld-Voyager",
      "full_name": "Tencent-Hunyuan/HunyuanWorld-Voyager",
      "owner": "Tencent-Hunyuan",
      "description": "Voyager is an interactive RGBD video generation model conditioned on camera input, and supports real-time 3D reconstruction.",
      "url": "https://github.com/Tencent-Hunyuan/HunyuanWorld-Voyager",
      "clone_url": "https://github.com/Tencent-Hunyuan/HunyuanWorld-Voyager.git",
      "ssh_url": "git@github.com:Tencent-Hunyuan/HunyuanWorld-Voyager.git",
      "homepage": "https://3d-models.hunyuan.tencent.com/world/",
      "created_at": "2025-08-27T09:34:10Z",
      "updated_at": "2025-09-20T01:01:58Z",
      "pushed_at": "2025-09-10T08:21:15Z"
    },
    "stats": {
      "stars": 1139,
      "forks": 74,
      "watchers": 1139,
      "open_issues": 14,
      "size": 850180
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 468553,
        "Shell": 1060
      },
      "license": "Other",
      "topics": [
        "3d",
        "3d-generation",
        "aigc",
        "hunyuan3d",
        "image-to-3d",
        "image-to-video",
        "scene-generation",
        "world-model",
        "world-models"
      ]
    },
    "content": {
      "readme": "[ä¸­æ–‡é˜…è¯»](README_zh.md)\n\n# **HunyuanWorld-Voyager**\n\n<p align=\"center\">\n  <img src=\"assets/teaser.png\">\n</p>\n\n<div align=\"center\">\n  <a href=\"https://3d-models.hunyuan.tencent.com/world/\" target=\"_blank\"><img src=\"https://img.shields.io/static/v1?label=Project%20Page&message=Web&color=green\" height=22px></a>\n  <a href=\"https://3d-models.hunyuan.tencent.com/voyager/voyager_en/assets/HYWorld_Voyager.pdf\" target=\"_blank\"><img src=\"https://img.shields.io/static/v1?label=Tech%20Report&message=arxiv&color=red\" height=22px></a>\n  <a href=\"https://huggingface.co/tencent/HunyuanWorld-Voyager\" target=\"_blank\"><img src=\"https://img.shields.io/static/v1?label=HunyuanWorld-Voyager&message=HuggingFace&color=yellow\" height=22px></a>\n</div>\n\n-----\n\nWe introduce HunyuanWorld-Voyager, a novel video diffusion framework that generates world-consistent 3D point-cloud sequences from a single image with user-defined camera path. Voyager can generate 3D-consistent scene videos for world exploration following custom camera trajectories. It can also generate aligned depth and RGB video for efficient and direct 3D reconstruction.\n\n\n## ğŸ”¥ğŸ”¥ğŸ”¥ News!!\n* Sep 2, 2025: ğŸ‘‹ We release the code and model weights of HunyuanWorld-Voyager. [Download](ckpts/README.md).\n\n> Join our **[Wechat](#)** and **[Discord](https://discord.gg/dNBrdrGGMa)** group to discuss and find help from us.\n\n| Wechat Group                                     | Xiaohongshu                                           | X                                           | Discord                                           |\n|--------------------------------------------------|-------------------------------------------------------|---------------------------------------------|---------------------------------------------------|\n| <img src=\"assets/qrcode/wechat.png\"  height=140> | <img src=\"assets/qrcode/xiaohongshu.png\"  height=140> | <img src=\"assets/qrcode/x.png\"  height=140> | <img src=\"assets/qrcode/discord.png\"  height=140> | \n\n## ğŸ¥ Demo\n### Demo Video\n\n<div align=\"center\">\n  <video src=\"https://github.com/user-attachments/assets/2eb844c9-30ba-4770-8066-189c123affee\" width=\"80%\" poster=\"\"> </video>\n</div>\n\n### Camera-Controllable Video Generation\n\n|  Input | Generated Video  |\n|:----------------:|:----------------:|\n|  <img src=\"assets/demo/camera/input1.png\" width=\"80%\">        |       <video src=\"https://github.com/user-attachments/assets/2b03ecd5-9a8f-455c-bf04-c668d3a61b04\" width=\"100%\"> </video>        |\n| <img src=\"assets/demo/camera/input2.png\" width=\"80%\">         |       <video src=\"https://github.com/user-attachments/assets/45844ac0-c65a-4e04-9f7d-4c72d47e0339\" width=\"100%\"> </video>        | \n| <img src=\"assets/demo/camera/input3.png\" width=\"80%\">         |       <video src=\"https://github.com/user-attachments/assets/f7f48473-3bb5-4a30-bd22-af3ca95ee8dc\" width=\"100%\"> </video>        |\n\n### Multiple Applications\n\n- Video Reconstruction\n\n| Generated Video | Reconstructed Point Cloud |\n|:---------------:|:--------------------------------:|\n| <video src=\"https://github.com/user-attachments/assets/72a41804-63fc-4596-963d-1497e68f7790\" width=\"100%\"> </video> | <video src=\"https://github.com/user-attachments/assets/67574e9c-9e21-4ed6-9503-e65d187086a2\" width=\"100%\"> </video> |\n\n- Image-to-3D Generation\n\n| | |\n|:---------------:|:---------------:|\n| <video src=\"https://github.com/user-attachments/assets/886aa86d-990e-4b86-97a5-0b9110862d14\" width=\"100%\"> </video> | <video src=\"https://github.com/user-attachments/assets/4c1734ba-4e78-4979-b30e-3c8c97aa984b\" width=\"100%\"> </video> |\n\n- Video Depth Estimation\n\n| | |\n|:---------------:|:---------------:|\n| <video src=\"https://github.com/user-attachments/assets/e4c8b729-e880-4be3-826f-429a5c1f12cd\" width=\"100%\"> </video> | <video src=\"https://github.com/user-attachments/assets/7ede0745-cde7-42f1-9c28-e4dca90dac52\" width=\"100%\"> </video> |\n\n\n## â˜¯ï¸ **HunyuanWorld-Voyager Introduction**\n###  Architecture\n\nVoyager consists of two key components:\n\n(1) World-Consistent Video Diffusion: A unified architecture that jointly generates aligned RGB and depth video sequences, conditioned on existing world observation to ensure global coherence.\n\n(2) Long-Range World Exploration: An efficient world cache with point culling and an auto-regressive inference with smooth video sampling for iterative scene extension with context-aware consistency.\n\nTo train Voyager, we propose a scalable data engine, i.e., a video reconstruction pipeline that automates camera pose estimation and metric depth prediction for arbitrary videos, enabling large-scale, diverse training data curation without manual 3D annotations. Using this pipeline, we compile a dataset of over 100,000 video clips, combining real-world captures and synthetic Unreal Engine renders.\n\n<p align=\"center\">\n  <img src=\"assets/backbone.jpg\"  height=500>\n</p>\n\n### Performance\n\n<table class=\"comparison-table\">\n  <thead>\n    <tr>\n      <th>Method</th>\n      <th>WorldScore Average</th>\n      <th>Camera Con",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-20T02:14:26.224159"
  },
  {
    "basic_info": {
      "name": "map-anything",
      "full_name": "facebookresearch/map-anything",
      "owner": "facebookresearch",
      "description": "MapAnything: Universal Feed-Forward Metric 3D Reconstruction",
      "url": "https://github.com/facebookresearch/map-anything",
      "clone_url": "https://github.com/facebookresearch/map-anything.git",
      "ssh_url": "git@github.com:facebookresearch/map-anything.git",
      "homepage": "",
      "created_at": "2025-09-04T14:37:36Z",
      "updated_at": "2025-09-20T01:56:12Z",
      "pushed_at": "2025-09-19T10:17:35Z"
    },
    "stats": {
      "stars": 1127,
      "forks": 42,
      "watchers": 1127,
      "open_issues": 11,
      "size": 6164
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 1727697,
        "Shell": 157713
      },
      "license": "Apache License 2.0",
      "topics": [
        "3d-reconstruction",
        "ai",
        "calibration",
        "depth-completion",
        "depth-estimation",
        "image-to-3d",
        "multi-view-stereo",
        "robotics",
        "sfm"
      ]
    },
    "content": {
      "readme": "<div align=\"center\">\n<h1>MapAnything: Universal Feed-Forward Metric <br>3D Reconstruction</h1>\n<a href=\"https://map-anything.github.io/assets/MapAnything.pdf\"><img src=\"https://img.shields.io/badge/Paper-blue\" alt=\"Paper\"></a>\n<a href=\"https://arxiv.org/abs/2509.13414\"><img src=\"https://img.shields.io/badge/arXiv-2509.13414-b31b1b\" alt=\"arXiv\"></a>\n<a href=\"https://map-anything.github.io/\"><img src=\"https://img.shields.io/badge/Project_Page-green\" alt=\"Project Page\"></a>\n<a href=\"https://huggingface.co/spaces/facebook/map-anything\"><img src='https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Demo-blue'></a>\n<br>\n<br>\n<strong>\n<a href=\"https://nik-v9.github.io/\">Nikhil Keetha<sup>1,2</sup></a>\n&nbsp;&nbsp;\n<a href=\"https://sirwyver.github.io/\">Norman MÃ¼ller<sup>1</sup></a>\n&nbsp;&nbsp;\n<a href=\"https://demuc.de/\">Johannes SchÃ¶nberger<sup>1</sup></a>\n&nbsp;&nbsp;\n<a href=\"https://www.linkedin.com/in/lorenzoporzi\">Lorenzo Porzi<sup>1</sup></a>\n&nbsp;&nbsp;\n<a href=\"https://infinity1096.github.io/\">Yuchen Zhang<sup>2</sup></a>\n<br>\n<a href=\"https://tobiasfshr.github.io/\">Tobias Fischer<sup>1</sup></a>\n&nbsp;&nbsp;\n<a href=\"https://www.linkedin.com/in/arno-knapitsch\">Arno Knapitsch<sup>1</sup></a>\n&nbsp;&nbsp;\n<a href=\"https://www.linkedin.com/in/duncan-zauss\">Duncan Zauss<sup>1</sup></a>\n&nbsp;&nbsp;\n<a href=\"https://ethanweber.me/\">Ethan Weber<sup>1</sup></a>\n&nbsp;&nbsp;\n<a href=\"https://www.linkedin.com/in/nelsonantunes7\">Nelson Antunes<sup>1</sup></a>\n<br>\n<a href=\"https://x.com/jonathonluiten?lang=en\">Jonathon Luiten<sup>1</sup></a>\n&nbsp;&nbsp;\n<a href=\"https://m.lopezantequera.com/\">Manuel Lopez-Antequera<sup>1</sup></a>\n&nbsp;&nbsp;\n<a href=\"https://scholar.google.com/citations?user=484sccEAAAAJ\">Samuel Rota BulÃ²<sup>1</sup></a>\n&nbsp;&nbsp;\n<a href=\"https://richardt.name/\">Christian Richardt<sup>1</sup></a>\n<br>\n<a href=\"https://www.cs.cmu.edu/~deva/\">Deva Ramanan<sup>2</sup></a>\n&nbsp;&nbsp;\n<a href=\"https://theairlab.org/team/sebastian/\">Sebastian Scherer<sup>2</sup></a>\n&nbsp;&nbsp;\n<a href=\"https://www.linkedin.com/in/peter-kontschieder-2a6410134\">Peter Kontschieder<sup>1</sup></a>\n<br>\n<br>\n<sup>1</sup> Meta &nbsp;&nbsp;\n<sup>2</sup> Carnegie Mellon University\n</strong>\n\n</div>\n\n## Overview\n\nMapAnything is a simple, end-to-end trained transformer model that directly regresses the factored metric 3D geometry of a scene given various types of inputs (images, calibration, poses, or depth). A single feed-forward model supports over 12 different 3D reconstruction tasks including multi-image sfm, multi-view stereo, monocular metric depth estimation, registration, depth completion and more.\n\n![Overview](./assets/teaser.png)\n\n## Table of Contents\n\n- [Quick Start](#quick-start)\n  - [Installation](#installation)\n  - [Image-Only Inference](#image-only-inference)\n  - [Multi-Modal Inference](#multi-modal-inference)\n- [Interactive Demos](#interactive-demos)\n  - [Online Demo](#online-demo)\n  - [Local Gradio Demo](#local-gradio-demo)\n  - [Rerun Demo](#rerun-demo)\n- [COLMAP & GSplat Support](#colmap--gsplat-support)\n  - [Exporting to COLMAP Format](#exporting-to-colmap-format)\n  - [Integration with Gaussian Splatting](#integration-with-gaussian-splatting)\n- [Data Processing for Training & Benchmarking](#data-processing-for-training--benchmarking)\n- [Training](#training)\n- [Benchmarking](#benchmarking)\n- [Code License](#code-license)\n- [Models](#models)\n- [Building Blocks for MapAnything](#building-blocks-for-mapanything)\n- [Acknowledgments](#acknowledgments)\n- [Citation](#citation)\n\n## Quick Start\n\n### Installation\n\n```bash\ngit clone https://github.com/facebookresearch/map-anything.git\ncd map-anything\n\n# Create and activate conda environment\nconda create -n mapanything python=3.12 -y\nconda activate mapanything\n\n# Optional: Install torch, torchvision & torchaudio specific to your system\n# Install MapAnything\npip install -e .\n\n# For all optional dependencies\n# See pyproject.toml for more details\npip install -e \".[all]\"\npre-commit install\n```\n\nNote that we don't pin a specific version of PyTorch or CUDA in our requirements. Please feel free to install PyTorch based on your specific system.\n\n### Image-Only Inference\n\nFor metric 3D reconstruction from images without additional geometric inputs:\n\n```python\n# Optional config for better memory efficiency\nimport os\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n\n# Required imports\nimport torch\nfrom mapanything.models import MapAnything\nfrom mapanything.utils.image import load_images\n\n# Get inference device\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# Init model - This requries internet access or the huggingface hub cache to be pre-downloaded\n# For Apache 2.0 license model, use \"facebook/map-anything-apache\"\nmodel = MapAnything.from_pretrained(\"facebook/map-anything\").to(device)\n\n# Load and preprocess images from a folder or list of paths\nimages = \"path/to/your/images/\"  # or [\"path/to/img1.jpg\", \"path/to/img2.jpg\", ...]\nviews = loa",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-20T02:14:27.359249"
  }
]