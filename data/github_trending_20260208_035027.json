[
  {
    "basic_info": {
      "name": "nanobot",
      "full_name": "HKUDS/nanobot",
      "owner": "HKUDS",
      "description": "\"ğŸˆ nanobot: The Ultra-Lightweight Clawdbot\"",
      "url": "https://github.com/HKUDS/nanobot",
      "clone_url": "https://github.com/HKUDS/nanobot.git",
      "ssh_url": "git@github.com:HKUDS/nanobot.git",
      "homepage": "",
      "created_at": "2026-02-01T07:16:15Z",
      "updated_at": "2026-02-08T03:50:25Z",
      "pushed_at": "2026-02-07T18:15:19Z"
    },
    "stats": {
      "stars": 12463,
      "forks": 1677,
      "watchers": 12463,
      "open_issues": 223,
      "size": 32775
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 201175,
        "TypeScript": 7851,
        "Shell": 7009,
        "Dockerfile": 1294,
        "JavaScript": 1256
      },
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "<div align=\"center\">\n  <img src=\"nanobot_logo.png\" alt=\"nanobot\" width=\"500\">\n  <h1>nanobot: Ultra-Lightweight Personal AI Assistant</h1>\n  <p>\n    <a href=\"https://pypi.org/project/nanobot-ai/\"><img src=\"https://img.shields.io/pypi/v/nanobot-ai\" alt=\"PyPI\"></a>\n    <a href=\"https://pepy.tech/project/nanobot-ai\"><img src=\"https://static.pepy.tech/badge/nanobot-ai\" alt=\"Downloads\"></a>\n    <img src=\"https://img.shields.io/badge/python-â‰¥3.11-blue\" alt=\"Python\">\n    <img src=\"https://img.shields.io/badge/license-MIT-green\" alt=\"License\">\n    <a href=\"./COMMUNICATION.md\"><img src=\"https://img.shields.io/badge/Feishu-Group-E9DBFC?style=flat&logo=feishu&logoColor=white\" alt=\"Feishu\"></a>\n    <a href=\"./COMMUNICATION.md\"><img src=\"https://img.shields.io/badge/WeChat-Group-C5EAB4?style=flat&logo=wechat&logoColor=white\" alt=\"WeChat\"></a>\n    <a href=\"https://discord.gg/MnCvHqpUGB\"><img src=\"https://img.shields.io/badge/Discord-Community-5865F2?style=flat&logo=discord&logoColor=white\" alt=\"Discord\"></a>\n  </p>\n</div>\n\nğŸˆ **nanobot** is an **ultra-lightweight** personal AI assistant inspired by [Clawdbot](https://github.com/openclaw/openclaw) \n\nâš¡ï¸ Delivers core agent functionality in just **~4,000** lines of code â€” **99% smaller** than Clawdbot's 430k+ lines.\n\nğŸ“ Real-time line count: **3,422 lines** (run `bash core_agent_lines.sh` to verify anytime)\n\n## ğŸ“¢ News\n\n- **2026-02-07** ğŸš€ Released v0.1.3.post5 with Qwen support & several improvements! Check [here](https://github.com/HKUDS/nanobot/releases/tag/v0.1.3.post5) for details.\n- **2026-02-06** âœ¨ Added Moonshot/Kimi provider, Discord integration, and enhanced security hardening!\n- **2026-02-05** âœ¨ Added Feishu channel, DeepSeek provider, and enhanced scheduled tasks support!\n- **2026-02-04** ğŸš€ Released v0.1.3.post4 with multi-provider & Docker support! Check [here](https://github.com/HKUDS/nanobot/releases/tag/v0.1.3.post4) for details.\n- **2026-02-03** âš¡ Integrated vLLM for local LLM support and improved natural language task scheduling!\n- **2026-02-02** ğŸ‰ nanobot officially launched! Welcome to try ğŸˆ nanobot!\n\n## Key Features of nanobot:\n\nğŸª¶ **Ultra-Lightweight**: Just ~4,000 lines of core agent code â€” 99% smaller than Clawdbot.\n\nğŸ”¬ **Research-Ready**: Clean, readable code that's easy to understand, modify, and extend for research.\n\nâš¡ï¸ **Lightning Fast**: Minimal footprint means faster startup, lower resource usage, and quicker iterations.\n\nğŸ’ **Easy-to-Use**: One-click to deploy and you're ready to go.\n\n## ğŸ—ï¸ Architecture\n\n<p align=\"center\">\n  <img src=\"nanobot_arch.png\" alt=\"nanobot architecture\" width=\"800\">\n</p>\n\n## âœ¨ Features\n\n<table align=\"center\">\n  <tr align=\"center\">\n    <th><p align=\"center\">ğŸ“ˆ 24/7 Real-Time Market Analysis</p></th>\n    <th><p align=\"center\">ğŸš€ Full-Stack Software Engineer</p></th>\n    <th><p align=\"center\">ğŸ“… Smart Daily Routine Manager</p></th>\n    <th><p align=\"center\">ğŸ“š Personal Knowledge Assistant</p></th>\n  </tr>\n  <tr>\n    <td align=\"center\"><p align=\"center\"><img src=\"case/search.gif\" width=\"180\" height=\"400\"></p></td>\n    <td align=\"center\"><p align=\"center\"><img src=\"case/code.gif\" width=\"180\" height=\"400\"></p></td>\n    <td align=\"center\"><p align=\"center\"><img src=\"case/scedule.gif\" width=\"180\" height=\"400\"></p></td>\n    <td align=\"center\"><p align=\"center\"><img src=\"case/memory.gif\" width=\"180\" height=\"400\"></p></td>\n  </tr>\n  <tr>\n    <td align=\"center\">Discovery â€¢ Insights â€¢ Trends</td>\n    <td align=\"center\">Develop â€¢ Deploy â€¢ Scale</td>\n    <td align=\"center\">Schedule â€¢ Automate â€¢ Organize</td>\n    <td align=\"center\">Learn â€¢ Memory â€¢ Reasoning</td>\n  </tr>\n</table>\n\n## ğŸ“¦ Install\n\n**Install from source** (latest features, recommended for development)\n\n```bash\ngit clone https://github.com/HKUDS/nanobot.git\ncd nanobot\npip install -e .\n```\n\n**Install with [uv](https://github.com/astral-sh/uv)** (stable, fast)\n\n```bash\nuv tool install nanobot-ai\n```\n\n**Install from PyPI** (stable)\n\n```bash\npip install nanobot-ai\n```\n\n## ğŸš€ Quick Start\n\n> [!TIP]\n> Set your API key in `~/.nanobot/config.json`.\n> Get API keys: [OpenRouter](https://openrouter.ai/keys) (Global) Â· [DashScope](https://dashscope.console.aliyun.com) (Qwen) Â· [Brave Search](https://brave.com/search/api/) (optional, for web search)\n\n**1. Initialize**\n\n```bash\nnanobot onboard\n```\n\n**2. Configure** (`~/.nanobot/config.json`)\n\nFor OpenRouter - recommended for global users:\n```json\n{\n  \"providers\": {\n    \"openrouter\": {\n      \"apiKey\": \"sk-or-v1-xxx\"\n    }\n  },\n  \"agents\": {\n    \"defaults\": {\n      \"model\": \"anthropic/claude-opus-4-5\"\n    }\n  }\n}\n```\n\n**3. Chat**\n\n```bash\nnanobot agent -m \"What is 2+2?\"\n```\n\nThat's it! You have a working AI assistant in 2 minutes.\n\n## ğŸ–¥ï¸ Local Models (vLLM)\n\nRun nanobot with your own local models using vLLM or any OpenAI-compatible server.\n\n**1. Start your vLLM server**\n\n```bash\nvllm serve meta-llama/Llama-3.1-8B-Instruct --port 8000\n```\n\n**2. Configure** (`~/.nanobot/config.json`)\n\n```json\n{\n  \"providers\": {\n    \"vllm\": {\n      \"apiKey\": \"dummy\",\n   ",
      "default_branch": "main"
    },
    "fetched_at": "2026-02-08T03:50:28.727447"
  },
  {
    "basic_info": {
      "name": "daily_stock_analysis",
      "full_name": "ZhuLinsen/daily_stock_analysis",
      "owner": "ZhuLinsen",
      "description": "LLMé©±åŠ¨çš„ A/H/ç¾è‚¡æ™ºèƒ½åˆ†æå™¨ï¼Œå¤šæ•°æ®æºè¡Œæƒ… + å®æ—¶æ–°é—» + Gemini å†³ç­–ä»ªè¡¨ç›˜ + å¤šæ¸ é“æ¨é€ï¼Œé›¶æˆæœ¬ï¼Œçº¯ç™½å«–ï¼Œå®šæ—¶è¿è¡Œ",
      "url": "https://github.com/ZhuLinsen/daily_stock_analysis",
      "clone_url": "https://github.com/ZhuLinsen/daily_stock_analysis.git",
      "ssh_url": "git@github.com:ZhuLinsen/daily_stock_analysis.git",
      "homepage": "",
      "created_at": "2026-01-10T06:43:20Z",
      "updated_at": "2026-02-08T03:37:42Z",
      "pushed_at": "2026-02-07T14:08:32Z"
    },
    "stats": {
      "stars": 9829,
      "forks": 10324,
      "watchers": 9829,
      "open_issues": 16,
      "size": 42966
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 883270,
        "TypeScript": 92329,
        "CSS": 16643,
        "JavaScript": 10662,
        "Shell": 10583,
        "PowerShell": 4120,
        "HTML": 3408,
        "Dockerfile": 1648
      },
      "license": "MIT License",
      "topics": [
        "agent",
        "ai",
        "aigc",
        "gemini",
        "llm",
        "quant",
        "quantitative-trading",
        "rag",
        "stock"
      ]
    },
    "content": {
      "readme": "<div align=\"center\">\n\n# ğŸ“ˆ è‚¡ç¥¨æ™ºèƒ½åˆ†æç³»ç»Ÿ\n\n[![GitHub stars](https://img.shields.io/github/stars/ZhuLinsen/daily_stock_analysis?style=social)](https://github.com/ZhuLinsen/daily_stock_analysis/stargazers)\n[![CI](https://github.com/ZhuLinsen/daily_stock_analysis/actions/workflows/ci.yml/badge.svg)](https://github.com/ZhuLinsen/daily_stock_analysis/actions/workflows/ci.yml)\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)\n[![GitHub Actions](https://img.shields.io/badge/GitHub%20Actions-Ready-2088FF?logo=github-actions&logoColor=white)](https://github.com/features/actions)\n[![Docker](https://img.shields.io/badge/Docker-Ready-2496ED?logo=docker&logoColor=white)](https://hub.docker.com/)\n\n> ğŸ¤– åŸºäº AI å¤§æ¨¡å‹çš„ Aè‚¡/æ¸¯è‚¡/ç¾è‚¡è‡ªé€‰è‚¡æ™ºèƒ½åˆ†æç³»ç»Ÿï¼Œæ¯æ—¥è‡ªåŠ¨åˆ†æå¹¶æ¨é€ã€Œå†³ç­–ä»ªè¡¨ç›˜ã€åˆ°ä¼ä¸šå¾®ä¿¡/é£ä¹¦/Telegram/é‚®ç®±\n\n[**åŠŸèƒ½ç‰¹æ€§**](#-åŠŸèƒ½ç‰¹æ€§) Â· [**å¿«é€Ÿå¼€å§‹**](#-å¿«é€Ÿå¼€å§‹) Â· [**æ¨é€æ•ˆæœ**](#-æ¨é€æ•ˆæœ) Â· [**å®Œæ•´æŒ‡å—**](docs/full-guide.md) Â· [**å¸¸è§é—®é¢˜**](docs/FAQ.md) Â· [**æ›´æ–°æ—¥å¿—**](docs/CHANGELOG.md)\n\nç®€ä½“ä¸­æ–‡ | [English](docs/README_EN.md) | [ç¹é«”ä¸­æ–‡](docs/README_CHT.md)\n\n</div>\n\n## ğŸ’– èµåŠ©å•† (Sponsors)\n<div align=\"center\">\n  <a href=\"https://serpapi.com/baidu-search-api?utm_source=github_daily_stock_analysis\" target=\"_blank\">\n    <img src=\"./sources/serpapi_banner_zh.png\" alt=\"è½»æ¾æŠ“å–æœç´¢å¼•æ“ä¸Šçš„å®æ—¶é‡‘èæ–°é—»æ•°æ® - SerpApi\" height=\"160\">\n  </a>\n</div>\n<br>\n\n\n## âœ¨ åŠŸèƒ½ç‰¹æ€§\n\n| æ¨¡å— | åŠŸèƒ½ | è¯´æ˜ |\n|------|------|------|\n| AI | å†³ç­–ä»ªè¡¨ç›˜ | ä¸€å¥è¯æ ¸å¿ƒç»“è®º + ç²¾ç¡®ä¹°å–ç‚¹ä½ + æ“ä½œæ£€æŸ¥æ¸…å• |\n| åˆ†æ | å¤šç»´åº¦åˆ†æ | æŠ€æœ¯é¢ + ç­¹ç åˆ†å¸ƒ + èˆ†æƒ…æƒ…æŠ¥ + å®æ—¶è¡Œæƒ… |\n| å¸‚åœº | å…¨çƒå¸‚åœº | æ”¯æŒ Aè‚¡ã€æ¸¯è‚¡ã€ç¾è‚¡ |\n| å¤ç›˜ | å¤§ç›˜å¤ç›˜ | æ¯æ—¥å¸‚åœºæ¦‚è§ˆã€æ¿å—æ¶¨è·Œã€åŒ—å‘èµ„é‡‘ |\n| æ¨é€ | å¤šæ¸ é“é€šçŸ¥ | ä¼ä¸šå¾®ä¿¡ã€é£ä¹¦ã€Telegramã€é’‰é’‰ã€é‚®ä»¶ã€Pushover |\n| è‡ªåŠ¨åŒ– | å®šæ—¶è¿è¡Œ | GitHub Actions å®šæ—¶æ‰§è¡Œï¼Œæ— éœ€æœåŠ¡å™¨ |\n\n### æŠ€æœ¯æ ˆä¸æ•°æ®æ¥æº\n\n| ç±»å‹ | æ”¯æŒ |\n|------|------|\n| AI æ¨¡å‹ | Geminiï¼ˆå…è´¹ï¼‰ã€OpenAI å…¼å®¹ã€DeepSeekã€é€šä¹‰åƒé—®ã€Claudeã€Ollama |\n| è¡Œæƒ…æ•°æ® | AkShareã€Tushareã€Pytdxã€Baostockã€YFinance |\n| æ–°é—»æœç´¢ | Tavilyã€SerpAPIã€Bochaã€Brave |\n\n### å†…ç½®äº¤æ˜“çºªå¾‹\n\n| è§„åˆ™ | è¯´æ˜ |\n|------|------|\n| ä¸¥ç¦è¿½é«˜ | ä¹–ç¦»ç‡ > 5% è‡ªåŠ¨æç¤ºé£é™© |\n| è¶‹åŠ¿äº¤æ˜“ | MA5 > MA10 > MA20 å¤šå¤´æ’åˆ— |\n| ç²¾ç¡®ç‚¹ä½ | ä¹°å…¥ä»·ã€æ­¢æŸä»·ã€ç›®æ ‡ä»· |\n| æ£€æŸ¥æ¸…å• | æ¯é¡¹æ¡ä»¶ä»¥ã€Œæ»¡è¶³ / æ³¨æ„ / ä¸æ»¡è¶³ã€æ ‡è®° |\n\n## ğŸš€ å¿«é€Ÿå¼€å§‹\n\n### æ–¹å¼ä¸€ï¼šGitHub Actionsï¼ˆæ¨èï¼‰\n\n> 5 åˆ†é’Ÿå®Œæˆéƒ¨ç½²ï¼Œé›¶æˆæœ¬ï¼Œæ— éœ€æœåŠ¡å™¨ã€‚\n\n\n#### 1. Fork æœ¬ä»“åº“\n\nç‚¹å‡»å³ä¸Šè§’ `Fork` æŒ‰é’®ï¼ˆé¡ºä¾¿ç‚¹ä¸ª Starâ­ æ”¯æŒä¸€ä¸‹ï¼‰\n\n#### 2. é…ç½® Secrets\n\n`Settings` â†’ `Secrets and variables` â†’ `Actions` â†’ `New repository secret`\n\n**AI æ¨¡å‹é…ç½®ï¼ˆäºŒé€‰ä¸€ï¼‰**\n\n| Secret åç§° | è¯´æ˜ | å¿…å¡« |\n|------------|------|:----:|\n| `GEMINI_API_KEY` | [Google AI Studio](https://aistudio.google.com/) è·å–å…è´¹ Key | âœ…* |\n| `OPENAI_API_KEY` | OpenAI å…¼å®¹ API Keyï¼ˆæ”¯æŒ DeepSeekã€é€šä¹‰åƒé—®ç­‰ï¼‰ | å¯é€‰ |\n| `OPENAI_BASE_URL` | OpenAI å…¼å®¹ API åœ°å€ï¼ˆå¦‚ `https://api.deepseek.com/v1`ï¼‰ | å¯é€‰ |\n| `OPENAI_MODEL` | æ¨¡å‹åç§°ï¼ˆå¦‚ `deepseek-chat`ï¼‰ | å¯é€‰ |\n\n> æ³¨ï¼š`GEMINI_API_KEY` å’Œ `OPENAI_API_KEY` è‡³å°‘é…ç½®ä¸€ä¸ª\n\n<details>\n<summary><b>é€šçŸ¥æ¸ é“é…ç½®</b>ï¼ˆç‚¹å‡»å±•å¼€ï¼Œè‡³å°‘é…ç½®ä¸€ä¸ªï¼‰</summary>\n\n\n| Secret åç§° | è¯´æ˜ | å¿…å¡« |\n|------------|------|:----:|\n| `WECHAT_WEBHOOK_URL` | ä¼ä¸šå¾®ä¿¡ Webhook URL | å¯é€‰ |\n| `FEISHU_WEBHOOK_URL` | é£ä¹¦ Webhook URL | å¯é€‰ |\n| `TELEGRAM_BOT_TOKEN` | Telegram Bot Tokenï¼ˆ@BotFather è·å–ï¼‰ | å¯é€‰ |\n| `TELEGRAM_CHAT_ID` | Telegram Chat ID | å¯é€‰ |\n| `TELEGRAM_MESSAGE_THREAD_ID` | Telegram Topic ID (ç”¨äºå‘é€åˆ°å­è¯é¢˜) | å¯é€‰ |\n| `EMAIL_SENDER` | å‘ä»¶äººé‚®ç®±ï¼ˆå¦‚ `xxx@qq.com`ï¼‰ | å¯é€‰ |\n| `EMAIL_PASSWORD` | é‚®ç®±æˆæƒç ï¼ˆéç™»å½•å¯†ç ï¼‰ | å¯é€‰ |\n| `EMAIL_RECEIVERS` | æ”¶ä»¶äººé‚®ç®±ï¼ˆå¤šä¸ªç”¨é€—å·åˆ†éš”ï¼Œç•™ç©ºåˆ™å‘ç»™è‡ªå·±ï¼‰ | å¯é€‰ |\n| `EMAIL_SENDER_NAME` | é‚®ä»¶å‘ä»¶äººæ˜¾ç¤ºåç§°ï¼ˆé»˜è®¤ï¼šdaily_stock_analysisè‚¡ç¥¨åˆ†æåŠ©æ‰‹ï¼‰ | å¯é€‰ |\n| `PUSHPLUS_TOKEN` | PushPlus Tokenï¼ˆ[è·å–åœ°å€](https://www.pushplus.plus)ï¼Œå›½å†…æ¨é€æœåŠ¡ï¼‰ | å¯é€‰ |\n| `SERVERCHAN3_SENDKEY` | Serveré…±Â³ Sendkeyï¼ˆ[è·å–åœ°å€](https://sc3.ft07.com/)ï¼Œæ‰‹æœºAPPæ¨é€æœåŠ¡ï¼‰ | å¯é€‰ |\n| `CUSTOM_WEBHOOK_URLS` | è‡ªå®šä¹‰ Webhookï¼ˆæ”¯æŒé’‰é’‰ç­‰ï¼Œå¤šä¸ªç”¨é€—å·åˆ†éš”ï¼‰ | å¯é€‰ |\n| `CUSTOM_WEBHOOK_BEARER_TOKEN` | è‡ªå®šä¹‰ Webhook çš„ Bearer Tokenï¼ˆç”¨äºéœ€è¦è®¤è¯çš„ Webhookï¼‰ | å¯é€‰ |\n| `SINGLE_STOCK_NOTIFY` | å•è‚¡æ¨é€æ¨¡å¼ï¼šè®¾ä¸º `true` åˆ™æ¯åˆ†æå®Œä¸€åªè‚¡ç¥¨ç«‹å³æ¨é€ | å¯é€‰ |\n| `REPORT_TYPE` | æŠ¥å‘Šç±»å‹ï¼š`simple`(ç²¾ç®€) æˆ– `full`(å®Œæ•´)ï¼ŒDockerç¯å¢ƒæ¨èè®¾ä¸º `full` | å¯é€‰ |\n| `ANALYSIS_DELAY` | ä¸ªè‚¡åˆ†æå’Œå¤§ç›˜åˆ†æä¹‹é—´çš„å»¶è¿Ÿï¼ˆç§’ï¼‰ï¼Œé¿å…APIé™æµï¼Œå¦‚ `10` | å¯é€‰ |\n\n> è‡³å°‘é…ç½®ä¸€ä¸ªæ¸ é“ï¼Œé…ç½®å¤šä¸ªåˆ™åŒæ—¶æ¨é€ã€‚æ›´å¤šé…ç½®è¯·å‚è€ƒ [å®Œæ•´æŒ‡å—](docs/full-guide.md)\n\n</details>\n\n**å…¶ä»–é…ç½®**\n\n| Secret åç§° | è¯´æ˜ | å¿…å¡« |\n|------------|------|:----:|\n| `STOCK_LIST` | è‡ªé€‰è‚¡ä»£ç ï¼Œå¦‚ `600519,hk00700,AAPL,TSLA` | âœ… |\n| `TAVILY_API_KEYS` | [Tavily](https://tavily.com/) æœç´¢ APIï¼ˆæ–°é—»æœç´¢ï¼‰ | æ¨è |\n| `SERPAPI_API_KEYS` | [SerpAPI](https://serpapi.com/baidu-search-api?utm_source=github_daily_stock_analysis) å…¨æ¸ é“æœç´¢ | å¯é€‰ |\n| `BOCHA_API_KEYS` | [åšæŸ¥æœç´¢](https://open.bocha.cn/) Web Search APIï¼ˆä¸­æ–‡æœç´¢ä¼˜åŒ–ï¼Œæ”¯æŒAIæ‘˜è¦ï¼Œå¤šä¸ªkeyç”¨é€—å·åˆ†éš”ï¼‰ | å¯é€‰ |\n| `BRAVE_API_KEYS` | [Brave Search](https://brave.com/search/api/) APIï¼ˆéšç§ä¼˜å…ˆï¼Œç¾è‚¡ä¼˜åŒ–ï¼Œå¤šä¸ªkeyç”¨é€—å·åˆ†éš”ï¼‰ | å¯é€‰ |\n| `TUSHARE_TOKEN` | [Tushare Pro](https://tushare.pro/weborder/#/login?reg=834638 ) Token | å¯é€‰ |\n| `WECHAT_MSG_TYPE` | ä¼å¾®æ¶ˆæ¯ç±»å‹ï¼Œé»˜è®¤ markdownï¼Œæ”¯æŒé…ç½® text ç±»å‹ï¼Œå‘é€çº¯ markdown æ–‡æœ¬ | å¯é€‰ |\n\n#### 3. å¯ç”¨ Actions\n\n`Actions` æ ‡ç­¾ â†’ `I understand my workflows, go ahead and enable them`\n\n#### 4. æ‰‹åŠ¨æµ‹è¯•\n\n`Actions` â†’ `æ¯æ—¥è‚¡ç¥¨åˆ†æ` â†’ `Run workflow` â†’ `Run workflow`\n\n#### å®Œæˆ\n\né»˜è®¤æ¯ä¸ª**å·¥ä½œæ—¥ 18:00ï¼ˆåŒ—äº¬æ—¶é—´ï¼‰**è‡ªåŠ¨æ‰§è¡Œï¼Œä¹Ÿå¯æ‰‹åŠ¨è§¦å‘\n\n### æ–¹å¼äºŒï¼šæœ¬åœ°è¿è¡Œ / Docker éƒ¨ç½²\n\n```bash\n# å…‹éš†é¡¹ç›®\ngit clone https://github.com/ZhuLinsen/daily_stock_analysis.git && cd daily_stock_analysis\n\n# å®‰è£…ä¾èµ–\npip install -r requirements.txt\n\n# é…ç½®ç¯å¢ƒå˜é‡\ncp .env.example .env && vim .env\n\n# è¿è¡Œåˆ†æ\npython main.py\n```\n\n> Do",
      "default_branch": "main"
    },
    "fetched_at": "2026-02-08T03:50:29.925460"
  },
  {
    "basic_info": {
      "name": "antigravity-awesome-skills",
      "full_name": "sickn33/antigravity-awesome-skills",
      "owner": "sickn33",
      "description": "The Ultimate Collection of 700+ Agentic Skills for Claude Code/Antigravity/Cursor. Battle-tested, high-performance skills for AI agents including official skills from Anthropic and Vercel.",
      "url": "https://github.com/sickn33/antigravity-awesome-skills",
      "clone_url": "https://github.com/sickn33/antigravity-awesome-skills.git",
      "ssh_url": "git@github.com:sickn33/antigravity-awesome-skills.git",
      "homepage": "https://github.com/sickn33/antigravity-awesome-skills",
      "created_at": "2026-01-14T17:48:09Z",
      "updated_at": "2026-02-08T03:14:22Z",
      "pushed_at": "2026-02-07T06:38:11Z"
    },
    "stats": {
      "stars": 7801,
      "forks": 1632,
      "watchers": 7801,
      "open_issues": 2,
      "size": 24207
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 1384503,
        "Shell": 285664,
        "JavaScript": 114735,
        "TypeScript": 52544,
        "HTML": 36813,
        "C#": 29255,
        "CSS": 6844,
        "C++": 2607
      },
      "license": "MIT License",
      "topics": [
        "agentic-skills",
        "ai-agents",
        "antigravity",
        "autonomous-coding",
        "claude-code",
        "mcp",
        "react-patterns",
        "security-auditing"
      ]
    },
    "content": {
      "readme": "# ğŸŒŒ Antigravity Awesome Skills: 713+ Agentic Skills for Claude Code, Gemini CLI, Cursor, Copilot & More\n\n> **The Ultimate Collection of 713+ Universal Agentic Skills for AI Coding Assistants â€” Claude Code, Gemini CLI, Codex CLI, Antigravity IDE, GitHub Copilot, Cursor, OpenCode, AdaL**\n\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n[![Claude Code](https://img.shields.io/badge/Claude%20Code-Anthropic-purple)](https://claude.ai)\n[![Gemini CLI](https://img.shields.io/badge/Gemini%20CLI-Google-blue)](https://github.com/google-gemini/gemini-cli)\n[![Codex CLI](https://img.shields.io/badge/Codex%20CLI-OpenAI-green)](https://github.com/openai/codex)\n[![Cursor](https://img.shields.io/badge/Cursor-AI%20IDE-orange)](https://cursor.sh)\n[![Copilot](https://img.shields.io/badge/GitHub%20Copilot-VSCode-lightblue)](https://github.com/features/copilot)\n[![OpenCode](https://img.shields.io/badge/OpenCode-CLI-gray)](https://github.com/opencode-ai/opencode)\n[![Antigravity](https://img.shields.io/badge/Antigravity-DeepMind-red)](https://github.com/sickn33/antigravity-awesome-skills)\n[![AdaL CLI](https://img.shields.io/badge/AdaL%20CLI-SylphAI-pink)](https://sylph.ai/)\n[![ASK Supported](https://img.shields.io/badge/ASK-Supported-blue)](https://github.com/yeasy/ask)\n[![Buy Me a Book](https://img.shields.io/badge/Buy%20me%20a-book-d13610?logo=buymeacoffee&logoColor=white)](https://buymeacoffee.com/sickn33)\n\nIf this project helps you, you can [support it here](https://buymeacoffee.com/sickn33) or simply â­ the repo.\n\n**Antigravity Awesome Skills** is a curated, battle-tested library of **713 high-performance agentic skills** designed to work seamlessly across all major AI coding assistants:\n\n- ğŸŸ£ **Claude Code** (Anthropic CLI)\n- ğŸ”µ **Gemini CLI** (Google DeepMind)\n- ğŸŸ¢ **Codex CLI** (OpenAI)\n- ğŸ”´ **Antigravity IDE** (Google DeepMind)\n- ğŸ©µ **GitHub Copilot** (VSCode Extension)\n- ğŸŸ  **Cursor** (AI-native IDE)\n- âšª **OpenCode** (Open-source CLI)\n- ğŸŒ¸ **AdaL CLI** (Self-evolving Coding Agent)\n\nThis repository provides essential skills to transform your AI assistant into a **full-stack digital agency**, including official capabilities from **Anthropic**, **OpenAI**, **Google**, **Supabase**, and **Vercel Labs**.\n\n## Table of Contents\n\n- [ğŸš€ New Here? Start Here!](#new-here-start-here)\n- [ğŸ”Œ Compatibility & Invocation](#compatibility--invocation)\n- [ğŸ› ï¸ Installation](#installation)\n- [ğŸ§¯ Troubleshooting](#troubleshooting)\n- [ğŸ Curated Collections (Bundles)](#curated-collections)\n- [ğŸ“¦ Features & Categories](#features--categories)\n- [ğŸ“š Browse 713+ Skills](#browse-713-skills)\n- [ğŸ¤ How to Contribute](#how-to-contribute)\n- [ğŸ¤ Community](#community)\n- [â˜• Support the Project](#support-the-project)\n- [ğŸ‘¥ Contributors & Credits](#credits--sources)\n- [ğŸ‘¥ Repo Contributors](#repo-contributors)\n- [âš–ï¸ License](#license)\n- [ğŸŒŸ Star History](#star-history)\n- [ğŸ·ï¸ GitHub Topics](#github-topics)\n\n---\n\n## New Here? Start Here!\n\n**Welcome to the V4.0.0 Enterprise Edition.** This isn't just a list of scripts; it's a complete operating system for your AI Agent.\n\n### 1. ğŸ£ Context: What is this?\n\n**Antigravity Awesome Skills** (Release 4.0.0) is a massive upgrade to your AI's capabilities.\n\nAI Agents (like Claude Code, Cursor, or Gemini) are smart, but they lack **specific tools**. They don't know your company's \"Deployment Protocol\" or the specific syntax for \"AWS CloudFormation\".\n**Skills** are small markdown files that teach them how to do these specific tasks perfectly, every time.\n\n### 2. âš¡ï¸ Quick Start (1 minute)\n\nInstall once; then use Starter Packs in [docs/BUNDLES.md](docs/BUNDLES.md) to focus on your role.\n\n1. **Install**:\n\n   ```bash\n   # Default path: ~/.agent/skills\n   npx antigravity-awesome-skills\n   ```\n\n2. **Verify**:\n\n   ```bash\n   test -d ~/.agent/skills && echo \"Skills installed in ~/.agent/skills\"\n   ```\n\n3. **Run your first skill**:\n\n   > \"Use **@brainstorming** to plan a SaaS MVP.\"\n\n4. **Pick a bundle**:\n   - **Web Dev?** start with `Web Wizard`.\n   - **Security?** start with `Security Engineer`.\n   - **General use?** start with `Essentials`.\n\n### 3. ğŸ§  How to use\n\nOnce installed, just ask your agent naturally:\n\n> \"Use the **@brainstorming** skill to help me plan a SaaS.\"\n> \"Run **@lint-and-validate** on this file.\"\n\nğŸ‘‰ **[Read the Full Getting Started Guide](docs/GETTING_STARTED.md)**\n\n---\n\n## Compatibility & Invocation\n\nThese skills follow the universal **SKILL.md** format and work with any AI coding assistant that supports agentic skills.\n\n| Tool            | Type | Invocation Example                | Path              |\n| :-------------- | :--- | :-------------------------------- | :---------------- |\n| **Claude Code** | CLI  | `>> /skill-name help me...`       | `.claude/skills/` |\n| **Gemini CLI**  | CLI  | `(User Prompt) Use skill-name...` | `.gemini/skills/` |\n| **Codex CLI**   | CLI  | `(User Prompt) Use skill-name...` | `.codex/skills/`  |\n| **Antigravity** | IDE  | `(Agent Mode) Use s",
      "default_branch": "main"
    },
    "fetched_at": "2026-02-08T03:50:31.118648"
  },
  {
    "basic_info": {
      "name": "Qwen3-TTS",
      "full_name": "QwenLM/Qwen3-TTS",
      "owner": "QwenLM",
      "description": "Qwen3-TTS is an open-source series of TTS models developed by the Qwen team at Alibaba Cloud, supporting stable, expressive, and streaming speech generation, free-form voice design, and vivid voice cloning.",
      "url": "https://github.com/QwenLM/Qwen3-TTS",
      "clone_url": "https://github.com/QwenLM/Qwen3-TTS.git",
      "ssh_url": "git@github.com:QwenLM/Qwen3-TTS.git",
      "homepage": null,
      "created_at": "2026-01-21T06:41:32Z",
      "updated_at": "2026-02-08T03:48:04Z",
      "pushed_at": "2026-02-06T04:11:37Z"
    },
    "stats": {
      "stars": 7129,
      "forks": 882,
      "watchers": 7129,
      "open_issues": 51,
      "size": 6947
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 402779
      },
      "license": "Apache License 2.0",
      "topics": []
    },
    "content": {
      "readme": "# Qwen3-TTS\n\n<br>\n\n<p align=\"center\">\n    <img src=\"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-TTS-Repo/qwen3_tts_logo.png\" width=\"400\"/>\n<p>\n\n<p align=\"center\">\n&nbsp&nbspğŸ¤— <a href=\"https://huggingface.co/collections/Qwen/qwen3-tts\">Hugging Face</a>&nbsp&nbsp | &nbsp&nbspğŸ¤– <a href=\"https://modelscope.cn/collections/Qwen/Qwen3-TTS\">ModelScope</a>&nbsp&nbsp | &nbsp&nbspğŸ“‘ <a href=\"https://qwen.ai/blog?id=qwen3tts-0115\">Blog</a>&nbsp&nbsp | &nbsp&nbspğŸ“‘ <a href=\"https://arxiv.org/abs/2601.15621\">Paper</a>&nbsp&nbsp\n<br>\nğŸ–¥ï¸ <a href=\"https://huggingface.co/spaces/Qwen/Qwen3-TTS\">Hugging Face Demo</a>&nbsp&nbsp | &nbsp&nbsp ğŸ–¥ï¸ <a href=\"https://modelscope.cn/studios/Qwen/Qwen3-TTS\">ModelScope Demo</a>&nbsp&nbsp | &nbsp&nbspğŸ’¬ <a href=\"https://github.com/QwenLM/Qwen/blob/main/assets/wechat.png\">WeChat (å¾®ä¿¡)</a>&nbsp&nbsp | &nbsp&nbspğŸ«¨ <a href=\"https://discord.gg/CV4E9rpNSD\">Discord</a>&nbsp&nbsp | &nbsp&nbspğŸ“‘ <a href=\"https://help.aliyun.com/zh/model-studio/qwen-tts-realtime\">API</a>\n\n</p>\n\nWe release **Qwen3-TTS**, a series of powerful speech generation capabilities developed by Qwen, offering comprehensive support for voice clone, voice design, ultra-high-quality human-like speech generation, and natural language-based voice control. It provides developers and users with the most extensive set of speech generation features available.\n\n\n## News\n* 2026.1.22: ğŸ‰ğŸ‰ğŸ‰ We have released [Qwen3-TTS](https://huggingface.co/collections/Qwen/qwen3-tts) series (0.6B/1.7B) based on Qwen3-TTS-Tokenizer-12Hz. Please check our [blog](https://qwen.ai/blog?id=qwen3tts-0115)!\n\n## Contents <!-- omit in toc -->\n\n- [Overview](#overview)\n  - [Introduction](#introduction)\n  - [Model Architecture](#model-architecture)\n  - [Released Models Description and Download](#released-models-description-and-download)\n- [Quickstart](#quickstart)\n  - [Environment Setup](#environment-setup)\n  - [Python Package Usage](#python-package-usage)\n    - [Custom Voice Generation](#custom-voice-generate)\n    - [Voice Design](#voice-design)\n    - [Voice Clone](#voice-clone)\n    - [Voice Design then Clone](#voice-design-then-clone)\n    - [Tokenizer Encode and Decode](#tokenizer-encode-and-decode)\n  - [Launch Local Web UI Demo](#launch-local-web-ui-demo)\n  - [DashScope API Usage](#dashscope-api-usage)\n- [vLLM Usage](#vllm-usage)\n- [Fine Tuning](#fine-tuning)\n- [Evaluation](#evaluation)\n- [Citation](#citation)\n\n## Overview\n### Introduction\n\n<p align=\"center\">\n    <img src=\"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-TTS-Repo/qwen3_tts_introduction.png\" width=\"90%\"/>\n<p>\n\nQwen3-TTS covers 10 major languages (Chinese, English, Japanese, Korean, German, French, Russian, Portuguese, Spanish, and Italian) as well as multiple dialectal voice profiles to meet global application needs. In addition, the models feature strong contextual understanding, enabling adaptive control of tone, speaking rate, and emotional expression based on instructions and text semantics, and they show markedly improved robustness to noisy input text. Key features:\n\n* **Powerful Speech Representation**: Powered by the self-developed Qwen3-TTS-Tokenizer-12Hz, it achieves efficient acoustic compression and high-dimensional semantic modeling of speech signals. It fully preserves paralinguistic information and acoustic environmental features, enabling high-speed, high-fidelity speech reconstruction through a lightweight non-DiT architecture.\n* **Universal End-to-End Architecture**: Utilizing a discrete multi-codebook LM architecture, it realizes full-information end-to-end speech modeling. This completely bypasses the information bottlenecks and cascading errors inherent in traditional LM+DiT schemes, significantly enhancing the modelâ€™s versatility, generation efficiency, and performance ceiling.\n* **Extreme Low-Latency Streaming Generation**: Based on the innovative Dual-Track hybrid streaming generation architecture, a single model supports both streaming and non-streaming generation. It can output the first audio packet immediately after a single character is input, with end-to-end synthesis latency as low as 97ms, meeting the rigorous demands of real-time interactive scenarios.\n* **Intelligent Text Understanding and Voice Control**: Supports speech generation driven by natural language instructions, allowing for flexible control over multi-dimensional acoustic attributes such as timbre, emotion, and prosody. By deeply integrating text semantic understanding, the model adaptively adjusts tone, rhythm, and emotional expression, achieving lifelike â€œwhat you imagine is what you hearâ€ output.\n\n\n### Model Architecture\n\n<p align=\"center\">\n    <img src=\"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-TTS-Repo/overview.png\" width=\"80%\"/>\n<p>\n\n### Released Models Description and Download\n\nBelow is an introduction and download information for the Qwen3-TTS models that have already been released. Other models mentioned in the technical report will be released in the near future. Please se",
      "default_branch": "main"
    },
    "fetched_at": "2026-02-08T03:50:32.330038"
  },
  {
    "basic_info": {
      "name": "knowledge-work-plugins",
      "full_name": "anthropics/knowledge-work-plugins",
      "owner": "anthropics",
      "description": "Open source repository of plugins primarily intended for knowledge workers to use in Claude Cowork",
      "url": "https://github.com/anthropics/knowledge-work-plugins",
      "clone_url": "https://github.com/anthropics/knowledge-work-plugins.git",
      "ssh_url": "git@github.com:anthropics/knowledge-work-plugins.git",
      "homepage": null,
      "created_at": "2026-01-23T20:11:54Z",
      "updated_at": "2026-02-08T03:48:14Z",
      "pushed_at": "2026-02-05T16:18:20Z"
    },
    "stats": {
      "stars": 6342,
      "forks": 519,
      "watchers": 6342,
      "open_issues": 30,
      "size": 704
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 310600,
        "HTML": 97651
      },
      "license": "Apache License 2.0",
      "topics": []
    },
    "content": {
      "readme": "# Knowledge Work Plugins\n\nPlugins that turn Claude into a specialist for your role, team, and company. Built for [Claude Cowork](https://claude.com/product/cowork), also compatible with [Claude Code](https://claude.com/product/claude-code).\n\n## Why Plugins\n\nCowork lets you set the goal and Claude delivers finished, professional work. Plugins let you go further: tell Claude how you like work done, which tools and data to pull from, how to handle critical workflows, and what slash commands to expose â€” so your team gets better and more consistent outcomes.\n\nEach plugin bundles the skills, connectors, slash commands, and sub-agents for a specific job function. Out of the box, they give Claude a strong starting point for helping anyone in that role. The real power comes when you customize them for your company â€” your tools, your terminology, your processes â€” so Claude works like it was built for your team.\n\n## Plugin Marketplace\n\nWe're open-sourcing 11 plugins built and inspired by our own work:\n\n| Plugin | How it helps | Connectors |\n|--------|-------------|------------|\n| **[productivity](./productivity)** | Manage tasks, calendars, daily workflows, and personal context so you spend less time repeating yourself. | Slack, Notion, Asana, Linear, Jira, Monday, ClickUp, Microsoft 365 |\n| **[sales](./sales)** | Research prospects, prep for calls, review your pipeline, draft outreach, and build competitive battlecards. | Slack, HubSpot, Close, Clay, ZoomInfo, Notion, Jira, Fireflies, Microsoft 365 |\n| **[customer-support](./customer-support)** | Triage tickets, draft responses, package escalations, research customer context, and turn resolved issues into knowledge base articles. | Slack, Intercom, HubSpot, Guru, Jira, Notion, Microsoft 365 |\n| **[product-management](./product-management)** | Write specs, plan roadmaps, synthesize user research, keep stakeholders updated, and track the competitive landscape. | Slack, Linear, Asana, Monday, ClickUp, Jira, Notion, Figma, Amplitude, Pendo, Intercom, Fireflies |\n| **[marketing](./marketing)** | Draft content, plan campaigns, enforce brand voice, brief on competitors, and report on performance across channels. | Slack, Canva, Figma, HubSpot, Amplitude, Notion, Ahrefs, SimilarWeb, Klaviyo |\n| **[legal](./legal)** | Review contracts, triage NDAs, navigate compliance, assess risk, prep for meetings, and draft templated responses. | Slack, Box, Egnyte, Jira, Microsoft 365 |\n| **[finance](./finance)** | Prep journal entries, reconcile accounts, generate financial statements, analyze variances, manage close, and support audits. | Snowflake, Databricks, BigQuery, Slack, Microsoft 365 |\n| **[data](./data)** | Query, visualize, and interpret datasets â€” write SQL, run statistical analysis, build dashboards, and validate your work before sharing. | Snowflake, Databricks, BigQuery, Hex, Amplitude, Jira |\n| **[enterprise-search](./enterprise-search)** | Find anything across email, chat, docs, and wikis â€” one query across all your company's tools. | Slack, Notion, Guru, Jira, Asana, Microsoft 365 |\n| **[bio-research](./bio-research)** | Connect to preclinical research tools and databases (literature search, genomics analysis, target prioritization) to accelerate early-stage life sciences R&D. | PubMed, BioRender, bioRxiv, ClinicalTrials.gov, ChEMBL, Synapse, Wiley, Owkin, Open Targets, Benchling |\n| **[cowork-plugin-management](./cowork-plugin-management)** | Create new plugins or customize existing ones for your organization's specific tools and workflows. | â€” |\n\nInstall these directly from Cowork, browse the full collection here on GitHub, or build your own.\n\n## Getting Started\n\n### Cowork\n\nInstall plugins from [claude.com/plugins](https://claude.com/plugins/).\n\n### Claude Code\n\n```bash\n# Add the marketplace first\nclaude plugin marketplace add anthropics/knowledge-work-plugins\n\n# Then install a specific plugin\nclaude plugin install sales@knowledge-work-plugins\n```\n\nOnce installed, plugins activate automatically. Skills fire when relevant, and slash commands are available in your session (e.g., `/sales:call-prep`, `/data:write-query`).\n\n## How Plugins Work\n\nEvery plugin follows the same structure:\n\n```\nplugin-name/\nâ”œâ”€â”€ .claude-plugin/plugin.json   # Manifest\nâ”œâ”€â”€ .mcp.json                    # Tool connections\nâ”œâ”€â”€ commands/                    # Slash commands you invoke explicitly\nâ””â”€â”€ skills/                      # Domain knowledge Claude draws on automatically\n```\n\n- **Skills** encode the domain expertise, best practices, and step-by-step workflows Claude needs to give you useful help. Claude draws on them automatically when relevant.\n- **Commands** are explicit actions you trigger (e.g., `/finance:reconciliation`, `/product-management:write-spec`).\n- **Connectors** wire Claude to the external tools your role depends on â€” CRMs, project trackers, data warehouses, design tools, and more â€” via [MCP servers](https://modelcontextprotocol.io/).\n\nEvery component is file-based â€” markdown and ",
      "default_branch": "main"
    },
    "fetched_at": "2026-02-08T03:50:33.550707"
  },
  {
    "basic_info": {
      "name": "hive",
      "full_name": "adenhq/hive",
      "owner": "adenhq",
      "description": "Outcome driven agent development framework that evolves",
      "url": "https://github.com/adenhq/hive",
      "clone_url": "https://github.com/adenhq/hive.git",
      "ssh_url": "git@github.com:adenhq/hive.git",
      "homepage": "",
      "created_at": "2026-01-12T00:04:22Z",
      "updated_at": "2026-02-08T03:42:15Z",
      "pushed_at": "2026-02-07T13:57:14Z"
    },
    "stats": {
      "stars": 6178,
      "forks": 3631,
      "watchers": 6178,
      "open_issues": 1236,
      "size": 4226
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 2770244,
        "Shell": 31038,
        "TypeScript": 17311,
        "Dockerfile": 1089,
        "Makefile": 810
      },
      "license": "Apache License 2.0",
      "topics": [
        "agent",
        "agent-framework",
        "agent-skills",
        "ai-evaluation",
        "anthropic",
        "automation",
        "autonomous-agents",
        "awesome",
        "claude",
        "claude-code",
        "human-in-the-loop",
        "observability-ai",
        "openai",
        "python",
        "self-hosted",
        "self-improving",
        "self-improving-agent",
        "self-improving-ai"
      ]
    },
    "content": {
      "readme": "<p align=\"center\">\n  <img width=\"100%\" alt=\"Hive Banner\" src=\"https://github.com/user-attachments/assets/a027429b-5d3c-4d34-88e4-0feaeaabbab3\" />\n</p>\n\n<p align=\"center\">\n  <a href=\"README.md\">English</a> |\n  <a href=\"docs/i18n/zh-CN.md\">ç®€ä½“ä¸­æ–‡</a> |\n  <a href=\"docs/i18n/es.md\">EspaÃ±ol</a> |\n  <a href=\"docs/i18n/hi.md\">à¤¹à¤¿à¤¨à¥à¤¦à¥€</a> |\n  <a href=\"docs/i18n/pt.md\">PortuguÃªs</a> |\n  <a href=\"docs/i18n/ja.md\">æ—¥æœ¬èª</a> |\n  <a href=\"docs/i18n/ru.md\">Ğ ÑƒÑÑĞºĞ¸Ğ¹</a> |\n  <a href=\"docs/i18n/ko.md\">í•œêµ­ì–´</a>\n</p>\n\n<p align=\"center\">\n  <a href=\"https://github.com/adenhq/hive/blob/main/LICENSE\"><img src=\"https://img.shields.io/badge/License-Apache%202.0-blue.svg\" alt=\"Apache 2.0 License\" /></a>\n  <a href=\"https://www.ycombinator.com/companies/aden\"><img src=\"https://img.shields.io/badge/Y%20Combinator-Aden-orange\" alt=\"Y Combinator\" /></a>\n  <a href=\"https://discord.com/invite/MXE49hrKDk\"><img src=\"https://img.shields.io/discord/1172610340073242735?logo=discord&labelColor=%235462eb&logoColor=%23f5f5f5&color=%235462eb\" alt=\"Discord\" /></a>\n  <a href=\"https://x.com/aden_hq\"><img src=\"https://img.shields.io/twitter/follow/teamaden?logo=X&color=%23f5f5f5\" alt=\"Twitter Follow\" /></a>\n  <a href=\"https://www.linkedin.com/company/teamaden/\"><img src=\"https://custom-icon-badges.demolab.com/badge/LinkedIn-0A66C2?logo=linkedin-white&logoColor=fff\" alt=\"LinkedIn\" /></a>\n  <img src=\"https://img.shields.io/badge/MCP-102_Tools-00ADD8?style=flat-square\" alt=\"MCP\" />\n</p>\n\n\n<p align=\"center\">\n  <img src=\"https://img.shields.io/badge/AI_Agents-Self--Improving-brightgreen?style=flat-square\" alt=\"AI Agents\" />\n  <img src=\"https://img.shields.io/badge/Multi--Agent-Systems-blue?style=flat-square\" alt=\"Multi-Agent\" />\n  <img src=\"https://img.shields.io/badge/Headless-Development-purple?style=flat-square\" alt=\"Headless\" />\n  <img src=\"https://img.shields.io/badge/Human--in--the--Loop-orange?style=flat-square\" alt=\"HITL\" />\n  <img src=\"https://img.shields.io/badge/Production--Ready-red?style=flat-square\" alt=\"Production\" />\n</p>\n<p align=\"center\">\n  <img src=\"https://img.shields.io/badge/OpenAI-supported-412991?style=flat-square&logo=openai\" alt=\"OpenAI\" />\n  <img src=\"https://img.shields.io/badge/Anthropic-supported-d4a574?style=flat-square\" alt=\"Anthropic\" />\n  <img src=\"https://img.shields.io/badge/Google_Gemini-supported-4285F4?style=flat-square&logo=google\" alt=\"Gemini\" />\n</p>\n\n## Overview\n\nBuild autonomous, reliable, self-improving AI agents without hardcoding workflows. Define your goal through conversation with a coding agent, and the framework generates a node graph with dynamically created connection code. When things break, the framework captures failure data, evolves the agent through the coding agent, and redeploys. Built-in human-in-the-loop nodes, credential management, and real-time monitoring give you control without sacrificing adaptability.\n\nVisit [adenhq.com](https://adenhq.com) for complete documentation, examples, and guides.\n\nhttps://github.com/user-attachments/assets/846c0cc7-ffd6-47fa-b4b7-495494857a55\n\n## Who Is Hive For?\n\nHive is designed for developers and teams who want to build **production-grade AI agents** without manually wiring complex workflows.\n\nHive is a good fit if you:\n\n- Want AI agents that **execute real business processes**, not demos\n- Prefer **goal-driven development** over hardcoded workflows\n- Need **self-healing and adaptive agents** that improve over time\n- Require **human-in-the-loop control**, observability, and cost limits\n- Plan to run agents in **production environments**\n\nHive may not be the best fit if youâ€™re only experimenting with simple agent chains or one-off scripts.\n\n## When Should You Use Hive?\n\nUse Hive when you need:\n\n- Long-running, autonomous agents\n- Strong guardrails, process, and controls\n- Continuous improvement based on failures\n- Multi-agent coordination\n- A framework that evolves with your goals\n\n## Quick Links\n\n- **[Documentation](https://docs.adenhq.com/)** - Complete guides and API reference\n- **[Self-Hosting Guide](https://docs.adenhq.com/getting-started/quickstart)** - Deploy Hive on your infrastructure\n- **[Changelog](https://github.com/adenhq/hive/releases)** - Latest updates and releases\n- **[Roadmap](docs/roadmap.md)** - Upcoming features and plans\n- **[Report Issues](https://github.com/adenhq/hive/issues)** - Bug reports and feature requests\n\n## Quick Start\n\n### Prerequisites\n\n- Python 3.11+ for agent development\n- Claude Code or Cursor for utilizing agent skills\n\n> **Note for Windows Users:** It is strongly recommended to use **WSL (Windows Subsystem for Linux)** or **Git Bash** to run this framework. Some core automation scripts may not execute correctly in standard Command Prompt or PowerShell.\n\n### Installation\n\n```bash\n# Clone the repository\ngit clone https://github.com/adenhq/hive.git\ncd hive\n\n# Run quickstart setup\n./quickstart.sh\n```\n\nThis sets up:\n\n- **framework** - Core agent runtime and graph executor (in `core/.venv`)\n- **aden_tools** - MCP tools for agent capa",
      "default_branch": "main"
    },
    "fetched_at": "2026-02-08T03:50:34.743743"
  },
  {
    "basic_info": {
      "name": "Engram",
      "full_name": "deepseek-ai/Engram",
      "owner": "deepseek-ai",
      "description": "Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models",
      "url": "https://github.com/deepseek-ai/Engram",
      "clone_url": "https://github.com/deepseek-ai/Engram.git",
      "ssh_url": "git@github.com:deepseek-ai/Engram.git",
      "homepage": "",
      "created_at": "2026-01-12T05:26:50Z",
      "updated_at": "2026-02-07T21:49:27Z",
      "pushed_at": "2026-01-14T01:13:02Z"
    },
    "stats": {
      "stars": 3601,
      "forks": 244,
      "watchers": 3601,
      "open_issues": 12,
      "size": 2296
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 15017
      },
      "license": "Apache License 2.0",
      "topics": []
    },
    "content": {
      "readme": "<!-- markdownlint-disable first-line-h1 -->\n<!-- markdownlint-disable html -->\n<!-- markdownlint-disable no-duplicate-header -->\n\n<div align=\"center\">\n  <img src=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/logo.svg?raw=true\" width=\"60%\" alt=\"DeepSeek-V3\" />\n</div>\n<hr>\n<div align=\"center\" style=\"line-height: 1;\">\n  <a href=\"https://www.deepseek.com/\"><img alt=\"Homepage\"\n    src=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/badge.svg?raw=true\"/></a>\n  <a href=\"https://chat.deepseek.com/\"><img alt=\"Chat\"\n    src=\"https://img.shields.io/badge/ğŸ¤–%20Chat-DeepSeek%20V3-536af5?color=536af5&logoColor=white\"/></a>\n  <a href=\"https://huggingface.co/deepseek-ai\"><img alt=\"Hugging Face\"\n    src=\"https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-DeepSeek%20AI-ffc107?color=ffc107&logoColor=white\"/></a>\n  <br>\n  <a href=\"https://discord.gg/Tc7c45Zzu5\"><img alt=\"Discord\"\n    src=\"https://img.shields.io/badge/Discord-DeepSeek%20AI-7289da?logo=discord&logoColor=white&color=7289da\"/></a>\n  <a href=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/qr.jpeg?raw=true\"><img alt=\"Wechat\"\n    src=\"https://img.shields.io/badge/WeChat-DeepSeek%20AI-brightgreen?logo=wechat&logoColor=white\"/></a>\n  <a href=\"https://twitter.com/deepseek_ai\"><img alt=\"Twitter Follow\"\n    src=\"https://img.shields.io/badge/Twitter-deepseek_ai-white?logo=x&logoColor=white\"/></a>\n  <br>\n  <a href=\"LICENSE\" style=\"margin: 2px;\">\n    <img alt=\"License\" src=\"https://img.shields.io/badge/License-Apache 2.0-f5de53?&color=f5de53\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <br>\n</div>\n\n## 1. Introduction\n\nThis repository contains the official implementation for the paper: **[Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models](Engram_paper.pdf)**.\n\n> **Abstract:** While Mixture-of-Experts (MoE) scales capacity via conditional computation, Transformers lack a native primitive for knowledge lookup. To address this, we explore **conditional memory** as a complementary sparsity axis, instantiated via **Engram**, a module that modernizes classic $N$-gram embeddings for $\\mathcal{O}(1)$ lookup.\n\n**Key Contributions:**\n- **Sparsity Allocation:** We formulate the trade-off between neural computation (MoE) and static memory (Engram), identifying a U-shaped scaling law that guides optimal capacity allocation.\n- **Empirical Verification:** Under strict iso-parameter and iso-FLOPs constraints, the Engram-27B model demonstrates consistent improvements over MoE baselines across knowledge, reasoning, code and math domains.\n- **Mechanistic Analysis:** Our analysis suggests that Engram relieves early layers from static pattern reconstruction, potentially preserving effective depth for complex reasoning.\n- **System Efficiency:** The module employs deterministic addressing, enabling the offloading of massive embedding tables to host memory with minimal inference overhead.\n\n\n## 2. Architecture\n\nThe Engram module augments the backbone by retrieving static $N$-gram memory and fusing it with dynamic hidden states. The architecture is shown below ([drawio provided](drawio/Engram.drawio)):\n\n<p align=\"center\">\n  <img width=\"75%\" src=\"figures/arch.png\" alt=\"Engram Architecture\">\n</p>\n\n## 3. Evaluation\n\n### Scaling Law\n<p align=\"center\">\n  <img width=\"90%\" src=\"figures/scaling_law.png\" alt=\"Scaling Law\">\n</p>\n\n---\n\n### Large Scale Pre-training\n<p align=\"center\">\n  <img width=\"80%\" src=\"figures/27b_exp_results.png\" alt=\"Pre-training Results\">\n</p>\n\n---\n\n### Long-context Training\n<p align=\"center\">\n  <img width=\"80%\" src=\"figures/long_context_results.png\" alt=\"Long Context Results\">\n</p>\n\n\n## 4. Case Study of Engram\n<p align=\"center\">\n  <img width=\"80%\" src=\"figures/case.png\" alt=\"Long Context Results\">\n</p>\n\n## 5. Quick Start\n\nWe recommend using Python 3.8+ and PyTorch.\n```bash\npip install torch numpy transformers sympy\n```\nWe provide a standalone implementation to demonstrate the core logic of the Engram module:\n```bash\npython engram_demo_v1.py\n```\n\n> âš ï¸ **Note:** The provided code is a demonstration version intended to illustrate the data flow. It mocks standard components (like Attention/MoE/mHC) to focus on the Engram module. \n\n\n## 6. License\nThe use of Engram models is subject to [the Model License](LICENSE).\n\n## 7. Contact\n\nIf you have any questions, please raise an issue or contact us at [service@deepseek.com](mailto:service@deepseek.com).",
      "default_branch": "main"
    },
    "fetched_at": "2026-02-08T03:50:35.960464"
  },
  {
    "basic_info": {
      "name": "original_performance_takehome",
      "full_name": "anthropics/original_performance_takehome",
      "owner": "anthropics",
      "description": "Anthropic's original performance take-home, now open for you to try!",
      "url": "https://github.com/anthropics/original_performance_takehome",
      "clone_url": "https://github.com/anthropics/original_performance_takehome.git",
      "ssh_url": "git@github.com:anthropics/original_performance_takehome.git",
      "homepage": null,
      "created_at": "2026-01-19T19:16:04Z",
      "updated_at": "2026-02-08T00:41:50Z",
      "pushed_at": "2026-01-22T01:11:08Z"
    },
    "stats": {
      "stars": 3372,
      "forks": 737,
      "watchers": 3372,
      "open_issues": 12,
      "size": 8
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 57361,
        "HTML": 4758
      },
      "license": null,
      "topics": []
    },
    "content": {
      "readme": "# Anthropic's Original Performance Take-Home\n\nThis repo contains a version of Anthropic's original performance take-home, before Claude Opus 4.5 started doing better than humans given only 2 hours.\n\nThe original take-home was a 4-hour one that starts close to the contents of this repo, after Claude Opus 4 beat most humans at that, it was updated to a 2-hour one which started with code which achieved 18532 cycles (7.97x faster than this repo starts you). This repo is based on the newer take-home which has a few more instructions and comes with better debugging tools, but has the starter code reverted to the slowest baseline. After Claude Opus 4.5 we started using a different base for our time-limited take-homes.\n\nNow you can try to beat Claude Opus 4.5 given unlimited time!\n\n## Performance benchmarks \n\nMeasured in clock cycles from the simulated machine. All of these numbers are for models doing the 2 hour version which started at 18532 cycles:\n\n- **2164 cycles**: Claude Opus 4 after many hours in the test-time compute harness\n- **1790 cycles**: Claude Opus 4.5 in a casual Claude Code session, approximately matching the best human performance in 2 hours\n- **1579 cycles**: Claude Opus 4.5 after 2 hours in our test-time compute harness\n- **1548 cycles**: Claude Sonnet 4.5 after many more than 2 hours of test-time compute\n- **1487 cycles**: Claude Opus 4.5 after 11.5 hours in the harness\n- **1363 cycles**: Claude Opus 4.5 in an improved test time compute harness\n- **??? cycles**: Best human performance ever is substantially better than the above, but we won't say how much.\n\nWhile it's no longer a good time-limited test, you can still use this test to get us excited about hiring you! If you optimize below 1487 cycles, beating Claude Opus 4.5's best performance at launch, email us at performance-recruiting@anthropic.com with your code (and ideally a resume) so we can be appropriately impressed, especially if you get near the best solution we've seen. New model releases may change what threshold impresses us though, and no guarantees that we keep this readme updated with the latest on that.\n\nRun `python tests/submission_tests.py` to see which thresholds you pass.\n\n## Warning: LLMs can cheat\n\nNone of the solutions we received on the first day post-release below 1300 cycles were valid solutions. In each case, a language model modified the tests to make the problem easier.\n\nIf you use an AI agent, we recommend instructing it not to change the `tests/` folder and to use `tests/submission_tests.py` for verification.\n\nPlease run the following commands to validate your submission, and mention that you did so when submitting:\n```\n# This should be empty, the tests folder must be unchanged\ngit diff origin/main tests/\n# You should pass some of these tests and use the cycle count this prints\npython tests/submission_tests.py\n```\n\nAn example of this kind of hack is a model noticing that `problem.py` has multicore support, implementing multicore as an optimization, noticing there's no speedup and \"debugging\" that `N_CORES = 1` and \"fixing\" the core count so they get a speedup. Multicore is disabled intentionally in this version.\n",
      "default_branch": "main"
    },
    "fetched_at": "2026-02-08T03:50:37.164146"
  },
  {
    "basic_info": {
      "name": "heartlib",
      "full_name": "HeartMuLa/heartlib",
      "owner": "HeartMuLa",
      "description": "HeartMuLa Official Repo: The Most Powerful Open-Source Music Generation Model of 2026",
      "url": "https://github.com/HeartMuLa/heartlib",
      "clone_url": "https://github.com/HeartMuLa/heartlib.git",
      "ssh_url": "git@github.com:HeartMuLa/heartlib.git",
      "homepage": "",
      "created_at": "2026-01-15T07:53:15Z",
      "updated_at": "2026-02-08T03:39:12Z",
      "pushed_at": "2026-02-03T06:48:57Z"
    },
    "stats": {
      "stars": 3188,
      "forks": 278,
      "watchers": 3188,
      "open_issues": 70,
      "size": 888
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 77582
      },
      "license": "Apache License 2.0",
      "topics": []
    },
    "content": {
      "readme": "<p align=\"center\">\n    <picture>\n        <source srcset=\"./assets/logo.png\" media=\"(prefers-color-scheme: dark)\">\n        <img src=\"./assets/logo.png\" width=\"30%\">\n    </picture>\n    \n</p>\n\n<p align=\"center\">\n    <a href=\"https://heartmula.github.io/\">Demo ğŸ¶</a> &nbsp;|&nbsp; ğŸ“‘ <a href=\"https://arxiv.org/pdf/2601.10547\">Paper</a>\n    <br>\n    <a href=\"https://huggingface.co/HeartMuLa/HeartMuLa-oss-3B\">HeartMuLa-oss-3B ğŸ¤—</a> &nbsp;|&nbsp; <a href=\"https://modelscope.cn/models/HeartMuLa/HeartMuLa-oss-3B\">HeartMuLa-oss-3B <picture>\n        <source srcset=\"./assets/badge.svg\" media=\"(prefers-color-scheme: dark)\">\n        <img src=\"./assets/badge.svg\" width=\"20px\">\n    </picture></a>\n    <br>\n    <a href=\"https://huggingface.co/HeartMuLa/HeartMuLa-RL-oss-3B-20260123\"> HeartMuLa-RL-oss-3B-20260123 ğŸ¤—</a> &nbsp;|&nbsp; <a href=\"https://modelscope.cn/models/HeartMuLa/HeartMuLa-RL-oss-3B-20260123\">HeartMuLa-RL-oss-3B-20260123 <picture>\n        <source srcset=\"./assets/badge.svg\" media=\"(prefers-color-scheme: dark)\">\n        <img src=\"./assets/badge.svg\" width=\"20px\">\n    </picture></a>\n    \n</p>\n\n---\n# HeartMuLa: A Family of Open Sourced Music Foundation Models\n\nHeartMuLa is a family of open sourced music foundation models including: \n1. HeartMuLa: a music language model that generates music conditioned on lyrics and tags with multilingual support including but not limited to English, Chinese, Japanese, Korean and Spanish.\n2. HeartCodec: a 12.5 hz music codec with high reconstruction fidelity;\n3. HeartTranscriptor: a whisper-based model specifically tuned for lyrics transcription; Check [this page](./examples/README.md) for its usage.\n4. HeartCLAP: an audioâ€“text alignment model that establishes a unified embedding space for music descriptions and cross-modal retrieval.\n---\n\n\nBelow shows the experiment result of our oss-3B version compared with other baselines.\n<p align=\"center\">\n    <picture>\n        <source srcset=\"./assets/exp.png\" media=\"(prefers-color-scheme: dark)\">\n        <img src=\"./assets/exp.png\" width=\"90%\">\n    </picture>\n    \n</p>\n\n---\n\n## ğŸ”¥ Highlight\n\nOur latest internal version of HeartMuLa-7B achieves **comparable performance with Suno** in terms of musicality, fidelity and controllability. If you are interested, welcome to reach us out via heartmula.ai@gmail.com\n\n## ğŸ“° News\nJoin on Discord! [<img alt=\"join discord\" src=\"https://img.shields.io/discord/842440537755353128?color=%237289da&logo=discord\"/>](https://discord.gg/BKXF5FgH)\n\n- âš–ï¸ **03 Feb. 2026**\n  We have released our [HeartMuLa-Benchmark](https://modelscope.cn/datasets/HeartMuLa/HeartMuLa-Benchmark) as introduced in our [paper](https://arxiv.org/pdf/2601.10547). This benchmark comprises heterogeneous AI-generated lyrics and tags across diverse languages and genres, providing a rigorous and fair evaluation framework.\n  \n- ğŸš€ **23 Jan. 2026**\n\n    By leveraging Reinforcement Learning, we have continuously refined our model and are proud to officially release **HeartMuLa-RL-oss-3B-20260123**. This version is designed to achieve more precise control over styles and tags. Simultaneously, we are launching **HeartCodec-oss-20260123**, which optimizes audio decoding quality.\n\n- ğŸ«¶ **20 Jan. 2026** \n    \n    [Benji](https://github.com/benjiyaya) has created a wonderful [ComfyUI custom node](https://github.com/benjiyaya/HeartMuLa_ComfyUI) for HeartMuLa. Thanks Benji!\n- âš–ï¸ **20 Jan. 2026** \n\n    License update: We update the license of this repo and all related model weights to **Apache 2.0**.\n- ğŸš€ **14 Jan. 2026**  \n    The official release of **HeartTranscriptor-oss** and the first **HeartMuLa-oss-3B** version along with our **HeartCodec-oss**.\n\n---\n## ğŸ§­ TODOs\n\n- â³ Release scripts for inference acceleration and streaming inference. The current inference speed is around RTF $\\approx 1.0$.\n- â³ Support **reference audio conditioning**, **fine-grained controllable music generation**, **hot song generation**.\n- â³ Release the **HeartMuLa-oss-7B** version.\n- âœ… Release inference code and pretrained checkpoints of  \n  **HeartCodec-oss, HeartMuLa-oss-3B, and HeartTranscriptor-oss**.\n\n---\n\n## ğŸ› ï¸ Local Deployment\n\n### âš™ï¸ Environment Setup\n\nWe recommend using `python=3.10` for local deployment.\n\nClone this repo and install locally.\n\n```\ngit clone https://github.com/HeartMuLa/heartlib.git\ncd heartlib\npip install -e .\n```\n\nDownload our pretrained checkpoints from huggingface or modelscope using the following command:\n\n```\n# if you are using huggingface\nhf download --local-dir './ckpt' 'HeartMuLa/HeartMuLaGen'\n\n## To use version released on 20260123 (recommended)\nhf download --local-dir './ckpt/HeartMuLa-oss-3B' 'HeartMuLa/HeartMuLa-RL-oss-3B-20260123'\nhf download --local-dir './ckpt/HeartCodec-oss' HeartMuLa/HeartCodec-oss-20260123\n\n## To use oss-3B version\nhf download --local-dir './ckpt/HeartMuLa-oss-3B' 'HeartMuLa/HeartMuLa-oss-3B'\nhf download --local-dir './ckpt/HeartCodec-oss' 'HeartMuLa/HeartCodec-oss'\n\n# if you are using modelscope\nmodelscope download --model 'He",
      "default_branch": "main"
    },
    "fetched_at": "2026-02-08T03:50:38.374457"
  },
  {
    "basic_info": {
      "name": "lingbot-world",
      "full_name": "Robbyant/lingbot-world",
      "owner": "Robbyant",
      "description": "Advancing Open-source World Models",
      "url": "https://github.com/Robbyant/lingbot-world",
      "clone_url": "https://github.com/Robbyant/lingbot-world.git",
      "ssh_url": "git@github.com:Robbyant/lingbot-world.git",
      "homepage": "https://technology.robbyant.com/lingbot-world",
      "created_at": "2026-01-28T04:52:50Z",
      "updated_at": "2026-02-08T03:18:38Z",
      "pushed_at": "2026-02-02T11:46:45Z"
    },
    "stats": {
      "stars": 2648,
      "forks": 215,
      "watchers": 2648,
      "open_issues": 15,
      "size": 47043
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 594764
      },
      "license": "Apache License 2.0",
      "topics": [
        "aigc",
        "image-to-video",
        "lingbot-world",
        "video-generation",
        "world-models"
      ]
    },
    "content": {
      "readme": "<div align=\"center\">\n  <img src=\"assets/teaser.png\">\n\n<h1>LingBot-World: Advancing Open-source World Models</h1>\n\nRobbyant Team\n\n</div>\n\n\n<div align=\"center\">\n\n[![Page](https://img.shields.io/badge/%F0%9F%8C%90%20Project%20Page-Demo-00bfff)](https://technology.robbyant.com/lingbot-world)\n[![Tech Report](https://img.shields.io/static/v1?label=Paper&message=PDF&color=red&logo=arxiv)](https://arxiv.org/abs/2601.20540)\n[![Model](https://img.shields.io/static/v1?label=%F0%9F%A4%97%20Model&message=HuggingFace&color=yellow)](https://huggingface.co/robbyant/lingbot-world-base-cam)\n[![Model](https://img.shields.io/static/v1?label=%F0%9F%A4%96%20Model&message=ModelScope&color=purple)](https://www.modelscope.cn/models/Robbyant/lingbot-world-base-cam)\n[![License](https://img.shields.io/badge/License-Apache--2.0-green)](LICENSE.txt)\n\n\n</div>\n\n-----\n\nWe are excited to introduce **LingBot-World**, an open-sourced world simulator stemming from video generation. Positioned\nas a top-tier world model, LingBot-World offers the following features. \n- **High-Fidelity & Diverse Environments**: It maintains high fidelity and robust dynamics in a broad spectrum of environments, including realism, scientific contexts, cartoon styles, and beyond. \n- **Long-Term Memory & Consistency**: It enables a minute-level horizon while preserving contextual consistency over time, which is also known as long-term memory. \n- **Real-Time Interactivity & Open Access**: It supports real-time interactivity, achieving a latency of under 1 second when producing 16 frames per second. We provide public access to the code and model in an effort to narrow the divide between open-source and closed-source technologies. We believe our release will empower the community with practical applications across areas like content creation, gaming, and robot learning.\n\n## ğŸ¬ Video Demo\n<div align=\"center\">\n  <video src=\"https://github.com/user-attachments/assets/ea4a7a8d-5d9e-4ccf-96e7-02f93797116e\" width=\"100%\" poster=\"\"> </video>\n</div>\n\n## ğŸ”¥ News\n- Jan 29, 2026: ğŸ‰ We release the technical report, code, and models for LingBot-World.\n\n<!-- ## ğŸ”– Introduction of LingBot-World\nWe present **LingBot-World**, an **open-sourced** world simulator stemming from video generation. Positioned\nas a top-tier world model, LingBot-World offers the following features. \n- It maintains high fidelity and robust dynamics in a broad spectrum of environments, including realism, scientific contexts, cartoon styles, and beyond. \n- It enables a minute-level horizon while preserving contextual consistency over time, which is also known as **long-term memory**. \n- It supports real-time interactivity, achieving a latency of under 1 second when producing 16 frames per second. We provide public access to the code and model in an effort to narrow the divide between open-source and closed-source technologies. We believe our release will empower the community with practical applications across areas like content creation, gaming, and robot learning. -->\n\n## âš™ï¸ Quick Start\nThis codebase is built upon [Wan2.2](https://github.com/Wan-Video/Wan2.2). Please refer to their documentation for installation instructions.\n### Installation\nClone the repo:\n```sh\ngit clone https://github.com/robbyant/lingbot-world.git\ncd lingbot-world\n```\nInstall dependencies:\n```sh\n# Ensure torch >= 2.4.0\npip install -r requirements.txt\n```\nInstall [`flash_attn`](https://github.com/Dao-AILab/flash-attention):\n```sh\npip install flash-attn --no-build-isolation\n```\n### Model Download\n\n| Model | Control Signals | Resolution | Download Links |\n| :---  | :--- | :--- | :--- |\n| **LingBot-World-Base (Cam)** | Camera Poses | 480P & 720P | ğŸ¤— [HuggingFace](https://huggingface.co/robbyant/lingbot-world-base-cam) ğŸ¤– [ModelScope](https://www.modelscope.cn/models/Robbyant/lingbot-world-base-cam) |\n| **LingBot-World-Base (Act)** | Actions | - | *To be released* |\n| **LingBot-World-Fast**       |    -    | - | *To be released* |\n\nDownload models using huggingface-cli:\n```sh\npip install \"huggingface_hub[cli]\"\nhuggingface-cli download robbyant/lingbot-world-base-cam --local-dir ./lingbot-world-base-cam\n```\nDownload models using modelscope-cli:\n ```sh\npip install modelscope\nmodelscope download robbyant/lingbot-world-base-cam --local_dir ./lingbot-world-base-cam\n```\n### Inference\nBefore running inference, you need to prepare:\n- Input image\n- Text prompt\n- Control signals (optional, can be generated from a video using [ViPE](https://github.com/nv-tlabs/vipe))\n  - `intrinsics.npy`: Shape `[num_frames, 4]`, where the 4 values represent `[fx, fy, cx, cy]`\n  - `poses.npy`: Shape `[num_frames, 4, 4]`, where each `[4, 4]` represents a transformation matrix in OpenCV coordinates\n\n- 480P:\n``` sh\ntorchrun --nproc_per_node=8 generate.py --task i2v-A14B --size 480*832 --ckpt_dir lingbot-world-base-cam --image examples/00/image.jpg --action_path examples/00 --dit_fsdp --t5_fsdp --ulysses_size 8 --frame_num 161 --prompt \"The video presents a soaring journey through a",
      "default_branch": "main"
    },
    "fetched_at": "2026-02-08T03:50:39.577159"
  },
  {
    "basic_info": {
      "name": "skills",
      "full_name": "trailofbits/skills",
      "owner": "trailofbits",
      "description": "Trail of Bits Claude Code skills for security research, vulnerability detection, and audit workflows",
      "url": "https://github.com/trailofbits/skills",
      "clone_url": "https://github.com/trailofbits/skills.git",
      "ssh_url": "git@github.com:trailofbits/skills.git",
      "homepage": "",
      "created_at": "2026-01-14T18:23:21Z",
      "updated_at": "2026-02-08T03:20:42Z",
      "pushed_at": "2026-02-06T16:34:45Z"
    },
    "stats": {
      "stars": 2445,
      "forks": 201,
      "watchers": 2445,
      "open_issues": 10,
      "size": 633
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 344574,
        "Shell": 68533,
        "YARA": 23399,
        "CodeQL": 12064,
        "C": 8714,
        "Swift": 5459,
        "C#": 5442,
        "Ruby": 5419,
        "Java": 5221,
        "JavaScript": 5033,
        "Kotlin": 5019,
        "TypeScript": 4645,
        "PHP": 3832,
        "Rust": 2608,
        "Go": 2213
      },
      "license": "Creative Commons Attribution Share Alike 4.0 International",
      "topics": [
        "agent-skills"
      ]
    },
    "content": {
      "readme": "# Trail of Bits Skills Marketplace\n\nA Claude Code plugin marketplace from Trail of Bits providing skills to enhance AI-assisted security analysis, testing, and development workflows.\n\n## Installation\n\n### Add the Marketplace\n\n```\n/plugin marketplace add trailofbits/skills\n```\n\n### Browse and Install Plugins\n\n```\n/plugin menu\n```\n\n### Local Development\n\nTo add the marketplace locally (e.g., for testing or development), navigate to the **parent directory** of this repository:\n\n```\ncd /path/to/parent  # e.g., if repo is at ~/projects/skills, be in ~/projects\n/plugins marketplace add ./skills\n```\n\n## Available Plugins\n\n### Smart Contract Security\n\n| Plugin | Description |\n|--------|-------------|\n| [building-secure-contracts](plugins/building-secure-contracts/) | Smart contract security toolkit with vulnerability scanners for 6 blockchains |\n| [entry-point-analyzer](plugins/entry-point-analyzer/) | Identify state-changing entry points in smart contracts for security auditing |\n\n### Code Auditing\n\n| Plugin | Description |\n|--------|-------------|\n| [audit-context-building](plugins/audit-context-building/) | Build deep architectural context through ultra-granular code analysis |\n| [burpsuite-project-parser](plugins/burpsuite-project-parser/) | Search and extract data from Burp Suite project files |\n| [differential-review](plugins/differential-review/) | Security-focused differential review of code changes with git history analysis |\n| [insecure-defaults](plugins/insecure-defaults/) | Detect insecure default configurations, hardcoded credentials, and fail-open security patterns |\n| [semgrep-rule-creator](plugins/semgrep-rule-creator/) | Create and refine Semgrep rules for custom vulnerability detection |\n| [semgrep-rule-variant-creator](plugins/semgrep-rule-variant-creator/) | Port existing Semgrep rules to new target languages with test-driven validation |\n| [sharp-edges](plugins/sharp-edges/) | Identify error-prone APIs, dangerous configurations, and footgun designs |\n| [static-analysis](plugins/static-analysis/) | Static analysis toolkit with CodeQL, Semgrep, and SARIF parsing |\n| [testing-handbook-skills](plugins/testing-handbook-skills/) | Skills from the [Testing Handbook](https://appsec.guide): fuzzers, static analysis, sanitizers, coverage |\n| [variant-analysis](plugins/variant-analysis/) | Find similar vulnerabilities across codebases using pattern-based analysis |\n\n### Malware Analysis\n\n| Plugin | Description |\n|--------|-------------|\n| [yara-authoring](plugins/yara-authoring/) | YARA detection rule authoring with linting, atom analysis, and best practices |\n\n### Verification\n\n| Plugin | Description |\n|--------|-------------|\n| [constant-time-analysis](plugins/constant-time-analysis/) | Detect compiler-induced timing side-channels in cryptographic code |\n| [property-based-testing](plugins/property-based-testing/) | Property-based testing guidance for multiple languages and smart contracts |\n| [spec-to-code-compliance](plugins/spec-to-code-compliance/) | Specification-to-code compliance checker for blockchain audits |\n\n### Audit Lifecycle\n\n| Plugin | Description |\n|--------|-------------|\n| [fix-review](plugins/fix-review/) | Verify fix commits address audit findings without introducing bugs |\n\n### Reverse Engineering\n\n| Plugin | Description |\n|--------|-------------|\n| [dwarf-expert](plugins/dwarf-expert/) | Interact with and understand the DWARF debugging format |\n\n### Mobile Security\n\n| Plugin | Description |\n|--------|-------------|\n| [firebase-apk-scanner](plugins/firebase-apk-scanner/) | Scan Android APKs for Firebase security misconfigurations |\n\n### Development\n\n| Plugin | Description |\n|--------|-------------|\n| [ask-questions-if-underspecified](plugins/ask-questions-if-underspecified/) | Clarify requirements before implementing |\n| [modern-python](plugins/modern-python/) | Modern Python tooling and best practices with uv, ruff, and pytest |\n\n### Team Management\n\n| Plugin | Description |\n|--------|-------------|\n| [culture-index](plugins/culture-index/) | Interpret Culture Index survey results for individuals and teams |\n\n### Tooling\n\n| Plugin | Description |\n|--------|-------------|\n| [claude-in-chrome-troubleshooting](plugins/claude-in-chrome-troubleshooting/) | Diagnose and fix Claude in Chrome MCP extension connectivity issues |\n\n## Trophy Case\n\nBugs discovered using Trail of Bits Skills. Found something? [Let us know!](https://github.com/trailofbits/skills/issues/new?template=trophy-case.yml)\n\nWhen reporting bugs you've found, feel free to mention:\n> Found using [Trail of Bits Skills](https://github.com/trailofbits/skills)\n\n| Skill | Bug |\n|-------|-----|\n| constant-time-analysis | [Timing side-channel in ML-DSA signing](https://github.com/RustCrypto/signatures/pull/1144) |\n\n## Contributing\n\nWe welcome contributions! Please see [CLAUDE.md](CLAUDE.md) for skill authoring guidelines.\n\n## License\n\nThis work is licensed under a [Creative Commons Attribution-ShareAlike 4.0 International License]",
      "default_branch": "main"
    },
    "fetched_at": "2026-02-08T03:50:40.785394"
  },
  {
    "basic_info": {
      "name": "last30days-skill",
      "full_name": "mvanhorn/last30days-skill",
      "owner": "mvanhorn",
      "description": "Claude Code skill that researches any topic across Reddit + X from the last 30 days, then writes copy-paste-ready prompts",
      "url": "https://github.com/mvanhorn/last30days-skill",
      "clone_url": "https://github.com/mvanhorn/last30days-skill.git",
      "ssh_url": "git@github.com:mvanhorn/last30days-skill.git",
      "homepage": null,
      "created_at": "2026-01-23T20:37:37Z",
      "updated_at": "2026-02-08T03:29:41Z",
      "pushed_at": "2026-02-07T19:48:11Z"
    },
    "stats": {
      "stars": 2295,
      "forks": 265,
      "watchers": 2295,
      "open_issues": 5,
      "size": 14216
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 189820
      },
      "license": null,
      "topics": [
        "ai-prompts",
        "claude",
        "claude-code",
        "reddit",
        "twitter"
      ]
    },
    "content": {
      "readme": "# /last30days v2\n\n**The AI world reinvents itself every month. This Claude Code skill keeps you current.** /last30days researches your topic across Reddit, X, and the web from the last 30 days, finds what the community is actually upvoting and sharing, and writes you a prompt that works today, not six months ago. Whether it's Ralph Wiggum loops, Suno music prompts, or the latest Midjourney techniques, you'll prompt like someone who's been paying attention.\n\n**New in V2:** Dramatically better search results. Smarter query construction finds posts that V1 missed entirely, and a new two-phase search automatically discovers key @handles and subreddits from initial results, then drills deeper. Also: free X search via [Bird CLI](https://github.com/steipete/bird) (no xAI key needed), `--days=N` for flexible lookback, and automatic model fallback. [Full changelog below.](#whats-new-in-v2)\n\n**The tradeoff:** V2 finds way more content but takes longer â€” typically 2-8 minutes depending on how niche your topic is. The old V1 was faster but regularly missed results (like returning 0 X posts on trending topics). We think the depth is worth the wait, but if you'd use a faster \"quick mode\" that trades some depth for speed, let us know: [@mvanhorn](https://x.com/mvanhorn) / [@slashlast30days](https://x.com/slashlast30days).\n\n**Best for prompt research**: discover what prompting techniques actually work for any tool (ChatGPT, Midjourney, Claude, Figma AI, etc.) by learning from real community discussions and best practices.\n\n**But also great for anything trending**: music, culture, news, product recommendations, viral trends, or any question where \"what are people saying right now?\" matters.\n\n## Installation\n\n```bash\n# Clone the repo\ngit clone https://github.com/mvanhorn/last30days-skill.git ~/.claude/skills/last30days\n\n# Add your API keys\nmkdir -p ~/.config/last30days\ncat > ~/.config/last30days/.env << 'EOF'\nOPENAI_API_KEY=sk-...\nXAI_API_KEY=xai-...       # optional if using Bird CLI\nEOF\nchmod 600 ~/.config/last30days/.env\n```\n\n### Optional: Bird CLI for free X search\n\n[Bird CLI](https://github.com/steipete/bird) lets you search X without an xAI API key. If installed and authenticated, /last30days uses it automatically.\n\n```bash\nnpm install -g @steipete/bird\nbird login\n```\n\nBird is free and doesn't require an xAI key. If both Bird and an xAI key are available, Bird is preferred.\n\n## Usage\n\n```\n/last30days [topic]\n/last30days [topic] for [tool]\n```\n\nExamples:\n- `/last30days prompting techniques for ChatGPT for legal questions`\n- `/last30days iOS app mockups for Nano Banana Pro`\n- `/last30days What are the best rap songs lately`\n- `/last30days remotion animations for Claude Code`\n\n## What It Does\n\n1. **Researches** - Scans Reddit and X for discussions from the last 30 days\n2. **Synthesizes** - Identifies patterns, best practices, and what actually works\n3. **Delivers** - Either writes copy-paste-ready prompts for your target tool, or gives you a curated expert-level answer\n\n### Use it for:\n- **Prompt research** - \"What prompting techniques work for legal questions in ChatGPT?\"\n- **Tool best practices** - \"How are people using Remotion with Claude Code?\"\n- **Trend discovery** - \"What are the best rap songs right now?\"\n- **Product research** - \"What do people think of the new M4 MacBook?\"\n- **Viral content** - \"What's the dog-as-human trend on ChatGPT?\"\n\n---\n\n## Example: Nano Banana Pro Prompting (Image Generation)\n\n**Query:** `/last30days nano banana pro prompting`\n\n**Research Output:**\n> JSON prompting is the dominant technique â€” The X community overwhelmingly uses structured JSON prompts rather than plain text. Creators like @Xmira_belle, @Cicily_aura, @RubenSalvo_, and @mahivisuals all share prompts in JSON format with nested objects for metadata, subject descriptions, lighting, camera specs, and scene composition. @realdigitaldao made a static ad \"in less than 5 min\" using a simple JSON prompt.\n>\n> Prompt adherence is Nano Banana Pro's killer advantage â€” JSON prompts hit 92% precision for color accuracy, lighting ratios, and composition vs ~68% for natural language prompts, per fofr.ai. @artingent directly compared it to GPT Image 1.5 and noted Nano Banana Pro has higher prompt adherence â€” \"GPT image 1.5 ignored the Top Down instruction.\"\n>\n> Stop writing \"tag soup\" â€” act like a Creative Director â€” The biggest shift is away from comma-separated keyword lists toward natural language descriptions with clear structure. The ICS Framework (Image type, Content, Style) and the 5-element formula (subject, composition, action, setting, style) are the two dominant approaches, per r/nanobanana2pro.\n\n**Key patterns discovered:**\n1. JSON > plain text â€” Structure prompts as JSON objects with keys for subject, composition, lighting, camera, style â€” per @Xmira_belle, @Cicily_aura\n2. The 5-element formula â€” Subject + Composition + Action + Setting + Style â€” per r/nanobanana2pro\n3. Separate subjects into distinct objects â€” Multi-character s",
      "default_branch": "main"
    },
    "fetched_at": "2026-02-08T03:50:42.003516"
  },
  {
    "basic_info": {
      "name": "DeepSeek-OCR-2",
      "full_name": "deepseek-ai/DeepSeek-OCR-2",
      "owner": "deepseek-ai",
      "description": "Visual Causal Flow",
      "url": "https://github.com/deepseek-ai/DeepSeek-OCR-2",
      "clone_url": "https://github.com/deepseek-ai/DeepSeek-OCR-2.git",
      "ssh_url": "git@github.com:deepseek-ai/DeepSeek-OCR-2.git",
      "homepage": "",
      "created_at": "2026-01-27T03:05:42Z",
      "updated_at": "2026-02-07T23:46:12Z",
      "pushed_at": "2026-02-03T00:34:18Z"
    },
    "stats": {
      "stars": 2131,
      "forks": 161,
      "watchers": 2131,
      "open_issues": 42,
      "size": 1075
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 107443
      },
      "license": "Apache License 2.0",
      "topics": []
    },
    "content": {
      "readme": "<!-- markdownlint-disable first-line-h1 -->\n<!-- markdownlint-disable html -->\n<!-- markdownlint-disable no-duplicate-header -->\n\n\n<div align=\"center\">\n  <img src=\"assets/logo.svg\" width=\"60%\" alt=\"DeepSeek AI\" />\n</div>\n\n\n<hr>\n<div align=\"center\">\n  <a href=\"https://www.deepseek.com/\" target=\"_blank\">\n    <img alt=\"Homepage\" src=\"assets/badge.svg\" />\n  </a>\n  <a href=\"https://huggingface.co/deepseek-ai/DeepSeek-OCR-2\" target=\"_blank\">\n    <img alt=\"Hugging Face\" src=\"https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-DeepSeek%20AI-ffc107?color=ffc107&logoColor=white\" />\n  </a>\n\n</div>\n\n<div align=\"center\">\n\n  <a href=\"https://discord.gg/Tc7c45Zzu5\" target=\"_blank\">\n    <img alt=\"Discord\" src=\"https://img.shields.io/badge/Discord-DeepSeek%20AI-7289da?logo=discord&logoColor=white&color=7289da\" />\n  </a>\n  <a href=\"https://twitter.com/deepseek_ai\" target=\"_blank\">\n    <img alt=\"Twitter Follow\" src=\"https://img.shields.io/badge/Twitter-deepseek_ai-white?logo=x&logoColor=white\" />\n  </a>\n\n</div>\n\n\n\n<p align=\"center\">\n  <a href=\"https://huggingface.co/deepseek-ai/DeepSeek-OCR-2\"><b>ğŸ“¥ Model Download</b></a> |\n  <a href=\"https://github.com/deepseek-ai/DeepSeek-OCR-2/blob/main/DeepSeek_OCR2_paper.pdf\"><b>ğŸ“„ Paper Link</b></a> |\n  <a href=\"https://arxiv.org/abs/2601.20552\"><b>ğŸ“„ Arxiv Paper Link</b></a> |\n</p>\n\n<h2>\n<p align=\"center\">\n  <a href=\"\">DeepSeek-OCR 2: Visual Causal Flow</a>\n</p>\n</h2>\n\n<p align=\"center\">\n<img src=\"assets/fig1.png\" style=\"width: 600px\" align=center>\n</p>\n<p align=\"center\">\n<a href=\"\">Explore more human-like visual encoding.</a>       \n</p>\n\n\n## Contents\n- [Install](#install)\n- [vLLM Inference](#vllm-inference)\n- [Transformers Inference](#transformers-inference)\n  \n\n\n\n\n## Install\n>Our environment is cuda11.8+torch2.6.0.\n1. Clone this repository and navigate to the DeepSeek-OCR-2 folder\n```bash\ngit clone https://github.com/deepseek-ai/DeepSeek-OCR-2.git\n```\n2. Conda\n```Shell\nconda create -n deepseek-ocr2 python=3.12.9 -y\nconda activate deepseek-ocr2\n```\n3. Packages\n\n- download the vllm-0.8.5 [whl](https://github.com/vllm-project/vllm/releases/tag/v0.8.5) \n```Shell\npip install torch==2.6.0 torchvision==0.21.0 torchaudio==2.6.0 --index-url https://download.pytorch.org/whl/cu118\npip install vllm-0.8.5+cu118-cp38-abi3-manylinux1_x86_64.whl\npip install -r requirements.txt\npip install flash-attn==2.7.3 --no-build-isolation\n```\n**Note:** if you want vLLM and transformers codes to run in the same environment, you don't need to worry about this installation error like: vllm 0.8.5+cu118 requires transformers>=4.51.1\n\n## vLLM-Inference\n- VLLM:\n>**Note:** change the INPUT_PATH/OUTPUT_PATH and other settings in the DeepSeek-OCR2-master/DeepSeek-OCR2-vllm/config.py\n```Shell\ncd DeepSeek-OCR2-master/DeepSeek-OCR2-vllm\n```\n1. image: streaming output\n```Shell\npython run_dpsk_ocr2_image.py\n```\n2. pdf: concurrency (on-par speed with DeepSeek-OCR)\n```Shell\npython run_dpsk_ocr2_pdf.py\n```\n3. batch eval for benchmarks (i.e., OmniDocBench v1.5)\n```Shell\npython run_dpsk_ocr2_eval_batch.py\n```\n\n## Transformers-Inference\n- Transformers\n```python\nfrom transformers import AutoModel, AutoTokenizer\nimport torch\nimport os\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\nmodel_name = 'deepseek-ai/DeepSeek-OCR-2'\n\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\nmodel = AutoModel.from_pretrained(model_name, _attn_implementation='flash_attention_2', trust_remote_code=True, use_safetensors=True)\nmodel = model.eval().cuda().to(torch.bfloat16)\n\n# prompt = \"<image>\\nFree OCR. \"\nprompt = \"<image>\\n<|grounding|>Convert the document to markdown. \"\nimage_file = 'your_image.jpg'\noutput_path = 'your/output/dir'\n\nres = model.infer(tokenizer, prompt=prompt, image_file=image_file, output_path = output_path, base_size = 1024, image_size = 768, crop_mode=True, save_results = True)\n```\nor you can\n```Shell\ncd DeepSeek-OCR2-master/DeepSeek-OCR2-hf\npython run_dpsk_ocr2.py\n```\n## Support-Modes\n- Dynamic resolution\n  - Default: (0-6)Ã—768Ã—768 + 1Ã—1024Ã—1024 â€” (0-6)Ã—144 + 256 visual tokens âœ…\n\n## Main Prompts\n```python\n# document: <image>\\n<|grounding|>Convert the document to markdown.\n# without layouts: <image>\\nFree OCR.\n```\n\n\n\n\n## Acknowledgement\n\nWe would like to thank [DeepSeek-OCR](https://github.com/deepseek-ai/DeepSeek-OCR/), [Vary](https://github.com/Ucas-HaoranWei/Vary/), [GOT-OCR2.0](https://github.com/Ucas-HaoranWei/GOT-OCR2.0/), [MinerU](https://github.com/opendatalab/MinerU), [PaddleOCR](https://github.com/PaddlePaddle/PaddleOCR) for their valuable models.\n\nWe also appreciate the benchmark [OmniDocBench](https://github.com/opendatalab/OmniDocBench).\n\n## Citation\n\n```bibtex\n@article{wei2025deepseek,\n  title={DeepSeek-OCR: Contexts Optical Compression},\n  author={Wei, Haoran and Sun, Yaofeng and Li, Yukun},\n  journal={arXiv preprint arXiv:2510.18234},\n  year={2025}\n}\n@article{wei2026deepseek,\n  title={DeepSeek-OCR 2: Visual Causal Flow},\n  author={Wei, Haoran and Sun, Yaofeng and Li, Yukun},\n  journal={arXiv pr",
      "default_branch": "main"
    },
    "fetched_at": "2026-02-08T03:50:43.201509"
  },
  {
    "basic_info": {
      "name": "nanocode",
      "full_name": "1rgs/nanocode",
      "owner": "1rgs",
      "description": "Minimal Claude Code alternative. Single Python file, zero dependencies, ~250 lines.",
      "url": "https://github.com/1rgs/nanocode",
      "clone_url": "https://github.com/1rgs/nanocode.git",
      "ssh_url": "git@github.com:1rgs/nanocode.git",
      "homepage": null,
      "created_at": "2026-01-11T02:12:27Z",
      "updated_at": "2026-02-07T20:11:50Z",
      "pushed_at": "2026-01-14T05:59:51Z"
    },
    "stats": {
      "stars": 1887,
      "forks": 163,
      "watchers": 1887,
      "open_issues": 8,
      "size": 183
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 8445
      },
      "license": null,
      "topics": []
    },
    "content": {
      "readme": "# nanocode\n\nMinimal Claude Code alternative. Single Python file, zero dependencies, ~250 lines.\n\nBuilt using Claude Code, then used to build itself.\n\n![screenshot](screenshot.png)\n\n## Features\n\n- Full agentic loop with tool use\n- Tools: `read`, `write`, `edit`, `glob`, `grep`, `bash`\n- Conversation history\n- Colored terminal output\n\n## Usage\n\n```bash\nexport ANTHROPIC_API_KEY=\"your-key\"\npython nanocode.py\n```\n\n### OpenRouter\n\nUse [OpenRouter](https://openrouter.ai) to access any model:\n\n```bash\nexport OPENROUTER_API_KEY=\"your-key\"\npython nanocode.py\n```\n\nTo use a different model:\n\n```bash\nexport OPENROUTER_API_KEY=\"your-key\"\nexport MODEL=\"openai/gpt-5.2\"\npython nanocode.py\n```\n\n## Commands\n\n- `/c` - Clear conversation\n- `/q` or `exit` - Quit\n\n## Tools\n\n| Tool | Description |\n|------|-------------|\n| `read` | Read file with line numbers, offset/limit |\n| `write` | Write content to file |\n| `edit` | Replace string in file (must be unique) |\n| `glob` | Find files by pattern, sorted by mtime |\n| `grep` | Search files for regex |\n| `bash` | Run shell command |\n\n## Example\n\n```\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nâ¯ what files are here?\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\nâº Glob(**/*.py)\n  â¿  nanocode.py\n\nâº There's one Python file: nanocode.py\n```\n\n## License\n\nMIT\n",
      "default_branch": "master"
    },
    "fetched_at": "2026-02-08T03:50:44.406181"
  },
  {
    "basic_info": {
      "name": "dash",
      "full_name": "agno-agi/dash",
      "owner": "agno-agi",
      "description": "Self-learning data agent that grounds its answers in 6 layers of context. Inspired by OpenAI's in-house implementation.",
      "url": "https://github.com/agno-agi/dash",
      "clone_url": "https://github.com/agno-agi/dash.git",
      "ssh_url": "git@github.com:agno-agi/dash.git",
      "homepage": "",
      "created_at": "2026-01-30T13:54:17Z",
      "updated_at": "2026-02-08T03:39:09Z",
      "pushed_at": "2026-02-01T19:53:23Z"
    },
    "stats": {
      "stars": 1425,
      "forks": 145,
      "watchers": 1425,
      "open_issues": 2,
      "size": 229
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 52528,
        "Shell": 10886,
        "Dockerfile": 662
      },
      "license": "Apache License 2.0",
      "topics": []
    },
    "content": {
      "readme": "# Dash\n\nDash is a **self-learning data agent** that grounds its answers in **6 layers of context** and improves with every run.\n\nInspired by [OpenAI's in-house data agent](https://openai.com/index/inside-our-in-house-data-agent/).\n\n## Quick Start\n\n```sh\n# Clone this repo\ngit clone https://github.com/agno-agi/dash.git && cd dash\n# Add OPENAI_API_KEY by adding to .env file or export OPENAI_API_KEY=sk-***\ncp example.env .env\n\n# Start the application\ndocker compose up -d --build\n\n# Load sample data and knowledge\ndocker exec -it dash-api python -m dash.scripts.load_data\ndocker exec -it dash-api python -m dash.scripts.load_knowledge\n```\n\nConfirm dash is running by navigation to [http://localhost:8000/docs](http://localhost:8000/docs).\n\n## Connect to the Web UI\n\n1. Open [os.agno.com](https://os.agno.com) and login\n2. Add OS â†’ Local â†’ `http://localhost:8000`\n3. Click \"Connect\"\n\n**Try it** (sample F1 dataset):\n\n- Who won the most F1 World Championships?\n- How many races has Lewis Hamilton won?\n- Compare Ferrari vs Mercedes points 2015-2020\n\n## Why Text-to-SQL Breaks in Practice\n\nOur goal is simple: ask a question in english, get a correct, meaningful answer. But raw LLMs writing SQL hit a wall fast:\n\n- **Schemas lack meaning.**\n- **Types are misleading.**\n- **Tribal knowledge is missing.**\n- **No way to learn from mistakes.**\n- **Results generally lack interpretation.**\n\nThe root cause is missing context and missing memory.\n\nDash solves this with **6 layers of grounded context**, a **self-learning loop** that improves with every query, and a focus on **understanding your question** to deliver insights you can act on.\n\n## The Six Layers of Context\n\n| Layer | Purpose | Source |\n|------|--------|--------|\n| **Table Usage** | Schema, columns, relationships | `knowledge/tables/*.json` |\n| **Human Annotations** | Metrics, definitions, and business rules | `knowledge/business/*.json` |\n| **Query Patterns** | SQL that is known to work | `knowledge/queries/*.sql` |\n| **Institutional Knowledge** | Docs, wikis, external references | MCP (optional) |\n| **Learnings** | Error patterns and discovered fixes | Agno `Learning Machine` |\n| **Runtime Context** | Live schema changes | `introspect_schema` tool |\n\nThe agent retrieves relevant context at query time via hybrid search, then generates SQL grounded in patterns that already work.\n\n## The Self-Learning Loop\n\nDash improves without retraining or fine-tuning. We call this gpu-poor continuous learning.\n\nIt learns through two complementary systems:\n\n| System | Stores | How It Evolves |\n|------|--------|----------------|\n| **Knowledge** | Validated queries and business context | Curated by you + dash |\n| **Learnings** | Error patterns and fixes | Managed by `Learning Machine` automatically |\n\n```\nUser Question\n     â†“\nRetrieve Knowledge + Learnings\n     â†“\nReason about intent\n     â†“\nGenerate grounded SQL\n     â†“\nExecute and interpret\n     â†“\n â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”\n â†“         â†“\nSuccess    Error\n â†“         â†“\n â†“         Diagnose â†’ Fix â†’ Save Learning\n â†“                           (never repeated)\n â†“\nReturn insight\n â†“\nOptionally save as Knowledge\n```\n\n**Knowledge** is curatedâ€”validated queries and business context you want the agent to build on.\n\n**Learnings** is discoveredâ€”patterns the agent finds through trial and error. When a query fails because `position` is TEXT not INTEGER, the agent saves that gotcha. Next time, it knows.\n\n## Insights, Not Just Rows\n\nDash reasons about what makes an answer useful, not just technically correct.\n\n**Question:**\nWho won the most races in 2019?\n\n| Typical SQL Agent | Dash |\n|------------------|------|\n| `Hamilton: 11` | Lewis Hamilton dominated 2019 with **11 wins out of 21 races**, more than double Bottasâ€™s 4 wins. This performance secured his sixth world championship. |\n\n## Deploy to Railway\n\n```sh\nrailway login\n\n./scripts/railway_up.sh\n```\n\n### Production Operations\n\n**Load data and knowledge:**\n```sh\nrailway run python -m dash.scripts.load_data\nrailway run python -m dash.scripts.load_knowledge\n```\n\n**View logs:**\n\n```sh\nrailway logs --service dash\n```\n\n**Run commands in production:**\n\n```sh\nrailway run python -m dash  # CLI mode\n```\n\n**Redeploy after changes:**\n\n```sh\nrailway up --service dash -d\n```\n\n**Open dashboard:**\n```sh\nrailway open\n```\n\n## Adding Knowledge\n\nDash works best when it understands how your organization talks about data.\n\n```\nknowledge/\nâ”œâ”€â”€ tables/      # Table meaning and caveats\nâ”œâ”€â”€ queries/     # Proven SQL patterns\nâ””â”€â”€ business/    # Metrics and language\n```\n\n### Table Metadata\n\n```\n{\n  \"table_name\": \"orders\",\n  \"table_description\": \"Customer orders with denormalized line items\",\n  \"use_cases\": [\"Revenue reporting\", \"Customer analytics\"],\n  \"data_quality_notes\": [\n    \"created_at is UTC\",\n    \"status values: pending, completed, refunded\",\n    \"amount stored in cents\"\n  ]\n}\n```\n\n### Query Patterns\n\n```\n-- <query name>monthly_revenue</query name>\n-- <query description>\n-- Monthly revenue calculation.\n-- Converts cents to dollars.\n-- Excludes ",
      "default_branch": "main"
    },
    "fetched_at": "2026-02-08T03:50:45.607619"
  },
  {
    "basic_info": {
      "name": "Qwen3-ASR",
      "full_name": "QwenLM/Qwen3-ASR",
      "owner": "QwenLM",
      "description": "Qwen3-ASR is an open-source series of ASR models developed by the Qwen team at Alibaba Cloud, supporting stable multilingual speech/music/song recognition, language detection and timestamp prediction.",
      "url": "https://github.com/QwenLM/Qwen3-ASR",
      "clone_url": "https://github.com/QwenLM/Qwen3-ASR.git",
      "ssh_url": "git@github.com:QwenLM/Qwen3-ASR.git",
      "homepage": null,
      "created_at": "2026-01-28T05:44:59Z",
      "updated_at": "2026-02-08T03:30:07Z",
      "pushed_at": "2026-01-30T03:24:24Z"
    },
    "stats": {
      "stars": 1355,
      "forks": 106,
      "watchers": 1355,
      "open_issues": 27,
      "size": 2336
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 231743
      },
      "license": "Apache License 2.0",
      "topics": []
    },
    "content": {
      "readme": "# Qwen3-ASR\n\n<br>\n\n<p align=\"center\">\n    <img src=\"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-ASR-Repo/logo.png\" width=\"400\"/>\n<p>\n\n<p align=\"center\">\n&nbsp&nbspğŸ¤— <a href=\"https://huggingface.co/collections/Qwen/qwen3-asr\">Hugging Face</a>&nbsp&nbsp | &nbsp&nbspğŸ¤– <a href=\"https://modelscope.cn/collections/Qwen/Qwen3-ASR\">ModelScope</a>&nbsp&nbsp | &nbsp&nbspğŸ“‘ <a href=\"https://qwen.ai/blog?id=qwen3asr\">Blog</a>&nbsp&nbsp | &nbsp&nbspğŸ“‘ <a href=\"https://arxiv.org/abs/2601.21337\">Paper</a>&nbsp&nbsp\n<br>\nğŸ–¥ï¸ <a href=\"https://huggingface.co/spaces/Qwen/Qwen3-ASR\">Hugging Face Demo</a>&nbsp&nbsp | &nbsp&nbsp ğŸ–¥ï¸ <a href=\"https://modelscope.cn/studios/Qwen/Qwen3-ASR\">ModelScope Demo</a>&nbsp&nbsp | &nbsp&nbspğŸ’¬ <a href=\"https://github.com/QwenLM/Qwen/blob/main/assets/wechat.png\">WeChat (å¾®ä¿¡)</a>&nbsp&nbsp | &nbsp&nbspğŸ«¨ <a href=\"https://discord.gg/CV4E9rpNSD\">Discord</a>&nbsp&nbsp | &nbsp&nbspğŸ“‘ <a href=\"https://help.aliyun.com/zh/model-studio/qwen-speech-recognition\">API</a>\n\n</p>\n\nWe release **Qwen3-ASR**, a family that includes two powerful all-in-one speech recognition models that support language identification and ASR for 52 languages and dialects, as well as a novel non-autoregressive speech forced-alignment model that can align textâ€“speech pairs in 11 languages.\n\n\n## News\n* 2026.1.29: ğŸ‰ğŸ‰ğŸ‰ We have released the [Qwen3-ASR](https://huggingface.co/collections/Qwen/qwen3-asr) series (0.6B/1.7B) and the Qwen3-ForcedAligner-0.6B model. Please check out our [blog](https://qwen.ai/blog?id=qwen3asr)!\n\n\n## Contents <!-- omit in toc -->\n\n- [Overview](#overview)\n  - [Introduction](#introduction)\n  - [Model Architecture](#model-architecture)\n  - [Released Models Description and Download](#released-models-description-and-download)\n- [Quickstart](#quickstart)\n  - [Environment Setup](#environment-setup)\n  - [Python Package Usage](#python-package-usage)\n    - [Quick Inference](#quick-inference)\n    - [vLLM Backend](#vllm-backend)\n    - [Streaming Inference](#streaming-inference)\n    - [ForcedAligner Usage](#forcedaligner-usage)\n  - [DashScope API Usage](#dashscope-api-usage)\n- [Launch Local Web UI Demo](#launch-local-web-ui-demo)\n  - [Gradio Demo](#gradio-demo)\n  - [Streaming Demo](#streaming-demo)\n- [Deployment with vLLM](#deployment-with-vllm)\n- [Fine Tuning](#fine-tuning)\n- [Docker](#docker)\n- [Evaluation](#evaluation)\n- [Citation](#citation)\n\n\n## Overview\n\n### Introduction\n\n<p align=\"center\">\n    <img src=\"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-ASR-Repo/qwen3_asr_introduction.png\" width=\"90%\"/>\n<p>\n\nThe Qwen3-ASR family includes Qwen3-ASR-1.7B and Qwen3-ASR-0.6B, which support language identification and ASR for 52 languages and dialects. Both leverage large-scale speech training data and the strong audio understanding capability of their foundation model, Qwen3-Omni. Experiments show that the 1.7B version achieves state-of-the-art performance among open-source ASR models and is competitive with the strongest proprietary commercial APIs. Here are the main features:\n\n* **All-in-one**: Qwen3-ASR-1.7B and Qwen3-ASR-0.6B support language identification and speech recognition for 30 languages and 22 Chinese dialects, so as to English accents from multiple countries and regions.\n\n* **Excellent and Fast**: The Qwen3-ASR family ASR models maintains high-quality and robust recognition under complex acoustic environments and challenging text patterns. Qwen3-ASR-1.7B achieves strong performance on both open-sourced and internal benchmarks. While the 0.6B version achieves accuracy-efficient trade-off, it reaches 2000 times throughput at a concurrency of 128. They both achieve streaming / offline unified inference with single model and support transcribe long audio.\n\n* **Novel and strong forced alignment Solution**: We introduce Qwen3-ForcedAligner-0.6B, which supports timestamp prediction for arbitrary units within up to 5 minutes of speech in 11 languages. Evaluations show its timestamp accuracy surpasses E2E based forced-alignment models.\n\n* **Comprehensive inference toolkit**: In addition to open-sourcing the architectures and weights of the Qwen3-ASR series, we also release a powerful, full-featured inference framework that supports vLLM-based batch inference, asynchronous serving, streaming inference, timestamp prediction, and more.\n\n### Model Architecture\n\n<p align=\"center\">\n    <img src=\"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-ASR-Repo/overview.jpg\" width=\"100%\"/>\n<p>\n\n\n### Released Models Description and Download\n\nBelow is an introduction and download information for the Qwen3-ASR models. Please select and download the model that fits your needs.\n\n| Model | Supported Languages | Supported Dialects | Inference Mode | Audio Types |\n|---|---|---|---|---|\n| Qwen3-ASR-1.7B & Qwen3-ASR-0.6B | Chinese (zh), English (en), Cantonese (yue), Arabic (ar), German (de), French (fr), Spanish (es), Portuguese (pt), Indonesian (id), Italian (it), Korean (ko), Russian (ru), Thai (th), Vietnamese (vi), Japan",
      "default_branch": "main"
    },
    "fetched_at": "2026-02-08T03:50:46.822702"
  },
  {
    "basic_info": {
      "name": "zlibrary-to-notebooklm",
      "full_name": "zstmfhy/zlibrary-to-notebooklm",
      "owner": "zstmfhy",
      "description": "ä¸€é”®å°† Z-Library ä¹¦ç±è‡ªåŠ¨ä¸‹è½½å¹¶ä¸Šä¼ åˆ° Google NotebookLM",
      "url": "https://github.com/zstmfhy/zlibrary-to-notebooklm",
      "clone_url": "https://github.com/zstmfhy/zlibrary-to-notebooklm.git",
      "ssh_url": "git@github.com:zstmfhy/zlibrary-to-notebooklm.git",
      "homepage": null,
      "created_at": "2026-01-14T00:27:42Z",
      "updated_at": "2026-02-07T05:57:50Z",
      "pushed_at": "2026-01-17T09:13:05Z"
    },
    "stats": {
      "stars": 1306,
      "forks": 157,
      "watchers": 1306,
      "open_issues": 4,
      "size": 39
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 35884
      },
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "# ğŸ“š Z-Library to NotebookLM\n\n[English](README.md) | [ç®€ä½“ä¸­æ–‡](README.zh-CN.md)\n\n> Automatically download books from Z-Library and upload them to Google NotebookLM with one command.\n\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)\n[![Claude Skill](https://img.shields.io/badge/Claude-Skill-success.svg)](https://claude.ai/claude-code)\n\n---\n\n## âš ï¸ Important Disclaimer\n\n**This project is for educational, research, and technical demonstration purposes only. Please strictly comply with local laws and copyright regulations. Use only for:**\n\n- âœ… Resources you have legal access to\n- âœ… Public domain or open-source licensed documents (e.g., arXiv, Project Gutenberg)\n- âœ… Content you personally own or have authorization to use\n\n**The author does not encourage or support any form of copyright infringement and assumes no legal liability. Use at your own risk.**\n\n**Please respect intellectual property rights and support authorized reading!**\n\n---\n\n## âœ¨ Features\n\n- ğŸ” **One-time Login, Forever Use** - Similar to `notebooklm login` experience\n- ğŸ“¥ **Smart Download** - Prioritizes PDF (preserves formatting), auto-fallback to EPUB â†’ Markdown\n- ğŸ“¦ **Smart Chunking** - Large files auto-split (>350k words) for reliable CLI upload\n- ğŸ¤– **Fully Automated** - Complete workflow with a single command\n- ğŸ¯ **Format Adaptive** - Automatically detects and processes multiple formats (PDF, EPUB, MOBI, etc.)\n- ğŸ“Š **Visual Progress** - Real-time display of download and conversion progress\n\n## ğŸ¯ Use as Claude Skill (Recommended)\n\n### Installation\n\n```bash\n# 1. Navigate to Claude Skills directory\ncd ~/.claude/skills  # Windows: %APPDATA%\\Claude\\skills\n\n# 2. Clone the repository\ngit clone https://github.com/zstmfhy/zlibrary-to-notebooklm.git zlib-to-notebooklm\n\n# 3. Complete initial login\ncd zlib-to-notebooklm\npython3 scripts/login.py\n```\n\n### Usage\n\nAfter installation, simply tell Claude Code:\n\n```text\nUse zlib-to-notebooklm skill to process this Z-Library link:\nhttps://zh.zlib.li/book/25314781/aa05a1/book-title\n```\n\nClaude will automatically:\n\n- Download the book (prioritizing PDF)\n- Create NotebookLM notebook\n- Upload the file\n- Return notebook ID\n- Suggest follow-up questions\n\n---\n\n## ğŸ› ï¸ Traditional Installation\n\n### 1. Install Dependencies\n\n```bash\n# Clone repository\ngit clone https://github.com/zstmfhy/zlibrary-to-notebooklm.git\ncd zlibrary-to-notebooklm\n\n# Install Python dependencies\npip install playwright ebooklib\n\n# Install Playwright browser\nplaywright install chromium\n```\n\n### 2. Login to Z-Library (One-time Only)\n\n```bash\npython3 scripts/login.py\n```\n\n**Steps:**\n1. Browser will automatically open and visit Z-Library\n2. Complete login in the browser\n3. Return to terminal and press **ENTER**\n4. Session saved!\n\n### 3. Download and Upload Books\n\n```bash\npython3 scripts/upload.py \"https://zh.zlib.li/book/...\"\n```\n\n**Automatically completes:**\n\n- âœ… Login using saved session\n- âœ… Download PDF (preserves formatting)\n- âœ… Fallback to EPUB â†’ Markdown\n- âœ… Smart chunking for large files (>350k words)\n- âœ… Create NotebookLM notebook\n- âœ… Upload content\n- âœ… Return notebook ID\n\n## ğŸ“– Usage Examples\n\n### Basic Usage\n\n```bash\n# Download single book\npython3 scripts/upload.py \"https://zh.zlib.li/book/12345/...\"\n```\n\n### Batch Processing\n\n```bash\n# Batch download multiple books\nfor url in \"url1\" \"url2\" \"url3\"; do\n    python3 scripts/upload.py \"$url\"\ndone\n```\n\n### Using NotebookLM\n\n```bash\n# After upload, use the notebook\nnotebooklm use <returned-notebook-id>\n\n# Start asking questions\nnotebooklm ask \"What are the core concepts of this book?\"\nnotebooklm ask \"Summarize Chapter 3\"\n```\n\n## ğŸ”„ Workflow\n\n```text\nZ-Library URL\n    â†“\n1. Launch browser (using saved session)\n    â†“\n2. Visit book page\n    â†“\n3. Smart format selection:\n   - Priority: PDF (preserves formatting)\n   - Fallback: EPUB (convert to Markdown)\n   - Other formats (auto-convert)\n    â†“\n4. Download to ~/Downloads\n    â†“\n5. Format processing:\n   - PDF â†’ Use directly\n   - EPUB â†’ Convert to Markdown\n   - Check file size â†’ Auto-chunk if >350k words\n    â†“\n6. Create NotebookLM notebook\n    â†“\n7. Upload content (chunked files uploaded individually)\n    â†“\n8. Return notebook ID âœ…\n```\n\n## ğŸ“ Project Structure\n\n```text\nzlibrary-to-notebooklm/\nâ”œâ”€â”€ SKILL.md              # Core Skill definition (required)\nâ”œâ”€â”€ README.md             # Project documentation\nâ”œâ”€â”€ README.zh-CN.md       # Chinese documentation\nâ”œâ”€â”€ LICENSE               # MIT License\nâ”œâ”€â”€ package.json          # npm config (for Claude Code skill)\nâ”œâ”€â”€ skill.yaml            # Skill configuration\nâ”œâ”€â”€ requirements.txt      # Python dependencies\nâ”œâ”€â”€ scripts/              # Executable scripts (official standard)\nâ”‚   â”œâ”€â”€ login.py         # Login script\nâ”‚   â”œâ”€â”€ upload.py        # Download + Upload script\nâ”‚   â””â”€â”€ convert_epub.py  # EPUB conversion tool\nâ”œâ”€â”€ docs/                 # Documentation\nâ”‚   â”œâ”€â”€ WORKFLOW.md      #",
      "default_branch": "main"
    },
    "fetched_at": "2026-02-08T03:50:48.049376"
  },
  {
    "basic_info": {
      "name": "Edit-Banana",
      "full_name": "BIT-DataLab/Edit-Banana",
      "owner": "BIT-DataLab",
      "description": "Edit Banana: A framework for converting statistical formats into editable.",
      "url": "https://github.com/BIT-DataLab/Edit-Banana",
      "clone_url": "https://github.com/BIT-DataLab/Edit-Banana.git",
      "ssh_url": "git@github.com:BIT-DataLab/Edit-Banana.git",
      "homepage": "https://editbanana.anxin6.cn/",
      "created_at": "2026-01-16T10:34:52Z",
      "updated_at": "2026-02-08T03:45:55Z",
      "pushed_at": "2026-02-06T09:35:22Z"
    },
    "stats": {
      "stars": 1265,
      "forks": 53,
      "watchers": 1265,
      "open_issues": 3,
      "size": 10336
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 450318
      },
      "license": null,
      "topics": [
        "ai",
        "data",
        "figure",
        "llm",
        "nanobanana",
        "open-source",
        "python",
        "pythonprogramming"
      ]
    },
    "content": {
      "readme": "<p align=\"center\">\n  <img src=\"/static/banana.jpg\" width=\"180\" alt=\"Edit Banana Logo\"/>\n</p>\n\n<h1 align=\"center\">ğŸŒ Edit Banana</h1>\n<h3 align=\"center\">Universal Content Re-Editor: Make the Uneditable, Editable</h3>\n\n<p align=\"center\">\nBreak free from static formats. Our platform empowers you to transform fixed content into fully manipulatable assets.\nPowered by SAM 3 and multimodal large models, it enables high-fidelity reconstruction that preserves the original diagram details and logical relationships.\n</p>\n\n<p align=\"center\">\n  <a href=\"https://www.python.org/\"><img src=\"https://img.shields.io/badge/Python-3.10+-3776AB?style=flat-square&logo=python&logoColor=white\" alt=\"Python\"/></a>\n  <a href=\"LICENSE\"><img src=\"https://img.shields.io/badge/License-Apache_2.0-2F80ED?style=flat-square&logo=apache&logoColor=white\" alt=\"License\"/></a>\n  <a href=\"https://developer.nvidia.com/cuda-downloads\"><img src=\"https://img.shields.io/badge/GPU-CUDA%20Recommended-76B900?style=flat-square&logo=nvidia\" alt=\"CUDA\"/></a>\n  <a href=\"#-join-wechat-group\"><img src=\"https://img.shields.io/badge/WeChat-Join%20Group-07C160?style=flat-square&logo=wechat&logoColor=white\" alt=\"WeChat\"/></a>\n</p>\n\n---\n\n<h3 align=\"center\">Try It Now!</h3>\n<p align=\"center\">\n  <a href=\"https://editbanana.anxin6.cn/\">\n    <img src=\"https://img.shields.io/badge/ğŸš€%20Try%20Online%20Demo-editbanana.anxin6.cn-FF6B6B?style=for-the-badge&logoColor=white\" alt=\"Try Online Demo\"/>\n  </a>\n</p>\n\n<p align=\"center\">\n  ğŸ‘† <b>Click above or https://editbanana.anxin6.cn/ to try Edit Banana online!</b> Upload an image or pdf, get <b>editable DrawIO (XML) or PPTX</b> in seconds. \n  <b>Please note</b>: Our GitHub repository currently trails behind our web-based service. For the most up-to-date features and performance, we recommend using our web platform.\n</p>\n\n---\n\n## ğŸ“¸ Effect Demonstration\n### High-Definition Input-Output Comparison (3 Typical Scenarios)\nTo demonstrate the high-fidelity conversion effect, we provides one-to-one comparisons between 3 scenarios of \"original static formats\" and \"editable reconstruction results\". All elements can be individually dragged, styled, and modified.\n\n#### Scenario 1: Figures to Drawio(xml, svg, pptx)\n\n| Example No. | Original Static Diagram (Input Â· Non-editable) | DrawIO Reconstruction Result (Output Â· Fully Editable) |\n|--------------|-----------------------------------------------|--------------------------------------------------------|\n| Example 1: Basic Flowchart | <img src=\"/static/demo/original_1.jpg\" width=\"400\" alt=\"Original Diagram 1\" style=\"border: 1px solid #eee; border-radius: 4px;\"/> | <img src=\"/static/demo/recon_1.png\" width=\"400\" alt=\"Reconstruction Result 1\" style=\"border: 1px solid #eee; border-radius: 4px;\"/> |\n| Example 2: Multi-level Architecture Diagram | <img src=\"/static/demo/original_2.png\" width=\"400\" alt=\"Original Diagram 2\" style=\"border: 1px solid #eee; border-radius: 4px;\"/> | <img src=\"/static/demo/recon_2.png\" width=\"400\" alt=\"Reconstruction Result 2\" style=\"border: 1px solid #eee; border-radius: 4px;\"/> |\n| Example 3: Technical Schematic | <img src=\"/static/demo/original_3.jpg\" width=\"400\" alt=\"Original Diagram 3\" style=\"border: 1px solid #eee; border-radius: 4px;\"/> | <img src=\"/static/demo/recon_3.png\" width=\"400\" alt=\"Reconstruction Result 3\" style=\"border: 1px solid #eee; border-radius: 4px;\"/> |\n| Example 4: Scientific Formula Diagram | <img src=\"/static/demo/original_4.jpg\" width=\"400\" alt=\"Original Diagram 4\" style=\"border: 1px solid #eee; border-radius: 4px;\"/> | <img src=\"/static/demo/recon_4.png\" width=\"400\" alt=\"Reconstruction Result 4\" style=\"border: 1px solid #eee; border-radius: 4px;\"/> |\n\n#### Scenario 2: PDF to PPTX\n\n\n#### Scenario 3: Human in the Loop Modification\n\n> âœ¨ Conversion Highlights:\n> 1.  Preserves the layout logic, color matching, and element hierarchy of the original diagram\n> 2.  1:1 restoration of shape stroke/fill and arrow styles (dashed lines/thickness)\n> 3.  Accurate text recognition, supporting direct subsequent editing and format adjustment\n> 4.  All elements are independently selectable, supporting native DrawIO template replacement and layout optimization\n\n## Key Features\n\n*   **Advanced Segmentation**: Using our fine-tuned **SAM 3 (Segment Anything Model 3)** for segmentation of diagram elements.\n*   **Fixed Multi-Round VLM Scanning**: An extraction process guided by **Multimodal LLMs (Qwen-VL/GPT-4V)**.\n*   **High-Quality OCR**:\n    *   **Azure Document Intelligence** for precise text localization.\n    *   **Fallback Mechanism**: Automatically switches to VLM-based end-to-end OCR if Azure services are unreachable.\n    *   **Mistral Vision/MLLM** for correcting text and converting mathematical formulas to **LaTeX** ($\\int f(x) dx$).\n    *   **Crop-Guided Strategy**: Extracts text/formula regions and sends high-res crops to LLMs for pixel-perfect recognition.\n*   **User System**: \n    *   **Registration**: New users receive **10 free credits**.\n    *   **Credi",
      "default_branch": "main"
    },
    "fetched_at": "2026-02-08T03:50:49.262711"
  },
  {
    "basic_info": {
      "name": "NanoBanana-PPT-Skills",
      "full_name": "op7418/NanoBanana-PPT-Skills",
      "owner": "op7418",
      "description": "NanoBanana PPT Skills åŸºäº AI è‡ªåŠ¨ç”Ÿæˆé«˜è´¨é‡ PPT å›¾ç‰‡å’Œè§†é¢‘çš„å¼ºå¤§å·¥å…·ï¼Œæ”¯æŒæ™ºèƒ½è½¬åœºå’Œäº¤äº’å¼æ’­æ”¾",
      "url": "https://github.com/op7418/NanoBanana-PPT-Skills",
      "clone_url": "https://github.com/op7418/NanoBanana-PPT-Skills.git",
      "ssh_url": "git@github.com:op7418/NanoBanana-PPT-Skills.git",
      "homepage": "",
      "created_at": "2026-01-09T05:02:44Z",
      "updated_at": "2026-02-07T23:34:45Z",
      "pushed_at": "2026-01-19T03:07:23Z"
    },
    "stats": {
      "stars": 1206,
      "forks": 155,
      "watchers": 1206,
      "open_issues": 2,
      "size": 2757
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 92804,
        "HTML": 23528,
        "Shell": 6934
      },
      "license": null,
      "topics": []
    },
    "content": {
      "readme": "# NanoBanana PPT Skills\n\n> åŸºäº AI è‡ªåŠ¨ç”Ÿæˆé«˜è´¨é‡ PPT å›¾ç‰‡å’Œè§†é¢‘çš„å¼ºå¤§å·¥å…·ï¼Œæ”¯æŒæ™ºèƒ½è½¬åœºå’Œäº¤äº’å¼æ’­æ”¾\n\n<div align=\"center\">\n\n![Version](https://img.shields.io/badge/version-2.0.0-blue.svg)\n![License](https://img.shields.io/badge/license-MIT-green.svg)\n![Python](https://img.shields.io/badge/python-3.8+-green.svg)\n\n**åˆ›ä½œè€…**: [æ­¸è—](https://github.com/op7418)\n\n[æ•ˆæœæ¼”ç¤º](#-æ•ˆæœæ¼”ç¤º) â€¢ [åŠŸèƒ½ç‰¹æ€§](#-åŠŸèƒ½ç‰¹æ€§) â€¢ [ä¸€é”®å®‰è£…](#-ä¸€é”®å®‰è£…) â€¢ [ä½œä¸º Skill ä½¿ç”¨](#-ä½œä¸º-claude-code-skill-ä½¿ç”¨) â€¢ [ä½¿ç”¨æŒ‡å—](#-ä½¿ç”¨æŒ‡å—) â€¢ [è§†é¢‘åŠŸèƒ½](#-è§†é¢‘åŠŸèƒ½) â€¢ [æ¶æ„æ–‡æ¡£](ARCHITECTURE.md) â€¢ [å¸¸è§é—®é¢˜](#-å¸¸è§é—®é¢˜)\n\n</div>\n\n---\n\n## ğŸ¬ æ•ˆæœæ¼”ç¤º\n\n<div align=\"center\">\n\nhttps://github.com/user-attachments/assets/b394de21-2848-489a-8d33-a8e262e60f60\n\n*AI è‡ªåŠ¨ç”Ÿæˆ PPT å¹¶æ·»åŠ æµç•…è½¬åœºåŠ¨ç”» - ä»æ–‡æ¡£åˆ†æåˆ°è§†é¢‘åˆæˆä¸€é”®å®Œæˆ*\n\n</div>\n\n---\n\n## ğŸ“– ç®€ä»‹\n\nNanoBanana PPT Skills æ˜¯ä¸€ä¸ªå¼ºå¤§çš„ AI é©±åŠ¨çš„ PPT ç”Ÿæˆå·¥å…·ï¼Œèƒ½å¤Ÿï¼š\n\n- ğŸ“„ **æ™ºèƒ½åˆ†ææ–‡æ¡£**ï¼Œè‡ªåŠ¨æå–æ ¸å¿ƒè¦ç‚¹å¹¶è§„åˆ’ PPT ç»“æ„\n- ğŸ¨ **ç”Ÿæˆé«˜è´¨é‡å›¾ç‰‡**ï¼Œä½¿ç”¨ Google Nano Banana Proï¼ˆGemini 3 Pro Image Previewï¼‰\n- ğŸ¬ **è‡ªåŠ¨ç”Ÿæˆè½¬åœºè§†é¢‘**ï¼Œä½¿ç”¨å¯çµ AI åˆ›å»ºæµç•…çš„é¡µé¢è¿‡æ¸¡åŠ¨ç”»\n- ğŸ® **äº¤äº’å¼è§†é¢‘æ’­æ”¾å™¨**ï¼Œæ”¯æŒé”®ç›˜æ§åˆ¶ã€å¾ªç¯é¢„è§ˆã€æ™ºèƒ½è½¬åœº\n- ğŸ¥ **å®Œæ•´è§†é¢‘å¯¼å‡º**ï¼Œä¸€é”®åˆæˆåŒ…å«æ‰€æœ‰è½¬åœºçš„å®Œæ•´ PPT è§†é¢‘\n\n### ğŸ¨ è§†è§‰é£æ ¼\n\n**æ¸å˜æ¯›ç»ç’ƒå¡ç‰‡é£æ ¼**\n- é«˜ç«¯ç§‘æŠ€æ„Ÿï¼ŒApple Keynote æç®€ä¸»ä¹‰\n- 3D ç»ç’ƒç‰©ä½“ + éœ“è™¹æ¸å˜\n- ç”µå½±çº§å…‰ç…§æ•ˆæœ\n- é€‚åˆï¼šç§‘æŠ€äº§å“ã€å•†åŠ¡æ¼”ç¤ºã€æ•°æ®æŠ¥å‘Š\n\n**çŸ¢é‡æ’ç”»é£æ ¼**\n- æ¸©æš–æ‰å¹³åŒ–è®¾è®¡ï¼Œå¤å¤é…è‰²\n- é»‘è‰²è½®å»“çº¿ + å‡ ä½•åŒ–å¤„ç†\n- ç©å…·æ¨¡å‹èˆ¬çš„å¯çˆ±æ„Ÿ\n- é€‚åˆï¼šæ•™è‚²åŸ¹è®­ã€åˆ›æ„ææ¡ˆã€å“ç‰Œæ•…äº‹\n\n---\n\n## âœ¨ åŠŸèƒ½ç‰¹æ€§\n\n### ğŸ¯ æ ¸å¿ƒèƒ½åŠ›\n\n- ğŸ¤– **æ™ºèƒ½æ–‡æ¡£åˆ†æ** - è‡ªåŠ¨æå–æ ¸å¿ƒè¦ç‚¹ï¼Œè§„åˆ’ PPT å†…å®¹ç»“æ„\n- ğŸ¨ **å¤šé£æ ¼æ”¯æŒ** - å†…ç½® 2 ç§ä¸“ä¸šé£æ ¼ï¼Œå¯æ— é™æ‰©å±•\n- ğŸ–¼ï¸ **é«˜è´¨é‡å›¾ç‰‡** - 16:9 æ¯”ä¾‹ï¼Œ2K/4K åˆ†è¾¨ç‡å¯é€‰\n- ğŸ¬ **AI è½¬åœºè§†é¢‘** - å¯çµ AI ç”Ÿæˆæµç•…çš„é¡µé¢è¿‡æ¸¡åŠ¨ç”»\n- ğŸ® **äº¤äº’å¼æ’­æ”¾å™¨** - è§†é¢‘+å›¾ç‰‡æ··åˆæ’­æ”¾ï¼Œæ”¯æŒé”®ç›˜å¯¼èˆª\n- ğŸ¥ **å®Œæ•´è§†é¢‘å¯¼å‡º** - FFmpeg åˆæˆåŒ…å«è½¬åœºçš„å®Œæ•´ PPT è§†é¢‘\n- ğŸ“Š **æ™ºèƒ½å¸ƒå±€** - å°é¢é¡µã€å†…å®¹é¡µã€æ•°æ®é¡µè‡ªåŠ¨è¯†åˆ«\n- âš¡ **å¿«é€Ÿç”Ÿæˆ** - 2K çº¦ 30 ç§’/é¡µ\n\n### ğŸ†• è§†é¢‘åŠŸèƒ½ï¼ˆv2.0ï¼‰\n\n- ğŸ¬ **é¦–é¡µå¾ªç¯é¢„è§ˆ** - è‡ªåŠ¨ç”Ÿæˆå¸å¼•çœ¼çƒçš„å¾ªç¯åŠ¨ç”»\n- ğŸï¸ **æ™ºèƒ½è½¬åœº** - è‡ªåŠ¨ç”Ÿæˆé¡µé¢é—´çš„è¿‡æ¸¡è§†é¢‘\n- ğŸ® **äº¤äº’å¼æ’­æ”¾** - æŒ‰é”®ç¿»é¡µæ—¶æ’­æ”¾è½¬åœºè§†é¢‘ï¼Œç»“æŸåæ˜¾ç¤ºé™æ€å›¾ç‰‡\n- ğŸ¥ **å®Œæ•´è§†é¢‘å¯¼å‡º** - åˆæˆåŒ…å«æ‰€æœ‰è½¬åœºå’Œé™æ€é¡µçš„å®Œæ•´è§†é¢‘\n- ğŸ”§ **å‚æ•°ç»Ÿä¸€** - è‡ªåŠ¨ç»Ÿä¸€æ‰€æœ‰è§†é¢‘åˆ†è¾¨ç‡å’Œå¸§ç‡ï¼Œç¡®ä¿æµç•…æ’­æ”¾\n\n### ğŸ› ï¸ æŠ€æœ¯äº®ç‚¹\n\n- âœ… Google Nano Banana Proï¼ˆGemini 3 Pro Image Previewï¼‰å›¾åƒç”Ÿæˆ\n- âœ… å¯çµ AI API é›†æˆï¼ˆè§†é¢‘ç”Ÿæˆã€æ•°å­—äººã€ä¸»ä½“åº“ï¼‰\n- âœ… FFmpeg è§†é¢‘åˆæˆä¸å‚æ•°ç»Ÿä¸€\n- âœ… å®Œæ•´çš„æç¤ºè¯å·¥ç¨‹å’Œé£æ ¼ç®¡ç†ç³»ç»Ÿ\n- âœ… å®‰å…¨çš„ .env ç¯å¢ƒå˜é‡ç®¡ç†\n- âœ… æ¨¡å—åŒ–è®¾è®¡ï¼Œæ˜“äºæ‰©å±•\n\n---\n\n## ğŸš€ ä¸€é”®å®‰è£…\n\n### æ–¹æ³•ä¸€ï¼šClaude Code è‡ªåŠ¨å®‰è£…ï¼ˆæ¨èï¼‰\n\n**åªéœ€å¤åˆ¶ä»¥ä¸‹æç¤ºè¯ï¼Œå‘é€ç»™ Claude Codeï¼Œå®ƒä¼šè‡ªåŠ¨å®Œæˆå…¨éƒ¨å®‰è£…ï¼**\n\n```\nè¯·å¸®æˆ‘å®‰è£… NanoBanana PPT Skillsï¼š\n\n1. å…‹éš†é¡¹ç›®å¹¶è¿›å…¥ç›®å½•ï¼š\n   git clone https://github.com/op7418/NanoBanana-PPT-Skills.git\n   cd NanoBanana-PPT-Skills\n\n2. åˆ›å»º Python è™šæ‹Ÿç¯å¢ƒï¼š\n   python3 -m venv venv\n   source venv/bin/activate  # Windows: venv\\Scripts\\activate\n\n3. å®‰è£…ä¾èµ–ï¼š\n   pip install google-genai pillow python-dotenv\n\n4. é…ç½® API å¯†é’¥ - åˆ›å»º .env æ–‡ä»¶ï¼š\n   cp .env.example .env\n\n5. ç¼–è¾‘ .env æ–‡ä»¶ï¼Œå¡«å…¥æˆ‘çš„ API å¯†é’¥ï¼š\n\n   GEMINI_API_KEY=YOUR_GEMINI_API_KEY\n   KLING_ACCESS_KEY=YOUR_KLING_ACCESS_KEY\n   KLING_SECRET_KEY=YOUR_KLING_SECRET_KEY\n\n   æ³¨æ„ï¼š\n   - GEMINI_API_KEY: Google AI API å¯†é’¥ï¼ˆå¿…éœ€ï¼Œç”¨äºç”Ÿæˆ PPT å›¾ç‰‡ï¼‰\n   - KLING_ACCESS_KEY å’Œ KLING_SECRET_KEY: å¯çµ AI å¯†é’¥ï¼ˆå¯é€‰ï¼Œç”¨äºç”Ÿæˆè½¬åœºè§†é¢‘ï¼‰\n\n6. éªŒè¯å®‰è£…ï¼š\n   python3 generate_ppt.py --help\n\nå®Œæˆåï¼Œå‘Šè¯‰æˆ‘å®‰è£…ç»“æœå’Œå¦‚ä½•ä½¿ç”¨ã€‚\n\næˆ‘çš„ API å¯†é’¥ï¼š\n- GEMINI_API_KEY: YOUR_GEMINI_API_KEY_HERE\n- KLING_ACCESS_KEY: YOUR_KLING_ACCESS_KEY_HERE (å¯é€‰)\n- KLING_SECRET_KEY: YOUR_KLING_SECRET_KEY_HERE (å¯é€‰)\n```\n\n**ä½¿ç”¨è¯´æ˜**ï¼š\n1. å…ˆè·å– API å¯†é’¥ï¼š\n   - **å¿…éœ€**: [Google AI API å¯†é’¥](https://aistudio.google.com/apikey)\n   - **å¯é€‰**: [å¯çµ AI API å¯†é’¥](https://klingai.com)ï¼ˆç”¨äºè§†é¢‘è½¬åœºåŠŸèƒ½ï¼‰\n2. å¤åˆ¶ä¸Šé¢çš„æç¤ºè¯\n3. å°† `YOUR_GEMINI_API_KEY_HERE` ç­‰æ›¿æ¢ä¸ºä½ çš„çœŸå® API å¯†é’¥\n4. å‘é€ç»™ Claude Code\n5. Claude Code ä¼šè‡ªåŠ¨æ‰§è¡Œæ‰€æœ‰å®‰è£…æ­¥éª¤å¹¶å‘ŠçŸ¥ç»“æœ\n\n### æ–¹æ³•äºŒï¼šæ‰‹åŠ¨å®‰è£…\n\nå¦‚æœä½ æƒ³æ‰‹åŠ¨å®‰è£…ï¼ŒæŒ‰ç…§ä»¥ä¸‹æ­¥éª¤æ“ä½œï¼š\n\n#### 1. å…‹éš†é¡¹ç›®\n\n```bash\ngit clone https://github.com/op7418/NanoBanana-PPT-Skills.git\ncd NanoBanana-PPT-Skills\n```\n\n#### 2. åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ\n\n```bash\npython3 -m venv venv\nsource venv/bin/activate  # Windows: venv\\Scripts\\activate\n```\n\n#### 3. å®‰è£…ä¾èµ–\n\n```bash\npip install google-genai pillow\n```\n\nå¦‚æœéœ€è¦è§†é¢‘åŠŸèƒ½ï¼Œè¿˜éœ€è¦å®‰è£… FFmpegï¼š\n\n```bash\n# macOS\nbrew install ffmpeg\n\n# Ubuntu/Debian\nsudo apt-get install ffmpeg\n\n# Windows\n# ä¸‹è½½ FFmpeg å¹¶æ·»åŠ åˆ°ç³»ç»Ÿ PATH\n```\n\n#### 4. é…ç½® API å¯†é’¥\n\n**æ¨èæ–¹å¼ï¼š.env æ–‡ä»¶ï¼ˆå·²å†…ç½®æ”¯æŒï¼‰**\n\n```bash\n# å¤åˆ¶ç¤ºä¾‹æ–‡ä»¶\ncp .env.example .env\n\n# ç¼–è¾‘ .env æ–‡ä»¶\nnano .env  # æˆ–ä½¿ç”¨ä½ å–œæ¬¢çš„ç¼–è¾‘å™¨\n```\n\nåœ¨ `.env` æ–‡ä»¶ä¸­å¡«å…¥ä½ çš„ API å¯†é’¥ï¼š\n\n```bash\n# Google AI API å¯†é’¥ï¼ˆå¿…éœ€ï¼‰\nGEMINI_API_KEY=your_gemini_api_key_here\n\n# å¯çµ AI API å¯†é’¥ï¼ˆå¯é€‰ï¼Œç”¨äºè§†é¢‘è½¬åœºåŠŸèƒ½ï¼‰\nKLING_ACCESS_KEY=your_kling_access_key_here\nKLING_SECRET_KEY=your_kling_secret_key_here\n```\n\n**æ›¿ä»£æ–¹å¼ï¼šç³»ç»Ÿç¯å¢ƒå˜é‡**\n\n```bash\n# zsh ç”¨æˆ· (macOS é»˜è®¤)\necho 'export GEMINI_API_KEY=\"your-api-key-here\"' >> ~/.zshrc\nsource ~/.zshrc\n\n# bash ç”¨æˆ·\necho 'export GEMINI_API_KEY=\"your-api-key-here\"' >> ~/.bashrc\nsource ~/.bashrc\n```\n\n#### 5. éªŒè¯å®‰è£…\n\n```bash\npython3 generate_ppt.py --help\n```\n\nåº”è¯¥æ˜¾ç¤ºå¸®åŠ©ä¿¡æ¯ï¼Œè¡¨ç¤ºå®‰è£…æˆåŠŸã€‚\n\n---\n\n## ğŸ¯ ä½œä¸º Claude Code Skill ä½¿ç”¨\n\nNanoBanana PPT Skills å®Œå…¨æ”¯æŒ Claude Code Skill æ ‡å‡†ï¼Œå¯ä»¥ç›´æ¥é€šè¿‡ Claude Code è°ƒç”¨ã€‚\n\n### å¿«é€Ÿå®‰è£…ä¸º Skill\n\n**æ–¹æ³•ä¸€ï¼šClaude Code è‡ªåŠ¨å®‰è£…ä¸º Skillï¼ˆæœ€ç®€å•ï¼‰**\n\n**åªéœ€å¤åˆ¶ä»¥ä¸‹æç¤ºè¯ï¼Œå‘é€ç»™ Claude Codeï¼Œå®ƒä¼šè‡ªåŠ¨å®Œæˆ Skill å®‰è£…ï¼**\n\n```\nè¯·å¸®æˆ‘å°† NanoBanana PPT Skills å®‰è£…ä¸º Claude Code Skillï¼š\n\n1. åˆ›å»º Skill ç›®å½•ï¼š\n   mkdir -p ~/.claude/skills/ppt-generator\n\n2. å…‹éš†é¡¹ç›®åˆ° Skill ç›®å½•ï¼š\n   git clone https://github.com/op7418/NanoBanana-PPT-Skills.git ~/.claude/skills/ppt-generator\n\n3. è¿›å…¥ç›®å½•å¹¶å®‰è£…ä¾èµ–ï¼š\n   cd ~/.claude/skills/ppt-generator\n   python3 -m venv venv\n   source venv/bin/activate\n   pip install google-genai pillow python-dotenv\n\n4. é…ç½® API å¯†é’¥ï¼š\n   cp .env.example .env\n\n   ç„¶åç¼–è¾‘ .env æ–‡ä»¶ï¼Œå¡«å…¥æˆ‘çš„ API å¯†é’¥ï¼š\n   GEMINI_API_KEY=YOUR_GEMINI_API_KEY\n   KLING_ACCESS_KEY=YOUR_KLING_ACCESS_KEY\n   KLING_SECRET_KEY=YOUR_KLING_SECRET_KEY\n\n5. éªŒè¯å®‰è£…ï¼š\n   python3 generate_ppt.py --help\n\nå®Œæˆåï¼Œå‘Šè¯‰æˆ‘å¦‚ä½•åœ¨ Claude Code ä¸­ä½¿ç”¨è¿™ä¸ª Skillã€‚\n\næˆ‘çš„ API å¯†é’¥ï¼š\n- GEMINI_API",
      "default_branch": "main"
    },
    "fetched_at": "2026-02-08T03:50:50.480236"
  },
  {
    "basic_info": {
      "name": "Youtube-clipper-skill",
      "full_name": "op7418/Youtube-clipper-skill",
      "owner": "op7418",
      "description": null,
      "url": "https://github.com/op7418/Youtube-clipper-skill",
      "clone_url": "https://github.com/op7418/Youtube-clipper-skill.git",
      "ssh_url": "git@github.com:op7418/Youtube-clipper-skill.git",
      "homepage": null,
      "created_at": "2026-01-22T03:28:37Z",
      "updated_at": "2026-02-07T22:49:41Z",
      "pushed_at": "2026-01-22T04:56:21Z"
    },
    "stats": {
      "stars": 1086,
      "forks": 185,
      "watchers": 1086,
      "open_issues": 6,
      "size": 56
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 57805,
        "Shell": 6975
      },
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "# YouTube Clipper Skill\n\n> AI-powered YouTube video clipper for Claude Code. Download videos, generate semantic chapters, clip segments, translate subtitles to bilingual format, and burn subtitles into videos.\n\n[![License](https://img.shields.io/badge/license-MIT-blue.svg)](LICENSE)\n[![Python](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)\n\nEnglish | [ç®€ä½“ä¸­æ–‡](README.zh-CN.md)\n\n[Features](#features) â€¢ [Installation](#installation) â€¢ [Usage](#usage) â€¢ [Requirements](#requirements) â€¢ [Configuration](#configuration) â€¢ [Troubleshooting](#troubleshooting)\n\n---\n\n## Features\n\n- **AI Semantic Analysis** - Generate fine-grained chapters (2-5 minutes each) by understanding video content, not just mechanical time splitting\n- **Precise Clipping** - Use FFmpeg to extract video segments with frame-accurate timing\n- **Bilingual Subtitles** - Batch translate subtitles to Chinese/English with 95% API call reduction\n- **Subtitle Burning** - Hardcode bilingual subtitles into videos with customizable styling\n- **Content Summarization** - Auto-generate social media content (Xiaohongshu, Douyin, WeChat)\n\n---\n\n## Installation\n\n### Option 1: npx skills (Recommended)\n\n```bash\nnpx skills add https://github.com/op7418/Youtube-clipper-skill\n```\n\nThis command will automatically install the skill to `~/.claude/skills/youtube-clipper/`.\n\n### Option 2: Manual Installation\n\n```bash\ngit clone https://github.com/op7418/Youtube-clipper-skill.git\ncd Youtube-clipper-skill\nbash install_as_skill.sh\n```\n\nThe install script will:\n- Copy files to `~/.claude/skills/youtube-clipper/`\n- Install Python dependencies (yt-dlp, pysrt, python-dotenv)\n- Check system dependencies (Python, yt-dlp, FFmpeg)\n- Create `.env` configuration file\n\n---\n\n## Requirements\n\n### System Dependencies\n\n| Dependency | Version | Purpose | Installation |\n|------------|---------|---------|--------------|\n| **Python** | 3.8+ | Script execution | [python.org](https://www.python.org/downloads/) |\n| **yt-dlp** | Latest | YouTube download | `brew install yt-dlp` (macOS)<br>`sudo apt install yt-dlp` (Ubuntu)<br>`pip install yt-dlp` (pip) |\n| **FFmpeg with libass** | Latest | Video processing & subtitle burning | `brew install ffmpeg-full` (macOS)<br>`sudo apt install ffmpeg libass-dev` (Ubuntu) |\n\n### Python Packages\n\nThese are automatically installed by the install script:\n- `yt-dlp` - YouTube downloader\n- `pysrt` - SRT subtitle parser\n- `python-dotenv` - Environment variable management\n\n### Important: FFmpeg libass Support\n\n**macOS users**: The standard `ffmpeg` package from Homebrew does NOT include libass support (required for subtitle burning). You must install `ffmpeg-full`:\n\n```bash\n# Remove standard ffmpeg (if installed)\nbrew uninstall ffmpeg\n\n# Install ffmpeg-full (includes libass)\nbrew install ffmpeg-full\n```\n\n**Verify libass support**:\n```bash\nffmpeg -filters 2>&1 | grep subtitles\n# Should output: subtitles    V->V  (...)\n```\n\n---\n\n## Usage\n\n### In Claude Code\n\nSimply tell Claude to clip a YouTube video:\n\n```\nClip this YouTube video: https://youtube.com/watch?v=VIDEO_ID\n```\n\nor\n\n```\nå‰ªè¾‘è¿™ä¸ª YouTube è§†é¢‘ï¼šhttps://youtube.com/watch?v=VIDEO_ID\n```\n\n### Workflow\n\n1. **Environment Check** - Verifies yt-dlp, FFmpeg, and Python dependencies\n2. **Video Download** - Downloads video (up to 1080p) and English subtitles\n3. **AI Chapter Analysis** - Claude analyzes subtitles to generate semantic chapters (2-5 min each)\n4. **User Selection** - Choose which chapters to clip and processing options\n5. **Processing** - Clips video, translates subtitles, burns subtitles (if requested)\n6. **Output** - Organized files in `./youtube-clips/<timestamp>/`\n\n### Output Files\n\nFor each clipped chapter:\n\n```\n./youtube-clips/20260122_143022/\nâ””â”€â”€ Chapter_Title/\n    â”œâ”€â”€ Chapter_Title_clip.mp4              # Original clip (no subtitles)\n    â”œâ”€â”€ Chapter_Title_with_subtitles.mp4    # With burned subtitles\n    â”œâ”€â”€ Chapter_Title_bilingual.srt         # Bilingual subtitle file\n    â””â”€â”€ Chapter_Title_summary.md            # Social media content\n```\n\n---\n\n## Configuration\n\nThe skill uses environment variables for customization. Edit `~/.claude/skills/youtube-clipper/.env`:\n\n### Key Settings\n\n```bash\n# FFmpeg path (auto-detected if empty)\nFFMPEG_PATH=\n\n# Output directory (default: current working directory)\nOUTPUT_DIR=./youtube-clips\n\n# Video quality limit (720, 1080, 1440, 2160)\nMAX_VIDEO_HEIGHT=1080\n\n# Translation batch size (20-25 recommended)\nTRANSLATION_BATCH_SIZE=20\n\n# Target language for translation\nTARGET_LANGUAGE=ä¸­æ–‡\n\n# Target chapter duration in seconds (180-300 recommended)\nTARGET_CHAPTER_DURATION=180\n```\n\nFor full configuration options, see [.env.example](.env.example).\n\n---\n\n## Examples\n\n### Example 1: Extract highlights from a tech interview\n\n**Input**:\n```\nClip this video: https://youtube.com/watch?v=Ckt1cj0xjRM\n```\n\n**Output** (AI-generated chapters):\n```\n1. [00:00 - 03:15] AGI as an exponential curve, not a point in time\n2. [03:15 - 06:30] China's gap in AI development\n3",
      "default_branch": "main"
    },
    "fetched_at": "2026-02-08T03:50:51.729994"
  }
]