[
  {
    "basic_info": {
      "name": "GuitarPedal",
      "full_name": "torvalds/GuitarPedal",
      "owner": "torvalds",
      "description": null,
      "url": "https://github.com/torvalds/GuitarPedal",
      "clone_url": "https://github.com/torvalds/GuitarPedal.git",
      "ssh_url": "git@github.com:torvalds/GuitarPedal.git",
      "homepage": null,
      "created_at": "2025-09-17T01:01:29Z",
      "updated_at": "2025-09-18T23:07:23Z",
      "pushed_at": "2025-09-18T23:07:20Z"
    },
    "stats": {
      "stars": 440,
      "forks": 10,
      "watchers": 440,
      "open_issues": 2,
      "size": 1853
    },
    "tech_info": {
      "language": null,
      "languages": {},
      "license": "GNU General Public License v2.0",
      "topics": []
    },
    "content": {
      "readme": "## Random guitar pedal board design\n\n### Background\n\nThis is a personal toy project that has gone through several phases, but\nthe common theme has been that it makes absolutely no sense outside of\nthe very specific niche of \"Linus is trying to learn random things about\nelectronics\".\n\nSo keep that in mind: there is very little point to any of this to\nanybody else.  Don't expect some great useful guitar pedal experience.\n\nI call it my \"LEGO for adults\" hobby, because this got started when I\nwanted to extend my traditional after-Christmas activity (which was\nreceiving and building _actual_ LEGO kits, which has been a thing for me\nsince I was a wee tyke) with something else.\n\nSo for Christmas 2024, I got a new soldering iron and randomly started\ndoing guitar pedal kits.  And so over the next month or two, I built at\nleast two dozen kits, and had to literally look for victims to give them\naway to because I had no use for them myself.\n\n> [!NOTE]\n> Of all the kits I built, the ones I enjoyed the most were the Aion FX\n> ones, and if you are looking for a kit build of traditional analog\n> guitar pedals, I can heartily recommend them.\n>\n> The documentation, the customer service, the components, and the\n> enclosures were all top notch. See [\"Aion FX\"](https://aionfx.com/)\n\nAnyway, after building a lot of these traditional analog guitar pedal\nkits I decided I really wanted to actually understand what they did,\nbecause I really had very little experience with any analog circuits.\n\nWhile I've done some very limited electronics most of my life, almost\nall of it has been related to computers, so it's been either digital\nlogic or power supplies for them.\n\nAlso, I was looking for a different kind of soldering experience where\nthere was less snipping of legs of through-hole components.  I actually\nlike soldering SMT components, but that doesn't tend to be what those\nguitar pedal kits do.\n\nI had done some very limited PCB design with kicad a few years ago, so I\ndecided to just start learning more about analog circuits.  And then it\nkind of grew from that.\n\n### Electrical design\n\nThis is the \"fourth generation\" of my guitar pedal design journey, and\nis a new repository because the goal of the learning experience has\nevolved.\n\nWhat started out being about the analog circuits (and the power rails:\nthose were always a big thing) got to the point where I realized I\nreally want to do a mixed signal design: understanding what the circuits\ndo is one thing, re-creating some analog design from the 70s when you\ndon't actually care about the sound is another thing entirely.\n\nAlso, on the actual analog signal side, I started out using op-amps, but\nas I was attempting to learn how things actually worked, I had switched\nover to a \"discrete components only\" model, and this continues that\ntrend (except for the whole digital side, of course).\n\n> [!NOTE]\n> To me \"discrete components\" does include more optimized packages:\n> things like dual diodes or matched transistors, but not more complex\n> circuits like a op-amp (or a 555 timer or D Flip-flop or other classic\n> logic IC)\n\nAlso, because I don't typically *listen* to the end result, but look at\nit with a signal generator and an oscilloscope, I've grown to detest\npower supply noise.\n\nNot knowing what I was doing, quite a lot of my circuits have been very\nnoisy indeed, and have coupled in noise from the power supply into the\nsignal chain, and you can really see that on an oscilloscope even when\nit's not always audible.\n\nEven in op-amp designs, where the op-amp itself has a very high PSRR and\nisn't mixing power supply noise into the signal, my biasing circuits\nwere often not great, and so the op-amp would see not just the signal\nbut the power supply noise coming in through the DC biasing.\n\nAnd every time I tried a dual power rail (so that I could just keep the\nsignal ground-referenced), the noise from the switching ended up just\nalways noticeable, and the extra complexity was annoying when a lot of\neffects then didn't have any real use for the dual rail.\n\nFiltering obviously helps, but this is just a long-winded explanation\nfor why I ended up really appreciating the \"bias to ground\" JFET model\nfor the signal input side, and the common drain follower in particular.\n\nThat works with a single JFET (the MMBF5103 worked well for me), but my\nfavorite design so far is a dual-JFET LS844 with the second matched JFET\nused as a current sink.  It has basically infinite input impedance (and\ncould be DC coupled, although I do the coupling capacitor with resistor\nto ground) and gives a good output signal somewhere roughly in the\nmiddle of the single-supply 9V rail.\n\nSee [LS844 Application note](https://www.linearsystems.com/_files/ugd/7e8069_52b1022fbded45fab609459acb337629.pdf)\n\nWhy do I mention this in particular? Mainly because it's a great example\nof how completely *insane* my designs are.  That LS844 is used as a\nvoltage follower with a noticeable DC offset, and that single dual-JFET\nSOT-23-6 component is m",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-18T23:11:06.390286"
  },
  {
    "basic_info": {
      "name": "comfydeploy",
      "full_name": "comfy-deploy/comfydeploy",
      "owner": "comfy-deploy",
      "description": "ComfyDeployed",
      "url": "https://github.com/comfy-deploy/comfydeploy",
      "clone_url": "https://github.com/comfy-deploy/comfydeploy.git",
      "ssh_url": "git@github.com:comfy-deploy/comfydeploy.git",
      "homepage": null,
      "created_at": "2025-09-17T02:53:50Z",
      "updated_at": "2025-09-18T22:55:29Z",
      "pushed_at": "2025-09-18T16:31:41Z"
    },
    "stats": {
      "stars": 283,
      "forks": 42,
      "watchers": 283,
      "open_issues": 0,
      "size": 327
    },
    "tech_info": {
      "language": null,
      "languages": {},
      "license": "GNU General Public License v3.0",
      "topics": []
    },
    "content": {
      "readme": "# ComfyDeploy\n\nRe: Open-sourcing ComfyDeploy\n\nTL;DR: We are open-sourcing ComfyDeploy again, with full platform backend + frontend.\n\n[Discord](https://discord.gg/qtHUaVNRVM) | [Website](https://www.comfydeploy.com)\n\n---\n\n### How we got here?\n\nIn late 2023, I started ComfyDeploy as an open source project while I was working at my previous company. We had a problem deploying ComfyUI to our production server due to how complicated it is to get ComfyUI into a serverless environment.\n\nLittle did I know I was going to meet my co-founder Nick and embark on a journey I never dreamt of.\n\nI posted on Twitter about this little project that I was working on as an indie hacker, and it blew up overnight. I woke up to 100k impressions on the post. I put up my cal link and people started scheduling calls. I got to talk to a bunch of people across the globe, with people reaching out to help or potentially use ComfyDeploy.\n\nNick was one of the early contributors, and later we decided to apply to a bunch of accelerators together. It was early February, just a couple of weeks after we met. We applied to YC—we were unsure if we would even get in, but we still decided to quit our jobs, and the day after we quit, we got in.\n\nIt was insane to even think that we would have had a chance.\n\n### Life Changing Decision\n\nWe immediately said yes.\n\nWe got into YC with ComfyDeploy around 2.5K MRR.\n\nAround the same timeframe, ComfyOrg was introduced, Stability collapsed, and Flux just came out.\n\nWe kept building ComfyDeploy, for months, and kept things going.\n\nThe company was growing, but very slowly. We realized the biggest issue is that we are still really early, and it takes time for businesses and enterprises to really adopt such a niche tool. And we are not ComfyOrg.\n\n### Dynamic Changes\n\nMeanwhile, closed source models dropped, and a bunch of workflows that we knew became no longer useful.\n\nComing from a game developer background, I saw huge potential with ComfyUI at first, but I never would have imagined that one giant model could do exactly what you put into words, and you still need workflows to fine-tune and control the exact outputs. And ComfyDeploy did make it possible for teams to experiment with this.\n\nWe were just unsure about the future of the company. While we kept getting more and more business inquiries about ComfyUI, this is what really kept us going before giving up. It's a mixed signal.\n\nMeanwhile, my friends bootstrapped their mobile app to $500k a month. I was really stressed—were we going to live up to the aspirations that we set out, both personally and for the company?\n\n### Problems\n\nI feel immensely grateful for the customers that we were able to help, and this is something I feel forever privileged to experience—working with the best creative teams across the globe and brands that I look up to.\n\nBut we were stuck in the middle. First, we are not ComfyOrg; second, closed source models were doing things way better and slowly eating up the market.\n\nAnd it's inevitable that ComfyOrg will have some sort of cloud solution. And we do appreciate all the work ComfyOrg has put into making ComfyUI great again. We realized we would be competing in some ways especially in the cloud space, and we could never grow out of this, which constrained the company's growth at the same time.\n\nAs of today, ComfyDeploy is doing $29k MRR, and our last 30 days' revenue was $50k processed. Which is the highest we have ever got, but also the most depressing day I have ever had.\n\n### So what now?\n\nWe have been working for months on our pay-as-you-go tier—no longer requiring you to talk to us, just pay for the cloud resources you use. And also, going back to our roots: open-sourcing the ENTIRE CLOUD PLATFORM while continuing to support existing customers.\n\nOur current customers and creative teams are the people who kept us going for the last year, and I will be forever grateful. But this might just not be the field for us, and we respect Comfy and do not want to get in the way. And I think it's going to be really good for everyone.\n\n#### What does this mean for current customers: \n\nThe service will stay online and supported until the last standing user!\n\n#### What does this mean for future customers: \nYou can either go with the pay-as-you-go tier to get started, or self-host the entire platform. If ComfyOrg eventually builds something like ComfyDeploy, the official solution will be recommended by us!\n\n#### What does this mean for ComfyDeploy team: \n\nWe will continue supporting existing customers. At the same time we will start exploring new ideas. And this does not mean the end to us. But rather a fresh start. Stay tuned for what's coming next.\n\n#### What I will be doing for the next couple days\n1. Documentation and tutorials to set up ComfyDeploy\n2. Explore new problems that could resonate with us.\n\n### Credits\n\nA list of credits that got us here (doesn't go in order)\nComfy, Nick, Karrix, Jeff, Edgar, Ecjojo, Brad, Aashay and Semil, Jon, Choco",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-18T23:11:07.598564"
  },
  {
    "basic_info": {
      "name": "pingoo",
      "full_name": "pingooio/pingoo",
      "owner": "pingooio",
      "description": "The fast and secure Load Balancer / API Gateway / Reverse Proxy with built-in service discovery, GeoIP, WAF, bot protection and much more - https://pingoo.io",
      "url": "https://github.com/pingooio/pingoo",
      "clone_url": "https://github.com/pingooio/pingoo.git",
      "ssh_url": "git@github.com:pingooio/pingoo.git",
      "homepage": "https://pingoo.io",
      "created_at": "2025-09-17T07:18:40Z",
      "updated_at": "2025-09-18T22:42:55Z",
      "pushed_at": "2025-09-18T09:23:28Z"
    },
    "stats": {
      "stars": 135,
      "forks": 5,
      "watchers": 135,
      "open_issues": 6,
      "size": 331
    },
    "tech_info": {
      "language": "Rust",
      "languages": {
        "Rust": 183548,
        "TypeScript": 7020,
        "Dockerfile": 6485,
        "Makefile": 2265,
        "Shell": 1620,
        "HTML": 892,
        "CSS": 620,
        "Vim Script": 19
      },
      "license": "MIT License",
      "topics": [
        "akamai",
        "anti-bot",
        "apache2",
        "api",
        "api-gateway",
        "captcha",
        "cloudflare",
        "fastly",
        "firewall",
        "haproxy",
        "load-balancer",
        "nginx",
        "pingoo",
        "proxy",
        "quic",
        "reverse-proxy",
        "rust",
        "security",
        "service-discovery",
        "waf"
      ]
    },
    "content": {
      "readme": "<p align=\"center\">\n  <a href=\"https://pingoo.io\" target=\"_blank\" rel=\"noopener\"><img alt=\"Pingoo logo\" src=\"https://pingoo.io/icon-256.png\" height=\"128\" /></a>\n  <h1 align=\"center\">Pingoo</h1>\n  <h3 align=\"center\">The fast and secure Load Balancer / API Gateway / Reverse Proxy with built-in service discovery, GeoIP, WAF, bot protection and much more</h3>\n  <h3 align=\"center\">\n    <a href=\"https://pingoo.io\">Documentation</a> | <a href=\"https://kerkour.com/announcing-pingoo\">Read the launch post</a>\n  </h3>\n</p>\n\nOpen Source load balancers and reverse proxies are stuck in the past century with a very slow pace of development and most of the important features reserved for \"Enterprise Editions\" which lead developers to use third-party cloud services, exposing their users' traffic to legal, security and reliability risks.\n\nPingoo is a modern Load Balancer / API Gateway / Reverse Proxy that run on your own servers and already have (or will have soon) all the features you expect from managed services and even more. All of that with a huge boost in performance and security thanks to reduced latency and, of course, Rust ;)\n\n* Service Discovery (Docker, DNS...)\n* Web Application Firewall (WAF)\n* Easy compliance because the data never leaves your servers\n* Bot protection and management\n* TCP proxying\n* Post-Quantum TLS\n* GeoIP (country, ASN)\n* Static sites\n* And much more\n\n> ⚠️ Pingoo is currently in beta, use with caution.\n\n## Quickstart\n\n```bash\n# You have a static site in the www folder\n$ ls www\nindex.html\n$ docker run --rm -ti --network host -v `pwd`/www:/var/wwww ghcr.io/pingooio/pingoo\n# Pingoo is now listenning on http://0.0.0.0:8080\n```\n\n## Documentation\n\nSee https://pingoo.io\n\n\n## Updates\n\n[Click Here](https://kerkour.com/blog) to visit the blog and [subscribe](https://kerkour.com/subscribe) by RSS or email to get weekly / monthly updates. No spam ever, only technical deep dives.\n\n\n## Contributing\n\nPlease open an issue to discuss your idea before submitting a Pull Request.\n\n\n## Support\n\nDo you have custom needs? Do you want your features to be prioritized? Are you under attack and need help? Do you need support for deploying and self-hosting Pingoo?\n\nFeel free to reach our team of experts to see how we can help: https://pingoo.io/contact\n\n\n## Security\n\nWe are committed to make Pingoo the most secure Load Balancer / Reverse Proxy in the universe and beyond. If you've found a security issue in Pingoo, we appreciate your help in disclosing it to us in a responsible manner by contacting us: https://pingoo.io/contact\n\n\n## License\n\nMIT. See `LICENSE.txt`\n\nForever Open Source. No Open Core or \"Enterprise Edition\".\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-18T23:11:08.819977"
  },
  {
    "basic_info": {
      "name": "gemini_icpc2025",
      "full_name": "google-deepmind/gemini_icpc2025",
      "owner": "google-deepmind",
      "description": "Gemini 2025 ICPC World Finals Code Submissions",
      "url": "https://github.com/google-deepmind/gemini_icpc2025",
      "clone_url": "https://github.com/google-deepmind/gemini_icpc2025.git",
      "ssh_url": "git@github.com:google-deepmind/gemini_icpc2025.git",
      "homepage": "",
      "created_at": "2025-09-17T10:57:24Z",
      "updated_at": "2025-09-18T23:05:03Z",
      "pushed_at": "2025-09-17T11:09:46Z"
    },
    "stats": {
      "stars": 102,
      "forks": 7,
      "watchers": 102,
      "open_issues": 0,
      "size": 27
    },
    "tech_info": {
      "language": "C++",
      "languages": {
        "C++": 54237
      },
      "license": "Apache License 2.0",
      "topics": []
    },
    "content": {
      "readme": "# Gemini ICPC 2025 Submissions\n\nThis repository contains the code submissions from an advanced version of\n[Gemini 2.5 Deep Think](https://blog.google/products/gemini/gemini-2-5-deep-think/)\nfor the 2025 International Collegiate Programming Contest World Finals.\n\n## License and disclaimer\n\nCopyright 2025 Google LLC\n\nAll software is licensed under the Apache License, Version 2.0 (Apache 2.0);\nyou may not use this file except in compliance with the Apache 2.0 license.\nYou may obtain a copy of the Apache 2.0 license at:\nhttps://www.apache.org/licenses/LICENSE-2.0\n\nAll other materials are licensed under the Creative Commons Attribution 4.0\nInternational License (CC-BY). You may obtain a copy of the CC-BY license at:\nhttps://creativecommons.org/licenses/by/4.0/legalcode\n\nUnless required by applicable law or agreed to in writing, all software and\nmaterials distributed here under the Apache 2.0 or CC-BY licenses are\ndistributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,\neither express or implied. See the licenses for the specific language governing\npermissions and limitations under those licenses.\n\nThis is not an official Google product.\n",
      "default_branch": "release"
    },
    "fetched_at": "2025-09-18T23:11:10.040047"
  },
  {
    "basic_info": {
      "name": "CLOV",
      "full_name": "Aihy/CLOV",
      "owner": "Aihy",
      "description": "Valuation of tokens corresponding to influential individuals on social platforms through AI algorithms",
      "url": "https://github.com/Aihy/CLOV",
      "clone_url": "https://github.com/Aihy/CLOV.git",
      "ssh_url": "git@github.com:Aihy/CLOV.git",
      "homepage": "",
      "created_at": "2025-09-17T08:06:46Z",
      "updated_at": "2025-09-18T23:05:57Z",
      "pushed_at": "2025-09-17T08:14:22Z"
    },
    "stats": {
      "stars": 73,
      "forks": 12,
      "watchers": 73,
      "open_issues": 0,
      "size": 2988
    },
    "tech_info": {
      "language": "TypeScript",
      "languages": {
        "TypeScript": 462909,
        "CSS": 12292,
        "JavaScript": 372
      },
      "license": "MIT License",
      "topics": [
        "ai",
        "ai-agents",
        "chatbot"
      ]
    },
    "content": {
      "readme": "# Valuation of Social Media Influencers' Tokens with AI\n\nThis project explores the **valuation of tokens corresponding to influential individuals on social platforms**. The platform allows users to input the identity (e.g., username or profile link) of a social media influencer. An AI-powered system then performs a comprehensive analysis and provides an estimated **market capitalization** for a hypothetical cryptocurrency tied to that influencer.  \n\nOur approach combines:\n- **Multi-model AI computations**\n- **Data-driven analysis of engagement and pump/dump activities**\n- **Simulation of tokenized valuation dynamics**\n\n---\n\n## 🚀 Features\n- Input any social media influencer (Twitter, Instagram, TikTok, etc.)\n- AI-powered sentiment, influence, and reach analysis\n- Pump-activity and market manipulation detection\n- Estimated **cryptocurrency market cap valuation**\n- Extensible architecture for integrating more data sources\n\n---\n\n## 📊 Example Workflow\n\n1. **User Input**: Enter the influencer’s handle (e.g., `@elonmusk`).\n2. **AI Analysis**:\n   - Retrieve metrics (followers, engagement rates, sentiment).\n   - Apply multi-model AI analysis (influence scoring + pump activity detection).\n   - Predict potential crypto token valuation.\n3. **Output**: Market cap estimation, confidence intervals, and visual analytics.\n\n---\n\n## ⚙️ Installation\n\nClone this repository:\n```bash\ngit clone https://github.com/yourusername/influencer-token-valuation.git\ncd influencer-token-valuation\n\nInstall dependencies:\n\npip install -r requirements.txt\n\n🧑‍💻 Usage\nCommand Line\n\npython main.py --influencer \"@elonmusk\"\n\nSample Output\n\n{\n  \"influencer\": \"@elonmusk\",\n  \"influence_score\": 97.5,\n  \"predicted_market_cap\": \"12.5B USD\",\n  \"confidence_interval\": \"10.2B - 14.8B\",\n  \"pump_activity_risk\": \"High\"\n}\n\n🧩 Code Examples\n1. Basic Influencer Analysis\n\nfrom valuation import InfluencerValuation\n\nanalyzer = InfluencerValuation()\n\nresult = analyzer.evaluate_influencer(\"@elonmusk\")\nprint(result)\n\n2. Multi-Model AI Integration\n\nfrom models import SentimentModel, InfluenceModel, PumpActivityModel\n\ndef run_analysis(username):\n    sentiment = SentimentModel().analyze(username)\n    influence = InfluenceModel().score(username)\n    pump_risk = PumpActivityModel().detect(username)\n\n    market_cap = (influence * sentiment) / (1 + pump_risk)\n    return {\n        \"sentiment\": sentiment,\n        \"influence\": influence,\n        \"pump_risk\": pump_risk,\n        \"predicted_market_cap\": f\"{market_cap:.2f}B USD\"\n    }\n\nprint(run_analysis(\"@vitalikbuterin\"))\n\n3. API Example (Flask)\n\nfrom flask import Flask, request, jsonify\nfrom valuation import InfluencerValuation\n\napp = Flask(__name__)\nanalyzer = InfluencerValuation()\n\n@app.route(\"/evaluate\", methods=[\"POST\"])\ndef evaluate():\n    data = request.get_json()\n    username = data.get(\"influencer\")\n    result = analyzer.evaluate_influencer(username)\n    return jsonify(result)\n\nif __name__ == \"__main__\":\n    app.run(debug=True)\n\n📐 Mathematical Formula\n\nWe approximate the valuation using a simplified formula:\nPredictedMarketCap≈(InfluenceScore×SentimentScore)÷(1+PumpRiskFactor)\nPredictedMarketCap≈(InfluenceScore×SentimentScore)÷(1+PumpRiskFactor)\n\nWhere:\n\n    Influence Score = Derived from followers, engagement, and reach.\n\n    Sentiment Score = Weighted average of positive/negative sentiment.\n\n    Pump Risk Factor = Likelihood of manipulative activity.\n\n📈 Roadmap\n\nExpand social media API coverage\n\nImprove AI model ensemble strategies\n\nAdd visualization dashboards\n\n    Deploy as a hosted web app\n\n🤝 Contributing\n\nContributions are welcome! Please submit a pull request or open an issue to discuss ideas.\n",
      "default_branch": "master"
    },
    "fetched_at": "2025-09-18T23:11:11.254035"
  },
  {
    "basic_info": {
      "name": "worldexplorer",
      "full_name": "mschneider456/worldexplorer",
      "owner": "mschneider456",
      "description": "[SIGGRAPH Asia 2025] WorldExplorer: Towards Generating Fully Navigable 3D Scenes",
      "url": "https://github.com/mschneider456/worldexplorer",
      "clone_url": "https://github.com/mschneider456/worldexplorer.git",
      "ssh_url": "git@github.com:mschneider456/worldexplorer.git",
      "homepage": "https://mschneider456.github.io/world-explorer/",
      "created_at": "2025-09-17T01:03:33Z",
      "updated_at": "2025-09-18T20:49:31Z",
      "pushed_at": "2025-09-18T08:17:31Z"
    },
    "stats": {
      "stars": 59,
      "forks": 3,
      "watchers": 59,
      "open_issues": 0,
      "size": 130409
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 1361565,
        "Shell": 3692
      },
      "license": "Other",
      "topics": [
        "3d-gaussian-splatting",
        "scene-generation",
        "video-diffusion"
      ]
    },
    "content": {
      "readme": "# [SIGGRAPH Asia 2025] WorldExplorer: Towards Generating Fully Navigable 3D Scenes\n\nWorldExplorer produces high-quality scenes that remain stable under large camera motion, enabling realistic and unrestricted exploration.\n\nThis is the official repository for the SIGGRAPH Asia 2025 paper \"WorldExplorer: Towards Generating Fully Navigable 3D Scenes\".\n\n[[arXiv](https://arxiv.org/abs/2506.01799)] [[Project Page](https://mschneider456.github.io/world-explorer/)] [[Video](https://youtu.be/N6NJsNyiv6I)]\n\n![Teaser](docs/teaser.jpg \"WorldExplorer\")\n\nIf you find WorldExplorer useful for your work please consider giving a star ⭐️ and citing:\n\n```\n@InProceedings{schneider_hoellein_2025_worldexplorer,\n    title={WorldExplorer: Towards Generating Fully Navigable 3D Scenes},\n    author={Schneider, Manuel-Andreas and H{\\\"o}llein, Lukas and Nie{\\ss}ner, Matthias},\n    journal={arXiv preprint arXiv:2506.01799},\n    year={2025}\n}\n```\n\n\n## Installation\n\nWe have tested the below instructions with PyTorch 2.8.0+cu128, CUDA 12.8, PyTorch3D 0.7.8. \n\n```\nconda env create -f environment.yml\nconda activate worldexplorer\n\ncd model/stable-virtual-camera\npip install -e .\ncd ../..\n\nCUDA_HOME=/usr/local/cuda-12.8 pip install \"git+https://github.com/facebookresearch/pytorch3d.git@stable\"\npip install diffusers[\"torch\"] transformers protobuf transformers[sentencepiece] easydict plyfile\npip install --upgrade pip setuptools && pip install git+https://github.com/nerfstudio-project/nerfstudio.git@50e0e3c\npip install -U xformers --index-url https://download.pytorch.org/whl/cu128\n```\n\nIf you encounter [this issue](https://github.com/nerfstudio-project/gsplat/issues/249) during 3DGS optimization, it's likely that your CUDA version is not consistent between terminal sessions (check through `nvcc --version`). You can ensure the CUDA version stays fixed by setting the PATH variable in your `~/.bashrc`. For CUDA 12.8, this can be done by running:\n\n```\necho 'export PATH=/usr/local/cuda-12.8/bin:$PATH' >> ~/.bashrc\n```\n\n### Download checkpoints\n\nDownload the pretrained model checkpoints:\n\n```\nwget -O model/Depth_Anything_V2/checkpoints/depth_anything_v2_metric_hypersim_vitl.pth \"https://huggingface.co/depth-anything/Depth-Anything-V2-Metric-Hypersim-Large/resolve/main/depth_anything_v2_metric_hypersim_vitl.pth?download=true\"\n\nwget -O model/Video-Depth-Anything/checkpoints/video_depth_anything_vits.pth \"https://huggingface.co/depth-anything/Video-Depth-Anything-Small/resolve/main/video_depth_anything_vits.pth?download=true\"\n``` \n\n### Authenticate with Hugging Face\n\nThe `FLUX.1-dev` model used for initial image generation is gated and requires authentication.\n\n1. Request access to the model on its Hugging Face page:\n   https://huggingface.co/black-forest-labs/FLUX.1-dev\n\n2. Log in through the terminal and add a read-only token: \n   ```bash\n   pip install -U \"huggingface_hub[cli]\"\n   hf auth login\n   ```\n\nThe weights for the model will automatically be downloaded and saved the first time you generate a scene.\n\n## Usage\n\nWorldExplorer provides a command-line interface through `worldexplorer.py` for generating scene scaffolds (panoramas with known camera extrinsics and intrinsics) and 3D scenes.\n\n### Quick Start\n\nGenerate a complete 3D scene from a text description:\n```bash\npython worldexplorer.py generate \"bioluminescent, gravity-defying, telepathic cosmic jellyfish hive\"\n```\n\nPlease note that full 3D scene generation takes about 6 to 7 hours as 32 videos are generated in the process. The simplest way to track your progress is by checking the output at `./scenes/[theme_name]_[translation_scaling_factor]_[timestamp]/img2trajvid`.\n\n### Available Commands\n\n#### 1. `generate` - Full Pipeline (Scaffold + 3D Expansion)\nGenerates panoramic images from text and expands them into navigable 3D scenes.\n\n```bash\n# Indoor scene with automatic selection (CLIP-based)\npython worldexplorer.py generate \"Modern Apartment\" --mode automatic\n\n# Indoor scene with manual selection (yiels best results!)\npython worldexplorer.py generate \"Cozy Library\" --mode manual\n\n# Indoor scene with fast mode\npython worldexplorer.py generate \"Rustic Farmhouse (Wood, Leather, Wool)\" --mode fast\n\n# Custom/outdoor scene (requires four separate prompts for each viewing direction)\npython worldexplorer.py generate --custom\n\n# Skip 3D expansion (scaffold only)\npython worldexplorer.py generate \"Beach House\" --skip-expansion\n```\n\n**Options:**\n- `--mode, -m`: Panorama scaffold generation mode - `fast` (single output), `automatic` (CLIP selection), or `manual` (human curation)\n- `--translation-scaling, -t`: Movement scale factor (default: 3.0). The higher the translation scaling factor, the further your trajectories will expand into the scene. For indoor scenes the recommended range is 2.0 to 7.0, and for outdoors scenes 8.0 to 20.0. \n- `--skip-expansion`: Generate scaffold only without 3D expansion\n- `--custom, -c`: Custom mode for outdoor scenes or custom indoor scenes\n- `--root-dir`: Directory containing ",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-18T23:11:12.483721"
  },
  {
    "basic_info": {
      "name": "TradingView-PCapp",
      "full_name": "darklight797t6/TradingView-PCapp",
      "owner": "darklight797t6",
      "description": null,
      "url": "https://github.com/darklight797t6/TradingView-PCapp",
      "clone_url": "https://github.com/darklight797t6/TradingView-PCapp.git",
      "ssh_url": "git@github.com:darklight797t6/TradingView-PCapp.git",
      "homepage": null,
      "created_at": "2025-09-17T12:35:15Z",
      "updated_at": "2025-09-18T20:17:36Z",
      "pushed_at": "2025-09-17T12:48:01Z"
    },
    "stats": {
      "stars": 52,
      "forks": 0,
      "watchers": 52,
      "open_issues": 0,
      "size": 1463
    },
    "tech_info": {
      "language": null,
      "languages": {},
      "license": null,
      "topics": []
    },
    "content": {
      "readme": "\n# TradingView App on pc\n\nAccess realtime market prices and indicator values from TradingView!\n\n▌Features\n\n- [x] Access to TradingView's enhanced data feeds\n- [x] Efficiently backtest strategies and explore different settings\n- [x] Retrieve drawings from your charts\n- [x] Compatible with invite-only indicators\n- [x] Supports a large number of simultaneous indicators (subject to rate limits)\n- [x] Realtime data streaming\n- [x] Access TradingView's technical analysis data\n- [x] Replay mode + Simulated Replay mode (for free plans)\n- [x] Retrieve historical data for specific date ranges\n- [ ] TradingView socket server compatibility layer (alpha)\n- [ ] Interact with public chats\n- [ ] Access Screener top values\n- [ ] Get Hotlists\n- [ ] Get Calendar\n- IF YOU WANT A FEATURE, ASK ME!\n\n▌Potential Uses\n\n- Algorithmic trading\n- Custom Discord alerts\n- Advanced backtesting\n- Machine learning research\n- Free replay mode on certain timeframes, subject to limitations\n\n___\n\n▌Installation\n\nStable version:\n\nnpm i @mathieuc/tradingview\n\nLast version:\n\nnpm i github:Mathieu2301/TradingView-API\n\n▌Examples\n\nYou can find all the examples and snippets in ./examples folder.\n\n▌Before opening an issue\n\nPlease look at examples and previously resolved issues before opening a new one. I can't help everyone (especially for questions that are not library related but JavaScript related). Thank you for your understanding.\n___\n\n```\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-18T23:11:13.717387"
  },
  {
    "basic_info": {
      "name": "usgc-invoice",
      "full_name": "usgraphics/usgc-invoice",
      "owner": "usgraphics",
      "description": "Invoice LaTeX template",
      "url": "https://github.com/usgraphics/usgc-invoice",
      "clone_url": "https://github.com/usgraphics/usgc-invoice.git",
      "ssh_url": "git@github.com:usgraphics/usgc-invoice.git",
      "homepage": null,
      "created_at": "2025-09-17T17:37:28Z",
      "updated_at": "2025-09-18T21:13:05Z",
      "pushed_at": "2025-09-18T14:36:45Z"
    },
    "stats": {
      "stars": 44,
      "forks": 7,
      "watchers": 44,
      "open_issues": 1,
      "size": 7
    },
    "tech_info": {
      "language": "Typst",
      "languages": {
        "Typst": 2936,
        "TeX": 2539
      },
      "license": "BSD 3-Clause \"New\" or \"Revised\" License",
      "topics": []
    },
    "content": {
      "readme": "# usgc-invoice\nLaTeX template used by U.S. Graphics Company. Licensed under BSD 3 clause license, see LICENSE.md. Credit to U.S. Graphics Company would be much appreciated :)\n\n<img width=\"640\" alt=\"screenshot-2025-09-17_19-38-40\" src=\"https://github.com/user-attachments/assets/74e11f1d-00c7-48b5-b9ed-c9773220a910\" />\n",
      "default_branch": "master"
    },
    "fetched_at": "2025-09-18T23:11:14.967794"
  },
  {
    "basic_info": {
      "name": "ai-humanizer-api",
      "full_name": "HuzefaUsama25/ai-humanizer-api",
      "owner": "HuzefaUsama25",
      "description": "AI Humanizer API converts AI text into high quality undetectable human-like writing. Bypasses Turnitin, GPTZero, Originality.ai, CopyLeaks. Fast, natural, high quality output for students, marketers, and content creators",
      "url": "https://github.com/HuzefaUsama25/ai-humanizer-api",
      "clone_url": "https://github.com/HuzefaUsama25/ai-humanizer-api.git",
      "ssh_url": "git@github.com:HuzefaUsama25/ai-humanizer-api.git",
      "homepage": null,
      "created_at": "2025-09-18T08:21:03Z",
      "updated_at": "2025-09-18T20:11:30Z",
      "pushed_at": "2025-09-18T08:37:02Z"
    },
    "stats": {
      "stars": 40,
      "forks": 0,
      "watchers": 40,
      "open_issues": 0,
      "size": 4
    },
    "tech_info": {
      "language": null,
      "languages": {},
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "# AI Humanizer API\n\nThe **AI Humanizer API** is the **best and only truly effective AI humanizer API**. It transforms AI-generated text into **natural, human-like writing** that passes every major AI detector without losing meaning, tone, or quality. Unlike other tools that produce awkward or robotic rewrites, the AI Humanizer API generates fluent, authentic, and **indistinguishable human text**.\n\nThis repository provides examples, quick integration steps, and developer resources for the [WriteHybrid AI Humanizer API](https://writehybrid.com/api-docs).\n\n---\n\n## Table of Contents\n\n- [Why Use the AI Humanizer API](#why-use-the-ai-humanizer-api)  \n- [Key Features](#key-features)  \n- [Quick Start](#quick-start)  \n  - [Authentication](#authentication)  \n  - [Base URL](#base-url)  \n  - [Example Request (cURL)](#example-request-curl)  \n  - [Example Response](#example-response)  \n- [Parameters](#parameters)  \n- [Pricing](#pricing)  \n- [Who Is It For](#who-is-it-for)  \n- [Why Trust This API](#why-trust-this-api)  \n- [Get Started](#get-started-with-ai-humanizer-api)  \n\n---\n\n## Why Use the AI Humanizer API\n\nAI text often gets flagged by detectors such as **Turnitin, GPTZero, Originality.ai, Copyleaks, Sapling, and more**. Getting flagged can result in:\n\n- SEO penalties and lower Google rankings  \n- Academic or publishing rejection  \n- Damaged trust with clients, readers, or customers  \n\nThe **AI Humanizer API is the only solution that reliably bypasses every major detector** while preserving the original context and style. It is fast, scalable, and trusted by real users.\n\n---\n\n## Key Features\n\n- **Only Useful Humanizer API**: Every other tool falls short; this one works.  \n- **Detector-Proof**: Passes Turnitin, GPTZero, Originality.ai, Copyleaks, and more.  \n- **Transparent Billing**: 1 credit = 1 word.  \n- **High Quality**: Meaning and nuance are preserved.  \n- **Fast**: Humanize thousands of words in seconds.  \n- **Natural Output**: Fluent, human-like text every time.  \n- **Developer Friendly**: RESTful API with simple JSON responses.  \n- **Scalable**: From indie projects to enterprise-grade platforms.  \n\n---\n\n## Quick Start\n\n### Authentication\n\nAll requests require an API key. Add it to the `Authorization` header as a Bearer token.\n\n### Base URL\n\n[https://whbserver.com/api/v1](https://whbserver.com/api/v1)\n\n\n### Example Request (cURL)\n\n```bash\ncurl -X POST https://whbserver.com/api/v1/humanizer/ \\\n  -H \"Authorization: Bearer YOUR_API_KEY\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n        \"text\": \"AI-generated text goes here...\",\n        \"complexity\": \"medium\",\n        \"purpose\": \"general\"\n      }'\n````\n\n### Example Response\n\n```json\n{\n  \"success\": true,\n  \"humanized_text\": \"In today’s fast-paced world, technology continues to evolve at incredible speed...\",\n  \"original_length\": 245,\n  \"humanized_length\": 263,\n  \"processing_time\": 1.1,\n  \"detection_score\": 0.01,\n  \"credits_used\": 245\n}\n```\n\n---\n\n## Parameters\n\n| Field        | Type   | Description                                                      |\n| ------------ | ------ | ---------------------------------------------------------------- |\n| `text`       | string | The AI-generated text you want to humanize                       |\n| `complexity` | string | Output style: `simple`, `medium`, `complex`                      |\n| `purpose`    | string | Rewrite context: `general`, `seo`, `academic`, `marketing`, etc. |\n\n---\n\n## Pricing\n\n* **Starter** – 20,000 credits/month (20,000 words), email support\n* **Pro** – 60,000 credits/month (60,000 words), advanced tones, faster processing\n* **Agency** – 150,000 credits/month (150,000 words), team access, priority support\n* **Enterprise** – Custom high-volume solutions\n\n**Note:** 1 credit = 1 word.\n\nFull details: [writehybrid.com/api-docs](https://writehybrid.com/api-docs)\n\n---\n\n## Who Is It For?\n\n* **SEO Teams** – Safely scale content that ranks without AI penalties.\n* **Students & Academics** – Submit detection-proof work that reads as authentic.\n* **Content Agencies** – Deliver client-ready copy at scale with no manual rewrites.\n* **SaaS Builders** – Add humanization as a core feature inside your product.\n* **Freelancers** – Ensure client work is never penalized for AI generation.\n\n---\n\n## Why Trust This API\n\n* **Proven**: Successfully bypasses every major AI detector.\n* **Transparent**: 1 credit = 1 word, no hidden rules.\n* **Reliable**: Built on robust, scalable infrastructure.\n* **Trusted**: Used daily by agencies, startups, and independent creators worldwide.\n\n---\n\n## Get Started with AI Humanizer API\n\nStop wasting time on tools that don’t work. The **AI Humanizer API is the only real solution** for creating undetectable, human-like text at scale.\n\n👉 [**Get your API key today**](https://writehybrid.com/api-docs) and start humanizing AI text instantly.\n\n\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-18T23:11:16.200940"
  },
  {
    "basic_info": {
      "name": "ECE_F_CRT_PYTHON",
      "full_name": "gilshan-s/ECE_F_CRT_PYTHON",
      "owner": "gilshan-s",
      "description": "CODING",
      "url": "https://github.com/gilshan-s/ECE_F_CRT_PYTHON",
      "clone_url": "https://github.com/gilshan-s/ECE_F_CRT_PYTHON.git",
      "ssh_url": "git@github.com:gilshan-s/ECE_F_CRT_PYTHON.git",
      "homepage": null,
      "created_at": "2025-09-17T04:49:52Z",
      "updated_at": "2025-09-17T05:02:14Z",
      "pushed_at": "2025-09-17T04:49:52Z"
    },
    "stats": {
      "stars": 38,
      "forks": 41,
      "watchers": 38,
      "open_issues": 0,
      "size": 0
    },
    "tech_info": {
      "language": null,
      "languages": {},
      "license": null,
      "topics": []
    },
    "content": {
      "readme": "# ECE_F_CRT_PYTHON\nCODING\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-18T23:11:17.440749"
  },
  {
    "basic_info": {
      "name": "daedalus-keyboard",
      "full_name": "Perseus333/daedalus-keyboard",
      "owner": "Perseus333",
      "description": "An open-source, split, wireless ergonomic keyboard with trackpad and encoder",
      "url": "https://github.com/Perseus333/daedalus-keyboard",
      "clone_url": "https://github.com/Perseus333/daedalus-keyboard.git",
      "ssh_url": "git@github.com:Perseus333/daedalus-keyboard.git",
      "homepage": "https://perseuslynx.dev/projects/daedalus-kb",
      "created_at": "2025-09-17T09:29:38Z",
      "updated_at": "2025-09-18T22:02:20Z",
      "pushed_at": "2025-09-18T08:39:59Z"
    },
    "stats": {
      "stars": 37,
      "forks": 1,
      "watchers": 37,
      "open_issues": 0,
      "size": 98245
    },
    "tech_info": {
      "language": null,
      "languages": {},
      "license": "GNU Affero General Public License v3.0",
      "topics": [
        "ergonomic-keyboard",
        "keyboard",
        "mechanical-keyboard",
        "rotary-encoder",
        "split-keyboard",
        "trackpad",
        "wireless-keyboard",
        "zmk"
      ]
    },
    "content": {
      "readme": "# Daedalus Keyboard\n\nDaedalus is a compact open-source, split, wireless ergonomic keyboard with trackpad and encoder.\n\n![Image](/assets/main-img.jpg)\n\n## Daedalus Keyboard\n\nIt's a 36 key low-profile, split, wireless, ergonomic keyboard that includes a trackpad and a rotary encoder powered with ZMK on 2 Nice!Nano microcontrollers running on 2 110mAh batteries. It features a key splay inspired from the TOTEM, and a key layout modelled after my hand and inspired by the Corne. It's meant to be portable, silent and unobstrusive thanks to the Ambient Twighlight silent switches and the wireless connectivity.\n\n## Mission\n\nThe Daedalus Keyboard has been a hobby project that I worked on during the last year. It has been my first real hardware project that I've done, and I learned a lot throughout the way. Therefore, I aim for this project to be the most open and educational keyboard project possible for everyone.\n\nTherefore, here you will find everything needed to get started or to satiate your curiosity: all of the code, the schematics, the CAD files and even my notes during research.\n\nIf you're interested in building your own keyboard, then the [report](/docs/report.pdf) might interest you. It is a compreheensive document that describes the whole process of creation of the keyboard step by step, whilst giving some useful tips throughout and some valuable lessons learned at the end.\n\n## Repository Navigation\n\nThe repository is split in the following directories:\n\n- **Chassis**: Everything related to the chassis and components that are not hardware - CAD files, drawings and files ready for 3D printing or laser cutting right away\n- **Hardware**: Related to the PCB and electronic components - KiCAD schematic and PCB, and even gerber files to submit to a PCB manufacturer\n- **Firmware**: The code necessary to run the keyboard - Keybinds, ZMK config, etc.\n- **Docs**: The documentation to help you get started building this keyboard or any other - Build log, BOM, research, etc.\n\n## Current development\n\nCurrently I'm working on adding to this repository the following, in order:\n\n- Porting the CATIA files into an open source file format\n- A standalone step by step build guide (& BOM)\n- A contributing/editing guide\n- Publishing the Drawings\n\n## Contributing\n\nYou're welcome to contribute to this project in any way shape or form that is constructive.\n\n## Licence\n\nThis project is licenced differently for each domain, with the intention of promoting open source hardware and software:\n\n- Firmware and code: [GNU Affero General Public License v3 (AGPLv3)](./LICENSE-FIRMWARE)\n- Hardware: [CERN Open Hardware License v2 - Strongly Reciprocal](./LICENSE-HARDWARE)\n- Documentation and Media: [Creative Commons Attribution-ShareAlike 4.0](https://creativecommons.org/licenses/by-sa/4.0/)\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-18T23:11:18.668359"
  },
  {
    "basic_info": {
      "name": "lazycommit",
      "full_name": "KartikLabhshetwar/lazycommit",
      "owner": "KartikLabhshetwar",
      "description": "A CLI that writes your git commit messages for you with AI using Groq. Never write a commit message again.",
      "url": "https://github.com/KartikLabhshetwar/lazycommit",
      "clone_url": "https://github.com/KartikLabhshetwar/lazycommit.git",
      "ssh_url": "git@github.com:KartikLabhshetwar/lazycommit.git",
      "homepage": "https://www.npmjs.com/package/lazycommitt",
      "created_at": "2025-09-17T13:31:09Z",
      "updated_at": "2025-09-18T22:59:48Z",
      "pushed_at": "2025-09-18T16:13:49Z"
    },
    "stats": {
      "stars": 30,
      "forks": 1,
      "watchers": 30,
      "open_issues": 0,
      "size": 730
    },
    "tech_info": {
      "language": "TypeScript",
      "languages": {
        "TypeScript": 71744,
        "HTML": 6323,
        "Ruby": 614
      },
      "license": "Apache License 2.0",
      "topics": [
        "ai",
        "cli",
        "git",
        "github",
        "groq",
        "groq-api",
        "npm",
        "npm-package",
        "openai"
      ]
    },
    "content": {
      "readme": "<div align=\"center\">\n  <div>\n    <h1 align=\"center\">lazycommit</h1>\n<img width=\"2816\" height=\"1536\" alt=\"lazycommit\" src=\"https://github.com/user-attachments/assets/ee0419ef-2461-4b45-8509-973f3bb0f55c\" />\n\n  </div>\n\t<p>A CLI that writes your git commit messages for you with AI using Groq. Never write a commit message again.</p>\n\t<a href=\"https://www.npmjs.com/package/lazycommitz\"><img src=\"https://img.shields.io/npm/v/lazycommitt\" alt=\"Current version\"></a>\n\t<a href=\"https://github.com/KartikLabhshetwar/lazycommit\"><img src=\"https://img.shields.io/github/stars/KartikLabhshetwar/lazycommit\" alt=\"GitHub stars\"></a>\n\t<a href=\"https://github.com/KartikLabhshetwar/lazycommit/blob/main/LICENSE\"><img src=\"https://img.shields.io/npm/l/lazycommitt\" alt=\"License\"></a>\n</div>\n\n---\n\n## Setup\n\n> The minimum supported version of Node.js is v18. Check your Node.js version with `node --version`.\n\n1. Install _lazycommit_:\n\n   ```sh\n   npm install -g lazycommitt\n   ```\n\n### Install via Homebrew (macOS)\n\n```sh\nbrew install lazycommit\n```\n\nUpgrade:\n\n```sh\nbrew upgrade lazycommit\n```\n\n2. Retrieve your API key from [Groq Console](https://console.groq.com/keys)\n\n   > Note: If you haven't already, you'll have to create an account and get your API key.\n\n3. Set the key so lazycommit can use it:\n\n   ```sh\n   lazycommit config set GROQ_API_KEY=<your token>\n   ```\n\n   This will create a `.lazycommit` file in your home directory.\n\n### Upgrading\n\nCheck the installed version with:\n\n```\nlazycommit --version\n```\n\nIf it's not the [latest version](https://github.com/KartikLabhshetwar/lazycommit/releases/latest), run:\n\n```sh\nnpm update -g lazycommitt\n```\n\n## Usage\n\n### CLI mode\n\nYou can call `lazycommit` directly to generate a commit message for your staged changes:\n\n```sh\ngit add <files...>\nlazycommit\n```\n\n`lazycommit` passes down unknown flags to `git commit`, so you can pass in [`commit` flags](https://git-scm.com/docs/git-commit).\n\nFor example, you can stage all changes in tracked files as you commit:\n\n```sh\nlazycommit --all # or -a\n```\n\n> 👉 **Tip:** Use the `lzc` alias if `lazycommit` is too long for you.\n\n#### Generate multiple recommendations\n\nSometimes the recommended commit message isn't the best so you want it to generate a few to pick from. You can generate multiple commit messages at once by passing in the `--generate <i>` flag, where 'i' is the number of generated messages:\n\n```sh\nlazycommit --generate <i> # or -g <i>\n```\n\n> Warning: this uses more tokens, meaning it costs more.\n\n#### Generating Conventional Commits\n\nIf you'd like to generate [Conventional Commits](https://conventionalcommits.org/), you can use the `--type` flag followed by `conventional`. This will prompt `lazycommit` to format the commit message according to the Conventional Commits specification:\n\n```sh\nlazycommit --type conventional # or -t conventional\n```\n\nThis feature can be useful if your project follows the Conventional Commits standard or if you're using tools that rely on this commit format.\n\n#### Exclude files from analysis\n\nYou can exclude specific files from AI analysis using the `--exclude` flag:\n\n```sh\nlazycommit --exclude package-lock.json --exclude dist/\n```\n\n#### Automatic multi-commit mode\n\nWhen you stage many files, `lazycommit` can automatically split your changes into logical groups and create multiple commits with proper Conventional Commit messages.\n\n- Auto-trigger: when staged files ≥ 5, or when the diff is large\n- Grouping: buckets by type/scope (e.g., `feat(api)`, `docs`, `ci`, `build`, `test`, `chore`)\n- Deep split: if everything falls into one big bucket (e.g., `app/api/*`), it auto-splits by second-level directory (like `analytics`, `projects`, `sessions`)\n- Token-safe AI: each group uses a compact `git diff --cached --numstat` summary (not full diffs) to generate the commit line\n\nUsage:\n\n```sh\n# Just run as usual; grouping triggers automatically when applicable\nlazycommit\n\n# Force grouping even for < 5 files\nlazycommit --split\n```\n\n#### Handling large diffs\n\nFor large commits with many files, lazycommit automatically stays within API limits and maintains clean history:\n\n- **Automatic detection**: Large diffs and many-file changes are detected\n- **Logical grouping**: Files are grouped into conventional buckets; single huge buckets are auto-split by second-level directory (e.g., `app/api/<group>/...`)\n- **Token-safe summaries**: Each group sends a small `--numstat` summary to AI instead of full diffs\n- **Sequential commits**: In multi-commit mode, groups are committed one-by-one with their own messages\n\n### Git hook\n\nYou can also integrate _lazycommit_ with Git via the [`prepare-commit-msg`](https://git-scm.com/docs/githooks#_prepare_commit_msg) hook. This lets you use Git like you normally would, and edit the commit message before committing.\n\n#### Install\n\nIn the Git repository you want to install the hook in:\n\n```sh\nlazycommit hook install\n```\n\n#### Uninstall\n\nIn the Git repository you want to uninstall the hook from:\n\n```sh\nlazyc",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-18T23:11:19.897988"
  },
  {
    "basic_info": {
      "name": "homed-wiki",
      "full_name": "u236/homed-wiki",
      "owner": "u236",
      "description": null,
      "url": "https://github.com/u236/homed-wiki",
      "clone_url": "https://github.com/u236/homed-wiki.git",
      "ssh_url": "git@github.com:u236/homed-wiki.git",
      "homepage": null,
      "created_at": "2025-09-17T15:45:06Z",
      "updated_at": "2025-09-18T13:34:53Z",
      "pushed_at": "2025-09-18T10:24:57Z"
    },
    "stats": {
      "stars": 26,
      "forks": 1,
      "watchers": 26,
      "open_issues": 0,
      "size": 16344
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 1122
      },
      "license": null,
      "topics": []
    },
    "content": {
      "readme": "![HOMEd Wiki](.github/logo.png)\n\n# HOMEd Wiki\n\nФайлы для сайта документации HOMEd:\\\nhttps://wiki.homed.dev\n",
      "default_branch": "master"
    },
    "fetched_at": "2025-09-18T23:11:21.152188"
  },
  {
    "basic_info": {
      "name": "ComfyUI-VoxCPM",
      "full_name": "wildminder/ComfyUI-VoxCPM",
      "owner": "wildminder",
      "description": "ComfyUI node for highly expressive speech and realistic zero-shot voice cloning",
      "url": "https://github.com/wildminder/ComfyUI-VoxCPM",
      "clone_url": "https://github.com/wildminder/ComfyUI-VoxCPM.git",
      "ssh_url": "git@github.com:wildminder/ComfyUI-VoxCPM.git",
      "homepage": "",
      "created_at": "2025-09-17T22:40:11Z",
      "updated_at": "2025-09-18T22:47:18Z",
      "pushed_at": "2025-09-18T18:32:49Z"
    },
    "stats": {
      "stars": 26,
      "forks": 3,
      "watchers": 26,
      "open_issues": 1,
      "size": 142
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 106675
      },
      "license": "Apache License 2.0",
      "topics": [
        "ai-voice",
        "text-to-speech",
        "tts"
      ]
    },
    "content": {
      "readme": "<!-- Improved compatibility of back to top link -->\n<a id=\"readme-top\"></a>\n\n<div align=\"center\">\n  <h1 align=\"center\">ComfyUI-VoxCPM</h1>\n\n  <a href=\"https://github.com/wildminder/ComfyUI-VoxCPM\">    \n    <img src=\"https://github.com/user-attachments/assets/3f8a9544-6893-4893-8b3e-ac58dc6d0f95\" alt=\"ComfyUI-VoxCPM\" width=\"70%\">\n  </a>\n  \n\n  <p align=\"center\">\n    A custom node for ComfyUI that integrates <strong>VoxCPM</strong>, a novel tokenizer-free TTS system for context-aware speech generation and true-to-life voice cloning.\n    <br />\n    <br />\n    <a href=\"https://github.com/wildminder/ComfyUI-VoxCPM/issues/new?labels=bug&template=bug-report---.md\">Report Bug</a>\n    ·\n    <a href=\"https://github.com/wildminder/ComfyUI-VoxCPM/issues/new?labels=enhancement&template=feature-request---.md\">Request Feature</a>\n  </p>\n</div>\n\n<!-- PROJECT SHIELDS -->\n<div align=\"center\">\n\n[![Stargazers][stars-shield]][stars-url]\n[![Issues][issues-shield]][issues-url]\n[![Forks][forks-shield]][forks-url]\n\n</div>\n\n<br>\n\n## About The Project\n\nVoxCPM is a novel tokenizer-free Text-to-Speech system that redefines realism in speech synthesis by modeling speech in a continuous space. Built on the MiniCPM-4 backbone, it excels at generating highly expressive speech and performing accurate zero-shot voice cloning.\n\n<div align=\"center\">\n      <img src=\"./example_workflows/VoxCPM_example.png\" alt=\"ComfyUI-VoxCPM example workflow\">\n  </div>\n  \nThis custom node handles everything from model downloading and memory management to audio processing, allowing you to generate high-quality speech directly from a text script and optional reference audio files.\n\n**✨ Key Features:**\n*   **Context-Aware Expressive Speech:** The model understands text context to generate appropriate prosody and vocal expression.\n*   **True-to-Life Voice Cloning:** Clone a voice's timbre, accent, and emotional tone from a short audio sample.\n*   **Zero-Shot TTS:** Generate high-quality speech without any reference audio.\n*   **Automatic Model Management:** The required VoxCPM model is downloaded automatically and managed efficiently by ComfyUI to save VRAM.\n*   **Fine-Grained Control:** Adjust parameters like CFG scale and inference steps to tune the performance and style of the generated speech.\n*   **High-Efficiency Synthesis:** VoxCPM is designed for speed, enabling fast generation even on consumer-grade hardware.\n\n<p align=\"right\">(<a href=\"#readme-top\">back to top</a>)</p>\n\n## 🚀 Getting Started\n\nThe easiest way to install is via **ComfyUI Manager**. Search for `ComfyUI-VoxCPM` and click \"Install\".\n\nAlternatively, to install manually:\n\n1.  **Clone the Repository:**\n    Navigate to your `ComfyUI/custom_nodes/` directory and clone this repository:\n    ```sh\n    git clone https://github.com/wildminder/ComfyUI-VoxCPM.git\n    ```\n\n2.  **Install Dependencies:**\n    Open a terminal or command prompt, navigate into the cloned `ComfyUI-VoxCPM` directory, and install the required Python packages:\n    ```sh\n    cd ComfyUI-VoxCPM\n    pip install -r requirements.txt\n    ```\n\n3.  **Start/Restart ComfyUI:**\n    Launch ComfyUI. The \"VoxCPM TTS\" node will appear under the `audio/tts` category. The first time you use the node, it will automatically download the selected model to your `ComfyUI/models/tts/VoxCPM/` folder.\n\n## Models\nThis node automatically downloads the required model files.\n\n| Model | Parameters | Hugging Face Link |\n|:---|:---:|:---|\n| VoxCPM-0.5B | 0.5B | [openbmb/VoxCPM-0.5B](https://huggingface.co/openbmb/VoxCPM-0.5B) |\n\n<p align=\"right\">(<a href=\"#readme-top\">back to top</a>)</p>\n\n## 🛠️ Usage\n\n1.  **Add Nodes:** Add the `VoxCPM TTS` node to your graph. For voice cloning, add a `Load Audio` node to load your reference voice file.\n2.  **Connect Voice (for Cloning):** Connect the `AUDIO` output from the `Load Audio` node to the `prompt_audio` input on the VoxCPM TTS node.\n3.  **Write Text:**\n    *   For **voice cloning**, provide the transcript of your reference audio in the `prompt_text` field.\n    *   Enter the text you want to generate in the main `text` field.\n4.  **Generate:** Queue the prompt. The node will process the text and generate a single audio file.\n\n> [!NOTE]\n> **Denoising:** The original VoxCPM library includes a built-in denoiser (ZipEnhancer). This feature has been intentionally removed from the node. The ComfyUI philosophy encourages modular, single-purpose nodes. For denoising, please use a dedicated audio processing node before passing the `prompt_audio` to this one. This keeps the workflow clean and flexible.\n\n### Node Inputs\n\n*   **`model_name`**: Select the VoxCPM model to use. Official models are downloaded automatically.\n*   **`text`**: The target text to synthesize into speech.\n*   **`prompt_audio` (Optional)**: A reference audio clip for voice cloning.\n*   **`prompt_text` (Optional)**: The exact transcript of the `prompt_audio`. This is **required** for voice cloning.\n*   **`cfg_value`**: Classifier-Free Guidance scale. Higher values",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-18T23:11:22.373198"
  },
  {
    "basic_info": {
      "name": "Telegram-Adding-Users",
      "full_name": "duongtsa/Telegram-Adding-Users",
      "owner": "duongtsa",
      "description": "An automation for the process of adding communitymembers to any target of your choice. Simple to use and up to date!",
      "url": "https://github.com/duongtsa/Telegram-Adding-Users",
      "clone_url": "https://github.com/duongtsa/Telegram-Adding-Users.git",
      "ssh_url": "git@github.com:duongtsa/Telegram-Adding-Users.git",
      "homepage": "",
      "created_at": "2025-09-18T11:54:18Z",
      "updated_at": "2025-09-18T16:13:09Z",
      "pushed_at": "2025-09-18T11:56:56Z"
    },
    "stats": {
      "stars": 23,
      "forks": 0,
      "watchers": 23,
      "open_issues": 0,
      "size": 19779
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 56
      },
      "license": null,
      "topics": [
        "adders",
        "memberss-scri",
        "scraper",
        "telegram",
        "telegram-add",
        "telegram-add-user",
        "telegram-add-users",
        "telegram-adding-members",
        "telegram-channel-clone",
        "telegram-copy-channel",
        "telegram-copy-members",
        "telegram-copy-users",
        "telegram-group-clone",
        "telegram-member-adder-2025",
        "telegram-member-adders-2025",
        "telegram-memberadder",
        "telegram-members-add",
        "telegram-members-adder-2025",
        "telegram-membersadder",
        "telegram-user-adders-2025"
      ]
    },
    "content": {
      "readme": "# Telegram-Adding-Users\nAn automation for the process of adding communitymembers to any target of your choice. Simple to use and up to date!\n\n# 📁 GET IT HERE: https://shorturl.at/MdvDy\n# CONTACT FOR QUESTIONS: https://shorturl.at/qRqTq\n\n<img src='UI1.png' width='450'>\n\n- EXTRACT MEMBERS, MESSAGES, MEDIA, CHANNELS AND MORE! (EVEN HIDDEN MEMBERS)\n![](scrap.gif)\n- ADD MEMBERS TO YOUR GROUPS/CHANNELS AUTOMATICALLY!\n- FILTERING ONLY PREMIUM MEMBERS POSSIBLE! (OPTIONAL)\n![](add.gif)\n- AUTOMATICALLY FORWARD ANY POST/MESSAGE/MEDIA TO ANY TARGET!\n- MASSDM ANYONE ON TELEGRAM!\n![](mass.gif)\n- CLONE AND COPY ANY CHANNELS/GROUPS!\n![](copy.gif)\n- JOIN TO TARGETS WITH ALL OF YOUR ACCOUNTS AUTOMATICALLY!\n![](join.gif)\n- GET RID OF YOUR COMPETITION EASILY!\n- GROW YOUR AUDIENCE EASILY!\n- GROW YOUR VIEWS AUTOMATICALLY!\n![](view_post.gif)\n- VOTE ON ANY POLLS AUTOMATICALLY!\n- UNSPAM AND UNFREEZE YOUR ACCOUNTS EASILY!\n- REACT TO ANY POST AUTOMATICALLY WITH EMOJI'S!\n- MAKE BACKUPS!\n- NO CODING SKILLS REQUIRED!\n- PROXY SUPPORTED (OPTIONAL)\n- THE ONLY TG TOOL WHICH IS UPDATED TO 2025!\n- SUPPORT AND UPDATES FOR LIFETIME!\n- AND MUCH MORE!\n\nNEW FEATURES WILL BE IMPLEMENTED AT WISH!\nIf you have any questions, make sure to contact us.\n\n# 📁 GET IT HERE: https://shorturl.at/MdvDy\n# CONTACT FOR QUESTIONS: https://shorturl.at/qRqTq",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-18T23:11:23.597376"
  },
  {
    "basic_info": {
      "name": "RamTorch",
      "full_name": "lodestone-rock/RamTorch",
      "owner": "lodestone-rock",
      "description": "RAM is all you need",
      "url": "https://github.com/lodestone-rock/RamTorch",
      "clone_url": "https://github.com/lodestone-rock/RamTorch.git",
      "ssh_url": "git@github.com:lodestone-rock/RamTorch.git",
      "homepage": null,
      "created_at": "2025-09-18T12:16:49Z",
      "updated_at": "2025-09-18T21:55:33Z",
      "pushed_at": "2025-09-18T14:44:33Z"
    },
    "stats": {
      "stars": 22,
      "forks": 0,
      "watchers": 22,
      "open_issues": 0,
      "size": 14
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 9145
      },
      "license": "Apache License 2.0",
      "topics": []
    },
    "content": {
      "readme": "# RamTorch\n\n**RAM is All You Need** - A PyTorch library for memory-efficient deep learning that enables training and inference of large models that don't fit in GPU memory.\n\n## Overview\n\nRamTorch provides CPU-GPU hybrid implementations of neural network components that keep parameters in CPU memory and transfer them to GPU on-demand. This approach dramatically reduces GPU memory usage while maintaining computational efficiency through asynchronous CUDA streams and intelligent batching.\n\n## Key Features\n\n- **Memory-Efficient Linear Layers**: CPU-stored parameters with on-demand GPU transfer\n- **Asynchronous CUDA Streams**: Overlap computation with data transfer for minimal latency\n- **ZeRO-1 Optimizer Support**: Distributed optimizer state sharding across multiple GPUs\n- **Drop-in Replacement**: Compatible with existing PyTorch code\n- **Configurable Transfer Throttling**: Controllable memory usage\n\n## Installation\n\n```bash\npip install ramtorch\n```\n\nOr install from source:\n\n```bash\ngit clone https://github.com/lodestone-rock/RamTorch.git\ncd RamTorch\npip install -e .\n```\n\n## Quick Start\n\n### Basic Usage\n\nReplace `torch.nn.Linear` with `ramtorch.modules.Linear` for automatic memory optimization:\n\n```python\nimport torch\nimport ramtorch.modules as ram_modules\n\n# Standard PyTorch approach (high GPU memory usage)\n# linear = torch.nn.Linear(1000, 1000)\n\n# RamTorch approach (low GPU memory usage)\nlinear = ram_modules.Linear(1000, 1000, device=\"cuda\")\n\n# Use exactly like a normal PyTorch layer\nx = torch.randn(32, 1000, device=\"cuda\")\noutput = linear(x)  # Parameters automatically transferred from CPU to GPU\n```\n\n### Building Models\n\n```python\nimport torch.nn as nn\nimport ramtorch.modules as ram_modules\n\nclass MemoryEfficientModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layers = nn.Sequential(\n            ram_modules.Linear(1000, 2000),\n            nn.ReLU(),\n            ram_modules.Linear(2000, 2000),\n            nn.ReLU(),\n            ram_modules.Linear(2000, 100)\n        )\n    \n    def forward(self, x):\n        return self.layers(x)\n\nmodel = MemoryEfficientModel()\n```\n\n### ZeRO-1 Optimizer Sharding\n\nFor distributed training with optimizer state sharding:\n\n```python\nimport torch.distributed as dist\nfrom ramtorch.zero1 import create_zero_param_groups, broadcast_zero_params\n\n# Initialize distributed training\ndist.init_process_group(backend='nccl')\nmodel = YourModel()\nall_params = list(model.parameters())\nrank = dist.get_rank()\nworld_size = dist.get_world_size()\n\n# Create ZeRO-1 sharded optimizer\nparam_groups = [{'params': all_params, 'lr': 1e-3, 'weight_decay': 0.01}]\nsharded_groups, owner_ranks = create_zero_param_groups(param_groups, rank, world_size)\noptimizer = torch.optim.AdamW(sharded_groups)\n\n# Scheduler works normally with sharded optimizer\nscheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n\n# Training loop\nfor epoch in range(num_epochs):\n    for batch in dataloader:\n        # Forward/backward with gradient accumulation\n        for micro_batch in split_batch(batch):\n            loss = model(micro_batch)\n            loss.backward()\n\n        # All-reduce gradients across ranks (you need to implement this)\n        all_reduce_gradients(all_params)\n        \n        # Each rank updates only its owned parameters\n        optimizer.step()\n        \n        # Broadcast updated parameters from owners to all ranks\n        broadcast_zero_params(all_params, owner_ranks)\n        \n        model.zero_grad()\n        scheduler.step()\n```\n## Configuration\n\n### Environment Variables\n\n- `MAX_INFLIGHT`: Maximum number of concurrent CPU-to-GPU transfers (default: 2)\n  ```bash\n  export MAX_INFLIGHT=4  # Allow more concurrent transfers\n  ```\n\n### Transfer Stream Management\n\nRamTorch automatically manages CUDA streams for optimal performance. The library uses a dedicated transfer stream to overlap data movement with computation.\n\n## Performance Considerations\n\n### When to Use RamTorch\n\n**Best suited for:**\n- Large models that don't fit in GPU memory\n- Inference scenarios with memory constraints\n- Training with limited GPU memory but abundant CPU memory\n- Distributed training with many parameters\n\n**Less suitable for:**\n- Small models that fit comfortably in GPU memory\n- Scenarios where CPU-GPU bandwidth is the bottleneck\n- Real-time applications requiring minimal latency\n\n### Optimization Tips\n\n1. **Use Larger Batch Sizes**: Helps amortize transfer costs\n2. **Configure MAX_INFLIGHT**: Tune based on your GPU memory availability\n3. **Mixed Precision**: Combine with `torch.cuda.amp` for additional memory savings\n4. **Strategic Placement**: Use RamTorch layers for the largest components only\n\n## Architecture\n\n### CPU Bouncing Linear Layer\n\n\n1. Stores parameters on CPU memory (with `share_memory_()` for multiprocessing)\n2. Asynchronously transfers weights to GPU during forward pass\n3. Uses CUDA events for proper stream synchronization\n4. Automatically throttles transfers to preve",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-18T23:11:24.819800"
  },
  {
    "basic_info": {
      "name": "model-memory-calculator",
      "full_name": "KolosalAI/model-memory-calculator",
      "owner": "KolosalAI",
      "description": "Simple model memory requirements calculator for GGUF",
      "url": "https://github.com/KolosalAI/model-memory-calculator",
      "clone_url": "https://github.com/KolosalAI/model-memory-calculator.git",
      "ssh_url": "git@github.com:KolosalAI/model-memory-calculator.git",
      "homepage": null,
      "created_at": "2025-09-18T06:51:30Z",
      "updated_at": "2025-09-18T22:17:56Z",
      "pushed_at": "2025-09-18T16:59:26Z"
    },
    "stats": {
      "stars": 22,
      "forks": 2,
      "watchers": 22,
      "open_issues": 1,
      "size": 43
    },
    "tech_info": {
      "language": "HTML",
      "languages": {
        "HTML": 26568
      },
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "# GGUF Metadata Reader (Browser)\n\nA single-file, static web app to read GGUF model metadata directly in the browser and estimate memory usage (RAM/VRAM) for a chosen context window and KV cache quantization.\n\n- Works with remote URLs that support HTTP Range requests (e.g., many Hugging Face files)\n- Works with local `.gguf` files (drag-and-drop via file picker)\n- Detects sharded models (e.g., `-00001-of-00013`) and sums total size\n- No server required; everything runs client-side\n\n## Quick Start\n\nOption A: Open the file directly\n1. Open `index.html` in a modern browser (Chrome, Edge, Safari).\n2. Paste a GGUF URL or choose a local `.gguf` file and click the corresponding button.\n\nOption B: Serve locally (helps with some CORS setups)\n```bash\ncd path/to/model-memory-calculator\npython -m http.server 8000\n# Then open http://localhost:8000 in your browser\n```\n\n## Usage\n\n- GGUF URL: Paste a direct link to a `.gguf` (e.g., a Hugging Face “resolve/main” URL). Many hosts allow partial download via HTTP Range.\n- Or choose a local GGUF file: Uses the browser’s File API; no upload leaves your machine.\n- Context size (tokens): Select the desired context window (e.g., 4K, 16K, 128K).\n- KV cache quantization: Choose how keys/values are stored in memory. Options show approximate bytes per value.\n- Verbose: Prints debug logs of what’s read and how size is determined.\n\nClick “Read URL” or “Read File”. If successful, you’ll see:\n- Extracted params: `attention_heads`, `kv_heads`, `hidden_layers`, `hidden_size`, `split_count` (if present)\n- Memory estimate: model size + KV cache size at your chosen context/quantization\n\n## How It Works\n\n- GGUF parsing: Reads just enough of the GGUF header to extract:\n  - `.attention.head_count`\n  - `.attention.head_count_kv`\n  - `.block_count`\n  - `.embedding_length`\n  - `split.count` (if present)\n- Remote file size:\n  - Tries `HEAD` to get `Content-Length`.\n  - Falls back to a `Range: bytes=0-0` request and reads `Content-Range`.\n- Sharded models:\n  - Detects `-00001-of-000NN` style patterns in URLs or uses `split.count` metadata.\n  - Sums sizes across parts (remote) or estimates total from a single shard (local) when possible.\n- KV cache estimate:\n  - Uses a simplified formula: `bytes_per_value × hidden_size × hidden_layers × context_tokens`.\n  - Shows total as: `Model + KV` (MB/GB). Actual usage can vary by backend/implementation.\n\n## Notes & Limitations\n\n- GGUF versions: Supports GGUF v1–v3 headers for the fields listed above.\n- CORS & Range: Remote hosts must allow cross-origin requests and HTTP Range. If not, size detection may fail; download the file and use the local option instead.\n- Range ignored: Some servers respond `200` without honoring `Range`. The app avoids downloading the full body for size only; estimates can fail in this case.\n- Sanity limits: Very long strings/arrays in metadata are bounded to avoid huge reads.\n- Estimates only: KV cache math is intentionally simplified. Different runtimes store KV differently (e.g., layout, precision, per-head factors).\n\n## Troubleshooting\n\n- “Failed to read params.”\n  - The file may not be GGUF or uses unsupported/unexpected metadata. Try another file or update the URL.\n- “Could not determine file size or compute usage (CORS/Range?).”\n  - The remote host may block CORS or not report size via `HEAD`/`Range`. Try serving the page locally, a different host, or the local file picker.\n- Split detection issues\n  - Ensure URLs use a stable pattern (e.g., `-00001-of-000xx`) or that `split.count` is present in metadata.\n\n## Privacy\n\n- The local file option never uploads your file; parsing happens entirely in your browser.\n- For remote URLs, the app performs small range requests to read the header and determine file size. It aborts early once required metadata is read.\n\n## Development\n\n- No build step required. The app is a single page:\n  - `index.html` — All logic and UI\n- Open in a browser or serve with any static server.\n\n## License\n\nThis project is licensed under the terms in `LICENSE`.\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-18T23:11:26.053623"
  },
  {
    "basic_info": {
      "name": "autozero-gpt",
      "full_name": "ai-joon/autozero-gpt",
      "owner": "ai-joon",
      "description": null,
      "url": "https://github.com/ai-joon/autozero-gpt",
      "clone_url": "https://github.com/ai-joon/autozero-gpt.git",
      "ssh_url": "git@github.com:ai-joon/autozero-gpt.git",
      "homepage": null,
      "created_at": "2025-09-17T09:34:54Z",
      "updated_at": "2025-09-18T07:45:25Z",
      "pushed_at": "2025-09-18T01:14:59Z"
    },
    "stats": {
      "stars": 20,
      "forks": 2,
      "watchers": 20,
      "open_issues": 0,
      "size": 107177
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 1546291,
        "TypeScript": 591591,
        "Dart": 203562,
        "HCL": 27393,
        "C++": 23419,
        "CMake": 18862,
        "MDX": 12901,
        "CSS": 9233,
        "Smarty": 7518,
        "Dockerfile": 6417,
        "JavaScript": 5968,
        "Shell": 4513,
        "HTML": 3413,
        "Ruby": 2803,
        "PLpgSQL": 2443,
        "Jinja": 2398,
        "Swift": 2384,
        "C": 1425,
        "Batchfile": 478,
        "Kotlin": 140,
        "Objective-C": 38
      },
      "license": null,
      "topics": []
    },
    "content": {
      "readme": "# AutoGPT: Build, Deploy, and Run AI Agents\n\n**AutoGPT** is a powerful platform that allows you to create, deploy, and manage continuous AI agents that automate complex workflows. \n\n## Hosting Options \n   - Download to self-host\n   - [Join the Waitlist](https://bit.ly/3ZDijAI) for the cloud-hosted beta  \n\n## How to Setup for Self-Hosting\n> [!NOTE]\n> Setting up and hosting the AutoGPT Platform yourself is a technical process. \n> If you'd rather something that just works, we recommend [joining the waitlist](https://bit.ly/3ZDijAI) for the cloud-hosted beta.\n\nhttps://github.com/user-attachments/assets/d04273a5-b36a-4a37-818e-f631ce72d603\n\nThis tutorial assumes you have Docker, VSCode, git and npm installed.\n\n### 🧱 AutoGPT Frontend\n\nThe AutoGPT frontend is where users interact with our powerful AI automation platform. It offers multiple ways to engage with and leverage our AI agents. This is the interface where you'll bring your AI automation ideas to life:\n\n   **Agent Builder:** For those who want to customize, our intuitive, low-code interface allows you to design and configure your own AI agents. \n   \n   **Workflow Management:** Build, modify, and optimize your automation workflows with ease. You build your agent by connecting blocks, where each block     performs a single action.\n   \n   **Deployment Controls:** Manage the lifecycle of your agents, from testing to production.\n   \n   **Ready-to-Use Agents:** Don't want to build? Simply select from our library of pre-configured agents and put them to work immediately.\n   \n   **Agent Interaction:** Whether you've built your own or are using pre-configured agents, easily run and interact with them through our user-friendly      interface.\n\n   **Monitoring and Analytics:** Keep track of your agents' performance and gain insights to continually improve your automation processes.\n\n[Read this guide](https://docs.agpt.co/server/new_blocks/) to learn how to build your own custom blocks.\n\n### 💽 AutoGPT Server\n\nThe AutoGPT Server is the powerhouse of our platform This is where your agents run. Once deployed, agents can be triggered by external sources and can operate continuously. It contains all the essential components that make AutoGPT run smoothly.\n\n   **Source Code:** The core logic that drives our agents and automation processes.\n   \n   **Infrastructure:** Robust systems that ensure reliable and scalable performance.\n   \n   **Marketplace:** A comprehensive marketplace where you can find and deploy a wide range of pre-built agents.\n\n### 🐙 Example Agents\n\nHere are two examples of what you can do with AutoGPT:\n\n1. **Generate Viral Videos from Trending Topics**\n   - This agent reads topics on Reddit.\n   - It identifies trending topics.\n   - It then automatically creates a short-form video based on the content. \n\n2. **Identify Top Quotes from Videos for Social Media**\n   - This agent subscribes to your YouTube channel.\n   - When you post a new video, it transcribes it.\n   - It uses AI to identify the most impactful quotes to generate a summary.\n   - Then, it writes a post to automatically publish to your social media. \n\nThese examples show just a glimpse of what you can achieve with AutoGPT! You can create customized workflows to build agents for any use case.\n\n---\n",
      "default_branch": "master"
    },
    "fetched_at": "2025-09-18T23:11:27.344219"
  },
  {
    "basic_info": {
      "name": "perceptron",
      "full_name": "perceptron-ai-inc/perceptron",
      "owner": "perceptron-ai-inc",
      "description": "The official Python SDK for the Perceptron API",
      "url": "https://github.com/perceptron-ai-inc/perceptron",
      "clone_url": "https://github.com/perceptron-ai-inc/perceptron.git",
      "ssh_url": "git@github.com:perceptron-ai-inc/perceptron.git",
      "homepage": "https://www.perceptron.inc",
      "created_at": "2025-09-17T02:25:49Z",
      "updated_at": "2025-09-18T22:22:05Z",
      "pushed_at": "2025-09-18T20:58:37Z"
    },
    "stats": {
      "stars": 20,
      "forks": 2,
      "watchers": 20,
      "open_issues": 1,
      "size": 310
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 279555
      },
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "# Perceptron SDK\n\nPython SDK and CLI for perceptive-language models. The SDK is provider-agnostic and lets you compose visual + language tasks, run them locally for inspection, or execute them via a configured provider. Choose a provider and optional model per call; keep your application code stable across model updates.\n\n---\n\n## Installation\n- Prerequisites: Python 3.10+, `pip` 23+ (or [`uv`](https://github.com/astral-sh/uv))\n\n```bash\npip install perceptron\n\n# Optional extras\npip install \"perceptron[torch]\"   # TensorStream helpers (requires PyTorch)\npip install \"perceptron[dev]\"     # Dev tools (ruff, pytest, pre-commit)\n```\n\nUsing `uv`:\n```bash\nuv pip install perceptron\nuv pip install \"perceptron[torch]\"\nuv pip install \"perceptron[dev]\"\n```\n\nThe CLI entry point `perceptron` is available after install.\n\n---\n\n## Configuration\nSet credentials and defaults via environment, programmatically, or the CLI. The SDK ships with a `fal` provider; you can add others by extending `perceptron.client._PROVIDER_CONFIG`.\n\n- `PERCEPTRON_PROVIDER`: provider identifier (default `fal`)\n- `PERCEPTRON_API_KEY`: API key for the selected provider\n- `PERCEPTRON_BASE_URL`: override provider base URL when needed\n- `FAL_KEY`: alternative env var used when `provider=fal`\n\nProgrammatic configuration:\n```python\nfrom perceptron import configure, config\n\nconfigure(provider=\"fal\", api_key=\"sk_live_...\", base_url=\"https://api.example/v1\")\n\nwith config(max_tokens=512):\n    ...  # temporary overrides inside the context\n```\n\nCLI helper:\n```bash\nperceptron config --provider fal --api-key sk_live_...\n```\n\nNo credentials? Helpers return compile-only payloads so you can inspect tasks without sending requests.\n\n---\n\n## Python Quickstart\n```python\nfrom perceptron import caption, detect\n\n# Caption an image (provider default model)\nresult = caption(\"/path/to/image.png\", style=\"concise\")\nprint(result.text)\n\n# Stream grounded detections; optionally select a specific model\nfor event in detect(\"local.png\", classes=[\"person\", \"forklift\"], model=\"perceptron\", stream=True):\n    if event[\"type\"] == \"text.delta\":\n        print(\"chunk\", event[\"chunk\"])\n    elif event[\"type\"] == \"points.delta\":\n        print(\"bbox\", event[\"points\"])\n    elif event[\"type\"] == \"final\":\n        print(\"final\", event[\"result\"][\"points\"])\n```\n\n### Few-shot detection from COCO\n```python\nfrom perceptron import detect_from_coco\n\nruns = detect_from_coco(\n    \"/datasets/demo\",\n    split=\"train\",\n    shots=4,                 # build balanced in-context examples automatically\n    classes=[\"defect\", \"ok\"],\n)\n\nfor sample in runs:\n    print(sample.image_path.name)\n    for box in sample.result.points or []:\n        print(\" -\", box.mention, box)\n```\n\n---\n\n## CLI Usage\nThe CLI mirrors the high-level helpers and supports directory batching (JSON summaries written alongside input folders).\n\n```bash\n# Generate captions\nperceptron caption image.jpg\nperceptron caption ./images --style detailed\n\n# OCR with a custom prompt\nperceptron ocr schematic.png --prompt \"Extract component labels\"\n\n# Batched detection (writes detections.json)\nperceptron detect ./frames --classes defect,warning\n\n# Grounded question answering\nperceptron question image.jpg \"What stands out?\" --expects box --format json\n```\n\nDirectory mode disables streaming and logs raw responses, plus per-file validation issues.\n\n---\n\n## High-Level APIs\n- `caption(image, *, style=\"concise\", stream=False, **kwargs)`\n- `ocr(image, *, prompt=None, stream=False, **kwargs)`\n- `detect(image, *, classes=None, examples=None, stream=False, **kwargs)`\n- `detect_from_coco(dataset_dir, *, split=None, classes=None, shots=0, limit=None, **kwargs)`\n\nNotes\n- Pass `model=\"...\"`, `provider=\"...\"`, `max_tokens=...`, etc., through `**kwargs` on any helper.\n- `detect_from_coco` discovers annotations, constructs balanced examples when `shots > 0`, and returns `CocoDetectResult` objects.\n- For advanced workflows, build tasks with the typed DSL (`text`, `system`, `image`, `point`, `box`, `polygon`, `collection`) and decorate with `@perceive` / `@async_perceive`.\n\n### Using the DSL with `@perceive`\n```python\nfrom perceptron import perceive, image, text\n\n@perceive(expects=\"box\")\ndef describe_landmark(path):\n    return image(path) + text(\"Highlight the main structures in one sentence.\")\n\nresult = describe_landmark(\"./landmark.jpg\")\nprint(result.text)\nfor box in result.points or []:\n    print(box.mention, box)\n\n# Inspect the compiled payload without executing the request\nprint(describe_landmark.inspect(\"./landmark.jpg\"))\n```\n\nSet `stream=True` in the decorator to receive incremental events (`text.delta`, `points.delta`, `final`). Swap `expects` to `text`, `point`, or `polygon` when you need alternate structures.\n\n---\n\n## Troubleshooting\n| Symptom | Likely Cause | Resolution |\n| --- | --- | --- |\n| Compile-only result (no text) | Missing provider credentials | Export `FAL_KEY` / `PERCEPTRON_API_KEY` or call `configure(...)` |\n| `stream_buffer_overflow` warning | Long streaming r",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-18T23:11:28.566928"
  },
  {
    "basic_info": {
      "name": "Trotski",
      "full_name": "iluxu/Trotski",
      "owner": "iluxu",
      "description": "Real-time AI Interview Assistant. Transcribes live audio with faster-whisper, detects questions, and generates answers using an LLM. Includes a multi-platform client and a web UI dashboard. Your personal interview co-pilot.",
      "url": "https://github.com/iluxu/Trotski",
      "clone_url": "https://github.com/iluxu/Trotski.git",
      "ssh_url": "git@github.com:iluxu/Trotski.git",
      "homepage": null,
      "created_at": "2025-09-17T07:07:42Z",
      "updated_at": "2025-09-18T13:15:41Z",
      "pushed_at": "2025-09-17T12:08:52Z"
    },
    "stats": {
      "stars": 18,
      "forks": 2,
      "watchers": 18,
      "open_issues": 0,
      "size": 59
    },
    "tech_info": {
      "language": "HTML",
      "languages": {
        "HTML": 42318,
        "Python": 35225
      },
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "# Trotski - Real-Time AI Interview Assistant\n\nThis project provides a high-performance, real-time audio transcription and AI-powered answering server. It uses faster-whisper for low-latency STT (Speech-to-Text) and an LLM (like GPT models) to intelligently detect questions from the transcript and generate relevant, in-character answers on the fly.\n\nThe system is composed of three main parts:\n\n- **The Server** (`optimized_stt_server_v3.py`): A WebSocket server that receives raw audio, transcribes it, analyzes the text for questions, and generates answers.\n- **The Client** (`stable_audio_client_multi_os.py`): A robust, multi-platform audio streaming client that captures microphone input using FFmpeg and streams it to the server.\n- **The UI** (`index.html`): A standalone, zero-dependency web interface that connects to the server to display the live transcript and Q&A panel.\n\n## ✨ Features\n\n- **Real-Time Transcription**: Low-latency audio transcription using faster-whisper\n- **Intelligent Question Detection**: An LLM-powered analyzer detects questions from the live transcript\n- **AI-Powered Answer Generation**: Generates context-aware, in-character answers for detected questions\n- **Standalone Web UI**: A feature-rich, single-file `index.html` dashboard to monitor the interview\n- **Multi-Platform Support**: The server and client run on Windows, macOS, and Linux\n- **Robust & Stable**: Includes automatic reconnection, backpressure handling, and stable connection parameters\n- **Highly Configurable**: Nearly every aspect can be configured via environment variables\n\n## 📋 Prerequisites\n\nBefore you begin, ensure you have the following installed:\n\n### Python 3.9+\n\n### FFmpeg\nRequired by the audio client to capture microphone audio.\n\n- **Windows**: Download from the [official website](https://ffmpeg.org/download.html) and add to PATH, or use Chocolatey (`choco install ffmpeg`)\n- **macOS**: Install via Homebrew: `brew install ffmpeg`\n- **Linux**: Install via your package manager: `sudo apt-get install ffmpeg` (Debian/Ubuntu)\n\n### NVIDIA GPU with CUDA (Recommended)\nFor significant performance gains with the Whisper model.\n\n- Install the latest [NVIDIA Driver](https://www.nvidia.com/drivers/)\n- Install the [CUDA Toolkit](https://developer.nvidia.com/cuda-toolkit) (v11.x is compatible)\n- Install [cuDNN](https://developer.nvidia.com/cudnn)\n\n### OpenAI API Key\nRequired for question detection and answer generation.\n\n## 🚀 Installation\n\n### 1. Clone the Repository\n\n```bash\ngit clone https://github.com/iluxu/Trotski.git\ncd Trotski\n```\n\n### 2. Create a Virtual Environment\n\n```bash\npython -m venv venv\n\n# On Windows\n.\\venv\\Scripts\\activate\n\n# On macOS/Linux\nsource venv/bin/activate\n```\n\n### 3. Install Python Dependencies\n\nCreate a `requirements.txt` file with the content specified below and run:\n\n```bash\npip install -r requirements.txt\n```\n\n**CPU-Only Note**: If you don't have an NVIDIA GPU, first install the CPU version of PyTorch:\n```bash\npip install torch --index-url https://download.pytorch.org/whl/cpu\n```\nThen run `pip install -r requirements.txt`.\n\n### 4. Set Up Environment Variables\n\nCreate a `.env` file by copying the example:\n\n```bash\n# On Windows\ncopy .env.example .env\n\n# On macOS/Linux\ncp .env.example .env\n```\n\nNow, edit the `.env` file and add your `OPENAI_API_KEY`. See the `.env.example` section for all options.\n\n## ⚙️ Usage\n\nThe process involves three steps: starting the server, opening the UI, and starting the audio client.\n\n### 1. Run the Server\n\nStart the server in a terminal. It will download the Whisper model on its first run.\n\n```bash\npython optimized_stt_server_v3.py\n```\n\nYou should see output indicating the server is ready:\n```\n🎤 Server ready on ws://127.0.0.1:8123/\n```\n\n### 2. Open the Web UI\n\nSimply open the `index.html` file in your web browser (e.g., Chrome, Firefox, Safari). No web server is needed. The page will automatically try to connect to the WebSocket server running on your local machine.\n\n### 3. Run the Audio Client\n\nThe client needs to know which microphone to use.\n\n#### Step A: Find Your Audio Device\n\nOpen a new terminal and run the client with the `--list-devices` flag:\n\n```bash\npython stable_audio_client_multi_os.py --list-devices\n```\n\nThis will show you a list of available microphones and the correct name to use for your operating system.\n\n#### Step B: Start Streaming\n\nNow, run the client with the device name you found.\n\n```bash\n# Example for Windows\npython stable_audio_client_multi_os.py --device \"Mixage stéréo (Realtek(R) Audio)\"\n\n# Example for macOS\npython stable_audio_client_multi_os.py --device \":0\"\n\n# Example for Linux\npython stable_audio_client_multi_os.py --device \"hw:0,0\"\n```\n\nThe client will connect to the server. Start speaking, and you will see the live transcript and Q&A appear in the `index.html` UI in your browser.\n\n## 🖥️ Web UI Features (index.html)\n\nThe web UI is a powerful dashboard for monitoring the interview in real-time.\n\n<!-- It's a good idea to add a screenshot of your ",
      "default_branch": "main"
    },
    "fetched_at": "2025-09-18T23:11:29.784883"
  }
]