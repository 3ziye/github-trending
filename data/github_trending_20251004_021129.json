[
  {
    "basic_info": {
      "name": "chrome-devtools-mcp",
      "full_name": "ChromeDevTools/chrome-devtools-mcp",
      "owner": "ChromeDevTools",
      "description": "Chrome DevTools for coding agents",
      "url": "https://github.com/ChromeDevTools/chrome-devtools-mcp",
      "clone_url": "https://github.com/ChromeDevTools/chrome-devtools-mcp.git",
      "ssh_url": "git@github.com:ChromeDevTools/chrome-devtools-mcp.git",
      "homepage": "https://npmjs.org/package/chrome-devtools-mcp",
      "created_at": "2025-09-11T10:39:55Z",
      "updated_at": "2025-10-04T02:07:25Z",
      "pushed_at": "2025-10-03T15:46:38Z"
    },
    "stats": {
      "stars": 8997,
      "forks": 472,
      "watchers": 8997,
      "open_issues": 36,
      "size": 1342
    },
    "tech_info": {
      "language": "TypeScript",
      "languages": {
        "TypeScript": 208149,
        "JavaScript": 6211
      },
      "license": "Apache License 2.0",
      "topics": [
        "browser",
        "chrome",
        "chrome-devtools",
        "debugging",
        "devtools",
        "mcp",
        "mcp-server",
        "puppeteer"
      ]
    },
    "content": {
      "readme": "# Chrome DevTools MCP\n\n[![npm chrome-devtools-mcp package](https://img.shields.io/npm/v/chrome-devtools-mcp.svg)](https://npmjs.org/package/chrome-devtools-mcp)\n\n`chrome-devtools-mcp` lets your coding agent (such as Gemini, Claude, Cursor or Copilot)\ncontrol and inspect a live Chrome browser. It acts as a Model-Context-Protocol\n(MCP) server, giving your AI coding assistant access to the full power of\nChrome DevTools for reliable automation, in-depth debugging, and performance analysis.\n\n## [Tool reference](./docs/tool-reference.md) | [Changelog](./CHANGELOG.md) | [Contributing](./CONTRIBUTING.md) | [Troubleshooting](./docs/troubleshooting.md)\n\n## Key features\n\n- **Get performance insights**: Uses [Chrome\n  DevTools](https://github.com/ChromeDevTools/devtools-frontend) to record\n  traces and extract actionable performance insights.\n- **Advanced browser debugging**: Analyze network requests, take screenshots and\n  check the browser console.\n- **Reliable automation**. Uses\n  [puppeteer](https://github.com/puppeteer/puppeteer) to automate actions in\n  Chrome and automatically wait for action results.\n\n## Disclaimers\n\n`chrome-devtools-mcp` exposes content of the browser instance to the MCP clients\nallowing them to inspect, debug, and modify any data in the browser or DevTools.\nAvoid sharing sensitive or personal information that you don't want to share with\nMCP clients.\n\n## Requirements\n\n- [Node.js](https://nodejs.org/) v20.19 or a newer [latest maintenance LTS](https://github.com/nodejs/Release#release-schedule) version.\n- [Chrome](https://www.google.com/chrome/) current stable version or newer.\n- [npm](https://www.npmjs.com/).\n\n## Getting started\n\nAdd the following config to your MCP client:\n\n```json\n{\n  \"mcpServers\": {\n    \"chrome-devtools\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"chrome-devtools-mcp@latest\"]\n    }\n  }\n}\n```\n\n> [!NOTE]  \n> Using `chrome-devtools-mcp@latest` ensures that your MCP client will always use the latest version of the Chrome DevTools MCP server.\n\n### MCP Client configuration\n\n<details>\n  <summary>Claude Code</summary>\n    Use the Claude Code CLI to add the Chrome DevTools MCP server (<a href=\"https://docs.anthropic.com/en/docs/claude-code/mcp\">guide</a>):\n\n```bash\nclaude mcp add chrome-devtools npx chrome-devtools-mcp@latest\n```\n\n</details>\n\n<details>\n  <summary>Cline</summary>\n  Follow https://docs.cline.bot/mcp/configuring-mcp-servers and use the config provided above.\n</details>\n\n<details>\n  <summary>Codex</summary>\n  Follow the <a href=\"https://github.com/openai/codex/blob/main/docs/advanced.md#model-context-protocol-mcp\">configure MCP guide</a>\n  using the standard config from above. You can also install the Chrome DevTools MCP server using the Codex CLI:\n\n```bash\ncodex mcp add chrome-devtools -- npx chrome-devtools-mcp@latest\n```\n\n**On Windows 11**\n\nConfigure the Chrome install location and increase the startup timeout by updating `.codex/config.toml` and adding the following `env` and `startup_timeout_ms` parameters:\n\n```\n[mcp_servers.chrome-devtools]\ncommand = \"cmd\"\nargs = [\n    \"/c\",\n    \"npx\",\n    \"-y\",\n    \"chrome-devtools-mcp@latest\",\n]\nenv = { SystemRoot=\"C:\\\\Windows\", PROGRAMFILES=\"C:\\\\Program Files\" }\nstartup_timeout_ms = 20_000\n```\n\n</details>\n\n<details>\n  <summary>Copilot CLI</summary>\n\nStart Copilot CLI:\n\n```\ncopilot\n```\n\nStart the dialog to add a new MCP server by running:\n\n```\n/mcp add\n```\n\nConfigure the following fields and press `CTR-S` to save the configuration:\n\n- **Server name:** `chrome-devtools`\n- **Server Type:** `[1] Local`\n- **Command:** `npx`\n- **Arguments:** `-y, chrome-devtools-mcp@latest`\n\n</details>\n\n<details>\n  <summary>Copilot / VS Code</summary>\n  Follow the MCP install <a href=\"https://code.visualstudio.com/docs/copilot/chat/mcp-servers#_add-an-mcp-server\">guide</a>,\n  with the standard config from above. You can also install the Chrome DevTools MCP server using the VS Code CLI:\n  \n  ```bash\n  code --add-mcp '{\"name\":\"chrome-devtools\",\"command\":\"npx\",\"args\":[\"chrome-devtools-mcp@latest\"]}'\n  ```\n</details>\n\n<details>\n  <summary>Cursor</summary>\n\n**Click the button to install:**\n\n[<img src=\"https://cursor.com/deeplink/mcp-install-dark.svg\" alt=\"Install in Cursor\">](https://cursor.com/en/install-mcp?name=chrome-devtools&config=eyJjb21tYW5kIjoibnB4IC15IGNocm9tZS1kZXZ0b29scy1tY3BAbGF0ZXN0In0%3D)\n\n**Or install manually:**\n\nGo to `Cursor Settings` -> `MCP` -> `New MCP Server`. Use the config provided above.\n\n</details>\n\n<details>\n  <summary>Gemini CLI</summary>\nInstall the Chrome DevTools MCP server using the Gemini CLI.\n\n**Project wide:**\n\n```bash\ngemini mcp add chrome-devtools npx chrome-devtools-mcp@latest\n```\n\n**Globally:**\n\n```bash\ngemini mcp add -s user chrome-devtools npx chrome-devtools-mcp@latest\n```\n\nAlternatively, follow the <a href=\"https://github.com/google-gemini/gemini-cli/blob/main/docs/tools/mcp-server.md#how-to-set-up-your-mcp-server\">MCP guide</a> and use the standard config from above.\n\n</details>\n\n<details>\n  <summary>Gemi",
      "default_branch": "main"
    },
    "fetched_at": "2025-10-04T02:11:29.960261"
  },
  {
    "basic_info": {
      "name": "term.everything",
      "full_name": "mmulet/term.everything",
      "owner": "mmulet",
      "description": "Run any GUI app in the terminal‚ùó",
      "url": "https://github.com/mmulet/term.everything",
      "clone_url": "https://github.com/mmulet/term.everything.git",
      "ssh_url": "git@github.com:mmulet/term.everything.git",
      "homepage": "",
      "created_at": "2025-09-07T02:52:48Z",
      "updated_at": "2025-10-03T22:33:00Z",
      "pushed_at": "2025-09-30T15:05:23Z"
    },
    "stats": {
      "stars": 6166,
      "forks": 133,
      "watchers": 6166,
      "open_issues": 18,
      "size": 53558
    },
    "tech_info": {
      "language": "TypeScript",
      "languages": {
        "TypeScript": 219331,
        "C++": 39626,
        "Shell": 2857,
        "JavaScript": 2607,
        "Meson": 2286,
        "C": 314
      },
      "license": "GNU Affero General Public License v3.0",
      "topics": [
        "alacritty",
        "cli",
        "foss",
        "iterm2",
        "kitty",
        "linux",
        "ssh",
        "terminal",
        "wayland",
        "wayland-compositor"
      ]
    },
    "content": {
      "readme": "\n\n\n<table>\n  <tr>\n    <td valign=\"middle\">\n      <img width=\"128\" height=\"128\" alt=\"icon2\" src=\"./resources/icon.png\" />\n    </td>\n    <td><h1>Term.Everything‚ùó</h1></td>\n    <td><a href=\"https://github.com/mmulet/term.everything/releases\">Download the beta test now!</a></td>\n    <td><a href=\"./resources/HowIDidIt.md\">HowIDidIt.md</a></td>\n  </tr>\n  <tr>\n    <td></td>\n    <td>Works on both x11 and Wayland host systems.</td>\n    <td></td>\n    <td></td>\n  </tr>\n</table>\n\n## Run every GUI app in the terminal!\n\n![warp_into_terminal0001-0195](./resources/graphics/warp_in_2.gif)\n\n## Even over ssh!\nBehold as I play a [video game in a font](https://github.com/mmulet/font-game-engine) in a web browser in a terminal transmitted over ssh (with one hand tied behind my back)!\n\n![ssh_example](./resources/graphics/ssh_example.gif)\n\n### Read about how it works!\nCheck out [HowIDidIt.md](./resources/HowIDidIt.md)\n\n## More Examples\nThe quality of the window is limited to the number of rows and columns in your\nterminal. If you increase the resolution (ctrl - in alacritty, check your\nterminal) the quality will go up, (but performance may go down).\n\nHere I open up the Wing It! movie, and increase the quality until I get both\na good frame rate and resolution:\n\n![increase resolution](./resources/graphics/show_increase_res.gif)\n\n----------------\n\nIf your terminal supports images (like [kitty](https://sw.kovidgoyal.net/kitty/)\nor [iterm2](https://iterm2.com/)) you can render windows at full resolution\n(performance may degrade).\n\nIn this example, on my mac, I open iTerm2 ssh into ubuntu and open firefox\nat full resolution:\n\n![full_resultion](resources/graphics/full_resultion.gif)\n\n------------\n\nI feel like every single day I hear about another terminal file viewer. I say, stop making terminal file viewers because you can just use the file viewer you already have! In your terminal!\n\n![file_manager](./resources/graphics/file_manager.gif)\n\n-------------\n\nTerminal in a terminal in a terminal in a terminal in a terminal.... it's terminals all the way down.\n![terminal_in_terminal](./resources/graphics/terminal_in_terminal.gif)\n\n-------------\nWith only a small amount hacking, it can run Doom (shareware episode)!\n\n![Doom](./resources/graphics/doom.gif)\n------\nRun an entire Desktop in your terminal!\n[@ismail-yilmaz](https://github.com/ismail-yilmaz) is running Firefox, on [KDE Neon](https://neon.kde.org) in a [VM](https://gitlab.gnome.org/GNOME/gnome-boxes) on [Bobcat](https://github.com/ismail-yilmaz/Bobcat)\n![Desktop in VM](./resources/graphics/desktop_in_vm.gif)\n\nAnd this isn't even full resolution! Checkout the [full vid in in the discussions](https://github.com/mmulet/term.everything/discussions/16#discussioncomment-14390137)\n\n## About\n`term.everything‚ùó` is a Linux CLI program to run GUI windows in your terminal. Specifically, `term.everything‚ùó` is a built-from-scratch [Wayland](https://wiki.archlinux.org/title/Wayland) compositor that outputs to a terminal rather than your monitor.\n\n>Don't know what Wayland is or just want to know more about how this works? Then, head over to [HowIDidIt.md](./resources/HowIDidIt.md) where I will explain how everything works in detail.\n\n## Try it out!\n[Download the beta test now!](https://github.com/mmulet/term.everything/releases)\n\n## Roadmap\n1. [x] Term some things <--- This is where we are at\n  - Some apps or (even most apps) may fail to launch or even crash! Please create [an issue]( https://github.com/mmulet/term.everything/issues) if you have problems.\n2. [ ] Term most things\n3. [ ] Term everything‚ùó\n\n## Help and Usage\nCheck out the [help file here](./resources/help.md) for a usage guide on how to use `term.everything‚ùó`\n\n## Contributing\nterm.everything‚ùó is written in developer friendly [Typescript](https://www.typescriptlang.org/) using the [bun](https://bun.com/) engine, with a just a smidge of C++.\nSee [./Contributing.md](./Contributing.md).\n\n## Legal:\n\nterm.everything‚ùó copyright 2025 Late for Dinner Studios, LLC\n---\nFontemon copyright 2021 Late for Dinner Studios, LLC\n---\nWing It! movie is licensed under the Creative Commons Attribution 4.0 license\n[Wing it licensing page](https://studio.blender.org/projects/wing-it/pages/licensing/)\nAttribution:\n(CC) Blender Foundation | studio.blender.org\n---\nDoom shareware episode is copyright 1993 id Software\n---\n\n## Bonus:\nThis is Gwerm the Term Worm.\n\n![this is gwern](./resources/graphics/this_is_gwern.gif)\n\nHe is doing okay. Thanks for asking.\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-10-04T02:11:31.059555"
  },
  {
    "basic_info": {
      "name": "LidAngleSensor",
      "full_name": "samhenrigold/LidAngleSensor",
      "owner": "samhenrigold",
      "description": "tfw when you when your lid when uhh angle your lid sensor",
      "url": "https://github.com/samhenrigold/LidAngleSensor",
      "clone_url": "https://github.com/samhenrigold/LidAngleSensor.git",
      "ssh_url": "git@github.com:samhenrigold/LidAngleSensor.git",
      "homepage": "https://samhenri.gold",
      "created_at": "2025-09-06T19:07:20Z",
      "updated_at": "2025-10-03T11:12:41Z",
      "pushed_at": "2025-09-08T21:30:26Z"
    },
    "stats": {
      "stars": 3395,
      "forks": 129,
      "watchers": 3395,
      "open_issues": 35,
      "size": 1486
    },
    "tech_info": {
      "language": "Objective-C",
      "languages": {
        "Objective-C": 49586
      },
      "license": "Apache License 2.0",
      "topics": []
    },
    "content": {
      "readme": "# Lid Angle Sensor\n\nHi, I‚Äôm Sam Gold. Did you know that you have ~rights~ a lid angle sensor in your MacBook? [The ~Constitution~ human interface device utility says you do.](https://youtu.be/wqnHtGgVAUE?t=21)\n\nThis is a little utility that shows the angle from the sensor and, optionally, plays a wooden door creaking sound if you adjust it reeaaaaaal slowly.\n\n## FAQ\n\n**What is a lid angle sensor?**\n\nDespite what the name would have you believe, it is a sensor that detects the angle of the lid.\n\n**Which devices have a lid angle sensor?**\n\nIt was introduced with the 2019 16-inch MacBook Pro. If your laptop is newer, you probably have it. [People have reported](https://github.com/samhenrigold/LidAngleSensor/issues/13) that it **does not work on M1 devices**, I have not yet figured out a fix.\n\n**My laptop should have it, why doesn't it show up?**\n\nI've only tested this on my M4 MacBook Pro and have hard-coded it to look for a specific sensor. If that doesn't work, try running [this script](https://gist.github.com/samhenrigold/42b5a92d1ee8aaf2b840be34bff28591) and report the output in [an issue](https://github.com/samhenrigold/LidAngleSensor/issues/new/choose).\n\nKnown problematic models:\n\n- M1 MacBook Air\n- M1 MacBook Pro\n\n**Can I use this on my iMac?**\n\n~~Not yet tested. Feel free to slam your computer into your desk and make a PR with your results.~~\n\n[It totally works](https://github.com/samhenrigold/LidAngleSensor/issues/33). If it doesn't work for you, try slamming your computer harder?\n\n**Why?**\n\nA lot of free time. I'm open to full-time work in NYC or remote. I'm a designer/design-engineer. https://samhenri.gold\n\n**No I mean like why does my laptop need to know the exact angle of its lid?**\n\nOh. I don't know.\n\n**Can I contribute?**\n\nI guess.\n\n**Why does it say it's by Lisa?**\n\nI signed up for my developer account when I was a kid, used my mom's name, and now it's stuck that way forever and I can't change it. That's life.\n\n**How come the audio feels kind of...weird?**\n\nI'm bad at audio.\n\n**Where did the sound effect come from?**\n\nLEGO Batman 3: Beyond Gotham. But you knew that already.\n\n**Can I turn off the sound?**\n\nYes, never click \"Start Audio\". But this energy isn't encouraged.\n\n## Building\n\nAccording to [this issue](https://github.com/samhenrigold/LidAngleSensor/issues/12), building requires having Xcode installed. I've only tested this on Xcode 26. YMMV.\n\n## Installation\n\nVia Homebrew:\n\n```shell\nbrew install lidanglesensor\n```\n\n## Related projects\n\n- [Python library that taps into this sensor](https://github.com/tcsenpai/pybooklid)\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-10-04T02:11:32.262769"
  },
  {
    "basic_info": {
      "name": "Dayflow",
      "full_name": "JerryZLiu/Dayflow",
      "owner": "JerryZLiu",
      "description": "Generate a timeline of your day, automatically",
      "url": "https://github.com/JerryZLiu/Dayflow",
      "clone_url": "https://github.com/JerryZLiu/Dayflow.git",
      "ssh_url": "git@github.com:JerryZLiu/Dayflow.git",
      "homepage": "",
      "created_at": "2025-09-23T01:58:21Z",
      "updated_at": "2025-10-04T02:06:23Z",
      "pushed_at": "2025-10-03T10:23:41Z"
    },
    "stats": {
      "stars": 2984,
      "forks": 120,
      "watchers": 2984,
      "open_issues": 12,
      "size": 41708
    },
    "tech_info": {
      "language": "Swift",
      "languages": {
        "Swift": 796203,
        "Shell": 27102
      },
      "license": "MIT License",
      "topics": [
        "gemini",
        "lmstudio",
        "ollama",
        "productivity",
        "productivity-tools",
        "swift",
        "time",
        "timeline"
      ]
    },
    "content": {
      "readme": "\n<div align=\"center\">\n  <img src=\"docs/images/dayflow_header.png\" alt=\"Dayflow\" width=\"400\">\n</div>\n\n<div align=\"center\">\n  <em>A timeline of your day, automatically.</em><br>\n  Turns your screen activity into a clean timeline with AI summaries and distraction highlights.\n</div>\n\n<div align=\"center\">\n  <!-- Badges -->\n  <img src=\"https://img.shields.io/badge/macOS-13%2B-000?logo=apple\" alt=\"Platform: macOS 13+\">\n  <img src=\"https://img.shields.io/badge/SwiftUI-‚úì-orange\" alt=\"SwiftUI\">\n  <img src=\"https://img.shields.io/badge/Updates-Sparkle-informational\" alt=\"Updates: Sparkle\">\n  <img src=\"https://img.shields.io/badge/AI-Gemini%20or%20Local-blue\" alt=\"AI: Gemini / Local\">\n  <img src=\"https://img.shields.io/badge/License-MIT-green\" alt=\"License: MIT\">\n</div>\n\n<div align=\"center\">\n  <img src=\"docs/images/hero_animation_1080p.gif\" alt=\"Dayflow Hero Animation\" width=\"800\">\n</div>\n\n<div align=\"center\">\n  <a href=\"https://github.com/JerryZLiu/Dayflow/releases/latest\">\n    <img src=\"https://img.shields.io/badge/Download%20for%20Mac-‚¨á%20%20Dayflow.dmg-blue?style=for-the-badge&logo=apple\" alt=\"Download for Mac\">\n  </a>\n</div>\n\n<p align=\"center\">\n  <a href=\"#quickstart\">Quickstart</a> ‚Ä¢\n  <a href=\"#why-i-built-dayflow\">Why I built Dayflow</a> ‚Ä¢\n  <a href=\"#features\">Features</a> ‚Ä¢\n  <a href=\"#how-it-works\">How it works</a> ‚Ä¢\n  <a href=\"#installation\">Installation</a> ‚Ä¢\n  <a href=\"#data--privacy\">Data & Privacy</a> ‚Ä¢\n  <a href=\"#debug--developer-tools\">Debug & Developer Tools</a> ‚Ä¢\n  <a href=\"#auto-updates-sparkle\">Auto‚Äëupdates</a> ‚Ä¢\n  <a href=\"#contributing\">Contributing</a>\n</p>\n\n---\n\n## What is Dayflow?\n\nDayflow is a **native macOS app** (SwiftUI) that records your screen at **1 FPS**, analyzes it **every 15 minutes** with AI, and generates a **timeline** of your activities with summaries. \nIt's lightweight (25MB app size) and uses ~100MB of RAM and <1% cpu. \n\n> _Privacy‚Äëminded by design_: You choose your AI provider. Use **Gemini** (bring your own API key) or **local models** (Ollama / LM Studio). See **Data & Privacy** for details.\n\n\n## Why I built Dayflow\n\nI built Dayflow after realizing that my calendar wasn't the source of truth for how I actually spent my time. My screen was. I wanted a calm, trustworthy timeline that let me see my workday without turning into yet another dashboard I had to maintain.\n\nDayflow stands for ownership and privacy by default. You control the data, you choose the AI provider, and you can keep everything local if that's what makes you comfortable. It's MIT licensed and fully open source because anything that watches your screen all day should be completely transparent about what it does with that information. The app should feel like a quiet assistant: respectful of your attention, honest about what it captures, and easy to shut off.\n\n\n---\n\n## Features\n\n- **Automatic timeline** of your day with concise summaries.\n- **1 FPS recording** - minimal CPU/storage impact.\n- **15-minute analysis intervals** for timely updates.\n- **Watch timelapses of your day**.\n- **Auto storage cleanup** - removes old recordings after 3 days.\n- **Distraction highlights** to see what pulled you off‚Äëtask.\n- **Native UX** built with **SwiftUI**.\n- **Auto‚Äëupdates** with **Sparkle** (daily check + background download).\n\n### Coming soon\n\n- **Infinitely customizable dashboard** ‚Äî ask any question about your workday, pipe the answers into tiles you arrange yourself, and track trends over time.\n\n  <div align=\"center\">\n    <img src=\"docs/images/DashboardPreview.png\" alt=\"Dayflow dashboard preview\" width=\"800\">\n  </div>\n\n- **Daily journal** ‚Äî review the highlights Dayflow captured, reflect with guided prompts, and drop screenshots or notes alongside your generated timeline.\n\n  <div align=\"center\">\n    <img src=\"docs/images/JournalPreview.png\" alt=\"Dayflow journal preview\" width=\"800\">\n  </div>\n\n## How it works\n\n1) **Capture** ‚Äî Records screen at 1 FPS in 15-second chunks.\n2) **Analyze** ‚Äî Every 15 minutes, sends recent footage to AI.\n3) **Generate** ‚Äî AI creates timeline cards with activity summaries.\n4) **Display** ‚Äî Shows your day as a visual timeline.\n5) **Cleanup** ‚Äî Auto-deletes recordings older than 3 days.\n\n### AI Processing Pipeline\n\nThe efficiency of your timeline generation depends on your chosen AI provider:\n\n```mermaid\nflowchart LR\n    subgraph Gemini[\"Gemini Flow: 2 LLM Calls\"]\n        direction LR\n        GV[Video] --> GU[Upload + Transcribe<br/>1 LLM call] --> GC[Generate Cards<br/>1 LLM call] --> GD[Done]\n    end\n\n    subgraph Local[\"Local Flow: 33+ LLM Calls\"]\n        direction LR\n        LV[Video] --> LE[Extract 30 frames] --> LD[30 descriptions<br/>30 LLM calls] --> LM[Merge<br/>1 call] --> LT[Title<br/>1 call] --> LC[Merge Check<br/>1 call] --> LMC[Merge Cards<br/>1 call] --> LD2[Done]\n    end\n\n    %% Styling\n    classDef geminiFlow fill:#e8f5e8,stroke:#4caf50,stroke-width:2px\n    classDef localFlow fill:#fff8e1,stroke:#ff9800,stroke-width:2px\n    classDef geminiStep fill:#4caf50,color:#fff\n    cla",
      "default_branch": "main"
    },
    "fetched_at": "2025-10-04T02:11:33.458707"
  },
  {
    "basic_info": {
      "name": "RustGPT",
      "full_name": "tekaratzas/RustGPT",
      "owner": "tekaratzas",
      "description": "An transformer based LLM. Written completely in Rust",
      "url": "https://github.com/tekaratzas/RustGPT",
      "clone_url": "https://github.com/tekaratzas/RustGPT.git",
      "ssh_url": "git@github.com:tekaratzas/RustGPT.git",
      "homepage": null,
      "created_at": "2025-09-13T22:05:55Z",
      "updated_at": "2025-10-03T23:42:49Z",
      "pushed_at": "2025-10-03T15:04:21Z"
    },
    "stats": {
      "stars": 2742,
      "forks": 221,
      "watchers": 2742,
      "open_issues": 5,
      "size": 177
    },
    "tech_info": {
      "language": "Rust",
      "languages": {
        "Rust": 64894
      },
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "# ü¶Ä Rust LLM from Scratch\n\n[![Rust](https://github.com/tekaratzas/RustGPT/actions/workflows/rust.yml/badge.svg)](https://github.com/tekaratzas/RustGPT/actions/workflows/rust.yml)\n\nhttps://github.com/user-attachments/assets/ec4a4100-b03a-4b3c-a7d6-806ea54ed4ed\n\nA complete **Large Language Model implementation in pure Rust** with no external ML frameworks. Built from the ground up using only `ndarray` for matrix operations.\n\n## üöÄ What This Is\n\nThis project demonstrates how to build a transformer-based language model from scratch in Rust, including:\n- **Pre-training** on factual text completion\n- **Instruction tuning** for conversational AI\n- **Interactive chat mode** for testing\n- **Full backpropagation** with gradient clipping\n- **Modular architecture** with clean separation of concerns\n\n## ‚ùå What This Isn't\n\nThis is not a production grade LLM. It is so far away from the larger models.\n\nThis is just a toy project that demonstrates how these models work under the hood.\n\n## üîç Key Files to Explore\n\nStart with these two core files to understand the implementation:\n\n- **[`src/main.rs`](src/main.rs)** - Training pipeline, data preparation, and interactive mode\n- **[`src/llm.rs`](src/llm.rs)** - Core LLM implementation with forward/backward passes and training logic\n\n## üèóÔ∏è Architecture\n\nThe model uses a **transformer-based architecture** with the following components:\n\n```\nInput Text ‚Üí Tokenization ‚Üí Embeddings ‚Üí Transformer Blocks ‚Üí Output Projection ‚Üí Predictions\n```\n\n### Project Structure\n\n```\nsrc/\n‚îú‚îÄ‚îÄ main.rs              # üéØ Training pipeline and interactive mode\n‚îú‚îÄ‚îÄ llm.rs               # üß† Core LLM implementation and training logic\n‚îú‚îÄ‚îÄ lib.rs               # üìö Library exports and constants\n‚îú‚îÄ‚îÄ transformer.rs       # üîÑ Transformer block (attention + feed-forward)\n‚îú‚îÄ‚îÄ self_attention.rs    # üëÄ Multi-head self-attention mechanism\n‚îú‚îÄ‚îÄ feed_forward.rs      # ‚ö° Position-wise feed-forward networks\n‚îú‚îÄ‚îÄ embeddings.rs        # üìä Token embedding layer\n‚îú‚îÄ‚îÄ output_projection.rs # üé∞ Final linear layer for vocabulary predictions\n‚îú‚îÄ‚îÄ vocab.rs            # üìù Vocabulary management and tokenization\n‚îú‚îÄ‚îÄ layer_norm.rs       # üßÆ Layer normalization\n‚îî‚îÄ‚îÄ adam.rs             # üèÉ Adam optimizer implementation\n\ntests/\n‚îú‚îÄ‚îÄ llm_test.rs         # Tests for core LLM functionality\n‚îú‚îÄ‚îÄ transformer_test.rs # Tests for transformer blocks\n‚îú‚îÄ‚îÄ self_attention_test.rs # Tests for attention mechanisms\n‚îú‚îÄ‚îÄ feed_forward_test.rs # Tests for feed-forward layers\n‚îú‚îÄ‚îÄ embeddings_test.rs  # Tests for embedding layers\n‚îú‚îÄ‚îÄ vocab_test.rs       # Tests for vocabulary handling\n‚îú‚îÄ‚îÄ adam_test.rs        # Tests for optimizer\n‚îî‚îÄ‚îÄ output_projection_test.rs # Tests for output layer\n```\n\n## üß™ What The Model Learns\n\nThe implementation includes two training phases:\n\n1. **Pre-training**: Learns basic world knowledge from factual statements\n   - \"The sun rises in the east and sets in the west\"\n   - \"Water flows downhill due to gravity\"\n   - \"Mountains are tall and rocky formations\"\n\n2. **Instruction Tuning**: Learns conversational patterns\n   - \"User: How do mountains form? Assistant: Mountains are formed through tectonic forces...\"\n   - Handles greetings, explanations, and follow-up questions\n\n## üöÄ Quick Start\n\n```bash\n# Clone and run\ngit clone https://github.com/tekaratzas/RustGPT.git\ncd RustGPT\ncargo run\n\n# The model will:\n# 1. Build vocabulary from training data\n# 2. Pre-train on factual statements (100 epochs)\n# 3. Instruction-tune on conversational data (100 epochs)\n# 4. Enter interactive mode for testing\n```\n\n## üéÆ Interactive Mode\n\nAfter training, test the model interactively:\n\n```\nEnter prompt: How do mountains form?\nModel output: Mountains are formed through tectonic forces or volcanism over long geological time periods\n\nEnter prompt: What causes rain?\nModel output: Rain is caused by water vapor in clouds condensing into droplets that become too heavy to remain airborne\n```\n\n## üßÆ Technical Implementation\n\n### Model Configuration\n- **Vocabulary Size**: Dynamic (built from training data)\n- **Embedding Dimension**: 128 (defined by `EMBEDDING_DIM` in `src/lib.rs`)\n- **Hidden Dimension**: 256 (defined by `HIDDEN_DIM` in `src/lib.rs`)\n- **Max Sequence Length**: 80 tokens (defined by `MAX_SEQ_LEN` in `src/lib.rs`)\n- **Architecture**: 3 Transformer blocks + embeddings + output projection\n\n### Training Details\n- **Optimizer**: Adam with gradient clipping\n- **Pre-training LR**: 0.0005 (100 epochs)\n- **Instruction Tuning LR**: 0.0001 (100 epochs)\n- **Loss Function**: Cross-entropy loss\n- **Gradient Clipping**: L2 norm capped at 5.0\n\n### Key Features\n- **Custom tokenization** with punctuation handling\n- **Greedy decoding** for text generation\n- **Gradient clipping** for training stability\n- **Modular layer system** with clean interfaces\n- **Comprehensive test coverage** for all components\n\n## üîß Development\n\n```bash\n# Run all tests\ncargo test\n\n# Test specific components\ncargo test --test llm_test\ncargo test --test transformer_test\ncargo test --test self_attention_test\n\n# Buil",
      "default_branch": "main"
    },
    "fetched_at": "2025-10-04T02:11:34.589421"
  },
  {
    "basic_info": {
      "name": "Qwen3-Omni",
      "full_name": "QwenLM/Qwen3-Omni",
      "owner": "QwenLM",
      "description": "Qwen3-omni is a natively end-to-end, omni-modal LLM developed by the Qwen team at Alibaba Cloud, capable of understanding text, audio, images, and video, as well as generating speech in real time.",
      "url": "https://github.com/QwenLM/Qwen3-Omni",
      "clone_url": "https://github.com/QwenLM/Qwen3-Omni.git",
      "ssh_url": "git@github.com:QwenLM/Qwen3-Omni.git",
      "homepage": null,
      "created_at": "2025-09-21T09:46:10Z",
      "updated_at": "2025-10-03T22:40:32Z",
      "pushed_at": "2025-09-27T02:03:00Z"
    },
    "stats": {
      "stars": 2413,
      "forks": 113,
      "watchers": 2413,
      "open_issues": 11,
      "size": 26943
    },
    "tech_info": {
      "language": "Jupyter Notebook",
      "languages": {
        "Jupyter Notebook": 38834506,
        "Python": 29676
      },
      "license": "Apache License 2.0",
      "topics": []
    },
    "content": {
      "readme": "# Qwen3-Omni\n\n<br>\n\n<p align=\"center\">\n    <img src=\"https://qianwen-res.oss-cn-beijing.aliyuncs.com//Qwen3-Omni/qwen3_omni_logo.png\" width=\"400\"/>\n<p>\n\n<p align=\"center\">\n        üíú <a href=\"https://chat.qwen.ai/\"><b>Qwen Chat</b></a>&nbsp&nbsp | &nbsp&nbspü§ó <a href=\"https://huggingface.co/collections/Qwen/qwen3-omni-68d100a86cd0906843ceccbe\">Hugging Face</a>&nbsp&nbsp | &nbsp&nbspü§ñ <a href=\"https://modelscope.cn/collections/Qwen3-Omni-867aef131e7d4f\">ModelScope</a>&nbsp&nbsp | &nbsp&nbspüìë <a href=\"https://qwen.ai/blog?id=65f766fc2dcba7905c1cb69cc4cab90e94126bf4&from=research.latest-advancements-list\">Blog</a>&nbsp&nbsp | &nbsp&nbspüìö <a href=\"https://github.com/QwenLM/Qwen3-Omni/tree/main/cookbooks\">Cookbooks</a>&nbsp&nbsp | &nbsp&nbspüìë <a href=\"https://arxiv.org/pdf/2509.17765\">Paper</a>&nbsp&nbsp\n<br>\nüñ•Ô∏è <a href=\"https://huggingface.co/spaces/Qwen/Qwen3-Omni-Demo\">Hugging Face Demo</a>&nbsp&nbsp | &nbsp&nbsp üñ•Ô∏è <a href=\"https://modelscope.cn/studios/Qwen/Qwen3-Omni-Demo\">ModelScope Demo</a>&nbsp&nbsp | &nbsp&nbspüí¨ <a href=\"https://github.com/QwenLM/Qwen/blob/main/assets/wechat.png\">WeChat (ÂæÆ‰ø°)</a>&nbsp&nbsp | &nbsp&nbspü´® <a href=\"https://discord.gg/CV4E9rpNSD\">Discord</a>&nbsp&nbsp | &nbsp&nbspüìë <a href=\"https://help.aliyun.com/zh/model-studio/user-guide/qwen-omni\">API</a>\n\n</p>\n\nWe release **Qwen3-Omni**, the natively end-to-end multilingual omni-modal foundation models. It is designed to process diverse inputs including text, images, audio, and video, while delivering real-time streaming responses in both text and natural speech. Click the video below for more information üòÉ\n\n<details open>\n<summary>English Version</summary>\n<a href=\"https://youtu.be/_zdOrPju4_g\" target=\"_blank\">\n  <img src=\"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/videocover.png\" alt=\"Open English Video\"/>\n</a>\n</details>\n\n<details>\n<summary>Chinese Version</summary>\n<a href=\"https://youtu.be/Wtjsw5deXfQ\" target=\"_blank\">\n  <img src=\"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/videocover.png\" alt=\"ÊâìÂºÄ‰∏≠ÊñáËßÜÈ¢ë\"/>\n</a>\n</details>\n\n\n## News\n* 2025.09.26: ‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è Qwen3-Omni reaches top-1 on Hugging Face Trending! \n* 2025.09.22: üéâüéâüéâ We have released [Qwen3-Omni](https://huggingface.co/collections/Qwen/qwen3-omni-68d100a86cd0906843ceccbe). For more details, please check our [blog](https://qwen.ai/blog?id=65f766fc2dcba7905c1cb69cc4cab90e94126bf4&from=research.latest-advancements-list)!\n\n## Contents <!-- omit in toc -->\n\n- [Overview](#overview)\n  - [Introduction](#introduction)\n  - [Model Architecture](#model-architecture)\n  - [Cookbooks for Usage Cases](#cookbooks-for-usage-cases)\n- [QuickStart](#quickstart)\n  - [Model Description and Download](#model-description-and-download)\n  - [Transformers Usage](#transformers-usage)\n  - [vLLM Usage](#vllm-usage)\n  - [DashScope API Usage](#dashscope-api-usage)\n  - [Usage Tips (Recommended Reading)](#usage-tips-recommended-reading)\n- [Interaction with Qwen3-Omni](#interaction-with-qwen3-omni)\n  - [Online Demo](#online-demo)\n  - [Real-Time Interaction](#real-time-interaction)\n  - [Launch Local Web UI Demo](#launch-local-web-ui-demo)\n- [Docker](#-docker)\n- [Evaluation](#evaluation)\n  - [Performance of Qwen3-Omni](#performance-of-qwen3-omni)\n  - [Setting for Evaluation](#setting-for-evaluation)\n- [Citation](#citation)\n\n## Overview\n### Introduction\n\n<p align=\"center\">\n    <img src=\"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/q3o_introduction.png\" width=\"90%\"/>\n<p>\n\nQwen3-Omni is the natively end-to-end multilingual omni-modal foundation models. It processes text, images, audio, and video, and delivers real-time streaming responses in both text and natural speech. We introduce several architectural upgrades to improve performance and efficiency. Key features:\n\n* **State-of-the-art across modalities**: Early text-first pretraining and mixed multimodal training provide native multimodal support. While achieving strong audio and audio-video results, unimodal text and image performance does not regress. Reaches SOTA on 22 of 36 audio/video benchmarks and open-source SOTA on 32 of 36; ASR, audio understanding, and voice conversation performance is comparable to Gemini 2.5 Pro.\n\n* **Multilingual**: Supports 119 text languages, 19 speech input languages, and 10 speech output languages.\n  - **Speech Input**: English, Chinese, Korean, Japanese, German, Russian, Italian, French, Spanish, Portuguese, Malay, Dutch, Indonesian, Turkish, Vietnamese, Cantonese, Arabic, Urdu.\n  - **Speech Output**: English, Chinese, French, German, Russian, Italian, Spanish, Portuguese, Japanese, Korean.\n\n* **Novel Architecture**: MoE-based Thinker‚ÄìTalker design with AuT pretraining for strong general representations, plus a multi-codebook design that drives latency to a minimum.\n\n* **Real-time Audio/Video Interaction**: Low-latency streaming with natural turn-taking and immediate text or speech responses.\n\n* **Flexible Control**: Customize behavior via system prompts for fine-grained control and easy ",
      "default_branch": "main"
    },
    "fetched_at": "2025-10-04T02:11:35.705028"
  },
  {
    "basic_info": {
      "name": "map-anything",
      "full_name": "facebookresearch/map-anything",
      "owner": "facebookresearch",
      "description": "MapAnything: Universal Feed-Forward Metric 3D Reconstruction",
      "url": "https://github.com/facebookresearch/map-anything",
      "clone_url": "https://github.com/facebookresearch/map-anything.git",
      "ssh_url": "git@github.com:facebookresearch/map-anything.git",
      "homepage": "",
      "created_at": "2025-09-04T14:37:36Z",
      "updated_at": "2025-10-03T22:39:52Z",
      "pushed_at": "2025-10-01T23:50:30Z"
    },
    "stats": {
      "stars": 1864,
      "forks": 94,
      "watchers": 1864,
      "open_issues": 24,
      "size": 6086
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 1799268,
        "Shell": 148858
      },
      "license": "Apache License 2.0",
      "topics": [
        "3d-reconstruction",
        "ai",
        "calibration",
        "depth-completion",
        "depth-estimation",
        "image-to-3d",
        "multi-view-stereo",
        "robotics",
        "sfm"
      ]
    },
    "content": {
      "readme": "<div align=\"center\">\n<h1>MapAnything: Universal Feed-Forward Metric <br>3D Reconstruction</h1>\n<a href=\"https://map-anything.github.io/assets/MapAnything.pdf\"><img src=\"https://img.shields.io/badge/Paper-blue\" alt=\"Paper\"></a>\n<a href=\"https://arxiv.org/abs/2509.13414\"><img src=\"https://img.shields.io/badge/arXiv-2509.13414-b31b1b\" alt=\"arXiv\"></a>\n<a href=\"https://map-anything.github.io/\"><img src=\"https://img.shields.io/badge/Project_Page-green\" alt=\"Project Page\"></a>\n<a href=\"https://x.com/Nik__V__/status/1968316841618518371\"><img src=\"https://img.shields.io/badge/X_Thread-1DA1F2\" alt=\"X Thread\"></a>\n<a href=\"https://huggingface.co/spaces/facebook/map-anything\"><img src='https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Demo-blue'></a>\n<br>\n<br>\n<strong>\n<a href=\"https://nik-v9.github.io/\">Nikhil Keetha<sup>1,2</sup></a>\n&nbsp;&nbsp;\n<a href=\"https://sirwyver.github.io/\">Norman M√ºller<sup>1</sup></a>\n&nbsp;&nbsp;\n<a href=\"https://demuc.de/\">Johannes Sch√∂nberger<sup>1</sup></a>\n&nbsp;&nbsp;\n<a href=\"https://www.linkedin.com/in/lorenzoporzi\">Lorenzo Porzi<sup>1</sup></a>\n&nbsp;&nbsp;\n<a href=\"https://infinity1096.github.io/\">Yuchen Zhang<sup>2</sup></a>\n<br>\n<a href=\"https://tobiasfshr.github.io/\">Tobias Fischer<sup>1</sup></a>\n&nbsp;&nbsp;\n<a href=\"https://www.linkedin.com/in/arno-knapitsch\">Arno Knapitsch<sup>1</sup></a>\n&nbsp;&nbsp;\n<a href=\"https://www.linkedin.com/in/duncan-zauss\">Duncan Zauss<sup>1</sup></a>\n&nbsp;&nbsp;\n<a href=\"https://ethanweber.me/\">Ethan Weber<sup>1</sup></a>\n&nbsp;&nbsp;\n<a href=\"https://www.linkedin.com/in/nelsonantunes7\">Nelson Antunes<sup>1</sup></a>\n<br>\n<a href=\"https://x.com/jonathonluiten?lang=en\">Jonathon Luiten<sup>1</sup></a>\n&nbsp;&nbsp;\n<a href=\"https://m.lopezantequera.com/\">Manuel Lopez-Antequera<sup>1</sup></a>\n&nbsp;&nbsp;\n<a href=\"https://scholar.google.com/citations?user=484sccEAAAAJ\">Samuel Rota Bul√≤<sup>1</sup></a>\n&nbsp;&nbsp;\n<a href=\"https://richardt.name/\">Christian Richardt<sup>1</sup></a>\n<br>\n<a href=\"https://www.cs.cmu.edu/~deva/\">Deva Ramanan<sup>2</sup></a>\n&nbsp;&nbsp;\n<a href=\"https://theairlab.org/team/sebastian/\">Sebastian Scherer<sup>2</sup></a>\n&nbsp;&nbsp;\n<a href=\"https://www.linkedin.com/in/peter-kontschieder-2a6410134\">Peter Kontschieder<sup>1</sup></a>\n<br>\n<br>\n<sup>1</sup> Meta &nbsp;&nbsp;\n<sup>2</sup> Carnegie Mellon University\n</strong>\n\n</div>\n\n## Overview\n\nMapAnything is a simple, end-to-end trained transformer model that directly regresses the factored metric 3D geometry of a scene given various types of inputs (images, calibration, poses, or depth). A single feed-forward model supports over 12 different 3D reconstruction tasks including multi-image sfm, multi-view stereo, monocular metric depth estimation, registration, depth completion and more.\n\n![Overview](./assets/teaser.png)\n\n## Table of Contents\n\n- [Quick Start](#quick-start)\n  - [Installation](#installation)\n  - [Image-Only Inference](#image-only-inference)\n  - [Multi-Modal Inference](#multi-modal-inference)\n- [Interactive Demos](#interactive-demos)\n  - [Online Demo](#online-demo)\n  - [Local Gradio Demo](#local-gradio-demo)\n  - [Rerun Demo](#rerun-demo)\n  - [Demo Inference on COLMAP outputs](#demo-inference-on-colmap-outputs)\n- [COLMAP & GSplat Support](#colmap--gsplat-support)\n  - [Exporting to COLMAP Format](#exporting-to-colmap-format)\n  - [Integration with Gaussian Splatting](#integration-with-gaussian-splatting)\n- [Data Processing for Training & Benchmarking](#data-processing-for-training--benchmarking)\n- [Training](#training)\n- [Benchmarking](#benchmarking)\n- [Code License](#code-license)\n- [Models](#models)\n- [Building Blocks for MapAnything](#building-blocks-for-mapanything)\n- [Acknowledgments](#acknowledgments)\n- [Citation](#citation)\n\n## Quick Start\n\n### Installation\n\n```bash\ngit clone https://github.com/facebookresearch/map-anything.git\ncd map-anything\n\n# Create and activate conda environment\nconda create -n mapanything python=3.12 -y\nconda activate mapanything\n\n# Optional: Install torch, torchvision & torchaudio specific to your system\n# Install MapAnything\npip install -e .\n\n# For all optional dependencies\n# See pyproject.toml for more details\npip install -e \".[all]\"\npre-commit install\n```\n\nNote that we don't pin a specific version of PyTorch or CUDA in our requirements. Please feel free to install PyTorch based on your specific system.\n\n### Image-Only Inference\n\nFor metric 3D reconstruction from images without additional geometric inputs:\n\n```python\n# Optional config for better memory efficiency\nimport os\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n\n# Required imports\nimport torch\nfrom mapanything.models import MapAnything\nfrom mapanything.utils.image import load_images\n\n# Get inference device\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# Init model - This requries internet access or the huggingface hub cache to be pre-downloaded\n# For Apache 2.0 license model, use \"facebook/map-anything-apache\"\nmodel = MapAnythin",
      "default_branch": "main"
    },
    "fetched_at": "2025-10-04T02:11:36.813578"
  },
  {
    "basic_info": {
      "name": "bdh",
      "full_name": "pathwaycom/bdh",
      "owner": "pathwaycom",
      "description": "Baby Dragon Hatchling (BDH) ‚Äì Architecture and Code",
      "url": "https://github.com/pathwaycom/bdh",
      "clone_url": "https://github.com/pathwaycom/bdh.git",
      "ssh_url": "git@github.com:pathwaycom/bdh.git",
      "homepage": "",
      "created_at": "2025-09-30T12:05:01Z",
      "updated_at": "2025-10-04T01:57:26Z",
      "pushed_at": "2025-10-02T08:32:06Z"
    },
    "stats": {
      "stars": 1757,
      "forks": 54,
      "watchers": 1757,
      "open_issues": 0,
      "size": 996
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 8798
      },
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "## Baby Dragon Hatchling\nThis repository contains source code from the paper: Adrian Kosowski, Przemys≈Çaw Uzna≈Ñski, Jan Chorowski, Zuzanna Stamirowska, Micha≈Ç Bartoszkiewicz, _\"The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain\"_, [link](https://doi.org/10.48550/arXiv.2509.26507).\n\n## Architecture\n<img src=\"figs/architecture.png\" width=\"600\"/> \n\n## Relation to Tranformers\n<img src=\"figs/vocab.png\" width=\"600\"/> \n\n## Scaling laws\n<img src=\"figs/bdh_scaling.png\" width=\"600\"/> \n\n## Abstract:\nThe relationship between computing systems and the brain has served as motivation for pioneering theoreticians since John von Neumann and Alan Turing. \nUniform, scale-free biological networks, such as the brain, have powerful properties, including generalizing over time, which is the main barrier for Machine Learning on the path to Universal Reasoning Models.\n\nWe introduce `Dragon Hatchling' (BDH), a new Large Language Model architecture based on a scale-free biologically inspired network of $n$ locally-interacting neuron particles. BDH couples strong theoretical foundations and inherent interpretability without sacrificing Transformer-like performance.\n\nBDH is a practical, performant state-of-the-art \nattention-based state space sequence learning architecture. \nIn addition to being a graph model, BDH admits a GPU-friendly formulation.\nIt exhibits Transformer-like scaling laws: we find empirically that BDH rivals GPT2-architecture Transformer performance on language and translation tasks, at the same number of parameters (10M to 1B), for the same training data.\n\nBDH provides theoretical foundations for understanding model behavior in the limit of large size and reasoning time. \nOur results, formalized as a chain of reductions of expressiveness in the framework of computational Complexity Theory and Distributed Computing, and combined with findings on the BDH model, show a macro-to-micro correspondence of function between the general attention mechanisms in state-of-the-art Language Models, and attention mechanisms observed in the brain. These attention mechanisms formally converge as closed-form local graph dynamics at neurons and synapses: _the equations of reasoning_.\n\nBDH can be represented as a brain model. It contains $n$ neurons, organized as an excitatory circuit and an inhibitory circuit with integrate-and-fire thresholding of input signals at neurons. The working memory of BDH during inference entirely relies on synaptic plasticity with Hebbian learning using spiking neurons, at potentiation scales of minutes for the brain (up to hundreds of tokens). We confirm empirically that specific, individual synapses strengthen connection whenever BDH hears or reasons about a specific concept while processing language inputs. The neuron interaction network of BDH is a graph of high modularity with heavy-tailed degree distribution. The BDH model is biologically plausible, explaining one possible mechanism which human neurons could use to achieve speech.\n\nBDH is designed for interpretability. Activation vectors of BDH are sparse and positive. We demonstrate monosemanticity in BDH on language tasks, including representation of concept abstractions, which happens even for small models, below 100M-parameter scale. Interpretability of state, which goes beyond interpretability of neurons and model parameters, is an inherent feature of the BDH architecture. \n\nWe believe BDH opens the door to a new theory of _Thermodynamic Limit_ behavior for language and reasoning models, with the ultimate goal of Probably Approximately Correct (PAC)-like bounds for generalization of reasoning over time.\n\n## Running the code\n\nTo train and sample from the BDH model on a toy language modeling task please do:\n1. `pip install -r requirements.txt`\n2. `python train.py`\n\n## Acknowledgements\nWe thank Andrej Karpathy for the [nanoGPT](https://github.com/karpathy/nanoGPT/) code and the tiny Shapespeare dataset used in this demonstration.\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-10-04T02:11:37.933736"
  },
  {
    "basic_info": {
      "name": "The-Accidental-CTO",
      "full_name": "subhashchy/The-Accidental-CTO",
      "owner": "subhashchy",
      "description": "How I Scaled from Zero to a Million Store on Dukaan,  Without a CS Degree.  .. A System Design Handbook by  Subhash Choudhary ",
      "url": "https://github.com/subhashchy/The-Accidental-CTO",
      "clone_url": "https://github.com/subhashchy/The-Accidental-CTO.git",
      "ssh_url": "git@github.com:subhashchy/The-Accidental-CTO.git",
      "homepage": "",
      "created_at": "2025-09-26T09:07:20Z",
      "updated_at": "2025-10-04T01:54:18Z",
      "pushed_at": "2025-09-29T20:20:39Z"
    },
    "stats": {
      "stars": 1728,
      "forks": 129,
      "watchers": 1728,
      "open_issues": 5,
      "size": 12232
    },
    "tech_info": {
      "language": null,
      "languages": {},
      "license": null,
      "topics": [
        "scaling",
        "system-design"
      ]
    },
    "content": {
      "readme": "\n# **The Accidental CTO**\n\n## **How I Scaled from Zero to a Million Stores on Dukaan, Without a CS Degree**\n\nI never set out to be a CTO. In fact, I didn‚Äôt even have a computer science degree. But somewhere between firefighting server crashes at 3 a.m. and obsessing over replication lag graphs, I found myself building systems that would eventually power over a **million online stores** at Dukaan.\n\nThis book, *The Accidental CTO*, is my behind-the-scenes account of that journey. It‚Äôs not a dry academic manual filled with abstract diagrams. Instead, it‚Äôs a story-driven handbook ‚Äî one that mixes late-night startup battles with the **hard system design lessons** that only come from being in the trenches.\n\nFrom scaling a scrappy MVP to running massive distributed pipelines, I‚Äôll take you through the challenges we faced and the decisions that made (or nearly broke) us.\n\n---\n\n### What You‚Äôll Learn Inside\n\n* **Scaling applications**: How we went from thousands to millions of users without falling apart.\n* **Replication, sharding, caching, queues**: When to use them, when *not* to, and what tradeoffs they carry.\n* **Observability as survival**: Why metrics, logs, traces, SLAs, and SLOs aren‚Äôt optional ‚Äî they‚Äôre lifelines.\n* **Resilience engineering**: Circuit breakers, retries, graceful degradation ‚Äî designing for failure, not against it.\n* **The hidden costs of cloud**: Why at scale, your AWS bill can become your biggest investor, and when it makes sense to go self-hosted.\n* **The consistency/availability/latency triangle**: Why you can never fully win, and how to navigate the tradeoffs in real systems.\n\n---\n\n### Why I Wrote This Book\n\nI didn‚Äôt want to write another \"theory of distributed systems\" book. There are already plenty of those.\n\nWhat I wanted to share is the **practical side** of system design ‚Äî the part you only learn when a real company, with real customers and real money at stake, is on fire. The part where you‚Äôre not solving toy interview questions but dealing with:\n\n* angry merchants refreshing dashboards,\n* Kafka pipelines silently choking on one bad partition,\n* a database replica 10 minutes behind and nobody knowing why.\n\nThis is the stuff no textbook teaches you.\n\n---\n\n### Who This Book Is For\n\nWhether you‚Äôre a **software engineer**, **architect**, or **startup founder**, I wrote this book to help you see distributed systems not as academic puzzles, but as **living, evolving machines** that you can actually build, operate, and grow.\n\nIf you‚Äôve ever wondered *how real companies actually scale* ‚Äî not in theory, but in practice ‚Äî this is my candid, first-hand story.\n\nAnd maybe, just maybe, you‚Äôll find a bit of yourself in *The Accidental CTO*.",
      "default_branch": "main"
    },
    "fetched_at": "2025-10-04T02:11:39.038612"
  },
  {
    "basic_info": {
      "name": "HunyuanImage-3.0",
      "full_name": "Tencent-Hunyuan/HunyuanImage-3.0",
      "owner": "Tencent-Hunyuan",
      "description": "HunyuanImage-3.0: A Powerful Native Multimodal Model for Image Generation",
      "url": "https://github.com/Tencent-Hunyuan/HunyuanImage-3.0",
      "clone_url": "https://github.com/Tencent-Hunyuan/HunyuanImage-3.0.git",
      "ssh_url": "git@github.com:Tencent-Hunyuan/HunyuanImage-3.0.git",
      "homepage": "https://hunyuan.tencent.com/image",
      "created_at": "2025-09-27T07:18:47Z",
      "updated_at": "2025-10-04T01:57:27Z",
      "pushed_at": "2025-10-02T06:24:24Z"
    },
    "stats": {
      "stars": 1641,
      "forks": 53,
      "watchers": 1641,
      "open_issues": 22,
      "size": 34775
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 374880,
        "Shell": 806
      },
      "license": "Other",
      "topics": [
        "image-generation",
        "native-multimodal-model"
      ]
    },
    "content": {
      "readme": "[‰∏≠ÊñáÊñáÊ°£](./README_zh_CN.md)\n\n<div align=\"center\">\n\n<img src=\"./assets/logo.png\" alt=\"HunyuanImage-3.0 Logo\" width=\"600\">\n\n# üé® HunyuanImage-3.0: A Powerful Native Multimodal Model for Image Generation\n\n</div>\n\n\n<div align=\"center\">\n<img src=\"./assets/banner.png\" alt=\"HunyuanImage-3.0 Banner\" width=\"800\">\n\n</div>\n\n<div align=\"center\">\n  <a href=https://hunyuan.tencent.com/image target=\"_blank\"><img src=https://img.shields.io/badge/Official%20Site-333399.svg?logo=homepage height=22px></a>\n  <a href=https://huggingface.co/tencent/HunyuanImage-3.0 target=\"_blank\"><img src=https://img.shields.io/badge/%F0%9F%A4%97%20Models-d96902.svg height=22px></a>\n  <a href=https://github.com/Tencent-Hunyuan/HunyuanImage-3.0 target=\"_blank\"><img src= https://img.shields.io/badge/Page-bb8a2e.svg?logo=github height=22px></a>\n  <a href=https://arxiv.org/pdf/2509.23951 target=\"_blank\"><img src=https://img.shields.io/badge/Report-b5212f.svg?logo=arxiv height=22px></a>\n  <a href=https://x.com/TencentHunyuan target=\"_blank\"><img src=https://img.shields.io/badge/Hunyuan-black.svg?logo=x height=22px></a>\n  <a href=https://docs.qq.com/doc/DUVVadmhCdG9qRXBU target=\"_blank\"><img src=https://img.shields.io/badge/üìö-PromptHandBook-blue.svg?logo=book height=22px></a>\n</div>\n\n\n<p align=\"center\">\n    üëè Join our <a href=\"./assets/WECHAT.md\" target=\"_blank\">WeChat</a> and <a href=\"https://discord.gg/ehjWMqF5wY\">Discord</a> | \nüíª <a href=\"https://hunyuan.tencent.com/modelSquare/home/play?modelId=289&from=/visual\">Official website(ÂÆòÁΩë) Try our model!</a>&nbsp&nbsp\n</p>\n\n## üî•üî•üî• News\n- **September 28, 2025**: üìñ **HunyuanImage-3.0 Technical Report Released** - Comprehensive technical documentation now available\n- **September 28, 2025**: üöÄ **HunyuanImage-3.0 Open Source Release** - Inference code and model weights publicly available\n\n\n## üß© Community Contributions\n\nIf you develop/use HunyuanImage-3.0 in your projects, welcome to let us know.\n\n## üìë Open-source Plan\n\n- HunyuanImage-3.0 (Image Generation Model)\n  - [x] Inference \n  - [x] HunyuanImage-3.0 Checkpoints\n  - [ ] HunyuanImage-3.0-Instruct Checkpoints (with reasoning)\n  - [ ] VLLM Support\n  - [ ] Distilled Checkpoints\n  - [ ] Image-to-Image Generation\n  - [ ] Multi-turn Interaction\n\n\n## üóÇÔ∏è Contents\n- [üî•üî•üî• News](#-news)\n- [üß© Community Contributions](#-community-contributions)\n- [üìë Open-source Plan](#-open-source-plan)\n- [üìñ Introduction](#-introduction)\n- [‚ú® Key Features](#-key-features)\n- [üõ†Ô∏è Dependencies and Installation](#-dependencies-and-installation)\n  - [üíª System Requirements](#-system-requirements)\n  - [üì¶ Environment Setup](#-environment-setup)\n  - [üì• Install Dependencies](#-install-dependencies)\n  - [Performance Optimizations](#performance-optimizations)\n- [üöÄ Usage](#-usage)\n  - [üî• Quick Start with Transformers](#-quick-start-with-transformers)\n  - [üè† Local Installation & Usage](#-local-installation--usage)\n  - [üé® Interactive Gradio Demo](#-interactive-gradio-demo)\n- [üß± Models Cards](#-models-cards)\n- [üìù Prompt Guide](#-prompt-guide)\n  - [Manually Writing Prompts](#manually-writing-prompts)\n  - [System Prompt For Automatic Rewriting the Prompt](#system-prompt-for-automatic-rewriting-the-prompt)\n  - [Advanced Tips](#advanced-tips)\n  - [More Cases](#more-cases)\n- [üìä Evaluation](#-evaluation)\n- [üìö Citation](#-citation)\n- [üôè Acknowledgements](#-acknowledgements)\n- [üåüüöÄ  Github Star History](#-github-star-history)\n\n---\n\n## üìñ Introduction\n\n**HunyuanImage-3.0** is a groundbreaking native multimodal model that unifies multimodal understanding and generation within an autoregressive framework. Our text-to-image module achieves performance **comparable to or surpassing** leading closed-source models.\n\n\n<div align=\"center\">\n  <img src=\"./assets/framework.png\" alt=\"HunyuanImage-3.0 Framework\" width=\"90%\">\n</div>\n\n## ‚ú® Key Features\n\n* üß† **Unified Multimodal Architecture:** Moving beyond the prevalent DiT-based architectures, HunyuanImage-3.0 employs a unified autoregressive framework. This design enables a more direct and integrated modeling of text and image modalities, leading to surprisingly effective and contextually rich image generation.\n\n* üèÜ **The Largest Image Generation MoE Model:** This is the largest open-source image generation Mixture of Experts (MoE) model to date. It features 64 experts and a total of 80 billion parameters, with 13 billion activated per token, significantly enhancing its capacity and performance.\n\n* üé® **Superior Image Generation Performance:** Through rigorous dataset curation and advanced reinforcement learning post-training, we've achieved an optimal balance between semantic accuracy and visual excellence. The model demonstrates exceptional prompt adherence while delivering photorealistic imagery with stunning aesthetic quality and fine-grained details.\n\n* üí≠ **Intelligent World-Knowledge Reasoning:** The unified multimodal architecture endows HunyuanImage-3.0 with powerful reasoning capabilities. It leverages its extensive world knowledge to intelligently interpret user int",
      "default_branch": "main"
    },
    "fetched_at": "2025-10-04T02:11:40.153237"
  },
  {
    "basic_info": {
      "name": "Super-Mario-Bros.-Remastered-Public",
      "full_name": "JHDev2006/Super-Mario-Bros.-Remastered-Public",
      "owner": "JHDev2006",
      "description": "A Remake / Celebration of the original 'Super Mario Bros.' games. Features new levels, custom modes, new characters, alongside a full level editor / custom level system!",
      "url": "https://github.com/JHDev2006/Super-Mario-Bros.-Remastered-Public",
      "clone_url": "https://github.com/JHDev2006/Super-Mario-Bros.-Remastered-Public.git",
      "ssh_url": "git@github.com:JHDev2006/Super-Mario-Bros.-Remastered-Public.git",
      "homepage": "",
      "created_at": "2025-09-13T15:29:58Z",
      "updated_at": "2025-10-04T00:37:27Z",
      "pushed_at": "2025-10-04T00:37:24Z"
    },
    "stats": {
      "stars": 1598,
      "forks": 193,
      "watchers": 1598,
      "open_issues": 181,
      "size": 55365
    },
    "tech_info": {
      "language": "GDScript",
      "languages": {
        "GDScript": 968723,
        "GAP": 36590,
        "C#": 10724,
        "GDShader": 2322
      },
      "license": "GNU General Public License v3.0",
      "topics": []
    },
    "content": {
      "readme": "# Super Mario Bros Remastered\nA Remake / Celebration of the original 'Super Mario Bros.' games. Features new levels, custom modes, new characters, alongside a full level editor / custom level system!\n\n<img width=\"3840\" height=\"2160\" alt=\"SMB1R_BANNER_printable\" src=\"https://github.com/user-attachments/assets/ed0e97a8-614a-44e2-b69f-2654fca6196c\" />\n\n### Art by [@krystalphantasm.bsky.social](https://bsky.app/profile/krystalphantasm.bsky.social/post/3lvgmgvjeks2f)\n\n### Download: https://github.com/JHDev2006/Super-Mario-Bros.-Remastered-Public/releases\n\n# Requires an original SMB1 NES ROM to play! None of the original assets are contained in the source code, unless it was originally made by us!\n\n# This does NOT act as a replacement for the original Super Mario Bros. games. Super Mario Bros. & Super Mario Bros.: The Lost Levels, can be played now on Nintendo Switch, through Nintendo Switch Online\n\n## Features\n- Super Mario Bros., Super Mario Bros.: The Lost Levels, Super Mario Bros. Special and All Night Nippon: Super Mario Bros. Fully recreated from the ground up!\n- Improved physics / level design\n- Resource Packs! Fully customize how the game looks and sounds.\n- Custom Characters - Add in your own characters to use in game.\n- Fully Open Source!\n- Level Share Square Partnered\n- Portable mode by creating `portable.txt` in the executable directory\n\n## Downloading\n\n### Windows/Linux\n1. Go to the 'Releases' page\n2. Look for the latest version\n3. Download the .zip for your OS\n4. Extract and run\n5. Enjoy!\n\n### macOS (Unofficial)\n1. Go to the [macOS repo](https://github.com/yuriko-shimizu/Super-Mario-Bros.-Remastered-Public-Mac/releases)\n2. (NOTE: THIS IS AN UNOFFICIAL FORK OF THE GAME)\n3. Look for the latest version\n4. Download the .zip file\n5. Extract, drag into the 'Applictions' folder and run\n6. Enjoy!\n\n## Importing for editing\n1. Download the source\n2. Download Godot 4.5 beta 3\n3. Import the project\n4. Enjoy!\n\n## Contributing\nYou are more than welcome to contribute any fixes / improvements you'd like, simply open a pull request, and I'll review it ASAP!\n\n## System Requirements\n\nPlease refer to the Godot engine requirements for minimum and recommended hardware specifications.\n\n[Minimum Requirements](https://docs.godotengine.org/en/latest/about/system_requirements.html#desktop-or-laptop-pc-minimum)\n\n[Recommended Requirements](https://docs.godotengine.org/en/latest/about/system_requirements.html#id3)\n\n\n## Issues\nWhen opening an issue, please keep it to one report, per post, and try and be as helpful as possible, when telling me what has occured, so that its as easy to fix as possible.\nPlease do not open issues, for feature requests, suggestions, or opinions. BUG REPORTS ONLY\n\n## Known Issues\nThere are a couple known issues, mainly due to being built off of Godot, and these issues existing in the engine itself.\n- Steam deck controls do not work natively, you can circumvent this by setting up controller bindings to emulate keys instead, apologies.\n- Physics are weird, when interacting with corners + the camera barrier\n- Drop shadows jitter when playing with \"Smooth Rendering\"\n- Several entities jitter at times.\n- Blocks + coins, respawn when reloading resource packs\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-10-04T02:11:41.275548"
  },
  {
    "basic_info": {
      "name": "VoxCPM",
      "full_name": "OpenBMB/VoxCPM",
      "owner": "OpenBMB",
      "description": "VoxCPM: Tokenizer-Free TTS for Context-Aware Speech Generation and True-to-Life Voice Cloning",
      "url": "https://github.com/OpenBMB/VoxCPM",
      "clone_url": "https://github.com/OpenBMB/VoxCPM.git",
      "ssh_url": "git@github.com:OpenBMB/VoxCPM.git",
      "homepage": "",
      "created_at": "2025-09-16T03:41:49Z",
      "updated_at": "2025-10-04T02:01:32Z",
      "pushed_at": "2025-09-30T02:47:46Z"
    },
    "stats": {
      "stars": 1586,
      "forks": 167,
      "watchers": 1586,
      "open_issues": 26,
      "size": 1456
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 113165
      },
      "license": "Apache License 2.0",
      "topics": []
    },
    "content": {
      "readme": "## üéôÔ∏è VoxCPM: Tokenizer-Free TTS for Context-Aware Speech Generation and True-to-Life Voice Cloning\n\n\n[![Project Page](https://img.shields.io/badge/Project%20Page-GitHub-blue)](https://github.com/OpenBMB/VoxCPM/) [![Technical Report](https://img.shields.io/badge/Technical%20Report-Arxiv-red)](https://arxiv.org/abs/2509.24650) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-OpenBMB-yellow)](https://huggingface.co/openbmb/VoxCPM-0.5B) [![ModelScope](https://img.shields.io/badge/ModelScope-OpenBMB-purple)](https://modelscope.cn/models/OpenBMB/VoxCPM-0.5B)  [![Live Playground](https://img.shields.io/badge/Live%20PlayGround-Demo-orange)](https://huggingface.co/spaces/OpenBMB/VoxCPM-Demo) [![Samples](https://img.shields.io/badge/Audio%20Samples-Page-green)](https://openbmb.github.io/VoxCPM-demopage)\n\n\n\n<div align=\"center\">\n  <img src=\"assets/voxcpm_logo.png\" alt=\"VoxCPM Logo\" width=\"40%\">\n</div>\n\n<div align=\"center\">\n\nüëã Contact us on [WeChat](assets/wechat.png)\n\n</div>\n\n## News \n* [2025.09.30] üî• üî• üî•  We Release VoxCPM [Technical Report](https://arxiv.org/abs/2509.24650)!\n* [2025.09.16] üî• üî• üî•  We Open Source the VoxCPM-0.5B [weights](https://huggingface.co/openbmb/VoxCPM-0.5B)!\n* [2025.09.16] üéâ üéâ üéâ  We Provide the [Gradio PlayGround](https://huggingface.co/spaces/OpenBMB/VoxCPM-Demo) for VoxCPM-0.5B, try it now! \n\n## Overview\n\nVoxCPM is a novel tokenizer-free Text-to-Speech (TTS) system that redefines realism in speech synthesis. By modeling speech in a continuous space, it overcomes the limitations of discrete tokenization and enables two flagship capabilities: context-aware speech generation and true-to-life zero-shot voice cloning.\n\nUnlike mainstream approaches that convert speech to discrete tokens, VoxCPM uses an end-to-end diffusion autoregressive architecture that directly generates continuous speech representations from text. Built on [MiniCPM-4](https://huggingface.co/openbmb/MiniCPM4-0.5B) backbone, it achieves implicit semantic-acoustic decoupling through hierachical language modeling and FSQ constraints, greatly enhancing both expressiveness and generation stability.\n\n<div align=\"center\">\n  <img src=\"assets/voxcpm_model.png\" alt=\"VoxCPM Model Architecture\" width=\"90%\">\n</div>\n\n\n###  üöÄ Key Features\n- **Context-Aware, Expressive Speech Generation** - VoxCPM comprehends text to infer and generate appropriate prosody, delivering speech with remarkable expressiveness and natural flow. It spontaneously adapts speaking style based on content, producing highly fitting vocal expression trained on a massive 1.8 million-hour bilingual corpus.\n- **True-to-Life Voice Cloning** - With only a short reference audio clip, VoxCPM performs accurate zero-shot voice cloning, capturing not only the speaker‚Äôs timbre but also fine-grained characteristics such as accent, emotional tone, rhythm, and pacing to create a faithful and natural replica.\n- **High-Efficiency Synthesis** - VoxCPM supports streaming synthesis with a Real-Time Factor (RTF) as low as 0.17 on a consumer-grade NVIDIA RTX 4090 GPU, making it possible for real-time applications.\n\n\n\n\n\n##  Quick Start\n\n### üîß Install from PyPI\n``` sh\npip install voxcpm\n```\n### 1.  Model Download (Optional)\nBy default, when you first run the script, the model will be downloaded automatically, but you can also download the model in advance.\n- Download VoxCPM-0.5B\n    ```\n    from huggingface_hub import snapshot_download\n    snapshot_download(\"openbmb/VoxCPM-0.5B\")\n    ```\n- Download ZipEnhancer and SenseVoice-Small. We use ZipEnhancer to enhance speech prompts and SenseVoice-Small for speech prompt ASR in the web demo.\n    ```\n    from modelscope import snapshot_download\n    snapshot_download('iic/speech_zipenhancer_ans_multiloss_16k_base')\n    snapshot_download('iic/SenseVoiceSmall')\n    ```\n\n### 2. Basic Usage\n```python\nimport soundfile as sf\nimport numpy as np\nfrom voxcpm import VoxCPM\n\nmodel = VoxCPM.from_pretrained(\"openbmb/VoxCPM-0.5B\")\n\n# Non-streaming\nwav = model.generate(\n    text=\"VoxCPM is an innovative end-to-end TTS model from ModelBest, designed to generate highly expressive speech.\",\n    prompt_wav_path=None,      # optional: path to a prompt speech for voice cloning\n    prompt_text=None,          # optional: reference text\n    cfg_value=2.0,             # LM guidance on LocDiT, higher for better adherence to the prompt, but maybe worse\n    inference_timesteps=10,   # LocDiT inference timesteps, higher for better result, lower for fast speed\n    normalize=True,           # enable external TN tool\n    denoise=True,             # enable external Denoise tool\n    retry_badcase=True,        # enable retrying mode for some bad cases (unstoppable)\n    retry_badcase_max_times=3,  # maximum retrying times\n    retry_badcase_ratio_threshold=6.0, # maximum length restriction for bad case detection (simple but effective), it could be adjusted for slow pace speech\n)\n\nsf.write(\"output.wav\", wav, 16000)\nprint(\"saved: output.wav\")\n\n# Streaming\nchunks = []",
      "default_branch": "main"
    },
    "fetched_at": "2025-10-04T02:11:42.385859"
  },
  {
    "basic_info": {
      "name": "jscamp",
      "full_name": "midudev/jscamp",
      "owner": "midudev",
      "description": "Contenido y ejercicios del JSCamp InfoJobs",
      "url": "https://github.com/midudev/jscamp",
      "clone_url": "https://github.com/midudev/jscamp.git",
      "ssh_url": "git@github.com:midudev/jscamp.git",
      "homepage": "https://jscamp.dev",
      "created_at": "2025-09-28T13:28:51Z",
      "updated_at": "2025-10-04T02:08:46Z",
      "pushed_at": "2025-10-01T18:32:24Z"
    },
    "stats": {
      "stars": 1390,
      "forks": 101,
      "watchers": 1390,
      "open_issues": 2,
      "size": 141
    },
    "tech_info": {
      "language": "CSS",
      "languages": {
        "CSS": 5675,
        "HTML": 4226
      },
      "license": null,
      "topics": [
        "bootcamp"
      ]
    },
    "content": {
      "readme": "![CleanShot 2025-10-01 at 20 11 30@2x](https://github.com/user-attachments/assets/b6ef8402-d367-4a99-b939-8f11dedf91d7)\n\n# üöÄ JSCamp InfoJobs\n\nBienvenidos al bootcamp intensivo de JavaScript y desarrollo web full-stack dise√±ado para llevarte desde los fundamentos hasta las tecnolog√≠as m√°s avanzadas del ecosistema JavaScript. Veremos HTML, CSS, JavaScript, TypeScript, Node.js, SQL, CI/CD y Docker.\n\n## üé® El Proyecto Pr√°ctico\n\n![CleanShot 2025-10-01 at 20 26 08@2x](https://github.com/user-attachments/assets/d9abec4d-ac41-4962-845c-93006bfe768b)\n\nA lo largo de este bootcamp, construiremos un proyecto completo **desde cero y paso a paso**, aplicando todos los conocimientos de cada m√≥dulo.\n\nüëâ [Ver dise√±o del proyecto](https://stitch.withgoogle.com/projects/7508115667617706440)\n\nEste proyecto te permitir√° consolidar todo lo aprendido y tener una aplicaci√≥n real en tu portafolio.\n\n## üì∫ La Plataforma\n\nEn **[JSCamp.dev](https://jscamp.dev)** encontrar√°s todos los videos y contenido del bootcamp para que puedas revisarlo cuando quieras. El registro es gratis.\nLos videos y materiales se ir√°n subiendo **poco a poco** a medida que avancemos en el bootcamp.\n\n### ¬øTiene certificado?\n\nS√≠, existe un certificado opcional y muy limitado de pago que incluye:\n\n- üéì **Certificado Digital** - Certifica tus logros en el bootcamp\n- üìù **Seguimiento de Ejercicios** - Revisaremos y corregiremos tus ejercicios\n- üí¨ **Canal Exclusivo en Discord** - Comunidad premium y soporte directo\n- üé• **Directos Exclusivos** - Clases de repaso exclusivas con dudas y preguntas\n- üìÑ **Revisi√≥n de tu CV** - Equipo de expertos revisan tu CV y te dan feedback\n- üè¢ **Workshop Presencial** - Entrada asegurada a los workshops de Barcelona y Madrid\n\n**Entra a [https://jscamp.dev](https://jscamp.dev), inicia sesi√≥n y consigue acceso.**\n\n## üìö Contenido del Bootcamp\n\n- **00** - HTML & CSS\n- **01** - JavaScript\n- **02** - React\n- **03** - Estado Global y React Router\n- **04** - Node.js\n- **05** - TypeScript\n- **06** - SQL\n- **07** - CI/CD\n- **08** - Docker\n\n## üíª Requisitos de Instalaci√≥n\n\nAntes de comenzar, aseg√∫rate de tener instalado el siguiente software:\n\n- **Navegador moderno** - Chrome, Firefox, Edge o Safari actualizado\n- **[Visual Studio Code](https://code.visualstudio.com/)** - Editor de c√≥digo (recomendado)\n- **[Extensi√≥n Live Preview](https://marketplace.visualstudio.com/items?itemName=ms-vscode.live-server)** - Extensi√≥n para ver HTML/CSS\n- **[Node.js](https://nodejs.org/)** (versi√≥n 20 o superior) - Runtime de JavaScript\n- **[Git](https://git-scm.com/)** - Control de versiones\n- **[Docker](https://www.docker.com/)** - Para el m√≥dulo de Docker\n- **[Terminal Warp](https://midu.link/warp)** - Terminal con IA y Agentes\n\n## üë®‚Äçüíª Instructor\n\nEste bootcamp es impartido por **midudev**, desarrollador y creador de contenido educativo con una gran comunidad en espa√±ol.\n\n### üåê Redes Sociales\n\n- üê¶ **X**: [@midudev](https://twitter.com/midudev)\n- üì∫ **YouTube**: [@midudev](https://youtube.com/@midudev)\n- üéÆ **Twitch**: [midudev](https://twitch.tv/midudev)\n- üì∏ **Instagram**: [@midu.dev](https://instagram.com/midu.dev)\n- üíº **LinkedIn**: [midudev](https://linkedin.com/in/midudev)\n- üåç **Web**: [midu.dev](https://midu.dev)\n\n## üéØ Objetivos\n\nAl finalizar JSCAMP ser√°s capaz de:\n\n- ‚úÖ Construir aplicaciones web completas desde cero\n- ‚úÖ Dominar el ecosistema de JavaScript moderno\n- ‚úÖ Crear APIs REST con Node.js\n- ‚úÖ Desarrollar interfaces con React\n- ‚úÖ Implementar bases de datos SQL\n- ‚úÖ Configurar pipelines de CI/CD\n- ‚úÖ Containerizar aplicaciones con Docker\n- ‚úÖ Aplicar TypeScript en proyectos reales\n\n## üöÄ C√≥mo Empezar\n\nCada m√≥dulo contiene ejercicios pr√°cticos y proyectos reales. Navega a la carpeta correspondiente y sigue las instrucciones.\n\n```bash\n# Clona el repositorio\ngit clone git@github.com:midudev/jscamp.git\n\n# Navega al m√≥dulo que desees\ncd jscamp/00-html-css\n\n# ¬°Comienza a aprender!\n```\n\n---\n\n‚≠êÔ∏è Si este contenido te resulta √∫til, no olvides dar una estrella al repositorio\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-10-04T02:11:43.481505"
  },
  {
    "basic_info": {
      "name": "Paper2Agent",
      "full_name": "jmiao24/Paper2Agent",
      "owner": "jmiao24",
      "description": "Paper2Agent is a multi-agent AI system that automatically transforms research papers into interactive AI agents with minimal human input.",
      "url": "https://github.com/jmiao24/Paper2Agent",
      "clone_url": "https://github.com/jmiao24/Paper2Agent.git",
      "ssh_url": "git@github.com:jmiao24/Paper2Agent.git",
      "homepage": null,
      "created_at": "2025-09-09T11:06:50Z",
      "updated_at": "2025-10-04T01:50:39Z",
      "pushed_at": "2025-09-21T17:12:30Z"
    },
    "stats": {
      "stars": 1201,
      "forks": 185,
      "watchers": 1201,
      "open_issues": 2,
      "size": 570
    },
    "tech_info": {
      "language": "Jupyter Notebook",
      "languages": {
        "Jupyter Notebook": 480650,
        "Python": 20766,
        "Shell": 18320
      },
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "<p align=\"center\">\n  <img src=\"./logo/paper2agent_logo.png\" alt=\"Paper2Agent Logo\" width=\"600px\" />\n</p>\n\n# Paper2Agent: Reimagining Papers As AI Agents\n\n## üìñ Overview\n`Paper2Agent` is a multi-agent AI system that automatically transforms research papers into interactive AI agents with minimal human input. Here are some [Demos](#-demos) of the Paper2Agent-generated agent.\n\n## üöÄ Quick Start \n\n### Basic Usage\nAutomatically detects and runs all relevant tutorials from a research paper‚Äôs codebase.\n\n> **‚ö†Ô∏è Prerequisites**: Complete the [installation & setup](#Ô∏è-installation--setup) below before running Paper2Agent.\n>\n> **‚è±Ô∏è Runtime & Cost**: Processing time varies from 30 minutes to 3+ hours based on codebase complexity. Estimated cost: ~$15 for complex repositories like AlphaGenome using Claude Sonnet 4 (one-time cost).\n\n```bash\ncd Paper2Agent\n\nbash Paper2Agent.sh \\\n  --project_dir <PROJECT_DIR> \\\n  --github_url <GITHUB_URL>\n```\n\n### Advanced Usage\n\n#### Targeted Tutorial Processing\nProcess only specific tutorials by title or URL:\n\n```bash\nbash Paper2Agent.sh \\\n  --project_dir <PROJECT_DIR> \\\n  --github_url <GITHUB_URL> \\\n  --tutorials <TUTORIALS_URL or TUTORIALS_TITLE>\n```\n\n#### Repository with API Key\nFor repositories requiring authentication:\n\n```bash\nbash Paper2Agent.sh \\\n  --project_dir <PROJECT_DIR> \\\n  --github_url <GITHUB_URL> \\\n  --api <API_KEY>\n```\n\n### Parameters\n\n**Required:**\n- `--project_dir <directory>`: Name of the project directory to create\n  - Example: `TISSUE_Agent`\n- `--github_url <url>`: GitHub repository URL to analyze\n  - Example: `https://github.com/sunericd/TISSUE`\n\n**Optional:**\n- `--tutorials <filter>`: Filter tutorials by title or URL\n  - Example: `\"Preprocessing and clustering\"` or tutorial URL\n- `--api <key>`: API key for repositories requiring authentication\n  - Example: `your_api_key_here`\n\n### Examples\n\n#### TISSUE Agent\nCreate an AI agent from the [TISSUE](https://github.com/sunericd/TISSUE) research paper codebase for uncertainty-calibrated single-cell spatial transcriptomics analysis:\n\n```bash\nbash Paper2Agent.sh \\\n  --project_dir TISSUE_Agent \\\n  --github_url https://github.com/sunericd/TISSUE\n```\n\n#### Scanpy Agent for Preprocessing and Clustering\nCreate an AI agent from the [Scanpy](https://github.com/scverse/scanpy) research paper codebase for single-cell analysis preprocessing and clustering:\n\n```bash\n# Filter by tutorial title\nbash Paper2Agent.sh \\\n  --project_dir Scanpy_Agent \\\n  --github_url https://github.com/scverse/scanpy \\\n  --tutorials \"Preprocessing and clustering\"\n\n# Filter by tutorial URL\nbash Paper2Agent.sh \\\n  --project_dir Scanpy_Agent \\\n  --github_url https://github.com/scverse/scanpy \\\n  --tutorials \"https://github.com/scverse/scanpy/blob/main/docs/tutorials/basics/clustering.ipynb\"\n```\n\n#### AlphaGenome Agent\nCreate an AI agent from the [AlphaGenome](https://github.com/google-deepmind/alphagenome) research paper codebase for genomic data interpretation:\n\n```bash\nbash Paper2Agent.sh \\\n  --project_dir AlphaGenome_Agent \\\n  --github_url https://github.com/google-deepmind/alphagenome \\\n  --api <ALPHAGENOME_API_KEY>\n```\n\n## ‚öôÔ∏è Installation & Setup\n\n### Prerequisites\n- **Python**: Version 3.10 or higher\n- **Claude Code**: Install following instructions at [anthropic.com/claude-code](https://www.anthropic.com/claude-code)\n\n### Installation Steps\n1. **Clone the Paper2Agent Repository**\n   ```bash\n   git clone https://github.com/jmiao24/Paper2Agent.git\n   cd Paper2Agent\n   ```\n\n2. **Install Python Dependencies**\n   ```bash\n   pip install fastmcp\n   ```\n\n3. **Install and Configure Claude Code**\n   ```bash\n   npm install -g @anthropic-ai/claude-code\n   claude\n   ```\n\n## ü§ñ How to Create a Paper Agent?\nTo streamline usage, we recommend creating Paper Agents by connecting Paper MCP servers to an AI coding agent, such as [Claude Code](https://www.anthropic.com/claude-code) or the [Google Gemini CLI](https://google-gemini.github.io/gemini-cli/) (it's free with a Google account!).\nWe are also actively developing our own base agent, which will be released soon.\n\n### Automatic Launch\nAfter pipeline completion, Claude Code will automatically open with your new MCP server loaded.\n\n### Manual Launch with Local MCP Server\nTo restart your agent later:\n```bash\ncd <working_dir>\nfastmcp install claude-code <project_dir>/src/<repo_name>_mcp.py \\\n--python <project_dir>/<repo_name>-env/bin/python\n```\n\n### Manual Launch with Remote MCP Server Hosted on Hugging Face\nTo create a paper agent in Claude Code with the Paper MCP server of interest, use the following script with your own working directory, MCP name, and server URL:\n```bash\nbash launch_remote_mcp.sh \\\n  --working_dir <working_dir> \\\n  --mcp_name <mcp_name> \\\n  --mcp_url <remote_mcp_url>\n```\n\nFor example, to create an AlphaGenome Agent, run:\n```bash\nbash launch_remote_mcp.sh \\\n  --working_dir analysis_dir \\\n  --mcp_name alphagenome \\\n  --mcp_url https://Paper2Agent-alphagenome-mcp.hf.space\n```\n\n‚úÖ You will now have an **AlphaGe",
      "default_branch": "main"
    },
    "fetched_at": "2025-10-04T02:11:44.601114"
  },
  {
    "basic_info": {
      "name": "sj.h",
      "full_name": "rxi/sj.h",
      "owner": "rxi",
      "description": "A tiny little JSON parsing library",
      "url": "https://github.com/rxi/sj.h",
      "clone_url": "https://github.com/rxi/sj.h.git",
      "ssh_url": "git@github.com:rxi/sj.h.git",
      "homepage": null,
      "created_at": "2025-09-19T18:20:20Z",
      "updated_at": "2025-10-03T23:43:34Z",
      "pushed_at": "2025-09-21T18:29:44Z"
    },
    "stats": {
      "stars": 1180,
      "forks": 26,
      "watchers": 1180,
      "open_issues": 4,
      "size": 10
    },
    "tech_info": {
      "language": "C",
      "languages": {
        "C": 4081
      },
      "license": "The Unlicense",
      "topics": []
    },
    "content": {
      "readme": "# sj.h\nA tiny little JSON parsing library\n\n- ~150 lines of C99\n- Zero-allocations with minimal state\n- Error messages with `line:column:` location\n- No number parsing: `strtod`, `atoi`? Handle them how you want\n- No string parsing: bring your own unicode surrogate pair handling (or don't)\n\n\n## Usage\nA small program to load a rectangle from a JSON string into a `Rect` struct:\n```c\nchar *json_text = \"{ \\\"x\\\": 10, \\\"y\\\": 20, \\\"w\\\": 30, \\\"h\\\": 40 }\";\n\ntypedef struct { int x, y, w, h; } Rect;\n\nbool eq(sj_Value val, char *s) {\n    size_t len = val.end - val.start;\n    return strlen(s) == len && !memcmp(s, val.start, len);\n}\n\nint main(void) {\n    Rect rect = {0};\n\n    sj_Reader r = sj_reader(json_text, strlen(json_text));\n    sj_Value obj = sj_read(&r);\n\n    sj_Value key, val;\n    while (sj_iter_object(&r, obj, &key, &val)) {\n        if (eq(key, \"x\")) { rect.x = atoi(val.start); }\n        if (eq(key, \"y\")) { rect.y = atoi(val.start); }\n        if (eq(key, \"w\")) { rect.w = atoi(val.start); }\n        if (eq(key, \"h\")) { rect.h = atoi(val.start); }\n    }\n\n    printf(\"rect: { %d, %d, %d, %d }\\n\", rect.x, rect.y, rect.w, rect.h);\n    return 0;\n}\n```\n\nSee the [**demo**](demo/) folder for further usage examples.\n\n\n## License\nThis is free and unencumbered software released into the public domain. See\n[LICENSE](LICENSE) for details.",
      "default_branch": "master"
    },
    "fetched_at": "2025-10-04T02:11:45.691057"
  },
  {
    "basic_info": {
      "name": "sidekick.nvim",
      "full_name": "folke/sidekick.nvim",
      "owner": "folke",
      "description": "Your Neovim AI sidekick",
      "url": "https://github.com/folke/sidekick.nvim",
      "clone_url": "https://github.com/folke/sidekick.nvim.git",
      "ssh_url": "git@github.com:folke/sidekick.nvim.git",
      "homepage": "",
      "created_at": "2025-09-26T10:26:48Z",
      "updated_at": "2025-10-04T02:09:23Z",
      "pushed_at": "2025-10-03T14:05:14Z"
    },
    "stats": {
      "stars": 1180,
      "forks": 18,
      "watchers": 1180,
      "open_issues": 7,
      "size": 324
    },
    "tech_info": {
      "language": "Lua",
      "languages": {
        "Lua": 150855,
        "Shell": 127
      },
      "license": "Apache License 2.0",
      "topics": [
        "claude-code",
        "codex-cli",
        "copilot",
        "copilot-cli",
        "gemini-cli",
        "neovim",
        "neovim-plugin",
        "nvim",
        "nvim-plugin"
      ]
    },
    "content": {
      "readme": "# ü§ñ `sidekick.nvim`\n\n**sidekick.nvim** is your Neovim AI sidekick that integrates Copilot LSP's\n\"Next Edit Suggestions\" with a built-in terminal for any AI CLI.\nReview and apply diffs, chat with AI assistants, and streamline your coding,\nwithout leaving your editor.\n\n<img width=\"2311\" height=\"1396\" alt=\"image\" src=\"https://github.com/user-attachments/assets/63a33610-9a8e-45e2-bbd0-b7e3a4fde621\" />\n\n## ‚ú® Features\n\n- **ü§ñ Next Edit Suggestions (NES) powered by Copilot LSP**\n  - ü™Ñ **Automatic Suggestions**: Fetches suggestions automatically when you pause typing or move the cursor.\n  - üé® **Rich Diffs**: Visualizes changes with inline and block-level diffs, featuring Treesitter-based syntax highlighting with granular diffing down to the word or character level.\n  - üß≠ **Hunk-by-Hunk Navigation**: Jump through edits to review them one by one before applying.\n  - üìä **Statusline Integration**: Shows Copilot LSP's status, request progress, and preview text in your statusline.\n\n- **üí¨ Integrated AI CLI Terminal**\n  - üöÄ **Direct Access to AI CLIs**: Interact with your favorite AI command-line tools without leaving Neovim.\n  - üì¶ **Pre-configured for Popular Tools**: Out-of-the-box support for Claude, Gemini, Grok, Codex, Copilot CLI, and more.\n  - ‚ú® **Context-Aware Prompts**: Automatically include file content, cursor position, and diagnostics in your prompts.\n  - üìù **Prompt Library**: A library of pre-defined prompts for common tasks like explaining code, fixing issues, or writing tests.\n  - üîÑ **Session Persistence**: Keep your CLI sessions alive with `tmux` and `zellij` integration.\n  - üìÇ **Automatic File Watching**: Automatically reloads files in Neovim when they are modified by AI tools.\n\n- **üîå Extensible and Customizable**\n  - ‚öôÔ∏è **Flexible Configuration**: Fine-tune every aspect of the plugin to your liking.\n  - üß© **Plugin-Friendly API**: A rich API for integrating with other plugins and building custom workflows.\n  - üé® **Customizable UI**: Change the appearance of diffs, signs, and more.\n\n## üìã Requirements\n\n- **Neovim** `>= 0.11.2` or newer\n- The official [copilot-language-server](https://github.com/github/copilot-language-server-release) LSP server,\n  enabled with `vim.lsp.enable`. Can be installed in multiple ways:\n  1. install using `npm` or your OS's package manager\n  2. install with [mason-lspconfig.nvim](https://github.com/mason-org/mason-lspconfig.nvim)\n  3. [copilot.lua](https://github.com/zbirenbaum/copilot.lua) and [copilot.vim](https://github.com/github/copilot.vim)\n     both bundle the LSP Server in their plugin.\n- A working `lsp/copilot.lua` configuration.\n  - **TIP:** Included in [nvim-lspconfig](https://github.com/neovim/nvim-lspconfig)\n- [snacks.nvim](https://github.com/folke/snacks.nvim) for better prompt/tool selection **_(optional)_**\n- [nvim-treesitter-textobjects](https://github.com/nvim-treesitter/nvim-treesitter-textobjects) **_(`main` branch)_** for `{function}` and `{class}` context variables **_(optional)_**\n- AI cli tools, such as Codex, Claude, Copilot, Gemini, ‚Ä¶ **_(optional)_**\n  see the [ü§ñ AI CLI Integration](#-ai-cli-integration) section for details.\n\n## üöÄ Quick Start\n\n1. **Install** the plugin with your package manager (see below)\n2. **Configure Copilot LSP** - must be enabled with `vim.lsp.enable`\n3. **Check health**: `:checkhealth sidekick`\n4. **Sign in to Copilot**: `:LspCopilotSignIn`\n5. **Try it out**:\n   - Type some code and pause - watch for Next Edit Suggestions appearing\n   - Press `<Tab>` to navigate through or apply suggestions\n   - Use `<leader>aa` to open AI CLI tools\n\n> [!NOTE]\n> **New to Next Edit Suggestions?** Unlike inline completions, NES suggests entire refactorings or multi-line changes anywhere in your file - think of it as Copilot's \"big picture\" suggestions.\n\n## üì¶ Installation\n\nInstall with your favorite manager. With [lazy.nvim](https://github.com/folke/lazy.nvim):\n\n<!-- setup_base:start -->\n\n```lua\n{\n  \"folke/sidekick.nvim\",\n  opts = {\n    -- add any options here\n    cli = {\n      mux = {\n        backend = \"zellij\",\n        enabled = true,\n      },\n    },\n  },\n  -- stylua: ignore\n  keys = {\n    {\n      \"<tab>\",\n      function()\n        -- if there is a next edit, jump to it, otherwise apply it if any\n        if not require(\"sidekick\").nes_jump_or_apply() then\n          return \"<Tab>\" -- fallback to normal tab\n        end\n      end,\n      expr = true,\n      desc = \"Goto/Apply Next Edit Suggestion\",\n    },\n    {\n      \"<leader>aa\",\n      function() require(\"sidekick.cli\").toggle() end,\n      desc = \"Sidekick Toggle CLI\",\n    },\n    {\n      \"<leader>as\",\n      function() require(\"sidekick.cli\").select() end,\n      -- Or to select only installed tools:\n      -- require(\"sidekick.cli\").select({ filter = { installed = true } })\n      desc = \"Select CLI\",\n    },\n    {\n      \"<leader>at\",\n      function() require(\"sidekick.cli\").send({ msg = \"{this}\" }) end,\n      mode = { \"x\", \"n\" },\n      desc = \"Send This\",\n    },\n    {\n      \"<leader>av\",\n      function() requ",
      "default_branch": "main"
    },
    "fetched_at": "2025-10-04T02:11:46.825130"
  },
  {
    "basic_info": {
      "name": "Selene",
      "full_name": "MoonTechLab/Selene",
      "owner": "MoonTechLab",
      "description": "‰∏Ä‰∏™‰ª• MoonTV v100 ÁâàÊú¨ / Helios ‰∏∫ÂêéÁ´ØÁöÑ Android/iOS ÂÆ¢Êà∑Á´ØÔºåÈíàÂØπÁßªÂä®Á´Ø‰ΩìÈ™å‰ºòÂåñ",
      "url": "https://github.com/MoonTechLab/Selene",
      "clone_url": "https://github.com/MoonTechLab/Selene.git",
      "ssh_url": "git@github.com:MoonTechLab/Selene.git",
      "homepage": "",
      "created_at": "2025-09-20T08:29:29Z",
      "updated_at": "2025-10-04T02:07:06Z",
      "pushed_at": "2025-09-27T12:52:08Z"
    },
    "stats": {
      "stars": 1157,
      "forks": 147,
      "watchers": 1157,
      "open_issues": 0,
      "size": 7586
    },
    "tech_info": {
      "language": null,
      "languages": {},
      "license": null,
      "topics": []
    },
    "content": {
      "readme": "# Selene\n\n<div align=\"center\">\n  <img src=\"logo.jpg\" alt=\"Selene Logo\" width=\"120\">\n</div>\n\n> üé¨ **Selene** ÊòØ‰ª• [MoonTV](https://github.com/MoonTechLab/LunaTV) v100 ÁâàÊú¨ / [Helios](https://github.com/MoonTechLab/Helios) ‰∏∫ÂêéÁ´ØÁöÑÂÆ¢Êà∑Á´ØÔºå‰øùËØÅÂéüÊ±ÅÂéüÂë≥ÁöÑÂêåÊó∂Ôºå‰ºòÂåñ‰∫ÜÁßªÂä®Á´ØÊìç‰Ωú‰ΩìÈ™å„ÄÇÂÆÉÂü∫‰∫é **Flutter** ÊûÑÂª∫Ôºå‰∏ì‰∏∫ÁßªÂä®Á´ØÊâìÈÄ†ÔºåÁõÆÂâçÊîØÊåÅ Android-ArmV8 Âíå iOS Âπ≥Âè∞„ÄÇ\n\n<div align=\"center\">\n\n![Flutter](https://img.shields.io/badge/Flutter-3.4.3-02569B?logo=flutter)\n![Dart](https://img.shields.io/badge/Dart-3.4.3-0175C2?logo=dart)\n![Android](https://img.shields.io/badge/Android-21+-3DDC84?logo=android)\n![iOS](https://img.shields.io/badge/iOS-12.0+-000000?logo=ios)\n\n</div>\n\n<details>\n  <summary>ÁÇπÂáªÊü•ÁúãÈ°πÁõÆÊà™Âõæ</summary>\n  <img src=\"screenshot1.jpg\" alt=\"È°πÁõÆÊà™Âõæ\" width=300>\n  <img src=\"screenshot2.jpg\" alt=\"È°πÁõÆÊà™Âõæ\" width=300>\n  <img src=\"screenshot3.jpg\" alt=\"È°πÁõÆÊà™Âõæ\" width=300>\n  <img src=\"screenshot4.jpg\" alt=\"È°πÁõÆÊà™Âõæ\" width=300>\n  <img src=\"screenshot5.jpg\" alt=\"È°πÁõÆÊà™Âõæ\" width=300>\n  <img src=\"screenshot6.jpg\" alt=\"È°πÁõÆÊà™Âõæ\" width=300>\n</details>\n\n### ËØ∑‰∏çË¶ÅÂú® BÁ´ô„ÄÅÂ∞èÁ∫¢‰π¶„ÄÅÂæÆ‰ø°ÂÖ¨‰ºóÂè∑„ÄÅÊäñÈü≥„ÄÅ‰ªäÊó•Â§¥Êù°ÊàñÂÖ∂‰ªñ‰∏≠ÂõΩÂ§ßÈôÜÁ§æ‰∫§Âπ≥Âè∞ÂèëÂ∏ÉËßÜÈ¢ëÊàñÊñáÁ´†ÂÆ£‰º†Êú¨È°πÁõÆÔºå‰∏çÊéàÊùÉ‰ªª‰Ωï‚ÄúÁßëÊäÄÂë®Âàä/ÊúàÂàä‚ÄùÁ±ªÈ°πÁõÆÊàñÁ´ôÁÇπÊî∂ÂΩïÊú¨È°πÁõÆ„ÄÇ\n\n---\n\n## ‚ú® ÂäüËÉΩÁâπÊÄß\n\n### üéØ Ê†∏ÂøÉÂäüËÉΩ\n- **Â§öÊ∫êËÅöÂêàÊêúÁ¥¢** - ÊîØÊåÅÂ§ö‰∏™ËßÜÈ¢ëÊ∫êÁöÑËÅöÂêàÊêúÁ¥¢ÔºåÂø´ÈÄüÊâæÂà∞ÊÉ≥ÁúãÁöÑÂÜÖÂÆπ\n- **Êô∫ËÉΩÊí≠ÊîæËÆ∞ÂΩï** - Ëá™Âä®ËÆ∞ÂΩïÊí≠ÊîæËøõÂ∫¶ÔºåÊîØÊåÅÊñ≠ÁÇπÁª≠Êí≠\n- **‰∏™‰∫∫Êî∂ËóèÂ§π** - Êî∂ËóèÂñúÊ¨¢ÁöÑÂΩ±ËßÜ‰ΩúÂìÅÔºåÊñπ‰æøÈöèÊó∂ËßÇÁúã\n- **Â§öÂπ≥Âè∞ÊîØÊåÅ** - ÊîØÊåÅÁîµÂΩ±„ÄÅÁîµËßÜÂâß„ÄÅÂä®Êº´„ÄÅÁªºËâ∫Á≠âÂ§öÁßçÂÜÖÂÆπÁ±ªÂûã\n\n### üé® Áî®Êà∑‰ΩìÈ™å\n- **Áé∞‰ª£Âåñ UI** - Âü∫‰∫é Material Design 3 ÁöÑÁé∞‰ª£ÂåñÁïåÈù¢ËÆæËÆ°\n- **Ê∑±Ëâ≤Ê®°Âºè** - ÊîØÊåÅÊ∑±Ëâ≤/ÊµÖËâ≤‰∏ªÈ¢òÂàáÊç¢ÔºåÊä§ÁúºÊõ¥ËàíÈÄÇ\n- **ÊµÅÁïÖÂä®Áîª** - ‰∏∞ÂØåÁöÑ‰∫§‰∫íÂä®ÁîªÔºåÊèêÂçá‰ΩøÁî®‰ΩìÈ™å\n\n### üîß ÊäÄÊúØÁâπÊÄß\n- **È´òÊÄßËÉΩÊí≠Êîæ** - Âü∫‰∫é FVP Êí≠ÊîæÂô®ÔºåÊîØÊåÅÂ§öÁßçËßÜÈ¢ëÊ†ºÂºè\n- **Êô∫ËÉΩÁºìÂ≠ò** - Ë±ÜÁì£Êï∞ÊçÆÁºìÂ≠òÊú∫Âà∂ÔºåÊèêÂçáÂä†ËΩΩÈÄüÂ∫¶\n- **ÁΩëÁªú‰ºòÂåñ** - ÊîØÊåÅ SSE ÂÆûÊó∂ÊêúÁ¥¢ÔºåÂìçÂ∫îÊõ¥ËøÖÈÄü\n\n## üì± ÊîØÊåÅÂπ≥Âè∞\n\n- **Android** - ÊúÄ‰ΩéÊîØÊåÅ Android 5.0 (API 21)\n- **iOS** - ÊúÄ‰ΩéÊîØÊåÅ iOS 12.0\n\n## üìñ ‰ΩøÁî®ËØ¥Êòé\n\n### È¶ñÊ¨°‰ΩøÁî®\n1. ÂêØÂä®Â∫îÁî®ÂêéÔºåÁ≥ªÁªü‰ºöËá™Âä®Ê£ÄÊü•ÁôªÂΩïÁä∂ÊÄÅ\n2. Â¶ÇÊú™ÁôªÂΩïÔºå‰ºöË∑≥ËΩ¨Âà∞ÁôªÂΩïÈ°µÈù¢\n3. ÁôªÂΩïÊàêÂäüÂêéËøõÂÖ•‰∏ªÁïåÈù¢\n\n### ‰∏ªË¶ÅÂäüËÉΩ\n- **È¶ñÈ°µ** - Êü•ÁúãÁÉ≠Èó®ÂÜÖÂÆπ„ÄÅÁªßÁª≠ËßÇÁúã„ÄÅ‰∏™‰∫∫Êî∂Ëóè\n- **ÊêúÁ¥¢** - Â§öÊ∫êËÅöÂêàÊêúÁ¥¢ÔºåÊîØÊåÅÂÆûÊó∂ÊêúÁ¥¢Âª∫ËÆÆ\n- **ÂàÜÁ±ªÊµèËßà** - ÊåâÁîµÂΩ±„ÄÅÁîµËßÜÂâß„ÄÅÂä®Êº´„ÄÅÁªºËâ∫ÂàÜÁ±ªÊµèËßà\n- **Êí≠ÊîæÂô®** - ÊîØÊåÅÂ§öÁßçÊí≠ÊîæÊéßÂà∂ÔºåËá™Âä®ËÆ∞ÂΩïÊí≠ÊîæËøõÂ∫¶\n\n## üèóÔ∏è ÊäÄÊúØÊû∂ÊûÑ\n\n### Ê†∏ÂøÉÊäÄÊúØÊ†à\n- **Flutter** - Ë∑®Âπ≥Âè∞ UI Ê°ÜÊû∂\n- **Dart** - ÁºñÁ®ãËØ≠Ë®Ä\n- **Provider** - Áä∂ÊÄÅÁÆ°ÁêÜ\n- **Dio** - HTTP ÁΩëÁªúËØ∑Ê±Ç\n- **FVP** - ËßÜÈ¢ëÊí≠ÊîæÂô®\n\n## ‚ö†Ô∏è ÂÖçË¥£Â£∞Êòé\n\n**ÈáçË¶ÅÊèêÈÜíÔºö**\n\n1. **‰ªÖ‰æõÂ≠¶‰π†‰∫§ÊµÅ** - Êú¨È°πÁõÆ‰ªÖÁî®‰∫éÊäÄÊúØÂ≠¶‰π†Âíå‰∫§ÊµÅÁõÆÁöÑÔºå‰∏çÊèê‰æõ‰ªª‰ΩïÂïÜ‰∏öÊúçÂä°„ÄÇ\n\n2. **ÂÜÖÂÆπÊù•Ê∫ê** - Êú¨Â∫îÁî®ËÅöÂêàÁöÑÂÜÖÂÆπÊù•Ê∫ê‰∫éÁ¨¨‰∏âÊñπÂπ≥Âè∞ÔºåÊàë‰ª¨‰∏çÂØπÂÜÖÂÆπÁöÑÂêàÊ≥ïÊÄß„ÄÅÂáÜÁ°ÆÊÄß„ÄÅÂÆåÊï¥ÊÄßÊàñÂèØÁî®ÊÄßÊâøÊãÖ‰ªª‰ΩïË¥£‰ªª„ÄÇ\n\n3. **ÁâàÊùÉÂ£∞Êòé** - ÊâÄÊúâÂΩ±ËßÜÂÜÖÂÆπÁöÑÁâàÊùÉÂΩíÂéü‰ΩúËÄÖÂíåÁâàÊùÉÊñπÊâÄÊúâÔºåËØ∑Áî®Êà∑Ëá™ËßâÈÅµÂÆàÁõ∏ÂÖ≥Ê≥ïÂæãÊ≥ïËßÑÔºåÊîØÊåÅÊ≠£Áâà„ÄÇ\n\n4. **‰ΩøÁî®È£éÈô©** - Áî®Êà∑‰ΩøÁî®Êú¨Â∫îÁî®ÊâÄ‰∫ßÁîüÁöÑ‰ªª‰ΩïÁõ¥Êé•ÊàñÈó¥Êé•ÊçüÂ§±ÔºåÂºÄÂèëËÄÖ‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªª„ÄÇ\n\n5. **ÂêàËßÑ‰ΩøÁî®** - ËØ∑Áî®Êà∑Âú®‰ΩøÁî®ËøáÁ®ã‰∏≠ÈÅµÂÆàÂΩìÂú∞Ê≥ïÂæãÊ≥ïËßÑÔºå‰∏çÂæóÁî®‰∫é‰ªª‰ΩïËøùÊ≥ïÁî®ÈÄî„ÄÇ\n\n6. **Êï∞ÊçÆÂÆâÂÖ®** - ËôΩÁÑ∂Êàë‰ª¨ÈáçËßÜÁî®Êà∑ÈöêÁßÅÔºå‰ΩÜËØ∑Áî®Êà∑Ëá™Ë°åÊâøÊãÖÊï∞ÊçÆÂÆâÂÖ®È£éÈô©„ÄÇ\n\n**‰ΩøÁî®Êú¨Â∫îÁî®Âç≥Ë°®Á§∫ÊÇ®Â∑≤ÈòÖËØªÂπ∂ÂêåÊÑè‰∏äËø∞ÂÖçË¥£Â£∞Êòé„ÄÇ**\n\n## üôè Ëá¥Ë∞¢\n\n- [MoonTV](https://github.com/MoonTechLab/LunaTV) - ÂêéÁ´ØÊúçÂä°ÊîØÊåÅ\n- [Flutter](https://flutter.dev/) - Ë∑®Âπ≥Âè∞ÂºÄÂèëÊ°ÜÊû∂\n- ÊâÄÊúâÁî®Êà∑ÁöÑÊîØÊåÅ\n---\n\n<div align=\"center\">\n  <p>Â¶ÇÊûúËøô‰∏™È°πÁõÆÂØπÊÇ®ÊúâÂ∏ÆÂä©ÔºåËØ∑Áªô‰∏™ ‚≠êÔ∏è ÊîØÊåÅ‰∏Ä‰∏ãÔºÅ</p>\n</div>\n\n[![Star History Chart](https://api.star-history.com/svg?repos=MoonTechLab/Selene&type=Date)](https://www.star-history.com/#MoonTechLab/Selene&Date)\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-10-04T02:11:47.921505"
  },
  {
    "basic_info": {
      "name": "cfnew",
      "full_name": "byJoey/cfnew",
      "owner": "byJoey",
      "description": null,
      "url": "https://github.com/byJoey/cfnew",
      "clone_url": "https://github.com/byJoey/cfnew.git",
      "ssh_url": "git@github.com:byJoey/cfnew.git",
      "homepage": null,
      "created_at": "2025-09-06T17:10:05Z",
      "updated_at": "2025-10-04T01:29:44Z",
      "pushed_at": "2025-10-04T01:05:52Z"
    },
    "stats": {
      "stars": 1151,
      "forks": 834,
      "watchers": 1151,
      "open_issues": 9,
      "size": 12975
    },
    "tech_info": {
      "language": null,
      "languages": {},
      "license": null,
      "topics": []
    },
    "content": {
      "readme": "### ÊÑüËßâË∑≥ËΩ¨Âä†Áæ§ÊúâÁÇπÊµÅÊ∞ìË°å‰∏∫ ÊîπÊàê\n\n<img width=\"1708\" height=\"884\" alt=\"image\" src=\"https://github.com/user-attachments/assets/ca35ae39-6971-4291-b182-28cb292c0353\" />\n\n\nÊÉ≥Âä†Áæ§ÁöÑËá™Â∑±ÁÇπÂáªÊ∑ªÂä†Âêß tg‰∫§ÊµÅÁæ§ https://t.me/+ft-zI76oovgwNmRh \n\n###  Snippets\n\n<img width=\"1128\" height=\"801\" alt=\"image\" src=\"https://github.com/user-attachments/assets/ae108dd2-c543-4a63-b448-d56d4d520e1d\" />\n\n#### Âä†ÂÖ•Â§öÂÆ¢Êà∑Á´ØÊîØÊåÅ ÂüüÂêç/‰Ω†ÁöÑuuidÂç≥ÂèØÁúãËßÅ\n\n###  ÈÖçÂ•óÂ∑•ÂÖ∑\n\n| Á±ªÂûã | ÊèèËø∞ | ÈìæÊé• |\n| :--- | :--- | :--- |\n|  **ÊñáÂ≠óÊïôÁ®ã** | ËØ¶ÁªÜÁöÑÈÉ®ÁΩ≤‰∏é‰ΩøÁî®ËØ¥ÊòéÂçöÂÆ¢ÊñáÁ´† | [https://joeyblog.net/yuanchuang/1146.html](https://joeyblog.net/yuanchuang/1146.html) |\n|  **WorkersËßÜÈ¢ëÊïôÁ®ã** | Áõ¥ËßÇÁöÑÊìç‰ΩúÊºîÁ§∫ÂíåÂäüËÉΩËÆ≤Ëß£ | https://youtu.be/Rlypv_iswD8 |\n|  **SnippetsËßÜÈ¢ëÊïôÁ®ã** | Áõ¥ËßÇÁöÑÊìç‰ΩúÊºîÁ§∫ÂíåÂäüËÉΩËÆ≤Ëß£ | https://www.youtube.com/watch?v=xeFeH3Akcu8 |\n\n###  ÈÉ®ÁΩ≤\n\t\nÂä†ÂÖ•‰∫ÜÂçÉÂëº‰∏áÂî§ÁöÑËÆ¢ÈòÖÊØè15ÂàÜÈíüËá™Âä®‰ºòÈÄâ‰∏ÄÊ¨°\n| ÂèòÈáèÂêç | ÂÄº | ËØ¥Êòé |\n| :--- | :--- | :--- |\n| `u` | `‰Ω†ÁöÑ UUID` | **ÂøÖÈúÄ**„ÄÇ |\n| `p` | `proxyip` | **ÂèØÈÄâ**‰ΩÜÂº∫ÁÉàÊé®ËçêÔºåÂ°´ÂÜô‰∏Ä‰∏™Á®≥ÂÆöÁöÑÁî®Êù•ËÆøÈóÆ Cloudflare IP ÂèØ‰ª•Áî® ProxyIP.cmliussss.net CMÊèê‰æõÁöÑÂÖ¨ÁõäÈ°πÁõÆ Âú®Ê¨°ÊÑüË∞¢„ÄÇ |\n| `s` | `‰Ω†ÁöÑSOCKS5Âú∞ÂùÄ` | **ÂèØÈÄâ**„ÄÇÁî®‰∫éÂ∞ÜÊâÄÊúâÂá∫Á´ôÊµÅÈáèÈÄöËøá SOCKS5 ‰ª£ÁêÜËΩ¨ÂèëÔºåÊ†ºÂºè‰∏∫ `user:pass@host:port` Êàñ `host:port`„ÄÇ |\n| `d` | `‰Ω†ÁöÑËÆ¢ÈòÖÂú∞ÂùÄ` | **ÂèØÈÄâ**„ÄÇ‰∏çÂ°´Â∞±ÊòØ/‰Ω†ÁöÑuuid |\n| `yx` | `Ëá™ÂÆö‰πâ‰ºòÈÄâIP/ÂüüÂêç` | **ÂèØÈÄâ**„ÄÇËá™ÂÆö‰πâ‰ºòÈÄâIPÂíåÂüüÂêçÔºåÊîØÊåÅÁ´ØÂè£ÔºåÊ†ºÂºèÔºö`1.1.1.1:8080,2.2.2.2,example.com:8443`„ÄÇÂΩìËÆæÁΩÆÊ≠§ÂèòÈáèÊó∂ÔºåÂ∞ÜÂè™‰ΩøÁî®ÂéüÁîüÂú∞ÂùÄÂíåËá™ÂÆö‰πâ‰ºòÈÄâÔºå‰∏çÁîüÊàêÈªòËÆ§‰ºòÈÄâ„ÄÇ |\n| `qj` | `no` | **ÂèØÈÄâ**„ÄÇÈôçÁ∫ßÊéßÂà∂ÔºåËÆæÁΩÆ‰∏∫`no`Êó∂ÂêØÁî®ÈôçÁ∫ßÊ®°ÂºèÔºöCFÁõ¥ËøûÂ§±Ë¥•‚ÜíSOCKS5ËøûÊé•‚ÜífallbackÂú∞ÂùÄ„ÄÇ |\n| `dkby` | `yes` | **ÂèØÈÄâ**„ÄÇÁ´ØÂè£ÊéßÂà∂ÔºåËÆæÁΩÆ‰∏∫`yes`Êó∂Á¶ÅÁî®80Á´ØÂè£ËäÇÁÇπÔºåÂè™‰øùÁïô443Á´ØÂè£ÂíåÂÖ∂‰ªñÈùû80Á´ØÂè£ËäÇÁÇπ„ÄÇ |\n| `yxby` | `yes` | **ÂèØÈÄâ**„ÄÇ‰ºòÈÄâÊéßÂà∂ÔºåËÆæÁΩÆ‰∏∫`yes`Êó∂ÂÖ≥Èó≠ÊâÄÊúâ‰ºòÈÄâÂäüËÉΩÔºåÂè™‰ΩøÁî®ÂéüÁîüÂú∞ÂùÄÔºå‰∏çÁîüÊàê‰ºòÈÄâIPÂíåÂüüÂêçËäÇÁÇπ„ÄÇ |\n\n###  Êñ∞ÂäüËÉΩ\n\n#### Ëá™ÂÆö‰πâ‰ºòÈÄâÊîØÊåÅ\n- ÊîØÊåÅËá™ÂÆö‰πâ‰ºòÈÄâIPÂíåÂüüÂêçÔºåÂÆåÂÖ®Êõø‰ª£ÈªòËÆ§‰ºòÈÄâ\n- Ê†ºÂºèÔºö`yx=1.1.1.1:8080,2.2.2.2,example.com:8443`\n- ÊîØÊåÅIPv4/IPv6Âú∞ÂùÄÂíåÂüüÂêçÔºåÊîØÊåÅËá™ÂÆö‰πâÁ´ØÂè£\n- ËÆæÁΩÆÂêéÂè™ÁîüÊàêÂéüÁîüÂú∞ÂùÄ+Ëá™ÂÆö‰πâ‰ºòÈÄâÔºåË∑≥ËøáÈªòËÆ§‰ºòÈÄâ\n\n#### Êô∫ËÉΩÈôçÁ∫ß\n- ÂΩìCFÁõ¥ËøûÂ§±Ë¥•Êó∂Ëá™Âä®ÈôçÁ∫ß\n- ËÆæÁΩÆÔºö`qj=no` ÂêØÁî®ÈôçÁ∫ßÊ®°Âºè\n- ÈôçÁ∫ßÊµÅÁ®ãÔºöCFÁõ¥ËøûÂ§±Ë¥• ‚Üí SOCKS5ËøûÊé• ‚Üí fallbackÂú∞ÂùÄ\n- ÊèêÈ´òËøûÊé•ÊàêÂäüÁéáÔºåÂ¢ûÂº∫ÂÆπÈîôËÉΩÂäõ\n\n#### Á´ØÂè£ÊéßÂà∂\n- ÊîØÊåÅÁ¶ÅÁî®ÁâπÂÆöÁ´ØÂè£ËäÇÁÇπ\n- ËÆæÁΩÆÔºö`dkby=yes` Á¶ÅÁî®80Á´ØÂè£ËäÇÁÇπ\n- Âè™‰øùÁïô443Á´ØÂè£ÂíåÂÖ∂‰ªñÈùû80Á´ØÂè£ËäÇÁÇπ\n- ÈÄÇÁî®‰∫éÈúÄË¶ÅÈÅøÂÖç80Á´ØÂè£ÈôêÂà∂ÁöÑÂú∫ÊôØ\n\n#### ‰ºòÈÄâÊéßÂà∂\n- ÊîØÊåÅÂÆåÂÖ®ÂÖ≥Èó≠‰ºòÈÄâÂäüËÉΩ\n- ËÆæÁΩÆÔºö`yxby=yes` ÂÖ≥Èó≠ÊâÄÊúâ‰ºòÈÄâ\n- Âè™‰ΩøÁî®ÂéüÁîüÂú∞ÂùÄÔºå‰∏çÁîüÊàê‰ºòÈÄâIPÂíåÂüüÂêçËäÇÁÇπ\n- ÈÄÇÁî®‰∫éÈúÄË¶ÅÁÆÄÂåñËäÇÁÇπÂàóË°®ÁöÑÂú∫ÊôØ\n\n###  Ëá¥Ë∞¢\n\n  * Êú¨È°πÁõÆÂü∫‰∫é [zizifn/edgetunnel](https://github.com/zizifn/edgetunnel) ‰øÆÊîπÔºåÊÑüË∞¢Âéü‰ΩúËÄÖÁöÑË¥°ÁåÆ„ÄÇ\n\n## Star History\n\n[![Star History Chart](https://api.star-history.com/svg?repos=byJoey/cfnew&type=Timeline)](https://www.star-history.com/#byJoey/cfnew&Timeline&LogScale)\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-10-04T02:11:49.024627"
  },
  {
    "basic_info": {
      "name": "Nano-Bananary",
      "full_name": "ZHO-ZHO-ZHO/Nano-Bananary",
      "owner": "ZHO-ZHO-ZHO",
      "description": "È¶ôËïâË∂ÖÂ∏ÇÔΩúÂêÑÁßçÁé©Ê≥ï‰∏ÄÈîÆÁîüÊàêÔºåÊó†ÈúÄÊèêÁ§∫ËØçÔºåÊîØÊåÅÂ±ÄÈÉ®Ê∂ÇÈÄâ„ÄÅËøûÁª≠ÁºñËæë",
      "url": "https://github.com/ZHO-ZHO-ZHO/Nano-Bananary",
      "clone_url": "https://github.com/ZHO-ZHO-ZHO/Nano-Bananary.git",
      "ssh_url": "git@github.com:ZHO-ZHO-ZHO/Nano-Bananary.git",
      "homepage": null,
      "created_at": "2025-09-05T10:18:39Z",
      "updated_at": "2025-10-03T19:35:49Z",
      "pushed_at": "2025-09-16T15:14:05Z"
    },
    "stats": {
      "stars": 1138,
      "forks": 204,
      "watchers": 1138,
      "open_issues": 6,
      "size": 104
    },
    "tech_info": {
      "language": "TypeScript",
      "languages": {
        "TypeScript": 154642,
        "HTML": 2693
      },
      "license": null,
      "topics": []
    },
    "content": {
      "readme": "<div align=\"center\">\n<img width=\"2816\" height=\"1536\" alt=\"Group 333\" src=\"https://github.com/user-attachments/assets/0800046e-71aa-4dee-93a6-731b9f914a35\" />\n\n\n\n# Nano Bananary ÔΩú È¶ôËïâË∂ÖÂ∏Ç ÔΩú ZHO\n\n\n\n<img width=\"1251\" height=\"2051\" alt=\"Group 336\" src=\"https://github.com/user-attachments/assets/6b1fc8a2-e86b-403b-be21-97eb2ff65034\" />\n\n\n\n<img width=\"1556\" height=\"1086\" alt=\"screenshot-20250905-191422\" src=\"https://github.com/user-attachments/assets/23953d15-8ebb-4574-bbc9-74b44b79f341\" />\n\n\n</div>\n\n\n## üÜï Êñ∞Â¢û ËßÜÈ¢ëÁîüÊàê + ‰∏≠ÊñáÁïåÈù¢ + ÊµÖËâ≤‰∏ªÈ¢òÔºÅ\n\n\n### 1Ôºâ‰∏≠ÊñáÁïåÈù¢ + ÊµÖËâ≤‰∏ªÈ¢ò ‰∏ÄÈîÆÂàáÊç¢ÔºÅ\n\n<img width=\"1311\" height=\"738\" alt=\"screenshot-20250916-153520\" src=\"https://github.com/user-attachments/assets/476241e6-b7bf-4f66-81c5-fdaaa0695bcd\" />\n\n\n### 2Ôºâ‰∏äÁ∫øËßÜÈ¢ëÁîüÊàêÂäüËÉΩÔºÅÊâÄÊúâÁé©Ê≥ï‰∏ÄÈîÆËΩ¨Âåñ‰∏∫ËßÜÈ¢ëÔºåÁ∫µ‰∫´‰∏ùÊªëÔºÅ\n\n\nhttps://github.com/user-attachments/assets/4cce75d2-9783-41a4-94cc-2837c365d5a8\n\n\n\n\nhttps://github.com/user-attachments/assets/be93114f-3bc4-4ddf-947c-4694268444e4\n\n\n\n\n## 1ÔºâÊó†ÈúÄÊèêÁ§∫ËØçÔºå‰∏ùÊªëË°îÊé•\n\n\n\nhttps://github.com/user-attachments/assets/39976fe1-fafe-4ecf-94b2-5f6053f92c7f\n\n\n\n‚úÖÂêÑÁßçÁé©Ê≥ï‰∏ÄÈîÆÁîüÊàêÔºåÊó†ÈúÄÊèêÁ§∫ËØçÔºåÊîØÊåÅÂ±ÄÈÉ®Ê∂ÇÈÄâ„ÄÅËøûÁª≠ÁºñËæëÔºö\n\n1‚É£ÈÄâÊã©Áî®Ê≥ï\n\n2‚É£‰∏ä‰º†ÂõæÁâá\n\n3‚É£ÁÇπÂáªÁîüÊàê/Â±ÄÈÉ®Ê∂ÇÈÄâ\n\n4‚É£Áõ¥Êé•ÂèëÈÄÅÂà∞Êñ∞Áé©Ê≥ï‰∏≠ÁªßÁª≠\n\n\n## 2ÔºâÊ†∏ÂøÉÂäüËÉΩÔºöÊØèÊ¨°ËæìÂá∫ÈÉΩËÉΩÁõ¥Êé•‰Ωú‰∏∫ËæìÂÖ•ËøõË°å‰∏ã‰∏ÄÊ¨°ÁºñËæë/ÁîüÊàê\n\n\n\nhttps://github.com/user-attachments/assets/ca1cc851-ccca-44c6-b3f4-138b0650c0f9\n\n\n\n<img width=\"1664\" height=\"1248\" alt=\"comparison-image-1757069194058\" src=\"https://github.com/user-attachments/assets/6260a835-8404-4772-a152-303d10ab9551\" />\n\n\n<img width=\"1392\" height=\"1280\" alt=\"screenshot-20250905-220201\" src=\"https://github.com/user-attachments/assets/5cb573dc-78dd-4c5c-8194-9172af94d65d\" />\n\n\n\n<img width=\"2048\" height=\"2507\" alt=\"Group 334\" src=\"https://github.com/user-attachments/assets/a4dd528c-e0a6-4a52-844c-ff0c28d0c99c\" />\n\n\n\n## Online\n\n\nUse in AI Studio: https://ai.studio/apps/drive/1JknFrFFdiOm7FIA8MLOJa_vtJN2g24c1\n\n\n## Run Locally\n\n**Prerequisites:**  Node.js\n\n\n1. Install dependencies:\n   `npm install`\n2. Set the `GEMINI_API_KEY` in [.env.local](.env.local) to your Gemini API key\n3. Run the app:\n   `npm run dev`\n\n\n\n\n## Êõ¥Êñ∞Êó•Âøó\n\n- 20250916\n\n  Êñ∞Â¢û ËßÜÈ¢ëÁîüÊàê + ‰∏≠ÊñáÁïåÈù¢ + ÊµÖËâ≤‰∏ªÈ¢ò\n\n\n- 20250906\n\n  ÂäüËÉΩÊõ¥Êñ∞ÔºöÂ¢ûÂä†ÂéÜÂè≤ËÆ∞ÂΩïÂäüËÉΩÔºåÊñπ‰æøÁõ¥Êé•‰ΩøÁî®Â∑≤ÁîüÊàêÁöÑÂõæÂÉè‰Ωú‰∏∫ËæìÂÖ•\n\n  Áé©Ê≥ïÊõ¥Êñ∞ÔºöÂ∑≤ÁªèÊää ÊàëÁöÑ[üçåÊèêÁ§∫ËØçÂ∫ì](https://github.com/ZHO-ZHO-ZHO/ZHO-nano-banana-Creation)ÁöÑ‰∏ªË¶ÅÁé©Ê≥ïÊõ¥Êñ∞‰∏äÂéª‰∫Ü\n\n\n- 20250905\n \n  Êõ¥Êñ∞ÈÖçËâ≤Ôºöblack & orange\n\n  Êñ∞Â¢ûÂäüËÉΩÔºö\n  \n  1‚É£ ÊîæÂ§ßÈ¢ÑËßà\n  \n  2‚É£Ê®™ÁΩÆÂØπÊØî + ÂØπÊØîÂõæÁõ¥Êé•‰∏ãËΩΩÔºàÊñπ‰æøÁõ¥Êé•ÂèëÁ§æ‰∫§Â™í‰ΩìÔºâ\n  \n  3‚É£ÊªëÂùóÂØπÊØîÔºöÊñπ‰æøÂéüÂõæÂØπÁÖß\n  \n  4‚É£Ëá™ÂÆö‰πâÊèêÁ§∫ËØçÊ®°ÂùóÔºöÊñπ‰æø‰∏™ÊÄßÂåñÁîüÊàê\n\n\n- 20250095\n  \n  ÂàõÂª∫È°πÁõÆ\n  \n\n## Stars \n\n[![Star History Chart](https://api.star-history.com/svg?repos=ZHO-ZHO-ZHO/Nano-Bananary&type=Date)](https://star-history.com/#ZHO-ZHO-ZHO/Nano-Bananary&Date)\n\n\n## ÂÖ≥‰∫éÊàë | About me\n\nüì¨ **ËÅîÁ≥ªÊàë**Ôºö\n- ÈÇÆÁÆ±Ôºözhozho3965@gmail.com\n  \n\nüîó **Á§æ‰∫§Â™í‰Ωì**Ôºö\n- ‰∏™‰∫∫È°µÔºö[-Zho-](https://jike.city/zho)\n- BilibiliÔºö[ÊàëÁöÑBÁ´ô‰∏ªÈ°µ](https://space.bilibili.com/484366804)\n- XÔºàTwitterÔºâÔºö[ÊàëÁöÑTwitter](https://twitter.com/ZHO_ZHO_ZHO)\n- Â∞èÁ∫¢‰π¶Ôºö[ÊàëÁöÑÂ∞èÁ∫¢‰π¶‰∏ªÈ°µ](https://www.xiaohongshu.com/user/profile/63f11530000000001001e0c8?xhsshare=CopyLink&appuid=63f11530000000001001e0c8&apptime=1690528872)\n\nüí° **ÊîØÊåÅÊàë**Ôºö\n- BÁ´ôÔºö[BÁ´ôÂÖÖÁîµ](https://space.bilibili.com/484366804)\n- Áà±ÂèëÁîµÔºö[‰∏∫ÊàëÂÖÖÁîµ](https://afdian.com/a/ZHOZHO)\n\n\n## Credits\n\n[Gemini 2.5 Flash Image](https://gemini.google.com/app)\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-10-04T02:11:50.138337"
  },
  {
    "basic_info": {
      "name": "PromptEnhancer",
      "full_name": "Hunyuan-PromptEnhancer/PromptEnhancer",
      "owner": "Hunyuan-PromptEnhancer",
      "description": "PromptEnhancer is a prompt-rewriting tool, refining prompts into clearer, structured versions for better image generation.",
      "url": "https://github.com/Hunyuan-PromptEnhancer/PromptEnhancer",
      "clone_url": "https://github.com/Hunyuan-PromptEnhancer/PromptEnhancer.git",
      "ssh_url": "git@github.com:Hunyuan-PromptEnhancer/PromptEnhancer.git",
      "homepage": "https://hunyuan-promptenhancer.github.io/",
      "created_at": "2025-09-09T08:56:47Z",
      "updated_at": "2025-10-04T01:35:11Z",
      "pushed_at": "2025-10-03T04:01:55Z"
    },
    "stats": {
      "stars": 1088,
      "forks": 94,
      "watchers": 1088,
      "open_issues": 5,
      "size": 11909
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 34562,
        "Shell": 379
      },
      "license": "Other",
      "topics": [
        "hunyuan",
        "hunyuan-image",
        "image-editing",
        "image-to-image",
        "prompt",
        "prompt-engineering",
        "prompt-enhancer",
        "text-to-image",
        "vlm"
      ]
    },
    "content": {
      "readme": "<div align=\"center\">\n\n# PromptEnhancer: A Simple Approach to Enhance Text-to-Image Models via Chain-of-Thought Prompt Rewriting\n\n[**Linqing Wang**](https://scholar.google.com/citations?hl=en&view_op=list_works&gmla=AH8HC4z9rmDHYjp5o28xKk8U4ddD_n7BuMnk8UZFP-jygFBtHUSz6pf-5FP32B_yKMpRU9VpDY3iT8eM0zORHA&user=Hy12lcEAAAAJ) ¬∑ \n[**Ximing Xing**](https://ximinng.github.io/) ¬∑ \n[**Yiji Cheng**](https://scholar.google.com/citations?user=Plo8ZSYAAAAJ&hl=en) ¬∑ \nZhiyuan Zhao ¬∑ \nDonghao Li ¬∑\nTiankai Hang ¬∑\nZhenxi Li ¬∑\n[**Jiale Tao**](https://scholar.google.com/citations?user=WF5DPWkAAAAJ&hl=en) ¬∑ \n[**QiXun Wang**](https://github.com/wangqixun) ¬∑ \n[**Ruihuang Li**](https://scholar.google.com/citations?user=8CfyOtQAAAAJ&hl=en) ¬∑ \nComi Chen ¬∑\nXin Li ¬∑ \n[**Mingrui Wu**](https://scholar.google.com/citations?user=sbCKwnYAAAAJ&hl=en) ¬∑ \nXinchi Deng ¬∑ \nShuyang Gu ¬∑ \n[**Chunyu Wang**](https://scholar.google.com/citations?user=VXQV5xwAAAAJ&hl=en)<sup>‚Ä†</sup> ¬∑ \n[**Qinglin Lu**](https://luqinglin.weebly.com/)<sup>*</sup>\n\nTencent Hunyuan\n\n<sup>‚Ä†</sup>Project Lead ¬∑ <sup>*</sup>Corresponding Author\n\n</div>\n\n<p align=\"center\">\n  <a href=\"https://www.arxiv.org/abs/2509.04545\"><img src=\"https://img.shields.io/badge/Paper-arXiv:2509.04545-red?logo=arxiv\" alt=\"arXiv\"></a>\n  <a href=\"https://zhuanlan.zhihu.com/p/1949013083109459515\"><img src=\"https://img.shields.io/badge/Áü•‰πé-ÊäÄÊúØËß£ËØª-0084ff?logo=zhihu\" alt=\"Zhihu\"></a>\n  <a href=\"https://huggingface.co/tencent/HunyuanImage-2.1/tree/main/reprompt\"><img src=\"https://img.shields.io/badge/Model-PromptEnhancer_7B-blue?logo=huggingface\" alt=\"HuggingFace Model\"></a>\n  <a href=\"https://huggingface.co/PromptEnhancer/PromptEnhancer-Img2img-Edit\"><img src=\"https://img.shields.io/badge/Model-PromptEnhancer_Img2Img_Edit-blue?logo=huggingface\" alt=\"HuggingFace Model\"></a>\n  <!-- <a href=\"https://huggingface.co/PromptEnhancer/PromptEnhancer-32B\"><img src=\"https://img.shields.io/badge/Model-PromptEnhancer_32B-blue?logo=huggingface\" alt=\"HuggingFace Model\"></a> -->\n  <a href=\"https://huggingface.co/datasets/PromptEnhancer/T2I-Keypoints-Eval\"><img src=\"https://img.shields.io/badge/Benchmark-T2I_Keypoints_Eval-blue?logo=huggingface\" alt=\"T2I-Keypoints-Eval Dataset\"></a>\n  <a href=\"https://hunyuan-promptenhancer.github.io/\"><img src=\"https://img.shields.io/badge/Homepage-PromptEnhancer-1abc9c?logo=homeassistant&logoColor=white\" alt=\"Homepage\"></a>\n  <a href=\"https://github.com/Tencent-Hunyuan/HunyuanImage-2.1\"><img src=\"https://img.shields.io/badge/Code-HunyuanImage2.1-2ecc71?logo=github\" alt=\"HunyuanImage2.1 Code\"></a>\n  <a href=\"https://huggingface.co/tencent/HunyuanImage-2.1\"><img src=\"https://img.shields.io/badge/Model-HunyuanImage2.1-3498db?logo=huggingface\" alt=\"HunyuanImage2.1 Model\"></a>\n  <a href=https://x.com/TencentHunyuan target=\"_blank\"><img src=https://img.shields.io/badge/Hunyuan-black.svg?logo=x height=22px></a>\n</p>\n\n---\n\n<p align=\"center\">\n  <img src=\"assets/teaser-1.png\" alt=\"PromptEnhancer Teaser\"/>\n</p>\n\n## Overview\n\nHunyuan-PromptEnhancer is a prompt rewriting utility that **supports both Text-to-Image generation and Image-to-Image editing**. It restructures input prompts while preserving original intent, producing clearer, structured prompts for downstream image generation tasks.\n\n**Key Features:**\n- **Dual-mode support**: Text-to-Image prompt enhancement and Image-to-Image editing instruction refinement with visual context\n- **Intent preservation**: Maintains all key elements (subject, action, style, layout, attributes, etc.) across rewriting\n- **Robust parsing**: Multi-level fallback mechanism ensures reliable output\n- **Flexible deployment**: Supports full-precision (7B/32B), quantized (GGUF), and vision-language models\n\n## üî•üî•üî•Updates\n\n- [2025-09-30] ‚ú® Release [PromptEnhancer-Img2Img Editing model](https://huggingface.co/PromptEnhancer/PromptEnhancer-Img2img-Edit).\n- [2025-09-22] üöÄ Thanks @mradermacher for adding **GGUF model support** for efficient inference with quantized models!\n- [2025-09-18] ‚ú® Try the [PromptEnhancer-32B](https://huggingface.co/PromptEnhancer/PromptEnhancer-32B) for higher-quality prompt enhancement!\n- [2025-09-16] ‚ú® Release [T2I-Keypoints-Eval dataset](https://huggingface.co/datasets/PromptEnhancer/T2I-Keypoints-Eval).\n- [2025-09-07] ‚ú® Release [PromptEnhancer-7B model](https://huggingface.co/tencent/HunyuanImage-2.1/tree/main/reprompt).\n- [2025-09-07] ‚ú® Release [technical report](https://arxiv.org/abs/2509.04545).\n\n## Installation\n\n### Option 1: Standard Installation (Recommended)\n```bash\npip install -r requirements.txt\n```\n\n### Option 2: GGUF Installation (For quantized models with CUDA support)\n```bash\nchmod +x script/install_gguf.sh && ./script/install_gguf.sh\n```\n\n> **üí° Tip**: Choose GGUF installation if you want faster inference with lower memory usage, especially for the 32B model.\n\n## Model Download\n\n### üéØ Quick Start\n\nFor most users, we recommend starting with the **PromptEnhancer-7B** model:\n\n```bash\n# Download PromptEnhancer-7B (13GB) - Best balance of q",
      "default_branch": "main"
    },
    "fetched_at": "2025-10-04T02:11:51.273304"
  }
]