[
  {
    "basic_info": {
      "name": "nanochat",
      "full_name": "karpathy/nanochat",
      "owner": "karpathy",
      "description": "The best ChatGPT that $100 can buy.",
      "url": "https://github.com/karpathy/nanochat",
      "clone_url": "https://github.com/karpathy/nanochat.git",
      "ssh_url": "git@github.com:karpathy/nanochat.git",
      "homepage": "",
      "created_at": "2025-10-13T13:46:35Z",
      "updated_at": "2025-10-26T02:26:34Z",
      "pushed_at": "2025-10-25T19:39:18Z"
    },
    "stats": {
      "stars": 32505,
      "forks": 3551,
      "watchers": 32505,
      "open_issues": 95,
      "size": 192
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 322549,
        "HTML": 20192,
        "Rust": 16627,
        "Shell": 14077
      },
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "# nanochat\n\n![nanochat logo](dev/nanochat.png)\n\n> The best ChatGPT that $100 can buy.\n\nThis repo is a full-stack implementation of an LLM like ChatGPT in a single, clean, minimal, hackable, dependency-lite codebase. nanochat is designed to run on a single 8XH100 node via scripts like [speedrun.sh](speedrun.sh), that run the entire pipeline start to end. This includes tokenization, pretraining, finetuning, evaluation, inference, and web serving over a simple UI so that you can talk to your own LLM just like ChatGPT. nanochat will become the capstone project of the course LLM101n being developed by Eureka Labs.\n\n## Talk to it\n\nTo get a sense of the endpoint of this repo, you can currently find [nanochat d32](https://github.com/karpathy/nanochat/discussions/8) hosted on [nanochat.karpathy.ai](https://nanochat.karpathy.ai/). \"d32\" means that this model has 32 layers in the Transformer neural network. This model has 1.9 billion parameters, it was trained on 38 billion tokens by simply running the single script [run1000.sh](run1000.sh), and the total cost of training was ~$800 (about 33 hours training time on 8XH100 GPU node). While today this is enough to outperform GPT-2 of 2019, it falls dramatically short of moden Large Language Models like GPT-5. When talking to these micro models, you'll see that they make a lot of mistakes, they are a little bit naive and silly and they hallucinate a ton, a bit like children. It's kind of amusing. But what makes nanochat unique is that it is fully yours - fully configurable, tweakable, hackable, and trained by you from start to end. To train and talk to your own, we turn to...\n\n## Quick start\n\nThe fastest way to feel the magic is to run the speedrun script [speedrun.sh](speedrun.sh), which trains and inferences the $100 tier of nanochat. On an 8XH100 node at $24/hr, this gives a total run time of about 4 hours. Boot up a new 8XH100 GPU box from your favorite provider (e.g. I use and like [Lambda](https://lambda.ai/service/gpu-cloud)), and kick off the training script:\n\n```bash\nbash speedrun.sh\n```\n\nAlternatively, since the script runs for 4 hours, I like to launch it like this inside a new screen session `speedrun` (and also log output to `speedrun.log`):\n\n```bash\nscreen -L -Logfile speedrun.log -S speedrun bash speedrun.sh\n```\n\nSee the [screen cheatsheet](https://gist.github.com/jctosta/af918e1618682638aa82) if you are less familiar. You can watch it go inside the screen session, or detach with `Ctrl-a d` and `tail speedrun.log` to view progress. Now wait 4 hours. Once it's done, you can talk to your LLM via the ChatGPT-like web UI. Make sure again that your local uv virtual environment is active (run `source .venv/bin/activate`), and serve it:\n\n```bash\npython -m scripts.chat_web\n```\n\nAnd then visit the URL shown. Make sure to access it correctly, e.g. on Lambda use the public IP of the node you're on, followed by the port, so for example [http://209.20.xxx.xxx:8000/](http://209.20.xxx.xxx:8000/), etc. Then talk to your LLM as you'd normally talk to ChatGPT! Get it to write stories or poems. Ask it to tell you who you are to see a hallucination. Ask it why the sky is blue. Or why it's green. The speedrun is a 4e19 FLOPs capability model so it's a bit like talking to a kindergartener :).\n\n---\n\n<img width=\"2672\" height=\"1520\" alt=\"image\" src=\"https://github.com/user-attachments/assets/ed39ddf8-2370-437a-bedc-0f39781e76b5\" />\n\n---\n\nYou can also `cat report.md` file which appeared in the project directory and contains the \"report card\" of the run, i.e. a bunch of evaluations and metrics. At the very end, you'll see a summary table, for example:\n\n---\n\n- Characters: 333,989\n- Lines: 8,304\n- Files: 44\n- Tokens (approx): 83,497\n- Dependencies (uv.lock lines): 2,004\n\n| Metric          | BASE     | MID      | SFT      | RL       |\n|-----------------|----------|----------|----------|----------|\n| CORE            | 0.2219   | -        | -        | -        |\n| ARC-Challenge   | -        | 0.2875   | 0.2807   | -        |\n| ARC-Easy        | -        | 0.3561   | 0.3876   | -        |\n| GSM8K           | -        | 0.0250   | 0.0455   | 0.0758   |\n| HumanEval       | -        | 0.0671   | 0.0854   | -        |\n| MMLU            | -        | 0.3111   | 0.3151   | -        |\n| ChatCORE        | -        | 0.0730   | 0.0884   | -        |\n\nTotal wall clock time: 3h51m\n\n---\n\n(Your table might be missing the RL number by default). For a lot more information around the speedrun script and what to look for and expect, please refer to the walkthrough that I posted in Discussions of the repo: [\"Introducing nanochat: The best ChatGPT that $100 can buy\"](https://github.com/karpathy/nanochat/discussions/1).\n\n## Bigger models\n\nUnsurprisingly, $100 is not enough to train a highly performant ChatGPT clone. In fact, LLMs are famous for their multi-million dollar capex. For our purposes, I think there are two more scales of interest. First is the ~$300 tier d26 model (i.e. depth=26) that trains in ~12",
      "default_branch": "master"
    },
    "fetched_at": "2025-10-26T02:28:01.267590"
  },
  {
    "basic_info": {
      "name": "DeepSeek-OCR",
      "full_name": "deepseek-ai/DeepSeek-OCR",
      "owner": "deepseek-ai",
      "description": "Contexts Optical Compression",
      "url": "https://github.com/deepseek-ai/DeepSeek-OCR",
      "clone_url": "https://github.com/deepseek-ai/DeepSeek-OCR.git",
      "ssh_url": "git@github.com:deepseek-ai/DeepSeek-OCR.git",
      "homepage": null,
      "created_at": "2025-10-17T06:14:27Z",
      "updated_at": "2025-10-26T02:22:12Z",
      "pushed_at": "2025-10-25T02:43:18Z"
    },
    "stats": {
      "stars": 17116,
      "forks": 1071,
      "watchers": 17116,
      "open_issues": 131,
      "size": 7948
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 113538
      },
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "<!-- markdownlint-disable first-line-h1 -->\n<!-- markdownlint-disable html -->\n<!-- markdownlint-disable no-duplicate-header -->\n\n\n<div align=\"center\">\n  <img src=\"assets/logo.svg\" width=\"60%\" alt=\"DeepSeek AI\" />\n</div>\n\n\n<hr>\n<div align=\"center\">\n  <a href=\"https://www.deepseek.com/\" target=\"_blank\">\n    <img alt=\"Homepage\" src=\"assets/badge.svg\" />\n  </a>\n  <a href=\"https://huggingface.co/deepseek-ai/DeepSeek-OCR\" target=\"_blank\">\n    <img alt=\"Hugging Face\" src=\"https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-DeepSeek%20AI-ffc107?color=ffc107&logoColor=white\" />\n  </a>\n\n</div>\n\n<div align=\"center\">\n\n  <a href=\"https://discord.gg/Tc7c45Zzu5\" target=\"_blank\">\n    <img alt=\"Discord\" src=\"https://img.shields.io/badge/Discord-DeepSeek%20AI-7289da?logo=discord&logoColor=white&color=7289da\" />\n  </a>\n  <a href=\"https://twitter.com/deepseek_ai\" target=\"_blank\">\n    <img alt=\"Twitter Follow\" src=\"https://img.shields.io/badge/Twitter-deepseek_ai-white?logo=x&logoColor=white\" />\n  </a>\n\n</div>\n\n\n\n<p align=\"center\">\n  <a href=\"https://huggingface.co/deepseek-ai/DeepSeek-OCR\"><b>üì• Model Download</b></a> |\n  <a href=\"https://github.com/deepseek-ai/DeepSeek-OCR/blob/main/DeepSeek_OCR_paper.pdf\"><b>üìÑ Paper Link</b></a> |\n  <a href=\"https://arxiv.org/abs/2510.18234\"><b>üìÑ Arxiv Paper Link</b></a> |\n</p>\n\n<h2>\n<p align=\"center\">\n  <a href=\"\">DeepSeek-OCR: Contexts Optical Compression</a>\n</p>\n</h2>\n\n<p align=\"center\">\n<img src=\"assets/fig1.png\" style=\"width: 1000px\" align=center>\n</p>\n<p align=\"center\">\n<a href=\"\">Explore the boundaries of visual-text compression.</a>       \n</p>\n\n## Release\n- [2025/10/23]üöÄüöÄüöÄ DeepSeek-OCR is now officially supported in upstream [vLLM](https://docs.vllm.ai/projects/recipes/en/latest/DeepSeek/DeepSeek-OCR.html#installing-vllm). Thanks to the [vLLM](https://github.com/vllm-project/vllm) team for their help.\n- [2025/10/20]üöÄüöÄüöÄ We release DeepSeek-OCR, a model to investigate the role of vision encoders from an LLM-centric viewpoint.\n\n## Contents\n- [Install](#install)\n- [vLLM Inference](#vllm-inference)\n- [Transformers Inference](#transformers-inference)\n  \n\n\n\n\n## Install\n>Our environment is cuda11.8+torch2.6.0.\n1. Clone this repository and navigate to the DeepSeek-OCR folder\n```bash\ngit clone https://github.com/deepseek-ai/DeepSeek-OCR.git\n```\n2. Conda\n```Shell\nconda create -n deepseek-ocr python=3.12.9 -y\nconda activate deepseek-ocr\n```\n3. Packages\n\n- download the vllm-0.8.5 [whl](https://github.com/vllm-project/vllm/releases/tag/v0.8.5) \n```Shell\npip install torch==2.6.0 torchvision==0.21.0 torchaudio==2.6.0 --index-url https://download.pytorch.org/whl/cu118\npip install vllm-0.8.5+cu118-cp38-abi3-manylinux1_x86_64.whl\npip install -r requirements.txt\npip install flash-attn==2.7.3 --no-build-isolation\n```\n**Note:** if you want vLLM and transformers codes to run in the same environment, you don't need to worry about this installation error like: vllm 0.8.5+cu118 requires transformers>=4.51.1\n\n## vLLM-Inference\n- VLLM:\n>**Note:** change the INPUT_PATH/OUTPUT_PATH and other settings in the DeepSeek-OCR-master/DeepSeek-OCR-vllm/config.py\n```Shell\ncd DeepSeek-OCR-master/DeepSeek-OCR-vllm\n```\n1. image: streaming output\n```Shell\npython run_dpsk_ocr_image.py\n```\n2. pdf: concurrency ~2500tokens/s(an A100-40G)\n```Shell\npython run_dpsk_ocr_pdf.py\n```\n3. batch eval for benchmarks\n```Shell\npython run_dpsk_ocr_eval_batch.py\n```\n\n**[2025/10/23] The version of upstream [vLLM](https://docs.vllm.ai/projects/recipes/en/latest/DeepSeek/DeepSeek-OCR.html#installing-vllm):**\n\n```shell\nuv venv\nsource .venv/bin/activate\n# Until v0.11.1 release, you need to install vLLM from nightly build\nuv pip install -U vllm --pre --extra-index-url https://wheels.vllm.ai/nightly\n```\n\n```python\nfrom vllm import LLM, SamplingParams\nfrom vllm.model_executor.models.deepseek_ocr import NGramPerReqLogitsProcessor\nfrom PIL import Image\n\n# Create model instance\nllm = LLM(\n    model=\"deepseek-ai/DeepSeek-OCR\",\n    enable_prefix_caching=False,\n    mm_processor_cache_gb=0,\n    logits_processors=[NGramPerReqLogitsProcessor]\n)\n\n# Prepare batched input with your image file\nimage_1 = Image.open(\"path/to/your/image_1.png\").convert(\"RGB\")\nimage_2 = Image.open(\"path/to/your/image_2.png\").convert(\"RGB\")\nprompt = \"<image>\\nFree OCR.\"\n\nmodel_input = [\n    {\n        \"prompt\": prompt,\n        \"multi_modal_data\": {\"image\": image_1}\n    },\n    {\n        \"prompt\": prompt,\n        \"multi_modal_data\": {\"image\": image_2}\n    }\n]\n\nsampling_param = SamplingParams(\n            temperature=0.0,\n            max_tokens=8192,\n            # ngram logit processor args\n            extra_args=dict(\n                ngram_size=30,\n                window_size=90,\n                whitelist_token_ids={128821, 128822},  # whitelist: <td>, </td>\n            ),\n            skip_special_tokens=False,\n        )\n# Generate output\nmodel_outputs = llm.generate(model_input, sampling_param)\n\n# Print output\nfor output in model_outputs:\n    print(output.outputs[0].text)\n```\n## ",
      "default_branch": "main"
    },
    "fetched_at": "2025-10-26T02:28:02.410412"
  },
  {
    "basic_info": {
      "name": "TinyRecursiveModels",
      "full_name": "SamsungSAILMontreal/TinyRecursiveModels",
      "owner": "SamsungSAILMontreal",
      "description": null,
      "url": "https://github.com/SamsungSAILMontreal/TinyRecursiveModels",
      "clone_url": "https://github.com/SamsungSAILMontreal/TinyRecursiveModels.git",
      "ssh_url": "git@github.com:SamsungSAILMontreal/TinyRecursiveModels.git",
      "homepage": null,
      "created_at": "2025-10-07T13:24:28Z",
      "updated_at": "2025-10-25T22:57:50Z",
      "pushed_at": "2025-10-08T19:46:47Z"
    },
    "stats": {
      "stars": 5165,
      "forks": 691,
      "watchers": 5165,
      "open_issues": 24,
      "size": 1266
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 147529
      },
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "# Less is More: Recursive Reasoning with Tiny Networks\n\nThis is the codebase for the paper: \"Less is More: Recursive Reasoning with Tiny Networks\". TRM is a recursive reasoning approach that achieves amazing scores of 45% on ARC-AGI-1 and 8% on ARC-AGI-2 using a tiny 7M parameters neural network.\n\n[Paper](https://arxiv.org/abs/2510.04871)\n\n### Motivation\n\nTiny Recursion Model (TRM) is a recursive reasoning model that achieves amazing scores of 45% on ARC-AGI-1 and 8% on ARC-AGI-2 with a tiny 7M parameters neural network. The idea that one must rely on massive foundational models trained for millions of dollars by some big corporation in order to achieve success on hard tasks is a trap. Currently, there is too much focus on exploiting LLMs rather than devising and expanding new lines of direction. With recursive reasoning, it turns out that ‚Äúless is more‚Äù: you don‚Äôt always need to crank up model size in order for a model to reason and solve hard problems. A tiny model pretrained from scratch, recursing on itself and updating its answers over time, can achieve a lot without breaking the bank.\n\nThis work came to be after I learned about the recent innovative Hierarchical Reasoning Model (HRM). I was amazed that an approach using small models could do so well on hard tasks like the ARC-AGI competition (reaching 40% accuracy when normally only Large Language Models could compete). But I kept thinking that it is too complicated, relying too much on biological arguments about the human brain, and that this recursive reasoning process could be greatly simplified and improved. Tiny Recursion Model (TRM) simplifies recursive reasoning to its core essence, which ultimately has nothing to do with the human brain, does not require any mathematical (fixed-point) theorem, nor any hierarchy.\n\n### How TRM works\n\n<p align=\"center\">\n  <img src=\"https://AlexiaJM.github.io/assets/images/TRM_fig.png\" alt=\"TRM\"  style=\"width: 30%;\">\n</p>\n\nTiny Recursion Model (TRM) recursively improves its predicted answer y with a tiny network. It starts with the embedded input question x and initial embedded answer y and latent z. For up to K improvements steps, it tries to improve its answer y. It does so by i) recursively updating n times its latent z given the question x, current answer y, and current latent z (recursive reasoning), and then ii) updating its answer y given the current answer y and current latent z. This recursive process allows the model to progressively improve its answer (potentially addressing any errors from its previous answer) in an extremely parameter-efficient manner while minimizing overfitting.\n\n### Requirements\n\n- Python 3.10 (or similar)\n- Cuda 12.6.0 (or similar)\n\n```bash\npip install --upgrade pip wheel setuptools\npip install --pre --upgrade torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu126 # install torch based on your cuda version\npip install -r requirements.txt # install requirements\npip install --no-cache-dir --no-build-isolation adam-atan2 \nwandb login YOUR-LOGIN # login if you want the logger to sync results to your Weights & Biases (https://wandb.ai/)\n```\n\n### Dataset Preparation\n\n```bash\n# ARC-AGI-1\npython -m dataset.build_arc_dataset \\\n  --input-file-prefix kaggle/combined/arc-agi \\\n  --output-dir data/arc1concept-aug-1000 \\\n  --subsets training evaluation concept \\\n  --test-set-name evaluation\n\n# ARC-AGI-2\npython -m dataset.build_arc_dataset \\\n  --input-file-prefix kaggle/combined/arc-agi \\\n  --output-dir data/arc2concept-aug-1000 \\\n  --subsets training2 evaluation2 concept \\\n  --test-set-name evaluation2\n\n## Note: You cannot train on both ARC-AGI-1 and ARC-AGI-2 and evaluate them both because ARC-AGI-2 training data contains some ARC-AGI-1 eval data\n\n# Sudoku-Extreme\npython dataset/build_sudoku_dataset.py --output-dir data/sudoku-extreme-1k-aug-1000  --subsample-size 1000 --num-aug 1000  # 1000 examples, 1000 augments\n\n# Maze-Hard\npython dataset/build_maze_dataset.py # 1000 examples, 8 augments\n```\n\n## Experiments\n\n### ARC-AGI-1 (assuming 4 H-100 GPUs):\n\n```bash\nrun_name=\"pretrain_att_arc1concept_4\"\ntorchrun --nproc-per-node 4 --rdzv_backend=c10d --rdzv_endpoint=localhost:0 --nnodes=1 pretrain.py \\\narch=trm \\\ndata_paths=\"[data/arc1concept-aug-1000]\" \\\narch.L_layers=2 \\\narch.H_cycles=3 arch.L_cycles=4 \\\n+run_name=${run_name} ema=True\n\n```\n\n*Runtime:* ~3 days\n\n### ARC-AGI-2 (assuming 4 H-100 GPUs):\n\n```bash\nrun_name=\"pretrain_att_arc2concept_4\"\ntorchrun --nproc-per-node 4 --rdzv_backend=c10d --rdzv_endpoint=localhost:0 --nnodes=1 pretrain.py \\\narch=trm \\\ndata_paths=\"[data/arc2concept-aug-1000]\" \\\narch.L_layers=2 \\\narch.H_cycles=3 arch.L_cycles=4 \\\n+run_name=${run_name} ema=True\n\n```\n\n*Runtime:* ~3 days\n\n### Sudoku-Extreme (assuming 1 L40S GPU):\n\n```bash\nrun_name=\"pretrain_mlp_t_sudoku\"\npython pretrain.py \\\narch=trm \\\ndata_paths=\"[data/sudoku-extreme-1k-aug-1000]\" \\\nevaluators=\"[]\" \\\nepochs=50000 eval_interval=5000 \\\nlr=1e-4 puzzle_emb_lr=1e-4 weight_decay=1.0 puzzle",
      "default_branch": "main"
    },
    "fetched_at": "2025-10-26T02:28:03.550408"
  },
  {
    "basic_info": {
      "name": "neutts-air",
      "full_name": "neuphonic/neutts-air",
      "owner": "neuphonic",
      "description": "On-device TTS model by Neuphonic",
      "url": "https://github.com/neuphonic/neutts-air",
      "clone_url": "https://github.com/neuphonic/neutts-air.git",
      "ssh_url": "git@github.com:neuphonic/neutts-air.git",
      "homepage": null,
      "created_at": "2025-10-02T12:48:55Z",
      "updated_at": "2025-10-26T02:05:01Z",
      "pushed_at": "2025-10-17T10:20:23Z"
    },
    "stats": {
      "stars": 3662,
      "forks": 351,
      "watchers": 3662,
      "open_issues": 32,
      "size": 1906
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 14928
      },
      "license": "Apache License 2.0",
      "topics": []
    },
    "content": {
      "readme": "# NeuTTS Air ‚òÅÔ∏è\n\nHuggingFace ü§ó: [Model](https://huggingface.co/neuphonic/neutts-air), [Q8 GGUF](https://huggingface.co/neuphonic/neutts-air-q8-gguf), [Q4 GGUF](https://huggingface.co/neuphonic/neutts-air-q4-gguf) [Spaces](https://huggingface.co/spaces/neuphonic/neutts-air)\n\n[Demo Video](https://github.com/user-attachments/assets/020547bc-9e3e-440f-b016-ae61ca645184)\n\n_Created by [Neuphonic](http://neuphonic.com/) - building faster, smaller, on-device voice AI_\n\nState-of-the-art Voice AI has been locked behind web APIs for too long. NeuTTS Air is the world‚Äôs first super-realistic, on-device, TTS speech language model with instant voice cloning. Built off a 0.5B LLM backbone, NeuTTS Air brings natural-sounding speech, real-time performance, built-in security and speaker cloning to your local device - unlocking a new category of embedded voice agents, assistants, toys, and compliance-safe apps.\n\n## Key Features\n\n- üó£Best-in-class realism for its size - produces natural, ultra-realistic voices that sound human\n- üì±Optimised for on-device deployment - provided in GGML format, ready to run on phones, laptops, or even Raspberry Pis\n- üë´Instant voice cloning - create your own speaker with as little as 3 seconds of audio\n- üöÑSimple LM + codec architecture built off a 0.5B backbone - the sweet spot between speed, size, and quality for real-world applications\n\n> [!CAUTION]\n> Websites like neutts.com are popping up and they're not affliated with Neuphonic, our github or this repo.\n>\n> We are on neuphonic.com only. Please be careful out there! üôè\n\n## Model Details\n\nNeuTTS Air is built off Qwen 0.5B - a lightweight yet capable language model optimised for text understanding and generation - as well as a powerful combination of technologies designed for efficiency and quality:\n\n- **Supported Languages**: English\n- **Audio Codec**: [NeuCodec](https://huggingface.co/neuphonic/neucodec) - our 50hz neural audio codec that achieves exceptional audio quality at low bitrates using a single codebook\n- **Context Window**: 2048 tokens, enough for processing ~30 seconds of audio (including prompt duration)\n- **Format**: Available in GGML format for efficient on-device inference\n- **Responsibility**: Watermarked outputs\n- **Inference Speed**: Real-time generation on mid-range devices\n- **Power Consumption**: Optimised for mobile and embedded devices\n\n## Get Started\n\n> [!NOTE]\n> We have added a [streaming example](examples/basic_streaming_example.py) using the `llama-cpp-python` library as well as a [finetuning script](examples/finetune.py). For finetuning, please refer to the [finetune guide](TRAINING.md) for more details.\n\n1. **Clone Git Repo**\n\n   ```bash\n   git clone https://github.com/neuphonic/neutts-air.git\n   cd neutts-air\n   ```\n\n2. **Install `espeak` (required dependency)**\n\n   Please refer to the following link for instructions on how to install `espeak`:\n\n   https://github.com/espeak-ng/espeak-ng/blob/master/docs/guide.md\n\n   ```bash\n   # Mac OS\n   brew install espeak\n\n   # Ubuntu/Debian\n   sudo apt install espeak\n   ```\n\n   Mac users may need to put the following lines at the top of the neutts.py file.\n\n   ```python\n   from phonemizer.backend.espeak.wrapper import EspeakWrapper\n   _ESPEAK_LIBRARY = '/opt/homebrew/Cellar/espeak/1.48.04_1/lib/libespeak.1.1.48.dylib'  #use the Path to the library.\n   EspeakWrapper.set_library(_ESPEAK_LIBRARY)\n   ```\n\n   Windows users may need to run (see https://github.com/bootphon/phonemizer/issues/163)\n\n   ```pwsh\n   $env:PHONEMIZER_ESPEAK_LIBRARY = \"c:\\Program Files\\eSpeak NG\\libespeak-ng.dll\"\n   $env:PHONEMIZER_ESPEAK_PATH = \"c:\\Program Files\\eSpeak NG\"\n   setx PHONEMIZER_ESPEAK_LIBRARY \"c:\\Program Files\\eSpeak NG\\libespeak-ng.dll\"\n   setx PHONEMIZER_ESPEAK_PATH \"c:\\Program Files\\eSpeak NG\"\n   ```\n\n3. **Install Python dependencies**\n\n   The requirements file includes the dependencies needed to run the model with PyTorch.\n   When using an ONNX decoder or a GGML model, some dependencies (such as PyTorch) are no longer required.\n\n   The inference is compatible and tested on `python>=3.11`.\n\n   ```bash\n   pip install -r requirements.txt\n   ```\n\n4. **(Optional) Install Llama-cpp-python to use the `GGUF` models.**\n\n   ```bash\n   pip install llama-cpp-python\n   ```\n\n   To run llama-cpp with GPU suport (CUDA, MPS) support please refer to:\n   https://pypi.org/project/llama-cpp-python/\n\n5. **(Optional) Install onnxruntime to use the `.onnx` decoder.**\n   If you want to run the onnxdecoder\n   ```bash\n   pip install onnxruntime\n   ```\n\n## Running the Model\n\nRun the basic example script to synthesize speech:\n\n```bash\npython -m examples.basic_example \\\n  --input_text \"My name is Dave, and um, I'm from London\" \\\n  --ref_audio samples/dave.wav \\\n  --ref_text samples/dave.txt\n```\n\nTo specify a particular model repo for the backbone or codec, add the `--backbone` argument. Available backbones are listed in [NeuTTS-Air huggingface collection](https://huggingface.co/collections/neuphonic/neutts-air-68cc14b7033b4",
      "default_branch": "main"
    },
    "fetched_at": "2025-10-26T02:28:04.723963"
  },
  {
    "basic_info": {
      "name": "bdh",
      "full_name": "pathwaycom/bdh",
      "owner": "pathwaycom",
      "description": "Baby Dragon Hatchling (BDH) ‚Äì Architecture and Code",
      "url": "https://github.com/pathwaycom/bdh",
      "clone_url": "https://github.com/pathwaycom/bdh.git",
      "ssh_url": "git@github.com:pathwaycom/bdh.git",
      "homepage": "",
      "created_at": "2025-09-30T12:05:01Z",
      "updated_at": "2025-10-25T22:06:34Z",
      "pushed_at": "2025-10-14T07:57:02Z"
    },
    "stats": {
      "stars": 3224,
      "forks": 138,
      "watchers": 3224,
      "open_issues": 2,
      "size": 1005
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 8721
      },
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "# Baby Dragon Hatchling\n\n## **Bridging the Gap Between Transformers and the Brain**\n\n**Baby Dragon Hatchling (BDH)** is a biologically inspired large language model architecture that connects principles of deep learning with the foundations of neuroscience. Developed by researchers at [Pathway](https://pathway.com), BDH provides a theoretical and practical framework for understanding the emergence of reasoning and generalization in artificial systems.\n\nThis repository contains the official implementation from the paper:\n> *A. Kosowski, P. Uzna≈Ñski, J. Chorowski, Z. Stamirowska, M. Bartoszkiewicz.*\n> [_The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain_](https://doi.org/10.48550/arXiv.2509.26507), arXiv (2025).\n\n\n## Overview\n\nBDH represents a **scale-free, locally interacting network of neurons** capable of intrinsic reasoning dynamics. BDH scales like a Transformer on performance benchmarks‚Äîyet retains full interpretability and theoretical grounding in the fine-grained dynamics of neuron interactions.\n\n**Key properties:**\n\n- **Scale-free network topology** mimicking biological connectivity\n- **Locally interacting neuron particles** with excitatory/inhibitory dynamics\n- **Hebbian working memory** based on synaptic plasticity, displaying monosemanticity\n- **GPU-friendly state-space formulation** for efficient implementation\n- **Interpretable activations** that are sparse and positive\n\nBDH formalizes a bridge between **neural computation and machine-based language understanding**. It shows how **macro reasoning behavior** in large AI models emerges from **micro-level neuron dynamics**, guided by principles of graph theory and local computation.\n\nEmpirically, BDH matches **GPT-2‚Äìscale Transformers** across language and translation tasks at equivalent parameter scales (10M‚Äì1B).\n\n\n***\n\n## Architecture\n\n<img src=\"figs/architecture.png\" width=\"600\"/>\n\n***\n\n## Relation to Transformers\n\n<img src=\"figs/vocab.png\" width=\"600\"/>\n\nBDH and the Transformer share attention-inspired computation; however, BDH‚Äôs graph-based architecture makes its attention **emerge naturally from neuron-level interactions**, reflecting attention as seen in biological systems.\n\n***\n\n## Scaling Laws\n\n<img src=\"figs/bdh_scaling.png\" width=\"600\"/>\n\nBDH follows **Transformer-like scaling laws**, maintaining parameter efficiency while achieving interpretability at any scale.\n\n***\n\n## Installation and Training\n\n```bash\n# install dependencies\npip install -r requirements.txt\n\n# train BDH on a toy dataset\npython train.py\n```\n\n<!--For visualization and interpretability analysis, explore the example notebooks in `notebooks/`.-->\n\n\n\n## Learn and Discuss\n\n- Watch the *SuperDataScience podcast* [‚ñ∂Ô∏è *Dragon Hatchling: The Missing Link Between Transformers and the Brain*](https://www.youtube.com/watch?v=mfV44-mtg7c) (72 min.) featuring Adrian‚ÄØKosowski in conversation with Jon‚ÄØKrohn, unpacking BDH‚Äôs neuron-level architecture and sparse reasoning dynamics.\n\n- Read about BDH in\n[*Forbes*](https://www.forbes.com/sites/victordey/2025/10/08/can-ai-learn-and-evolve-like-a-brain-pathways-bold-research-thinks-so/),\n[*Semafor*](https://www.semafor.com/article/10/01/2025/new-ai-research-claims-to-be-getting-closer-to-modeling-human-brain),\n[*The Turing Post*](https://www.turingpost.com/p/fod-121-300-million-to-start-a-big-promise-for-science#the-freshest-research-papers-catego),\n[*Quantum Zeitgeist*](https://quantumzeitgeist.com/palo-alto-ai-firm-pathway-unveils-post-transformer-architecture-for-autonomous-ai/),\n[*Golem*](https://www.golem.de/news/neue-ki-architektur-was-ist-baby-dragon-hatchling-2510-201047-2.html),\nand elsewhere in the media.\n\n- Discuss and share the BDH paper on:\n[*Hugging Face Papers*](https://huggingface.co/papers/2509.26507), \n[*Alphaxiv*](https://alphaxiv.org/abs/2509.26507),\nand [*EmergentMind*](https://emergentmind.com/papers/2509.26507).\n\n## Community Forks\n\n- [adamskrodzki/bdh](https://github.com/adamskrodzki/bdh): dynamic vocabulary, stateful attention\n- [mosure/burn_dragon_hatchling](https://github.com/mosure/burn_dragon_hatchling): Burn port\n- [severian42/bdh](https://github.com/severian42/bdh): MLX port\n- [Git-Faisal/bdh](https://github.com/Git-Faisal/bdh)\n- [GrahLnn/bdh](https://github.com/GrahLnn/bdh)\n\n## Acknowledgements\nWe thank Andrej Karpathy for the [nanoGPT](https://github.com/karpathy/nanoGPT/) code and the tiny Shapespeare dataset used in this demonstration.\n\nBDH research stands at the intersection of **AI architecture**, **biological learning models**, and **theoretical computer science**‚Äîan effort to map the *equations of reasoning* between artificial and biological intelligence.\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-10-26T02:28:05.887543"
  },
  {
    "basic_info": {
      "name": "HunyuanImage-3.0",
      "full_name": "Tencent-Hunyuan/HunyuanImage-3.0",
      "owner": "Tencent-Hunyuan",
      "description": "HunyuanImage-3.0: A Powerful Native Multimodal Model for Image Generation",
      "url": "https://github.com/Tencent-Hunyuan/HunyuanImage-3.0",
      "clone_url": "https://github.com/Tencent-Hunyuan/HunyuanImage-3.0.git",
      "ssh_url": "git@github.com:Tencent-Hunyuan/HunyuanImage-3.0.git",
      "homepage": "https://hunyuan.tencent.com/image",
      "created_at": "2025-09-27T07:18:47Z",
      "updated_at": "2025-10-25T21:38:08Z",
      "pushed_at": "2025-10-14T08:42:04Z"
    },
    "stats": {
      "stars": 2297,
      "forks": 96,
      "watchers": 2297,
      "open_issues": 28,
      "size": 34784
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 374880,
        "Shell": 806
      },
      "license": "Other",
      "topics": [
        "image-generation",
        "native-multimodal-model"
      ]
    },
    "content": {
      "readme": "[‰∏≠ÊñáÊñáÊ°£](./README_zh_CN.md)\n\n<div align=\"center\">\n\n<img src=\"./assets/logo.png\" alt=\"HunyuanImage-3.0 Logo\" width=\"600\">\n\n# üé® HunyuanImage-3.0: A Powerful Native Multimodal Model for Image Generation\n\n</div>\n\n\n<div align=\"center\">\n<img src=\"./assets/banner.png\" alt=\"HunyuanImage-3.0 Banner\" width=\"800\">\n\n</div>\n\n<div align=\"center\">\n  <a href=https://hunyuan.tencent.com/image target=\"_blank\"><img src=https://img.shields.io/badge/Official%20Site-333399.svg?logo=homepage height=22px></a>\n  <a href=https://huggingface.co/tencent/HunyuanImage-3.0 target=\"_blank\"><img src=https://img.shields.io/badge/%F0%9F%A4%97%20Models-d96902.svg height=22px></a>\n  <a href=https://github.com/Tencent-Hunyuan/HunyuanImage-3.0 target=\"_blank\"><img src= https://img.shields.io/badge/Page-bb8a2e.svg?logo=github height=22px></a>\n  <a href=https://arxiv.org/pdf/2509.23951 target=\"_blank\"><img src=https://img.shields.io/badge/Report-b5212f.svg?logo=arxiv height=22px></a>\n  <a href=https://x.com/TencentHunyuan target=\"_blank\"><img src=https://img.shields.io/badge/Hunyuan-black.svg?logo=x height=22px></a>\n  <a href=https://docs.qq.com/doc/DUVVadmhCdG9qRXBU target=\"_blank\"><img src=https://img.shields.io/badge/üìö-PromptHandBook-blue.svg?logo=book height=22px></a>\n</div>\n\n\n<p align=\"center\">\n    üëè Join our <a href=\"./assets/WECHAT.md\" target=\"_blank\">WeChat</a> and <a href=\"https://discord.gg/ehjWMqF5wY\">Discord</a> | \nüíª <a href=\"https://hunyuan.tencent.com/modelSquare/home/play?modelId=289&from=/visual\">Official website(ÂÆòÁΩë) Try our model!</a>&nbsp&nbsp\n</p>\n\n## üî•üî•üî• News\n- **September 28, 2025**: üìñ **HunyuanImage-3.0 Technical Report Released** - Comprehensive technical documentation now available\n- **September 28, 2025**: üöÄ **HunyuanImage-3.0 Open Source Release** - Inference code and model weights publicly available\n\n\n## üß© Community Contributions\n\nIf you develop/use HunyuanImage-3.0 in your projects, welcome to let us know.\n\n## üìë Open-source Plan\n\n- HunyuanImage-3.0 (Image Generation Model)\n  - [x] Inference \n  - [x] HunyuanImage-3.0 Checkpoints\n  - [ ] HunyuanImage-3.0-Instruct Checkpoints (with reasoning)\n  - [ ] VLLM Support\n  - [ ] Distilled Checkpoints\n  - [ ] Image-to-Image Generation\n  - [ ] Multi-turn Interaction\n\n\n## üóÇÔ∏è Contents\n- [üî•üî•üî• News](#-news)\n- [üß© Community Contributions](#-community-contributions)\n- [üìë Open-source Plan](#-open-source-plan)\n- [üìñ Introduction](#-introduction)\n- [‚ú® Key Features](#-key-features)\n- [üõ†Ô∏è Dependencies and Installation](#-dependencies-and-installation)\n  - [üíª System Requirements](#-system-requirements)\n  - [üì¶ Environment Setup](#-environment-setup)\n  - [üì• Install Dependencies](#-install-dependencies)\n  - [Performance Optimizations](#performance-optimizations)\n- [üöÄ Usage](#-usage)\n  - [üî• Quick Start with Transformers](#-quick-start-with-transformers)\n  - [üè† Local Installation & Usage](#-local-installation--usage)\n  - [üé® Interactive Gradio Demo](#-interactive-gradio-demo)\n- [üß± Models Cards](#-models-cards)\n- [üìù Prompt Guide](#-prompt-guide)\n  - [Manually Writing Prompts](#manually-writing-prompts)\n  - [System Prompt For Automatic Rewriting the Prompt](#system-prompt-for-automatic-rewriting-the-prompt)\n  - [Advanced Tips](#advanced-tips)\n  - [More Cases](#more-cases)\n- [üìä Evaluation](#-evaluation)\n- [üìö Citation](#-citation)\n- [üôè Acknowledgements](#-acknowledgements)\n- [üåüüöÄ  Github Star History](#-github-star-history)\n\n---\n\n## üìñ Introduction\n\n**HunyuanImage-3.0** is a groundbreaking native multimodal model that unifies multimodal understanding and generation within an autoregressive framework. Our text-to-image module achieves performance **comparable to or surpassing** leading closed-source models.\n\n\n<div align=\"center\">\n  <img src=\"./assets/framework.png\" alt=\"HunyuanImage-3.0 Framework\" width=\"90%\">\n</div>\n\n## ‚ú® Key Features\n\n* üß† **Unified Multimodal Architecture:** Moving beyond the prevalent DiT-based architectures, HunyuanImage-3.0 employs a unified autoregressive framework. This design enables a more direct and integrated modeling of text and image modalities, leading to surprisingly effective and contextually rich image generation.\n\n* üèÜ **The Largest Image Generation MoE Model:** This is the largest open-source image generation Mixture of Experts (MoE) model to date. It features 64 experts and a total of 80 billion parameters, with 13 billion activated per token, significantly enhancing its capacity and performance.\n\n* üé® **Superior Image Generation Performance:** Through rigorous dataset curation and advanced reinforcement learning post-training, we've achieved an optimal balance between semantic accuracy and visual excellence. The model demonstrates exceptional prompt adherence while delivering photorealistic imagery with stunning aesthetic quality and fine-grained details.\n\n* üí≠ **Intelligent World-Knowledge Reasoning:** The unified multimodal architecture endows HunyuanImage-3.0 with powerful reasoning capabilities. It leverages its extensive world knowledge to intelligently interpret user int",
      "default_branch": "main"
    },
    "fetched_at": "2025-10-26T02:28:07.063791"
  },
  {
    "basic_info": {
      "name": "dexter",
      "full_name": "virattt/dexter",
      "owner": "virattt",
      "description": "An autonomous agent for deep financial research",
      "url": "https://github.com/virattt/dexter",
      "clone_url": "https://github.com/virattt/dexter.git",
      "ssh_url": "git@github.com:virattt/dexter.git",
      "homepage": null,
      "created_at": "2025-10-14T21:02:00Z",
      "updated_at": "2025-10-26T02:24:49Z",
      "pushed_at": "2025-10-22T00:20:41Z"
    },
    "stats": {
      "stars": 2218,
      "forks": 257,
      "watchers": 2218,
      "open_issues": 8,
      "size": 27
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 57621,
        "JavaScript": 228
      },
      "license": null,
      "topics": []
    },
    "content": {
      "readme": "# Dexter ü§ñ\n\nDexter is an autonomous financial research agent that thinks, plans, and learns as it works. It performs analysis using task planning, self-reflection, and real-time market data. Think Claude Code, but built specifically for financial research.\n\n\n<img width=\"979\" height=\"651\" alt=\"Screenshot 2025-10-14 at 6 12 35‚ÄØPM\" src=\"https://github.com/user-attachments/assets/5a2859d4-53cf-4638-998a-15cef3c98038\" />\n\n## Overview\n\nDexter takes complex financial questions and turns them into clear, step-by-step research plans. It runs those tasks using live market data, checks its own work, and refines the results until it has a confident, data-backed answer.  \n\nIt‚Äôs not just another chatbot.  It‚Äôs an agent that plans ahead, verifies its progress, and keeps iterating until the job is done.\n\n**Key Capabilities:**\n- **Intelligent Task Planning**: Automatically decomposes complex queries into structured research steps\n- **Autonomous Execution**: Selects and executes the right tools to gather financial data\n- **Self-Validation**: Checks its own work and iterates until tasks are complete\n- **Real-Time Financial Data**: Access to income statements, balance sheets, and cash flow statements\n- **Safety Features**: Built-in loop detection and step limits to prevent runaway execution\n\n[![Twitter Follow](https://img.shields.io/twitter/follow/virattt?style=social)](https://twitter.com/virattt)\n\n### Prerequisites\n\n- Python 3.10 or higher\n- [uv](https://github.com/astral-sh/uv) package manager\n- OpenAI API key (get [here](https://platform.openai.com/api-keys))\n- Financial Datasets API key (get [here](https://financialdatasets.ai))\n\n### Installation\n\n1. Clone the repository:\n```bash\ngit clone https://github.com/virattt/dexter.git\ncd dexter\n```\n\n2. Install dependencies with uv:\n```bash\nuv sync\n```\n\n3. Set up your environment variables:\n```bash\n# Copy the example environment file\ncp env.example .env\n\n# Edit .env and add your API keys\n# OPENAI_API_KEY=your-openai-api-key\n# FINANCIAL_DATASETS_API_KEY=your-financial-datasets-api-key\n```\n\n### Usage\n\nRun Dexter in interactive mode:\n```bash\nuv run dexter-agent\n```\n\n### Example Queries\n\nTry asking Dexter questions like:\n- \"What was Apple's revenue growth over the last 4 quarters?\"\n- \"Compare Microsoft and Google's operating margins for 2023\"\n- \"Analyze Tesla's cash flow trends over the past year\"\n- \"What is Amazon's debt-to-equity ratio based on recent financials?\"\n\nDexter will automatically:\n1. Break down your question into research tasks\n2. Fetch the necessary financial data\n3. Perform calculations and analysis\n4. Provide a comprehensive, data-rich answer\n\n## Architecture\n\nDexter uses a multi-agent architecture with specialized components:\n\n- **Planning Agent**: Analyzes queries and creates structured task lists\n- **Action Agent**: Selects appropriate tools and executes research steps\n- **Validation Agent**: Verifies task completion and data sufficiency\n- **Answer Agent**: Synthesizes findings into comprehensive responses\n\n## Project Structure\n\n```\ndexter/\n‚îú‚îÄ‚îÄ src/\n‚îÇ   ‚îú‚îÄ‚îÄ dexter/\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ agent.py      # Main agent orchestration logic\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ model.py      # LLM interface\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ tools.py      # Financial data tools\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ prompts.py    # System prompts for each component\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ schemas.py    # Pydantic models\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ utils/        # Utility functions\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ cli.py        # CLI entry point\n‚îú‚îÄ‚îÄ pyproject.toml\n‚îî‚îÄ‚îÄ uv.lock\n```\n\n## Configuration\n\nDexter supports configuration via the `Agent` class initialization:\n\n```python\nfrom dexter.agent import Agent\n\nagent = Agent(\n    max_steps=20,              # Global safety limit\n    max_steps_per_task=5       # Per-task iteration limit\n)\n```\n\n## How to Contribute\n\n1. Fork the repository\n2. Create a feature branch\n3. Commit your changes\n4. Push to the branch\n5. Create a Pull Request\n\n**Important**: Please keep your pull requests small and focused.  This will make it easier to review and merge.\n\n\n## License\n\nThis project is licensed under the MIT License.\n\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-10-26T02:28:08.219763"
  },
  {
    "basic_info": {
      "name": "DreamOmni2",
      "full_name": "dvlab-research/DreamOmni2",
      "owner": "dvlab-research",
      "description": "This project is the official implementation of 'DreamOmni2: Multimodal Instruction-based Editing and Generation''",
      "url": "https://github.com/dvlab-research/DreamOmni2",
      "clone_url": "https://github.com/dvlab-research/DreamOmni2.git",
      "ssh_url": "git@github.com:dvlab-research/DreamOmni2.git",
      "homepage": "",
      "created_at": "2025-09-28T05:20:51Z",
      "updated_at": "2025-10-26T02:08:22Z",
      "pushed_at": "2025-10-20T06:43:08Z"
    },
    "stats": {
      "stars": 2076,
      "forks": 184,
      "watchers": 2076,
      "open_issues": 19,
      "size": 15842
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 142712
      },
      "license": "Apache License 2.0",
      "topics": [
        "image-editing",
        "image-generation",
        "unified-generation-editing-model"
      ]
    },
    "content": {
      "readme": "# DreamOmni2: Multimodal Instruction-based Editing and Generation\n\n<p align=\"center\">\n    <a href=\"https://arxiv.org/html/2510.06679v1\">\n            <img alt=\"Build\" src=\"https://img.shields.io/badge/arXiv%20paper-2510.06679v1-b31b1b.svg\">\n    </a>\n    <a href=\"https://pbihao.github.io/projects/DreamOmni2/index.html\">\n        <img alt=\"Project Page\" src=\"https://img.shields.io/badge/Project-Page-blue\">\n    </a>\n    <a href=\"https://www.youtube.com/watch?v=8xpoiRK57uU\">\n        <img alt=\"Video Demo\" src=\"https://img.shields.io/badge/Video-Demo-red\">\n    </a>\n    <a href=\"https://huggingface.co/datasets/xiabs/DreamOmni2Bench\">\n        <img alt=\"Build\" src=\"https://img.shields.io/badge/DreamOmni2-Benchmark-green\">\n    </a>\n    <a href=\"https://huggingface.co/xiabs/DreamOmni2\">\n        <img alt=\"Build\" src=\"https://img.shields.io/badge/ü§ó-HF%20Model-yellow\">\n    </a>    \n    <a href=\"https://huggingface.co/spaces/wcy1122/DreamOmni2-Edit\">\n        <img alt=\"Build\" src=\"https://img.shields.io/badge/ü§ó-HF%20Editing%20Demo-yellow\">\n    </a>\n    <a href=\"https://huggingface.co/spaces/wcy1122/DreamOmni2-Gen\">\n        <img alt=\"Build\" src=\"https://img.shields.io/badge/ü§ó-HF%20Generation%20Demo-yellow\">\n    </a>\n    <a href=\"https://www.runninghub.ai/workflow/1980131298238959618\">\n        <img alt=\"Build\" src=\"https://img.shields.io/badge/ComfyUI-Runninghub-blue\">\n    </a>\n</p>\n\n## üî• News\n- üî•**2025.10.10**: Release DreamOmni2 [editing demo](https://huggingface.co/spaces/wcy1122/DreamOmni2-Edit) and [generation demo](https://huggingface.co/spaces/wcy1122/DreamOmni2-Gen)\n- üî•**2025.10.10**: Release DreamOmni2 [Benchmark](https://huggingface.co/datasets/xiabs/DreamOmni2Bench).\n- üî•**2025.10.10**: Release DreamOmni2's [codes](https://github.com/dvlab-research/DreamOmni2) and [models](https://huggingface.co/xiabs/DreamOmni2).\n- üî•**2025.10.09**: Release DreamOmni2 [tech report](https://arxiv.org/html/2510.06679v1).\n\n\n<p align=\"center\">\n  <img width=\"600\" src=\"imgs/gallery.png\">\n</p>\n\n\n<div align=\"center\">\n  <a href=\"https://cloud.video.taobao.com/vod/HxWB8i8sYkh0DdfvfByoMHqRtezNMCpWJdjzWTOCqdY.mp4\">\n    <img src=\"imgs/cover.png\" alt=\"Watch the video\" style=\"width: 600px;\">\n  </a>\n</div>\n\n\n## Introduction\n\n**(1) Multimodal Instruction-based Generation**\n\nFor traditional subject-driven generation based on concrete objects, DreamOmni2 achieves the best results among open-source models, showing superior identity and pose consistency. Additionally, DreamOmni2 can reference abstract attributes (such as material, texture, makeup, hairstyle, posture, design style, artistic style, etc.), even surpassing commercial models in this area.\n\n**(2) Multimodal Instruction-based Editing**\n\nBeyond traditional instruction-based editing models, DreamOmni2 supports multimodal instruction editing. In everyday editing tasks, there are often elements that are difficult to describe purely with language and require reference images. Our model addresses this need, supporting references to any concrete objects and abstract attributes, with performance comparable to commercial models.\n\n**(3) Unified Generation and Editing Model**\n\nBuilding upon these two new tasks, we introduce DreamOmni2, which is capable of multimodal instruction-based editing and generation under any concrete or abstract concept guidance. Overall, DreamOmni2 is a more intelligent and powerful open-sourced unified generation and editing model, offering enhanced capabilities across a wide range of tasks.\n\n## Editing and Generation Model?\nEditing and generation are distinct tasks. Editing requires strict consistency in preserving the non-edited areas of the source image, while generation only needs to retain the ID, IP, or attribution from the reference image as per the instructions, allowing the entire image to be regenerated with a focus on aesthetics. We‚Äôve found that the instructions for generation and editing are often similar, so we‚Äôve separated these two tasks to make it easier for users to choose the appropriate task type.\n\n## Quick Start\n\n### Requirements and Installation\n\nFirst, install the necessary dependencies:\n```bash\ngit clone https://github.com/dvlab-research/DreamOmni2\ncd ./DreamOmni2\npip install -r requirements.txt\n```\n\nNext, download the DreamOmni2 weights into the models folder.\n\n```bash\nhuggingface-cli download --resume-download --local-dir-use-symlinks False xiabs/DreamOmni2 --local-dir ./models\n```\n\n### Inference\n\nMultimodal Instriction-based Editing\n\n**Notably, for editing tasks, due to the format settings of the training data, we need to place the image to be edited in the first position.**\n\n```bash\npython3 /mnt/bn/unifygen/xiabin_dev/iclr2026/DreamOmni2/inference_edit.py \\\n    --input_img_path \"example_input/edit_tests/src.jpg\" \"example_input/edit_tests/ref.jpg\" \\\n    --input_instruction \"Make the woman from the second image stand on the road in the first image.\" \\\n    --output_path \"example_input/edit_tests/edit_res.png\"\n```\n\nMultimodal Instriction-based Generation\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-10-26T02:28:09.364001"
  },
  {
    "basic_info": {
      "name": "agentic-design-patterns-cn",
      "full_name": "ginobefun/agentic-design-patterns-cn",
      "owner": "ginobefun",
      "description": "„ÄäAgentic Design Patterns„Äã‰∏≠ÊñáÁøªËØëÁâà",
      "url": "https://github.com/ginobefun/agentic-design-patterns-cn",
      "clone_url": "https://github.com/ginobefun/agentic-design-patterns-cn.git",
      "ssh_url": "git@github.com:ginobefun/agentic-design-patterns-cn.git",
      "homepage": null,
      "created_at": "2025-10-09T04:36:28Z",
      "updated_at": "2025-10-26T01:51:22Z",
      "pushed_at": "2025-10-26T01:51:19Z"
    },
    "stats": {
      "stars": 2052,
      "forks": 233,
      "watchers": 2052,
      "open_issues": 1,
      "size": 7747
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 114372
      },
      "license": null,
      "topics": []
    },
    "content": {
      "readme": "[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/ginobefun-agentic-design-patterns-cn-badge.png)](https://mseep.ai/app/ginobefun-agentic-design-patterns-cn)\n\n# Agentic Design Patterns | <mark>Êô∫ËÉΩ‰ΩìËÆæËÆ°Ê®°Âºè</mark>\n\n## A Hands-On Guide to Building Intelligent Systems | <mark>ÊûÑÂª∫Êô∫ËÉΩÁ≥ªÁªüÁöÑÂÆûË∑µÊåáÂçó</mark>\n\n[![License: CC BY-NC 4.0](https://img.shields.io/badge/License-CC%20BY--NC%204.0-lightgrey.svg)](https://creativecommons.org/licenses/by-nc/4.0/)\n[![GitHub stars](https://img.shields.io/github/stars/ginobefun/agentic-design-patterns-cn)](https://github.com/ginobefun/agentic-design-patterns-cn/stargazers)\n[![GitHub forks](https://img.shields.io/github/forks/ginobefun/agentic-design-patterns-cn)](https://github.com/ginobefun/agentic-design-patterns-cn/network)\n\n**Âéü‰π¶‰ΩúËÄÖ (Author)**: [Antonio Gulli](https://www.linkedin.com/in/searchguy/)\n\n**Âéü‰π¶ÈìæÊé• (Original Book)**: [Amazon](https://www.amazon.com/Agentic-Design-Patterns-Hands-Intelligent/dp/3032014018/)\n\n**ÂéüÂßãÊñáÊ°£ÈìæÊé• (Original Book Link)**: [Google Docs](https://docs.google.com/document/d/1rsaK53T3Lg5KoGwvf8ukOUvbELRtH-V0LnOIFDxBryE/preview?tab=t.0#heading=h.pxcur8v2qagu)\n\n---\n\n## üìñ È°πÁõÆÁÆÄ‰ªã | Project Description\n\nÊú¨È°πÁõÆÊòØÂØπ Antonio Gulli ÊâÄËëó„ÄäAgentic Design Patterns: A Hands-On Guide to Building Intelligent Systems„ÄãÁöÑ**‰∏≠Ëã±ÊñáÂØπÁÖßÁøªËØë**„ÄÇËØ•‰π¶ÊòØ‰∏ÄÈÉ®ÂÖ®Èù¢ÁöÑÊäÄÊúØÊåáÂçóÔºåÊ∂µÁõñ‰∫ÜÁé∞‰ª£‰∫∫Â∑•Êô∫ËÉΩÁ≥ªÁªü‰∏≠Êô∫ËÉΩ‰Ωì (Agent) ËÆæËÆ°ÁöÑÊ†∏ÂøÉÊ¶ÇÂøµÂíåÂÆûË∑µÊñπÊ≥ï„ÄÇ\n\nThis project is a **bilingual Chinese-English translation** of \"Agentic Design Patterns: A Hands-On Guide to Building Intelligent Systems\" by Antonio Gulli. The book is a comprehensive technical guide covering core concepts and practical approaches to agent design in modern AI systems.\n\n---\n\n## üéØ È°πÁõÆÁâπËâ≤ | Key Features\n\n- üìö **‰∏≠Ëã±ÊñáÂØπÁÖß** - ÂÆåÊï¥ÁöÑÂèåËØ≠ÂØπÁÖßÁøªËØë\n- üé® **È´ò‰∫ÆÊòæÁ§∫** - ‰∏≠ÊñáÂÜÖÂÆπ‰ΩøÁî®ÈªÑËâ≤È´ò‰∫ÆÔºåÊòì‰∫éÂå∫ÂàÜ\n- üìù **Ê†ºÂºèËßÑËåÉ** - ‰∏•Ê†ºÈÅµÂæ™ Markdown Ê†áÂáÜÂíåÁøªËØëËßÑËåÉ\n- üîó **‰ª£Á†ÅÈìæÊé•** - ‰øùÁïôÊâÄÊúâÂéü‰π¶‰ª£Á†ÅÁ§∫‰æãÈìæÊé•\n- ‚ö° **ÊåÅÁª≠Êõ¥Êñ∞** - ÈÄêÁ´†ÁøªËØëÔºåÊåÅÁª≠Êõ¥Êñ∞ËøõÂ∫¶\n\n---\n\n## üìã ÁøªËØëËøõÂ∫¶ | Translation Progress\n\n**<mark>ÊÄªÈ°µÊï∞Ôºö424 È°µ</mark>** | **Total: 424 Pages**\n\n### ÂâçÁΩÆÂÜÖÂÆπ | Front Matter\n\n| Á´†ËäÇ | Ê¶ÇËø∞ | Ë¥üË¥£‰∫∫ | AI ÁøªËØë | ‰∫∫Â∑•ËØÑÂÆ° | ‰∫§ÂèâËØÑÂÆ° |\n|------|------|--------|---------|----------|----------|\n| [ÁåÆËæû](01-Dedication.md) | ‰ΩúËÄÖÁöÑÁåÆËæû‰∏éËá¥Êï¨ | @ginobefun | ‚úÖ | ‚úÖ | ‚è≥ |\n| [Ëá¥Ë∞¢](02-Acknowledgment.md) | Ëá¥Ë∞¢‰∏éÊÑüË∞¢ÂêçÂçï | @ginobefun | ‚úÖ | ‚úÖ | ‚è≥ |\n| [Â∫èË®Ä](03-Foreword.md) | Êú¨‰π¶ÁöÑÂ∫èË®Ä‰∏éËÉåÊôØ‰ªãÁªç | @ginobefun | ‚úÖ | ‚úÖ | ‚è≥ |\n| [ÊÄùÊÉ≥È¢ÜË¢ñÁöÑÊ¥ûËßÅ](04-Thought-Leader.md) | ÊùÉÂäõ‰∏éË¥£‰ªªÁöÑÊ∑±Â∫¶ÊÄùËÄÉ | @ginobefun | ‚úÖ | ‚úÖ | ‚è≥ |\n| [‰ªãÁªç](05-Introduction.md) | ÂÖ®‰π¶ÂºïË®Ä‰∏éÊ†∏ÂøÉÊ¶ÇÂøµ | @ginobefun | ‚úÖ | ‚úÖ | ‚è≥ |\n| [‰ªÄ‰πàÊòØ\"Êô∫ËÉΩ‰Ωì\"Ôºü](06-What-Makes-Agent.md) | ÂÆö‰πâ AI Á≥ªÁªüÁöÑ\"Êô∫ËÉΩ‰Ωì\"ÁâπÂæÅ | @ginobefun | ‚úÖ | ‚úÖ | ‚è≥ |\n\n### Á¨¨‰∏ÄÈÉ®ÂàÜÔºöÊ†∏ÂøÉËÆæËÆ°Ê®°Âºè | Part One: Core Patterns (103 È°µ)\n\n| Á´†ËäÇ | ËÆæËÆ°Ê®°ÂºèÊ¶ÇËø∞ | Ë¥üË¥£‰∫∫ | AI ÁøªËØë | ‰∫∫Â∑•ËØÑÂÆ° | ‰∫§ÂèâËØÑÂÆ° |\n|------|-------------|--------|---------|----------|----------|\n| [Á¨¨ 1 Á´†ÔºöÊèêÁ§∫Èìæ](07-Chapter-01-Prompt-Chaining.md) | ÂàÜËÄåÊ≤ª‰πãÁöÑ‰ªªÂä°ÂàÜËß£Ê®°ÂºèÔºåÂ∞ÜÂ§çÊùÇ‰ªªÂä°ÂàÜËß£‰∏∫Â§ÑÁêÜÊµÅÊ∞¥Á∫ø | @ginobefun | ‚úÖ | ‚úÖ | ‚è≥ |\n| [Á¨¨ 2 Á´†ÔºöË∑ØÁî±](08-Chapter-02-Routing.md) | Êô∫ËÉΩÂÜ≥Á≠ñ‰∏éÂä®ÊÄÅÂàÜÂèëÔºåÊ†πÊçÆÊÉÖÂ¢ÉÈÄâÊã©ÊúÄ‰Ω≥Ë°åÂä®Ë∑ØÂæÑ | @ginobefun | ‚úÖ | ‚úÖ | ‚è≥ |\n| [Á¨¨ 3 Á´†ÔºöÂπ∂Ë°åÂåñ](09-Chapter-03-Parallelization.md) | Âπ∂ÂèëÊâßË°å‰∏éÊÄßËÉΩÊèêÂçáÔºåÂêåÊó∂ÊâßË°åÂ§ö‰∏™Áã¨Á´ã‰ªªÂä° | @ginobefun | ‚úÖ | ‚úÖ | ‚è≥ |\n| [Á¨¨ 4 Á´†ÔºöÂèçÊÄù](10-Chapter-04-Reflection.md) | Ëá™ÊàëËØÑ‰º∞ÂíåËø≠‰ª£ÊîπËøõÔºåÈÄöËøáÂèçÈ¶àÂæ™ÁéØ‰ºòÂåñËæìÂá∫Ë¥®Èáè | @ginobefun | ‚úÖ | ‚úÖ | ‚è≥ |\n| [Á¨¨ 5 Á´†ÔºöÂ∑•ÂÖ∑‰ΩøÁî®](11-Chapter-05-Tool-Use.md) | Â§ñÈÉ®Â∑•ÂÖ∑‰∏é API ÈõÜÊàêÔºåÊâ©Â±ïÊô∫ËÉΩ‰ΩìËÉΩÂäõËæπÁïå | @ginobefun | ‚úÖ | ‚úÖ | ‚è≥ |\n| [Á¨¨ 6 Á´†ÔºöËßÑÂàí](12-Chapter-06-Planning.md) | Â§öÊ≠•È™§ËÆ°ÂàíÂà∂ÂÆö‰∏éÊâßË°åÔºåÂÆûÁé∞Â§çÊùÇÁõÆÊ†áÂàÜËß£ | @ginobefun | ‚úÖ | ‚úÖ | ‚è≥ |\n| [Á¨¨ 7 Á´†ÔºöÂ§öÊô∫ËÉΩ‰ΩìÂçè‰Ωú](13-Chapter-07-Multi-Agent-Collaboration.md) | ÂçèÂêåÂ∑•‰ΩúÊû∂ÊûÑÔºåÂ§ö‰∏™Êô∫ËÉΩ‰ΩìÈÖçÂêàÂÆåÊàê‰ªªÂä° | @ginobefun | ‚úÖ  | ‚úÖ | ‚ùå |\n\n### Á¨¨‰∫åÈÉ®ÂàÜÔºöÈ´òÁ∫ßËÆæËÆ°Ê®°Âºè | Part Two: Advanced Patterns (61 È°µ)\n\n| Á´†ËäÇ | ËÆæËÆ°Ê®°ÂºèÊ¶ÇËø∞ | Ë¥üË¥£‰∫∫ | AI ÁøªËØë | ‰∫∫Â∑•ËØÑÂÆ° | ‰∫§ÂèâËØÑÂÆ° |\n|------|-------------|--------|---------|----------|----------|\n| [Á¨¨ 8 Á´†ÔºöËÆ∞ÂøÜÁÆ°ÁêÜ](14-Chapter-08-Memory-Management.md) | Áü≠ÊúüÂíåÈïøÊúüËÆ∞ÂøÜÁÆ°ÁêÜÔºåÁª¥ÊåÅ‰∏ä‰∏ãÊñáËøûÁª≠ÊÄß | @ÈÉëÊ∂õ | ‚úÖ | ‚úÖ | ‚ùå |\n| [Á¨¨ 9 Á´†ÔºöÂ≠¶‰π†‰∏éÈÄÇÂ∫î](15-Chapter-09-Learning-and-Adaptation.md) | ‰ªéÁªèÈ™å‰∏≠Â≠¶‰π†ÔºåÊåÅÁª≠‰ºòÂåñÊô∫ËÉΩ‰ΩìË°å‰∏∫ | @ÈôàËØó‰∏≠ | ‚è≥ | ‚ùå | ‚ùå |\n| [Á¨¨ 10 Á´†ÔºöÊ®°Âûã‰∏ä‰∏ãÊñáÂçèËÆÆ](16-Chapter-10-Model-Context-Protocol.md) | Ê†áÂáÜÂåñ‰∫§‰∫íÂçèËÆÆÔºåËßÑËåÉÊô∫ËÉΩ‰ΩìÈÄö‰ø°ÊñπÂºè | @ÈÉëÊ∂õ | ‚è≥ | ‚ùå | ‚ùå |\n| [Á¨¨ 11 Á´†ÔºöÁõÆÊ†áËÆæÂÆö‰∏éÁõëÊéß](17-Chapter-11-Goal-Setting-and-Monitoring.md) | Âä®ÊÄÅÁõÆÊ†áÁÆ°ÁêÜÔºåÂÆûÊó∂ËøΩË∏™‰ªªÂä°ËøõÂ±ï | [@ÊùéÊµ™Ê∫™](https://github.com/seabornlee) | ‚úÖ | ‚úÖ | ‚è≥ |\n\n### Á¨¨‰∏âÈÉ®ÂàÜÔºöÈõÜÊàêËÆæËÆ°Ê®°Âºè | Part Three: Integration Patterns (34 È°µ)\n\n| Á´†ËäÇ | ËÆæËÆ°Ê®°ÂºèÊ¶ÇËø∞ | Ë¥üË¥£‰∫∫ | AI ÁøªËØë | ‰∫∫Â∑•ËØÑÂÆ° | ‰∫§ÂèâËØÑÂÆ° |\n|------|-------------|--------|---------|----------|----------|\n| [Á¨¨ 12 Á´†ÔºöÂºÇÂ∏∏Â§ÑÁêÜ‰∏éÊÅ¢Â§ç](18-Chapter-12-Exception-Handling-and-Recovery.md) | ‰ºòÈõÖÈîôËØØÂ§ÑÁêÜÔºåÁ°Æ‰øùÁ≥ªÁªüÁ®≥ÂÆöÊÄß | @EE | ‚ùå | ‚ùå | ‚ùå |\n| [Á¨¨ 13 Á´†Ôºö‰∫∫Êú∫Âçè‰Ωú](19-Chapter-13-Human-in-the-Loop.md) | ‰∫∫Êú∫Âçè‰ΩúÂÜ≥Á≠ñÔºåËûçÂêà‰∫∫Á±ªÊô∫ÊÖß‰∏é AI ËÉΩÂäõ | @ÊõæÊ±â | ‚úÖ | ‚úÖ | ‚è≥ |\n| [Á¨¨ 14 Á´†ÔºöÁü•ËØÜÊ£ÄÁ¥¢ (RAG)](20-Chapter-14-Knowledge-Retrieval.md) | Ê£ÄÁ¥¢Â¢ûÂº∫ÁîüÊàêÊäÄÊúØÔºåÁªìÂêàÂ§ñÈÉ®Áü•ËØÜÂ∫ì | @EE | ‚úÖ | ‚úÖ | ‚è≥ |\n\n### Á¨¨ÂõõÈÉ®ÂàÜÔºöÁîü‰∫ßËÆæËÆ°Ê®°Âºè | Part Four: Production Patterns (114 È°µ)\n\n| Á´†ËäÇ | ËÆæËÆ°Ê®°ÂºèÊ¶ÇËø∞ | Ë¥üË¥£‰∫∫ | AI ÁøªËØë | ‰∫∫Â∑•ËØÑÂÆ° | ‰∫§ÂèâËØÑÂÆ° |\n|------|-------------|--------|---------|----------|----------|\n| [Á¨¨ 15 Á´†ÔºöÊô∫ËÉΩ‰ΩìÈó¥ÈÄö‰ø° (A2A)](21-Chapter-15-Inter-Agent-Communication.md) | Êô∫ËÉΩ‰ΩìÈÄö‰ø°ÂçèËÆÆÔºåÂÆûÁé∞Êô∫ËÉΩ‰ΩìÈó¥È´òÊïà‰∫§‰∫í | @ÊúµÊúµËÇ• | ‚úÖ | ‚ùå | ‚ùå |\n| [Á¨¨ 16 Á´†ÔºöËµÑÊ∫êÊÑüÁü•‰ºòÂåñ](22-Chapter-16-Resource-Aware-Optimization.md) | ËµÑÊ∫ê‰ºòÂåñÁÆ°ÁêÜÔºåÂπ≥Ë°°ÊÄßËÉΩ‰∏éÊàêÊú¨ | @IsaacZhaoo | ‚úÖ | ‚úÖ | ‚è≥ |\n| [Á¨¨ 17 Á´†ÔºöÊé®ÁêÜÊäÄÊúØ](23-Chapter-17-Reasoning-Techniques.md) | Â¢ûÂº∫Êé®ÁêÜËÉΩÂäõÔºåÊèêÂçáÂÜ≥Á≠ñË¥®Èáè | @Diqing | ‚ùå | ‚ùå | ‚ùå |\n| [Á¨¨ 18 Á´†ÔºöÊä§Ê†è/ÂÆâÂÖ®Ê®°Âºè](24-Chapter-18-Guardrails-Safety-Patterns.md) | ÂÆâÂÖ®‰øùÈöúÊú∫Âà∂ÔºåÈò≤Ê≠¢‰∏çÂΩìË°å‰∏∫ | @IsaacZhaoo | ‚è≥ | ‚ùå | ‚ùå |\n| [Á¨¨ 19 Á´†ÔºöËØÑ‰º∞‰∏éÁõëÊéß](25-Chapter-19-Evaluation-and-Monitoring.md) | ÊÄßËÉΩËØÑ‰º∞‰ΩìÁ≥ªÔºåÈáèÂåñÊô∫ËÉΩ‰ΩìË°®Áé∞ | @ÊúµÊúµËÇ• | ‚ùå | ‚ùå ",
      "default_branch": "main"
    },
    "fetched_at": "2025-10-26T02:28:10.524543"
  },
  {
    "basic_info": {
      "name": "Skill_Seekers",
      "full_name": "yusufkaraaslan/Skill_Seekers",
      "owner": "yusufkaraaslan",
      "description": "Single powerful tool to convert ANY documentation website into a Claude skill",
      "url": "https://github.com/yusufkaraaslan/Skill_Seekers",
      "clone_url": "https://github.com/yusufkaraaslan/Skill_Seekers.git",
      "ssh_url": "git@github.com:yusufkaraaslan/Skill_Seekers.git",
      "homepage": "",
      "created_at": "2025-10-17T14:43:48Z",
      "updated_at": "2025-10-26T02:23:23Z",
      "pushed_at": "2025-10-25T11:54:37Z"
    },
    "stats": {
      "stars": 1818,
      "forks": 175,
      "watchers": 1818,
      "open_issues": 134,
      "size": 584
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 402689,
        "Shell": 7440
      },
      "license": "MIT License",
      "topics": [
        "ai-tools",
        "automation",
        "claude-ai",
        "claude-skills",
        "documentation",
        "documentation-generator",
        "mcp",
        "mcp-server",
        "python",
        "web-scraping"
      ]
    },
    "content": {
      "readme": "[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/yusufkaraaslan-skill-seekers-badge.png)](https://mseep.ai/app/yusufkaraaslan-skill-seekers)\n\n# Skill Seeker\n\n[![Version](https://img.shields.io/badge/version-1.2.0-blue.svg)](https://github.com/yusufkaraaslan/Skill_Seekers/releases/tag/v1.2.0)\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)\n[![MCP Integration](https://img.shields.io/badge/MCP-Integrated-blue.svg)](https://modelcontextprotocol.io)\n[![Tested](https://img.shields.io/badge/Tests-207%20Passing-brightgreen.svg)](tests/)\n[![Project Board](https://img.shields.io/badge/Project-Board-purple.svg)](https://github.com/users/yusufkaraaslan/projects/2)\n\n**Automatically convert any documentation website into a Claude AI skill in minutes.**\n\n> üìã **[View Development Roadmap & Tasks](https://github.com/users/yusufkaraaslan/projects/2)** - 134 tasks across 10 categories, pick any to contribute!\n\n## What is Skill Seeker?\n\nSkill Seeker is an automated tool that transforms any documentation website into a production-ready [Claude AI skill](https://claude.ai). Instead of manually reading and summarizing documentation, Skill Seeker:\n\n1. **Scrapes** documentation websites automatically\n2. **Organizes** content into categorized reference files\n3. **Enhances** with AI to extract best examples and key concepts\n4. **Packages** everything into an uploadable `.zip` file for Claude\n\n**Result:** Get comprehensive Claude skills for any framework, API, or tool in 20-40 minutes instead of hours of manual work.\n\n## Why Use This?\n\n- üéØ **For Developers**: Quickly create Claude skills for your favorite frameworks (React, Vue, Django, etc.)\n- üéÆ **For Game Devs**: Generate skills for game engines (Godot, Unity documentation, etc.)\n- üîß **For Teams**: Create internal documentation skills for your company's APIs\n- üìö **For Learners**: Build comprehensive reference skills for technologies you're learning\n\n## Key Features\n\n### üåê Documentation Scraping\n- ‚úÖ **llms.txt Support** - Automatically detects and uses LLM-ready documentation files (10x faster)\n- ‚úÖ **Universal Scraper** - Works with ANY documentation website\n- ‚úÖ **Smart Categorization** - Automatically organizes content by topic\n- ‚úÖ **Code Language Detection** - Recognizes Python, JavaScript, C++, GDScript, etc.\n- ‚úÖ **8 Ready-to-Use Presets** - Godot, React, Vue, Django, FastAPI, and more\n\n### üìÑ PDF Support (**v1.2.0**)\n- ‚úÖ **Basic PDF Extraction** - Extract text, code, and images from PDF files\n- ‚úÖ **OCR for Scanned PDFs** - Extract text from scanned documents\n- ‚úÖ **Password-Protected PDFs** - Handle encrypted PDFs\n- ‚úÖ **Table Extraction** - Extract complex tables from PDFs\n- ‚úÖ **Parallel Processing** - 3x faster for large PDFs\n- ‚úÖ **Intelligent Caching** - 50% faster on re-runs\n\n### ü§ñ AI & Enhancement\n- ‚úÖ **AI-Powered Enhancement** - Transforms basic templates into comprehensive guides\n- ‚úÖ **No API Costs** - FREE local enhancement using Claude Code Max\n- ‚úÖ **MCP Server for Claude Code** - Use directly from Claude Code with natural language\n\n### ‚ö° Performance & Scale\n- ‚úÖ **Large Documentation Support** - Handle 10K-40K+ page docs with intelligent splitting\n- ‚úÖ **Router/Hub Skills** - Intelligent routing to specialized sub-skills\n- ‚úÖ **Parallel Scraping** - Process multiple skills simultaneously\n- ‚úÖ **Checkpoint/Resume** - Never lose progress on long scrapes\n- ‚úÖ **Caching System** - Scrape once, rebuild instantly\n\n### ‚úÖ Quality Assurance\n- ‚úÖ **Fully Tested** - 207 tests with 100% pass rate\n\n## Quick Example\n\n### Option 1: Use from Claude Code (Recommended)\n\n```bash\n# One-time setup (5 minutes)\n./setup_mcp.sh\n\n# Then in Claude Code, just ask:\n\"Generate a React skill from https://react.dev/\"\n\"Scrape PDF at docs/manual.pdf and create skill\"\n```\n\n**Time:** Automated | **Quality:** Production-ready | **Cost:** Free\n\n### Option 2: Use CLI Directly (HTML Docs)\n\n```bash\n# Install dependencies (2 pip packages)\npip3 install requests beautifulsoup4\n\n# Generate a React skill in one command\npython3 cli/doc_scraper.py --config configs/react.json --enhance-local\n\n# Upload output/react.zip to Claude - Done!\n```\n\n**Time:** ~25 minutes | **Quality:** Production-ready | **Cost:** Free\n\n### Option 3: Use CLI for PDF Documentation\n\n```bash\n# Install PDF support\npip3 install PyMuPDF\n\n# Basic PDF extraction\npython3 cli/pdf_scraper.py --pdf docs/manual.pdf --name myskill\n\n# Advanced features\npython3 cli/pdf_scraper.py --pdf docs/manual.pdf --name myskill \\\n    --extract-tables \\        # Extract tables\n    --parallel \\              # Fast parallel processing\n    --workers 8               # Use 8 CPU cores\n\n# Scanned PDFs (requires: pip install pytesseract Pillow)\npython3 cli/pdf_scraper.py --pdf docs/scanned.pdf --name myskill --ocr\n\n# Password-protected PDFs\npython3 cli/pdf_scraper.py --pdf docs/encrypted.pdf --name myskill --password",
      "default_branch": "development"
    },
    "fetched_at": "2025-10-26T02:28:11.671428"
  },
  {
    "basic_info": {
      "name": "RAE",
      "full_name": "bytetriper/RAE",
      "owner": "bytetriper",
      "description": "Official PyTorch Implementation of \"Diffusion Transformers with Representation Autoencoders\"",
      "url": "https://github.com/bytetriper/RAE",
      "clone_url": "https://github.com/bytetriper/RAE.git",
      "ssh_url": "git@github.com:bytetriper/RAE.git",
      "homepage": null,
      "created_at": "2025-09-28T07:29:37Z",
      "updated_at": "2025-10-26T00:48:45Z",
      "pushed_at": "2025-10-15T09:15:27Z"
    },
    "stats": {
      "stars": 1352,
      "forks": 32,
      "watchers": 1352,
      "open_issues": 3,
      "size": 282
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 245415
      },
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "## Diffusion Transformers with Representation Autoencoders (RAE)<br><sub>Official PyTorch Implementation</sub>\n\n### [Paper](https://arxiv.org/abs/2510.11690) | [Project Page](https://rae-dit.github.io/) \n\n\nThis repository contains **PyTorch/GPU** and **TorchXLA/TPU** implementations of our paper: \nDiffusion Transformers with Representation Autoencoders. For JAX/TPU implementation, please refer to [diffuse_nnx](https://github.com/willisma/diffuse_nnx)\n\n> [**Diffusion Transformers with Representation Autoencoders**](https://arxiv.org/abs/2510.11690)<br>\n> [Boyang Zheng](https://bytetriper.github.io/), [Nanye Ma](https://willisma.github.io), [Shengbang Tong](https://tsb0601.github.io/),  [Saining Xie](https://www.sainingxie.com)\n> <br>New York University<br>\n\nWe present Representation Autoencoders (RAE), a class of autoencoders that utilize  pretrained, frozen representation encoders such as [DINOv2](https://arxiv.org/abs/2304.07193) and [SigLIP2](https://arxiv.org/abs/2502.14786) as encoders with trained ViT decoders. RAE can be used in a two-stage training pipeline for high-fidelity image synthesis, where a Stage 2 diffusion model is trained on the latent space of a pretrained RAE to generate images.\n\nThis repository contains:\n\nPyTorch/GPU:\n* A PyTorch implementation of RAE and pretrained weights.\n* A PyTorch implementation of LightningDiT, DiT<sup>DH</sup> and pretrained weights.\n* Training and sampling scripts for the two-stage RAE+DiT pipeline.\n\nTorchXLA/TPU:\n* A TPU implementation of RAE and pretrained weights.\n* Sampling of RAE and DiT<sup>DH</sup> on TPU.\n\n## Environment\n\n### Dependency Setup\n1. Create environment and install via `uv`:\n   ```bash\n   conda create -n rae python=3.10 -y\n   conda activate rae\n   pip install uv\n   \n   # Install PyTorch 2.2.0 with CUDA 12.1\n   uv pip install torch==2.2.0 torchvision==0.17.0 torchaudio --index-url https://download.pytorch.org/whl/cu121\n   \n   # Install other dependencies\n   uv pip install timm==0.9.16 accelerate==0.23.0 torchdiffeq==0.2.5 wandb\n   uv pip install \"numpy<2\" transformers einops omegaconf\n   ```\n\n## Data & Model Preparation\n\n### Download Pre-trained Models\n\nWe release three kind of models: RAE decoders, DiT<sup>DH</sup> diffusion transformers and stats for latent normalization. To download all models at once:\n\n\n```bash\n\ncd RAE\npip install huggingface_hub\nhf download nyu-visionx/RAE-collections \\\n  --local-dir models \n```\n\n\nTo download specific models, run:\n```bash\nhf download nyu-visionx/RAE-collections \\\n  <remote_model_path> \\\n  --local-dir models \n```\n\n### Prepare Dataset\n\n1. Download ImageNet-1k.\n2. Point Stage 1 and Stage 2 scripts to the training split via `--data-path`.\n\n\n## Config-based Initialization\n\nAll training and sampling entrypoints are driven by OmegaConf YAML files. A\nsingle config describes the Stage 1 autoencoder, the Stage 2 diffusion model,\nand the solver used during training or inference. A minimal example looks like:\n\n```yaml\nstage_1:\n   target: stage1.RAE\n   params: { ... }\n   ckpt: <path_to_ckpt>  \n\nstage_2:\n   target: stage2.models.DDT.DiTwDDTHead\n   params: { ... }\n   ckpt: <path_to_ckpt>  \n\ntransport:\n   params:\n      path_type: Linear\n      prediction: velocity\n      ...\nsampler:\n   mode: ODE\n   params:\n      num_steps: 50\n      ...\nguidance:\n   method: cfg/autoguidance\n   scale: 1.0\n   ...\nmisc:\n   latent_size: [768, 16, 16]\n   num_classes: 1000\ntraining:\n   ...\n```\n\n- `stage_1` instantiates the frozen encoder and trainable decoder. For Stage 1\n  training you can point to an existing checkpoint via `stage_1.ckpt` or start\n  from `pretrained_decoder_path`.\n- `stage_2` defines the diffusion transformer. During sampling you must provide\n  `ckpt`; during training you typically omit it so weights initialise randomly.\n- `transport`, `sampler`, and `guidance` select the forward/backward SDE/ODE\n  integrator and optional classifier-free or autoguidance schedule.\n- `misc` collects shapes, class counts, and scaling constants used by both\n  stages.\n- `training` contains defaults that the training scripts consume (epochs,\n  learning rate, EMA decay, gradient accumulation, etc.).\n\nStage 1 training configs additionally include a top-level `gan` block that\nconfigures the discriminator architecture and the LPIPS/GAN loss schedule.\n\n\n### Provided Configs:\n\n#### Stage1\n\nWe release decoders for DINOv2-B, SigLIP-B, MAE-B, at `configs/stage1/pretrained/`.\n\nThere is also a training script for training a ViT-XL decoder on DINOv2-B: `configs/stage1/training/DINOv2-B_decXL.yaml`\n\n#### Stage2\n\nWe release our best model, DiT<sup>DH</sup>-XL and it's guidance model on both $256\\times 256$ and $512\\times 512$, at `configs/stage2/sampling/`.\n\nWe also provide training configs for DiT<sup>DH</sup> at `configs/stage2/training/`.\n\n## Stage 1: Representation Autoencoder\n\n### Train the decoder\n\n`src/train_stage1.py` fine-tunes the ViT decoder while keeping the\nrepresentation encoder frozen. Launch it with PyTorch DDP (single or multi-GPU):\n\n```bash\ntor",
      "default_branch": "main"
    },
    "fetched_at": "2025-10-26T02:28:12.823236"
  },
  {
    "basic_info": {
      "name": "Paper2Video",
      "full_name": "showlab/Paper2Video",
      "owner": "showlab",
      "description": "Automatic Video Generation from Scientific Papers",
      "url": "https://github.com/showlab/Paper2Video",
      "clone_url": "https://github.com/showlab/Paper2Video.git",
      "ssh_url": "git@github.com:showlab/Paper2Video.git",
      "homepage": "https://showlab.github.io/Paper2Video/",
      "created_at": "2025-10-03T08:50:16Z",
      "updated_at": "2025-10-25T18:01:32Z",
      "pushed_at": "2025-10-20T12:20:48Z"
    },
    "stats": {
      "stars": 1251,
      "forks": 160,
      "watchers": 1251,
      "open_issues": 1,
      "size": 392267
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 931762,
        "TeX": 250773,
        "BibTeX Style": 26973,
        "Shell": 5155
      },
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "# Paper2Video\n\n<p align=\"right\">\n  <b>English</b> | <a href=\"./README-CN.md\">ÁÆÄ‰Ωì‰∏≠Êñá</a>\n</p>\n\n\n<p align=\"center\">\n  <b>Paper2Video: Automatic Video Generation from Scientific Papers</b>\n<br>\n‰ªéÂ≠¶ÊúØËÆ∫ÊñáËá™Âä®ÁîüÊàêÊºîËÆ≤ËßÜÈ¢ë\n</p>\n\n<p align=\"center\">\n  <a href=\"https://zeyu-zhu.github.io/webpage/\">Zeyu Zhu*</a>,\n  <a href=\"https://qhlin.me/\">Kevin Qinghong Lin*</a>,\n  <a href=\"https://scholar.google.com/citations?user=h1-3lSoAAAAJ&hl=en\">Mike Zheng Shou</a> <br>\n  Show Lab, National University of Singapore\n</p>\n\n\n<p align=\"center\">\n¬† <a href=\"https://arxiv.org/abs/2510.05096\">üìÑ Paper</a> &nbsp; | &nbsp;\n  <a href=\"https://huggingface.co/papers/2510.05096\">ü§ó Daily Paper</a> &nbsp; | &nbsp;\n¬† <a href=\"https://huggingface.co/datasets/ZaynZhu/Paper2Video\">üìä Dataset</a> &nbsp; | &nbsp;\n¬† <a href=\"https://showlab.github.io/Paper2Video/\">üåê Project Website</a> &nbsp; | &nbsp;\n¬† <a href=\"https://x.com/KevinQHLin/status/1976105129146257542\">üí¨ X (Twitter)</a>\n</p>\n\n- **Input:** a paper ‚ûï an image ‚ûï an audio\n  \n| Paper | Image | Audio |\n|--------|--------|--------|\n| <img src=\"https://github.com/showlab/Paper2Video/blob/page/assets/hinton/paper.png\" width=\"180\"/><br>[üîó Paper link](https://arxiv.org/pdf/1509.01626) | <img src=\"https://github.com/showlab/Paper2Video/blob/page/assets/hinton/hinton_head.jpeg\" width=\"180\"/> <br>Hinton's photo| <img src=\"assets/sound.png\" width=\"180\"/><br>[üîó Audio sample](https://github.com/showlab/Paper2Video/blob/page/assets/hinton/ref_audio_10.wav) |\n\n\n- **Output:** a presentation video\n\n\n\nhttps://github.com/user-attachments/assets/39221a9a-48cb-4e20-9d1c-080a5d8379c4\n\n\n\n\nCheck out more examples at [üåê project page](https://showlab.github.io/Paper2Video/).\n\n## üî• Update\n**Any contributions are welcome!**\n- [x] [2025.10.15] We update a new version without talking-head for fast generation!\n- [x] [2025.10.11] Our work receives attention on [YC Hacker News](https://news.ycombinator.com/item?id=45553701).\n- [x] [2025.10.9] Thanks AK for sharing our work on [Twitter](https://x.com/_akhaliq/status/1976099830004072849)!\n- [x] [2025.10.9] Our work is reported by [Medium](https://medium.com/@dataism/how-ai-learned-to-make-scientific-videos-from-slides-to-a-talking-head-0d807e491b27).\n- [x] [2025.10.8] Check out our demo video below!\n- [x] [2025.10.7] We release the [arxiv paper](https://arxiv.org/abs/2510.05096).\n- [x] [2025.10.6] We release the [code](https://github.com/showlab/Paper2Video) and [dataset](https://huggingface.co/datasets/ZaynZhu/Paper2Video).\n- [x] [2025.9.28] Paper2Video has been accepted to the **Scaling Environments for Agents Workshop([SEA](https://sea-workshop.github.io/)) at NeurIPS 2025**.\n\n\nhttps://github.com/user-attachments/assets/a655e3c7-9d76-4c48-b946-1068fdb6cdd9\n\n\n\n\n---\n\n### Table of Contents\n- [üåü Overview](#-overview)\n- [üöÄ Quick Start: PaperTalker](#-try-papertalker-for-your-paper-)\n  - [1. Requirements](#1-requirements)\n  - [2. Configure LLMs](#2-configure-llms)\n  - [3. Inference](#3-inference)\n- [üìä Evaluation: Paper2Video](#-evaluation-paper2video)\n- [üòº Fun: Paper2Video for Paper2Video](#-fun-paper2video-for-paper2video)\n- [üôè Acknowledgements](#-acknowledgements)\n- [üìå Citation](#-citation)\n\n---\n\n## üåü Overview\n<p align=\"center\">\n  <img src=\"assets/teaser.png\" alt=\"Overview\" width=\"100%\">\n</p>\n\nThis work solves two core problems for academic presentations:\n\n- **Left: How to create a presentation video from a paper?**  \n  *PaperTalker* ‚Äî an agent that integrates **slides**, **subtitling**, **cursor grounding**, **speech synthesis**, and **talking-head video rendering**.\n\n- **Right: How to evaluate a presentation video?**  \n  *Paper2Video* ‚Äî a benchmark with well-designed metrics to evaluate presentation quality.\n\n\n---\n\n## üöÄ Try PaperTalker for your Paper!\n<p align=\"center\">\n  <img src=\"assets/method.png\" alt=\"Approach\" width=\"100%\">\n</p>\n\n### 1. Requirements\nPrepare the environment:\n```bash\ncd src\nconda create -n p2v python=3.10\nconda activate p2v\npip install -r requirements.txt\nconda install -c conda-forge tectonic\n```\n**[Optional] [Skip](#2-configure-llms) this part if you do not need a human presenter.**\n\nDownload the dependent code and follow the instructions in **[Hallo2](https://github.com/fudan-generative-vision/hallo2)** to download the model weight.\n```bash\ngit clone https://github.com/fudan-generative-vision/hallo2.git\n```\nYou need to **prepare the environment separately for talking-head generation** to potential avoide package conflicts, please refer to  <a href=\"git clone https://github.com/fudan-generative-vision/hallo2.git\">Hallo2</a>. After installing, use `which python` to get the python environment path.\n```bash\ncd hallo2\nconda create -n hallo python=3.10\nconda activate hallo\npip install -r requirements.txt\n```\n\n### 2. Configure LLMs\nExport your **API credentials**:\n```bash\nexport GEMINI_API_KEY=\"your_gemini_key_here\"\nexport OPENAI_API_KEY=\"your_openai_key_here\"\n```\nThe best practice is to use **GPT4.1** or **Gemini2.5-Pro** for both LLM and VLMs. We also support locally depl",
      "default_branch": "main"
    },
    "fetched_at": "2025-10-26T02:28:13.998554"
  },
  {
    "basic_info": {
      "name": "kimi-cli",
      "full_name": "MoonshotAI/kimi-cli",
      "owner": "MoonshotAI",
      "description": "Kimi CLI is your next CLI agent.",
      "url": "https://github.com/MoonshotAI/kimi-cli",
      "clone_url": "https://github.com/MoonshotAI/kimi-cli.git",
      "ssh_url": "git@github.com:MoonshotAI/kimi-cli.git",
      "homepage": null,
      "created_at": "2025-10-15T12:58:03Z",
      "updated_at": "2025-10-26T02:26:30Z",
      "pushed_at": "2025-10-25T18:04:10Z"
    },
    "stats": {
      "stars": 1084,
      "forks": 66,
      "watchers": 1084,
      "open_issues": 32,
      "size": 1176
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 430313,
        "Makefile": 2817
      },
      "license": "Apache License 2.0",
      "topics": []
    },
    "content": {
      "readme": "# Kimi CLI\n\n[![Commit Activity](https://img.shields.io/github/commit-activity/w/MoonshotAI/kimi-cli)](https://github.com/MoonshotAI/kimi-cli/graphs/commit-activity)\n[![Checks](https://img.shields.io/github/check-runs/MoonshotAI/kimi-cli/main)](https://github.com/MoonshotAI/kimi-cli/actions)\n[![Version](https://img.shields.io/pypi/v/kimi-cli)](https://pypi.org/project/kimi-cli/)\n[![Downloads](https://img.shields.io/pypi/dw/kimi-cli)](https://pypistats.org/packages/kimi-cli)\n\n[‰∏≠Êñá](https://www.kimi.com/coding/docs/kimi-cli.html)\n\nKimi CLI is a new CLI agent that can help you with your software development tasks and terminal operations.\n\n> [!IMPORTANT]\n> Kimi CLI is currently in technical preview.\n\n## Key features\n\n- Shell-like UI and raw shell command execution\n- Zsh integration\n- [Agent Client Protocol] support\n- MCP support\n- And more to come...\n\n[Agent Client Protocol]: https://github.com/agentclientprotocol/agent-client-protocol\n\n## Installation\n\n> [!IMPORTANT]\n> Kimi CLI currently only supports macOS and Linux. Windows support is coming soon.\n\nKimi CLI is published as a Python package on PyPI. We highly recommend installing it with [uv](https://docs.astral.sh/uv/). If you have not installed uv yet, please follow the instructions [here](https://docs.astral.sh/uv/getting-started/installation/) to install it first.\n\nOnce uv is installed, you can install Kimi CLI with:\n\n```sh\nuv tool install --python 3.13 kimi-cli\n```\n\nRun `kimi --help` to check if Kimi CLI is installed successfully.\n\n> [!IMPORTANT]\n> Due to the security checks on macOS, the first time you run `kimi` command may take 10 seconds or more depending on your system environment.\n\n## Upgrading\n\nUpgrade Kimi CLI to the latest version with:\n\n```sh\nuv tool upgrade kimi-cli --no-cache\n```\n\n## Usage\n\nRun `kimi` command in the directory you want to work on, then send `/setup` to setup Kimi CLI:\n\n![](./docs/images/setup.png)\n\nAfter setup, Kimi CLI will be ready to use. You can send `/help` to get more information.\n\n## Features\n\n### Shell mode\n\nKimi CLI is not only a coding agent, but also a shell. You can switch the mode by pressing `Ctrl-K`. In shell mode, you can directly run shell commands without leaving Kimi CLI.\n\n> [!NOTE]\n> Built-in shell commands like `cd` are not supported yet.\n\n### Zsh integration\n\nYou can use Kimi CLI together with Zsh, to empower your shell experience with AI agent capabilities.\n\nInstall the [zsh-kimi-cli](https://github.com/MoonshotAI/zsh-kimi-cli) plugin via:\n\n```sh\ngit clone https://github.com/MoonshotAI/zsh-kimi-cli.git \\\n  ${ZSH_CUSTOM:-~/.oh-my-zsh/custom}/plugins/kimi-cli\n```\n\n> [!NOTE]\n> If you are using a plugin manager other than Oh My Zsh, you may need to refer to the plugin's README for installation instructions.\n\nThen add `kimi-cli` to your Zsh plugin list in `~/.zshrc`:\n\n```sh\nplugins=(... kimi-cli)\n```\n\nAfter restarting Zsh, you can switch to agent mode by pressing `Ctrl-K`.\n\n### ACP support\n\nKimi CLI supports [Agent Client Protocol] out of the box. You can use it together with any ACP-compatible editor or IDE.\n\nFor example, to use Kimi CLI with [Zed](https://zed.dev/), add the following configuration to your `~/.config/zed/settings.json`:\n\n```json\n{\n  \"agent_servers\": {\n    \"Kimi CLI\": {\n      \"command\": \"kimi\",\n      \"args\": [\"--acp\"],\n      \"env\": {}\n    }\n  }\n}\n```\n\nThen you can create Kimi CLI threads in Zed's agent panel.\n\n### Using MCP tools\n\nKimi CLI supports the well-established MCP config convention. For example:\n\n```json\n{\n  \"mcpServers\": {\n    \"context7\": {\n      \"url\": \"https://mcp.context7.com/mcp\",\n      \"headers\": {\n        \"CONTEXT7_API_KEY\": \"YOUR_API_KEY\"\n      }\n    },\n    \"chrome-devtools\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"chrome-devtools-mcp@latest\"]\n    }\n  }\n}\n```\n\nRun `kimi` with `--mcp-config-file` option to connect to the specified MCP servers:\n\n```sh\nkimi --mcp-config-file /path/to/mcp.json\n```\n\n## Development\n\nTo develop Kimi CLI, run:\n\n```sh\ngit clone https://github.com/MoonshotAI/kimi-cli.git\ncd kimi-cli\n\nmake prepare  # prepare the development environment\n```\n\nThen you can start working on Kimi CLI.\n\nRefer to the following commands after you make changes:\n\n```sh\nuv run kimi  # run Kimi CLI\n\nmake format  # format code\nmake check  # run linting and type checking\nmake test  # run tests\nmake help  # show all make targets\n```\n\n## Contributing\n\nWe welcome contributions to Kimi CLI! Please refer to [CONTRIBUTING.md](./CONTRIBUTING.md) for more information.\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-10-26T02:28:15.156043"
  },
  {
    "basic_info": {
      "name": "DeepSeek-V3.2-Exp",
      "full_name": "deepseek-ai/DeepSeek-V3.2-Exp",
      "owner": "deepseek-ai",
      "description": null,
      "url": "https://github.com/deepseek-ai/DeepSeek-V3.2-Exp",
      "clone_url": "https://github.com/deepseek-ai/DeepSeek-V3.2-Exp.git",
      "ssh_url": "git@github.com:deepseek-ai/DeepSeek-V3.2-Exp.git",
      "homepage": null,
      "created_at": "2025-09-29T03:25:13Z",
      "updated_at": "2025-10-25T08:47:34Z",
      "pushed_at": "2025-10-02T02:49:20Z"
    },
    "stats": {
      "stars": 926,
      "forks": 59,
      "watchers": 926,
      "open_issues": 14,
      "size": 1077
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 59696
      },
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "# DeepSeek-V3.2-Exp\n\n<!-- markdownlint-disable first-line-h1 -->\n<!-- markdownlint-disable html -->\n<!-- markdownlint-disable no-duplicate-header -->\n\n<div align=\"center\">\n  <img src=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/logo.svg?raw=true\" width=\"60%\" alt=\"DeepSeek-V3\" />\n</div>\n<hr>\n<div align=\"center\" style=\"line-height: 1;\">\n  <a href=\"https://www.deepseek.com/\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Homepage\" src=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/badge.svg?raw=true\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://chat.deepseek.com/\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Chat\" src=\"https://img.shields.io/badge/ü§ñ%20Chat-DeepSeek%20V3-536af5?color=536af5&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://huggingface.co/deepseek-ai\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Hugging Face\" src=\"https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-DeepSeek%20AI-ffc107?color=ffc107&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n</div>\n<div align=\"center\" style=\"line-height: 1;\">\n  <a href=\"https://discord.gg/Tc7c45Zzu5\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Discord\" src=\"https://img.shields.io/badge/Discord-DeepSeek%20AI-7289da?logo=discord&logoColor=white&color=7289da\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/qr.jpeg?raw=true\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Wechat\" src=\"https://img.shields.io/badge/WeChat-DeepSeek%20AI-brightgreen?logo=wechat&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://twitter.com/deepseek_ai\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Twitter Follow\" src=\"https://img.shields.io/badge/Twitter-deepseek_ai-white?logo=x&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n</div>\n<div align=\"center\" style=\"line-height: 1;\">\n  <a href=\"LICENSE\" style=\"margin: 2px;\">\n    <img alt=\"License\" src=\"https://img.shields.io/badge/License-MIT-f5de53?&color=f5de53\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n</div>\n\n## Introduction\n\n\nWe are excited to announce the official release of DeepSeek-V3.2-Exp, an experimental version of our model. As an intermediate step toward our next-generation architecture, V3.2-Exp builds upon V3.1-Terminus by introducing DeepSeek Sparse Attention‚Äîa sparse attention mechanism designed to explore and validate optimizations for training and inference efficiency in long-context scenarios.\n\nThis experimental release represents our ongoing research into more efficient transformer architectures, particularly focusing on improving computational efficiency when processing extended text sequences.\n\n<div align=\"center\">\n <img src=\"cost.jpg\" >\n</div>\n\n- DeepSeek Sparse Attention (DSA) achieves fine-grained sparse attention for the first time, delivering substantial improvements in long-context training and inference efficiency while maintaining virtually identical model output quality.\n\n\n- To rigorously evaluate the impact of introducing sparse attention, we deliberately aligned the training configurations of DeepSeek-V3.2-Exp with V3.1-Terminus. Across public benchmarks in various domains, DeepSeek-V3.2-Exp demonstrates performance on par with V3.1-Terminus.\n\n\n| Benchmark | DeepSeek-V3.1-Terminus | DeepSeek-V3.2-Exp |\n| :--- | :---: | :---: |\n| **Reasoning Mode w/o Tool Use** | | |\n| MMLU-Pro | 85.0 | 85.0 |\n| GPQA-Diamond | 80.7 | 79.9 |\n| Humanity's Last Exam | 21.7 | 19.8 |\n| LiveCodeBench | 74.9 | 74.1 |\n| AIME 2025 | 88.4 | 89.3 |\n| HMMT 2025 | 86.1 | 83.6 |\n| Codeforces | 2046 | 2121 |\n| Aider-Polyglot | 76.1 | 74.5 |\n| **Agentic Tool Use** | | |\n| BrowseComp | 38.5 | 40.1 |\n| BrowseComp-zh | 45.0 | 47.9 |\n| SimpleQA | 96.8 | 97.1 |\n| SWE Verified | 68.4 | 67.8 |\n| SWE-bench Multilingual | 57.8 | 57.9 |\n| Terminal-bench | 36.7 | 37.7 |\n\n\n\n## Open-Source Kernels\n\nFor TileLang kernels with **better readability and research-purpose design**, please refer to [TileLang](https://github.com/tile-ai/tilelang/tree/main/examples/deepseek_v32).\n\nFor **high-performance CUDA kernels**, indexer logit kernels (including paged versions) are available in [DeepGEMM](https://github.com/deepseek-ai/DeepGEMM/pull/200). Sparse attention kernels are released in [FlashMLA](https://github.com/deepseek-ai/FlashMLA/pull/98).\n\n\n\n## How to Run Locally\n\n### HuggingFace\nWe provide an updated inference demo code in the [inference](https://huggingface.co/deepseek-ai/DeepSeek-V3.2-Exp/tree/main/inference) folder to help the community quickly get started with our model and understand its architectural details.\n\nFirst convert huggingface model weights to the the format required by our inference demo. Set `MP` to match your available GPU count:\n```bash\ncd inference\nexpor",
      "default_branch": "main"
    },
    "fetched_at": "2025-10-26T02:28:16.314959"
  },
  {
    "basic_info": {
      "name": "AgentFlow",
      "full_name": "lupantech/AgentFlow",
      "owner": "lupantech",
      "description": "AgentFlow: In-the-Flow Agentic System Optimization",
      "url": "https://github.com/lupantech/AgentFlow",
      "clone_url": "https://github.com/lupantech/AgentFlow.git",
      "ssh_url": "git@github.com:lupantech/AgentFlow.git",
      "homepage": "https://agentflow.stanford.edu",
      "created_at": "2025-09-27T02:17:05Z",
      "updated_at": "2025-10-26T02:11:08Z",
      "pushed_at": "2025-10-17T04:50:45Z"
    },
    "stats": {
      "stars": 919,
      "forks": 102,
      "watchers": 919,
      "open_issues": 4,
      "size": 7031
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 563581,
        "Shell": 14322
      },
      "license": "MIT License",
      "topics": [
        "agentic-ai",
        "agentic-systems",
        "llms",
        "llms-reasoning",
        "multi-agent-systems",
        "reinforcement-learning",
        "tool-augmented"
      ]
    },
    "content": {
      "readme": "<a name=\"readme-top\"></a>\n\n<p align=\"center\">\n  <picture>\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"assets/img/logo.png\">\n    <img alt=\"AgentFlow\" src=\"assets/img/logo.png\" width=31%>\n  </picture>\n</p>\n\n<h3 align=\"center\">\nAgentFlow: In-the-Flow Agentic System Optimization\n</h3>\n\n\n<!--- BADGES: START --->\n<p align=\"center\">\n    <a href=\"https://arxiv.org/abs/2510.05592\"><img src=\"https://img.shields.io/badge/arXiv-2510.05592-B31B1B.svg?logo=arxiv\" alt=\"Arxiv\"></a>\n    <a href=\"https://huggingface.co/spaces/AgentFlow/agentflow\"><img src=\"https://img.shields.io/badge/Gradio-Demo-F97316.svg?logo=gradio\" alt=\"Gradio Demo\"></a>\n    <a href=\"https://huggingface.co/papers/2510.05592\"><img src=\"https://img.shields.io/badge/Huggingface-Paper-FFD21E.svg?logo=huggingface\" alt=\"Huggingface Paper\"></a>\n    <a href=\"https://huggingface.co/AgentFlow\"><img src=\"https://img.shields.io/badge/Huggingface-Model-FFD21E.svg?logo=huggingface\" alt=\"Huggingface Model\"></a>\n    <a href=\"https://agentflow.stanford.edu/\"><img src=\"https://img.shields.io/badge/Website-AgentFlow-E5426E?logo=kashflow\" alt=\"Website\"></a>\n    <a href=\"https://x.com/lupantech/status/1976016000345919803\"><img src=\"https://img.shields.io/badge/Coverage-AgentFlow-2176BC.svg?logo=x\" alt=\"X\"></a>\n    <a href=\"https://www.youtube.com/watch?v=kIQbCQIH1SI\"><img src=\"https://img.shields.io/badge/YouTube-Tutorial-FF0000?logo=youtube\" alt=\"Youtube\"></a>\n    <a href=\"https://join.slack.com/t/agentflow-co/shared_invite/zt-3f712xngl-LfxS4gmftAeKvcxR3nSkWQ\"><img src=\"https://img.shields.io/badge/Slack-AgentFlow-D41544.svg?logo=slack\" alt=\"Slack\"></a>\n    <a href=\"https://github.com/lupantech/AgentFlow/blob/main/assets/img/wechat_group.jpg\">\n  <img src=\"https://img.shields.io/badge/Wechat-AgentFlow-07C160.svg?logo=wechat\" alt=\"Wechat AgentFlow\">\n</a>\n  \n  </p>\n<!--- BADGES: END --->\n\n\n## üì£ News\n- **[2025.10.16]** üèÜ Our paper has been accepted by [**NeurIPS 2025 Efficient Reasoning Workshop**](https://efficient-reasoning.github.io/)!\n- **[2025.10.13]** üì∏ Excited to have a tutorial video for AgentFlow covered by Discover AI on **[YouTube](https://www.youtube.com/watch?v=kIQbCQIH1SI)**!\n- **[2025.10.10]** üöÄ Our X [post](https://x.com/lupantech/status/1976016000345919803) received **1K+ likes**! Feel free to check out the post and join the discussion! üí¨\n- **[2025.10.08]** üî• We are honored to be featured as ü§ó HuggingFace **[Daily Paper #2](https://huggingface.co/papers/2510.05592)**.\n\n## üåü Why AgentFlow?\nAgentFlow is a **trainable, tool-integrated agentic framework** designed to overcome the **scalability** and **generalization limits** of today‚Äôs tool-augmented reasoning approaches. \n\nUnlike prevailing approaches such as [Search-R1](https://github.com/PeterGriffinJin/Search-R1) which train a **single LLM** to interleave reasoning steps with tool calls, **AgentFlow** introduces a **modular agentic system** with four specialized modules: üß≠ **Planner**, üõ† **Executor**, ‚úÖ **Verifier**, and ‚úçÔ∏è **Generator**.\n\n![framework_overall](assets/img/framework.png)\n\nFor effective planning and tool use, the framework directly **optimizes planner agent within the system** in an **online fashion** using **Flow-based Group Refined Policy Optimization (Flow-GRPO)**, achieving superior performance across diverse domains with improved tool-calling reliability and long-horizon reasoning capabilities.\n\n![flow_grpo](assets/img/flow_grpo.png)\n\n## üì∫ YouTube Tutorial\nExcited to have a tutorial video for AgentFlow covered by [Discover AI](https://www.youtube.com/@code4AI) on YouTube!\n\n<!-- [![AgentFlow Tutorial](https://img.youtube.com/vi/kIQbCQIH1SI/0.jpg)](https://www.youtube.com/watch?v=kIQbCQIH1SI) -->\n\n<div align=\"center\">\n  <a href=\"https://www.youtube.com/watch?v=kIQbCQIH1SI\">\n    <img src=\"https://img.youtube.com/vi/kIQbCQIH1SI/maxresdefault.jpg\" alt=\"AgentFlow Tutorial\" width=\"100%\">\n  </a>\n</div>\n\n\n## üöÄ Key Features\n\n- üß© **Modular Agentic System** ‚Äì Four specialized agent modules (**Planner**, **Executor**, **Verifier**, **Generator**) that coordinate via evolving memory and integrated tools across multiple turns.  \n- üîó **Multi-Tool Integration** ‚Äì Seamlessly connect with diverse tool ecosystems, including `base_generator`, `python_coder`, `google_search`, `wikipedia_search`, `web_search`, and more.  \n- üéØ **Flow-GRPO Algorithm** ‚Äì Enables **in-the-flow agent optimization** for **long-horizon reasoning tasks** with sparse rewards.\n- üìà **Proven Results** ‚Äì **AgentFlow (7B Backbone)** beats top baselines on 10 benchmarks, with **+14.9% search**, **+14.0% agentic**, **+14.5% math**, **+4.1% science**, even outperforming ~200B-parameter **GPT-4o**.\n\n## üèÜ Experiments\n\n### üìä Main Results\n**AgentFlow (Qwen-2.5-7B-Instruct Backbone)** outperforms top baselines on 10 benchmarks:  \n- **+14.9%** on search  \n- **+14.0%** on agentic reasoning  \n- **+14.5%** on math  \n- **+4.1%** on science  \n\nüí° Even surpasses larger proprietary models like **GPT-4o (~200B)**.\n\n![main_table1](assets/img/maintabl",
      "default_branch": "main"
    },
    "fetched_at": "2025-10-26T02:28:17.467463"
  },
  {
    "basic_info": {
      "name": "DeepAnalyze",
      "full_name": "ruc-datalab/DeepAnalyze",
      "owner": "ruc-datalab",
      "description": "DeepAnalyze is the first agentic LLM for autonomous data science.",
      "url": "https://github.com/ruc-datalab/DeepAnalyze",
      "clone_url": "https://github.com/ruc-datalab/DeepAnalyze.git",
      "ssh_url": "git@github.com:ruc-datalab/DeepAnalyze.git",
      "homepage": "",
      "created_at": "2025-10-11T11:19:21Z",
      "updated_at": "2025-10-26T02:25:53Z",
      "pushed_at": "2025-10-25T03:17:18Z"
    },
    "stats": {
      "stars": 869,
      "forks": 99,
      "watchers": 869,
      "open_issues": 6,
      "size": 22819
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 5372510,
        "Shell": 220988,
        "Jupyter Notebook": 61761,
        "Jinja": 3956,
        "Dockerfile": 1056,
        "Makefile": 997,
        "Batchfile": 764,
        "Scheme": 403
      },
      "license": "MIT License",
      "topics": [
        "agent",
        "agentic",
        "agentic-ai",
        "ai",
        "ai-scientist",
        "chatbot",
        "chatgpt",
        "data",
        "data-analysis",
        "data-engineering",
        "data-science",
        "data-visualization",
        "database",
        "gpt",
        "llama",
        "llm",
        "qwen",
        "science",
        "structured-data",
        "vllm"
      ]
    },
    "content": {
      "readme": "<p align=\"center\" width=\"100%\">\n<img src=\"assets/logo.png\" alt=\"DeepAnalyze\" style=\"width: 60%; min-width: 300px; display: block; margin: auto;\">\n</p>\n\n# DeepAnalyze: Agentic Large Language Models for Autonomous Data Science\n[![arXiv](https://img.shields.io/badge/arXiv-2510.16872-b31b1b.svg?logo=arXiv)](https://arxiv.org/abs/2510.16872)\n[![homepage](https://img.shields.io/badge/%F0%9F%8C%90%20Homepage%20-DeepAnalyze%20Cases-blue.svg)](https://ruc-deepanalyze.github.io/)\n[![model](https://img.shields.io/badge/%F0%9F%A4%97%20Huggingface%20-DeepAnalyze--8B-orange.svg)](https://huggingface.co/RUC-DataLab/DeepAnalyze-8B)\n[![data](https://img.shields.io/badge/%F0%9F%93%9A%20Datasets%20-DataScience--Instruct--500K-darkgreen.svg)](https://huggingface.co/datasets/RUC-DataLab/DataScience-Instruct-500K)\n[![star](https://img.shields.io/github/stars/ruc-datalab/DeepAnalyze?style=social&label=Code+Stars)](https://github.com/ruc-datalab/DeepAnalyze)\n![Badge](https://hitscounter.dev/api/hit?url=https%3A%2F%2Fgithub.com%2Fruc-datalab%2FDeepAnalyze&label=Visitors&icon=graph-up&color=%23dc3545&message=&style=flat&tz=UTC)  [![wechat](https://img.shields.io/badge/WeChat-%E5%8A%A0%E5%85%A5DeepAnalyze%E4%BA%A4%E6%B5%81%E8%AE%A8%E8%AE%BA%E7%BE%A4-black?logo=wechat&logoColor=07C160)](./assets/wechat.jpg) \n\n[![twitter](https://img.shields.io/badge/@Brian%20Roemmele-gray?logo=x&logoColor=white&labelColor=black)](https://x.com/BrianRoemmele/status/1981015483823571352) [![twitter](https://img.shields.io/badge/@Dr%20Singularity-gray?logo=x&logoColor=white&labelColor=black)](https://x.com/Dr_Singularity/status/1981010771338498241) [![twitter](https://img.shields.io/badge/@Gorden%20Sun-gray?logo=x&logoColor=white&labelColor=black)](https://x.com/Gorden_Sun/status/1980573407386423408) [![twitter](https://img.shields.io/badge/@AIGCLINK-gray?logo=x&logoColor=white&labelColor=black)](https://x.com/aigclink/status/1980554517126246642) [![twitter](https://img.shields.io/badge/@Python%20Developer-gray?logo=x&logoColor=white&labelColor=black)](https://x.com/Python_Dv/status/1980667557318377871) [![twitter](https://img.shields.io/badge/@meng%20shao-gray?logo=x&logoColor=white&labelColor=black)](https://x.com/shao__meng/status/1980623242114314531) \n\n\n> **Authors**: **[Shaolei Zhang](https://zhangshaolei1998.github.io/), [Ju Fan*](http://iir.ruc.edu.cn/~fanj/), [Meihao Fan](https://scholar.google.com/citations?user=9RTm2qoAAAAJ), [Guoliang Li](https://dbgroup.cs.tsinghua.edu.cn/ligl/), [Xiaoyong Du](http://info.ruc.edu.cn/jsky/szdw/ajxjgcx/jsjkxyjsx1/js2/7374b0a3f58045fc9543703ccea2eb9c.htm)**\n>\n> Renmin University of China, Tsinghua University\n\n\n**DeepAnalyze** is the first agentic LLM for autonomous data science. It can autonomously complete a wide range of data-centric tasks without human intervention, supporting:\n- üõ† **Entire data science pipeline**: Automatically perform any data science tasks such as data preparation, analysis, modeling, visualization, and report generation.\n- üîç **Open-ended data research**: Conduct deep research on diverse data sources, including structured data (Databases, CSV, Excel), semi-structured data (JSON, XML, YAML), and unstructured data (TXT, Markdown), and finally produce analyst-grade research reports.\n- üìä **Fully open-source**: The [model](https://huggingface.co/RUC-DataLab/DeepAnalyze-8B), [code](https://github.com/ruc-datalab/DeepAnalyze), [training data](https://huggingface.co/datasets/RUC-DataLab/DataScience-Instruct-500K), and [demo](https://huggingface.co/RUC-DataLab/DeepAnalyze-8B) of DeepAnalyze are all open-sourced, allowing you to deploy or extend your own data analysis assistant.\n\n<p align=\"center\" width=\"100%\">\n<img src=\"./assets/deepanalyze.jpg\" alt=\"deepanalyze\" style=\"width: 70%; min-width: 300px; display: block; margin: auto;\">\n</p>\n\n\n> Welcome to ‚≠ê star DeepAnalyze. Any useful issues and pull requests will be included in contributors.\n\n\n## üñ• Demo\n\n\n<p align=\"center\" width=\"100%\">\nUpload the data, DeepAnalyze can perform data-oriented deep research üîç and any data-centric tasks üõ†\n</p>\n\nhttps://github.com/user-attachments/assets/04184975-7ee7-4ae0-8761-7a7550c5c8fe\n\n> [!TIP]\n>\n> Clone this repository to deploy DeepAnalyze locally as your data analyst, completing any data science tasks without any workflow or closed-source APIs.\n>\n> üî• The UI of the demo is an initial version. Welcome to further develop it, and we will include you as a contributor.\n\n\n- Clone this repo and download [DeepAnalyze-8B](https://huggingface.co/RUC-DataLab/DeepAnalyze-8B).\n- Run these scripts to launch the API and interface, and then interact through the browser (http://localhost:4000):\n    ```bash\n    cd demo/chat\n    npm install\n    cd ..\n    bash start.sh\n\n    # stop the api and interface\n    bash stop.sh\n    ```\n- If you want to deploy under a specific IP, please replace localhost with your IP address in [./demo/backend.py](./demo/backend.py) and [./demo/chat/lib/config.ts](./demo/chat/lib/config.ts)\n\n## üöÄ Quick Start\n\n#",
      "default_branch": "main"
    },
    "fetched_at": "2025-10-26T02:28:18.804755"
  },
  {
    "basic_info": {
      "name": "MimicKit",
      "full_name": "xbpeng/MimicKit",
      "owner": "xbpeng",
      "description": "Suite of motion imitation methods for training motion controllers.",
      "url": "https://github.com/xbpeng/MimicKit",
      "clone_url": "https://github.com/xbpeng/MimicKit.git",
      "ssh_url": "git@github.com:xbpeng/MimicKit.git",
      "homepage": "",
      "created_at": "2025-10-08T15:33:01Z",
      "updated_at": "2025-10-25T23:08:11Z",
      "pushed_at": "2025-10-23T15:27:22Z"
    },
    "stats": {
      "stars": 802,
      "forks": 74,
      "watchers": 802,
      "open_issues": 2,
      "size": 10369
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 349330
      },
      "license": "BSD 3-Clause \"New\" or \"Revised\" License",
      "topics": []
    },
    "content": {
      "readme": "# MimicKit\n\n\n![Teaser](images/MimicKit_teaser.gif)\n\nThis framework provides a suite of motion imitation methods for training motion controllers. A more detailed overview of MimicKit is available in the [Starter Guide](https://arxiv.org/abs/2510.13794). This codebase includes implementations of:\n- [DeepMimic](https://xbpeng.github.io/projects/DeepMimic/index.html)\n- [AMP](https://xbpeng.github.io/projects/AMP/index.html)\n- [ASE](https://xbpeng.github.io/projects/ASE/index.html)\n- [ADD](https://xbpeng.github.io/projects/ADD/index.html)\n\nWe also include the following RL algorithms:\n- [PPO](https://arxiv.org/abs/1707.06347)\n- [AWR](https://xbpeng.github.io/projects/AWR/index.html)\n\n---\n\n## Installation\n\nInstall IsaacGym: https://developer.nvidia.com/isaac-gym\n\nInstall requirements:\n```\npip install -r requirements.txt\n```\nDownload assets and motion data from [here](https://1sfu-my.sharepoint.com/:u:/g/personal/xbpeng_sfu_ca/EclKq9pwdOBAl-17SogfMW0Bved4sodZBQ_5eZCiz9O--w?e=bqXBaa), then extract the contents into [`data/`](data/).\n\n---\n\n## Training\n\nTo train a model, run the following command:\n```\npython mimickit/run.py --mode train --num_envs 4096 --env_config data/envs/deepmimic_humanoid_env.yaml --agent_config data/agents/deepmimic_humanoid_ppo_agent.yaml --visualize true --log_file output/log.txt --out_model_file output/model.pt\n```\n- `--mode` selects either `train` or `test` mode.\n- `--num_envs` specifies the number of parallel environments used for simulation.\n- `--env_config` specifies the configuration file for the environment.\n- `--agent_config` specifies configuration file for the agent.\n- `--visualize` enables visualization. Rendering should be disabled for faster training.\n- `--log_file` specifies the output log file, which will keep track of statistics during training.\n- `--out_model_file` specifies the output model file, which contains the model parameters.\n- `--logger` specifies the logger used to record training stats. The options are TensorBoard `tb` or `wandb`.\n\nInstead of specifying all arguments through the command line, arguments can also be loaded from an `arg_file`:\n```\npython mimickit/run.py --arg_file args/deepmimic_humanoid_ppo_args.txt --visualize true\n```\nThe arguments in `arg_file` are treated the same as command line arguments. Arguments for all algorithms are provided in [`args/`](args/).\n\n\n## Testing\n\nTo test a model, run the following command:\n```\npython mimickit/run.py --arg_file args/deepmimic_humanoid_ppo_args.txt --num_envs 4 --visualize true --mode test --model_file data/models/deepmimic_humanoid_spinkick_model.pt\n```\n- `--model_file` specifies the `.pt` file that contains the parameters of the trained model. Pretrained models are available in [`data/models/`](data/models/), and the corresponding training log files are available in [`data/logs/`](data/logs/).\n\n\n## Distributed Training\n\nTo use distributed training with multi-CPU or multi-GPU:\n```\npython mimickit/run.py --arg_file args/deepmimic_humanoid_ppo_args.txt --num_workers 2 --device cuda:0\n```\n- `--num_workers` specifies the number of worker processes used to parallelize training. \n- `--device` specifies the device used for training, which can be `cpu` or `cuda:0`. When training with multiple GPUs, the number of worker processes used to parallelize training must be less than or equal to the number of GPUs available on the system.\n\n## Visualizing Training Logs\n\nWhen using the TensorBoard logger during training, a TensorBoard `events` file will be saved the same output directory as the log file. The log can be viewed with:\n```\ntensorboard --logdir=output/ --port=6006 --samples_per_plugin scalars=999999\n```\nThe output log `.txt` file can also be plotted using the plotting script [`plot_log.py`](tools/plot_log/plot_log.py).\n\n---\n\n## Motion Data\nMotion data is stored in [`data/motions/`](data/motions/). The `motion_file` field in the environment configuration file can be used to specify the reference motion clip. In addition to imitating individual motion clips, `motion_file` can also specify a dataset file, located in [`data/datasets/`](data/datasets/), which will train a model to imitate a dataset containing multiple motion clips.\n\nThe `view_motion` environment can be used to visualize motion clips:\n```\npython mimickit/run.py --mode test --arg_file args/view_motion_humanoid_args.txt --visualize true\n```\n\nMotion clips are represented by the `Motion` class implemented in [`motion.py`](mimickit/anim/motion.py). Each motion clip is stored in a `.pkl` file. Each frame in a motion specifies the pose of the character according to\n```\n[root position (3D), root rotation (3D), joint rotations]\n```\nwhere 3D rotations are specified using 3D exponential maps. Joint rotations are recorded in the order that the joints are specified in the `.xml` file (i.e. depth-first traversal of the kinematic tree). For example, in the case of [`humanoid.xml`](data/assets/humanoid.xml), each frame is represented as\n```\n[root position (3D), root rotatio",
      "default_branch": "main"
    },
    "fetched_at": "2025-10-26T02:28:19.954276"
  },
  {
    "basic_info": {
      "name": "Code2Video",
      "full_name": "showlab/Code2Video",
      "owner": "showlab",
      "description": "Video generation via code",
      "url": "https://github.com/showlab/Code2Video",
      "clone_url": "https://github.com/showlab/Code2Video.git",
      "ssh_url": "git@github.com:showlab/Code2Video.git",
      "homepage": "https://showlab.github.io/Code2Video/",
      "created_at": "2025-09-29T08:15:44Z",
      "updated_at": "2025-10-25T18:19:42Z",
      "pushed_at": "2025-10-22T12:44:55Z"
    },
    "stats": {
      "stars": 770,
      "forks": 98,
      "watchers": 770,
      "open_issues": 0,
      "size": 130209
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 179924,
        "Shell": 2370
      },
      "license": "MIT License",
      "topics": [
        "coding",
        "education",
        "multi-agent",
        "video-generation"
      ]
    },
    "content": {
      "readme": "\n# Code2Video: Video Generation via Code\n\n<p align=\"right\">\n  <b>English</b> | <a href=\"./README.zh-CN.md\">ÁÆÄ‰Ωì‰∏≠Êñá</a>\n</p>\n\n<p align=\"center\">\n  <b>Code2Video: A Code-centric Paradigm for Educational Video Generation</b>\n<br>\n‰ª•‰ª£Á†Å‰∏∫‰∏≠ÂøÉÁöÑÊïôÂ≠¶ËßÜÈ¢ëÁîüÊàêÊñ∞ËåÉÂºè\n</p>\n<video src=\"assets/video.mp4\" width=\"600\" controls>\n  Your browser does not support the video tag.\n</video>\n\n\n\n\n\n\n<p align=\"center\">\n  <a href=\"https://scholar.google.com.hk/citations?user=9lIMS-EAAAAJ&hl=zh-CN&oi=sra\">Yanzhe Chen*</a>,\n  <a href=\"https://qhlin.me/\">Kevin Qinghong Lin*</a>,\n  <a href=\"https://scholar.google.com/citations?user=h1-3lSoAAAAJ&hl=en\">Mike Zheng Shou</a> <br>\n  Show Lab @ National University of Singapore\n</p>\n\n\n<p align=\"center\">\n¬† <a href=\"https://arxiv.org/abs/2510.01174\">üìÑ Paper</a> &nbsp; | &nbsp;\n¬† <a href=\"https://huggingface.co/papers/2510.01174\">ü§ó Daily Paper</a> &nbsp; | &nbsp;\n¬† <a href=\"https://huggingface.co/datasets/YanzheChen/MMMC\">ü§ó Dataset</a> &nbsp; | &nbsp;\n¬† <a href=\"https://showlab.github.io/Code2Video/\">üåê Project Website</a> &nbsp; | &nbsp;\n¬† <a href=\"https://x.com/KevinQHLin/status/1974199353695941114\">üí¨ X (Twitter)</a>\n</p>\n\nhttps://github.com/user-attachments/assets/d906423f-734a-41c9-b102-b113ad3b3c25\n\n\n\n<!-- <p align=\"center\">\n<table>\n  <thead>\n    <tr>\n      <th style=\"text-align: center;\">Learning Topic</th>\n      <th style=\"text-align: center;\">Veo3</th>\n      <th style=\"text-align: center;\">Wan2.2</th>\n      <th style=\"text-align: center;\">Code2Video (Ours)</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td style=\"text-align: center; vertical-align: middle;\"><strong>Hanoi Problem</strong></td>\n      <td style=\"text-align: center;\">\n        <img src=\"assets/videos/veo/Hanoi.gif\">\n      </td>\n      <td style=\"text-align: center;\">\n        <img src=\"assets/videos/wan/Hanoi.gif\">\n      </td>\n      <td style=\"text-align: center;\">\n        <img src=\"assets/videos/code2video/Hanoi_4K_SpeedUp.gif\">\n      </td>\n    </tr>\n    <tr>\n      <td style=\"text-align: center; vertical-align: middle;\"><strong>Large Language Model</strong></td>\n      <td style=\"text-align: center;\">\n        <img src=\"assets/videos/veo/LLM.gif\">\n      </td>\n      <td style=\"text-align: center;\">\n        <img src=\"assets/videos/wan/LLM.gif\">\n      </td>\n      <td style=\"text-align: center;\">\n        <img src=\"assets/videos/code2video/LLM_speed.gif\">\n      </td>\n    </tr>\n    <tr>\n      <td style=\"text-align: center; vertical-align: middle;\"><strong>Pure Fourier Series</strong></td>\n      <td style=\"text-align: center;\">\n        <img src=\"assets/videos/veo/fourier.gif\">\n      </td>\n      <td style=\"text-align: center;\">\n        <img src=\"assets/videos/wan/fourier.gif\">\n      </td>\n      <td style=\"text-align: center;\">\n        <img src=\"assets/videos/code2video/fourier_speed.gif\">\n      </td>\n    </tr>\n    </tbody>\n</table>\n</p> -->\n\n<p align=\"center\">\n<table style=\"width: 90%; border-collapse: collapse; text-align: center; margin: auto;\">\n  <thead>\n    <tr>\n      <th style=\"text-align: center; padding: 8px;\">Learning Topic</th>\n      <th style=\"text-align: center; padding: 8px;\">Veo3</th>\n      <th style=\"text-align: center; padding: 8px;\">Wan2.2</th>\n      <th style=\"text-align: center; padding: 8px;\">Code2Video (Ours)</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td style=\"text-align: center; vertical-align: middle; font-weight: bold;\">Hanoi Problem</td>\n      <td>\n        <div style=\"width: 320px; aspect-ratio: 16 / 9; overflow: hidden; margin: auto;\">\n          <img src=\"assets/videos/veo/Hanoi.gif\" style=\"width: 100%; height: 100%; object-fit: cover;\">\n        </div>\n      </td>\n      <td>\n        <div style=\"width: 320px; aspect-ratio: 16 / 9; overflow: hidden; margin: auto;\">\n          <img src=\"assets/videos/wan/Hanoi.gif\" style=\"width: 100%; height: 100%; object-fit: cover;\">\n        </div>\n      </td>\n      <td>\n        <div style=\"width: 320px; aspect-ratio: 16 / 9; overflow: hidden; margin: auto;\">\n          <img src=\"assets/videos/code2video/Hanoi_4K_SpeedUp.gif\" style=\"width: 100%; height: 100%; object-fit: cover;\">\n        </div>\n      </td>\n    </tr>\n    <tr>\n      <td style=\"text-align: center; vertical-align: middle; font-weight: bold;\">Large Language Model</td>\n      <td>\n        <div style=\"width: 320px; aspect-ratio: 16 / 9; overflow: hidden; margin: auto;\">\n          <img src=\"assets/videos/veo/LLM.gif\" style=\"width: 100%; height: 100%; object-fit: cover;\">\n        </div>\n      </td>\n      <td>\n        <div style=\"width: 320px; aspect-ratio: 16 / 9; overflow: hidden; margin: auto;\">\n          <img src=\"assets/videos/wan/LLM.gif\" style=\"width: 100%; height: 100%; object-fit: cover;\">\n        </div>\n      </td>\n      <td>\n        <div style=\"width: 320px; aspect-ratio: 16 / 9; overflow: hidden; margin: auto;\">\n          <img src=\"assets/videos/code2video/LLM_speed.gif\" style=\"width: 100%; height: 100%; object-fit: cover;\">\n        </div>\n      </td>\n    </tr>\n    <tr>\n      <td style=\"text-align: center; vertical-align",
      "default_branch": "main"
    },
    "fetched_at": "2025-10-26T02:28:21.113406"
  },
  {
    "basic_info": {
      "name": "LuoGen-agent",
      "full_name": "LuoGen-AI/LuoGen-agent",
      "owner": "LuoGen-AI",
      "description": "‰∏ÄÈîÆ‰∫ßÂá∫ÁàÜÊ¨æËßÜÈ¢ëÔºö1.Ëá™Âä®ÊèêÂèñÂØπÊ†áÊñáÊ°à 2.Ëá™Âä®ËøõË°åÊñáÊ°à‰ªøÂÜô 3.Ëá™Âä®Ê†πÊçÆÊñáÊ°àÂ£∞Èü≥ÂÖãÈöÜ 4.Ëá™Âä®ÁîüÊàêÊï∞Â≠ó‰∫∫Âè£Êí≠ 5.Ëá™Âä®Ê∑ªÂä†Â≠óÂπï 6.Ëá™Âä®Ê∑ªÂä†ËÉåÊôØÈü≥‰πê 7.Ëá™Âä®Ê∑ªÂä†ËßÜÈ¢ëÊ†áÈ¢ò 8.Ëá™Âä®ÁîüÊàêËßÜÈ¢ëÂ∞ÅÈù¢ 9.Ëá™Âä®Â∞ÜËßÜÈ¢ëÂèëÂ∏ÉÂà∞ÂêÑÂπ≥Âè∞",
      "url": "https://github.com/LuoGen-AI/LuoGen-agent",
      "clone_url": "https://github.com/LuoGen-AI/LuoGen-agent.git",
      "ssh_url": "git@github.com:LuoGen-AI/LuoGen-agent.git",
      "homepage": null,
      "created_at": "2025-10-02T12:12:18Z",
      "updated_at": "2025-10-25T17:15:52Z",
      "pushed_at": "2025-10-03T12:03:31Z"
    },
    "stats": {
      "stars": 743,
      "forks": 76,
      "watchers": 743,
      "open_issues": 5,
      "size": 217
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 55666,
        "Batchfile": 1966
      },
      "license": "GNU General Public License v3.0",
      "topics": []
    },
    "content": {
      "readme": "# `ÁúüÊ≠£ÁöÑÂïÜ‰∏öÁ∫ßÂ∫îÁî®` üöÄ ‰∏ÄÈîÆÁîüÊàêÁàÜÊ¨æËßÜÈ¢ëËá™Âä®ÂåñÂ∑•ÂÖ∑\n\n![ÂÆ¢Êà∑Á´ØUI](show.png)\n\n- Áî±‰∫é‰ª£Á†Å‰ΩìÁßØÂèäÊ®°ÂûãÊñá‰ª∂ËøáÂ§ßÔºåËØ∑ËØ∏‰ΩçÁßªÊ≠• [‰ª£Á†ÅÂú∞ÂùÄ](‰ª£Á†ÅÂú∞ÂùÄ.txt) ËøõË°å‰∏ãËΩΩ„ÄÇ\n- Áî±‰∫éËØ•Â∫îÁî®‰∏∫Êú¨Âú∞ËøêË°åÁöÑÂÆ¢Êà∑Á´ØÂ∫îÁî®Ôºå‰∏∫‰∫ÜËØ∏‰ΩçÁöÑ‰ΩøÁî®‰ΩìÈ™åÔºåÂÖàËøõË°å [‰ΩøÁî®ÂâçÂøÖË£Ö](‰ΩøÁî®ÂâçÂøÖË£Ö.txt) ËøõË°å‰∏ãËΩΩÂÆâË£Ö„ÄÇ\n\n> ËØ∏Â§ö‰∏ç‰æøÔºåÊï¨ËØ∑Ë∞ÖËß£„ÄÇ\n\n**È°πÁõÆÊèèËø∞**  \nÊú¨Â∑•ÂÖ∑ÈÄöËøáËá™Âä®ÂåñÊµÅÁ®ãÔºåÂ∏ÆÂä©Áî®Êà∑Âø´ÈÄüÁîüÊàêÈ´òË¥®ÈáèÁöÑÊï∞Â≠ó‰∫∫Âè£Êí≠ËßÜÈ¢ëÂπ∂ÂèëÂ∏ÉËá≥Â§öÂπ≥Âè∞„ÄÇÊ†∏ÂøÉÂäüËÉΩÂåÖÊã¨Ôºö\n- üìù **Êô∫ËÉΩÊñáÊ°àÂ§ÑÁêÜ**ÔºöËá™Âä®ÊèêÂèñÂØπÊ†áÊñáÊ°à + Êô∫ËÉΩ‰ªøÂÜô‰ºòÂåñ\n- üé§ **Â£∞Èü≥ÂÖãÈöÜ**ÔºöÂü∫‰∫é Whisper Âíå CosyVoice ÂÆûÁé∞È´ò‰øùÁúüËØ≠Èü≥ÂêàÊàê\n- üë• **Êï∞Â≠ó‰∫∫ÁîüÊàê**ÔºöÈõÜÊàê HeyGem ÂÆûÁé∞Ëá™ÁÑ∂Âè£Êí≠ÊïàÊûú\n- üé¨ **ÂÖ®ÊµÅÁ®ãËßÜÈ¢ëÂà∂‰Ωú**ÔºöÂ≠óÂπï/BGM/Ê†áÈ¢ò/Â∞ÅÈù¢Ëá™Âä®ÁîüÊàê + Â§öÂπ≥Âè∞ÂèëÂ∏É\n\n## üåü Ê†∏ÂøÉÂäüËÉΩ\n| ÂäüËÉΩÊ®°Âùó          | ÊäÄÊúØÂÆûÁé∞                     |\n|-------------------|------------------------------|\n| ËØ≠Èü≥ÂÖãÈöÜ          | WhisperÔºàËØ≠Èü≥ËØÜÂà´Ôºâ + CosyVoiceÔºàËØ≠Èü≥ÂêàÊàêÔºâ |\n| Êï∞Â≠ó‰∫∫Âè£Êí≠        | HeyGem Êï∞Â≠ó‰∫∫ÂºïÊìé            |\n| ËßÜÈ¢ëÂêéÊúü          | FFmpegÔºàÂêàÊàêÔºâ + Âä®ÊÄÅÂ≠óÂπï    |\n| Â§öÂπ≥Âè∞ÂèëÂ∏É        | Âπ≥Âè∞ API ÈõÜÊàêÔºàÊäñÈü≥/BÁ´ôÁ≠âÔºâ  |\n\n\n## ü§ù Ëá¥Ë∞¢\nÊú¨È°πÁõÆÂü∫‰∫é‰ª•‰∏ã‰ºòÁßÄÂºÄÊ∫êÈ°πÁõÆÊûÑÂª∫Ôºö\n- [social-auto-upload](https://github.com/...) - Â§öÂπ≥Âè∞ÂèëÂ∏ÉÊ°ÜÊû∂\n- [CosyVoice](https://github.com/tencent-ailab/cosyvoice) - È´òË¥®ÈáèËØ≠Èü≥ÂêàÊàê\n- [HeyGem](https://github.com/...) - Êï∞Â≠ó‰∫∫È©±Âä®ÂºïÊìé\n- [Whisper](https://github.com/openai/whisper) - Á≤æÂáÜËØ≠Èü≥ËØÜÂà´\n\n\n## ‚ö†Ô∏è‰ΩøÁî®ÈôêÂà∂\n- Êú¨È°πÁõÆ‰ªÖÈôê‰∏™‰∫∫Â≠¶‰π†„ÄÅÁ†îÁ©∂‰ΩøÁî®Ôºå‰∏•Á¶Å‰ªª‰ΩïÂΩ¢ÂºèÁöÑÂïÜ‰∏öÁî®ÈÄîÔºàÂåÖÊã¨‰ΩÜ‰∏çÈôê‰∫éÂîÆÂçñÂ∑•ÂÖ∑„ÄÅÊèê‰æõ‰ªòË¥πÊúçÂä°Á≠âÔºâ„ÄÇ\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-10-26T02:28:22.278352"
  },
  {
    "basic_info": {
      "name": "flareprox",
      "full_name": "MrTurvey/flareprox",
      "owner": "MrTurvey",
      "description": "Use Cloudflare to create HTTP pass-through proxies for unique IP rotation, similar to fireprox",
      "url": "https://github.com/MrTurvey/flareprox",
      "clone_url": "https://github.com/MrTurvey/flareprox.git",
      "ssh_url": "git@github.com:MrTurvey/flareprox.git",
      "homepage": null,
      "created_at": "2025-09-26T11:15:43Z",
      "updated_at": "2025-10-25T22:18:59Z",
      "pushed_at": "2025-09-29T09:43:10Z"
    },
    "stats": {
      "stars": 640,
      "forks": 55,
      "watchers": 640,
      "open_issues": 3,
      "size": 175
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 28222
      },
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "# FlareProx üî•\n\n**Simple IP Rotation & URL Redirection via Cloudflare Workers** \n\nFlareProx automatically deploys HTTP proxy endpoints on Cloudflare Workers for easy redirection of all traffic to any URL you specify. It supports all HTTP methods (GET, POST, PUT, DELETE, etc.) and provides IP masking through Cloudflare's global network. 100k requests per day are free!\n\n## Features\n\n- **HTTP Support**: All HTTP methods (GET, POST, PUT, DELETE, PATCH, OPTIONS, HEAD)\n- **Simple URL Redirection**: Provide any URL and FlareProx redirects traffic through Cloudflare\n- **Global Network**: Leverage Cloudflare's worldwide CDN infrastructure\n- **Free Tier**: 100,000 requests per day on Cloudflare's free plan\n- **Easy Deployment**: Single command deployment and management\n\n## How It Works\n\nFlareProx deploys Cloudflare Workers that act as HTTP proxies. When you make a request:\n\n1. **Request Routing**: Your request is sent to a FlareProx endpoint\n2. **URL Extraction**: The Worker extracts the target URL from query params or custom HTTP header\n3. **Request Proxying**: The Worker forwards your request to the target URL\n4. **Response Relay**: The target's response is relayed back through Cloudflare\n5. **IP Masking**: Your original IP is masked by Cloudflare's infrastructure\n\n## Screenshots\n### Create Proxies:\n![List](screenshots/proxyscreate.png \"list\")\n### Test Proxies:\n![Usage](screenshots/proxys.png \"usage\")\n### Using a Proxy with BurpSuite:\n![Create](screenshots/request.png \"create\")\n\n## Quick Start\n\n### 1. Install Dependencies\n```bash\ngit clone <repository-url>\ncd flareprox\npip install -r requirements.txt\n```\n\n### 2. Configure Cloudflare Access\n\nRun `python3 fireprox.py config` or directly edit `flareprox.json` in the project directory:\n```json\n{\n  \"cloudflare\": {\n    \"api_token\": \"your_cloudflare_api_token\",\n    \"account_id\": \"your_cloudflare_account_id\"\n  }\n}\n```\n\n### 3. Deploy Proxy Endpoints\n```bash\n# Create 2 proxy endpoints\npython3 flareprox.py create --count 2\n\n# View deployed endpoints\npython3 flareprox.py list\n```\n\n### 4. Use Your Proxies\n```bash\n# Test all endpoints are functioning\npython3 flareprox.py test\n\n# Example per HTTP Method\n\n# GET request\ncurl \"https://your-worker.account.workers.dev?url=https://httpbin.org/get\"\n\n# POST request with data\ncurl -X POST -d \"username=admin\" \"https://your-worker.account.workers.dev?url=https://httpbin.org/post\"\n\n# PUT request with JSON\ncurl -X PUT -d '{\"username\":\"admin\"}' -H \"Content-Type: application/json\" \\\n  \"https://your-worker.account.workers.dev?url=https://httpbin.org/put\"\n\n# DELETE request\ncurl -X DELETE \"https://your-worker.account.workers.dev?url=https://httpbin.org/delete\"\n```\nEach deployed FlareProx endpoint accepts requests in two formats:\n\n```bash\n# Query parameter\ncurl \"https://your-worker.account.workers.dev?url=https://httpbin.org/ip\"\n\n# Custom header\ncurl -H \"X-Target-URL: https://httpbin.org/ip\" https://your-worker.account.workers.dev\n```\n\n### 5. Proxy Cleanup\n```bash\n# Delete all proxy endpoints\npython3 flareprox.py cleanup\n```\n\n\n## Getting Cloudflare Credentials\n\n### Cloudflare Workers Setup\n1. Sign up at [Cloudflare](https://cloudflare.com)\n2. Go to [API Tokens](https://dash.cloudflare.com/profile/api-tokens)\n3. Click 'Create Token' and use the 'Edit Cloudflare Workers' template\n4. Set the 'account resources' and 'zone resources' to all. Click 'Continue to Summary'\n5. Click 'Create Token' and copy the token and your Account ID from the dashboard\n\n\n## Programmatic Usage\n\nFlareProx can be imported and used directly in your Python applications. Here's how to send a POST request:\n\n```python\n#!/usr/bin/env python3\nfrom flareprox import FlareProx, FlareProxError\nimport json\n\n# Initialize FlareProx\nflareprox = FlareProx(config_file=\"flareprox.json\")\n\n# Check if configured\nif not flareprox.is_configured:\n    print(\"FlareProx not configured. Run: python3 flareprox.py config\")\n    exit(1)\n\n# Create some endpoints if none exist\nendpoints = flareprox.sync_endpoints()\nif not endpoints:\n    print(\"Creating proxy endpoints...\")\n    flareprox.create_proxies(count=2)\n\n# Make a POST request through FlareProx\ntry:\n    # Prepare POST data\n    post_data = json.dumps({\n        \"username\": \"testuser\",\n        \"message\": \"Hello from FlareProx!\",\n        \"timestamp\": \"2025-01-01T12:00:00Z\"\n    })\n\n    headers = {\n        \"Content-Type\": \"application/json\",\n        \"User-Agent\": \"FlareProx-Client/1.0\"\n    }\n\n    # Send POST request via random FlareProx endpoint\n    response = flareprox.redirect_request(\n        target_url=\"https://httpbin.org/post\",\n        method=\"POST\",\n        headers=headers,\n        data=post_data\n    )\n\n    if response.status_code == 200:\n        result = response.json()\n        print(f\"‚úì POST successful via FlareProx\")\n        print(f\"Origin IP: {result.get('origin', 'unknown')}\")\n        print(f\"Posted data: {result.get('json', {})}\")\n    else:\n        print(f\"Request failed with status: {response.status_code}\")\n\nexcept FlareProxError as e:\n    print(",
      "default_branch": "main"
    },
    "fetched_at": "2025-10-26T02:28:23.438319"
  }
]