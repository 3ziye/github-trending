[
  {
    "basic_info": {
      "name": "TinyRecursiveModels",
      "full_name": "SamsungSAILMontreal/TinyRecursiveModels",
      "owner": "SamsungSAILMontreal",
      "description": null,
      "url": "https://github.com/SamsungSAILMontreal/TinyRecursiveModels",
      "clone_url": "https://github.com/SamsungSAILMontreal/TinyRecursiveModels.git",
      "ssh_url": "git@github.com:SamsungSAILMontreal/TinyRecursiveModels.git",
      "homepage": null,
      "created_at": "2025-10-07T13:24:28Z",
      "updated_at": "2025-10-12T02:01:03Z",
      "pushed_at": "2025-10-08T19:46:47Z"
    },
    "stats": {
      "stars": 3554,
      "forks": 409,
      "watchers": 3554,
      "open_issues": 10,
      "size": 1266
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 147529
      },
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "# Less is More: Recursive Reasoning with Tiny Networks\n\nThis is the codebase for the paper: \"Less is More: Recursive Reasoning with Tiny Networks\". TRM is a recursive reasoning approach that achieves amazing scores of 45% on ARC-AGI-1 and 8% on ARC-AGI-2 using a tiny 7M parameters neural network.\n\n[Paper](https://arxiv.org/abs/2510.04871)\n\n### Motivation\n\nTiny Recursion Model (TRM) is a recursive reasoning model that achieves amazing scores of 45% on ARC-AGI-1 and 8% on ARC-AGI-2 with a tiny 7M parameters neural network. The idea that one must rely on massive foundational models trained for millions of dollars by some big corporation in order to achieve success on hard tasks is a trap. Currently, there is too much focus on exploiting LLMs rather than devising and expanding new lines of direction. With recursive reasoning, it turns out that ‚Äúless is more‚Äù: you don‚Äôt always need to crank up model size in order for a model to reason and solve hard problems. A tiny model pretrained from scratch, recursing on itself and updating its answers over time, can achieve a lot without breaking the bank.\n\nThis work came to be after I learned about the recent innovative Hierarchical Reasoning Model (HRM). I was amazed that an approach using small models could do so well on hard tasks like the ARC-AGI competition (reaching 40% accuracy when normally only Large Language Models could compete). But I kept thinking that it is too complicated, relying too much on biological arguments about the human brain, and that this recursive reasoning process could be greatly simplified and improved. Tiny Recursion Model (TRM) simplifies recursive reasoning to its core essence, which ultimately has nothing to do with the human brain, does not require any mathematical (fixed-point) theorem, nor any hierarchy.\n\n### How TRM works\n\n<p align=\"center\">\n  <img src=\"https://AlexiaJM.github.io/assets/images/TRM_fig.png\" alt=\"TRM\"  style=\"width: 30%;\">\n</p>\n\nTiny Recursion Model (TRM) recursively improves its predicted answer y with a tiny network. It starts with the embedded input question x and initial embedded answer y and latent z. For up to K improvements steps, it tries to improve its answer y. It does so by i) recursively updating n times its latent z given the question x, current answer y, and current latent z (recursive reasoning), and then ii) updating its answer y given the current answer y and current latent z. This recursive process allows the model to progressively improve its answer (potentially addressing any errors from its previous answer) in an extremely parameter-efficient manner while minimizing overfitting.\n\n### Requirements\n\n- Python 3.10 (or similar)\n- Cuda 12.6.0 (or similar)\n\n```bash\npip install --upgrade pip wheel setuptools\npip install --pre --upgrade torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu126 # install torch based on your cuda version\npip install -r requirements.txt # install requirements\npip install --no-cache-dir --no-build-isolation adam-atan2 \nwandb login YOUR-LOGIN # login if you want the logger to sync results to your Weights & Biases (https://wandb.ai/)\n```\n\n### Dataset Preparation\n\n```bash\n# ARC-AGI-1\npython -m dataset.build_arc_dataset \\\n  --input-file-prefix kaggle/combined/arc-agi \\\n  --output-dir data/arc1concept-aug-1000 \\\n  --subsets training evaluation concept \\\n  --test-set-name evaluation\n\n# ARC-AGI-2\npython -m dataset.build_arc_dataset \\\n  --input-file-prefix kaggle/combined/arc-agi \\\n  --output-dir data/arc2concept-aug-1000 \\\n  --subsets training2 evaluation2 concept \\\n  --test-set-name evaluation2\n\n## Note: You cannot train on both ARC-AGI-1 and ARC-AGI-2 and evaluate them both because ARC-AGI-2 training data contains some ARC-AGI-1 eval data\n\n# Sudoku-Extreme\npython dataset/build_sudoku_dataset.py --output-dir data/sudoku-extreme-1k-aug-1000  --subsample-size 1000 --num-aug 1000  # 1000 examples, 1000 augments\n\n# Maze-Hard\npython dataset/build_maze_dataset.py # 1000 examples, 8 augments\n```\n\n## Experiments\n\n### ARC-AGI-1 (assuming 4 H-100 GPUs):\n\n```bash\nrun_name=\"pretrain_att_arc1concept_4\"\ntorchrun --nproc-per-node 4 --rdzv_backend=c10d --rdzv_endpoint=localhost:0 --nnodes=1 pretrain.py \\\narch=trm \\\ndata_paths=\"[data/arc1concept-aug-1000]\" \\\narch.L_layers=2 \\\narch.H_cycles=3 arch.L_cycles=4 \\\n+run_name=${run_name} ema=True\n\n```\n\n*Runtime:* ~3 days\n\n### ARC-AGI-2 (assuming 4 H-100 GPUs):\n\n```bash\nrun_name=\"pretrain_att_arc2concept_4\"\ntorchrun --nproc-per-node 4 --rdzv_backend=c10d --rdzv_endpoint=localhost:0 --nnodes=1 pretrain.py \\\narch=trm \\\ndata_paths=\"[data/arc2concept-aug-1000]\" \\\narch.L_layers=2 \\\narch.H_cycles=3 arch.L_cycles=4 \\\n+run_name=${run_name} ema=True\n\n```\n\n*Runtime:* ~3 days\n\n### Sudoku-Extreme (assuming 1 L40S GPU):\n\n```bash\nrun_name=\"pretrain_mlp_t_sudoku\"\npython pretrain.py \\\narch=trm \\\ndata_paths=\"[data/sudoku-extreme-1k-aug-1000]\" \\\nevaluators=\"[]\" \\\nepochs=50000 eval_interval=5000 \\\nlr=1e-4 puzzle_emb_lr=1e-4 weight_decay=1.0 puzzle",
      "default_branch": "main"
    },
    "fetched_at": "2025-10-12T02:22:20.338323"
  },
  {
    "basic_info": {
      "name": "bdh",
      "full_name": "pathwaycom/bdh",
      "owner": "pathwaycom",
      "description": "Baby Dragon Hatchling (BDH) ‚Äì Architecture and Code",
      "url": "https://github.com/pathwaycom/bdh",
      "clone_url": "https://github.com/pathwaycom/bdh.git",
      "ssh_url": "git@github.com:pathwaycom/bdh.git",
      "homepage": "",
      "created_at": "2025-09-30T12:05:01Z",
      "updated_at": "2025-10-11T23:29:58Z",
      "pushed_at": "2025-10-07T14:53:05Z"
    },
    "stats": {
      "stars": 3017,
      "forks": 97,
      "watchers": 3017,
      "open_issues": 2,
      "size": 999
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 8722
      },
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "## Baby Dragon Hatchling\nThis repository contains source code from the paper: Adrian Kosowski, Przemys≈Çaw Uzna≈Ñski, Jan Chorowski, Zuzanna Stamirowska, Micha≈Ç Bartoszkiewicz, _\"The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain\"_, [link](https://doi.org/10.48550/arXiv.2509.26507).\n\n## Architecture\n<img src=\"figs/architecture.png\" width=\"600\"/> \n\n## Relation to Tranformers\n<img src=\"figs/vocab.png\" width=\"600\"/> \n\n## Scaling laws\n<img src=\"figs/bdh_scaling.png\" width=\"600\"/> \n\n## Abstract:\nThe relationship between computing systems and the brain has served as motivation for pioneering theoreticians since John von Neumann and Alan Turing. \nUniform, scale-free biological networks, such as the brain, have powerful properties, including generalizing over time, which is the main barrier for Machine Learning on the path to Universal Reasoning Models.\n\nWe introduce `Dragon Hatchling' (BDH), a new Large Language Model architecture based on a scale-free biologically inspired network of $n$ locally-interacting neuron particles. BDH couples strong theoretical foundations and inherent interpretability without sacrificing Transformer-like performance.\n\nBDH is a practical, performant state-of-the-art \nattention-based state space sequence learning architecture. \nIn addition to being a graph model, BDH admits a GPU-friendly formulation.\nIt exhibits Transformer-like scaling laws: we find empirically that BDH rivals GPT2-architecture Transformer performance on language and translation tasks, at the same number of parameters (10M to 1B), for the same training data.\n\nBDH provides theoretical foundations for understanding model behavior in the limit of large size and reasoning time. \nOur results, formalized as a chain of reductions of expressiveness in the framework of computational Complexity Theory and Distributed Computing, and combined with findings on the BDH model, show a macro-to-micro correspondence of function between the general attention mechanisms in state-of-the-art Language Models, and attention mechanisms observed in the brain. These attention mechanisms formally converge as closed-form local graph dynamics at neurons and synapses: _the equations of reasoning_.\n\nBDH can be represented as a brain model. It contains $n$ neurons, organized as an excitatory circuit and an inhibitory circuit with integrate-and-fire thresholding of input signals at neurons. The working memory of BDH during inference entirely relies on synaptic plasticity with Hebbian learning using spiking neurons, at potentiation scales of minutes for the brain (up to hundreds of tokens). We confirm empirically that specific, individual synapses strengthen connection whenever BDH hears or reasons about a specific concept while processing language inputs. The neuron interaction network of BDH is a graph of high modularity with heavy-tailed degree distribution. The BDH model is biologically plausible, explaining one possible mechanism which human neurons could use to achieve speech.\n\nBDH is designed for interpretability. Activation vectors of BDH are sparse and positive. We demonstrate monosemanticity in BDH on language tasks, including representation of concept abstractions, which happens even for small models, below 100M-parameter scale. Interpretability of state, which goes beyond interpretability of neurons and model parameters, is an inherent feature of the BDH architecture. \n\nWe believe BDH opens the door to a new theory of _Thermodynamic Limit_ behavior for language and reasoning models, with the ultimate goal of Probably Approximately Correct (PAC)-like bounds for generalization of reasoning over time.\n\n## Running the code\n\nTo train and sample from the BDH model on a toy language modeling task please do:\n1. `pip install -r requirements.txt`\n2. `python train.py`\n\n## Acknowledgements\nWe thank Andrej Karpathy for the [nanoGPT](https://github.com/karpathy/nanoGPT/) code and the tiny Shapespeare dataset used in this demonstration.\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-10-12T02:22:21.601353"
  },
  {
    "basic_info": {
      "name": "neutts-air",
      "full_name": "neuphonic/neutts-air",
      "owner": "neuphonic",
      "description": "On-device TTS model by Neuphonic",
      "url": "https://github.com/neuphonic/neutts-air",
      "clone_url": "https://github.com/neuphonic/neutts-air.git",
      "ssh_url": "git@github.com:neuphonic/neutts-air.git",
      "homepage": null,
      "created_at": "2025-10-02T12:48:55Z",
      "updated_at": "2025-10-12T02:17:29Z",
      "pushed_at": "2025-10-10T13:57:44Z"
    },
    "stats": {
      "stars": 2873,
      "forks": 236,
      "watchers": 2873,
      "open_issues": 22,
      "size": 1912
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 14928
      },
      "license": "Apache License 2.0",
      "topics": []
    },
    "content": {
      "readme": "# NeuTTS Air ‚òÅÔ∏è\n\nHuggingFace ü§ó: [Model](https://huggingface.co/neuphonic/neutts-air), [Q8 GGUF](https://huggingface.co/neuphonic/neutts-air-q8-gguf), [Q4 GGUF](https://huggingface.co/neuphonic/neutts-air-q4-gguf) [Spaces](https://huggingface.co/spaces/neuphonic/neutts-air)\n\n[Demo Video](https://github.com/user-attachments/assets/020547bc-9e3e-440f-b016-ae61ca645184)\n\n*Created by [Neuphonic](http://neuphonic.com/) - building faster, smaller, on-device voice AI*\n\nState-of-the-art Voice AI has been locked behind web APIs for too long. NeuTTS Air is the world‚Äôs first super-realistic, on-device, TTS speech language model with instant voice cloning. Built off a 0.5B LLM backbone, NeuTTS Air brings natural-sounding speech, real-time performance, built-in security and speaker cloning to your local device - unlocking a new category of embedded voice agents, assistants, toys, and compliance-safe apps.\n\n## Key Features\n\n- üó£Best-in-class realism for its size - produces natural, ultra-realistic voices that sound human\n- üì±Optimised for on-device deployment - provided in GGML format, ready to run on phones, laptops, or even Raspberry Pis\n- üë´Instant voice cloning - create your own speaker with as little as 3 seconds of audio\n- üöÑSimple LM + codec architecture built off a 0.5B backbone - the sweet spot between speed, size, and quality for real-world applications\n\n> [!CAUTION]\n> Websites like neutts.com are popping up and they're not affliated with Neuphonic, our github or this repo.\n>\n> We are on neuphonic.com only. Please be careful out there! üôè\n\n## Model Details\n\nNeuTTS Air is built off Qwen 0.5B - a lightweight yet capable language model optimised for text understanding and generation - as well as a powerful combination of technologies designed for efficiency and quality:\n- **Supported Languages**: English\n- **Audio Codec**: [NeuCodec](https://huggingface.co/neuphonic/neucodec) - our 50hz neural audio codec that achieves exceptional audio quality at low bitrates using a single codebook\n- **Context Window**: 2048 tokens, enough for processing ~30 seconds of audio (including prompt duration)\n- **Format**: Available in GGML format for efficient on-device inference\n- **Responsibility**: Watermarked outputs\n- **Inference Speed**: Real-time generation on mid-range devices\n- **Power Consumption**: Optimised for mobile and embedded devices\n\n## Get Started\n\n1. **Clone Git Repo**\n   ```bash\n   git clone https://github.com/neuphonic/neutts-air.git\n   ```\n   ```bash\n   cd neutts-air\n   ```\n\n2. **Install `espeak` (required dependency)**\n\n   Please refer to the following link for instructions on how to install `espeak`:\n\n   https://github.com/espeak-ng/espeak-ng/blob/master/docs/guide.md\n\n   ```bash\n   # Mac OS\n   brew install espeak\n\n   # Ubuntu/Debian\n   sudo apt install espeak\n   ```\n\n   Mac users may need to put the following lines at the top of the neutts.py file.\n   ```python\n   from phonemizer.backend.espeak.wrapper import EspeakWrapper\n   _ESPEAK_LIBRARY = '/opt/homebrew/Cellar/espeak/1.48.04_1/lib/libespeak.1.1.48.dylib'  #use the Path to the library.\n   EspeakWrapper.set_library(_ESPEAK_LIBRARY)\n   ```\n\n   Windows users may need to run (see https://github.com/bootphon/phonemizer/issues/163)\n   ```pwsh\n   $env:PHONEMIZER_ESPEAK_LIBRARY = \"c:\\Program Files\\eSpeak NG\\libespeak-ng.dll\"\n   $env:PHONEMIZER_ESPEAK_PATH = \"c:\\Program Files\\eSpeak NG\"\n   setx PHONEMIZER_ESPEAK_LIBRARY \"c:\\Program Files\\eSpeak NG\\libespeak-ng.dll\"\n   setx PHONEMIZER_ESPEAK_PATH \"c:\\Program Files\\eSpeak NG\"\n   ```\n\n3. **Install Python dependencies**\n\n   The requirements file includes the dependencies needed to run the model with PyTorch.\n   When using an ONNX decoder or a GGML model, some dependencies (such as PyTorch) are no longer required.\n\n   The inference is compatible and tested on `python>=3.11`.\n\n    ```\n    pip install -r requirements.txt\n    ```\n\n4. **(Optional) Install Llama-cpp-python to use the `GGUF` models.**\n   ```\n   pip install llama-cpp-python\n   ```\n   To run llama-cpp with GPU suport (CUDA, MPS) support please refer to:\n   https://pypi.org/project/llama-cpp-python/\n\n5. **(Optional) Install onnxruntime to use the `.onnx` decoder.**\n   If you want to run the onnxdecoder\n   ```\n   pip install onnxruntime\n   ```\n\n## Running the Model\n\nRun the basic example script to synthesize speech:\n```bash\npython -m examples.basic_example \\\n  --input_text \"My name is Dave, and um, I'm from London\" \\\n  --ref_audio samples/dave.wav \\\n  --ref_text samples/dave.txt\n```\n\nTo specify a particular model repo for the backbone or codec, add the `--backbone` argument. Available backbones are listed in [NeuTTS-Air huggingface collection](https://huggingface.co/collections/neuphonic/neutts-air-68cc14b7033b4c56197ef350).\n\nSeveral examples are available, including a Jupyter notebook in the `examples` folder.\n\n### One-Code Block Usage\n\n```python\nfrom neuttsair.neutts import NeuTTSAir\nimport soundfile as sf\n\ntts = NeuTTSAir(\n   backbone_repo=\"neuphonic/neutts-air\", #",
      "default_branch": "main"
    },
    "fetched_at": "2025-10-12T02:22:22.894470"
  },
  {
    "basic_info": {
      "name": "HunyuanImage-3.0",
      "full_name": "Tencent-Hunyuan/HunyuanImage-3.0",
      "owner": "Tencent-Hunyuan",
      "description": "HunyuanImage-3.0: A Powerful Native Multimodal Model for Image Generation",
      "url": "https://github.com/Tencent-Hunyuan/HunyuanImage-3.0",
      "clone_url": "https://github.com/Tencent-Hunyuan/HunyuanImage-3.0.git",
      "ssh_url": "git@github.com:Tencent-Hunyuan/HunyuanImage-3.0.git",
      "homepage": "https://hunyuan.tencent.com/image",
      "created_at": "2025-09-27T07:18:47Z",
      "updated_at": "2025-10-12T02:00:46Z",
      "pushed_at": "2025-10-02T06:24:24Z"
    },
    "stats": {
      "stars": 2100,
      "forks": 79,
      "watchers": 2100,
      "open_issues": 23,
      "size": 34775
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 374880,
        "Shell": 806
      },
      "license": "Other",
      "topics": [
        "image-generation",
        "native-multimodal-model"
      ]
    },
    "content": {
      "readme": "[‰∏≠ÊñáÊñáÊ°£](./README_zh_CN.md)\n\n<div align=\"center\">\n\n<img src=\"./assets/logo.png\" alt=\"HunyuanImage-3.0 Logo\" width=\"600\">\n\n# üé® HunyuanImage-3.0: A Powerful Native Multimodal Model for Image Generation\n\n</div>\n\n\n<div align=\"center\">\n<img src=\"./assets/banner.png\" alt=\"HunyuanImage-3.0 Banner\" width=\"800\">\n\n</div>\n\n<div align=\"center\">\n  <a href=https://hunyuan.tencent.com/image target=\"_blank\"><img src=https://img.shields.io/badge/Official%20Site-333399.svg?logo=homepage height=22px></a>\n  <a href=https://huggingface.co/tencent/HunyuanImage-3.0 target=\"_blank\"><img src=https://img.shields.io/badge/%F0%9F%A4%97%20Models-d96902.svg height=22px></a>\n  <a href=https://github.com/Tencent-Hunyuan/HunyuanImage-3.0 target=\"_blank\"><img src= https://img.shields.io/badge/Page-bb8a2e.svg?logo=github height=22px></a>\n  <a href=https://arxiv.org/pdf/2509.23951 target=\"_blank\"><img src=https://img.shields.io/badge/Report-b5212f.svg?logo=arxiv height=22px></a>\n  <a href=https://x.com/TencentHunyuan target=\"_blank\"><img src=https://img.shields.io/badge/Hunyuan-black.svg?logo=x height=22px></a>\n  <a href=https://docs.qq.com/doc/DUVVadmhCdG9qRXBU target=\"_blank\"><img src=https://img.shields.io/badge/üìö-PromptHandBook-blue.svg?logo=book height=22px></a>\n</div>\n\n\n<p align=\"center\">\n    üëè Join our <a href=\"./assets/WECHAT.md\" target=\"_blank\">WeChat</a> and <a href=\"https://discord.gg/ehjWMqF5wY\">Discord</a> | \nüíª <a href=\"https://hunyuan.tencent.com/modelSquare/home/play?modelId=289&from=/visual\">Official website(ÂÆòÁΩë) Try our model!</a>&nbsp&nbsp\n</p>\n\n## üî•üî•üî• News\n- **September 28, 2025**: üìñ **HunyuanImage-3.0 Technical Report Released** - Comprehensive technical documentation now available\n- **September 28, 2025**: üöÄ **HunyuanImage-3.0 Open Source Release** - Inference code and model weights publicly available\n\n\n## üß© Community Contributions\n\nIf you develop/use HunyuanImage-3.0 in your projects, welcome to let us know.\n\n## üìë Open-source Plan\n\n- HunyuanImage-3.0 (Image Generation Model)\n  - [x] Inference \n  - [x] HunyuanImage-3.0 Checkpoints\n  - [ ] HunyuanImage-3.0-Instruct Checkpoints (with reasoning)\n  - [ ] VLLM Support\n  - [ ] Distilled Checkpoints\n  - [ ] Image-to-Image Generation\n  - [ ] Multi-turn Interaction\n\n\n## üóÇÔ∏è Contents\n- [üî•üî•üî• News](#-news)\n- [üß© Community Contributions](#-community-contributions)\n- [üìë Open-source Plan](#-open-source-plan)\n- [üìñ Introduction](#-introduction)\n- [‚ú® Key Features](#-key-features)\n- [üõ†Ô∏è Dependencies and Installation](#-dependencies-and-installation)\n  - [üíª System Requirements](#-system-requirements)\n  - [üì¶ Environment Setup](#-environment-setup)\n  - [üì• Install Dependencies](#-install-dependencies)\n  - [Performance Optimizations](#performance-optimizations)\n- [üöÄ Usage](#-usage)\n  - [üî• Quick Start with Transformers](#-quick-start-with-transformers)\n  - [üè† Local Installation & Usage](#-local-installation--usage)\n  - [üé® Interactive Gradio Demo](#-interactive-gradio-demo)\n- [üß± Models Cards](#-models-cards)\n- [üìù Prompt Guide](#-prompt-guide)\n  - [Manually Writing Prompts](#manually-writing-prompts)\n  - [System Prompt For Automatic Rewriting the Prompt](#system-prompt-for-automatic-rewriting-the-prompt)\n  - [Advanced Tips](#advanced-tips)\n  - [More Cases](#more-cases)\n- [üìä Evaluation](#-evaluation)\n- [üìö Citation](#-citation)\n- [üôè Acknowledgements](#-acknowledgements)\n- [üåüüöÄ  Github Star History](#-github-star-history)\n\n---\n\n## üìñ Introduction\n\n**HunyuanImage-3.0** is a groundbreaking native multimodal model that unifies multimodal understanding and generation within an autoregressive framework. Our text-to-image module achieves performance **comparable to or surpassing** leading closed-source models.\n\n\n<div align=\"center\">\n  <img src=\"./assets/framework.png\" alt=\"HunyuanImage-3.0 Framework\" width=\"90%\">\n</div>\n\n## ‚ú® Key Features\n\n* üß† **Unified Multimodal Architecture:** Moving beyond the prevalent DiT-based architectures, HunyuanImage-3.0 employs a unified autoregressive framework. This design enables a more direct and integrated modeling of text and image modalities, leading to surprisingly effective and contextually rich image generation.\n\n* üèÜ **The Largest Image Generation MoE Model:** This is the largest open-source image generation Mixture of Experts (MoE) model to date. It features 64 experts and a total of 80 billion parameters, with 13 billion activated per token, significantly enhancing its capacity and performance.\n\n* üé® **Superior Image Generation Performance:** Through rigorous dataset curation and advanced reinforcement learning post-training, we've achieved an optimal balance between semantic accuracy and visual excellence. The model demonstrates exceptional prompt adherence while delivering photorealistic imagery with stunning aesthetic quality and fine-grained details.\n\n* üí≠ **Intelligent World-Knowledge Reasoning:** The unified multimodal architecture endows HunyuanImage-3.0 with powerful reasoning capabilities. It leverages its extensive world knowledge to intelligently interpret user int",
      "default_branch": "main"
    },
    "fetched_at": "2025-10-12T02:22:24.338234"
  },
  {
    "basic_info": {
      "name": "VoxCPM",
      "full_name": "OpenBMB/VoxCPM",
      "owner": "OpenBMB",
      "description": "VoxCPM: Tokenizer-Free TTS for Context-Aware Speech Generation and True-to-Life Voice Cloning",
      "url": "https://github.com/OpenBMB/VoxCPM",
      "clone_url": "https://github.com/OpenBMB/VoxCPM.git",
      "ssh_url": "git@github.com:OpenBMB/VoxCPM.git",
      "homepage": "",
      "created_at": "2025-09-16T03:41:49Z",
      "updated_at": "2025-10-12T02:10:37Z",
      "pushed_at": "2025-10-09T05:22:50Z"
    },
    "stats": {
      "stars": 1698,
      "forks": 175,
      "watchers": 1698,
      "open_issues": 23,
      "size": 1456
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 113165
      },
      "license": "Apache License 2.0",
      "topics": []
    },
    "content": {
      "readme": "## üéôÔ∏è VoxCPM: Tokenizer-Free TTS for Context-Aware Speech Generation and True-to-Life Voice Cloning\n\n\n[![Project Page](https://img.shields.io/badge/Project%20Page-GitHub-blue)](https://github.com/OpenBMB/VoxCPM/) [![Technical Report](https://img.shields.io/badge/Technical%20Report-Arxiv-red)](https://arxiv.org/abs/2509.24650) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-OpenBMB-yellow)](https://huggingface.co/openbmb/VoxCPM-0.5B) [![ModelScope](https://img.shields.io/badge/ModelScope-OpenBMB-purple)](https://modelscope.cn/models/OpenBMB/VoxCPM-0.5B)  [![Live Playground](https://img.shields.io/badge/Live%20PlayGround-Demo-orange)](https://huggingface.co/spaces/OpenBMB/VoxCPM-Demo) [![Samples](https://img.shields.io/badge/Audio%20Samples-Page-green)](https://openbmb.github.io/VoxCPM-demopage)\n\n\n\n<div align=\"center\">\n  <img src=\"assets/voxcpm_logo.png\" alt=\"VoxCPM Logo\" width=\"40%\">\n</div>\n\n<div align=\"center\">\n\nüëã Contact us on [WeChat](assets/wechat.png)\n\n</div>\n\n## News \n* [2025.09.30] üî• üî• üî•  We Release VoxCPM [Technical Report](https://arxiv.org/abs/2509.24650)!\n* [2025.09.16] üî• üî• üî•  We Open Source the VoxCPM-0.5B [weights](https://huggingface.co/openbmb/VoxCPM-0.5B)!\n* [2025.09.16] üéâ üéâ üéâ  We Provide the [Gradio PlayGround](https://huggingface.co/spaces/OpenBMB/VoxCPM-Demo) for VoxCPM-0.5B, try it now! \n\n## Overview\n\nVoxCPM is a novel tokenizer-free Text-to-Speech (TTS) system that redefines realism in speech synthesis. By modeling speech in a continuous space, it overcomes the limitations of discrete tokenization and enables two flagship capabilities: context-aware speech generation and true-to-life zero-shot voice cloning.\n\nUnlike mainstream approaches that convert speech to discrete tokens, VoxCPM uses an end-to-end diffusion autoregressive architecture that directly generates continuous speech representations from text. Built on [MiniCPM-4](https://huggingface.co/openbmb/MiniCPM4-0.5B) backbone, it achieves implicit semantic-acoustic decoupling through hierachical language modeling and FSQ constraints, greatly enhancing both expressiveness and generation stability.\n\n<div align=\"center\">\n  <img src=\"assets/voxcpm_model.png\" alt=\"VoxCPM Model Architecture\" width=\"90%\">\n</div>\n\n\n###  üöÄ Key Features\n- **Context-Aware, Expressive Speech Generation** - VoxCPM comprehends text to infer and generate appropriate prosody, delivering speech with remarkable expressiveness and natural flow. It spontaneously adapts speaking style based on content, producing highly fitting vocal expression trained on a massive 1.8 million-hour bilingual corpus.\n- **True-to-Life Voice Cloning** - With only a short reference audio clip, VoxCPM performs accurate zero-shot voice cloning, capturing not only the speaker‚Äôs timbre but also fine-grained characteristics such as accent, emotional tone, rhythm, and pacing to create a faithful and natural replica.\n- **High-Efficiency Synthesis** - VoxCPM supports streaming synthesis with a Real-Time Factor (RTF) as low as 0.17 on a consumer-grade NVIDIA RTX 4090 GPU, making it possible for real-time applications.\n\n\n\n\n\n##  Quick Start\n\n### üîß Install from PyPI\n``` sh\npip install voxcpm\n```\n### 1.  Model Download (Optional)\nBy default, when you first run the script, the model will be downloaded automatically, but you can also download the model in advance.\n- Download VoxCPM-0.5B\n    ```\n    from huggingface_hub import snapshot_download\n    snapshot_download(\"openbmb/VoxCPM-0.5B\")\n    ```\n- Download ZipEnhancer and SenseVoice-Small. We use ZipEnhancer to enhance speech prompts and SenseVoice-Small for speech prompt ASR in the web demo.\n    ```\n    from modelscope import snapshot_download\n    snapshot_download('iic/speech_zipenhancer_ans_multiloss_16k_base')\n    snapshot_download('iic/SenseVoiceSmall')\n    ```\n\n### 2. Basic Usage\n```python\nimport soundfile as sf\nimport numpy as np\nfrom voxcpm import VoxCPM\n\nmodel = VoxCPM.from_pretrained(\"openbmb/VoxCPM-0.5B\")\n\n# Non-streaming\nwav = model.generate(\n    text=\"VoxCPM is an innovative end-to-end TTS model from ModelBest, designed to generate highly expressive speech.\",\n    prompt_wav_path=None,      # optional: path to a prompt speech for voice cloning\n    prompt_text=None,          # optional: reference text\n    cfg_value=2.0,             # LM guidance on LocDiT, higher for better adherence to the prompt, but maybe worse\n    inference_timesteps=10,   # LocDiT inference timesteps, higher for better result, lower for fast speed\n    normalize=True,           # enable external TN tool\n    denoise=True,             # enable external Denoise tool\n    retry_badcase=True,        # enable retrying mode for some bad cases (unstoppable)\n    retry_badcase_max_times=3,  # maximum retrying times\n    retry_badcase_ratio_threshold=6.0, # maximum length restriction for bad case detection (simple but effective), it could be adjusted for slow pace speech\n)\n\nsf.write(\"output.wav\", wav, 16000)\nprint(\"saved: output.wav\")\n\n# Streaming\nchunks = []",
      "default_branch": "main"
    },
    "fetched_at": "2025-10-12T02:22:25.603497"
  },
  {
    "basic_info": {
      "name": "VLA-Adapter",
      "full_name": "OpenHelix-Team/VLA-Adapter",
      "owner": "OpenHelix-Team",
      "description": "VLA-Adapter: An Effective Paradigm for Tiny-Scale Vision-Language-Action Model",
      "url": "https://github.com/OpenHelix-Team/VLA-Adapter",
      "clone_url": "https://github.com/OpenHelix-Team/VLA-Adapter.git",
      "ssh_url": "git@github.com:OpenHelix-Team/VLA-Adapter.git",
      "homepage": "https://vla-adapter.github.io/",
      "created_at": "2025-09-20T10:20:54Z",
      "updated_at": "2025-10-12T01:56:11Z",
      "pushed_at": "2025-10-03T03:02:04Z"
    },
    "stats": {
      "stars": 954,
      "forks": 83,
      "watchers": 954,
      "open_issues": 5,
      "size": 10251
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 804168
      },
      "license": "MIT License",
      "topics": [
        "embodied-ai",
        "robotics",
        "vision-language-action-model"
      ]
    },
    "content": {
      "readme": "<div align=\"center\">\n  <img src=\"figure/LOGO2.png\" width=\"70%\" style=\"vertical-align:-7px;\" />\n\n\n[![Paper](https://img.shields.io/badge/Paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/pdf/2509.09372) [![Hugging Face Collection](https://img.shields.io/badge/Models-fcd022?style=for-the-badge&logo=huggingface&logoColor=white)](https://huggingface.co/VLA-Adapter) [![Twitter](https://img.shields.io/badge/AK-%23000000.svg?style=for-the-badge&logo=x&logoColor=white)](https://x.com/_akhaliq/status/1966610780838621241) [![WeChat](https://img.shields.io/badge/WeChat--Group-07C160?style=for-the-badge&logo=wechat&logoColor=white)](https://github.com/OpenHelix-Team/VLA-Adapter/issues/1)\n\n</div>\n\n### The official implementation of **VLA-Adapter**.\n<br/>\n\n<div id=\"top\" align=\"center\">\n<p align=\"center\">\n<img src=figure/Framework.png width=90% />\n</p>\n</div>\n\n> **üìù Paper: https://arxiv.org/abs/2509.09372**<br/>\n> **üåç Project page: https://vla-adapter.github.io/**<br/>\n> **ü§ó HuggingFace: https://huggingface.co/VLA-Adapter**<br/>\n> **Github: https://github.com/OpenHelix-Team/VLA-Adapter**\n\n<br/>\n\n## :loudspeaker: News!\n- **[2025/09/22]** We released our codes! An enhanced **Pro** version is also released (this version conforms to the pipeline in the original paper, but is optimized in implementation). Everyone is welcome to use it!üéâ\n- **[2025/09/13]** Our paper won the ü•á**first place** in the [daily list](https://huggingface.co/papers/date/2025-09-12), the ü•à**second place** in the [weekly list](https://huggingface.co/papers/week/2025-W37), and ü•â**third place** in the [Monthly list](https://huggingface.co/papers/month/2025-09) in HF! ‚≠ê\n- **[2025/09/12]** We released the original version of the VLA-Adapter for four LIBERO models on [HuggingFace](https://huggingface.co/VLA-Adapter).\n- **[2025/09/11]** We released our paper on [ArXiv](https://arxiv.org/abs/2509.09372).\n\n<br/>\n\n## :black_nib: TODO List<a name=\"todo\"></a>\n\n- [x]  Release **checkpoints** for reproduction.\n- [x]  Release [VLA-Adapter v2 paper](https://arxiv.org/abs/2509.09372).\n- [ ]  A more **powerful version**, **VLA-Adapter++**, and a detailed **technical report** üìù will be released soon.<br/>\n- [ ]  Continue to update the code to adapt to various **real-world systems** deployments, including the configuration of our paper, Franka, UR-5, and AGILE Piper.<br/>\n- [ ]  It will soon be compatible with **various foundation models**, including but not limited to [VPP](https://arxiv.org/abs/2412.14803), [œÄ0.5](https://arxiv.org/abs/2504.16054).<br/>\n- [ ]  We will update the **diffusion transformers** and **flow matching** policy networks in the future, and the results will be updated in the subsequent VLA-Adapter++ technical report.\n- [ ]  We will also update and give more experiments on **Frozen backbone**.\n- [ ]  We will expand its **generalization** further in the future. Work is in progress! So please stay tuned!\n- [ ]  **RL post-training** is also in progress. Interested researchers are welcome to join us in building this foundation!\n- [ ]  **The dual-system compatibility** of VLA-Adapter is under exploration!\n\n\n<br/>\n\n## üåü Table of Contents\n\n- [:rocket: Quick Start](#rocket-quick-start) \n  - [Conda Environment of VLA-Adapter](#conda-environment-of-vla-adapter)\n  - [Install Dependencies](#install-dependencies)\n- [:pencil: Data Preparation](#pencil-data-preparation) \n  - [LIBERO Benchmark](#libero-benchmark)\n  - [CALVIN Benchmark](#calvin-benchmark)\n  - [:video_game: Our Dependencies](#video_game-our-dependencies)\n  - [:pushpin: Benchmark Location](#pushpin-benchmark-location)\n- [‚öì VLM backbone](#vlm)\n- [:fire: Training for Different Configurations](#fire-training-for-different-configurations) &emsp; => Provides **training configurations** for GPUs ranging from **10GB** to **80GB** of VRAM.\n  - [:books: Related File for Training](#books-related-file-for-training)\n  - [:ledger: How to Train on Extremely Limited VRAM GPUs](#ledger-how-to-train-on-extremely-limited-vram-gpus) &emsp; => A card with 10GB-12GB *(e.g. NVIDIA GeForce RTX 2080Ti, 3060, 3080, 4070, 4080, and 5070)*\n  - [:ledger: How to Train on Low VRAM GPUs](#ledger-how-to-train-on-low-vram-gpus) &emsp; => A card with 24GB *(e.g. NVIDIA GeForce RTX 3090 and 4090)*\n  - [:ledger: How to Train on Larger VRAM GPUs](#ledger-how-to-train-on-larger-vram-gpus) &emsp; => A Consumer GPU with 32GB *(e.g. NVIDIA GeForce RTX 5090)* &emsp; A Professional-Grade GPU with 40GB-48GB *(e.g. NVIDIA A100-40GB, A800-40GB, L20, and RTX A6000).*\n  - [:ledger: How to Train on Sufficient VRAM GPUs](#ledger-how-to-train-on-sufficient-vram-gpus) &emsp; => Professional-Grade GPUs with ‚â•80GB *(e.g. NVIDIA A100-80GB, A800-80GB, H100, H800, H20-NVLink, and GB200).*\n- [:mechanical_arm: Inference](#mechanical_arm-inference)\n  - [:books: Related File for Inference](#books-related-file-for-inference)\n  - [ü§ó Checkpoint of VLA-Adapter](#ckpts)\n  - [:notebook: How to Eval](#evals)\n- [üåà Success Rate Comparison](#re",
      "default_branch": "main"
    },
    "fetched_at": "2025-10-12T02:22:26.857349"
  },
  {
    "basic_info": {
      "name": "DeepSeek-V3.2-Exp",
      "full_name": "deepseek-ai/DeepSeek-V3.2-Exp",
      "owner": "deepseek-ai",
      "description": null,
      "url": "https://github.com/deepseek-ai/DeepSeek-V3.2-Exp",
      "clone_url": "https://github.com/deepseek-ai/DeepSeek-V3.2-Exp.git",
      "ssh_url": "git@github.com:deepseek-ai/DeepSeek-V3.2-Exp.git",
      "homepage": null,
      "created_at": "2025-09-29T03:25:13Z",
      "updated_at": "2025-10-12T01:35:51Z",
      "pushed_at": "2025-10-02T02:49:20Z"
    },
    "stats": {
      "stars": 871,
      "forks": 49,
      "watchers": 871,
      "open_issues": 10,
      "size": 1077
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 59696
      },
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "# DeepSeek-V3.2-Exp\n\n<!-- markdownlint-disable first-line-h1 -->\n<!-- markdownlint-disable html -->\n<!-- markdownlint-disable no-duplicate-header -->\n\n<div align=\"center\">\n  <img src=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/logo.svg?raw=true\" width=\"60%\" alt=\"DeepSeek-V3\" />\n</div>\n<hr>\n<div align=\"center\" style=\"line-height: 1;\">\n  <a href=\"https://www.deepseek.com/\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Homepage\" src=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/badge.svg?raw=true\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://chat.deepseek.com/\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Chat\" src=\"https://img.shields.io/badge/ü§ñ%20Chat-DeepSeek%20V3-536af5?color=536af5&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://huggingface.co/deepseek-ai\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Hugging Face\" src=\"https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-DeepSeek%20AI-ffc107?color=ffc107&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n</div>\n<div align=\"center\" style=\"line-height: 1;\">\n  <a href=\"https://discord.gg/Tc7c45Zzu5\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Discord\" src=\"https://img.shields.io/badge/Discord-DeepSeek%20AI-7289da?logo=discord&logoColor=white&color=7289da\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/qr.jpeg?raw=true\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Wechat\" src=\"https://img.shields.io/badge/WeChat-DeepSeek%20AI-brightgreen?logo=wechat&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://twitter.com/deepseek_ai\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Twitter Follow\" src=\"https://img.shields.io/badge/Twitter-deepseek_ai-white?logo=x&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n</div>\n<div align=\"center\" style=\"line-height: 1;\">\n  <a href=\"LICENSE\" style=\"margin: 2px;\">\n    <img alt=\"License\" src=\"https://img.shields.io/badge/License-MIT-f5de53?&color=f5de53\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n</div>\n\n## Introduction\n\n\nWe are excited to announce the official release of DeepSeek-V3.2-Exp, an experimental version of our model. As an intermediate step toward our next-generation architecture, V3.2-Exp builds upon V3.1-Terminus by introducing DeepSeek Sparse Attention‚Äîa sparse attention mechanism designed to explore and validate optimizations for training and inference efficiency in long-context scenarios.\n\nThis experimental release represents our ongoing research into more efficient transformer architectures, particularly focusing on improving computational efficiency when processing extended text sequences.\n\n<div align=\"center\">\n <img src=\"cost.jpg\" >\n</div>\n\n- DeepSeek Sparse Attention (DSA) achieves fine-grained sparse attention for the first time, delivering substantial improvements in long-context training and inference efficiency while maintaining virtually identical model output quality.\n\n\n- To rigorously evaluate the impact of introducing sparse attention, we deliberately aligned the training configurations of DeepSeek-V3.2-Exp with V3.1-Terminus. Across public benchmarks in various domains, DeepSeek-V3.2-Exp demonstrates performance on par with V3.1-Terminus.\n\n\n| Benchmark | DeepSeek-V3.1-Terminus | DeepSeek-V3.2-Exp |\n| :--- | :---: | :---: |\n| **Reasoning Mode w/o Tool Use** | | |\n| MMLU-Pro | 85.0 | 85.0 |\n| GPQA-Diamond | 80.7 | 79.9 |\n| Humanity's Last Exam | 21.7 | 19.8 |\n| LiveCodeBench | 74.9 | 74.1 |\n| AIME 2025 | 88.4 | 89.3 |\n| HMMT 2025 | 86.1 | 83.6 |\n| Codeforces | 2046 | 2121 |\n| Aider-Polyglot | 76.1 | 74.5 |\n| **Agentic Tool Use** | | |\n| BrowseComp | 38.5 | 40.1 |\n| BrowseComp-zh | 45.0 | 47.9 |\n| SimpleQA | 96.8 | 97.1 |\n| SWE Verified | 68.4 | 67.8 |\n| SWE-bench Multilingual | 57.8 | 57.9 |\n| Terminal-bench | 36.7 | 37.7 |\n\n\n\n## Open-Source Kernels\n\nFor TileLang kernels with **better readability and research-purpose design**, please refer to [TileLang](https://github.com/tile-ai/tilelang/tree/main/examples/deepseek_v32).\n\nFor **high-performance CUDA kernels**, indexer logit kernels (including paged versions) are available in [DeepGEMM](https://github.com/deepseek-ai/DeepGEMM/pull/200). Sparse attention kernels are released in [FlashMLA](https://github.com/deepseek-ai/FlashMLA/pull/98).\n\n\n\n## How to Run Locally\n\n### HuggingFace\nWe provide an updated inference demo code in the [inference](https://huggingface.co/deepseek-ai/DeepSeek-V3.2-Exp/tree/main/inference) folder to help the community quickly get started with our model and understand its architectural details.\n\nFirst convert huggingface model weights to the the format required by our inference demo. Set `MP` to match your available GPU count:\n```bash\ncd inference\nexpor",
      "default_branch": "main"
    },
    "fetched_at": "2025-10-12T02:22:28.106604"
  },
  {
    "basic_info": {
      "name": "ml-simplefold",
      "full_name": "apple/ml-simplefold",
      "owner": "apple",
      "description": null,
      "url": "https://github.com/apple/ml-simplefold",
      "clone_url": "https://github.com/apple/ml-simplefold.git",
      "ssh_url": "git@github.com:apple/ml-simplefold.git",
      "homepage": null,
      "created_at": "2025-09-23T03:08:49Z",
      "updated_at": "2025-10-11T19:38:49Z",
      "pushed_at": "2025-09-27T04:47:33Z"
    },
    "stats": {
      "stars": 851,
      "forks": 54,
      "watchers": 851,
      "open_issues": 17,
      "size": 1251
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 662611,
        "Jupyter Notebook": 11558
      },
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "\n<h1 align=\"center\"><strong>SimpleFold: Folding Proteins is Simpler than You Think</strong></h1>\n\n\n<div align=\"center\">\n\nThis github repository accompanies the research paper, [*SimpleFold: Folding Proteins is Simpler than You Think*](https://arxiv.org/abs/2509.18480) (Arxiv 2025).\n\n*Yuyang Wang, Jiarui Lu, Navdeep Jaitly, Joshua M. Susskind, Miguel Angel Bautista*\n\n[[`Paper`](https://arxiv.org/abs/2509.18480)]  [[`BibTex`](#citation)]\n\n<img src=\"assets/intro.png\" width=\"750\">\n\n</div>\n\n\n## Introduction\n\nWe introduce SimpleFold, the first flow-matching based protein folding model that solely uses general purpose transformer layers. SimpleFold does not rely on expensive modules like triangle attention or pair representation biases, and is trained via a generative flow-matching objective. We scale SimpleFold to 3B parameters and train it on more than 8.6M distilled protein structures together with experimental PDB data. To the best of our knowledge, SimpleFold is the largest scale folding model ever developed. On standard folding benchmarks, SimpleFold-3B model achieves competitive performance compared to state-of-the-art baselines. Due to its generative training objective, SimpleFold also demonstrates strong performance in ensemble prediction. SimpleFold challenges the reliance on complex domain-specific architectures designs in folding, highlighting an alternative yet important avenue of progress in protein structure prediction.\n\n</div>\n\n\n## Installation\n\nTo install `simplefold` package from github repository, run\n```\ngit clone https://github.com/apple/ml-simplefold.git\ncd ml-simplefold\nconda create -n simplefold python=3.10\npython -m pip install -U pip build; pip install -e .\n```\nIf you want to use MLX backend on Apple silicon: \n```\npip install mlx==0.28.0\npip install git+https://github.com/facebookresearch/esm.git\n```\n\n## Example \n\nWe provide a jupyter notebook [`sample.ipynb`](sample.ipynb) to predict protein structures from example protein sequences. \n\n## Inference\n\nOnce you have `simplefold` package installed, you can predict the protein structure from target fasta file(s) via the following command line. We provide support for both [PyTorch](https://pytorch.org/) and [MLX](https://mlx-framework.org/) (recommended for Apple hardware) backends in inference. \n```\nsimplefold \\\n    --simplefold_model simplefold_100M \\  # specify folding model in simplefold_100M/360M/700M/1.1B/1.6B/3B\n    --num_steps 500 --tau 0.01 \\        # specify inference setting\n    --nsample_per_protein 1 \\           # number of generated conformers per target\n    --plddt \\                           # output pLDDT\n    --fasta_path [FASTA_PATH] \\         # path to the target fasta directory or file\n    --output_dir [OUTPUT_DIR] \\         # path to the output directory\n    --backend [mlx, torch]              # choose from MLX and PyTorch for inference backend \n```\n\n## Evaluation\n\nWe provide predicted structures from SimpleFold of different model sizes:\n```\nhttps://ml-site.cdn-apple.com/models/simplefold/cameo22_predictions.zip # predicted structures of CAMEO22\nhttps://ml-site.cdn-apple.com/models/simplefold/casp14_predictions.zip  # predicted structures of CASP14\nhttps://ml-site.cdn-apple.com/models/simplefold/apo_predictions.zip     # predicted structures of Apo\nhttps://ml-site.cdn-apple.com/models/simplefold/codnas_predictions.zip  # predicted structures of Fold-switch (CoDNaS)\n```\nWe use the docker image of [openstructure](https://git.scicore.unibas.ch/schwede/openstructure/) 2.9.1 to evaluate generated structures for folding tasks (i.e., CASP14/CAMEO22). Once having the docker image enabled, you can run evaluation via:\n```\npython src/simplefold/evaluation/analyze_folding.py \\\n    --data_dir [PATH_TO_TARGET_MMCIF] \\\n    --sample_dir [PATH_TO_PREDICTED_MMCIF] \\\n    --out_dir [PATH_TO_OUTPUT] \\\n    --max-workers [NUMBER_OF_WORKERS]\n```\nTo evaluate results of two-state prediction (i.e., Apo/CoDNaS), one need to compile the [TMsore](https://zhanggroup.org/TM-score/TMscore.cpp) and then run evaluation via:\n```\npython src/simplefold/evaluation/analyze_two_state.py \\ \n    --data_dir [PATH_TO_TARGET_DATA_DIRECTORY] \\\n    --sample_dir [PATH_TO_PREDICTED_PDB] \\\n    --tm_bin [PATH_TO_TMscore_BINARY] \\\n    --task apo \\ # choose from apo and codnas\n    --nsample 5\n```\n\n## Train\n\nYou can also train or tune SimpleFold on your end. Instructions below include details for SimpleFold training. \n\n### Data preparation\n\n#### Training targets\n\nSimpleFold is trained on joint datasets including experimental structures from [PDB](https://www.rcsb.org/), as well as distilled predictions from [AFDB SwissProt](https://alphafold.ebi.ac.uk/download#swissprot-section) and [AFESM](https://afesm.foldseek.com/). Target lists of filtered SwissProt and AFESM targets thta are used in our training can be found:\n```\nhttps://ml-site.cdn-apple.com/models/simplefold/swissprot_list.csv # list of filted SwissProt (~270K targets)\nhttps://ml-site.cdn-apple.com/models/simplefold/af",
      "default_branch": "main"
    },
    "fetched_at": "2025-10-12T02:22:29.377756"
  },
  {
    "basic_info": {
      "name": "MiMo-Audio",
      "full_name": "XiaomiMiMo/MiMo-Audio",
      "owner": "XiaomiMiMo",
      "description": "MiMo-Audio: Audio Language Models are Few-Shot Learners",
      "url": "https://github.com/XiaomiMiMo/MiMo-Audio",
      "clone_url": "https://github.com/XiaomiMiMo/MiMo-Audio.git",
      "ssh_url": "git@github.com:XiaomiMiMo/MiMo-Audio.git",
      "homepage": "https://xiaomimimo.github.io/MiMo-Audio-Demo/",
      "created_at": "2025-09-19T00:46:49Z",
      "updated_at": "2025-10-11T16:33:54Z",
      "pushed_at": "2025-09-20T19:03:26Z"
    },
    "stats": {
      "stars": 764,
      "forks": 72,
      "watchers": 764,
      "open_issues": 32,
      "size": 6029
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 223792
      },
      "license": "Apache License 2.0",
      "topics": []
    },
    "content": {
      "readme": "<div align=\"center\">\n  <picture>\n    <source srcset=\"https://github.com/XiaomiMiMo/MiMo-VL/raw/main/figures/Xiaomi_MiMo_darkmode.png?raw=true\" media=\"(prefers-color-scheme: dark)\">\n    <img src=\"https://github.com/XiaomiMiMo/MiMo-VL/raw/main/figures/Xiaomi_MiMo.png?raw=true\" width=\"60%\" alt=\"Xiaomi-MiMo\" />\n  </picture>\n</div>\n\n<h3 align=\"center\">\n  <b>\n    <span>‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ</span>\n    <br/>\n    MiMo Audio: Audio Language Models are Few-Shot Learners\n    <br/>\n    <span>‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ</span>\n    <br/>\n  </b>\n</h3>\n\n<br/>\n\n<div align=\"center\" style=\"line-height: 1;\">\n  |\n  <a href=\"https://huggingface.co/collections/XiaomiMiMo/mimo-audio-68cc7202692c27dae881cce0\" target=\"_blank\">ü§ó HuggingFace</a>\n  &nbsp;|\n  <a href=\"https://github.com/XiaomiMiMo/MiMo-Audio/blob/main/MiMo-Audio-Technical-Report.pdf\" target=\"_blank\">üìÑ Paper</a>\n  &nbsp;|\n  <a href=\"https://xiaomimimo.github.io/MiMo-Audio-Demo\" target=\"_blank\">üì∞ Blog</a>\n  &nbsp;|\n  <a href=\"https://huggingface.co/spaces/XiaomiMiMo/mimo_audio_chat\" target=\"_blank\">üî• Online Demo</a>\n  &nbsp;|\n  <a href=\"https://github.com/XiaomiMiMo/MiMo-Audio-Eval\" target=\"_blank\">üìä MiMo-Audio-Eval</a>\n  &nbsp;|\n\n  <br/>\n</div>\n\n<br/>\n\n## Introduction\n\nExisting audio language models typically rely on task-specific fine-tuning to accomplish particular audio tasks. In contrast, humans are able to generalize to new audio tasks with only a few examples or simple instructions. GPT-3 has shown that scaling next-token prediction pretraining enables strong generalization capabilities in text, and we believe this paradigm is equally applicable to the audio domain. By scaling MiMo-Audio's pretraining data to over one hundred million of hours, we observe the emergence of few-shot learning capabilities across a diverse set of audio tasks. We develop a systematic evaluation of these capabilities and find that MiMo-Audio-7B-Base achieves SOTA performance on both speech intelligence and audio understanding benchmarks among open-source models. Beyond standard metrics, MiMo-Audio-7B-Base generalizes to tasks absent from its training data, such as voice conversion, style transfer, and speech editing. MiMo-Audio-7B-Base also demonstrates powerful speech continuation capabilities, capable of generating highly realistic talk shows, recitations, livestreaming and debates. At the post-training stage, we curate a diverse instruction-tuning corpus and introduce thinking mechanisms into both audio understanding and generation. MiMo-Audio-7B-Instruct achieves open-source SOTA on audio understanding benchmarks, spoken dialogue benchmarks and instruct-TTS evaluations, approaching or surpassing closed-source models.\n\n\n![Results](assets/Results.png)\n\n\n\n## Architecture\n### MiMo-Audio-Tokenizer\nMiMo-Audio-Tokenizer is a 1.2B-parameter Transformer operating at 25 Hz. It employs an eight-layer RVQ stack to generate 200 tokens per second. By jointly optimizing semantic and reconstruction objectives, we train MiMo-Audio-Tokenizer from scratch on a 10-million-hour corpus, achieving superior reconstruction quality and facilitating downstream language modeling.\n\n![Tokenizer](assets/tokenizer.png)\n\nMiMo-Audio couples a patch encoder, an LLM, and a patch decoder to improve modeling efficiency for high-rate sequences and bridge the length mismatch between speech and text. The patch encoder aggregates four consecutive time steps of RVQ tokens into a single patch, downsampling the sequence to a 6.25 Hz representation for the LLM. The patch decoder autoregressively generates the full 25 Hz RVQ token sequence via a delayed-generation scheme.\n### MiMo-Audio\n![Arch](assets/architecture.png)\n\n##  Explore MiMo-Audio Now! üöÄüöÄüöÄ\n- üéß **Try the Hugging Face demo:** [MiMo-Audio Demo](https://huggingface.co/spaces/XiaomiMiMo/mimo_audio_chat)\n- üì∞ **Read the Official Blog:** [MiMo-Audio Blog](https://xiaomimimo.github.io/MiMo-Audio-Demo)\n- üìÑ **Dive into the Technical Report:** [MiMo-Audio Technical Report](https://github.com/XiaomiMiMo/MiMo-Audio/blob/main/MiMo-Audio-Technical-Report.pdf)\n\n\n## Model Download\n| Models   | ü§ó Hugging Face |\n|-------|-------|\n| MiMo-Audio-Tokenizer | [XiaomiMiMo/MiMo-Audio-Tokenizer](https://huggingface.co/XiaomiMiMo/MiMo-Audio-Tokenizer) |\n| MiMo-Audio-7B-Base | [XiaomiMiMo/MiMo-Audio-7B-Base](https://huggingface.co/XiaomiMiMo/MiMo-Audio-7B-Base) |\n| MiMo-Audio-7B-Instruct | [XiaomiMiMo/MiMo-Audio-7B-Instruct](https://huggingface.co/XiaomiMiMo/MiMo-Audio-7B-Instruct) |\n\n\n```bash\npip install huggingface-hub\n\nhf download XiaomiMiMo/MiMo-Audio-Tokenizer --local-dir ./models/MiMo-Audio-Tokenizer\nhf download XiaomiMiMo/MiMo-Audio-7B-Base --local-dir ./models/MiMo-Audio-7B-Base\nhf download XiaomiMiMo/MiMo-Audio-7B-Instruct --local-dir ./models/MiMo-Audio-7B-Instruct\n```\n\n## Getting Started\n\nSpin up the MiMo-Audio demo in minutes with the built-in Gradio app.\n\n### Prerequisites (Linux)\n\n* Python 3.12\n* CUDA >= 12.0\n\n### Installation\n\n```bash\ngit clone https",
      "default_branch": "main"
    },
    "fetched_at": "2025-10-12T02:22:30.633451"
  },
  {
    "basic_info": {
      "name": "LuoGen-agent",
      "full_name": "LuoGen-AI/LuoGen-agent",
      "owner": "LuoGen-AI",
      "description": "‰∏ÄÈîÆ‰∫ßÂá∫ÁàÜÊ¨æËßÜÈ¢ëÔºö1.Ëá™Âä®ÊèêÂèñÂØπÊ†áÊñáÊ°à 2.Ëá™Âä®ËøõË°åÊñáÊ°à‰ªøÂÜô 3.Ëá™Âä®Ê†πÊçÆÊñáÊ°àÂ£∞Èü≥ÂÖãÈöÜ 4.Ëá™Âä®ÁîüÊàêÊï∞Â≠ó‰∫∫Âè£Êí≠ 5.Ëá™Âä®Ê∑ªÂä†Â≠óÂπï 6.Ëá™Âä®Ê∑ªÂä†ËÉåÊôØÈü≥‰πê 7.Ëá™Âä®Ê∑ªÂä†ËßÜÈ¢ëÊ†áÈ¢ò 8.Ëá™Âä®ÁîüÊàêËßÜÈ¢ëÂ∞ÅÈù¢ 9.Ëá™Âä®Â∞ÜËßÜÈ¢ëÂèëÂ∏ÉÂà∞ÂêÑÂπ≥Âè∞",
      "url": "https://github.com/LuoGen-AI/LuoGen-agent",
      "clone_url": "https://github.com/LuoGen-AI/LuoGen-agent.git",
      "ssh_url": "git@github.com:LuoGen-AI/LuoGen-agent.git",
      "homepage": null,
      "created_at": "2025-10-02T12:12:18Z",
      "updated_at": "2025-10-12T02:17:17Z",
      "pushed_at": "2025-10-03T12:03:31Z"
    },
    "stats": {
      "stars": 678,
      "forks": 64,
      "watchers": 678,
      "open_issues": 3,
      "size": 217
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 55666,
        "Batchfile": 1966
      },
      "license": "GNU General Public License v3.0",
      "topics": []
    },
    "content": {
      "readme": "# `ÁúüÊ≠£ÁöÑÂïÜ‰∏öÁ∫ßÂ∫îÁî®` üöÄ ‰∏ÄÈîÆÁîüÊàêÁàÜÊ¨æËßÜÈ¢ëËá™Âä®ÂåñÂ∑•ÂÖ∑\n\n![ÂÆ¢Êà∑Á´ØUI](show.png)\n\n- Áî±‰∫é‰ª£Á†Å‰ΩìÁßØÂèäÊ®°ÂûãÊñá‰ª∂ËøáÂ§ßÔºåËØ∑ËØ∏‰ΩçÁßªÊ≠• [‰ª£Á†ÅÂú∞ÂùÄ](‰ª£Á†ÅÂú∞ÂùÄ.txt) ËøõË°å‰∏ãËΩΩ„ÄÇ\n- Áî±‰∫éËØ•Â∫îÁî®‰∏∫Êú¨Âú∞ËøêË°åÁöÑÂÆ¢Êà∑Á´ØÂ∫îÁî®Ôºå‰∏∫‰∫ÜËØ∏‰ΩçÁöÑ‰ΩøÁî®‰ΩìÈ™åÔºåÂÖàËøõË°å [‰ΩøÁî®ÂâçÂøÖË£Ö](‰ΩøÁî®ÂâçÂøÖË£Ö.txt) ËøõË°å‰∏ãËΩΩÂÆâË£Ö„ÄÇ\n\n> ËØ∏Â§ö‰∏ç‰æøÔºåÊï¨ËØ∑Ë∞ÖËß£„ÄÇ\n\n**È°πÁõÆÊèèËø∞**  \nÊú¨Â∑•ÂÖ∑ÈÄöËøáËá™Âä®ÂåñÊµÅÁ®ãÔºåÂ∏ÆÂä©Áî®Êà∑Âø´ÈÄüÁîüÊàêÈ´òË¥®ÈáèÁöÑÊï∞Â≠ó‰∫∫Âè£Êí≠ËßÜÈ¢ëÂπ∂ÂèëÂ∏ÉËá≥Â§öÂπ≥Âè∞„ÄÇÊ†∏ÂøÉÂäüËÉΩÂåÖÊã¨Ôºö\n- üìù **Êô∫ËÉΩÊñáÊ°àÂ§ÑÁêÜ**ÔºöËá™Âä®ÊèêÂèñÂØπÊ†áÊñáÊ°à + Êô∫ËÉΩ‰ªøÂÜô‰ºòÂåñ\n- üé§ **Â£∞Èü≥ÂÖãÈöÜ**ÔºöÂü∫‰∫é Whisper Âíå CosyVoice ÂÆûÁé∞È´ò‰øùÁúüËØ≠Èü≥ÂêàÊàê\n- üë• **Êï∞Â≠ó‰∫∫ÁîüÊàê**ÔºöÈõÜÊàê HeyGem ÂÆûÁé∞Ëá™ÁÑ∂Âè£Êí≠ÊïàÊûú\n- üé¨ **ÂÖ®ÊµÅÁ®ãËßÜÈ¢ëÂà∂‰Ωú**ÔºöÂ≠óÂπï/BGM/Ê†áÈ¢ò/Â∞ÅÈù¢Ëá™Âä®ÁîüÊàê + Â§öÂπ≥Âè∞ÂèëÂ∏É\n\n## üåü Ê†∏ÂøÉÂäüËÉΩ\n| ÂäüËÉΩÊ®°Âùó          | ÊäÄÊúØÂÆûÁé∞                     |\n|-------------------|------------------------------|\n| ËØ≠Èü≥ÂÖãÈöÜ          | WhisperÔºàËØ≠Èü≥ËØÜÂà´Ôºâ + CosyVoiceÔºàËØ≠Èü≥ÂêàÊàêÔºâ |\n| Êï∞Â≠ó‰∫∫Âè£Êí≠        | HeyGem Êï∞Â≠ó‰∫∫ÂºïÊìé            |\n| ËßÜÈ¢ëÂêéÊúü          | FFmpegÔºàÂêàÊàêÔºâ + Âä®ÊÄÅÂ≠óÂπï    |\n| Â§öÂπ≥Âè∞ÂèëÂ∏É        | Âπ≥Âè∞ API ÈõÜÊàêÔºàÊäñÈü≥/BÁ´ôÁ≠âÔºâ  |\n\n\n## ü§ù Ëá¥Ë∞¢\nÊú¨È°πÁõÆÂü∫‰∫é‰ª•‰∏ã‰ºòÁßÄÂºÄÊ∫êÈ°πÁõÆÊûÑÂª∫Ôºö\n- [social-auto-upload](https://github.com/...) - Â§öÂπ≥Âè∞ÂèëÂ∏ÉÊ°ÜÊû∂\n- [CosyVoice](https://github.com/tencent-ailab/cosyvoice) - È´òË¥®ÈáèËØ≠Èü≥ÂêàÊàê\n- [HeyGem](https://github.com/...) - Êï∞Â≠ó‰∫∫È©±Âä®ÂºïÊìé\n- [Whisper](https://github.com/openai/whisper) - Á≤æÂáÜËØ≠Èü≥ËØÜÂà´\n\n\n## ‚ö†Ô∏è‰ΩøÁî®ÈôêÂà∂\n- Êú¨È°πÁõÆ‰ªÖÈôê‰∏™‰∫∫Â≠¶‰π†„ÄÅÁ†îÁ©∂‰ΩøÁî®Ôºå‰∏•Á¶Å‰ªª‰ΩïÂΩ¢ÂºèÁöÑÂïÜ‰∏öÁî®ÈÄîÔºàÂåÖÊã¨‰ΩÜ‰∏çÈôê‰∫éÂîÆÂçñÂ∑•ÂÖ∑„ÄÅÊèê‰æõ‰ªòË¥πÊúçÂä°Á≠âÔºâ„ÄÇ\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-10-12T02:22:31.896141"
  },
  {
    "basic_info": {
      "name": "LongLive",
      "full_name": "NVlabs/LongLive",
      "owner": "NVlabs",
      "description": "LongLive: Real-time Interactive Long Video Generation",
      "url": "https://github.com/NVlabs/LongLive",
      "clone_url": "https://github.com/NVlabs/LongLive.git",
      "ssh_url": "git@github.com:NVlabs/LongLive.git",
      "homepage": "https://nvlabs.github.io/LongLive",
      "created_at": "2025-09-22T22:39:24Z",
      "updated_at": "2025-10-12T01:03:21Z",
      "pushed_at": "2025-10-02T17:59:13Z"
    },
    "stats": {
      "stars": 656,
      "forks": 35,
      "watchers": 656,
      "open_issues": 6,
      "size": 442252
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 628543,
        "Shell": 796
      },
      "license": "Other",
      "topics": [
        "efficient-tuning",
        "interactive",
        "long-context",
        "real-time",
        "sparse-attention",
        "video-genenratio"
      ]
    },
    "content": {
      "readme": "<p align=\"center\" style=\"border-radius: 10px\">\n  <img src=\"assets/LongLive-logo.png\" width=\"100%\" alt=\"logo\"/>\n</p>\n\n# üé¨ LongLive: Real-time Interactive Long Video Generation\n\n[![Paper](https://img.shields.io/badge/ArXiv-Paper-brown)](https://arxiv.org/abs/2509.22622)\n[![Code](https://img.shields.io/badge/GitHub-LongLive-blue)](https://github.com/NVlabs/LongLive)\n[![Model](https://img.shields.io/badge/HuggingFace-Model-yellow)](https://huggingface.co/Efficient-Large-Model/LongLive-1.3B)\n[![Video](https://img.shields.io/badge/YouTube-Video-red)](https://www.youtube.com/watch?v=CO1QC7BNvig)\n[![Demo](https://img.shields.io/badge/Demo-Page-bron)](https://nvlabs.github.io/LongLive)\n\n<div align=\"center\">\n\n[![Watch the video](assets/video-first-frame.png)](https://www.youtube.com/watch?v=CO1QC7BNvig)\n[![Watch the video](assets/Comparison_with_Sora2.png)](https://x.com/yukangchen_/status/1973405662177529993)\n\n</div>\n\n## üí° TLDR: Turn interactive prompts into long videos‚Äîinstantly, as you type!\n\n**LongLive: Real-time Interactive Long Video Generation [[Paper](https://arxiv.org/abs/2509.22622)]** <br />\n[Shuai Yang](https://andysonys.github.io/), [Wei Huang](https://aaron-weihuang.com/), [Ruihang Chu](https://ruihang-chu.github.io/), [Yicheng Xiao](https://easonxiao-888.github.io/), [Yuyang Zhao](https://yuyangzhao.com/), [Xianbang Wang](https://peppaking8.github.io/), [Muyang Li](https://lmxyy.me/), [Enze Xie](https://xieenze.github.io/), [Yingcong Chen](https://www.yingcong.me/), [Yao Lu](https://scholar.google.com/citations?user=OI7zFmwAAAAJ&hl=en), [Song Han](http://songhan.mit.edu/), [Yukang Chen](https://yukangchen.com/) <br />\n\nWe present LongLive, a frame-level autoregressive (AR) framework for real-time and interactive long video generation. Long video generation presents challenges in both efficiency and quality. Diffusion and Diffusion-Forcing models can produce high-quality videos but suffer from low efficiency due to bidirectional attention. Causal attention AR models support KV caching for faster inference, but often degrade in quality on long videos due to memory challenges during long-video training. In addition, beyond static prompt-based generation, interactive capabilities, such as streaming prompt inputs, are critical for dynamic content creation, enabling users to guide narratives in real time. This interactive requirement significantly increases complexity, especially in ensuring visual consistency and semantic coherence during prompt transitions. To address these challenges, LongLive adopts a causal, frame-level AR design that integrates a KV-recache mechanism that refreshes cached states with new prompts for smooth, adherent switches; streaming long tuning to enable long video training and to align training and inference (train-long-test-long); and short window attention paired with a frame-level attention sink, shorten as frame sink, preserving long-range consistency while enabling faster generation. With these key designs, LongLive fine-tunes a 1.3B-parameter short-clip model to minute-long generation in just 32 GPU-days. At inference, LongLive sustains 20.7 FPS on a single NVIDIA H100, achieves strong performance on VBench in both short and long videos. LongLive supports up to 240-second videos on a single H100 GPU. LongLive further supports INT8-quantized inference with only marginal quality loss.\n\n## TABLE OF CONTENTS\n1. [News](#news)\n2. [Highlights](#highlights)\n3. [Introduction](#introduction)\n4. [Installation](#installation)\n5. [Inference](#inference)\n6. [Training](#training)\n7. [How to contribute](#how-to-contribute)\n8. [Citation](#citation)\n9. [License](#license)\n10. [Acknowledgement](#acknowledgement)\n\n## News\n- [x] [2025.10.1] We compare Sora2 (+ GPT-5 prompt engineering) with LongLive-1.3B in the interactive long video generation. See [here](https://x.com/yukangchen_/status/1973405662177529993) for details.\n- [x] [2025.9.30] We release [example prompts](https://github.com/NVlabs/LongLive/tree/main/example) to reproduce our demo videos.\n- [x] [2025.9.29] We release [Paper](https://arxiv.org/abs/2509.22622), this GitHub repo [LongLive](https://github.com/NVlabs/LongLive) with all training and inference code, the model weight [LongLive-1.3B](https://huggingface.co/Efficient-Large-Model/LongLive-1.3B), and demo page [Website](https://nvlabs.github.io/LongLive).\n\n## Highlights\n1. **Long Video Gen**: LongLive supports up to 240s video generation, with visual consistency.\n2. **Real-time Inference**: LongLive supports 20.7 FPS generation speed on a single H100 GPU, and 24.8 FPS with FP8 quantization with marginal quality loss.\n3. **Efficient Fine-tuning**: LongLive extends a short-clip model to minute-long generation in 32 H100 GPU-days.\n\n## Introduction\n<p align=\"center\" style=\"border-radius: 10px\">\n  <img src=\"assets/pipeline.jpg\" width=\"100%\" alt=\"logo\"/>\n<strong>LongLive accepts sequential user prompts and generates corresponding videos in real time, enabling user-guided long video gen",
      "default_branch": "main"
    },
    "fetched_at": "2025-10-12T02:22:33.129229"
  },
  {
    "basic_info": {
      "name": "DEIMv2",
      "full_name": "Intellindust-AI-Lab/DEIMv2",
      "owner": "Intellindust-AI-Lab",
      "description": "[DEIMv2] Real Time Object Detection Meets DINOv3 ",
      "url": "https://github.com/Intellindust-AI-Lab/DEIMv2",
      "clone_url": "https://github.com/Intellindust-AI-Lab/DEIMv2.git",
      "ssh_url": "git@github.com:Intellindust-AI-Lab/DEIMv2.git",
      "homepage": "https://intellindust-ai-lab.github.io/projects/DEIMv2/",
      "created_at": "2025-09-19T02:27:19Z",
      "updated_at": "2025-10-12T01:20:52Z",
      "pushed_at": "2025-10-12T01:20:49Z"
    },
    "stats": {
      "stars": 639,
      "forks": 61,
      "watchers": 639,
      "open_issues": 22,
      "size": 533
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 564154,
        "Shell": 2759
      },
      "license": "Other",
      "topics": [
        "detection-transformer",
        "dinov3",
        "object-detection",
        "real-time",
        "real-time-detection"
      ]
    },
    "content": {
      "readme": "<h2 align=\"center\">\n  Real-Time Object Detection Meets DINOv3\n</h2>\n\n<p align=\"center\">\n    <a href=\"https://github.com/Intellindust-AI-Lab/DEIMv2/blob/master/LICENSE\">\n        <img alt=\"license\" src=\"https://img.shields.io/badge/LICENSE-Apache%202.0-blue\">\n    </a>\n    <a href=\"https://arxiv.org/abs/2509.20787\">\n        <img alt=\"arXiv\" src=\"https://img.shields.io/badge/arXiv-2509.20787-red\">\n    </a>\n   <a href=\"https://intellindust-ai-lab.github.io/projects/DEIMv2/\">\n        <img alt=\"project webpage\" src=\"https://img.shields.io/badge/Webpage-DEIMv2-purple\">\n    </a>\n    <a href=\"https://github.com/Intellindust-AI-Lab/DEIMv2/pulls\">\n        <img alt=\"prs\" src=\"https://img.shields.io/github/issues-pr/Intellindust-AI-Lab/DEIMv2\">\n    </a>\n    <a href=\"https://github.com/Intellindust-AI-Lab/DEIMv2/issues\">\n        <img alt=\"issues\" src=\"https://img.shields.io/github/issues/Intellindust-AI-Lab/DEIMv2?color=olive\">\n    </a>\n    <a href=\"https://github.com/Intellindust-AI-Lab/DEIMv2\">\n        <img alt=\"stars\" src=\"https://img.shields.io/github/stars/Intellindust-AI-Lab/DEIMv2\">\n    </a>\n    <a href=\"mailto:shenxi@intellindust.com\">\n        <img alt=\"Contact Us\" src=\"https://img.shields.io/badge/Contact-Email-yellow\">\n    </a>\n</p>\n\n<p align=\"center\">\n    DEIMv2 is an evolution of the DEIM framework while leveraging the rich features from DINOv3. Our method is designed with various model sizes, from an ultra-light version up to S, M, L, and X, to be adaptable for a wide range of scenarios. Across these variants, DEIMv2 achieves state-of-the-art performance, with the S-sized model notably surpassing 50 AP on the challenging COCO benchmark.\n</p>\n\n---\n\n\n<div align=\"center\">\n  <a href=\"http://www.shihuahuang.cn\">Shihua Huang</a><sup>1*</sup>,&nbsp;&nbsp;\n  Yongjie Hou<sup>1,2*</sup>,&nbsp;&nbsp;\n  Longfei Liu<sup>1*</sup>,&nbsp;&nbsp;\n  <a href=\"https://xuanlong-yu.github.io/\">Xuanlong Yu</a><sup>1</sup>,&nbsp;&nbsp;\n  <a href=\"https://xishen0220.github.io\">Xi Shen</a><sup>1‚Ä†</sup>&nbsp;&nbsp;\n</div>\n\n  \n<p align=\"center\">\n<i>\n1. <a href=\"https://intellindust-ai-lab.github.io\"> Intellindust AI Lab</a> &nbsp;&nbsp; 2. Xiamen University &nbsp; <br> \n* Equal Contribution &nbsp;&nbsp; ‚Ä† Corresponding Author\n</i>\n</p>\n\n\n<p align=\"center\">\n<strong>If you like our work, please give us a ‚≠ê!</strong>\n</p>\n\n\n<p align=\"center\">\n  <img src=\"./figures/deimv2_coco_AP_vs_Params.png\" alt=\"Image 1\" width=\"49%\">\n  <img src=\"./figures/deimv2_coco_AP_vs_GFLOPs.png\" alt=\"Image 2\" width=\"49%\">\n</p>\n\n</details>\n\n \n  \n## üöÄ Updates\n- [x] **\\[2025.10.2\\]** [DEIMv2 has been integrated into X-AnyLabeling!](https://github.com/Intellindust-AI-Lab/DEIMv2/issues/25#issue-3473960491) Many thanks to the X-AnyLabeling maintainers for making this possible.\n- [x] **\\[2025.9.26\\]** Release DEIMv2 series.\n\n## üß≠ Table of Content\n* [1. ü§ñ Model Zoo](#1-model-zoo)\n* [2. ‚ö° Quick Start](#2-quick-start)\n* [3. üõ†Ô∏è Usage](#3-usage)\n* [4. üß∞ Tools](#4-tools)\n* [5. üìú Citation](#5-citation)\n* [6. üôè Acknowledgement](#6-acknowledgement)\n* [7. ‚≠ê Star History](#7-star-history)\n  \n  \n## 1. Model Zoo\n\n| Model | Dataset | AP | #Params | GFLOPs | Latency (ms) | config | checkpoint | log |\n| :---: | :---: | :---: | :---: | :---: |:------------:| :---: | :---: | :---: |\n| **Atto** | COCO | **23.8** | 0.5M | 0.8 |     1.10     | [yml](./configs/deimv2/deimv2_hgnetv2_atto_coco.yml) | [ckpt](https://drive.google.com/file/d/18sRJXX3FBUigmGJ1y5Oo_DPC5C3JCgYc/view?usp=sharing) | [log](https://drive.google.com/file/d/1M7FLN8EeVHG02kegPN-Wxf_9BlkghZfj/view?usp=sharing) |\n| **Femto** | COCO | **31.0** | 1.0M | 1.7 |     1.45     | [yml](./configs/deimv2/deimv2_hgnetv2_femto_coco.yml) | [ckpt](https://drive.google.com/file/d/16hh6l9Oln9TJng4V0_HNf_Z7uYb7feds/view?usp=sharing) | [log](https://drive.google.com/file/d/1_KWVfOr3bB5TMHTNOmDIAO-tZJmKB9-b/view?usp=sharing) |\n| **Pico** | COCO | **38.5** | 1.5M | 5.2 |     2.13     | [yml](./configs/deimv2/deimv2_hgnetv2_pico_coco.yml) | [ckpt](https://drive.google.com/file/d/1PXpUxYSnQO-zJHtzrCPqQZ3KKatZwzFT/view?usp=sharing) | [log](https://drive.google.com/file/d/1GwyWotYSKmFQdVN9k2MM6atogpbh0lo1/view?usp=sharing) |\n| **N** | COCO | **43.0** | 3.6M | 6.8 |     2.32     | [yml](./configs/deimv2/deimv2_hgnetv2_n_coco.yml) | [ckpt](https://drive.google.com/file/d/1G_Q80EVO4T7LZVPfHwZ3sT65FX5egp9K/view?usp=sharing) | [log](https://drive.google.com/file/d/1QhYfRrUy8HrihD3OwOMJLC-ATr97GInV/view?usp=sharing) |\n| **S** | COCO | **50.9** | 9.7M | 25.6 |     5.78     | [yml](./configs/deimv2/deimv2_dinov3_s_coco.yml) | [ckpt](https://drive.google.com/file/d/1MDOh8UXD39DNSew6rDzGFp1tAVpSGJdL/view?usp=sharing) | [log](https://drive.google.com/file/d/1ydA4lWiTYusV1s3WHq5jSxIq39oxy-Nf/view?usp=sharing) |\n| **M** | COCO | **53.0** | 18.1M | 52.2 |     8.80     | [yml](./configs/deimv2/deimv2_dinov3_m_coco.yml) | [ckpt](https://drive.google.com/file/d/1nPKDHrotusQ748O1cQXJfi5wdShq6bKp/view?usp=sharing) | [log](https://drive.google.com/file/d/1i05Q1-O9UH-2Vb",
      "default_branch": "main"
    },
    "fetched_at": "2025-10-12T02:22:34.357089"
  },
  {
    "basic_info": {
      "name": "Code2Video",
      "full_name": "showlab/Code2Video",
      "owner": "showlab",
      "description": "Video generation via code",
      "url": "https://github.com/showlab/Code2Video",
      "clone_url": "https://github.com/showlab/Code2Video.git",
      "ssh_url": "git@github.com:showlab/Code2Video.git",
      "homepage": "https://showlab.github.io/Code2Video/",
      "created_at": "2025-09-29T08:15:44Z",
      "updated_at": "2025-10-12T02:00:13Z",
      "pushed_at": "2025-10-11T06:29:05Z"
    },
    "stats": {
      "stars": 621,
      "forks": 80,
      "watchers": 621,
      "open_issues": 0,
      "size": 130216
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 179924,
        "Shell": 2370
      },
      "license": "MIT License",
      "topics": [
        "coding",
        "education",
        "multi-agent",
        "video-generation"
      ]
    },
    "content": {
      "readme": "\n# Code2Video: Video Generation via Code\n\n\n\n<p align=\"center\">\n  <b>Code2Video: A Code-centric Paradigm for Educational Video Generation</b>\n</p>\n<video src=\"assets/video.mp4\" width=\"600\" controls>\n  Your browser does not support the video tag.\n</video>\n\n\n\n\n\n\n<p align=\"center\">\n  <a href=\"https://scholar.google.com.hk/citations?user=9lIMS-EAAAAJ&hl=zh-CN&oi=sra\">Yanzhe Chen*</a>,\n  <a href=\"https://qhlin.me/\">Kevin Qinghong Lin*</a>,\n  <a href=\"https://scholar.google.com/citations?user=h1-3lSoAAAAJ&hl=en\">Mike Zheng Shou</a> <br>\n  Show Lab @ National University of Singapore\n</p>\n\n\n<p align=\"center\">\n¬† <a href=\"https://arxiv.org/abs/2510.01174\">üìÑ Paper</a> &nbsp; | &nbsp;\n¬† <a href=\"https://huggingface.co/papers/2510.01174\">ü§ó Daily Paper</a> &nbsp; | &nbsp;\n¬† <a href=\"https://huggingface.co/datasets/YanzheChen/MMMC\">ü§ó Dataset</a> &nbsp; | &nbsp;\n¬† <a href=\"https://showlab.github.io/Code2Video/\">üåê Project Website</a> &nbsp; | &nbsp;\n¬† <a href=\"https://x.com/KevinQHLin/status/1974199353695941114\">üí¨ X (Twitter)</a>\n</p>\n\nhttps://github.com/user-attachments/assets/d906423f-734a-41c9-b102-b113ad3b3c25\n\n\n\n<!-- <p align=\"center\">\n<table>\n  <thead>\n    <tr>\n      <th style=\"text-align: center;\">Learning Topic</th>\n      <th style=\"text-align: center;\">Veo3</th>\n      <th style=\"text-align: center;\">Wan2.2</th>\n      <th style=\"text-align: center;\">Code2Video (Ours)</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td style=\"text-align: center; vertical-align: middle;\"><strong>Hanoi Problem</strong></td>\n      <td style=\"text-align: center;\">\n        <img src=\"assets/videos/veo/Hanoi.gif\">\n      </td>\n      <td style=\"text-align: center;\">\n        <img src=\"assets/videos/wan/Hanoi.gif\">\n      </td>\n      <td style=\"text-align: center;\">\n        <img src=\"assets/videos/code2video/Hanoi_4K_SpeedUp.gif\">\n      </td>\n    </tr>\n    <tr>\n      <td style=\"text-align: center; vertical-align: middle;\"><strong>Large Language Model</strong></td>\n      <td style=\"text-align: center;\">\n        <img src=\"assets/videos/veo/LLM.gif\">\n      </td>\n      <td style=\"text-align: center;\">\n        <img src=\"assets/videos/wan/LLM.gif\">\n      </td>\n      <td style=\"text-align: center;\">\n        <img src=\"assets/videos/code2video/LLM_speed.gif\">\n      </td>\n    </tr>\n    <tr>\n      <td style=\"text-align: center; vertical-align: middle;\"><strong>Pure Fourier Series</strong></td>\n      <td style=\"text-align: center;\">\n        <img src=\"assets/videos/veo/fourier.gif\">\n      </td>\n      <td style=\"text-align: center;\">\n        <img src=\"assets/videos/wan/fourier.gif\">\n      </td>\n      <td style=\"text-align: center;\">\n        <img src=\"assets/videos/code2video/fourier_speed.gif\">\n      </td>\n    </tr>\n    </tbody>\n</table>\n</p> -->\n\n<p align=\"center\">\n<table style=\"width: 90%; border-collapse: collapse; text-align: center; margin: auto;\">\n  <thead>\n    <tr>\n      <th style=\"text-align: center; padding: 8px;\">Learning Topic</th>\n      <th style=\"text-align: center; padding: 8px;\">Veo3</th>\n      <th style=\"text-align: center; padding: 8px;\">Wan2.2</th>\n      <th style=\"text-align: center; padding: 8px;\">Code2Video (Ours)</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td style=\"text-align: center; vertical-align: middle; font-weight: bold;\">Hanoi Problem</td>\n      <td>\n        <div style=\"width: 320px; aspect-ratio: 16 / 9; overflow: hidden; margin: auto;\">\n          <img src=\"assets/videos/veo/Hanoi.gif\" style=\"width: 100%; height: 100%; object-fit: cover;\">\n        </div>\n      </td>\n      <td>\n        <div style=\"width: 320px; aspect-ratio: 16 / 9; overflow: hidden; margin: auto;\">\n          <img src=\"assets/videos/wan/Hanoi.gif\" style=\"width: 100%; height: 100%; object-fit: cover;\">\n        </div>\n      </td>\n      <td>\n        <div style=\"width: 320px; aspect-ratio: 16 / 9; overflow: hidden; margin: auto;\">\n          <img src=\"assets/videos/code2video/Hanoi_4K_SpeedUp.gif\" style=\"width: 100%; height: 100%; object-fit: cover;\">\n        </div>\n      </td>\n    </tr>\n    <tr>\n      <td style=\"text-align: center; vertical-align: middle; font-weight: bold;\">Large Language Model</td>\n      <td>\n        <div style=\"width: 320px; aspect-ratio: 16 / 9; overflow: hidden; margin: auto;\">\n          <img src=\"assets/videos/veo/LLM.gif\" style=\"width: 100%; height: 100%; object-fit: cover;\">\n        </div>\n      </td>\n      <td>\n        <div style=\"width: 320px; aspect-ratio: 16 / 9; overflow: hidden; margin: auto;\">\n          <img src=\"assets/videos/wan/LLM.gif\" style=\"width: 100%; height: 100%; object-fit: cover;\">\n        </div>\n      </td>\n      <td>\n        <div style=\"width: 320px; aspect-ratio: 16 / 9; overflow: hidden; margin: auto;\">\n          <img src=\"assets/videos/code2video/LLM_speed.gif\" style=\"width: 100%; height: 100%; object-fit: cover;\">\n        </div>\n      </td>\n    </tr>\n    <tr>\n      <td style=\"text-align: center; vertical-align: middle; font-weight: bold;\">Pure Fourier Series</td>\n      <td>\n        <div style=\"width: 320px; ",
      "default_branch": "main"
    },
    "fetched_at": "2025-10-12T02:22:35.631343"
  },
  {
    "basic_info": {
      "name": "Qwen3-ASR-Toolkit",
      "full_name": "QwenLM/Qwen3-ASR-Toolkit",
      "owner": "QwenLM",
      "description": "Official Python toolkit for the Qwen3-ASR API. Parallel high‚Äëthroughput calls, robust long‚Äëaudio transcription, multi‚Äësample‚Äërate support.",
      "url": "https://github.com/QwenLM/Qwen3-ASR-Toolkit",
      "clone_url": "https://github.com/QwenLM/Qwen3-ASR-Toolkit.git",
      "ssh_url": "git@github.com:QwenLM/Qwen3-ASR-Toolkit.git",
      "homepage": "",
      "created_at": "2025-09-16T09:03:49Z",
      "updated_at": "2025-10-11T16:07:02Z",
      "pushed_at": "2025-09-22T10:38:51Z"
    },
    "stats": {
      "stars": 618,
      "forks": 52,
      "watchers": 618,
      "open_issues": 8,
      "size": 15
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 17592
      },
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "# Qwen3-ASR-Toolkit\n\n[![PyPI version](https://badge.fury.io/py/qwen3-asr-toolkit.svg)](https://badge.fury.io/py/qwen3-asr-toolkit)\n[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://www.python.org/downloads/)\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n\nAn advanced, high-performance Python command-line toolkit for using the **Qwen-ASR API** (formerly Qwen3-ASR-Flash). This implementation overcomes the API's 3-minute audio length limitation by intelligently splitting long audio/video files and processing them in parallel, enabling rapid transcription of hours-long content.\n\n## üöÄ Key Features\n\n-   **Break the 3-Minute Limit**: Seamlessly transcribe audio and video files of any length by bypassing the official API's duration constraint.\n-   **Smart Audio Splitting**: Utilizes **Voice Activity Detection (VAD)** to split audio into meaningful chunks at natural silent pauses. This ensures that words and sentences are not awkwardly cut off.\n-   **High-Speed Parallel Processing**: Leverages multi-threading to send audio chunks to the Qwen-ASR API concurrently, dramatically reducing the total transcription time for long files.\n-   **Intelligent Post-Processing**: Automatically detects and removes common ASR **hallucinations and repetitive artifacts** for cleaner, more accurate transcripts.\n-   **SRT Subtitle Generation**: Automatically create timestamped **`.srt` subtitle files** based on VAD segments, perfect for adding captions to video content.\n-   **Automatic Audio Resampling**: Automatically converts audio from any sample rate and channel count to the 16kHz mono format required by the Qwen-ASR API. You can use any audio file without worrying about pre-processing.\n-   **Universal Media Support**: Supports virtually any audio and video format (e.g., `.mp4`, `.mov`, `.mkv`, `.mp3`, `.wav`, `.m4a`) thanks to its reliance on FFmpeg.\n-   **Simple & Easy to Use**: A straightforward command-line interface allows you to get started with just a single command.\n\n## ‚öôÔ∏è How It Works\n\nThis tool follows a robust pipeline to deliver fast and accurate transcriptions for long-form media:\n\n1.  **Media Loading**: The script first loads your media file, whether it's a **local file or a remote URL**.\n2.  **VAD-based Chunking**: It analyzes the audio stream using Voice Activity Detection (VAD) to identify silent segments.\n3.  **Intelligent Splitting**: The audio is then split into smaller chunks based on the detected silences. Each chunk's duration is managed to stay under the 3-minute API limit, with a **user-configurable target length (defaulting to 120 seconds)**, preventing mid-sentence cuts.\n4.  **Parallel API Calls**: A thread pool is initiated to upload and process these chunks concurrently using the DashScope Qwen-ASR API.\n5.  **Result Aggregation & Cleaning**: The transcribed text segments from all chunks are collected, re-ordered, and then **post-processed to remove detected repetitions and hallucinations**.\n6.  **Output Generation**: The final, cleaned transcription is printed to the console and saved to a `.txt` file. **Optionally, a timestamped `.srt` subtitle file can also be generated.**\n\n## üèÅ Getting Started\n\nFollow these steps to set up and run the project on your local machine.\n\n### Prerequisites\n\n-   Python 3.8 or higher.\n-   **FFmpeg**: The script requires FFmpeg to be installed on your system to handle media files.\n    -   **Ubuntu/Debian**: `sudo apt update && sudo apt install ffmpeg`\n    -   **macOS**: `brew install ffmpeg`\n    -   **Windows**: Download from the [official FFmpeg website](https://ffmpeg.org/download.html) and add it to your system's PATH.\n-   **DashScope API Key**: You need an API key from Alibaba Cloud's DashScope.\n    -   You can obtain one from the [DashScope Console](https://dashscope.console.aliyun.com/apiKey). If you are calling the API services of Tongyi Qwen for the first time, you can follow the tutorial on [this website](https://help.aliyun.com/zh/model-studio/first-api-call-to-qwen) to create your own API Key.\n    -   For better security and convenience, it is **highly recommended** to set your API key as an environment variable named `DASHSCOPE_API_KEY`. The script will automatically use it, and you won't need to pass the `--api-key` argument in the command.\n\n        **On Linux/macOS:**\n        ```bash\n        export DASHSCOPE_API_KEY=\"your_api_key_here\"\n        ```\n        *(To make this permanent, add the line to your `~/.bashrc`, `~/.zshrc`, or `~/.profile` file.)*\n\n        **On Windows (Command Prompt):**\n        ```cmd\n        set DASHSCOPE_API_KEY=\"your_api_key_here\"\n        ```\n\n        **On Windows (PowerShell):**\n        ```powershell\n        $env:DASHSCOPE_API_KEY=\"your_api_key_here\"\n        ```\n        *(For a permanent setting on Windows, search for \"Edit the system environment variables\" in the Start Menu and add `DASHSCOPE_API_KEY` to your user variables.)*\n\n### Installation\n\nWe recommen",
      "default_branch": "main"
    },
    "fetched_at": "2025-10-12T02:22:36.886408"
  },
  {
    "basic_info": {
      "name": "unifolm-world-model-action",
      "full_name": "unitreerobotics/unifolm-world-model-action",
      "owner": "unitreerobotics",
      "description": null,
      "url": "https://github.com/unitreerobotics/unifolm-world-model-action",
      "clone_url": "https://github.com/unitreerobotics/unifolm-world-model-action.git",
      "ssh_url": "git@github.com:unitreerobotics/unifolm-world-model-action.git",
      "homepage": null,
      "created_at": "2025-09-12T13:51:15Z",
      "updated_at": "2025-10-12T00:34:13Z",
      "pushed_at": "2025-10-01T02:13:04Z"
    },
    "stats": {
      "stars": 599,
      "forks": 48,
      "watchers": 599,
      "open_issues": 6,
      "size": 123064
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 814096,
        "Shell": 3231
      },
      "license": "Other",
      "topics": []
    },
    "content": {
      "readme": "# UnifoLM-WMA-0: A World-Model-Action (WMA) Framework under UnifoLM Family\n<p style=\"font-size: 1.2em;\">\n    <a href=\"https://unigen-x.github.io/unifolm-world-model-action.github.io\"><strong>Project Page</strong></a> | \n    <a href=\"https://huggingface.co/collections/unitreerobotics/unifolm-wma-0-68ca23027310c0ca0f34959c\"><strong>Models</strong></a> |\n    <a href=\"https://huggingface.co/unitreerobotics/datasets\"><strong>Dataset</strong></a> \n  </p>\n<div align=\"center\">\n  <p align=\"right\">\n    <span> üåéEnglish </span> | <a href=\"README_cn.md\"> üá®üá≥‰∏≠Êñá </a>\n  </p>\n</div>\n<div align=\"justify\">\n    <b>UnifoLM-WMA-0</b> is Unitree‚Äòs open-source world-model‚Äìaction architecture spanning multiple types of robotic embodiments, designed specifically for general-purpose robot learning. Its core component is a world-model capable of understanding the physical interactions between robots and the environments. This world-model provides two key functions: (a) <b>Simulation Engine</b> ‚Äì operates as an interactive simulator to generate synthetic data for robot learning; (b) <b>Policy Enhancement</b> ‚Äì connects with an action head and, by predicting future interaction processes with the world-model, further optimizes decision-making performance.\n</div>\n\n## ü¶æ Real-Robot Demonstrations\n| <img src=\"assets/gifs/real_z1_stackbox.gif\" style=\"border:none;box-shadow:none;margin:0;padding:0;\" /> | <img src=\"assets/gifs/real_dual_stackbox.gif\" style=\"border:none;box-shadow:none;margin:0;padding:0;\" /> |\n|:---:|:---:|\n| <img src=\"assets/gifs/real_cleanup_pencils.gif\" style=\"border:none;box-shadow:none;margin:0;padding:0;\" /> | <img src=\"assets/gifs/real_g1_pack_camera.gif\" style=\"border:none;box-shadow:none;margin:0;padding:0;\" /> |\n\n**Note: the top-right window shows the world model‚Äôs pretion of future action videos.**\n\n## üî• News\n\n* Sep 22, 2025: üöÄ We released the deployment code for assisting experiments with [Unitree](https://www.unitree.com/) robots.\n* Sep 15, 2025: üöÄ We released the training and inference code along with the model weights of [**UnifoLM-WMA-0**](https://huggingface.co/collections/unitreerobotics/unifolm-wma-0-68ca23027310c0ca0f34959c).\n\n## üìë Opensource Plan\n- [x] Training \n- [x] Inference\n- [x] Checkpoints\n- [x] Deployment\n\n## ‚öôÔ∏è  Installation\n```\nconda create -n unifolm-wma python==3.10.18\nconda activate unifolm-wma\n\nconda install pinocchio=3.2.0 -c conda-forge -y\nconda install ffmpeg=7.1.1 -c conda-forge\n\ngit clone --recurse-submodules https://github.com/unitreerobotics/unifolm-world-model-action.git\n\n# If you already downloaded the repo:\ncd unifolm-world-model-action\ngit submodule update --init --recursive\n\npip install -e .\n\ncd external/dlimp\npip install -e .\n```\n## üß∞ Model Checkpoints\n| Model | Description | Link|\n|---------|-------|------|\n|$\\text{UnifoLM-WMA-0}_{Base}$| Fine-tuned on [Open-X](https://robotics-transformer-x.github.io/) dataset. | [HuggingFace](https://huggingface.co/unitreerobotics/UnifoLM-WMA-0-Base)|\n|$\\text{UnifoLM-WMA-0}_{Dual}$| Fine-tuned on five [Unitree opensource dataset](https://huggingface.co/collections/unitreerobotics/g1-dex1-datasets-68bae98bf0a26d617f9983ab) in both decision-making and simulation modes. | [HuggingFace](https://huggingface.co/unitreerobotics/UnifoLM-WMA-0-Dual)|\n\n## üõ¢Ô∏è Dataset\nIn our experiments, we consider the following three opensource dataset:\n| Dataset | Robot | Link |\n|---------|-------|------|\n|Z1_StackBox| [Unitree Z1](https://www.unitree.com/z1)|[Huggingface](https://huggingface.co/datasets/unitreerobotics/Z1_StackBox_Dataset/tree/v2.1)|\n|Z1_DualArm_StackBox|[Unitree Z1](https://www.unitree.com/z1)|[Huggingface](https://huggingface.co/datasets/unitreerobotics/Z1_Dual_Dex1_StackBox_Dataset/tree/v2.1)|\n|Z1_DualArm_StackBox_V2|[Unitree Z1](https://www.unitree.com/z1)|[Huggingface](https://huggingface.co/datasets/unitreerobotics/Z1_Dual_Dex1_StackBox_Dataset_V2/tree/v2.1)|\n|Z1_DualArm_Cleanup_Pencils|[Unitree Z1](https://www.unitree.com/z1)|[Huggingface](https://huggingface.co/datasets/unitreerobotics/Z1_Dual_Dex1_CleanupPencils_Dataset/tree/v2.1)|\n|G1_Pack_Camera|[Unitree G1](https://www.unitree.com/g1)|[Huggingface](https://huggingface.co/datasets/unitreerobotics/G1_Dex1_MountCameraRedGripper_Dataset/tree/v2.1)|\n\nTo train on your own dataset, first to have the data following the [Huggingface LeRobot V2.1](https://github.com/huggingface/lerobot) dataset format. Assume the dataset‚Äôs source directory structure is as follows:\n```\nsource_dir/\n    ‚îú‚îÄ‚îÄ dataset1_name\n    ‚îú‚îÄ‚îÄ dataset2_name\n    ‚îú‚îÄ‚îÄ dataset3_name\n    ‚îî‚îÄ‚îÄ ...\n```\nThen, convert a dataset to the required format using the command below:\n```python\ncd prepare_data\npython prepare_training_data.py \\\n    --source_dir /path/to/your/source_dir \\\n    --target_dir /path/to/save/the/converted/data \\\n    --dataset_name \"dataset1_name\" \\\n    --robot_name \"a tag of the robot in the dataset\" # e.g, Unitree Z1 Robot Arm or Unitree G1 Robot with Gripper.\n```\nThe resulting data structure (Note: model training only supports i",
      "default_branch": "main"
    },
    "fetched_at": "2025-10-12T02:22:38.160467"
  },
  {
    "basic_info": {
      "name": "crypto-tax-calculator",
      "full_name": "Uak0/crypto-tax-calculator",
      "owner": "Uak0",
      "description": "An advanced cryptocurrency & personal income tax calculator. ",
      "url": "https://github.com/Uak0/crypto-tax-calculator",
      "clone_url": "https://github.com/Uak0/crypto-tax-calculator.git",
      "ssh_url": "git@github.com:Uak0/crypto-tax-calculator.git",
      "homepage": "",
      "created_at": "2025-10-07T15:53:38Z",
      "updated_at": "2025-10-11T17:06:55Z",
      "pushed_at": "2025-10-11T17:08:30Z"
    },
    "stats": {
      "stars": 593,
      "forks": 174,
      "watchers": 593,
      "open_issues": 0,
      "size": 366
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 650911
      },
      "license": "GNU Affero General Public License v3.0",
      "topics": [
        "bitcoin",
        "crypto",
        "crypto-tax-reports",
        "cryptocurrency",
        "cryptotax",
        "tax",
        "tax-calculation",
        "tax-calculator",
        "us-tax"
      ]
    },
    "content": {
      "readme": "# Crypto Tax Calc \n[![Twitter Follow](https://img.shields.io/twitter/url/https/twitter.com/cloudposse.svg?style=social&label=Follow%20@bartoMer177)](https://x.com/bartoMerl77)\n[![License: AGPL v3](https://img.shields.io/badge/License-AGPL_v3-blue.svg)](https://www.gnu.org/licenses/agpl-3.0)\n[![Donate](https://img.shields.io/badge/Donate-PayPal-black.svg)](https://www.paypal.com/donate?business=)\n\n# Introduction\nCrypto Tax Calculator is an open-source model with UIE (Unified Import Engine) support for crypto and personal income tax calculation. It is designed for individuals, accountants, and organizations that require transparency, precision, and compliance across multiple tax jurisdictions. CryptoTaxCalc helps users consolidate all crypto activity ‚Äî trades, transfers, staking, airdrops, mining rewards, NFT sales, and more ‚Äî into a clear, tax-compliant report.\n\n# Getting Started\n**To use Crypto-Tax-Calc, your machine must meet the following requirements:**\n1. Windows/MacOS\n2. Git\n3. At least 4GB of RAM\n4. Python 3.10+ (all versions above are supported)\n   \n**To install the program on your machine, follow these instructions:**\n1. Install the program on your machine.\n```bash\ngit clone https://github.com/Uak0/crypto-tax-calculator\n```\n2. Setup the program.\n```bash\ncd crypto-tax-calculator\npython setup.py\n```\nThis will install all the required packages and prepare the program to work out of the box.\n\n# Configuration\nEdit the crypto_tax_calculator.conf file to configure the program. All the parameters used in the configuration are listed below.\n\n| Parameter | Default Value | Description |\n|------------|----------------|--------------|\n| **base_currency** | `'USD'` | Main fiat currency used for all calculations and final reports. You can change it to `'EUR'`, `'GBP'`, etc. |\n| **timezone** | `'Europe/London'` | Local timezone applied to all transaction timestamps. Must be a valid [IANA timezone string](https://en.wikipedia.org/wiki/List_of_tz_database_time_zones). |\n| **date_is_day_first** | `True` | Defines date format: `True` ‚Üí DD/MM/YYYY, `False` ‚Üí MM/DD/YYYY. |\n| **fiat_currencies** | `[USD, EUR, GBP, JPY, AUD]` | List of fiat currencies used in your transaction records. Add or remove as needed based on your data. |\n| **crypto_currencies** | `[BTC, ETH, USDT, BNB, SOL, XRP, DOGE]` | List of supported cryptoassets. Most major cryptocurrencies are supported ‚Äî you can freely extend this list. |\n| **price_data_sources.fiat** | `[ExchangeRateAPI, ECB]` | APIs used to fetch historical fiat currency exchange rates for conversion. |\n| **price_data_sources.crypto** | `[CoinGecko, CryptoCompare]` | APIs used to retrieve historical cryptocurrency prices. Multiple sources improve accuracy and redundancy. |\n| **trade_value_method** | `2` | Determines how the trade value is calculated:<br>‚Ä¢ `0` ‚Äì use buy-side value<br>‚Ä¢ `1` ‚Äì use sell-side value<br>‚Ä¢ `2` ‚Äì use priority value *(recommended)* |\n| **fee_handling_mode** | `2` | Defines how transaction fees are treated:<br>‚Ä¢ `0` ‚Äì ignore fees<br>‚Ä¢ `1` ‚Äì treat as expense<br>‚Ä¢ `2` ‚Äì include in cost basis *(recommended)* |\n| **transfer_fees_taxable** | `True` | If `True`, transfer fees (e.g. blockchain fees) are treated as taxable disposals. |\n| **include_transfers** | `False` | Whether to include wallet-to-wallet transfers in tax calculations. Usually disabled to avoid double counting. |\n| **lost_tokens_as_loss** | `True` | If enabled, tokens marked as *lost* or *burned* are considered realized capital losses. |\n| **show_empty_wallets** | `False` | Whether to display wallets with zero balance in generated reports. |\n| **hide_zero_balances** | `True` | Hides assets with a zero total value from the reports for better readability. |\n| **optimize_large_data** | `False` | Enables optimization for large transaction datasets. Increases speed at the cost of higher memory usage. |\n| **debug_mode** | `False` | Enables verbose logging for debugging and troubleshooting. Recommended only during testing. |\n\n# Usage\nYou can run Crypto Tax Calc from the command line or use it interactively.\nThe program automatically detects the exchange source, converts data, fetches historical prices, and calculates taxes.\n\n**Option 1 ‚Äî Direct File Processing (Recommended)**\n\nIf you have an exchange export (e.g. Binance, Kraken, Coinbase), simply run:\n\n```bash\ncryptotaxcalc ./data/binance_2024.csv\n```\n\nThe program will automatically:\n\n- Detect the source exchange\n- Convert the file to a unified internal format\n- Fetch historical prices\n- Calculate capital gains and personal income taxes\n- Generate a PDF/CSV report\n\nOutput example:\n```bash\nReport successfully generated: ./reports/tax_report_2024.pdf\n```\n**Option 2 ‚Äî Interactive Mode**\n\nYou can also start an interactive session without arguments:\n```bash\ncryptotaxcalc\n```\nThis will open the CLI interface.\n\n\n# Supported crypto exchanges and wallets:\n- [X] [Binance](https://www.binance.com/)\n- [X] [Bitmart](https://bitmart.com/)\n- [X] [MEXC](https://mexc.com/)\n- [X",
      "default_branch": "main"
    },
    "fetched_at": "2025-10-12T02:22:39.398571"
  },
  {
    "basic_info": {
      "name": "craftgpt",
      "full_name": "sammyuri/craftgpt",
      "owner": "sammyuri",
      "description": "Small language model built in Minecraft.",
      "url": "https://github.com/sammyuri/craftgpt",
      "clone_url": "https://github.com/sammyuri/craftgpt.git",
      "ssh_url": "git@github.com:sammyuri/craftgpt.git",
      "homepage": null,
      "created_at": "2025-09-28T13:23:19Z",
      "updated_at": "2025-10-12T01:20:37Z",
      "pushed_at": "2025-09-28T14:21:27Z"
    },
    "stats": {
      "stars": 589,
      "forks": 33,
      "watchers": 589,
      "open_issues": 10,
      "size": 5621
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 22427
      },
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "# CraftGPT\n\nA small language model built to run in Minecraft, trained on the [TinyChat dataset](https://huggingface.co/datasets/starhopp3r/TinyChat).\n\nBefore attempting to run CraftGPT, please be aware that you shouldn't have high expectations. The model is very prone to going off topic, producing responses that are not grammatically correct, or simply outputting garbage. The model also has a very small context window of only 64 tokens. The conversations in the showcase video show the model at its best, not necessarily at its average performance.\n\n[MCHPRS](https://github.com/MCHPR/MCHPRS) is essential for running CraftGPT within a reasonable amount of time. It's built using vanilla redstone mechanics, and should work in vanilla, but it could take upwards of 10 years to generate a response without increasing the tick rate. So if you don't have that long to wait, follow the instructions to install MCHPRS first.\n\nEven with MCHPRS it can still take hours to generate a response, so I also strongly recommend you try inputting your prompt on the emulator first (and potentially try some different RNG seeds).\n\n## How to run it\n\nYou will need a machine with at least 32GB of RAM to even load the server, but ideally you'd want 64GB or more.\n\n- Download MCHPRS, set the plot scale to 7 (this can be found at `./crates/core/src/plot/mod.rs`) and compile it.\n- Download the MCHPRS world, unpack it at `./target/release`, and rename it to `world`.\n- Log on to MCHPRS (version 1.20.4) and type `/rp c -io`. (The `-io` flags enable optimised compilation and prevent sending non-input/output block updates to the player. You can run it without these flags in order to see the redstone update, but it will be significantly slower.) It takes about 10 minutes to compile on my machine.\n- Type `/rtps unlimited` and `/wsr 1`.\n- The default RNG seed is `1`. If you want to enter a different one, enter it in binary at `230, 150, 1000` and push the button to confirm.\n- Type in your prompt and hit the enter key. Wait a couple hours for the response to be generated; the progress bar shows progress on the current token, and the binary counter shows the number of tokens processed so far. Once it's done, you can enter another prompt.\n\nThere's no reset or backspace button. If you want to reset it, the quickest way is just to load a fresh copy of the world, although it can be manually reset by pushing the button behind the screen, the buttons at all the attention block token counters, and clearing the input buffers.",
      "default_branch": "main"
    },
    "fetched_at": "2025-10-12T02:22:40.651201"
  },
  {
    "basic_info": {
      "name": "OpenTrack",
      "full_name": "GalaxyGeneralRobotics/OpenTrack",
      "owner": "GalaxyGeneralRobotics",
      "description": "Official implementation of OpenTrack.",
      "url": "https://github.com/GalaxyGeneralRobotics/OpenTrack",
      "clone_url": "https://github.com/GalaxyGeneralRobotics/OpenTrack.git",
      "ssh_url": "git@github.com:GalaxyGeneralRobotics/OpenTrack.git",
      "homepage": "",
      "created_at": "2025-09-19T07:11:12Z",
      "updated_at": "2025-10-11T15:25:36Z",
      "pushed_at": "2025-09-27T12:01:59Z"
    },
    "stats": {
      "stars": 582,
      "forks": 29,
      "watchers": 582,
      "open_issues": 8,
      "size": 15433
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 257606
      },
      "license": null,
      "topics": []
    },
    "content": {
      "readme": "<div align=\"center\">\n  <h1 align=\"center\"> OpenTrack </h1>\n  <h3 align=\"center\"> GALBOT ¬∑ Tsinghua </h3>\n<!--   <p align=\"center\">\n    <a href=\"README.md\"> English </a> | <a href=\"README_zh.md\">‰∏≠Êñá</a>\n  </p>     -->\n\n:page_with_curl:[Paper](https://arxiv.org/abs/2509.13833) | :house:[Website](https://zzk273.github.io/Any2Track/)\n\n\nThis repository is the official implementation of OpenTrack, an open-source humanoid motion tracking codebase that uses MuJoCo for simulation and supports multi-GPU parallel training.\n</div>\n\n# Prepare\n\n1. Clone the repository:\n   ```shell\n   git clone git@github.com:GalaxyGeneralRobotics/OpenTrack.git\n   ```\n\n2. Create a virtual environment and install dependencies:\n   ```shell\n   conda create -n any2track python=3.12\n   conda activate any2track\n   # Install torch to convert JAX to Torch. We don't require the GPU version of torch, but you can install any version as you like.\n   pip install torch==2.5.1 torchvision==0.20.1 torchaudio==2.5.1 --index-url https://download.pytorch.org/whl/cpu\n   pip install -r requirements.txt\n   ```\n\n5. Download the [mocap data](https://huggingface.co/datasets/robfiras/loco-mujoco-datasets/tree/main/Lafan1/mocap/UnitreeG1) and put them under `data/mocap/`. Thanks for the retargeting motions of LAFAN1 dataset from [LocoMuJoCo](https://github.com/robfiras/loco-mujoco/)!\n\n   The file structure should be like:\n\n   ```\n   data/\n   |-- xmls\n      |- ...\n   |-- mocap\n      |-- lafan1\n         |-- UnitreeG1\n               |-- dance1_subject1.npz\n               |--- ...\n   ```\n\n## Usage ##\n\n1. Train the model\n   ```shell\n   # Train on a flat terrain:\n   python train_policy.py --exp_name flat_terrain --terrain_type flat_terrain\n   # Train on a rough terrain:\n   python generate_terrain.py # generate various hfield with Perlin noise\n   python train_policy.py --exp_name rough_terrain --terrain_type rough_terrain\n   \n   # For debug mode (quick testing training without logging)\n   # python train_policy.py --exp_name debug \n   ```\n\n2. Evaluate the model\n   First, convert the Brax model checkpoint to PyTorch:\n   ```shell\n   # your_exp_name=<timestamp>_<exp_name>\n   python brax2torch.py --exp_name <your_exp_name>\n   ```\n\n   Next, run the evaluation script:\n   \n   ```shell\n   # your_exp_name=<timestamp>_<exp_name>\n   python play_policy.py --exp_name <your_exp_name> [--use_viewer] [--use_renderer] [---play_ref_motion]\n   ```\n\n# TODOs\n\n- [x] Release AnyTracker\n- [x] Release dynamics disturbances\n- [ ] Release AnyAdapter\n- [ ] Release real deployment code\n   \n## Acknowledgement\n\nThis repository is build upon `jax`, `brax`, `loco-mujoco`, and `mujoco_playground`.\n\nIf you find this repository helpful, please cite our work:\n\n```bibtex\n@article{zhang2025track,\n  title={Track Any Motions under Any Disturbances},\n  author={Zhikai Zhang and Jun Guo and Chao Chen and Jilong Wang and Chenghuai Lin and Yunrui Lian and Han Xue and Zhenrong Wang and Maoqi Liu and Huaping Liu and He Wang and Li Yi},\n  journal={arXiv preprint arXiv:2509.13833},\n  year={2025}\n}\n```",
      "default_branch": "main"
    },
    "fetched_at": "2025-10-12T02:22:41.908059"
  },
  {
    "basic_info": {
      "name": "Lucy-Edit-ComfyUI",
      "full_name": "DecartAI/Lucy-Edit-ComfyUI",
      "owner": "DecartAI",
      "description": null,
      "url": "https://github.com/DecartAI/Lucy-Edit-ComfyUI",
      "clone_url": "https://github.com/DecartAI/Lucy-Edit-ComfyUI.git",
      "ssh_url": "git@github.com:DecartAI/Lucy-Edit-ComfyUI.git",
      "homepage": null,
      "created_at": "2025-09-17T13:24:52Z",
      "updated_at": "2025-10-11T06:25:20Z",
      "pushed_at": "2025-09-22T13:38:59Z"
    },
    "stats": {
      "stars": 579,
      "forks": 64,
      "watchers": 579,
      "open_issues": 10,
      "size": 11267
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 12846
      },
      "license": null,
      "topics": []
    },
    "content": {
      "readme": "# Lucy Edit - ComfyUI\n\n<p align=\"center\">\n  <img src=\"assets/logo.png\" width=\"680\" alt=\"Lucy Edit Dev Logo\"/>\n</p>\n\n<p align=\"center\">\n  üß™ <a href=\"http://github.com/DecartAI/lucy-edit-comfyui\"><b>GitHub</b></a>\n  &nbsp;|&nbsp; ü§ó <a href=\"https://huggingface.co/decart-ai/Lucy-Edit-Dev\">Huggingface</a>\n  &nbsp;|&nbsp; üìñ <a href=\"https://platform.decart.ai\">Playground</a>\n  &nbsp;|&nbsp; üìë <a href=\"https://d2drjpuinn46lb.cloudfront.net/Lucy_Edit__High_Fidelity_Text_Guided_Video_Editing.pdf\">Technical Report</a>\n  &nbsp;|&nbsp; üí¨ <a href=\"https://discord.gg/decart\">Discord</a>\n</p>\n\n---\n\n<img width=\"2559\" height=\"812\" alt=\"image\" src=\"https://github.com/user-attachments/assets/291f41d2-f4a4-4d36-a0cf-f73a05fd0a0c\" />\n\n\n<div align=\"center\">\n\n<table>\n<tr>\n<td align=\"center\">\n  <video src=\"https://github.com/user-attachments/assets/5084db41-be23-47a2-97a2-4f6bf7229809\" width=\"100%\" controls>\n    Your browser does not support the video tag.\n  </video>\n  <br/>\n  <em>Put the woman in gothic black jeans and leather jacket and crop top under it.</em>\n</td>\n<td align=\"center\">\n  <video src=\"https://github.com/user-attachments/assets/f72e58e1-f00b-45a7-a2d4-28bea2aad11c\" width=\"100%\" controls>\n    Your browser does not support the video tag.\n  </video>\n  <br/>\n  <em>1.2) Put her in a clown outfit.</em>\n</td>\n<td align=\"center\">\n  <video src=\"https://github.com/user-attachments/assets/51263d11-66e9-4bdc-a41d-b59ee628332d\" width=\"100%\" controls>\n    Your browser does not support the video tag.\n  </video>\n  <br/>\n  <em>1.3) Put the woman in a red bikini with an open thick coat above it.</em>\n</td>\n</tr>\n</table>\n</div>\n\n\n**Lucy Edit** is a **video editing** model that performs **instruction-guided edits** on videos using free-text prompts ‚Äî it supports a variety of edits, such as **clothing & accessory changes**, **character changes**, **object insertions**, and **scene replacements** while preserving the motion and composition perfectly.\n\n- üèÉ‚Äç‚ôÇÔ∏è **Motion Preservation** - preserves the motion and composition of videos perfectly, allowing precise edits.\n- üéØ **Edit reliability** ‚Äî edits are more robust when compared to common inference time methods.\n- üß¢ **Wardrobe & accessories** ‚Äî change outfits, add glasses/earrings/hats/etc.\n- üßå **Character Changes** ‚Äî replace characters with monsters, animals and known characters. (e.g., \"Replace the person with a polar bear\")\n- üó∫Ô∏è **Scenery swap** ‚Äî move the scene (e.g., \"transform the scene into a 2D cartoon,\")  \n- üìù **Pure text instructions** ‚Äî no finetuning, no masks required for common edits  \n\n---\n\n## üõ†Ô∏è Quickstart\n\n### Installation\n\n1. Clone this repo into custom_nodes folder.\n1. Install dependencies: pip install -r requirements.txt\n\n### Download Model Weights\n\n1. Download the appropriate weights for your setup:\n\n   * **FP16 weights**:  \n     https://huggingface.co/decart-ai/Lucy-Edit-Dev-ComfyUI/resolve/main/lucy-edit-dev-cui-fp16.safetensors\n\n   * **FP32 weights**:  \n     https://huggingface.co/decart-ai/Lucy-Edit-Dev-ComfyUI/resolve/main/lucy-edit-dev-cui.safetensors\n\n2. Place the weights under: `models/diffusion_models/`\n\n### Usage\nPlease refer to the \"Prompting Guidelines & Supported Edits\" section for the best experience.\n\n#### Lucy Edit Pro (API)\n1. Load the workflow from `examples/basic-api-lucy-edit.json`.\n1. Get an api key from: https://platform.decart.ai/.\n\n\n#### Lucy Edit Dev (Local)\n1. Load the workflow from `examples/basic-lucy-edit-dev.json`\n\n## üé¨ Demos\n\n<div align=\"center\">\n### Sample 1\n<table>\n<tr>\n<td align=\"center\">\n  <video src=\"https://github.com/user-attachments/assets/0ac94178-ce03-4e9d-9326-676fe6146bc6\" width=\"100%\" controls>\n    Your browser does not support the video tag.\n  </video>\n  <br/>\n  <em>1.1) Replace the man with an alien wearing the same leather jacket.</em>\n</td>\n<td align=\"center\">\n  <video src=\"https://github.com/user-attachments/assets/78275b81-04b4-4ee7-afa2-79fdcf54b688\" width=\"100%\" controls>\n    Your browser does not support the video tag.\n  </video>\n  <br/>\n  <em>1.2) Replace the man witha polar bear.</em>\n</td>\n<td align=\"center\">\n  <video src=\"https://github.com/user-attachments/assets/3ad89caa-8b89-4322-a1ef-e92df45c907a\" width=\"100%\" controls>\n    Your browser does not support the video tag.\n  </video>\n  <br/>\n  <em>1.3) Make it snow.</em>\n</td>\n</tr>\n</table>\n\n### Sample 2\n<table>\n<tr>\n<td align=\"center\">\n  <video src=\"https://github.com/user-attachments/assets/443c36a8-dfc9-4a11-8873-4ed4985753ee\" width=\"100%\" controls>\n    Your browser does not support the video tag.\n  </video>\n  <br/>\n  <em>2.1) Replace the woman with Harley Quinn with full make up and a shirt with \"Daddy's Lil Monster\" written on it.</em>\n</td>\n<td align=\"center\">\n  <video src=\"https://github.com/user-attachments/assets/e9654e91-e0f4-479e-8632-d567178ea72f\" width=\"100%\" controls>\n    Your browser does not support the video tag.\n  </video>\n  <br/>\n  <em>2.2) Replace the girl with a lego character.</em>\n</td>\n<td align=\"center\">\n  <video src=\"https://github.com/",
      "default_branch": "main"
    },
    "fetched_at": "2025-10-12T02:22:43.152522"
  },
  {
    "basic_info": {
      "name": "MimicKit",
      "full_name": "xbpeng/MimicKit",
      "owner": "xbpeng",
      "description": "Suite of motion imitation methods for training motion controllers.",
      "url": "https://github.com/xbpeng/MimicKit",
      "clone_url": "https://github.com/xbpeng/MimicKit.git",
      "ssh_url": "git@github.com:xbpeng/MimicKit.git",
      "homepage": "",
      "created_at": "2025-10-08T15:33:01Z",
      "updated_at": "2025-10-11T21:00:00Z",
      "pushed_at": "2025-10-11T17:38:26Z"
    },
    "stats": {
      "stars": 570,
      "forks": 40,
      "watchers": 570,
      "open_issues": 4,
      "size": 8739
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 340321
      },
      "license": "BSD 3-Clause \"New\" or \"Revised\" License",
      "topics": []
    },
    "content": {
      "readme": "# MimicKit\n\n\n![Teaser](images/MimicKit_teaser.gif)\n\nThis framework provides a suite of motion imitation methods for training motion controllers. This codebase includes implementations of:\n- [DeepMimic](https://xbpeng.github.io/projects/DeepMimic/index.html)\n- [AMP](https://xbpeng.github.io/projects/AMP/index.html)\n- [ASE](https://xbpeng.github.io/projects/ASE/index.html)\n- [ADD](https://xbpeng.github.io/projects/ADD/index.html)\n\nWe also include the following RL algorithms:\n- [PPO](https://arxiv.org/abs/1707.06347)\n- [AWR](https://xbpeng.github.io/projects/AWR/index.html)\n\n## Installation\n\nInstall IsaacGym: https://developer.nvidia.com/isaac-gym\n\nInstall requirements:\n```\npip install -r requirements.txt\n```\nDownload assets and motion data from [here](https://1sfu-my.sharepoint.com/:u:/g/personal/xbpeng_sfu_ca/EclKq9pwdOBAl-17SogfMW0Bved4sodZBQ_5eZCiz9O--w?e=bqXBaa), then extract the contents into [`data/`](data/).\n\n\n## Training\n\nTo train a model, run the following command:\n```\npython mimickit/run.py --mode train --num_envs 4096 --env_config data/envs/deepmimic_humanoid_env.yaml --agent_config data/agents/deepmimic_humanoid_ppo_agent.yaml --visualize true --log_file output/log.txt --out_model_file output/model.pt\n```\n- `--mode` selects either `train` or `test` mode.\n- `--num_envs` specifies the number of parallel environments used for simulation.\n- `--env_config` specifies the configuration file for the environment.\n- `--agent_config` specifies configuration file for the agent.\n- `--visualize` enables visualization. Rendering should be disabled for faster training.\n- `--log_file` specifies the output log file, which will keep track of statistics during training.\n- `--out_model_file` specifies the output model file, which contains the model parameters.\n- `--logger` specifies the logger used to record training stats. The options are tensorboard `tb` or `wandb`.\n\nInstead of specifying all arguments through the command line, arguments can also be loaded from an `arg_file`:\n```\npython mimickit/run.py --arg_file args/deepmimic_humanoid_ppo_args.txt --visualize true\n```\nThe arguments in `arg_file` are treated the same as command line arguments. Arguments for all algorithms are provided in [`args/`](args/).\n\n\n## Testing\n\nTo test a model, run the following command:\n```\npython mimickit/run.py --arg_file args/deepmimic_humanoid_ppo_args.txt --num_envs 4 --visualize true --mode test --model_file data/models/deepmimic_humanoid_spinkick_model.pt\n```\n- `--model_file` specifies the `.pt` file that contains the parameters of the trained model. Pretrained models are available in [`data/models/`](data/models/), and the corresponding training log files are available in [`data/logs/`](data/logs/).\n\n\n## Distributed Training\n\nTo use distributed training with multi-CPU or multi-GPU:\n```\npython mimickit/run.py --arg_file args/deepmimic_humanoid_ppo_args.txt --num_workers 2 --device cuda:0\n```\n- `--num_workers` specifies the number of worker processes used to parallelize training. \n- `--device` specifies the device used for training, which can be `cpu` or `cuda:0`. When training with multiple GPUs, the number of worker processes used to parallelize training must be less than or equal to the number of GPUs available on the system.\n\n## Visualizing Training Logs\n\nWhen using the tensorboard logger during training, a tensorboard `events` file will be saved the same output directory as the log file. The log can be viewed with:\n```\ntensorboard --logdir=output/ --port=6006 --bind_all --samples_per_plugin scalars=999999\n```\nThe output log `.txt` file can also be plotted using the plotting script [`plot_log.py`](tools/plot_log/plot_log.py).\n\n\n## Motion Data\nMotion data is stored in [`data/motions/`](data/motions/). The `motion_file` field in the environment configuration file can be used to specify the reference motion clip. In addition to imitating individual motion clips, `motion_file` can also specify a dataset file, located in [`data/datasets/`](data/datasets/), which will train a model to imitate a dataset containing multiple motion clips.\n\nThe `view_motion` environment can be used to visualize motion clips:\n```\npython mimickit/run.py --mode test --arg_file args/view_motion_humanoid_args.txt --visualize true\n```\n\nMotion clips are represented by the `Motion` class implemented in [`motion.py`](mimickit/anim/motion.py). Each motion clip is stored in a `.pkl` file. Each frame in the motion specifies the pose of the character according to\n```\n[root position (3D), root rotation (3D), joint rotations]\n```\nwhere 3D rotations are specified using 3D exponential maps. Joint rotations are recorded in the order that the joints are specified in the `.xml` file (i.e. depth-first traversal of the kinematic tree). For example, in the case of [`humanoid.xml`](data/assets/humanoid.xml), each frame is represented as\n```\n[root position (3D), root rotation (3D), abdomen (3D), neck (3D), right_shoulder (3D), right_elbow (1D), left_shoulder (3D), left_elbow (1D),",
      "default_branch": "main"
    },
    "fetched_at": "2025-10-12T02:22:44.428228"
  }
]