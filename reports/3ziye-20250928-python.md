## ğŸ“ çƒ­ç‚¹é¡¹ç›®-20250928-python

<div style="background-color: #f5f5f5; padding: 15px; border-radius: 8px; margin: 20px 0;">
- ğŸ¤– æœ¬æŠ¥å‘ŠåŸºäº GitHub API è‡ªåŠ¨ç”Ÿæˆ
- ğŸ“… æ•°æ®æ›´æ–°æ—¶é—´: 2025-09-28 10:27:46 (ä¸Šæµ·æ—¶åŒº)
- ğŸŒŸ æ˜Ÿæ ‡æ•°ç­‰ç»Ÿè®¡ä¿¡æ¯ä¸ºç”Ÿæˆæ—¶çš„å®æ—¶æ•°æ®
- ğŸ“š é¡¹ç›®ä¿¡æ¯æ¥æºäºå„é¡¹ç›®çš„ README æ–‡æ¡£
- ğŸ’¡ çƒ­åº¦æŒ‡æ•°è®¡ç®—æ–¹å¼: æ˜Ÿæ ‡æ•° + Forkæ•° Ã— 0.5
- æœ¬æŠ¥å‘Šç”± ä¸‰å­å¶å¼€æº github-trendingé¡¹ç›®åˆ†æå·¥å…·è‡ªåŠ¨ç”Ÿæˆ
</div>

## ğŸ”— ç›¸å…³é“¾æ¥

- [GitHub API æ–‡æ¡£](https://docs.github.com/en/rest)
- [é¡¹ç›®æ•°æ®è·å–å™¨æºç ](https://github.com/3ziye/github-trending)

---

# çƒ­ç‚¹é¡¹ç›®-20250928-python

<div align="center">
ğŸ“Š <strong>ç”Ÿæˆæ—¶é—´</strong>: 2025å¹´09æœˆ28æ—¥ 10:27  â€¢  
ğŸ¯ <strong>é¡¹ç›®æ•°é‡</strong>: 20 ä¸ª  â€¢  
â±ï¸ <strong>çƒ­åº¦æ—¶é—´</strong>: æœˆæ¦œ  â€¢  
ğŸ”¥ <strong>æ•°æ®æ¥æº</strong>: GitHub API
</div>

---

## ğŸš€ çƒ­é—¨é¡¹ç›®è¯¦æƒ…

### 1. map-anything

**ğŸ”¥ çƒ­åº¦æŒ‡æ•°**: 1,800

**é¡¹ç›®åç§°**: [facebookresearch/map-anything](https://github.com/facebookresearch/map-anything)
**é¡¹ç›®æè¿°**: MapAnything: Universal Feed-Forward Metric 3D Reconstruction
**ä¸»è¦è¯­è¨€**: Python
**è®¸å¯è¯**: Apache License 2.0
**â­ æ˜Ÿæ ‡æ•°**: 1,759
**ğŸ´ Fork æ•°**: 82
**ğŸ‘€ å…³æ³¨æ•°**: 1,759
**ğŸ› å¼€æ”¾é—®é¢˜**: 23
**æœ€åæ›´æ–°**: 2025-09-28 (1å°æ—¶å‰)
**ğŸ“– é¡¹ç›®æ–‡æ¡£**: [README](3ziye-20250928-python/1-map-anything-Readme.md)

**ğŸ  é¡¹ç›®ä¸»é¡µ**: æ— 

**ğŸ“‚ å…‹éš†åœ°å€**: `https://github.com/facebookresearch/map-anything.git`

**ğŸ’» æŠ€æœ¯æ ˆ**: **ä¸»è¦è¯­è¨€**: Python

#### ğŸ·ï¸ é¡¹ç›®æ ‡ç­¾
- `3d-reconstruction`
- `ai`
- `calibration`
- `depth-completion`
- `depth-estimation`
- `image-to-3d`
- `multi-view-stereo`
- `robotics`

#### ğŸ¯ æ ¸å¿ƒç‰¹æ€§
- [Quick Start](#quick-start)
- [Interactive Demos](#interactive-demos)

#### ğŸ¨ é€‚ç”¨åœºæ™¯
- æ•°æ®ç§‘å­¦ä¸åˆ†æ
- æœºå™¨å­¦ä¹ é¡¹ç›®
- Web åº”ç”¨å¼€å‘

---

### 2. VoxCPM

**ğŸ”¥ çƒ­åº¦æŒ‡æ•°**: 1,519

**é¡¹ç›®åç§°**: [OpenBMB/VoxCPM](https://github.com/OpenBMB/VoxCPM)
**é¡¹ç›®æè¿°**: VoxCPM: Tokenizer-Free TTS for Context-Aware Speech Generation and True-to-Life Voice Cloning
**ä¸»è¦è¯­è¨€**: Python
**è®¸å¯è¯**: Apache License 2.0
**â­ æ˜Ÿæ ‡æ•°**: 1,445
**ğŸ´ Fork æ•°**: 148
**ğŸ‘€ å…³æ³¨æ•°**: 1,445
**ğŸ› å¼€æ”¾é—®é¢˜**: 22
**æœ€åæ›´æ–°**: 2025-09-28 (2åˆ†é’Ÿå‰)
**ğŸ“– é¡¹ç›®æ–‡æ¡£**: [README](3ziye-20250928-python/2-VoxCPM-Readme.md)

**ğŸ  é¡¹ç›®ä¸»é¡µ**: æ— 

**ğŸ“‚ å…‹éš†åœ°å€**: `https://github.com/OpenBMB/VoxCPM.git`

**ğŸ’» æŠ€æœ¯æ ˆ**: **ä¸»è¦è¯­è¨€**: Python

#### ğŸ¯ æ ¸å¿ƒç‰¹æ€§
- [2025.09.16] ğŸ”¥ ğŸ”¥ ğŸ”¥  We Open Source the VoxCPM-0.5B [weights](https://huggingface.co/openbmb/VoxCPM-0.5B)!
- [2025.09.16] ğŸ‰ ğŸ‰ ğŸ‰  We Provide the [Gradio PlayGround](https://huggingface.co/spaces/OpenBMB/VoxCPM-Demo) for VoxCPM-0.5B, try it now!
**High-Efficiency Synthesis** - VoxCPM supports streaming synthesis with a Real-Time Factor (RTF) as low as 0.17 on a consumer-grade NVIDIA RTX 4090 GPU, making it possible for real-time applications.

#### ğŸ¨ é€‚ç”¨åœºæ™¯
- æ•°æ®ç§‘å­¦ä¸åˆ†æ
- æœºå™¨å­¦ä¹ é¡¹ç›®
- Web åº”ç”¨å¼€å‘

---

### 3. multi-agent-coding-system

**ğŸ”¥ çƒ­åº¦æŒ‡æ•°**: 1,271

**é¡¹ç›®åç§°**: [Danau5tin/multi-agent-coding-system](https://github.com/Danau5tin/multi-agent-coding-system)
**é¡¹ç›®æè¿°**: Reached #13 on Stanford's Terminal Bench leaderboard. Orchestrator, explorer & coder agents working together with intell...
**ä¸»è¦è¯­è¨€**: Python
**è®¸å¯è¯**: None
**â­ æ˜Ÿæ ‡æ•°**: 1,199
**ğŸ´ Fork æ•°**: 145
**ğŸ‘€ å…³æ³¨æ•°**: 1,199
**ğŸ› å¼€æ”¾é—®é¢˜**: 2
**æœ€åæ›´æ–°**: 2025-09-27 (19å°æ—¶å‰)
**ğŸ“– é¡¹ç›®æ–‡æ¡£**: [README](3ziye-20250928-python/3-multi-agent-coding-system-Readme.md)

**ğŸ  é¡¹ç›®ä¸»é¡µ**: æ— 

**ğŸ“‚ å…‹éš†åœ°å€**: `https://github.com/Danau5tin/multi-agent-coding-system.git`

**ğŸ’» æŠ€æœ¯æ ˆ**: **ä¸»è¦è¯­è¨€**: Python

#### ğŸ¯ æ ¸å¿ƒç‰¹æ€§
- Over the weekend, quite unexpectedly, I made a multi-agent AI system that places slightly higher than Claude Code on Stanford's TerminalBench leaderboard (13th place).
- This AI system consists of an orchestration agent that dispatches multiple explorer and coder agents to do all the work.
- The orchestrator explicitly defines what knowledge artifacts subagents must return, then reuses and synthesises these artifacts across future tasks - creating compound intelligence where each action builds meaningfully on previous discoveries.

#### ğŸ¨ é€‚ç”¨åœºæ™¯
- æ•°æ®ç§‘å­¦ä¸åˆ†æ
- æœºå™¨å­¦ä¹ é¡¹ç›®
- Web åº”ç”¨å¼€å‘

---

### 4. SRPO

**ğŸ”¥ çƒ­åº¦æŒ‡æ•°**: 1,037

**é¡¹ç›®åç§°**: [Tencent-Hunyuan/SRPO](https://github.com/Tencent-Hunyuan/SRPO)
**é¡¹ç›®æè¿°**: Directly Aligning the Full Diffusion Trajectory with Fine-Grained Human Preference
**ä¸»è¦è¯­è¨€**: Python
**è®¸å¯è¯**: Other
**â­ æ˜Ÿæ ‡æ•°**: 1,021
**ğŸ´ Fork æ•°**: 32
**ğŸ‘€ å…³æ³¨æ•°**: 1,021
**ğŸ› å¼€æ”¾é—®é¢˜**: 13
**æœ€åæ›´æ–°**: 2025-09-28 (32åˆ†é’Ÿå‰)
**ğŸ“– é¡¹ç›®æ–‡æ¡£**: [README](3ziye-20250928-python/4-SRPO-Readme.md)

**ğŸ  é¡¹ç›®ä¸»é¡µ**: https://tencent.github.io/srpo-project-page/

**ğŸ“‚ å…‹éš†åœ°å€**: `https://github.com/Tencent-Hunyuan/SRPO.git`

**ğŸ’» æŠ€æœ¯æ ˆ**: **ä¸»è¦è¯­è¨€**: Python

#### ğŸ¨ é€‚ç”¨åœºæ™¯
- æ•°æ®ç§‘å­¦ä¸åˆ†æ
- æœºå™¨å­¦ä¹ é¡¹ç›®
- Web åº”ç”¨å¼€å‘

---

### 5. SpikingBrain-7B

**ğŸ”¥ çƒ­åº¦æŒ‡æ•°**: 1,037

**é¡¹ç›®åç§°**: [BICLab/SpikingBrain-7B](https://github.com/BICLab/SpikingBrain-7B)
**é¡¹ç›®æè¿°**: æš‚æ— æè¿°
**ä¸»è¦è¯­è¨€**: Python
**è®¸å¯è¯**: None
**â­ æ˜Ÿæ ‡æ•°**: 977
**ğŸ´ Fork æ•°**: 120
**ğŸ‘€ å…³æ³¨æ•°**: 977
**ğŸ› å¼€æ”¾é—®é¢˜**: 5
**æœ€åæ›´æ–°**: 2025-09-28 (1å°æ—¶å‰)
**ğŸ“– é¡¹ç›®æ–‡æ¡£**: [README](3ziye-20250928-python/5-SpikingBrain-7B-Readme.md)

**ğŸ  é¡¹ç›®ä¸»é¡µ**: æ— 

**ğŸ“‚ å…‹éš†åœ°å€**: `https://github.com/BICLab/SpikingBrain-7B.git`

**ğŸ’» æŠ€æœ¯æ ˆ**: **ä¸»è¦è¯­è¨€**: Python

#### ğŸ¯ æ ¸å¿ƒç‰¹æ€§
**Decoupled codebase**: Backend-specific code remains independent, keeping the vLLM core cleaner.
**Reduced maintenance cost**: vLLM developers can focus on general functionality without being affected by backend-specific implementations.
**Faster integration**: New backends can be integrated quickly and evolve independently with less engineering effort.

#### ğŸ¨ é€‚ç”¨åœºæ™¯
- æ•°æ®ç§‘å­¦ä¸åˆ†æ
- æœºå™¨å­¦ä¹ é¡¹ç›®
- Web åº”ç”¨å¼€å‘

---

### 6. PromptEnhancer

**ğŸ”¥ çƒ­åº¦æŒ‡æ•°**: 936

**é¡¹ç›®åç§°**: [Hunyuan-PromptEnhancer/PromptEnhancer](https://github.com/Hunyuan-PromptEnhancer/PromptEnhancer)
**é¡¹ç›®æè¿°**: PromptEnhancer is a prompt-rewriting tool, refining prompts into clearer, structured versions for better image generatio...
**ä¸»è¦è¯­è¨€**: Python
**è®¸å¯è¯**: Other
**â­ æ˜Ÿæ ‡æ•°**: 898
**ğŸ´ Fork æ•°**: 76
**ğŸ‘€ å…³æ³¨æ•°**: 898
**ğŸ› å¼€æ”¾é—®é¢˜**: 6
**æœ€åæ›´æ–°**: 2025-09-28 (4åˆ†é’Ÿå‰)
**ğŸ“– é¡¹ç›®æ–‡æ¡£**: [README](3ziye-20250928-python/6-PromptEnhancer-Readme.md)

**ğŸ  é¡¹ç›®ä¸»é¡µ**: https://hunyuan-promptenhancer.github.io/

**ğŸ“‚ å…‹éš†åœ°å€**: `https://github.com/Hunyuan-PromptEnhancer/PromptEnhancer.git`

**ğŸ’» æŠ€æœ¯æ ˆ**: **ä¸»è¦è¯­è¨€**: Python

#### ğŸ·ï¸ é¡¹ç›®æ ‡ç­¾
- `hunyuan`
- `hunyuan-image`
- `prompt`
- `prompt-engineering`
- `prompt-enhancer`
- `vlm`

#### ğŸ¨ é€‚ç”¨åœºæ™¯
- æ•°æ®ç§‘å­¦ä¸åˆ†æ
- æœºå™¨å­¦ä¹ é¡¹ç›®
- Web åº”ç”¨å¼€å‘

---

### 7. batch_invariant_ops

**ğŸ”¥ çƒ­åº¦æŒ‡æ•°**: 770

**é¡¹ç›®åç§°**: [thinking-machines-lab/batch_invariant_ops](https://github.com/thinking-machines-lab/batch_invariant_ops)
**é¡¹ç›®æè¿°**: æš‚æ— æè¿°
**ä¸»è¦è¯­è¨€**: Python
**è®¸å¯è¯**: MIT License
**â­ æ˜Ÿæ ‡æ•°**: 745
**ğŸ´ Fork æ•°**: 51
**ğŸ‘€ å…³æ³¨æ•°**: 745
**ğŸ› å¼€æ”¾é—®é¢˜**: 3
**æœ€åæ›´æ–°**: 2025-09-28 (6å°æ—¶å‰)
**ğŸ“– é¡¹ç›®æ–‡æ¡£**: [README](3ziye-20250928-python/7-batch_invariant_ops-Readme.md)

**ğŸ  é¡¹ç›®ä¸»é¡µ**: æ— 

**ğŸ“‚ å…‹éš†åœ°å€**: `https://github.com/thinking-machines-lab/batch_invariant_ops.git`

**ğŸ’» æŠ€æœ¯æ ˆ**: **ä¸»è¦è¯­è¨€**: Python

#### ğŸ¯ æ ¸å¿ƒç‰¹æ€§
- `torch.mm()` - Matrix multiplication
- `torch.addmm()` - Matrix multiplication with bias addition
- `torch.log_softmax()` - Log-softmax activation
- `torch.mean()` - Mean computation along specified dimensions

#### ğŸ¨ é€‚ç”¨åœºæ™¯
- æ•°æ®ç§‘å­¦ä¸åˆ†æ
- æœºå™¨å­¦ä¹ é¡¹ç›®
- Web åº”ç”¨å¼€å‘

---

### 8. checkpoint-engine

**ğŸ”¥ çƒ­åº¦æŒ‡æ•°**: 765

**é¡¹ç›®åç§°**: [MoonshotAI/checkpoint-engine](https://github.com/MoonshotAI/checkpoint-engine)
**é¡¹ç›®æè¿°**: Checkpoint-engine is a simple middleware to update model weights in LLM inference engines
**ä¸»è¦è¯­è¨€**: Python
**è®¸å¯è¯**: MIT License
**â­ æ˜Ÿæ ‡æ•°**: 742
**ğŸ´ Fork æ•°**: 47
**ğŸ‘€ å…³æ³¨æ•°**: 742
**ğŸ› å¼€æ”¾é—®é¢˜**: 5
**æœ€åæ›´æ–°**: 2025-09-28 (2å°æ—¶å‰)
**ğŸ“– é¡¹ç›®æ–‡æ¡£**: [README](3ziye-20250928-python/8-checkpoint-engine-Readme.md)

**ğŸ  é¡¹ç›®ä¸»é¡µ**: æ— 

**ğŸ“‚ å…‹éš†åœ°å€**: `https://github.com/MoonshotAI/checkpoint-engine.git`

**ğŸ’» æŠ€æœ¯æ ˆ**: **ä¸»è¦è¯­è¨€**: Python

#### ğŸ¯ æ ¸å¿ƒç‰¹æ€§
**Broadcast**: Used when a large number of inference instances need to update weights in synchronous. This is the fastest implementation and should be used as the default update method. See `_update_per_bucket`.

#### ğŸ¨ é€‚ç”¨åœºæ™¯
- æ•°æ®ç§‘å­¦ä¸åˆ†æ
- æœºå™¨å­¦ä¹ é¡¹ç›®
- Web åº”ç”¨å¼€å‘

---

### 9. FireRedTTS2

**ğŸ”¥ çƒ­åº¦æŒ‡æ•°**: 756

**é¡¹ç›®åç§°**: [FireRedTeam/FireRedTTS2](https://github.com/FireRedTeam/FireRedTTS2)
**é¡¹ç›®æè¿°**: Long-form streaming TTS system for multi-speaker dialogue generation
**ä¸»è¦è¯­è¨€**: Python
**è®¸å¯è¯**: Apache License 2.0
**â­ æ˜Ÿæ ‡æ•°**: 714
**ğŸ´ Fork æ•°**: 85
**ğŸ‘€ å…³æ³¨æ•°**: 714
**ğŸ› å¼€æ”¾é—®é¢˜**: 15
**æœ€åæ›´æ–°**: 2025-09-28 (1å°æ—¶å‰)
**ğŸ“– é¡¹ç›®æ–‡æ¡£**: [README](3ziye-20250928-python/9-FireRedTTS2-Readme.md)

**ğŸ  é¡¹ç›®ä¸»é¡µ**: æ— 

**ğŸ“‚ å…‹éš†åœ°å€**: `https://github.com/FireRedTeam/FireRedTTS2.git`

**ğŸ’» æŠ€æœ¯æ ˆ**: **ä¸»è¦è¯­è¨€**: Python

#### ğŸ¯ æ ¸å¿ƒç‰¹æ€§
**Long Conversational Speech Generation**: It currently supports 3 minutes dialogues with 4 speakers and can be easily scaled to longer conversations
**Multilingual Support**: It supports multiple languages including English, Chinese, Japanese, Korean, French, German, and Russian. Support zero-shot voice cloning for cross-lingual and code-switching scenarios.
**Strong Stability**ï¼šOur model achieves high similarity and low WER/CER in both monologue and dialogue tests.
**Random Timbre Generation**:Useful for creating ASR/speech interaction data.
- [2025/09/12] ğŸ”¥ *

#### ğŸ¨ é€‚ç”¨åœºæ™¯
- æ•°æ®ç§‘å­¦ä¸åˆ†æ
- æœºå™¨å­¦ä¹ é¡¹ç›®
- Web åº”ç”¨å¼€å‘

---

### 10. youtu-graphrag

**ğŸ”¥ çƒ­åº¦æŒ‡æ•°**: 751

**é¡¹ç›®åç§°**: [TencentCloudADP/youtu-graphrag](https://github.com/TencentCloudADP/youtu-graphrag)
**é¡¹ç›®æè¿°**: Official repository of Youtu-GraphRAG: Vertically Unified Agents for Graph Retrieval-Augmented Complex Reasoning
**ä¸»è¦è¯­è¨€**: Python
**è®¸å¯è¯**: Other
**â­ æ˜Ÿæ ‡æ•°**: 704
**ğŸ´ Fork æ•°**: 95
**ğŸ‘€ å…³æ³¨æ•°**: 704
**ğŸ› å¼€æ”¾é—®é¢˜**: 45
**æœ€åæ›´æ–°**: 2025-09-28 (37åˆ†é’Ÿå‰)
**ğŸ“– é¡¹ç›®æ–‡æ¡£**: [README](3ziye-20250928-python/10-youtu-graphrag-Readme.md)

**ğŸ  é¡¹ç›®ä¸»é¡µ**: https://arxiv.org/abs/2508.19855

**ğŸ“‚ å…‹éš†åœ°å€**: `https://github.com/TencentCloudADP/youtu-graphrag.git`

**ğŸ’» æŠ€æœ¯æ ˆ**: **ä¸»è¦è¯­è¨€**: Python

#### ğŸ·ï¸ é¡¹ç›®æ ‡ç­¾
- `agent`
- `graph`
- `graphrag`
- `llm`
- `rag`

#### ğŸ¯ æ ¸å¿ƒç‰¹æ€§
- ğŸŒ± **S

#### ğŸ¨ é€‚ç”¨åœºæ™¯
- æ•°æ®ç§‘å­¦ä¸åˆ†æ
- æœºå™¨å­¦ä¹ é¡¹ç›®
- Web åº”ç”¨å¼€å‘

---

### 11. MiMo-Audio

**ğŸ”¥ çƒ­åº¦æŒ‡æ•°**: 723

**é¡¹ç›®åç§°**: [XiaomiMiMo/MiMo-Audio](https://github.com/XiaomiMiMo/MiMo-Audio)
**é¡¹ç›®æè¿°**: MiMo-Audio: Audio Language Models are Few-Shot Learners
**ä¸»è¦è¯­è¨€**: Python
**è®¸å¯è¯**: Apache License 2.0
**â­ æ˜Ÿæ ‡æ•°**: 692
**ğŸ´ Fork æ•°**: 62
**ğŸ‘€ å…³æ³¨æ•°**: 692
**ğŸ› å¼€æ”¾é—®é¢˜**: 24
**æœ€åæ›´æ–°**: 2025-09-28 (28åˆ†é’Ÿå‰)
**ğŸ“– é¡¹ç›®æ–‡æ¡£**: [README](3ziye-20250928-python/11-MiMo-Audio-Readme.md)

**ğŸ  é¡¹ç›®ä¸»é¡µ**: https://xiaomimimo.github.io/MiMo-Audio-Demo/

**ğŸ“‚ å…‹éš†åœ°å€**: `https://github.com/XiaomiMiMo/MiMo-Audio.git`

**ğŸ’» æŠ€æœ¯æ ˆ**: **ä¸»è¦è¯­è¨€**: Python

#### ğŸ¨ é€‚ç”¨åœºæ™¯
- æ•°æ®ç§‘å­¦ä¸åˆ†æ
- æœºå™¨å­¦ä¹ é¡¹ç›®
- Web åº”ç”¨å¼€å‘

---

### 12. HuMo

**ğŸ”¥ çƒ­åº¦æŒ‡æ•°**: 651

**é¡¹ç›®åç§°**: [Phantom-video/HuMo](https://github.com/Phantom-video/HuMo)
**é¡¹ç›®æè¿°**: HuMo: Human-Centric Video Generation via Collaborative Multi-Modal Conditioning
**ä¸»è¦è¯­è¨€**: Python
**è®¸å¯è¯**: Apache License 2.0
**â­ æ˜Ÿæ ‡æ•°**: 624
**ğŸ´ Fork æ•°**: 54
**ğŸ‘€ å…³æ³¨æ•°**: 624
**ğŸ› å¼€æ”¾é—®é¢˜**: 7
**æœ€åæ›´æ–°**: 2025-09-28 (5å°æ—¶å‰)
**ğŸ“– é¡¹ç›®æ–‡æ¡£**: [README](3ziye-20250928-python/12-HuMo-Readme.md)

**ğŸ  é¡¹ç›®ä¸»é¡µ**: https://phantom-video.github.io/HuMo/

**ğŸ“‚ å…‹éš†åœ°å€**: `https://github.com/Phantom-video/HuMo.git`

**ğŸ’» æŠ€æœ¯æ ˆ**: **ä¸»è¦è¯­è¨€**: Python

#### ğŸ¯ æ ¸å¿ƒç‰¹æ€§
- A Best-Practice Guide for HuMo will be released soon. Stay tuned.
- Sep 17, 2025: ğŸ”¥ğŸ”¥ [ComfyUI](https://github.com/comfyanonymous/ComfyUI/pull/9903) officially supports HuMo-1.7B!
- Sep 13, 2025: ğŸ”¥ğŸ”¥ The 17B model is merged into [ComfyUI-Wan](https://github.com/kijai/ComfyUI-WanVideoWrapper), which can be run on a NVIDIA 3090 GPU. Thank [kijai](https://github.com/kijai) for the update!
- Sep 10, 2025: ğŸ”¥ğŸ”¥ We release the [17B weights](https://huggingface.co/bytedance-research/HuMo/tree/main/HuMo-17B) and inference codes.
- Sep 9, 2025: We release the [Project Page](https://phantom-video.github.io/HuMo/) and [Technique Report](https://arxiv.org/abs/2509.08519/) of **HuMo**.

#### ğŸ¨ é€‚ç”¨åœºæ™¯
- æ•°æ®ç§‘å­¦ä¸åˆ†æ
- æœºå™¨å­¦ä¹ é¡¹ç›®
- Web åº”ç”¨å¼€å‘

---

### 13. HunyuanImage-2.1

**ğŸ”¥ çƒ­åº¦æŒ‡æ•°**: 637

**é¡¹ç›®åç§°**: [Tencent-Hunyuan/HunyuanImage-2.1](https://github.com/Tencent-Hunyuan/HunyuanImage-2.1)
**é¡¹ç›®æè¿°**: HunyuanImage-2.1: An Efficient Diffusion Model for High-Resolution (2K) Text-to-Image Generationâ€‹
**ä¸»è¦è¯­è¨€**: Python
**è®¸å¯è¯**: Other
**â­ æ˜Ÿæ ‡æ•°**: 615
**ğŸ´ Fork æ•°**: 45
**ğŸ‘€ å…³æ³¨æ•°**: 615
**ğŸ› å¼€æ”¾é—®é¢˜**: 11
**æœ€åæ›´æ–°**: 2025-09-28 (5å°æ—¶å‰)
**ğŸ“– é¡¹ç›®æ–‡æ¡£**: [README](3ziye-20250928-python/13-HunyuanImage-2.1-Readme.md)

**ğŸ  é¡¹ç›®ä¸»é¡µ**: https://hunyuan.tencent.com/image/en?tabIndex=0

**ğŸ“‚ å…‹éš†åœ°å€**: `https://github.com/Tencent-Hunyuan/HunyuanImage-2.1.git`

**ğŸ’» æŠ€æœ¯æ ˆ**: **ä¸»è¦è¯­è¨€**: Python

#### ğŸ·ï¸ é¡¹ç›®æ ‡ç­¾
- `aigc`
- `diffusion-models`
- `diffusion-transformer`
- `image-generation`
- `text-to-image`

#### ğŸ¯ æ ¸å¿ƒç‰¹æ€§
- September 18, 2025: âœ¨ Try the [PromptEnhancer-32B model](https://huggingface.co/PromptEnhancer/PromptEnhancer-32B) for higher-quality prompt enhancement!â€‹.
- September 18, 2025: âœ¨ [ComfyUI workflow of HunyuanImage-2.1](https://github.com/KimbingNg/ComfyUI-HunyuanImage2.1) is available now!
- September 16, 2025: ğŸ‘‘ We achieved the Top1 on Arena's leaderboard for text-to-image open-source models. [Leaderboard](https://artificialanalysis.ai/text-to-image/arena/leaderboard-text)
- September 12, 2025: ğŸš€ Released FP8 quantized models! Making it possible to generate 2K images with only 24GB GPU memory!
- September 8, 2025: ğŸš€ Released inference code and model weights for HunyuanImage-2.1.

#### ğŸ¨ é€‚ç”¨åœºæ™¯
- æ•°æ®ç§‘å­¦ä¸åˆ†æ
- æœºå™¨å­¦ä¹ é¡¹ç›®
- Web åº”ç”¨å¼€å‘

---

### 14. ml-simplefold

**ğŸ”¥ çƒ­åº¦æŒ‡æ•°**: 614

**é¡¹ç›®åç§°**: [apple/ml-simplefold](https://github.com/apple/ml-simplefold)
**é¡¹ç›®æè¿°**: æš‚æ— æè¿°
**ä¸»è¦è¯­è¨€**: Python
**è®¸å¯è¯**: MIT License
**â­ æ˜Ÿæ ‡æ•°**: 602
**ğŸ´ Fork æ•°**: 25
**ğŸ‘€ å…³æ³¨æ•°**: 602
**ğŸ› å¼€æ”¾é—®é¢˜**: 9
**æœ€åæ›´æ–°**: 2025-09-28 (4åˆ†é’Ÿå‰)
**ğŸ“– é¡¹ç›®æ–‡æ¡£**: [README](3ziye-20250928-python/14-ml-simplefold-Readme.md)

**ğŸ  é¡¹ç›®ä¸»é¡µ**: æ— 

**ğŸ“‚ å…‹éš†åœ°å€**: `https://github.com/apple/ml-simplefold.git`

**ğŸ’» æŠ€æœ¯æ ˆ**: **ä¸»è¦è¯­è¨€**: Python

#### ğŸ¨ é€‚ç”¨åœºæ™¯
- æ•°æ®ç§‘å­¦ä¸åˆ†æ
- æœºå™¨å­¦ä¹ é¡¹ç›®
- Web åº”ç”¨å¼€å‘

---

### 15. DeepMCPAgent

**ğŸ”¥ çƒ­åº¦æŒ‡æ•°**: 635

**é¡¹ç›®åç§°**: [cryxnet/DeepMCPAgent](https://github.com/cryxnet/DeepMCPAgent)
**é¡¹ç›®æè¿°**: Model-agnostic plug-n-play LangChain/LangGraph agents powered entirely by MCP tools over HTTP/SSE.
**ä¸»è¦è¯­è¨€**: Python
**è®¸å¯è¯**: Apache License 2.0
**â­ æ˜Ÿæ ‡æ•°**: 591
**ğŸ´ Fork æ•°**: 89
**ğŸ‘€ å…³æ³¨æ•°**: 591
**ğŸ› å¼€æ”¾é—®é¢˜**: 1
**æœ€åæ›´æ–°**: 2025-09-28 (1å°æ—¶å‰)
**ğŸ“– é¡¹ç›®æ–‡æ¡£**: [README](3ziye-20250928-python/15-DeepMCPAgent-Readme.md)

**ğŸ  é¡¹ç›®ä¸»é¡µ**: https://cryxnet.github.io/DeepMCPAgent/

**ğŸ“‚ å…‹éš†åœ°å€**: `https://github.com/cryxnet/DeepMCPAgent.git`

**ğŸ’» æŠ€æœ¯æ ˆ**: **ä¸»è¦è¯­è¨€**: Python

#### ğŸ·ï¸ é¡¹ç›®æ ‡ç­¾
- `agent-framework`
- `agentic-ai`
- `agents`
- `ai`
- `ai-agents`
- `ai-framework`
- `artificial-intelligence`
- `autonomous-agents`

#### ğŸ¯ æ ¸å¿ƒç‰¹æ€§
- ğŸ”Œ **Zero manual tool wiring** â€” tools are discovered dynamically from MCP servers (HTTP/SSE)
- ğŸŒ **External APIs welcome** â€” connect to remote MCP servers (with headers/auth)
- ğŸ§  **Model-agnostic** â€” pass any LangChain chat model instance (OpenAI, Anthropic, Ollama, Groq, local, â€¦)
- âš¡ **DeepAgents (optional)** â€” if installed, you get a deep agent loop; otherwise robust LangGraph ReAct fallback
- ğŸ› ï¸ **Typed tool args** â€” JSON-Schema â†’ Pydantic â†’ LangChain `BaseTool` (typed, validated calls)
- ğŸ§ª **Quality bar** â€” mypy (strict), ruff, pytest, GitHub Actions, docs
- `dev` â†’ linting, typing, tests
- `docs` â†’ MkDocs + Material + mkdocstrings

#### ğŸ¨ é€‚ç”¨åœºæ™¯
- æ•°æ®ç§‘å­¦ä¸åˆ†æ
- æœºå™¨å­¦ä¹ é¡¹ç›®
- Web åº”ç”¨å¼€å‘

---

### 16. Qwen3-ASR-Toolkit

**ğŸ”¥ çƒ­åº¦æŒ‡æ•°**: 593

**é¡¹ç›®åç§°**: [QwenLM/Qwen3-ASR-Toolkit](https://github.com/QwenLM/Qwen3-ASR-Toolkit)
**é¡¹ç›®æè¿°**: Official Python toolkit for the Qwen3-ASR API. Parallel highâ€‘throughput calls, robust longâ€‘audio transcription, multiâ€‘sa...
**ä¸»è¦è¯­è¨€**: Python
**è®¸å¯è¯**: MIT License
**â­ æ˜Ÿæ ‡æ•°**: 569
**ğŸ´ Fork æ•°**: 49
**ğŸ‘€ å…³æ³¨æ•°**: 569
**ğŸ› å¼€æ”¾é—®é¢˜**: 4
**æœ€åæ›´æ–°**: 2025-09-28 (25åˆ†é’Ÿå‰)
**ğŸ“– é¡¹ç›®æ–‡æ¡£**: [README](3ziye-20250928-python/16-Qwen3-ASR-Toolkit-Readme.md)

**ğŸ  é¡¹ç›®ä¸»é¡µ**: æ— 

**ğŸ“‚ å…‹éš†åœ°å€**: `https://github.com/QwenLM/Qwen3-ASR-Toolkit.git`

**ğŸ’» æŠ€æœ¯æ ˆ**: **ä¸»è¦è¯­è¨€**: Python

#### ğŸ¯ æ ¸å¿ƒç‰¹æ€§
**Break the 3-Minute Limit**: Seamlessly transcribe audio and video files of any length by bypassing the official API's duration constraint.
**Smart Audio Splitting**: Utilizes **Voice Activity Detection (VAD)** to split audio into meaningful chunks at natural silent pauses. This ensures that words and sentences are not awkwardly cut off.
**High-Speed Parallel Processing**: Leverages multi-threading to send audio chunks to the Qwen-ASR API concurrently, dramatically reducing the total transcription time for long files.
**Intelligent Post-Processing**: Automatically detects and removes common ASR **hallucinations and repetitive artifacts** for cleaner, more accurate transcripts.
**SRT Subtitle Generation**: Automatically create timestamped **`.srt` subtitle files** based on VAD segments, perfect for adding captions to video content.
**Automatic Audio Resampling**: Automatically converts audio from any sample rate and channel count to the 16kHz mono format required by the Qwen-ASR API. You can use any audio file without worrying about pre-processing.
**Universal Media Support**: Supports virtually any audio and video format (e.g., `.mp4`, `.mov`, `.mkv`, `.mp3`, `.wav`, `.m4a`) thanks to its reliance on FFmpeg.
**Simple & Easy to Use**: A straightforward command-line interface allows you to get started with just a single command.

#### ğŸ¨ é€‚ç”¨åœºæ™¯
- æ•°æ®ç§‘å­¦ä¸åˆ†æ
- æœºå™¨å­¦ä¹ é¡¹ç›®
- Web åº”ç”¨å¼€å‘
- APIæœåŠ¡å¼€å‘

---

### 17. unifolm-world-model-action

**ğŸ”¥ çƒ­åº¦æŒ‡æ•°**: 588

**é¡¹ç›®åç§°**: [unitreerobotics/unifolm-world-model-action](https://github.com/unitreerobotics/unifolm-world-model-action)
**é¡¹ç›®æè¿°**: æš‚æ— æè¿°
**ä¸»è¦è¯­è¨€**: Python
**è®¸å¯è¯**: Other
**â­ æ˜Ÿæ ‡æ•°**: 566
**ğŸ´ Fork æ•°**: 44
**ğŸ‘€ å…³æ³¨æ•°**: 566
**ğŸ› å¼€æ”¾é—®é¢˜**: 5
**æœ€åæ›´æ–°**: 2025-09-27 (12å°æ—¶å‰)
**ğŸ“– é¡¹ç›®æ–‡æ¡£**: [README](3ziye-20250928-python/17-unifolm-world-model-action-Readme.md)

**ğŸ  é¡¹ç›®ä¸»é¡µ**: æ— 

**ğŸ“‚ å…‹éš†åœ°å€**: `https://github.com/unitreerobotics/unifolm-world-model-action.git`

**ğŸ’» æŠ€æœ¯æ ˆ**: **ä¸»è¦è¯­è¨€**: Python

#### ğŸ¯ æ ¸å¿ƒç‰¹æ€§
- Sep 22, 2025: ğŸš€ We released the deployment code for assisting experiments with [Unitree](https://www.unitree.com/) robots.
- Sep 15, 2025: ğŸš€ We released the training and inference code along with the model weights of [**UnifoLM-WMA-0**](https://huggingface.co/collections/unitreerobotics/unifolm-wma-0-68ca23027310c0ca0f34959c).
- [x] Training
- [x] Inference
- [x] Checkpoints
- [x] Deployment

#### ğŸ¨ é€‚ç”¨åœºæ™¯
- æ•°æ®ç§‘å­¦ä¸åˆ†æ
- æœºå™¨å­¦ä¹ é¡¹ç›®
- Web åº”ç”¨å¼€å‘

---

### 18. Lumina-DiMOO

**ğŸ”¥ çƒ­åº¦æŒ‡æ•°**: 579

**é¡¹ç›®åç§°**: [Alpha-VLLM/Lumina-DiMOO](https://github.com/Alpha-VLLM/Lumina-DiMOO)
**é¡¹ç›®æè¿°**: Lumina-DiMOO - An Open-Sourced Multi-Modal Large Diffusion Language Model
**ä¸»è¦è¯­è¨€**: Python
**è®¸å¯è¯**: Apache License 2.0
**â­ æ˜Ÿæ ‡æ•°**: 564
**ğŸ´ Fork æ•°**: 31
**ğŸ‘€ å…³æ³¨æ•°**: 564
**ğŸ› å¼€æ”¾é—®é¢˜**: 2
**æœ€åæ›´æ–°**: 2025-09-28 (5åˆ†é’Ÿå‰)
**ğŸ“– é¡¹ç›®æ–‡æ¡£**: [README](3ziye-20250928-python/18-Lumina-DiMOO-Readme.md)

**ğŸ  é¡¹ç›®ä¸»é¡µ**: https://synbol.github.io/Lumina-DiMOO/

**ğŸ“‚ å…‹éš†åœ°å€**: `https://github.com/Alpha-VLLM/Lumina-DiMOO.git`

**ğŸ’» æŠ€æœ¯æ ˆ**: **ä¸»è¦è¯­è¨€**: Python

#### ğŸ·ï¸ é¡¹ç›®æ ‡ç­¾
- `diffusion-large-language-model`
- `discrete-diffusion-models`
- `unified-multimodal-understanding-and-generation`

#### ğŸ¯ æ ¸å¿ƒç‰¹æ€§
**[2025-09-25]** ğŸ‰ğŸ‰ğŸ‰ We have released the Technical Report.
**[2025-09-12]** ğŸ‰ğŸ‰ğŸ‰ We have open-sourced Image Inpainting & Extrapolation code.
**[2025-09-11]** ğŸ‰ğŸ‰ğŸ‰ We have open-sourced the Max Logit-based Cache solution, offering a 2x speed improvement for sampling.
**[2025-09-10]** ğŸ‰ğŸ‰ğŸ‰ We release the initial version of **Lumina-DiMOO**, including:

#### ğŸ¨ é€‚ç”¨åœºæ™¯
- æ•°æ®ç§‘å­¦ä¸åˆ†æ
- æœºå™¨å­¦ä¹ é¡¹ç›®
- Web åº”ç”¨å¼€å‘

---

### 19. VibeVoice

**ğŸ”¥ çƒ­åº¦æŒ‡æ•°**: 620

**é¡¹ç›®åç§°**: [vibevoice-community/VibeVoice](https://github.com/vibevoice-community/VibeVoice)
**é¡¹ç›®æè¿°**: VibeVoice: Expressive, longform conversational speech synthesis. (Community fork)
**ä¸»è¦è¯­è¨€**: Python
**è®¸å¯è¯**: MIT License
**â­ æ˜Ÿæ ‡æ•°**: 519
**ğŸ´ Fork æ•°**: 202
**ğŸ‘€ å…³æ³¨æ•°**: 519
**ğŸ› å¼€æ”¾é—®é¢˜**: 5
**æœ€åæ›´æ–°**: 2025-09-28 (1å°æ—¶å‰)
**ğŸ“– é¡¹ç›®æ–‡æ¡£**: [README](3ziye-20250928-python/19-VibeVoice-Readme.md)

**ğŸ  é¡¹ç›®ä¸»é¡µ**: https://discord.gg/ZDEYTTRxWG

**ğŸ“‚ å…‹éš†åœ°å€**: `https://github.com/vibevoice-community/VibeVoice.git`

**ğŸ’» æŠ€æœ¯æ ˆ**: **ä¸»è¦è¯­è¨€**: Python

#### ğŸ¯ æ ¸å¿ƒç‰¹æ€§
**[2025-09-05]** Microsoft repo restored (without code) with statement about responsible AI use.
**[2025-09-04]** Community backup created after Microsoft removed original repo and models.
**[2025-08-26]** The [VibeVoice-7B](https://huggingface.co/vibevoice/VibeVoice-7B) model weights are open-sourced!
**[2025-08-28]** [Colab Notebook](https://colab.research.google.com/github/microsoft-community/VibeVoice/blob/main/demo/VibeVoice_colab.ipy

#### ğŸ¨ é€‚ç”¨åœºæ™¯
- æ•°æ®ç§‘å­¦ä¸åˆ†æ
- æœºå™¨å­¦ä¹ é¡¹ç›®
- Web åº”ç”¨å¼€å‘

---

### 20. Lucy-Edit-ComfyUI

**ğŸ”¥ çƒ­åº¦æŒ‡æ•°**: 545

**é¡¹ç›®åç§°**: [DecartAI/Lucy-Edit-ComfyUI](https://github.com/DecartAI/Lucy-Edit-ComfyUI)
**é¡¹ç›®æè¿°**: æš‚æ— æè¿°
**ä¸»è¦è¯­è¨€**: Python
**è®¸å¯è¯**: None
**â­ æ˜Ÿæ ‡æ•°**: 518
**ğŸ´ Fork æ•°**: 54
**ğŸ‘€ å…³æ³¨æ•°**: 518
**ğŸ› å¼€æ”¾é—®é¢˜**: 5
**æœ€åæ›´æ–°**: 2025-09-28 (28åˆ†é’Ÿå‰)
**ğŸ“– é¡¹ç›®æ–‡æ¡£**: [README](3ziye-20250928-python/20-Lucy-Edit-ComfyUI-Readme.md)

**ğŸ  é¡¹ç›®ä¸»é¡µ**: æ— 

**ğŸ“‚ å…‹éš†åœ°å€**: `https://github.com/DecartAI/Lucy-Edit-ComfyUI.git`

**ğŸ’» æŠ€æœ¯æ ˆ**: **ä¸»è¦è¯­è¨€**: Python

#### ğŸ¯ æ ¸å¿ƒç‰¹æ€§
- ğŸƒâ€â™‚ï¸ **Motion Preservation** - preserves the motion and composition of videos perfectly, allowing precise edits.
- ğŸ¯ **Edit reliability** â€” edits are more robust when compared to common inference time methods.
- ğŸ§¢ **Wardrobe & accessories** â€” change outfits, add glasses/earrings/hats/etc.
- ğŸ§Œ **Character Changes** â€” replace characters with monsters, animals and known characters. (e.g., "Replace the person with a polar bear")
- ğŸ—ºï¸ **Scenery swap** â€” move the scene (e.g., "transform the scene into a 2D cartoon,")
- ğŸ“ **Pure text instructions** â€” no finetuning, no masks required for common edits

#### ğŸ¨ é€‚ç”¨åœºæ™¯
- æ•°æ®ç§‘å­¦ä¸åˆ†æ
- æœºå™¨å­¦ä¹ é¡¹ç›®
- Web åº”ç”¨å¼€å‘

---