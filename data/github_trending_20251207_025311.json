[
  {
    "basic_info": {
      "name": "llm-council",
      "full_name": "karpathy/llm-council",
      "owner": "karpathy",
      "description": "LLM Council works together to answer your hardest questions",
      "url": "https://github.com/karpathy/llm-council",
      "clone_url": "https://github.com/karpathy/llm-council.git",
      "ssh_url": "git@github.com:karpathy/llm-council.git",
      "homepage": "",
      "created_at": "2025-11-22T23:24:14Z",
      "updated_at": "2025-12-07T02:40:43Z",
      "pushed_at": "2025-11-22T23:35:21Z"
    },
    "stats": {
      "stars": 9974,
      "forks": 1670,
      "watchers": 9974,
      "open_issues": 61,
      "size": 262
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 24729,
        "JavaScript": 20694,
        "CSS": 9346,
        "Shell": 625,
        "HTML": 357
      },
      "license": null,
      "topics": []
    },
    "content": {
      "readme": "# LLM Council\n\n![llmcouncil](header.jpg)\n\nThe idea of this repo is that instead of asking a question to your favorite LLM provider (e.g. OpenAI GPT 5.1, Google Gemini 3.0 Pro, Anthropic Claude Sonnet 4.5, xAI Grok 4, eg.c), you can group them into your \"LLM Council\". This repo is a simple, local web app that essentially looks like ChatGPT except it uses OpenRouter to send your query to multiple LLMs, it then asks them to review and rank each other's work, and finally a Chairman LLM produces the final response.\n\nIn a bit more detail, here is what happens when you submit a query:\n\n1. **Stage 1: First opinions**. The user query is given to all LLMs individually, and the responses are collected. The individual responses are shown in a \"tab view\", so that the user can inspect them all one by one.\n2. **Stage 2: Review**. Each individual LLM is given the responses of the other LLMs. Under the hood, the LLM identities are anonymized so that the LLM can't play favorites when judging their outputs. The LLM is asked to rank them in accuracy and insight.\n3. **Stage 3: Final response**. The designated Chairman of the LLM Council takes all of the model's responses and compiles them into a single final answer that is presented to the user.\n\n## Vibe Code Alert\n\nThis project was 99% vibe coded as a fun Saturday hack because I wanted to explore and evaluate a number of LLMs side by side in the process of [reading books together with LLMs](https://x.com/karpathy/status/1990577951671509438). It's nice and useful to see multiple responses side by side, and also the cross-opinions of all LLMs on each other's outputs. I'm not going to support it in any way, it's provided here as is for other people's inspiration and I don't intend to improve it. Code is ephemeral now and libraries are over, ask your LLM to change it in whatever way you like.\n\n## Setup\n\n### 1. Install Dependencies\n\nThe project uses [uv](https://docs.astral.sh/uv/) for project management.\n\n**Backend:**\n```bash\nuv sync\n```\n\n**Frontend:**\n```bash\ncd frontend\nnpm install\ncd ..\n```\n\n### 2. Configure API Key\n\nCreate a `.env` file in the project root:\n\n```bash\nOPENROUTER_API_KEY=sk-or-v1-...\n```\n\nGet your API key at [openrouter.ai](https://openrouter.ai/). Make sure to purchase the credits you need, or sign up for automatic top up.\n\n### 3. Configure Models (Optional)\n\nEdit `backend/config.py` to customize the council:\n\n```python\nCOUNCIL_MODELS = [\n    \"openai/gpt-5.1\",\n    \"google/gemini-3-pro-preview\",\n    \"anthropic/claude-sonnet-4.5\",\n    \"x-ai/grok-4\",\n]\n\nCHAIRMAN_MODEL = \"google/gemini-3-pro-preview\"\n```\n\n## Running the Application\n\n**Option 1: Use the start script**\n```bash\n./start.sh\n```\n\n**Option 2: Run manually**\n\nTerminal 1 (Backend):\n```bash\nuv run python -m backend.main\n```\n\nTerminal 2 (Frontend):\n```bash\ncd frontend\nnpm run dev\n```\n\nThen open http://localhost:5173 in your browser.\n\n## Tech Stack\n\n- **Backend:** FastAPI (Python 3.10+), async httpx, OpenRouter API\n- **Frontend:** React + Vite, react-markdown for rendering\n- **Storage:** JSON files in `data/conversations/`\n- **Package Management:** uv for Python, npm for JavaScript\n",
      "default_branch": "master"
    },
    "fetched_at": "2025-12-07T02:53:12.526045"
  },
  {
    "basic_info": {
      "name": "Z-Image",
      "full_name": "Tongyi-MAI/Z-Image",
      "owner": "Tongyi-MAI",
      "description": null,
      "url": "https://github.com/Tongyi-MAI/Z-Image",
      "clone_url": "https://github.com/Tongyi-MAI/Z-Image.git",
      "ssh_url": "git@github.com:Tongyi-MAI/Z-Image.git",
      "homepage": null,
      "created_at": "2025-11-26T09:18:10Z",
      "updated_at": "2025-12-07T02:50:25Z",
      "pushed_at": "2025-12-04T11:27:04Z"
    },
    "stats": {
      "stars": 5143,
      "forks": 268,
      "watchers": 5143,
      "open_issues": 38,
      "size": 51664
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 83168
      },
      "license": "Apache License 2.0",
      "topics": []
    },
    "content": {
      "readme": "<h1 align=\"center\">âš¡ï¸- Image<br><sub><sup>An Efficient Image Generation Foundation Model with Single-Stream Diffusion Transformer</sup></sub></h1>\n\n<div align=\"center\">\n\n[![Official Site](https://img.shields.io/badge/Official%20Site-333399.svg?logo=homepage)](https://tongyi-mai.github.io/Z-Image-blog/)&#160;\n[![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97%20Checkpoint-Z--Image--Turbo-yellow)](https://huggingface.co/Tongyi-MAI/Z-Image-Turbo)&#160;\n[![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97%20Online_Demo-Z--Image--Turbo-blue)](https://huggingface.co/spaces/Tongyi-MAI/Z-Image-Turbo)&#160;\n[![ModelScope Model](https://img.shields.io/badge/ğŸ¤–%20Checkpoint-Z--Image--Turbo-624aff)](https://www.modelscope.cn/models/Tongyi-MAI/Z-Image-Turbo)&#160;\n[![ModelScope Space](https://img.shields.io/badge/ğŸ¤–%20Online_Demo-Z--Image--Turbo-17c7a7)](https://www.modelscope.cn/aigc/imageGeneration?tab=advanced&versionId=469191&modelType=Checkpoint&sdVersion=Z_IMAGE_TURBO&modelUrl=modelscope%253A%252F%252FTongyi-MAI%252FZ-Image-Turbo%253Frevision%253Dmaster%7D%7BOnline)&#160;\n[![Art Gallery PDF](https://img.shields.io/badge/%F0%9F%96%BC%20Art_Gallery-PDF-ff69b4)](assets/Z-Image-Gallery.pdf)&#160;\n[![Web Art Gallery](https://img.shields.io/badge/%F0%9F%8C%90%20Web_Art_Gallery-online-00bfff)](https://modelscope.cn/studios/Tongyi-MAI/Z-Image-Gallery/summary)&#160;\n<a href=\"https://arxiv.org/abs/2511.22699\" target=\"_blank\"><img src=\"https://img.shields.io/badge/Report-b5212f.svg?logo=arxiv\" height=\"21px\"></a>\n\n\nWelcome to the official repository for the Z-Imageï¼ˆé€ ç›¸ï¼‰project!\n\n</div>\n\n\n\n## âœ¨ Z-Image\n\nZ-Image is a powerful and highly efficient image generation model with **6B** parameters. Currently there are three variants:\n\n- ğŸš€ **Z-Image-Turbo** â€“ A distilled version of Z-Image that matches or exceeds leading competitors with only **8 NFEs** (Number of Function Evaluations). It offers **âš¡ï¸sub-second inference latencyâš¡ï¸** on enterprise-grade H800 GPUs and fits comfortably within **16G VRAM consumer devices**. It excels in photorealistic image generation, bilingual text rendering (English & Chinese), and robust instruction adherence.\n\n- ğŸ§± **Z-Image-Base** â€“ The non-distilled foundation model. By releasing this checkpoint, we aim to unlock the full potential for community-driven fine-tuning and custom development.\n\n- âœï¸ **Z-Image-Edit** â€“ A variant fine-tuned on Z-Image specifically for image editing tasks. It supports creative image-to-image generation with impressive instruction-following capabilities, allowing for precise edits based on natural language prompts.\n\n### ğŸ“¥ Model Zoo\n\n| Model | Hugging Face                                                                                                                                                                                                                                                                                                              | ModelScope                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |\n| :--- |:--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| **Z-Image-Turbo** | [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97%20Checkpoint%20-Z--Image--Turbo-yellow)](https://huggingface.co/Tongyi-MAI/Z-Image-Turbo) <br> [![Hugging Face Space](https://img.shields.io/badge/%F0%9F%A4%97%20Online%20Demo-Z--Image--Turbo-blue)](https://huggingface.co/spaces/Tongyi-MAI/Z-Image-Turbo) | [![ModelScope Model](https://img.shields.io/badge/ğŸ¤–%20%20Checkpoint-Z--Image--Turbo-624aff)](https://www.modelscope.cn/models/Tongyi-MAI/Z-Image-Turbo) <br> [![ModelScope Space](https://img.shields.io/badge/%F0%9F%A4%96%20Online%20Demo-Z--Image--Turbo-17c7a7)](https://www.modelscope.cn/aigc/imageGeneration?tab=advanced&versionId=469191&modelType=Checkpoint&sdVersion=Z_IMAGE_TURBO&modelUrl=modelscope%3A%2F%2FTongyi-MAI%2FZ-Image-Turbo%3Frevision%3Dmaster) |\n| **",
      "default_branch": "main"
    },
    "fetched_at": "2025-12-07T02:53:13.784625"
  },
  {
    "basic_info": {
      "name": "Depth-Anything-3",
      "full_name": "ByteDance-Seed/Depth-Anything-3",
      "owner": "ByteDance-Seed",
      "description": "Depth Anything 3",
      "url": "https://github.com/ByteDance-Seed/Depth-Anything-3",
      "clone_url": "https://github.com/ByteDance-Seed/Depth-Anything-3.git",
      "ssh_url": "git@github.com:ByteDance-Seed/Depth-Anything-3.git",
      "homepage": "https://depth-anything-3.github.io/",
      "created_at": "2025-11-12T08:44:03Z",
      "updated_at": "2025-12-07T00:41:00Z",
      "pushed_at": "2025-12-04T08:10:20Z"
    },
    "stats": {
      "stars": 3257,
      "forks": 261,
      "watchers": 3257,
      "open_issues": 87,
      "size": 22616
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 657399,
        "Jupyter Notebook": 650520
      },
      "license": "Apache License 2.0",
      "topics": []
    },
    "content": {
      "readme": "<div align=\"center\">\n<h1 style=\"border-bottom: none; margin-bottom: 0px \">Depth Anything 3: Recovering the Visual Space from Any Views</h1>\n<!-- <h2 style=\"border-top: none; margin-top: 3px;\">Recovering the Visual Space from Any Views</h2> -->\n\n\n[**Haotong Lin**](https://haotongl.github.io/)<sup>&ast;</sup> Â· [**Sili Chen**](https://github.com/SiliChen321)<sup>&ast;</sup> Â· [**Jun Hao Liew**](https://liewjunhao.github.io/)<sup>&ast;</sup> Â· [**Donny Y. Chen**](https://donydchen.github.io)<sup>&ast;</sup> Â· [**Zhenyu Li**](https://zhyever.github.io/) Â· [**Guang Shi**](https://scholar.google.com/citations?user=MjXxWbUAAAAJ&hl=en) Â· [**Jiashi Feng**](https://scholar.google.com.sg/citations?user=Q8iay0gAAAAJ&hl=en)\n<br>\n[**Bingyi Kang**](https://bingykang.github.io/)<sup>&ast;&dagger;</sup>\n\n&dagger;project lead&emsp;&ast;Equal Contribution\n\n<a href=\"https://arxiv.org/abs/2511.10647\"><img src='https://img.shields.io/badge/arXiv-Depth Anything 3-red' alt='Paper PDF'></a>\n<a href='https://depth-anything-3.github.io'><img src='https://img.shields.io/badge/Project_Page-Depth Anything 3-green' alt='Project Page'></a>\n<a href='https://huggingface.co/spaces/depth-anything/Depth-Anything-3'><img src='https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Demo-blue'></a>\n<!-- <a href='https://huggingface.co/datasets/depth-anything/VGB'><img src='https://img.shields.io/badge/Benchmark-VisGeo-yellow' alt='Benchmark'></a> -->\n<!-- <a href='https://huggingface.co/datasets/depth-anything/data'><img src='https://img.shields.io/badge/Benchmark-xxx-yellow' alt='Data'></a> -->\n\n</div>\n\nThis work presents **Depth Anything 3 (DA3)**, a model that predicts spatially consistent geometry from\narbitrary visual inputs, with or without known camera poses.\nIn pursuit of minimal modeling, DA3 yields two key insights:\n- ğŸ’ A **single plain transformer** (e.g., vanilla DINO encoder) is sufficient as a backbone without architectural specialization,\n- âœ¨ A singular **depth-ray representation** obviates the need for complex multi-task learning.\n\nğŸ† DA3 significantly outperforms\n[DA2](https://github.com/DepthAnything/Depth-Anything-V2) for monocular depth estimation,\nand [VGGT](https://github.com/facebookresearch/vggt) for multi-view depth estimation and pose estimation.\nAll models are trained exclusively on **public academic datasets**.\n\n<!-- <p align=\"center\">\n  <img src=\"assets/images/da3_teaser.png\" alt=\"Depth Anything 3\" width=\"100%\">\n</p> -->\n<p align=\"center\">\n  <img src=\"assets/images/demo320-2.gif\" alt=\"Depth Anything 3 - Left\" width=\"70%\">\n</p>\n<p align=\"center\">\n  <img src=\"assets/images/da3_radar.png\" alt=\"Depth Anything 3\" width=\"100%\">\n</p>\n\n\n## ğŸ“° News\n- **30-11-2025:** Add [`use_ray_pose`](#use-ray-pose) and [`ref_view_strategy`](docs/funcs/ref_view_strategy.md) (reference view selection for multi-view inputs).   \n- **25-11-2025:** Add [Awesome DA3 Projects](#-awesome-da3-projects), a community-driven section featuring DA3-based applications.\n- **14-11-2025:** Paper, project page, code and models are all released.\n\n## âœ¨ Highlights\n\n### ğŸ† Model Zoo\nWe release three series of models, each tailored for specific use cases in visual geometry.\n\n- ğŸŒŸ **DA3 Main Series** (`DA3-Giant`, `DA3-Large`, `DA3-Base`, `DA3-Small`) These are our flagship foundation models, trained with a unified depth-ray representation. By varying the input configuration, a single model can perform a wide range of tasks:\n  + ğŸŒŠ **Monocular Depth Estimation**: Predicts a depth map from a single RGB image.\n  + ğŸŒŠ **Multi-View Depth Estimation**: Generates consistent depth maps from multiple images for high-quality fusion.\n  + ğŸ¯ **Pose-Conditioned Depth Estimation**: Achieves superior depth consistency when camera poses are provided as input.\n  + ğŸ“· **Camera Pose Estimation**:  Estimates camera extrinsics and intrinsics from one or more images.\n  + ğŸŸ¡ **3D Gaussian Estimation**: Directly predicts 3D Gaussians, enabling high-fidelity novel view synthesis.\n\n- ğŸ“ **DA3 Metric Series** (`DA3Metric-Large`) A specialized model fine-tuned for metric depth estimation in monocular settings, ideal for applications requiring real-world scale.\n\n- ğŸ” **DA3 Monocular Series** (`DA3Mono-Large`). A dedicated model for high-quality relative monocular depth estimation. Unlike disparity-based models (e.g.,  [Depth Anything 2](https://github.com/DepthAnything/Depth-Anything-V2)), it directly predicts depth, resulting in superior geometric accuracy.\n\nğŸ”— Leveraging these available models, we developed a **nested series** (`DA3Nested-Giant-Large`). This series combines a any-view giant model with a metric model to reconstruct visual geometry at a real-world metric scale.\n\n### ğŸ› ï¸ Codebase Features\nOur repository is designed to be a powerful and user-friendly toolkit for both practical application and future research.\n- ğŸ¨ **Interactive Web UI & Gallery**: Visualize model outputs and compare results with an easy-to-use Gradio-based web interface.\n- âš¡ **Flexible Command-Line Interface (CLI)**: Power",
      "default_branch": "main"
    },
    "fetched_at": "2025-12-07T02:53:15.048522"
  },
  {
    "basic_info": {
      "name": "RedInk",
      "full_name": "HisMax/RedInk",
      "owner": "HisMax",
      "description": "çº¢å¢¨ - åŸºäºğŸŒNano Banana ProğŸŒ çš„ä¸€ç«™å¼å°çº¢ä¹¦å›¾æ–‡ç”Ÿæˆå™¨ ã€Šä¸€å¥è¯ä¸€å¼ å›¾ç‰‡ç”Ÿæˆå°çº¢ä¹¦å›¾æ–‡ã€‹ Red Ink - A one-stop Xiaohongshu image-and-text generator based on the ğŸŒNano Banana ProğŸŒ, \"One Sentence, One Image: Generate Xiaohongshu Text and Images.\"",
      "url": "https://github.com/HisMax/RedInk",
      "clone_url": "https://github.com/HisMax/RedInk.git",
      "ssh_url": "git@github.com:HisMax/RedInk.git",
      "homepage": "",
      "created_at": "2025-11-25T10:12:54Z",
      "updated_at": "2025-12-07T02:42:50Z",
      "pushed_at": "2025-11-29T19:43:23Z"
    },
    "stats": {
      "stars": 2994,
      "forks": 595,
      "watchers": 2994,
      "open_issues": 13,
      "size": 19227
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 189067,
        "Vue": 117985,
        "TypeScript": 33936,
        "CSS": 23861,
        "Dockerfile": 1568,
        "HTML": 349
      },
      "license": "Other",
      "topics": []
    },
    "content": {
      "readme": "![](images/logo.png)\n\n---\n\n[![License: CC BY-NC-SA 4.0](https://img.shields.io/badge/License-CC%20BY--NC--SA%204.0-lightgrey.svg)](https://creativecommons.org/licenses/by-nc-sa/4.0/)\n[![Python 3.11+](https://img.shields.io/badge/python-3.11+-blue.svg)](https://www.python.org/downloads/)\n[![Vue 3](https://img.shields.io/badge/vue-3.x-green.svg)](https://vuejs.org/)\n\n# çº¢å¢¨ - å°çº¢ä¹¦AIå›¾æ–‡ç”Ÿæˆå™¨\n\n> è®©ä¼ æ’­ä¸å†éœ€è¦é—¨æ§›ï¼Œè®©åˆ›ä½œä»æœªå¦‚æ­¤ç®€å•\n\n![](images/index.gif)\n\n<p align=\"center\">\n  <em>çº¢å¢¨é¦–é¡µ</em>\n</p>\n\n<p align=\"center\">\n  <img src=\"images/showcase-grid.png\" alt=\"ä½¿ç”¨çº¢å¢¨ç”Ÿæˆçš„å„ç±»å°çº¢ä¹¦å°é¢\" width=\"600\"/>\n</p>\n\n<p align=\"center\">\n  <em>ä½¿ç”¨çº¢å¢¨ç”Ÿæˆçš„å„ç±»å°çº¢ä¹¦å°é¢ - AIé©±åŠ¨ï¼Œé£æ ¼ç»Ÿä¸€ï¼Œæ–‡å­—å‡†ç¡®</em>\n</p>\n\n\n\n## å†™åœ¨å‰é¢\n\nå‰æ®µæ—¶é—´é»˜å­åœ¨ Linux.do å‘äº†ä¸€ä¸ªç”¨ Nano banana Pro åš PPT çš„å¸–å­,æ”¶è·äº† 600 å¤šä¸ªèµã€‚å¾ˆå¤šäººç”¨ğŸŒNano banana Pro å»åšäº§å“å®£ä¼ å›¾ã€ç›´æ¥ç”Ÿæˆæ¼«ç”»ç­‰ç­‰ã€‚æˆ‘å°±åœ¨æƒ³:**ä¸ºä»€ä¹ˆä¸æ‹¿ğŸŒ2æ¥åšç‚¹æ›´åŠŸåˆ©ã€æ›´åˆºæ¿€çš„äº‹æƒ…?**\n\näºæ˜¯å°±æœ‰äº†è¿™ä¸ªé¡¹ç›®ã€‚ä¸€å¥è¯ä¸€å¼ å›¾ç‰‡ç”Ÿæˆå°çº¢ä¹¦å›¾æ–‡\n\n---\n\n## âœ¨ æ•ˆæœå±•ç¤º\n\n### è¾“å…¥ä¸€å¥è¯,å°±èƒ½ç”Ÿæˆå®Œæ•´çš„å°çº¢ä¹¦å›¾æ–‡\n\n#### æç¤ºè¯ï¼šç§‹å­£æ˜¾ç™½ç¾ç”²ï¼ˆæš—å¹¿ä¸€ä¸ªï¼šé»˜å­ç‰Œç¾ç”²ï¼‰ï¼Œå›¾ç‰‡ æ˜¯æˆ‘çš„å°çº¢ä¹¦ä¸»é¡µã€‚ç¬¦åˆæˆ‘çš„é£æ ¼ç”Ÿæˆ\n\n#### åŒæ—¶æˆ‘è¿˜æˆªå›¾äº†æˆ‘çš„å°çº¢ä¹¦ä¸»é¡µï¼ŒåŒ…æ‹¬æˆ‘çš„å¤´åƒï¼Œç­¾åï¼ŒèƒŒæ™¯ï¼Œå§“åä»€ä¹ˆçš„\n\n![ç¤ºä¾‹1](./images/example-1.png)\n\n#### ç„¶åç­‰å¾…10-20ç§’åï¼Œå°±ä¼šæœ‰æ¯ä¸€é¡µçš„å¤§çº²ï¼Œå¤§å®¶å¯ä»¥æ ¹æ®çš„è‡ªå·±çš„éœ€æ±‚å»è°ƒæ•´é¡µé¢é¡ºåºï¼ˆä¸å»ºè®®ï¼‰ï¼Œè‡ªå®šä¹‰æ¯ä¸€ä¸ªé¡µé¢çš„å†…å®¹ï¼ˆè¿™ä¸ªå¾ˆå»ºè®®ï¼‰\n\n![ç¤ºä¾‹2](./images/example-2.png)\n\n#### é¦–å…ˆç”Ÿæˆçš„æ˜¯å°é¢é¡µ\n\n![ç¤ºä¾‹3](./images/example-3.png)\n\n#### ç„¶åç¨ç­‰ä¸€ä¼šå„¿åï¼Œä¼šç”Ÿæˆåé¢çš„æ‰€æœ‰é¡µé¢ï¼ˆè¿™é‡Œæ˜¯å¹¶å‘ç”Ÿæˆçš„æ‰€æœ‰é¡µé¢ï¼ˆé»˜è®¤æ˜¯15ä¸ªï¼‰ï¼Œå¦‚æœå¤§å®¶çš„APIä¾›åº”å•†æ— æ³•æ”¯æŒé«˜å¹¶å‘çš„è¯ï¼Œè®°å¾—è¦å»æ”¹ä¸€ä¸‹è®¾ç½®ï¼‰\n\n![ç¤ºä¾‹4](./images/example-4.png)\n\n---\n\n## ğŸ—ï¸ æŠ€æœ¯æ¶æ„\n\n### åç«¯\n- **è¯­è¨€**: Python 3.11+\n- **æ¡†æ¶**: Flask\n- **AI æ¨¡å‹**:\n  - Gemini 3 (æ–‡æ¡ˆç”Ÿæˆ)\n  - ğŸŒNano banana Pro (å›¾ç‰‡ç”Ÿæˆ)\n- **åŒ…ç®¡ç†**: uv\n\n### å‰ç«¯\n- **æ¡†æ¶**: Vue 3 + TypeScript\n- **æ„å»º**: Vite\n- **çŠ¶æ€ç®¡ç†**: Pinia\n\n---\n\n## ğŸ“¦ å¦‚ä½•è‡ªå·±éƒ¨ç½²\n\n### æ–¹å¼ä¸€ï¼šDocker éƒ¨ç½²ï¼ˆæ¨èï¼‰\n\n**æœ€ç®€å•çš„éƒ¨ç½²æ–¹å¼ï¼Œä¸€è¡Œå‘½ä»¤å³å¯å¯åŠ¨ï¼š**\n\n```bash\ndocker run -d -p 12398:12398 -v ./history:/app/history -v ./output:/app/output histonemax/redink:latest\n```\n\nè®¿é—® http://localhost:12398ï¼Œåœ¨ Web ç•Œé¢çš„**è®¾ç½®é¡µé¢**é…ç½®ä½ çš„ API Key å³å¯ä½¿ç”¨ã€‚\n\n**ä½¿ç”¨ docker-composeï¼ˆå¯é€‰ï¼‰ï¼š**\n\nä¸‹è½½ [docker-compose.yml](https://github.com/HisMax/RedInk/blob/main/docker-compose.yml) åï¼š\n\n```bash\ndocker-compose up -d\n```\n\n**Docker éƒ¨ç½²è¯´æ˜ï¼š**\n- å®¹å™¨å†…ä¸åŒ…å«ä»»ä½• API Keyï¼Œéœ€è¦åœ¨ Web ç•Œé¢é…ç½®\n- ä½¿ç”¨ `-v ./history:/app/history` æŒä¹…åŒ–å†å²è®°å½•\n- ä½¿ç”¨ `-v ./output:/app/output` æŒä¹…åŒ–ç”Ÿæˆçš„å›¾ç‰‡\n- å¯é€‰ï¼šæŒ‚è½½è‡ªå®šä¹‰é…ç½®æ–‡ä»¶ `-v ./text_providers.yaml:/app/text_providers.yaml`\n\n---\n\n### æ–¹å¼äºŒï¼šæœ¬åœ°å¼€å‘éƒ¨ç½²\n\n**å‰ç½®è¦æ±‚ï¼š**\n- Python 3.11+\n- Node.js 18+\n- pnpm\n- uv\n\n### 1. å…‹éš†é¡¹ç›®\n```bash\ngit clone https://github.com/HisMax/RedInk.git\ncd RedInk\n```\n\n### 2. é…ç½® API æœåŠ¡\n\nå¤åˆ¶é…ç½®æ¨¡æ¿æ–‡ä»¶ï¼š\n```bash\ncp text_providers.yaml.example text_providers.yaml\ncp image_providers.yaml.example image_providers.yaml\n```\n\nç¼–è¾‘é…ç½®æ–‡ä»¶ï¼Œå¡«å…¥ä½ çš„ API Key å’ŒæœåŠ¡é…ç½®ã€‚ä¹Ÿå¯ä»¥å¯åŠ¨ååœ¨ Web ç•Œé¢çš„**è®¾ç½®é¡µé¢**è¿›è¡Œé…ç½®ã€‚\n\n### 3. å®‰è£…åç«¯ä¾èµ–\n```bash\nuv sync\n```\n\n### 4. å®‰è£…å‰ç«¯ä¾èµ–\n```bash\ncd frontend\npnpm install\n```\n\n### 5. å¯åŠ¨æœåŠ¡\n\n**å¯åŠ¨åç«¯:**\n```bash\nuv run python -m backend.app\n```\nè®¿é—®: http://localhost:12398\n\n**å¯åŠ¨å‰ç«¯:**\n```bash\ncd frontend\npnpm dev\n```\nè®¿é—®: http://localhost:5173\n\n---\n\n## ğŸ® ä½¿ç”¨æŒ‡å—\n\n### åŸºç¡€ä½¿ç”¨\n1. **è¾“å…¥ä¸»é¢˜**: åœ¨é¦–é¡µè¾“å…¥æƒ³è¦åˆ›ä½œçš„ä¸»é¢˜,å¦‚\"å¦‚ä½•åœ¨å®¶åšæ‹¿é“\"\n2. **ç”Ÿæˆå¤§çº²**: AI è‡ªåŠ¨ç”Ÿæˆ 6-9 é¡µçš„å†…å®¹å¤§çº²\n3. **ç¼–è¾‘ç¡®è®¤**: å¯ä»¥ç¼–è¾‘å’Œè°ƒæ•´æ¯ä¸€é¡µçš„æè¿°\n4. **ç”Ÿæˆå›¾ç‰‡**: ç‚¹å‡»ç”Ÿæˆ,å®æ—¶æŸ¥çœ‹è¿›åº¦\n5. **ä¸‹è½½ä½¿ç”¨**: ä¸€é”®ä¸‹è½½æ‰€æœ‰å›¾ç‰‡\n\n### è¿›é˜¶ä½¿ç”¨\n- **ä¸Šä¼ å‚è€ƒå›¾ç‰‡**: é€‚åˆå“ç‰Œæ–¹,ä¿æŒå“ç‰Œè§†è§‰é£æ ¼\n- **ä¿®æ”¹æè¿°è¯**: ç²¾ç¡®æ§åˆ¶æ¯ä¸€é¡µçš„å†…å®¹å’Œæ„å›¾\n- **é‡æ–°ç”Ÿæˆ**: å¯¹ä¸æ»¡æ„çš„é¡µé¢å•ç‹¬é‡æ–°ç”Ÿæˆ\n\n---\n\n## ğŸ”§ é…ç½®è¯´æ˜\n\n### é…ç½®æ–¹å¼\n\né¡¹ç›®æ”¯æŒä¸¤ç§é…ç½®æ–¹å¼ï¼š\n\n1. **Web ç•Œé¢é…ç½®ï¼ˆæ¨èï¼‰**ï¼šå¯åŠ¨æœåŠ¡åï¼Œåœ¨è®¾ç½®é¡µé¢å¯è§†åŒ–é…ç½®\n2. **YAML æ–‡ä»¶é…ç½®**ï¼šç›´æ¥ç¼–è¾‘é…ç½®æ–‡ä»¶\n\n### æ–‡æœ¬ç”Ÿæˆé…ç½®\n\né…ç½®æ–‡ä»¶: `text_providers.yaml`\n\n```yaml\n# å½“å‰æ¿€æ´»çš„æœåŠ¡å•†\nactive_provider: openai\n\nproviders:\n  # OpenAI å®˜æ–¹æˆ–å…¼å®¹æ¥å£\n  openai:\n    type: openai_compatible\n    api_key: sk-xxxxxxxxxxxxxxxxxxxx\n    base_url: https://api.openai.com/v1\n    model: gpt-4o\n\n  # Google Geminiï¼ˆåŸç”Ÿæ¥å£ï¼‰\n  gemini:\n    type: google_gemini\n    api_key: AIzaxxxxxxxxxxxxxxxxxxxxxxxxx\n    model: gemini-2.0-flash\n```\n\n### å›¾ç‰‡ç”Ÿæˆé…ç½®\n\né…ç½®æ–‡ä»¶: `image_providers.yaml`\n\n```yaml\n# å½“å‰æ¿€æ´»çš„æœåŠ¡å•†\nactive_provider: gemini\n\nproviders:\n  # Google Gemini å›¾ç‰‡ç”Ÿæˆ\n  gemini:\n    type: google_genai\n    api_key: AIzaxxxxxxxxxxxxxxxxxxxxxxxxx\n    model: gemini-3-pro-image-preview\n    high_concurrency: false  # é«˜å¹¶å‘æ¨¡å¼\n\n  # OpenAI å…¼å®¹æ¥å£\n  openai_image:\n    type: image_api\n    api_key: sk-xxxxxxxxxxxxxxxxxxxx\n    base_url: https://your-api-endpoint.com\n    model: dall-e-3\n    high_concurrency: false\n```\n\n### é«˜å¹¶å‘æ¨¡å¼è¯´æ˜\n\n- **å…³é—­ï¼ˆé»˜è®¤ï¼‰**ï¼šå›¾ç‰‡é€å¼ ç”Ÿæˆï¼Œé€‚åˆ GCP 300$ è¯•ç”¨è´¦å·æˆ–æœ‰é€Ÿç‡é™åˆ¶çš„ API\n- **å¼€å¯**ï¼šå›¾ç‰‡å¹¶è¡Œç”Ÿæˆï¼ˆæœ€å¤š15å¼ åŒæ—¶ï¼‰ï¼Œé€Ÿåº¦æ›´å¿«ï¼Œä½†éœ€è¦ API æ”¯æŒé«˜å¹¶å‘\n\nâš ï¸ **GCP 300$ è¯•ç”¨è´¦å·ä¸å»ºè®®å¯ç”¨é«˜å¹¶å‘**ï¼Œå¯èƒ½ä¼šè§¦å‘é€Ÿç‡é™åˆ¶å¯¼è‡´ç”Ÿæˆå¤±è´¥ã€‚\n\n---\n\n## âš ï¸ æ³¨æ„äº‹é¡¹\n\n1. **API é…é¢é™åˆ¶**:\n   - æ³¨æ„ Gemini å’Œå›¾ç‰‡ç”Ÿæˆ API çš„è°ƒç”¨é…é¢\n   - GCP è¯•ç”¨è´¦å·å»ºè®®å…³é—­é«˜å¹¶å‘æ¨¡å¼\n\n2. **ç”Ÿæˆæ—¶é—´**:\n   - å›¾ç‰‡ç”Ÿæˆéœ€è¦æ—¶é—´,è¯·è€å¿ƒç­‰å¾…ï¼ˆä¸è¦ç¦»å¼€é¡µé¢ï¼‰\n\n---\n\n## ğŸ¤ å‚ä¸è´¡çŒ®\n\næ¬¢è¿æäº¤ Issue å’Œ Pull Request!\n\nå¦‚æœè¿™ä¸ªé¡¹ç›®å¯¹ä½ æœ‰å¸®åŠ©,æ¬¢è¿ç»™ä¸ª Star â­\n\n### æœªæ¥è®¡åˆ’\n- [ ] æ”¯æŒæ›´å¤šå›¾ç‰‡æ ¼å¼ï¼Œä¾‹å¦‚ä¸€å¥è¯ç”Ÿæˆä¸€å¥—PPTä»€ä¹ˆçš„\n- [x] å†å²è®°å½•ç®¡ç†ä¼˜åŒ–\n- [ ] å¯¼å‡ºä¸ºå„ç§æ ¼å¼(PDFã€é•¿å›¾ç­‰)\n\n---\n\n## æ›´æ–°æ—¥å¿—\n\n### v1.4.0 (2025-11-30)\n- ğŸ—ï¸ åç«¯æ¶æ„é‡æ„ï¼šæ‹†åˆ†å•ä½“è·¯ç”±ä¸ºæ¨¡å—åŒ–è“å›¾ï¼ˆhistoryã€imagesã€generationã€outlineã€configï¼‰\n- ğŸ—ï¸ å‰ç«¯ç»„ä»¶é‡æ„ï¼šæå–å¯å¤ç”¨ç»„ä»¶ï¼ˆImageGalleryModalã€OutlineModalã€ShowcaseBackgroundç­‰ï¼‰\n- âœ¨ ä¼˜åŒ–é¦–é¡µè®¾è®¡ï¼Œç§»é™¤å†—ä½™å†…å®¹åŒºå—\n- âœ¨ èƒŒæ™¯å›¾ç‰‡é¢„åŠ è½½å’Œæ¸å…¥åŠ¨ç”»ï¼Œæå‡åŠ è½½ä½“éªŒ\n- âœ¨ å†å²è®°å½•æŒä¹…åŒ–æ”¯æŒï¼ˆDockeréƒ¨ç½²ï¼‰\n- ğŸ”§ ä¿®å¤å†å²è®°å½•é¢„è§ˆå’Œå¤§çº²æŸ¥çœ‹åŠŸèƒ½\n- ğŸ”§ ä¼˜åŒ–Modalç»„ä»¶å¯è§æ€§æ§åˆ¶\n- ğŸ§ª æ–°å¢65ä¸ªåç«¯å•å…ƒæµ‹è¯•\n\n### v1.3.0 (2025-11-26)\n- âœ¨ æ–°å¢ Docker æ”¯æŒï¼Œä¸€é”®éƒ¨ç½²\n- âœ¨ å‘å¸ƒå®˜æ–¹ Docker é•œåƒåˆ° Docker Hub: `histonemax/redink`\n- ğŸ”§ Flask è‡ªåŠ¨æ£€æµ‹å‰ç«¯æ„å»ºäº§ç‰©ï¼Œæ”¯æŒå•å®¹å™¨éƒ¨ç½²\n- ğŸ”§ Docker é•œåƒå†…ç½®ç©ºç™½é…ç½®æ¨¡æ¿ï¼Œä¿æŠ¤ API Key å®‰å…¨\n- ğŸ“ æ›´æ–° READMEï¼Œæ·»åŠ  Docker éƒ¨ç½²è¯´æ˜\n\n### v1.2.0 (2025-11-26)\n- âœ¨ æ–°å¢ç‰ˆæƒä¿¡æ¯å±•ç¤ºï¼Œæ‰€æœ‰é¡µé¢æ˜¾ç¤ºå¼€æºåè®®å’Œé¡¹ç›®é“¾æ¥\n- âœ¨ ä¼˜åŒ–å›¾ç‰‡é‡æ–°ç”ŸæˆåŠŸèƒ½ï¼Œæ”¯æŒå•å¼ å›¾ç‰‡é‡ç»˜\n- âœ¨ é‡æ–°ç”Ÿæˆå›¾ç‰‡æ—¶ä¿æŒé£æ ¼ä¸€è‡´ï¼Œä¼ é€’å®Œæ•´ä¸Šä¸‹æ–‡ï¼ˆå°é¢å›¾ã€å¤§çº²ã€ç”¨æˆ·è¾“å…¥ï¼‰\n- âœ¨ ä¿®å¤å›¾ç‰‡ç¼“å­˜é—®é¢˜ï¼Œé‡æ–°ç”Ÿæˆçš„å›¾ç‰‡ç«‹å³åˆ·æ–°æ˜¾ç¤º\n- âœ¨ ç»Ÿä¸€æ–‡æœ¬ç”Ÿæˆå®¢æˆ·ç«¯æ¥å£ï¼Œæ”¯æŒ Google Gemini å’Œ OpenAI å…¼å®¹æ¥å£è‡ªåŠ¨åˆ‡æ¢\n- âœ¨ æ–°å¢ Web ç•Œé¢é…ç½®åŠŸèƒ½ï¼Œå¯è§†åŒ–ç®¡ç† API æœåŠ¡å•†\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-12-07T02:53:16.293611"
  },
  {
    "basic_info": {
      "name": "reader3",
      "full_name": "karpathy/reader3",
      "owner": "karpathy",
      "description": "Quick illustration of how one can easily read books together with LLMs. It's great and I highly recommend it.",
      "url": "https://github.com/karpathy/reader3",
      "clone_url": "https://github.com/karpathy/reader3.git",
      "ssh_url": "git@github.com:karpathy/reader3.git",
      "homepage": null,
      "created_at": "2025-11-18T02:37:00Z",
      "updated_at": "2025-12-07T01:13:03Z",
      "pushed_at": "2025-11-18T02:37:51Z"
    },
    "stats": {
      "stars": 2522,
      "forks": 310,
      "watchers": 2522,
      "open_issues": 12,
      "size": 271
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 13925,
        "HTML": 8921
      },
      "license": null,
      "topics": []
    },
    "content": {
      "readme": "# reader 3\n\n![reader3](reader3.png)\n\nA lightweight, self-hosted EPUB reader that lets you read through EPUB books one chapter at a time. This makes it very easy to copy paste the contents of a chapter to an LLM, to read along. Basically - get epub books (e.g. [Project Gutenberg](https://www.gutenberg.org/) has many), open them up in this reader, copy paste text around to your favorite LLM, and read together and along.\n\nThis project was 90% vibe coded just to illustrate how one can very easily [read books together with LLMs](https://x.com/karpathy/status/1990577951671509438). I'm not going to support it in any way, it's provided here as is for other people's inspiration and I don't intend to improve it. Code is ephemeral now and libraries are over, ask your LLM to change it in whatever way you like.\n\n## Usage\n\nThe project uses [uv](https://docs.astral.sh/uv/). So for example, download [Dracula EPUB3](https://www.gutenberg.org/ebooks/345) to this directory as `dracula.epub`, then:\n\n```bash\nuv run reader3.py dracula.epub\n```\n\nThis creates the directory `dracula_data`, which registers the book to your local library. We can then run the server:\n\n```bash\nuv run server.py\n```\n\nAnd visit [localhost:8123](http://localhost:8123/) to see your current Library. You can easily add more books, or delete them from your library by deleting the folder. It's not supposed to be complicated or complex.\n\n## License\n\nMIT",
      "default_branch": "master"
    },
    "fetched_at": "2025-12-07T02:53:17.573003"
  },
  {
    "basic_info": {
      "name": "JiT",
      "full_name": "LTH14/JiT",
      "owner": "LTH14",
      "description": "PyTorch implementation of JiT https://arxiv.org/abs/2511.13720",
      "url": "https://github.com/LTH14/JiT",
      "clone_url": "https://github.com/LTH14/JiT.git",
      "ssh_url": "git@github.com:LTH14/JiT.git",
      "homepage": "",
      "created_at": "2025-11-10T22:37:40Z",
      "updated_at": "2025-12-07T02:41:37Z",
      "pushed_at": "2025-11-18T03:24:51Z"
    },
    "stats": {
      "stars": 1633,
      "forks": 88,
      "watchers": 1633,
      "open_issues": 22,
      "size": 67601
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 56577
      },
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "## Just image Transformer (JiT) for Pixel-space Diffusion\n\n[![arXiv](https://img.shields.io/badge/arXiv%20paper-2511.13720-b31b1b.svg)](https://arxiv.org/abs/2511.13720)&nbsp;\n\n<p align=\"center\">\n  <img src=\"demo/visual.jpg\" width=\"100%\">\n</p>\n\n\nThis is a PyTorch/GPU re-implementation of the paper [Back to Basics: Let Denoising Generative Models Denoise](https://arxiv.org/abs/2511.13720):\n\n```\n@article{li2025jit,\n  title={Back to Basics: Let Denoising Generative Models Denoise},\n  author={Li, Tianhong and He, Kaiming},\n  journal={arXiv preprint arXiv:2511.13720},\n  year={2025}\n}\n```\n\nJiT adopts a minimalist and self-contained design for pixel-level high-resolution image diffusion. \nThe original implementation was in JAX+TPU. This re-implementation is in PyTorch+GPU.\n\n<p align=\"center\">\n  <img src=\"demo/jit.jpg\" width=\"40%\">\n</p>\n\n### Dataset\nDownload [ImageNet](http://image-net.org/download) dataset, and place it in your `IMAGENET_PATH`.\n\n### Installation\n\nDownload the code:\n```\ngit clone https://github.com/LTH14/JiT.git\ncd JiT\n```\n\nA suitable [conda](https://conda.io/) environment named `jit` can be created and activated with:\n\n```\nconda env create -f environment.yaml\nconda activate jit\n```\n\nIf you get ```undefined symbol: iJIT_NotifyEvent``` when importing ```torch```, simply\n```\npip uninstall torch\npip install torch==2.5.1 --index-url https://download.pytorch.org/whl/cu124\n```\nCheck this [issue](https://github.com/conda/conda/issues/13812#issuecomment-2071445372) for more details.\n\n### Training\nThe below training scripts have been tested on 8 H200 GPUs.\n\nExample script for training JiT-B/16 on ImageNet 256x256 for 600 epochs:\n```\ntorchrun --nproc_per_node=8 --nnodes=1 --node_rank=0 \\\nmain_jit.py \\\n--model JiT-B/16 \\\n--proj_dropout 0.0 \\\n--P_mean -0.8 --P_std 0.8 \\\n--img_size 256 --noise_scale 1.0 \\\n--batch_size 128 --blr 5e-5 \\\n--epochs 600 --warmup_epochs 5 \\\n--gen_bsz 128 --num_images 50000 --cfg 2.9 --interval_min 0.1 --interval_max 1.0 \\\n--output_dir ${OUTPUT_DIR} --resume ${OUTPUT_DIR} \\\n--data_path ${IMAGENET_PATH} --online_eval\n```\n\nExample script for training JiT-B/32 on ImageNet 512x512 for 600 epochs:\n```\ntorchrun --nproc_per_node=8 --nnodes=1 --node_rank=0 \\\nmain_jit.py \\\n--model JiT-B/32 \\\n--proj_dropout 0.0 \\\n--P_mean -0.8 --P_std 0.8 \\\n--img_size 512 --noise_scale 2.0 \\\n--batch_size 128 --blr 5e-5 \\\n--epochs 600 --warmup_epochs 5 \\\n--gen_bsz 128 --num_images 50000 --cfg 2.9 --interval_min 0.1 --interval_max 1.0 \\\n--output_dir ${OUTPUT_DIR} --resume ${OUTPUT_DIR} \\\n--data_path ${IMAGENET_PATH} --online_eval\n```\n\nExample script for training JiT-H/16 on ImageNet 256x256 for 600 epochs:\n```\ntorchrun --nproc_per_node=8 --nnodes=1 --node_rank=0 \\\nmain_jit.py \\\n--model JiT-H/16 \\\n--proj_dropout 0.2 \\\n--P_mean -0.8 --P_std 0.8 \\\n--img_size 256 --noise_scale 1.0 \\\n--batch_size 128 --blr 5e-5 \\\n--epochs 600 --warmup_epochs 5 \\\n--gen_bsz 128 --num_images 50000 --cfg 2.2 --interval_min 0.1 --interval_max 1.0 \\\n--output_dir ${OUTPUT_DIR} --resume ${OUTPUT_DIR} \\\n--data_path ${IMAGENET_PATH} --online_eval\n```\n\n### Evaluation\n\nEvaluate a trained JiT:\n```\ntorchrun --nproc_per_node=8 --nnodes=1 --node_rank=0 \\\nmain_jit.py \\\n--model JiT-B/16 \\\n--img_size 256 --noise_scale 1.0 \\\n--gen_bsz 128 --num_images 50000 --cfg 2.9 --interval_min 0.1 --interval_max 1.0 \\\n--output_dir ${CKPT_DIR} --resume ${CKPT_DIR} \\\n--data_path ${IMAGENET_PATH} --evaluate_gen\n```\n\nWe use a customized [```torch-fidelity```](https://github.com/LTH14/torch-fidelity)\nto evaluate FID and IS against a reference image folder or statistics. You can use ```prepare_ref.py```\nto prepare the reference image folder, or directly use our pre-computed reference stats\nunder ```fid_stats```.\n\n### Acknowledgements\n\nWe thank Google TPU Research Cloud (TRC) for granting us access to TPUs, and the MIT\nORCD Seed Fund Grants for supporting GPU resources.\n\n### Contact\n\nIf you have any questions, feel free to contact me through email (tianhong@mit.edu). Enjoy!\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-12-07T02:53:18.918582"
  },
  {
    "basic_info": {
      "name": "HunyuanVideo-1.5",
      "full_name": "Tencent-Hunyuan/HunyuanVideo-1.5",
      "owner": "Tencent-Hunyuan",
      "description": "HunyuanVideo-1.5: A leading lightweight video generation model",
      "url": "https://github.com/Tencent-Hunyuan/HunyuanVideo-1.5",
      "clone_url": "https://github.com/Tencent-Hunyuan/HunyuanVideo-1.5.git",
      "ssh_url": "git@github.com:Tencent-Hunyuan/HunyuanVideo-1.5.git",
      "homepage": "https://hunyuan.tencent.com/video/zh?tabIndex=0",
      "created_at": "2025-11-20T06:15:42Z",
      "updated_at": "2025-12-07T02:42:55Z",
      "pushed_at": "2025-12-05T16:37:01Z"
    },
    "stats": {
      "stars": 1612,
      "forks": 76,
      "watchers": 1612,
      "open_issues": 23,
      "size": 448
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 482314
      },
      "license": "Other",
      "topics": [
        "image-to-video",
        "text-to-video",
        "video-generation"
      ]
    },
    "content": {
      "readme": "[ä¸­æ–‡æ–‡æ¡£](./README_CN.md)\n\n# HunyuanVideo-1.5\n\n<div align=\"center\">\n\n<img src=\"./assets/logo.png\" alt=\"HunyuanVideo-1.5 Logo\" width=\"80%\">\n\n# ğŸ¬ HunyuanVideo-1.5: A leading lightweight video generation model\n\n</div>\n\n\n<div align=\"center\">\n<!-- <img src=\"./assets/banner.png\" alt=\"HunyuanVideo-1.5 Banner\" width=\"800\"> -->\n\n</div>\n\n\nHunyuanVideo-1.5 is a video generation model that delivers top-tier quality with only 8.3B parameters, significantly lowering the barrier to usage. It runs smoothly on consumer-grade GPUs, making it accessible for every developer and creator. This repository provides the implementation and tools needed to generate creative videos.\n\n\n<div align=\"center\">\n  <a href=\"https://hunyuan.tencent.com/video/zh?tabIndex=0\" target=\"_blank\"><img src=https://img.shields.io/badge/Official%20Site-333399.svg?logo=homepage height=22px></a>\n  <a href=https://huggingface.co/tencent/HunyuanVideo-1.5 target=\"_blank\"><img src=https://img.shields.io/badge/%F0%9F%A4%97%20Models-d96902.svg height=22px></a>\n  <a href=https://github.com/Tencent-Hunyuan/HunyuanVideo-1.5 target=\"_blank\"><img src= https://img.shields.io/badge/Page-bb8a2e.svg?logo=github height=22px></a>\n  <a href=\"https://arxiv.org/pdf/2511.18870\" target=\"_blank\"><img src=https://img.shields.io/badge/Report-b5212f.svg?logo=arxiv height=22px></a>\n  <a href=https://x.com/TencentHunyuan target=\"_blank\"><img src=https://img.shields.io/badge/Hunyuan-black.svg?logo=x height=22px></a>\n  <a href=\"https://github.com/Tencent-Hunyuan/HunyuanVideo-1.5/blob/main/assets/HunyuanVideo_1_5_Prompt_Handbook_EN.md\" target=\"_blank\"><img src=https://img.shields.io/badge/ğŸ“š-PromptHandBook-blue.svg?logo=book height=22px></a> <br/>\n  <a href=\"./ComfyUI/README.md\" target=\"_blank\"><img src=https://img.shields.io/badge/ComfyUI-blue.svg?logo=book height=22px></a>\n  <a href=\"https://github.com/ModelTC/LightX2V\" target=\"_blank\"><img src=https://img.shields.io/badge/LightX2V-yellow.svg?logo=book height=22px></a>\n  <a href=\"https://tusi.cn/models/933574988890423836\" target=\"_blank\"><img src=https://img.shields.io/badge/åå¸-purple.svg?logo=book height=22px></a>\n  <a href=\"https://tensor.art/models/933574988890423836\" target=\"_blank\"><img src=https://img.shields.io/badge/TensorArt-cyan.svg?logo=book height=22px></a>\n\n</div>\n\n\n<p align=\"center\">\n    ğŸ‘ Join our <a href=\"./assets/wechat.png\" target=\"_blank\">WeChat</a> and <a href=\"https://discord.gg/ehjWMqF5wY\">Discord</a> | \nğŸ’» <a href=\"https://hunyuan.tencent.com/video/zh?tabIndex=0\">Official website Try our model!</a>&nbsp&nbsp\n</p>\n\n## ğŸ”¥ğŸ”¥ğŸ”¥ News\n* ğŸš€ Dec 05, 2025: **New Release**: We now release the [480p I2V step-distilled model](https://huggingface.co/tencent/HunyuanVideo-1.5/tree/main/transformer/480p_i2v_step_distilled), which generates videos in 8 or 12 steps (recommended)! On RTX 4090, end-to-end generation time is reduced by 75%, and a single RTX 4090 can generate videos within **75 seconds**. The step-distilled model maintains comparable quality to the original model while achieving significant speedup. See [Step Distillation Comparison](./assets/step_distillation_comparison.md) for detailed quality comparisons. For even faster generation, you can also try 4 steps (faster speed with slightly reduced quality). **To enable the step-distilled model, run `generate.py` with the `--enable_step_distill` parameter.** See [Usage](#-usage) for detailed usage instructions. ğŸ”¥ğŸ”¥ğŸ”¥ğŸ†•\n* ğŸ“š Dec 05, 2025: **Training Code Released**: We now open-source the training code for HunyuanVideo-1.5! The training script (`train.py`) provides a full training pipeline with support for distributed training, FSDP, context parallel, gradient checkpointing, and more. HunyuanVideo-1.5 is trained using the Muon optimizer, which we have open-sourced in the [Training](#-training) section. **If you would like to continue training our model or fine-tune it with LoRA, please use the Muon optimizer.** See [Training](#-training) section for detailed usage instructions. ğŸ”¥ğŸ”¥ğŸ”¥ğŸ†•\n* ğŸ‰ **Diffusers Support**: HunyuanVideo-1.5 is now available on Hugging Face Diffusers! Check out [Diffusers collection](https://huggingface.co/collections/hunyuanvideo-community/hunyuanvideo-15) for easy integration. ğŸ”¥ğŸ”¥ğŸ”¥ğŸ†•\n* ğŸš€ Nov 27, 2025: We now support cache inference (deepcache, teacache, taylorcache), achieving significant speedup! Pull the latest code to try it. ğŸ”¥ğŸ”¥ğŸ”¥ğŸ†• \n* ğŸš€ Nov 24, 2025: We now support deepcache inference.\n* ğŸ‘‹ Nov 20, 2025: We release the inference code and model weights of HunyuanVideo-1.5.\n\n\n## ğŸ¥ Demo\n<div align=\"center\">\n  <video src=\"https://github.com/user-attachments/assets/d45ec78e-ea40-47f1-8d4d-f4d9a0682e2d\" width=\"60%\"> </video>\n</div>\n\n## ğŸ§© Community Contributions\n\nIf you develop/use HunyuanVideo-1.5 in your projects, welcome to let us know.\n\n- **Diffusers** - [HunyuanVideo-1.5 Diffusers](https://huggingface.co/collections/hunyuanvideo-community/hunyuanvideo-15): Official Hugging Face Diffusers integration for HunyuanVideo-1.5. Easily use HunyuanVideo-1.5 with the Diffusers",
      "default_branch": "main"
    },
    "fetched_at": "2025-12-07T02:53:20.232117"
  },
  {
    "basic_info": {
      "name": "DeepSeek-Math-V2",
      "full_name": "deepseek-ai/DeepSeek-Math-V2",
      "owner": "deepseek-ai",
      "description": null,
      "url": "https://github.com/deepseek-ai/DeepSeek-Math-V2",
      "clone_url": "https://github.com/deepseek-ai/DeepSeek-Math-V2.git",
      "ssh_url": "git@github.com:deepseek-ai/DeepSeek-Math-V2.git",
      "homepage": null,
      "created_at": "2025-11-27T06:12:26Z",
      "updated_at": "2025-12-06T20:43:03Z",
      "pushed_at": "2025-12-01T07:01:42Z"
    },
    "stats": {
      "stars": 1419,
      "forks": 101,
      "watchers": 1419,
      "open_issues": 7,
      "size": 1028
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 44633,
        "Shell": 620
      },
      "license": "Apache License 2.0",
      "topics": []
    },
    "content": {
      "readme": "<!-- markdownlint-disable first-line-h1 -->\n<!-- markdownlint-disable html -->\n<!-- markdownlint-disable no-duplicate-header -->\n\n<div align=\"center\">\n  <img src=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/logo.svg?raw=true\" width=\"60%\" alt=\"DeepSeek-V3\" />\n</div>\n<hr>\n<div align=\"center\" style=\"line-height: 1;\">\n  <a href=\"https://www.deepseek.com/\"><img alt=\"Homepage\"\n    src=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/badge.svg?raw=true\"/></a>\n  <a href=\"https://chat.deepseek.com/\"><img alt=\"Chat\"\n    src=\"https://img.shields.io/badge/ğŸ¤–%20Chat-DeepSeek%20V3-536af5?color=536af5&logoColor=white\"/></a>\n  <a href=\"https://huggingface.co/deepseek-ai\"><img alt=\"Hugging Face\"\n    src=\"https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-DeepSeek%20AI-ffc107?color=ffc107&logoColor=white\"/></a>\n  <br>\n  <a href=\"https://discord.gg/Tc7c45Zzu5\"><img alt=\"Discord\"\n    src=\"https://img.shields.io/badge/Discord-DeepSeek%20AI-7289da?logo=discord&logoColor=white&color=7289da\"/></a>\n  <a href=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/qr.jpeg?raw=true\"><img alt=\"Wechat\"\n    src=\"https://img.shields.io/badge/WeChat-DeepSeek%20AI-brightgreen?logo=wechat&logoColor=white\"/></a>\n  <a href=\"https://twitter.com/deepseek_ai\"><img alt=\"Twitter Follow\"\n    src=\"https://img.shields.io/badge/Twitter-deepseek_ai-white?logo=x&logoColor=white\"/></a>\n  <br>\n  <a href=\"LICENSE\" style=\"margin: 2px;\">\n    <img alt=\"License\" src=\"https://img.shields.io/badge/License-Apache 2.0-f5de53?&color=f5de53\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <br>\n</div>\n\n# DeepSeekMath-V2: Towards Self-Verifiable Mathematical Reasoning\n\n## 1. Introduction\n\nLarge language models have made significant progress in mathematical reasoning, which serves as an important testbed for AI and could impact scientific research if further advanced.\nBy scaling reasoning with reinforcement learning that rewards correct final answers, LLMs have improved from poor performance to saturating quantitative reasoning competitions like AIME and HMMT in one year.\nHowever, this approach faces fundamental limitations.\nPursuing higher final answer accuracy doesn't address a key issue: correct answers don't guarantee correct reasoning.\nMoreover, many mathematical tasks like theorem proving require rigorous step-by-step derivation rather than numerical answers, making final answer rewards inapplicable.\nTo push the limits of deep reasoning, we believe it is necessary to verify the comprehensiveness and rigor of mathematical reasoning.\nSelf-verification is particularly important for scaling test-time compute, especially for open problems without known solutions.\nTowards self-verifiable mathematical reasoning, we investigate how to train an accurate and faithful LLM-based verifier for theorem proving.\nWe then train a proof generator using the verifier as the reward model, and incentivize the generator to identify and resolve as many issues as possible in their own proofs before finalizing them.\nTo maintain the generation-verification gap as the generator becomes stronger, we propose to scale verification compute to automatically label new hard-to-verify proofs, creating training data to further improve the verifier.\nOur resulting model, DeepSeekMath-V2, demonstrates strong theorem-proving capabilities, achieving gold-level scores on IMO 2025 and CMO 2024 and a near-perfect 118/120 on Putnam 2024 with scaled test-time compute.\nWhile much work remains, these results suggest that self-verifiable mathematical reasoning is a feasible research direction that may help develop more capable mathematical AI systems.\n\n## 2. Evaluation Results\n\nBelow are evaluation results on [IMO-ProofBench](https://github.com/google-deepmind/superhuman/tree/main/imobench) (developed by the DeepMind team behind DeepThink IMO-Gold) and recent mathematics competitions including IMO 2025, CMO 2024, and Putnam 2024.\nModel predictions are available in the `outputs` folder.\n\n**IMO-ProofBench**\n\n<p align=\"center\">\n  <img width=\"100%\" src=\"figures/IMO-ProofBench.png\">\n</p>\n\n\n---\n\n**Mathematics Competitions**\n\n<p align=\"center\">\n  <img width=41%\" src=\"figures/Competitions.png\">\n</p>\n\n## 4. Download & Quick Start\n\nDeepSeekMath-V2 is built on top of DeepSeek-V3.2-Exp-Base, which can be downloaded from [ğŸ¤— HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-Math-V2).\nFor inference support, please refer to [the DeepSeek-V3.2-Exp github repository](https://github.com/deepseek-ai/DeepSeek-V3.2-Exp).\n\n## 6. License\nThe use of DeepSeekMath-V2 models is subject to [the Model License](LICENSE).\n\n## 7. Citation\n\n```\n@misc{deepseek-math-v2,\n  author = {Zhihong Shao, Yuxiang Luo, Chengda Lu, Z.Z. Ren, Jiewen Hu, Tian Ye, Zhibin Gou, Shirong Ma, Xiaokang Zhang},\n  title = {DeepSeekMath-V2: Towards Self-Verifiable Mathematical Reasoning},\n  year = {2025},\n}\n```\n\n## 8. Contact\n\nIf you have any questions, please raise an issue or contact us at [service@deep",
      "default_branch": "main"
    },
    "fetched_at": "2025-12-07T02:53:21.538221"
  },
  {
    "basic_info": {
      "name": "crypto-trading-open",
      "full_name": "cryptocj520/crypto-trading-open",
      "owner": "cryptocj520",
      "description": "crypto-trading-open",
      "url": "https://github.com/cryptocj520/crypto-trading-open",
      "clone_url": "https://github.com/cryptocj520/crypto-trading-open.git",
      "ssh_url": "git@github.com:cryptocj520/crypto-trading-open.git",
      "homepage": null,
      "created_at": "2025-11-11T12:00:02Z",
      "updated_at": "2025-12-06T09:53:11Z",
      "pushed_at": "2025-11-11T12:03:28Z"
    },
    "stats": {
      "stars": 1303,
      "forks": 700,
      "watchers": 1303,
      "open_issues": 13,
      "size": 997
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 2825921,
        "Shell": 37998
      },
      "license": null,
      "topics": []
    },
    "content": {
      "readme": "# å¤šäº¤æ˜“æ‰€ç­–ç•¥è‡ªåŠ¨åŒ–ç³»ç»Ÿ\n\n**Multi-Exchange Strategy Automation System**\n\n## ğŸ¯ é¡¹ç›®ç®€ä»‹\n\nè¿™æ˜¯ä¸€ä¸ªä¼ä¸šçº§çš„å¤šäº¤æ˜“æ‰€åŠ å¯†è´§å¸è‡ªåŠ¨åŒ–äº¤æ˜“ç³»ç»Ÿï¼Œæä¾›é«˜æ€§èƒ½ã€é«˜å¯é æ€§çš„ç½‘æ ¼äº¤æ˜“ã€åˆ·é‡äº¤æ˜“ã€å¥—åˆ©ç›‘æ§å’Œå¸‚åœºç›‘æ§åŠŸèƒ½ã€‚ç³»ç»Ÿé‡‡ç”¨ä¸¥æ ¼çš„åˆ†å±‚æ¶æ„è®¾è®¡ï¼Œæ”¯æŒ Hyperliquidã€Backpackã€Lighterã€Binanceã€OKXã€EdgeX ç­‰å¤šä¸ªäº¤æ˜“æ‰€çš„å®Œæ•´é€‚é…ã€‚\n\n## ğŸ—ï¸ æ ¸å¿ƒç³»ç»Ÿæ¶æ„\n\n### ç³»ç»Ÿç»„ä»¶\n\n```\nå¤šäº¤æ˜“æ‰€ç­–ç•¥è‡ªåŠ¨åŒ–ç³»ç»Ÿ\nâ”œâ”€â”€ ğŸ“Š ç½‘æ ¼äº¤æ˜“ç³»ç»Ÿ (Grid Trading)\nâ”‚   â”œâ”€â”€ æ™®é€šç½‘æ ¼              # å›ºå®šä»·æ ¼åŒºé—´ç½‘æ ¼\nâ”‚   â”œâ”€â”€ é©¬ä¸ç½‘æ ¼              # é©¬ä¸æ ¼å°”é€’å¢ç­–ç•¥\nâ”‚   â”œâ”€â”€ ä»·æ ¼ç§»åŠ¨ç½‘æ ¼          # åŠ¨æ€è·Ÿéšä»·æ ¼\nâ”‚   â”œâ”€â”€ å‰¥å¤´çš®æ¨¡å¼            # å¿«é€Ÿæ­¢æŸç­–ç•¥\nâ”‚   â”œâ”€â”€ æ™ºèƒ½å‰¥å¤´çš®            # å¤šæ¬¡æ·±è·Œæ£€æµ‹\nâ”‚   â”œâ”€â”€ æœ¬é‡‘ä¿æŠ¤æ¨¡å¼          # è‡ªåŠ¨æ­¢æŸä¿æŠ¤\nâ”‚   â”œâ”€â”€ æ­¢ç›ˆæ¨¡å¼              # åˆ°è¾¾ç›®æ ‡è‡ªåŠ¨å¹³ä»“\nâ”‚   â””â”€â”€ ç°è´§é¢„ç•™ç®¡ç†          # ç°è´§å¸ç§é¢„ç•™\nâ”œâ”€â”€ ğŸ” ç½‘æ ¼æ³¢åŠ¨ç‡æ‰«æå™¨ (Grid Volatility Scanner)\nâ”‚   â”œâ”€â”€ è™šæ‹Ÿç½‘æ ¼æ¨¡æ‹Ÿ          # æ— éœ€å®é™…ä¸‹å•çš„æ¨¡æ‹Ÿç½‘æ ¼\nâ”‚   â”œâ”€â”€ å®æ—¶APRè®¡ç®—           # å‡†ç¡®é¢„æµ‹å¹´åŒ–æ”¶ç›Šç‡\nâ”‚   â”œâ”€â”€ ä»£å¸æ’è¡Œæ¦œ            # æŒ‰æ³¢åŠ¨ç‡å’ŒAPRæ’åº\nâ”‚   â”œâ”€â”€ æ™ºèƒ½è¯„çº§ç³»ç»Ÿ          # S/A/B/C/Dç­‰çº§è¯„ä¼°\nâ”‚   â””â”€â”€ ç»ˆç«¯ UI              # Rich å®æ—¶ç›‘æ§ç•Œé¢\nâ”œâ”€â”€ ğŸ’¹ åˆ·é‡äº¤æ˜“ç³»ç»Ÿ (Volume Maker)\nâ”‚   â”œâ”€â”€ æŒ‚å•æ¨¡å¼              # é™ä»·å•åˆ·é‡ï¼ˆBackpackï¼‰\nâ”‚   â””â”€â”€ å¸‚ä»·æ¨¡å¼              # å¸‚ä»·å•å¿«é€Ÿåˆ·é‡ï¼ˆLighterï¼‰\nâ”œâ”€â”€ ğŸ”„ å¥—åˆ©ç›‘æ§ç³»ç»Ÿ (Arbitrage Monitor)\nâ”‚   â”œâ”€â”€ ä»·æ ¼ç›‘æ§              # å®æ—¶ä»·æ ¼å·®ç›‘æ§\nâ”‚   â”œâ”€â”€ èµ„é‡‘è´¹ç‡ç›‘æ§          # è·¨äº¤æ˜“æ‰€è´¹ç‡å·®å¼‚\nâ”‚   â”œâ”€â”€ å¥—åˆ©æœºä¼šè¯†åˆ«          # ä»·å·®å’Œè´¹ç‡å¥—åˆ©\nâ”‚   â”œâ”€â”€ ç»ˆç«¯ UI              # Rich å®æ—¶ç›‘æ§ç•Œé¢\nâ”‚   â””â”€â”€ äº¤æ˜“å¯¹è‡ªåŠ¨å‘ç°        # å¤šäº¤æ˜“æ‰€äº¤æ˜“å¯¹åŒ¹é…\nâ”œâ”€â”€ ğŸ”” ä»·æ ¼æé†’ç³»ç»Ÿ (Price Alert)\nâ”‚   â”œâ”€â”€ ä»·æ ¼çªç ´ç›‘æ§          # ä»·æ ¼è§¦åŠç›®æ ‡æé†’\nâ”‚   â”œâ”€â”€ å¤šäº¤æ˜“æ‰€æ”¯æŒ          # æ”¯æŒæ‰€æœ‰æ¥å…¥çš„äº¤æ˜“æ‰€\nâ”‚   â”œâ”€â”€ ç»ˆç«¯ UI              # å®æ—¶ä»·æ ¼æ˜¾ç¤º\nâ”‚   â””â”€â”€ å£°éŸ³æé†’              # çªç ´æ—¶å£°éŸ³é€šçŸ¥\nâ”œâ”€â”€ ğŸ”— äº¤æ˜“æ‰€é€‚é…å±‚ (Exchange Adapters)\nâ”‚   â”œâ”€â”€ Hyperliquid é€‚é…å™¨    # æ°¸ç»­åˆçº¦ + ç°è´§\nâ”‚   â”œâ”€â”€ Backpack é€‚é…å™¨       # æ°¸ç»­åˆçº¦\nâ”‚   â”œâ”€â”€ Lighter é€‚é…å™¨        # æ°¸ç»­åˆçº¦ï¼ˆä½æ‰‹ç»­è´¹ï¼‰\nâ”‚   â”œâ”€â”€ Binance é€‚é…å™¨        # ç°è´§ + æ°¸ç»­åˆçº¦\nâ”‚   â”œâ”€â”€ OKX é€‚é…å™¨            # ç°è´§ + æ°¸ç»­åˆçº¦\nâ”‚   â”œâ”€â”€ EdgeX é€‚é…å™¨          # æ°¸ç»­åˆçº¦\nâ”‚   â””â”€â”€ ç»Ÿä¸€æ¥å£æ ‡å‡†          # æ ‡å‡†åŒ– API æ¥å£\nâ””â”€â”€ ğŸ›ï¸ åŸºç¡€è®¾æ–½å±‚ (Infrastructure)\n    â”œâ”€â”€ ä¾èµ–æ³¨å…¥å®¹å™¨          # DI å®¹å™¨ç®¡ç†\n    â”œâ”€â”€ äº‹ä»¶ç³»ç»Ÿ              # äº‹ä»¶é©±åŠ¨æ¶æ„\n    â”œâ”€â”€ æ—¥å¿—ç³»ç»Ÿ              # ç»“æ„åŒ–æ—¥å¿—\n    â”œâ”€â”€ é…ç½®ç®¡ç†              # YAML é…ç½®ç³»ç»Ÿ\n    â””â”€â”€ æ•°æ®èšåˆå™¨            # å¤šäº¤æ˜“æ‰€æ•°æ®èšåˆ\n```\n\n## ğŸš€ å¿«é€Ÿå¼€å§‹\n\n### ç³»ç»Ÿè¦æ±‚\n\n- Python 3.8+\n- æ”¯æŒçš„æ“ä½œç³»ç»Ÿï¼šLinuxã€macOSã€Windows\n- å¯é€‰ï¼štmuxï¼ˆç”¨äºå¤šè¿›ç¨‹ç®¡ç†ï¼‰\n\n### å®‰è£…ä¾èµ–\n\n```bash\n# å®‰è£… Python ä¾èµ–\npip install -r requirements.txt\n```\n\n### é…ç½® API å¯†é’¥\n\nåœ¨ `config/exchanges/` ç›®å½•ä¸‹é…ç½®å¯¹åº”äº¤æ˜“æ‰€çš„ API å¯†é’¥ï¼š\n\n```bash\nconfig/exchanges/\nâ”œâ”€â”€ hyperliquid_config.yaml   # Hyperliquid é…ç½®\nâ”œâ”€â”€ backpack_config.yaml       # Backpack é…ç½®\nâ”œâ”€â”€ lighter_config.yaml        # Lighter é…ç½®\nâ”œâ”€â”€ binance_config.yaml        # Binance é…ç½®\nâ”œâ”€â”€ okx_config.yaml            # OKX é…ç½®\nâ””â”€â”€ edgex_config.yaml          # EdgeX é…ç½®\n```\n\n### å¿«é€Ÿå¯åŠ¨å„ç³»ç»Ÿ\n\n#### ç½‘æ ¼äº¤æ˜“ç³»ç»Ÿ\n```bash\npython3 run_grid_trading.py config/grid/lighter-long-perp-btc.yaml\n```\n\n#### åˆ·é‡äº¤æ˜“ç³»ç»Ÿï¼ˆBackpackæŒ‚å•æ¨¡å¼ï¼‰\n```bash\npython3 run_volume_maker.py config/volume_maker/backpack_btc_volume_maker.yaml\n```\n\n#### åˆ·é‡äº¤æ˜“ç³»ç»Ÿï¼ˆLighterå¸‚ä»·æ¨¡å¼ï¼‰\n```bash\npython3 run_lighter_volume_maker.py config/volume_maker/lighter_volume_maker.yaml\n```\n\n#### å¥—åˆ©ç›‘æ§ç³»ç»Ÿ\n```bash\npython3 run_arbitrage_monitor.py\n```\n\n#### ä»·æ ¼æé†’ç³»ç»Ÿ\n```bash\npython3 run_price_alert.py config/price_alert/binance_alert.yaml\n```\n\n#### ç½‘æ ¼æ³¢åŠ¨ç‡æ‰«æå™¨\n```bash\npython3 grid_volatility_scanner/run_scanner.py\n```\n\n## ğŸ“‹ æ ¸å¿ƒåŠŸèƒ½è¯¦è§£\n\n### 1ï¸âƒ£ ç½‘æ ¼äº¤æ˜“ç³»ç»Ÿ\n\n#### åŠŸèƒ½ç‰¹æ€§\n\n- **å¤šç§ç½‘æ ¼æ¨¡å¼**ï¼šæ™®é€šç½‘æ ¼ã€é©¬ä¸ç½‘æ ¼ã€ä»·æ ¼ç§»åŠ¨ç½‘æ ¼\n- **æ™ºèƒ½ç­–ç•¥**ï¼šå‰¥å¤´çš®ã€æ™ºèƒ½å‰¥å¤´çš®ã€æœ¬é‡‘ä¿æŠ¤ã€æ­¢ç›ˆæ¨¡å¼\n- **å¥åº·æ£€æŸ¥**ï¼šè‡ªåŠ¨è®¢å•æ ¡éªŒå’Œä¿®å¤æœºåˆ¶\n- **ç»ˆç«¯ UI**ï¼šå®æ—¶ç›‘æ§ç•Œé¢ï¼Œæ˜¾ç¤ºæŒä»“ã€ç›ˆäºã€ç½‘æ ¼çŠ¶æ€\n- **ç°è´§æ”¯æŒ**ï¼šç°è´§é¢„ç•™ç®¡ç†ï¼ˆè‡ªåŠ¨ç»´æŒå¸ç§ä½™é¢ï¼‰\n- **å¤šäº¤æ˜“æ‰€**ï¼šæ”¯æŒ Hyperliquidã€Backpackã€Lighter\n\n#### é…ç½®æ–‡ä»¶ä½ç½®\n\n```\nconfig/grid/\nâ”œâ”€â”€ lighter_btc_perp_long.yaml              # Lighter BTC åšå¤š\nâ”œâ”€â”€ lighter_btc_perp_short.yaml             # Lighter BTC åšç©º\nâ”œâ”€â”€ hyperliquid_btc_perp_long.yaml          # Hyperliquid BTC åšå¤š\nâ”œâ”€â”€ hyperliquid_btc_perp_short.yaml         # Hyperliquid BTC åšç©º\nâ”œâ”€â”€ hyperliquid_btc_spot_long.yaml          # Hyperliquid ç°è´§åšå¤š\nâ”œâ”€â”€ backpack_capital_protection_long_btc.yaml   # Backpack BTC æœ¬é‡‘ä¿æŠ¤\nâ”œâ”€â”€ backpack_capital_protection_long_eth.yaml   # Backpack ETH æœ¬é‡‘ä¿æŠ¤\nâ”œâ”€â”€ backpack_capital_protection_long_sol.yaml   # Backpack SOL æœ¬é‡‘ä¿æŠ¤\nâ”œâ”€â”€ backpack_capital_protection_long_bnb.yaml   # Backpack BNB æœ¬é‡‘ä¿æŠ¤\nâ””â”€â”€ backpack_capital_protection_long_hype.yaml  # Backpack HYPE æœ¬é‡‘ä¿æŠ¤\n```\n\n#### å¯åŠ¨æ–¹å¼\n\n```bash\n# æ–¹å¼1ï¼šç›´æ¥å¯åŠ¨ï¼ˆæ¨èï¼‰\npython3 run_grid_trading.py config/grid/lighter_btc_perp_long.yaml\npython3 run_grid_trading.py config/grid/lighter_eth_perp_long.yaml\n\n# æ–¹å¼2ï¼šDEBUG æ¨¡å¼å¯åŠ¨ï¼ˆæŸ¥çœ‹è¯¦ç»†æ—¥å¿—ï¼‰\npython3 run_grid_trading.py config/grid/lighter_btc_perp_long.yaml --debug\n\n# æ–¹å¼3ï¼šä½¿ç”¨ Shell è„šæœ¬æ‰¹é‡å¯åŠ¨ï¼ˆtmuxï¼‰\n./scripts/start_all_grids.sh\n```\n\n#### æ ¸å¿ƒæ–‡ä»¶\n\n| æ–‡ä»¶è·¯å¾„ | è¯´æ˜ |\n|---------|------|\n| `run_grid_trading.py` | ç½‘æ ¼äº¤æ˜“ç³»ç»Ÿä¸»å¯åŠ¨è„šæœ¬ |\n| `core/services/grid/coordinator/grid_coordinator.py` | ç½‘æ ¼ç³»ç»Ÿåè°ƒå™¨ï¼ˆæ ¸å¿ƒé€»è¾‘ï¼‰ |\n| `core/services/grid/implementations/grid_engine_impl.py` | ç½‘æ ¼æ‰§è¡Œå¼•æ“ |\n| `core/services/grid/implementations/grid_strategy_impl.py` | ç½‘æ ¼ç­–ç•¥å®ç° |\n| `core/services/grid/implementations/position_tracker_impl.py` | æŒä»“è·Ÿè¸ªå™¨ |\n| `core/services/grid/implementations/order_health_checker.py` | è®¢å•å¥åº·æ£€æŸ¥å™¨ |\n| `core/services/grid/scalping/scalping_manager.py` | å‰¥å¤´çš®ç®¡ç†å™¨ |\n| `core/services/grid/scalping/smart_scalping_tracker.py` | æ™ºèƒ½å‰¥å¤´çš®è¿½è¸ªå™¨ |\n| `core/services/grid/capital_protection/capital_protection_manager.py` | æœ¬é‡‘ä¿æŠ¤ç®¡ç†å™¨ |\n| `core/services/grid/terminal_ui.py` | ç»ˆç«¯ UI ç•Œé¢ |\n\n### 2ï¸âƒ£ åˆ·é‡äº¤æ˜“ç³»ç»Ÿ\n\n#### åŠŸèƒ½ç‰¹æ€§\n\n- **åŒäº¤æ˜“æ¨¡å¼**ï¼šæŒ‚å•æ¨¡å¼ï¼ˆBackpackï¼‰ã€å¸‚ä»·æ¨¡å¼ï¼ˆLighterï¼‰\n- **ä¿¡å·æºæ”¯æŒ**ï¼šBackpack REST APIã€Hyperliquid WebSocket\n- **æ™ºèƒ½åˆ¤æ–­**ï¼šä¹°å–å•æ•°é‡å¯¹æ¯”ã€ä»·æ ¼å˜åŠ¨ç›‘æ§\n- **å®æ—¶ç»Ÿè®¡**ï¼šæˆäº¤é‡ã€",
      "default_branch": "main"
    },
    "fetched_at": "2025-12-07T02:53:22.800965"
  },
  {
    "basic_info": {
      "name": "code-mode",
      "full_name": "universal-tool-calling-protocol/code-mode",
      "owner": "universal-tool-calling-protocol",
      "description": "ğŸ”Œ Plug-and-play library to enable agents to call MCP and UTCP tools via code execution. ",
      "url": "https://github.com/universal-tool-calling-protocol/code-mode",
      "clone_url": "https://github.com/universal-tool-calling-protocol/code-mode.git",
      "ssh_url": "git@github.com:universal-tool-calling-protocol/code-mode.git",
      "homepage": "",
      "created_at": "2025-11-11T09:35:44Z",
      "updated_at": "2025-12-07T00:24:15Z",
      "pushed_at": "2025-12-01T16:23:13Z"
    },
    "stats": {
      "stars": 1154,
      "forks": 78,
      "watchers": 1154,
      "open_issues": 11,
      "size": 254
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 49996,
        "TypeScript": 41432,
        "JavaScript": 12971
      },
      "license": "Mozilla Public License 2.0",
      "topics": [
        "ai-agents",
        "codemode",
        "mcp",
        "model-context-protocol",
        "toolchain",
        "utcp"
      ]
    },
    "content": {
      "readme": "<div align=\"center\">\n<!-- <img alt=\"utcp code mode banner\" src=\"https://github.com/user-attachments/assets/77723130-ecbc-4d1d-9e9b-20f978882699\" width=\"80%\" style=\"margin: 20px auto;\"> -->\n\n<h1 align=\"center\">ğŸ¤– Code-Mode Library: First library for tool calls via code execution</h1>\n<p align=\"center\">\n    <a href=\"https://github.com/universal-tool-calling-protocol\">\n        <img src=\"https://img.shields.io/github/followers/universal-tool-calling-protocol?label=Follow%20Org&logo=github\" /></a>\n    <a href=\"https://img.shields.io/npm/dt/@utcp/code-mode\" title=\"PyPI Version\">\n        <img src=\"https://img.shields.io/npm/dt/@utcp/code-mode\"/></a>\n    <a href=\"https://github.com/universal-tool-calling-protocol/code-mode/blob/main/LICENSE\" alt=\"License\">\n        <img src=\"https://img.shields.io/github/license/universal-tool-calling-protocol/code-mode\" /></a>\n \n  [![npm](https://img.shields.io/npm/v/@utcp/code-mode)](https://www.npmjs.com/package/@utcp/code-mode)\n</p>\n</div>\n\n> Transform your AI agents from clunky tool callers into efficient code executors â€” in just 3 lines.\n\n## Why This Changes Everything\n\nLLMs excel at writing code but struggle with tool calls. Instead of exposing hundreds of tools directly, give them ONE tool that executes TypeScript code with access to your entire toolkit.\n\n[Apple](https://machinelearning.apple.com/research/codeact), [Cloudflare](https://blog.cloudflare.com/code-mode/), and [Anthropic](https://www.anthropic.com/engineering/code-execution-with-mcp) say that Code-Mode is a more efficient way to approach tool calling compared to the traditional dump function information and then extract a JSON for function calling.\n\n## Benchmarks\n\nIndependent [Python benchmark study](https://github.com/imran31415/codemode_python_benchmark) validates the performance claims with **$9,536/year cost savings** at 1,000 scenarios/day:\n\n| Scenario Complexity | Traditional | Code Mode | **Improvement** |\n|---------------------|-------------|-----------|----------------|\n| **Simple (2-3 tools)** | 3 iterations | 1 execution | **67% faster** |\n| **Medium (4-7 tools)** | 8 iterations | 1 execution | **75% faster** |\n| **Complex (8+ tools)** | 16 iterations | 1 execution | **88% faster** |\n\n### **Why Code Mode Dominates:**\n\n   **Batching Advantage** - Single code block replaces multiple API calls  \n   **Cognitive Efficiency** - LLMs excel at code generation vs. tool orchestration  \n   **Computational Efficiency** - No context re-processing between operations\n\n# Getting Started\n\n[<img width=\"2606\" height=\"1445\" alt=\"Frame 4 (4)\" src=\"https://github.com/user-attachments/assets/58ba26ab-6e77-459b-a59a-eeb60d711746\" />\n](https://www.youtube.com/watch?v=zsMjkPzmqhA)\n\n## Get Started in 3 Lines\n\n```typescript\nimport { CodeModeUtcpClient } from '@utcp/code-mode';\n\nconst client = await CodeModeUtcpClient.create();                    // 1. Initialize\nawait client.registerManual({ name: 'github', /* MCP config */ });  // 2. Add tools  \nconst { result } = await client.callToolChain(`/* TypeScript */`);   // 3. Execute code\n```\n\nThat's it. Your AI agent can now execute complex workflows in a single request instead of dozens.\n\n## What You Get\n\n### **Progressive Tool Discovery**\n```typescript\n// Agent discovers tools dynamically, loads only what it needs\nconst tools = await client.searchTools('github pull request');\n// Instead of 500 tool definitions â†’ 3 relevant tools\n```\n\n### **Natural Code Execution**  \n```typescript\nconst { result, logs } = await client.callToolChain(`\n  // Chain multiple operations in one request\n  const pr = await github.get_pull_request({ owner: 'microsoft', repo: 'vscode', pull_number: 1234 });\n  const comments = await github.get_pull_request_comments({ owner: 'microsoft', repo: 'vscode', pull_number: 1234 });\n  const reviews = await github.get_pull_request_reviews({ owner: 'microsoft', repo: 'vscode', pull_number: 1234 });\n  \n  // Process data efficiently in-sandbox\n  return {\n    title: pr.title,\n    commentCount: comments.length,\n    approvals: reviews.filter(r => r.state === 'APPROVED').length\n  };\n`);\n// Single API call replaces 15+ traditional tool calls\n```\n\n### **Auto-Generated TypeScript Interfaces**\n```typescript\nnamespace github {\n  interface get_pull_requestInput {\n    /** Repository owner */\n    owner: string;\n    /** Repository name */ \n    repo: string;\n    /** Pull request number */\n    pull_number: number;\n  }\n}\n```\n\n## Enterprise-Ready\n\n- **Secure VM Sandboxing** â€“ Node.js isolates prevent unauthorized access\n- **Timeout Protection** â€“ Configurable execution limits prevent runaway code  \n- **Complete Observability** â€“ Full console output capture and error handling\n- **Zero External Dependencies** â€“ Tools only accessible through registered UTCP/MCP servers\n- **Runtime Introspection** â€“ Dynamic interface discovery for adaptive workflows\n\nIf you're working at an enterprise, and need support, book a consultation [here](https://bevel.neetocal.com/meeting-with-ali).\n## Universal Protocol",
      "default_branch": "main"
    },
    "fetched_at": "2025-12-07T02:53:24.086975"
  },
  {
    "basic_info": {
      "name": "HunyuanOCR",
      "full_name": "Tencent-Hunyuan/HunyuanOCR",
      "owner": "Tencent-Hunyuan",
      "description": null,
      "url": "https://github.com/Tencent-Hunyuan/HunyuanOCR",
      "clone_url": "https://github.com/Tencent-Hunyuan/HunyuanOCR.git",
      "ssh_url": "git@github.com:Tencent-Hunyuan/HunyuanOCR.git",
      "homepage": null,
      "created_at": "2025-11-18T04:06:24Z",
      "updated_at": "2025-12-07T01:56:00Z",
      "pushed_at": "2025-12-04T02:46:14Z"
    },
    "stats": {
      "stars": 1146,
      "forks": 89,
      "watchers": 1146,
      "open_issues": 37,
      "size": 73571
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 22360
      },
      "license": "Other",
      "topics": []
    },
    "content": {
      "readme": "<div align=\"center\">\n\n[ä¸­æ–‡é˜…è¯»](./README_zh.md)\n\n</div>\n\n<div align=\"center\">\n\n# HunyuanOCR\n\n</div>\n\n<p align=\"center\">\n <img src=\"./assets/hyocr-head-img.png\" width=\"80%\"/> <br>\n</p>\n\n\n<p align=\"center\">\n<a href=\"https://huggingface.co/spaces/tencent/HunyuanOCR\"><b>ğŸ¯ Demo</b></a> |\n<a href=\"https://huggingface.co/tencent/HunyuanOCR\"><b>ğŸ“¥ Model Download</b></a> |\n<a href=\"https://arxiv.org/abs/2511.19575\"><b>ğŸ“„ Technical Report</b></a>\n</p>\n\n## ğŸ¤ Join Our Community\n\n<div align=\"center\">\n\n| Wechat Discussion Group | Discord Group |\n| :---: | :---: |\n| <img src=\"./assets/qrcode_for_hunyuanocr_wechat.jpg\" width=\"150\"> | [Join HunyuanOCR Discord](https://discord.gg/XeD3p2MRDk) |\n\n</div>\n\n## ğŸ”¥ News\n- **[2025/11/28]** ğŸ› ï¸ We fixed vLLM inference bugs and hyperparameter configuration issues such as system prompt. It is recommended to use the latest vLLM installation steps and the [inference script](https://github.com/Tencent-Hunyuan/HunyuanOCR/blob/main/Hunyuan-OCR-master/Hunyuan-OCR-vllm/run_hy_ocr.py) for performance testing. Currently, there is still a certain accuracy difference between Transformers and the vLLM framework (we are working on fixing this).\n- **[2025/11/25]** ğŸ“ Inference code and model weights publicly available.\n\n\n## ğŸ“– Introduction\n**HunyuanOCR** stands as a leading end-to-end OCR expert VLM powered by Hunyuan's native multimodal architecture. With a remarkably lightweight 1B parameter design, it has achieved multiple state-of-the-art benchmarks across the industry. The model demonstrates mastery in **complex multilingual document parsing** while excelling in practical applications including **text spotting, open-field information extraction, video subtitle extraction, and photo translation**.\n\n\n## âœ¨ Key Features\n\n- ğŸ’ª **Efficient Lightweight Architecture**: Built on Hunyuan's native multimodal architecture and training strategy, achieving SOTA performance with only 1B parameters, significantly reducing deployment costs.\n\n- ğŸ“‘ **Comprehensive OCR Capabilities**: A single model covering classic OCR tasks including text detection and recognition, complex document parsing, open-field information extraction and video subtitle extraction, while supporting end-to-end photo translation and document QA.\n\n- ğŸš€ **Ultimate Usability**: Deeply embraces the \"end-to-end\" philosophy of large models - achieving SOTA results with single instruction and single inference, offering greater efficiency and convenience compared to industry cascade solutions.\n\n- ğŸŒ **Extensive Language Support**: Robust support for over 100 languages, excelling in both single-language and mixed-language scenarios across various document types.\n\n<div align=\"left\">\n  <img src=\"./assets/hyocr-pipeline-v1.png\" alt=\"HunyuanOCR framework\" width=\"80%\">\n</div>\n\n\n\n\n## ğŸ› ï¸ Dependencies and Installation\n\n### System Requirements\n- ğŸ–¥ï¸ Operating System: Linux\n- ğŸ Python: 3.12+ (recommended and tested)\n- âš¡ CUDA: 12.9\n- ğŸ”¥ PyTorch: 2.7.1\n- ğŸ® GPU: NVIDIA GPU with CUDA support\n- ğŸ§  GPU Memory: 20GB (for vLLM)\n- ğŸ’¾ Disk Space: 6GB\n\n## ğŸš€ Quick Start with vLLM (â­ Recommended)\n\n- **[HunyuanOCR Usage Guide](https://docs.vllm.ai/projects/recipes/en/latest/Tencent-Hunyuan/HunyuanOCR.html)**\n\n### Installation\n```bash\npip install vllm>=0.12.0\npip install -r requirements.txt\n```\n\nNote: We suggest to install [cuda-compat-12-9](https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2404/x86_64/):\n```bash\nsudo dpkg -i cuda-compat-12-9_575.57.08-0ubuntu1_amd64.deb\necho 'export LD_LIBRARY_PATH=/usr/local/cuda-12.9/compat:$LD_LIBRARY_PATH' >> ~/.bashrc\nsource ~/.bashrc\n# verify cuda-compat-12-9\nls /usr/local/cuda-12.9/compat\n```\n\n### Model Deploy\n```bash\nvllm serve tencent/HunyuanOCR \\\n    --no-enable-prefix-caching \\\n    --mm-processor-cache-gb 0 \\\n    --gpu-memory-utilization 0.2\n```\n\n### Model Inference\n```python\nfrom vllm import LLM, SamplingParams\nfrom PIL import Image\nfrom transformers import AutoProcessor\n\ndef clean_repeated_substrings(text):\n    \"\"\"Clean repeated substrings in text\"\"\"\n    n = len(text)\n    if n<8000:\n        return text\n    for length in range(2, n // 10 + 1):\n        candidate = text[-length:] \n        count = 0\n        i = n - length\n        \n        while i >= 0 and text[i:i + length] == candidate:\n            count += 1\n            i -= length\n\n        if count >= 10:\n            return text[:n - length * (count - 1)]  \n\n    return text\n\nmodel_path = \"tencent/HunyuanOCR\"\nllm = LLM(model=model_path, trust_remote_code=True)\nprocessor = AutoProcessor.from_pretrained(model_path)\nsampling_params = SamplingParams(temperature=0, max_tokens=16384)\n\nimg_path = \"/path/to/image.jpg\"\nimg = Image.open(img_path)\nmessages = [\n    {\"role\": \"system\", \"content\": \"\"},\n    {\"role\": \"user\", \"content\": [\n        {\"type\": \"image\", \"image\": img_path},\n        {\"type\": \"text\", \"text\": \"æ£€æµ‹å¹¶è¯†åˆ«å›¾ç‰‡ä¸­çš„æ–‡å­—ï¼Œå°†æ–‡æœ¬åæ ‡æ ¼å¼åŒ–è¾“å‡ºã€‚\"}\n    ]}\n]\nprompt = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\ninputs = {\"prompt\": prompt, \"multi_modal_data\": {\"image\": [",
      "default_branch": "main"
    },
    "fetched_at": "2025-12-07T02:53:25.374997"
  },
  {
    "basic_info": {
      "name": "karpathy",
      "full_name": "K-Dense-AI/karpathy",
      "owner": "K-Dense-AI",
      "description": "An agentic Machine Learning Engineer",
      "url": "https://github.com/K-Dense-AI/karpathy",
      "clone_url": "https://github.com/K-Dense-AI/karpathy.git",
      "ssh_url": "git@github.com:K-Dense-AI/karpathy.git",
      "homepage": "https://k-dense.ai",
      "created_at": "2025-11-16T22:39:26Z",
      "updated_at": "2025-12-06T18:44:13Z",
      "pushed_at": "2025-11-24T01:50:20Z"
    },
    "stats": {
      "stars": 1122,
      "forks": 127,
      "watchers": 1122,
      "open_issues": 1,
      "size": 22
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 10658
      },
      "license": "MIT License",
      "topics": [
        "agentic-ai",
        "automl",
        "machine-learning"
      ]
    },
    "content": {
      "readme": "# Karpathy\n\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n[![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg)](https://github.com/K-Dense-AI/karpathy/pulls)\n\nAn agentic Machine Learning Engineer that trains state-of-the-art ML models using Claude Code SDK and Google ADK. This is a very simple implemenation demonstraing the power of Claude Scientific Skills for machine learning.\n\n## Prerequisites\n\n- Python 3.13 or higher\n- [uv](https://github.com/astral-sh/uv) package manager\n- Claude Code installed and authenticated (see [installation guide](https://www.claude.com/product/claude-code))\n\n## Setup\n\n### 1. Clone the Repository\n\n```bash\ngit clone https://github.com/K-Dense-AI/karpathy.git\ncd karpathy\n```\n\n### 2. Install Dependencies\n\nInstall dependencies using `uv`:\n\n```bash\nuv sync\n```\n\n### 3. Environment Variables\n\nCreate a `.env` file in the `karpathy` directory with your API keys:\n\n```bash\nOPENROUTER_API_KEY=your_openrouter_api_key_here\nAGENT_MODEL=your_model_name_here\n```\n\nThe `OPENROUTER_API_KEY` is required for the agent to function properly.\n\nThis is the same environment variable that will be copied to the `sandbox` directory so the agents can use any API keys you provide here.\n\n## Quick Start\n\nRun the startup script to set up the sandbox and start the ADK web interface:\n\n```bash\npython start.py\n```\n\nThis automatically:\n1. Creates a `sandbox` directory with scientific skills from Claude Scientific Skills\n2. Sets up a Python virtual environment with ML packages (PyTorch, transformers, scikit-learn, etc.)\n3. Copies your `.env` file to the sandbox\n4. Starts the ADK web interface\n5. Navigate to **http://localhost:8000** in your browser\n6. Select `karpathy` in the top left under 'Select an agent'\n7. All outputs will be in the `sandbox` directory so continue to monitor that as you converse with the agent\n\n**Note:** Any files you want the agent to use (datasets, scripts, etc.) should be manually added to the `sandbox` directory.\n\n## Community\n\nJoin our K-Dense Slack community to connect with other users, share ideas, and get support:\n\n**[Join K-Dense Slack Community](https://join.slack.com/t/k-densecommunity/shared_invite/zt-3iajtyls1-EwmkwIZk0g_o74311Tkf5g)**\n\n## Claude Scientific Skills\n\nThis repository is designed to work with the **[Claude Scientific Skills](https://github.com/K-Dense-AI/claude-scientific-skills)** collection of ready-to-use scientific tools and workflows ([link](https://github.com/K-Dense-AI/claude-scientific-skills)). The `start.py` setup script creates a `sandbox` that includes scientific skills from this collection so the `karpathy` agent can leverage specialized ML libraries and scientific workflows. For full details on the skills themselves, see the upstream repositoryâ€™s README and documentation [here](https://github.com/K-Dense-AI/claude-scientific-skills).\n\n## Manual Usage\n\nTo set up the sandbox without starting the web interface:\n\n```bash\npython -m karpathy.utils\n```\n\n**Note:** Any files you want the agent to use (datasets, scripts, etc.) should be manually added to the `sandbox` directory.\n\nTo run the ADK web interface manually:\n\n```bash\nadk web\n```\n\nThen navigate to **http://localhost:8000** in your browser.\n\n## Enhanced ML Capabilities\n\nIf you want substantially more powerful ML capabilities through a multi-agentic system, sign up for [www.k-dense.ai](https://www.k-dense.ai). Currently in closed beta, launching publicly in December 2025.\n\n## Upcoming Features\n\n- **Modal sandbox integration** - Choose any type of compute you want\n- **K-Dense Web features** - We might make some features from K-Dense Web available here based on interest\n\n## Star History\n\n[![Star History Chart](https://api.star-history.com/svg?repos=K-Dense-AI/karpathy&type=date&legend=top-left)](https://www.star-history.com/#K-Dense-AI/karpathy&type=date&legend=top-left)\n\n## Disclaimer\n\nThis project is **not** endorsed by or affiliated with Andrej Karpathy. The name is used as a tribute and out of deep respect for his contributions to AI and technical leadership.",
      "default_branch": "main"
    },
    "fetched_at": "2025-12-07T02:53:26.641774"
  },
  {
    "basic_info": {
      "name": "flux2",
      "full_name": "black-forest-labs/flux2",
      "owner": "black-forest-labs",
      "description": "Official inference repo for FLUX.2 models",
      "url": "https://github.com/black-forest-labs/flux2",
      "clone_url": "https://github.com/black-forest-labs/flux2.git",
      "ssh_url": "git@github.com:black-forest-labs/flux2.git",
      "homepage": null,
      "created_at": "2025-11-24T23:28:49Z",
      "updated_at": "2025-12-07T01:47:10Z",
      "pushed_at": "2025-12-01T13:32:55Z"
    },
    "stats": {
      "stars": 1108,
      "forks": 52,
      "watchers": 1108,
      "open_issues": 5,
      "size": 37542
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 84177
      },
      "license": "Apache License 2.0",
      "topics": []
    },
    "content": {
      "readme": "# FLUX.2\nby Black Forest Labs: https://bfl.ai.\n\nDocumentation for our API can be found here: [docs.bfl.ai](https://docs.bfl.ai/).\n\nThis repo contains minimal inference code to run image generation & editing with our FLUX.2 open-weight models.\n\n## `FLUX.2 [dev]`\n\n`FLUX.2 [dev]` is a 32B parameter flow matching transformer model capable of generating and editing (multiple) images. The model is released under the [FLUX.2-dev Non-Commercial License](model_licenses/LICENSE-FLUX-DEV) and can be found [here](https://huggingface.co/black-forest-labs/FLUX.2-dev).\n\nNote that the below script for `FLUX.2 [dev]` needs considerable amount of VRAM (H100-equivalent GPU). We partnered with Hugging Face to make quantized versions that run on consumer hardware; below you can find instructions on how to run it on a RTX 4090 with a remote text encoder, for other quantization sizes and combinations, check the [diffusers quantization guide here](docs/flux2_dev_hf.md).\n\n### Text-to-image examples\n\n![t2i-grid](assets/teaser_generation.png)\n\n### Editing examples\n\n![edit-grid](assets/teaser_editing.png)\n\n### Prompt upsampling\n\n`FLUX.2 [dev]` benefits significantly from prompt upsampling. The inference script below offers the option to use both local prompt upsampling with the same model we use for text encoding ([`Mistral-Small-3.2-24B-Instruct-2506`](https://huggingface.co/mistralai/Mistral-Small-3.2-24B-Instruct-2506)), or alternatively, use any model on [OpenRouter](https://openrouter.ai/) via an API call.\n\nSee the [upsampling guide](docs/flux2_with_prompt_upsampling.md) for additional details and guidance on when to use upsampling.\n\n## `FLUX.2` autoencoder\n\nThe FLUX.2 autoencoder has considerably improved over the [FLUX.1 autoencoder](https://huggingface.co/black-forest-labs/FLUX.1-dev/blob/main/ae.safetensors). The autoencoder is released under [Apache 2.0](https://huggingface.co/datasets/choosealicense/licenses/blob/main/markdown/apache-2.0.md) and can be found [here](https://huggingface.co/black-forest-labs/FLUX.2-dev/blob/main/ae.safetensors). For more information, see our [technical blogpost](https://bfl.ai/research/representation-comparison).\n\n## Local installation\n\nThe inference code was tested on GB200 and H100 (with CPU offloading).\n\n### GB200\n\nOn GB200, we tested `FLUX.2 [dev]` using CUDA 12.9 and Python 3.12.\n\n```bash\npython3.12 -m venv .venv\nsource .venv/bin/activate\npip install -e . --extra-index-url https://download.pytorch.org/whl/cu129 --no-cache-dir\n```\n\n### H100\n\nOn H100, we tested `FLUX.2 [dev]` using CUDA 12.6 and Python 3.10.\n\n```bash\npython3.10 -m venv .venv\nsource .venv/bin/activate\npip install -e . --extra-index-url https://download.pytorch.org/whl/cu126 --no-cache-dir\n```\n\n## Run the CLI\n\nBefore running the CLI, you may download the weights from [here](https://huggingface.co/black-forest-labs/FLUX.2-dev) and set the following environment variables.\n\n```bash\nexport FLUX2_MODEL_PATH=\"<flux2_path>\"\nexport AE_MODEL_PATH=\"<ae_path>\"\n```\n\nIf you don't set the environment variables, the weights will be downloaded\nautomatically.\n\nYou can start an interactive session with loaded weights by running the\nfollowing command. That will allow you to do both text to image generation as\nwell as editing one or multiple images.\n```bash\nexport PYTHONPATH=src\npython scripts/cli.py\n```\n\nOn H100, we additionally set the flag `--cpu_offloading True`.\n\n## Watermarking\n\nWe've added an option to embed invisible watermarks directly into the generated images\nvia the [invisible watermark library](https://github.com/ShieldMnt/invisible-watermark).\n\nAdditionally, we are recommending implementing a solution to mark the metadata of your outputs, such as [C2PA](https://c2pa.org/)\n\n## ğŸ§¨ Lower VRAM diffusers example\n\nThe below example should run on a RTX 4090. For more examples check the [diffusers quantization guide here](docs/flux2_dev_hf.md)\n\n```python\nimport torch\nfrom diffusers import Flux2Pipeline\nfrom diffusers.utils import load_image\nfrom huggingface_hub import get_token\nimport requests\nimport io\n\nrepo_id = \"diffusers/FLUX.2-dev-bnb-4bit\"\ndevice = \"cuda:0\"\ntorch_dtype = torch.bfloat16\n\ndef remote_text_encoder(prompts):\n    response = requests.post(\n        \"https://remote-text-encoder-flux-2.huggingface.co/predict\",\n        json={\"prompt\": prompts},\n        headers={\n            \"Authorization\": f\"Bearer {get_token()}\",\n            \"Content-Type\": \"application/json\"\n        }\n    )\n    prompt_embeds = torch.load(io.BytesIO(response.content))\n\n    return prompt_embeds.to(device)\n\npipe = Flux2Pipeline.from_pretrained(\n    repo_id, text_encoder=None, torch_dtype=torch_dtype\n).to(device)\n\nprompt = \"Realistic macro photograph of a hermit crab using a soda can as its shell, partially emerging from the can, captured with sharp detail and natural colors, on a sunlit beach with soft shadows and a shallow depth of field, with blurred ocean waves in the background. The can has the text `BFL Diffusers` on it and it has a color gradient that star",
      "default_branch": "main"
    },
    "fetched_at": "2025-12-07T02:53:27.934481"
  },
  {
    "basic_info": {
      "name": "gelab-zero",
      "full_name": "stepfun-ai/gelab-zero",
      "owner": "stepfun-ai",
      "description": "GELab: GUI Exploration Lab. One of the best GUI agent solutions in the galaxy, built by the StepFun-GELab team and powered by Stepâ€™s research capabilities.",
      "url": "https://github.com/stepfun-ai/gelab-zero",
      "clone_url": "https://github.com/stepfun-ai/gelab-zero.git",
      "ssh_url": "git@github.com:stepfun-ai/gelab-zero.git",
      "homepage": "https://opengelab.github.io/",
      "created_at": "2025-11-28T14:42:44Z",
      "updated_at": "2025-12-07T02:03:37Z",
      "pushed_at": "2025-12-05T02:50:46Z"
    },
    "stats": {
      "stars": 1044,
      "forks": 88,
      "watchers": 1044,
      "open_issues": 9,
      "size": 121535
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 116699
      },
      "license": "MIT License",
      "topics": [
        "agent",
        "gui-agents",
        "phone-use-agent",
        "pua"
      ]
    },
    "content": {
      "readme": "![GELab-Zero Main Image](./images/main_en.png)\n\n> ğŸ‘‹ Hi, everyone! We are proud to present the first fully open-source GUI Agent with both model and infrastructure. Our solution features plug-and-play engineering with no cloud dependencies, giving you complete privacy control.\n\n<p align=\"center\">\n  <!-- <a href=\"https://github.com/stepfun-ai/gelab-zero\"><img src=\"https://img.shields.io/badge/ğŸ’»%20GitHub-Repository-black\" alt=\"GitHub\" /></a> -->\n  <a href=\"https://opengelab.github.io/\"><img src=\"https://img.shields.io/badge/ğŸŒ%20Website-Project%20Page-blue\" alt=\"Website\" /></a>\n  <a href=\"https://huggingface.co/stepfun-ai/GELab-Zero-4B-preview\"><img src=\"https://img.shields.io/badge/ğŸ¤—%20Hugging%20Face-GELab--Zero--4B--preview-orange\" alt=\"Hugging Face Model\" /></a>\n  <a href=\"https://huggingface.co/datasets/stepfun-ai/AndroidDaily\"><img src=\"https://img.shields.io/badge/ğŸ“š%20Hugging%20Face-AndroidDaily-yellow\" alt=\"Hugging Face Dataset\" /></a>\n  <a href=\"https://modelscope.cn/models/stepfun-ai/GELab-Zero-4B-preview\"><img src=\"https://img.shields.io/badge/ğŸ¤–%20Model%20Scope-GELab--Zero--4B--preview-blue\" alt=\"Model Scope\" /></a>\n</p>\n\n<p align=\"center\">\n  <a href=\"./README.md\">English</a> |\n  <a href=\"./README_CN.md\">ç®€ä½“ä¸­æ–‡</a>\n</p>\n\n## ğŸ“° News\n\n* ğŸ **[Coming Soon...]**\n* ğŸ **[2025-12]** We thank the following projects and authors for providing quantization tools & tutorials: [GGUF_v1](https://huggingface.co/bartowski/stepfun-ai_GELab-Zero-4B-preview-GGUF), [GGUF_v2](https://huggingface.co/noctrex/GELab-Zero-4B-preview-GGUF), [EXL3](https://huggingface.co/ArtusDev/stepfun-ai_GELab-Zero-4B-preview-EXL3), [Tutorials_CN](http://xhslink.com/o/1WrmgHGWFYh), [Tutorials_EN](https://www.youtube.com/watch?v=4BMiDyQOpos)\n* ğŸ **[2025-11]** We release a lightweight **4B model** on [**Hugging Face**](https://huggingface.co/stepfun-ai/GELab-Zero-4B-preview) and [**Model Scope**](https://modelscope.cn/models/stepfun-ai/GELab-Zero-4B-preview).\n* ğŸ **[2025-11]** We release the tasks from the [**AndroidDaily**](https://huggingface.co/datasets/stepfun-ai/AndroidDaily) benchmark.\n* ğŸ **[2025-11]** We release the current **GELab-Zero** engineering infrastructure.\n* ğŸ **[2025-10]** Our [research](https://github.com/summoneryhl/gelab-engine) paper on **GELab-Engine** is accepted by **NeurIPS 2025**.\n\n\n## ğŸ“‘ Table of Contents\n\n- [ğŸ“– Background](#-background)\n- [ğŸ¥ Application Demonstrations](#-application-demonstrations)\n- [ğŸ“Š AndroidDaily](#-androiddaily-a-self-built-benchmark-close-to-daily-life)\n- [ğŸ† Open Benchmark](#-open-benchmark)\n- [ğŸš€ Installation & Quick Start](#-installation-quick-start)\n- [ğŸ“ Citation](#-citation)\n- [ğŸ“§ Contact](#-contact)\n\n## ğŸ“– Background\n\nAs AI experiences continue to penetrate consumer-grade terminal devices, mobile Agent research is at a critical juncture transitioning from \"feasibility verification\" to \"large-scale application.\" GUI-based solutions have emerged as the optimal approach for the current stage in addressing complex mobile ecosystems and achieving scalable Agent capabilities, thanks to their universal compatibility with all apps and zero-cost integration without requiring app vendor adaptation. However, due to the highly fragmented nature of mobile application ecosystems, getting GUI Agents to truly work across different brands and device models often faces numerous engineering challenges: multi-device ADB connections, dependency installation, permission configuration, inference service deployment, task recording and replay. This means Agent developers and MCP users need to handle substantial engineering infrastructure work, making it difficult to focus on strategic innovation.\n\nTo address this challenge, we are open-sourcing GELab-Zero to accelerate the innovation and application deployment of GUI Agents. It consists of two main components:\n\n- Plug-and-play complete inference engineering infrastructure that handles all the heavy lifting\n- A 4B GUI Agent model capable of running on local computer\n\nIt provides a one-click launch experience similar to open-source GUI Agent MCP, can be deployed entirely locally, and puts the entire inference pipeline under your complete control. Specific capabilities include:\n\n- **Local Deployment**: Supports 4B-scale models running on consumer-grade hardware, balancing low latency with privacy.\n- **One-click Launch**: Provides unified deployment pipeline that automatically handles environment dependencies and device management.\n- **Task Distribution**: Can distribute tasks to multiple phones while recording interaction trajectories for observability and reproducibility.\n- **Three Agent Modes**: Covers multiple working modes including ReAct loops, multi-agent collaboration, and scheduled tasks.\n\nThese capabilities enable GELab-Zero to flexibly handle complex task flows in real-world scenarios and provide a solid foundation for future extensions.\n\nFor Agent developers, this infrastructure enables rapid testing of new ideas and strategies, validating interaction approaches; ",
      "default_branch": "main"
    },
    "fetched_at": "2025-12-07T02:53:29.215923"
  },
  {
    "basic_info": {
      "name": "Video-Materials-AutoGEN-Workstation",
      "full_name": "Norsico/Video-Materials-AutoGEN-Workstation",
      "owner": "Norsico",
      "description": "ä¸€ä¸ªé›†å†…å®¹ç­–åˆ’ã€AIæ–‡æ¡ˆè‡ªåŠ¨ç”Ÿæˆã€TTS æ‰¹é‡è‡ªåŠ¨é…éŸ³ã€(AI)å›¾ç‰‡ç´ æåˆæˆã€ASRè‡ªåŠ¨æå–è¯­è¨€å­—å¹•è„šæœ¬ã€AIè‡ªç”±åˆ›ä½œäºä¸€ä½“çš„(çŸ­è§†é¢‘)ç”Ÿæˆå·¥ä½œç«™ã€‚æ–¹ä¾¿ç®¡ç†æ¯æœŸçš„è§†é¢‘é¡¹ç›®ã€‚",
      "url": "https://github.com/Norsico/Video-Materials-AutoGEN-Workstation",
      "clone_url": "https://github.com/Norsico/Video-Materials-AutoGEN-Workstation.git",
      "ssh_url": "git@github.com:Norsico/Video-Materials-AutoGEN-Workstation.git",
      "homepage": "",
      "created_at": "2025-11-18T13:04:59Z",
      "updated_at": "2025-12-07T02:35:15Z",
      "pushed_at": "2025-11-30T05:04:02Z"
    },
    "stats": {
      "stars": 1024,
      "forks": 206,
      "watchers": 1024,
      "open_issues": 1,
      "size": 60047
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 19833252,
        "JavaScript": 146698,
        "HTML": 45831,
        "CSS": 28271,
        "Dockerfile": 255,
        "Batchfile": 215
      },
      "license": null,
      "topics": []
    },
    "content": {
      "readme": "# Video Material GEN Workstation\n\nä¸€ä¸ªé›†å†…å®¹ç­–åˆ’ã€AIæ–‡æ¡ˆè‡ªåŠ¨ç”Ÿæˆã€TTS æ‰¹é‡è‡ªåŠ¨é…éŸ³ã€(AI)å›¾ç‰‡ç´ æåˆæˆã€ASRè‡ªåŠ¨æå–è¯­è¨€å­—å¹•è„šæœ¬ã€AIè‡ªç”±åˆ›ä½œäºä¸€ä½“çš„(çŸ­è§†é¢‘)ç”Ÿæˆå·¥ä½œç«™ã€‚æ–¹ä¾¿ç®¡ç†æ¯æœŸçš„è§†é¢‘é¡¹ç›®ã€‚\n\n# â—æ­£åœ¨è€ƒè™‘ä½¿ç”¨æœ€æ–°LangGraphæ¶æ„é‡æ„è¯¥é¡¹ç›®ï¼Œæ•¬è¯·æœŸå¾…â—\n\n## åŠŸèƒ½é€Ÿè§ˆ\n\n- æ”¯æŒæŒ‰æ¨¡æ¿æ‰¹é‡ç”Ÿæˆè§†é¢‘é¡¹ç›®ï¼Œè„šæœ¬ã€å›¾ç‰‡ç´ æ(AI)ã€å­—å¹•å’ŒéŸ³é¢‘ä¸€é”®é½å¤‡ã€‚\n- Gemini + TTSåˆæˆï¼Œæ—¢èƒ½æ”¹å†™è„šæœ¬åˆèƒ½ç›´æ¥è¾“å‡º(å¸¦æƒ…ç»ªçš„)é…éŸ³ã€‚\n- å›¾æ–‡åˆ†è½¨ç®¡ç†ï¼Œå¯åœ¨å‰ç«¯éšæ—¶æ›¿æ¢å›¾ç‰‡ã€å­—å¹•æˆ–éŸ³é¢‘å¹¶é¢„è§ˆç»“æœã€‚\n\n## æ•°æ®å±•ç¤º\n\n![æŠ–éŸ³æŠ•æ”¾æ•°æ®](img/æ•°æ®.png)\n\n\n## å‰ç«¯ç•Œé¢\n\n![ç•Œé¢ 1](img/1.png)\n![ç•Œé¢ 2](img/2.png)\n![ç•Œé¢ 3](img/3.png)\n![ç•Œé¢ 4](img/4.png)\n![ç•Œé¢ 5](img/5.png)\n![ç•Œé¢ 6](img/6.png)\n\n## é€šè¿‡Docker éƒ¨ç½²(ç›®å‰æœ‰Bug)\n\n1. å¤åˆ¶é…ç½®ï¼š`cp env.example.yaml env.yaml`ï¼Œå¡«å¥½å„ä¸ª Keyã€‚å®¹å™¨å†…å»ºè®®æŠŠ `Default-Project-Root` è®¾ä¸º `/data/projects`ï¼ˆä¼šè¢«æ˜ å°„åˆ°æœ¬åœ° `./data` ç›®å½•ï¼Œæ–¹ä¾¿æŒä¹…åŒ–ï¼‰ã€‚\n2. ä¸€é”®å¯åŠ¨ï¼š`docker compose up -d --build`ã€‚é¦–æ¬¡ä¼šè‡ªåŠ¨æ„å»ºã€‚\n3. æ‰“å¼€ `http://localhost:8765` ä½¿ç”¨ã€‚æŸ¥çœ‹æ—¥å¿—å¯ç”¨ `docker compose logs -f video-workstation`ã€‚\n4. å®¹å™¨æ˜¯æ— æ¡Œé¢ç¯å¢ƒï¼Œâ€œæ‰“å¼€é¡¹ç›®ç›®å½•/æ‰“å¼€TTSæ–‡ä»¶å¤¹â€ç­‰æŒ‰é’®ä¸ä¼šå¼¹å‡ºæ–‡ä»¶ç®¡ç†å™¨ï¼Œæ¥å£ä¼šç›´æ¥è¿”å›è·¯å¾„ï¼›è¯·åœ¨å®¿ä¸»æœºæ‰‹åŠ¨è¿›å…¥å¯¹åº”ç›®å½•ï¼ˆé»˜è®¤æŒ‚è½½åœ¨å½“å‰ä»“åº“çš„ `./data`ï¼‰ã€‚\n\n> nodeå¦‚æœæ‹‰ä¸ä¸‹æ¥ï¼Œæ¨èå…ˆä½¿ç”¨ `docker pull node:20-alpine` , å†è¿è¡Œ `docker compose up -d --build` \n\nå¦‚æœä¸æƒ³ç”¨ Composeï¼Œä¹Ÿå¯ä»¥ç”¨å•æ¡å‘½ä»¤è¿è¡Œé•œåƒï¼ˆéœ€è¦å…ˆ `docker build -t video-workstation .`ï¼‰ï¼š\n`docker run -d -p 8765:8765 -v $(pwd)/env.yaml:/app/env.yaml:ro -v $(pwd)/data:/data --name video-workstation video-workstation`\n\n## é€šè¿‡æºç éƒ¨ç½²\n\n1. å¤åˆ¶ `env.example.yaml` ä¸º `env.yaml`ï¼Œå¡«å…¥è‡ªå·±çš„ Gemini Keyã€Base URLã€æ¨¡å‹ã€TTS Key ä¸æç¤ºè¯ç­‰é…ç½®ï¼Œå¦åˆ™æ— æ³•è°ƒç”¨æ¥å£ã€‚\n2. ï¼ˆå¯é€‰ï¼‰åœ¨ `env.yaml` ä¸­è®¾ç½® `Default-Project-Root`ï¼Œç”¨äºå­˜æ”¾è‡ªåŠ¨ç”Ÿæˆçš„è„šæœ¬ã€éŸ³é¢‘ä¸å›¾ç‰‡æ–‡ä»¶ã€‚\n3. å®‰è£…ä¾èµ–ï¼š`npm install`ã€‚\n4. å¯åŠ¨æœåŠ¡ï¼š`npm start` æˆ–ç›´æ¥åŒå‡» `start.bat`ï¼Œé»˜è®¤è®¿é—®åœ°å€ä¸º `http://localhost:8765`ã€‚\n\n## åŠŸèƒ½ä»‹ç»\n\n1. **é¡¹ç›®æ€»è§ˆ**ï¼šä»¥å¡ç‰‡å½¢å¼ç®¡ç†æ‰¹é‡é¡¹ç›®ï¼Œæ˜¾ç¤ºè¾“å‡ºç›®å½•ã€åˆ›å»ºæ—¶é—´åŠåˆ é™¤åŠ¨ä½œï¼Œä¾¿äºå¿«é€Ÿå®šä½ã€‚\n2. **æ–‡æ¡ˆç”Ÿæˆ**ï¼šç»“æ„åŒ–å±•ç¤ºåœºæ™¯è„šæœ¬ï¼Œå¯å¤åˆ¶å•æ¡æˆ–æ•´æ®µæ–‡æ¡ˆï¼Œå·¦ä¾§å‹¾é€‰è”åŠ¨å³ä¾§æç¤ºè¯ã€‚\n3. **å­—å¹•è·å–**: éœ€é…åˆæˆ‘çš„å¦ä¸€ä¸ªé¡¹ç›®(n8n-http-tools): å¼€æºåœ°å€:[n8n-http-tools](https://github.com/Norsico/n8n-http-tools)\n4. **TTS åˆæˆ**ï¼šæ”¯æŒå•æ¡ä¸æ‰¹é‡ä¸¤ç§æ¨¡å¼ï¼Œè¾“å…¥åˆæˆæ–‡æœ¬ä¸æƒ…æ„Ÿæç¤ºå³å¯ç”Ÿæˆè¯­éŸ³ã€‚\n5. **å›¾ç‰‡ç”Ÿæˆ**ï¼šé›†ä¸­ç®¡ç†è§’è‰²æè¿°ã€åœºæ™¯æè¿°ç­‰æç¤ºè¯ï¼Œå‹¾é€‰åå³å¯æ‰¹é‡å¤åˆ¶åˆ°ç»˜å›¾ä»»åŠ¡ã€‚\n6. **ç«‹ç»˜/èƒŒæ™¯ç­‰ç”Ÿæˆ**ï¼šæä¾›æç¤ºè¯è¾“å…¥ã€å‚è€ƒå›¾ä¸Šä¼ ã€å®½é«˜æ¯”è®¾ç½®ä¸å†å²è®°å½•ï¼Œæ–¹ä¾¿éšæ—¶å¤ç”¨ç´ æã€‚\n7. **é€†å‘æ¥å£å®ç°ASRè‡ªåŠ¨æå–å‰ªè¾‘éœ€è¦çš„å­—å¹•æ–‡ä»¶**ï¼šåœ¨TTSåˆæˆç•Œé¢ä¸‹æ–¹ï¼Œæœ‰â€œå­—å¹•ç”Ÿæˆâ€åŠŸèƒ½ï¼Œç‚¹å‡»ä¸‹æ–¹çš„æŒ‰é’®å¯ä»¥æ‰“å¼€å­—å¹•ç”Ÿæˆå·¥å…·ã€‚æ­¤éƒ¨åˆ†ä»£ç ç”±å…¶å®ƒä½œè€…å¼€æºã€‚\n8. **å¸¸ç”¨æç¤ºè¯ä¸è‡ªç”±åˆ›ä½œ**ï¼šæ”¶è—é«˜é¢‘æç¤ºè¯å¹¶ä¸€é”®å¤åˆ¶ï¼ŒåŒæ—¶æä¾›è‡ªç”±åˆ›ä½œé¢æ¿è¿›è¡Œè‡ªå®šä¹‰ç»˜åˆ¶ã€‚\n\n### å…¶å®ƒåŠŸèƒ½æˆ‘å°±æ‡’å¾—ä¸€ä¸ªä¸€ä¸ªå†™äº†ï¼Œå…·ä½“æœ‰å•¥è‡ªå·±å¯ä»¥éƒ¨ç½²ä¸€ä¸‹å»ç©ï¼Œæ³¨æ„æ–‡æ¡ˆç”Ÿæˆè¿™é‡Œéœ€è¦é…åˆn8næ¥æ“ä½œï¼Œä¹‹å‰å†™çš„n8næ–‡ä»¶æ‰¾ä¸åˆ°äº†ï¼Œæ‰€ä»¥è¿™éƒ¨åˆ†å…¶å®å¯ä»¥å¿½ç•¥ï¼Œä¸»è¦å°±æ˜¯ä¸€ä¸ªç”¨äºç”Ÿæˆæ–‡æ¡ˆçš„è„šæœ¬AIæç¤ºè¯ä»¥åŠæˆ‘ä¸»é¡µå¦ä¸€ä¸ªä»“åº“ä¸­æœ‰çš„ä¸€ä¸ªå¼€æºçš„Bç«™è§†é¢‘å­—å¹•æå–å™¨ï¼ˆå½“ç„¶ç½‘ä¸Šä¹Ÿæœ‰ï¼‰ï¼ˆå‚è€ƒåˆ«äººé«˜æ’­æ”¾çš„è§†é¢‘è‡ªå·±å­¦èµ·æ¥ä¹Ÿä¼šå¿«å¾ˆå¤šï¼‰\n\n## æ¥ä¸‹æ¥å¦‚ä½•å¥½å¥½åˆ©ç”¨è¿™ä¸ªé¡¹ç›®è¿˜æ˜¯å¾—é è‡ªå·±ã€‚\n### å› ä¸ºä¸»è¦è¿˜æ˜¯åå‘ç®¡ç†ç”¨çš„ï¼ˆç®€å•æ¥è®²å°±æ˜¯åŠŸèƒ½ä¸ä¼šæœ‰ä½ æƒ³è±¡çš„é‚£ä¹ˆå®ç”¨ï¼‰ï¼Œè§†é¢‘å†…å®¹å¦‚ä½•å®šä¹‰ï¼Œå¦‚ä½•æ‰“é€ çˆ†æ¬¾è¿˜æ˜¯éœ€è¦åŠ¨è„‘å­ã€‚å½“ç„¶æœ¬é¡¹ç›®é‡Œé¢ä½¿ç”¨å›¾åƒç¼–è¾‘æ¨¡å‹çš„æ˜¯NanoBananaï¼Œæœ¬åœ°éƒ¨ç½²çš„AIStudioçš„åå‘ä»£ç†çš„æ¥å£ï¼Œç”¨æ¥ç”Ÿå›¾ç„¶åç»™Soraä¹Ÿæ˜¯ä¸é”™çš„ï¼Œèµ·ç æµ‹è¯•ä¸‹æ¥æ¯”è¾ƒç¨³å®šã€‚\n\n## Star History\n\n<a href=\"https://www.star-history.com/#Norsico/Video-Materials-AutoGEN-Workstation&type=date&legend=bottom-right\">\n <picture>\n   <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://api.star-history.com/svg?repos=Norsico/Video-Materials-AutoGEN-Workstation&type=date&theme=dark&legend=bottom-right\" />\n   <source media=\"(prefers-color-scheme: light)\" srcset=\"https://api.star-history.com/svg?repos=Norsico/Video-Materials-AutoGEN-Workstation&type=date&legend=bottom-right\" />\n   <img alt=\"Star History Chart\" src=\"https://api.star-history.com/svg?repos=Norsico/Video-Materials-AutoGEN-Workstation&type=date&legend=bottom-right\" />\n </picture>\n</a>\n\n## å…è´£å£°æ˜\n\n### é¡¹ç›®ä»…å…±å‚è€ƒäº¤æµå­¦ä¹ ä½¿ç”¨ï¼Œä¸å¯¹ä»»ä½•ä½¿ç”¨è€…äº§ç”Ÿçš„é—®é¢˜è´Ÿè´£\n\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-12-07T02:53:30.547994"
  },
  {
    "basic_info": {
      "name": "gmail-cleaner",
      "full_name": "Gururagavendra/gmail-cleaner",
      "owner": "Gururagavendra",
      "description": "web based GUI to cleanup gmail delete, mark as read, unsubsribe from uncessary things u dont like",
      "url": "https://github.com/Gururagavendra/gmail-cleaner",
      "clone_url": "https://github.com/Gururagavendra/gmail-cleaner.git",
      "ssh_url": "git@github.com:Gururagavendra/gmail-cleaner.git",
      "homepage": "https://gururagavendra.github.io/gmail-cleaner/",
      "created_at": "2025-11-29T09:19:54Z",
      "updated_at": "2025-12-07T02:18:13Z",
      "pushed_at": "2025-12-03T09:21:08Z"
    },
    "stats": {
      "stars": 994,
      "forks": 53,
      "watchers": 994,
      "open_issues": 11,
      "size": 19998
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 114276,
        "JavaScript": 78878,
        "HTML": 64248,
        "CSS": 36254,
        "Dockerfile": 720,
        "Procfile": 20
      },
      "license": "MIT License",
      "topics": []
    },
    "content": {
      "readme": "# Gmail Bulk Unsubscribe & Cleanup Tool\n\nA **free**, privacy-focused tool to bulk unsubscribe from emails, delete emails by sender, and mark emails as read. No subscriptions, no data collection - runs 100% on your machine.\n\n[![GitHub Sponsors](https://img.shields.io/badge/Sponsor-%E2%9D%A4-pink?style=flat-square&logo=github-sponsors)](https://github.com/sponsors/Gururagavendra)\n[![Buy Me A Coffee](https://img.shields.io/badge/Buy%20Me%20A%20Coffee-FFDD00?style=flat-square&logo=buy-me-a-coffee&logoColor=black)](https://buymeacoffee.com/gururagavendra)\n\n![Python](https://img.shields.io/badge/Python-3.9+-blue?style=flat-square&logo=python)\n![Docker](https://img.shields.io/badge/Docker-Ready-2496ED?style=flat-square&logo=docker)\n![Gmail API](https://img.shields.io/badge/Gmail-API-EA4335?style=flat-square&logo=gmail)\n![License](https://img.shields.io/badge/License-MIT-green?style=flat-square)\n![GitHub stars](https://img.shields.io/github/stars/Gururagavendra/gmail-cleaner?style=flat-square&logo=github)\n\n> **No Subscription Required - Free Forever**\n\n## Features\n\n| Feature | Description |\n|---------|-------------|\n| **Bulk Unsubscribe** | Find newsletters and unsubscribe with one click |\n| **Delete by Sender** | See who sends you the most emails, delete in bulk |\n| **Mark as Read** | Bulk mark thousands of unread emails as read |\n| **Smart Filters** | Filter by days, size of email, and category (Promotions, Social, Updates) |\n| **Privacy First** | Runs locally - your data never leaves your machine |\n| **Super Fast** | Gmail API with batch requests (100 emails per API call) |\n| **Gmail-style UI** | Clean, familiar interface |\n\n## Platform Support\n\nWorks on **all major platforms** - both Docker and local installation:\n\n| Platform | Docker | Local (Python) |\n|----------|--------|----------------|\n| Linux (x86_64) | Native | Native |\n| Windows (x86_64) | Native | Native |\n| macOS Intel | Native | Native |\n| macOS Apple Silicon (M1/M2/M3/M4) | Native | Native |\n\n## Demo\n\n![Gmail Cleaner Demo](demo.gif)\n\n**[Watch Setup Video on YouTube](https://youtu.be/CmOWn8Tm5ZE)** - Step-by-step video on how to setup the repo and run the project locally.\n\n## Feature Requests\n\nLets make this tool a better one by improving as much as possible, All features are welcome, To request a feature, [open a GitHub issue](https://github.com/Gururagavendra/gmail-cleaner/issues/new).\n\n## Prerequisites\n\n- **Docker**: [Docker Desktop](https://www.docker.com/products/docker-desktop/)\n- **Local (Python)**: [Python 3.9+](https://www.python.org/downloads/) and [uv](https://docs.astral.sh/uv/getting-started/installation/)\n\n## Setup\n\n**Important**: You must create your **OWN** Google Cloud credentials. This app doesn't include pre-configured OAuth - that's what makes it privacy-focused! Each user runs their own instance with their own credentials.\n\n### 1. Get Google OAuth Credentials\n\n**Video Tutorial**: [Watch on YouTube](https://youtu.be/CmOWn8Tm5ZE) for a visual walkthrough\n\n1. Go to [Google Cloud Console](https://console.cloud.google.com/)\n2. Create a new project (or select existing)\n3. Search for **\"Gmail API\"** and **Enable** it\n4. Go to **Google Auth Platform**  â†’ Click **\"Get started\"**\n5. Fill in the wizard:\n   - **App Information**: Enter app name (e.g., \"Gmail Cleanup\"), select your email\n   - **Audience**: Select **External**\n   - **Contact Information**: Add your email address\n   - Click **Create**\n6. Go to **Audience** (left sidebar) â†’ Scroll to **Test users**\n   - Click **Add Users** â†’ Add your Gmail address â†’ **Save**\n7. Go to **Clients** (left sidebar) â†’ **Create Client**\n   - Choose the application type based on your setup:\n   \n   | Setup | Application Type | Redirect URI |\n   |-------|------------------|--------------|\n   | **Local/Desktop** (Python with browser) | Desktop app | Not needed |\n   | **Docker/Remote Server** | Web application | `http://YOUR_HOST:8767/` |\n   \n   - Name: \"Gmail Cleanup\" (or anything)\n   - Click **Create**\n   - Click **Download** (downloads JSON file)\n   - Rename the downloaded file to `credentials.json`\n\n> **ğŸ’¡ Which should I choose?**\n> - Running locally with Python (`uv run python main.py`)? â†’ **Desktop app**\n> - Running with Docker or on a remote server? â†’ **Web application**\n\n### 2. Clone the Repository\n\n1. Clone the repo:\n```bash\ngit clone https://github.com/Gururagavendra/gmail-cleaner.git\n```\n\n2. Navigate to the folder:\n```bash\ncd gmail-cleaner\n```\n\n3. Put your `credentials.json` file in the project folder.\n\n## Usage\n\n### Option A: Docker (Recommended)\n\n1. Start the container:\n```bash\ndocker compose up -d\n```\n\n2. Open the app in your browser:\n```\nhttp://localhost:8766\n```\n\n3. Click **\"Sign In\"** button in the web UI\n\n4. Check logs for the OAuth URL (only after clicking Sign In!):\n```bash\ndocker logs $(docker ps -q --filter ancestor=ghcr.io/gururagavendra/gmail-cleaner)\n```\nOr if you built locally:\n```bash\ndocker logs $(docker ps -q --filter name=gmail-cleaner)\n```\n\n5. Copy the Google OAuth URL from lo",
      "default_branch": "main"
    },
    "fetched_at": "2025-12-07T02:53:31.827543"
  },
  {
    "basic_info": {
      "name": "dia2",
      "full_name": "nari-labs/dia2",
      "owner": "nari-labs",
      "description": "TTS model capable of streaming conversational audio in realtime.",
      "url": "https://github.com/nari-labs/dia2",
      "clone_url": "https://github.com/nari-labs/dia2.git",
      "ssh_url": "git@github.com:nari-labs/dia2.git",
      "homepage": "",
      "created_at": "2025-11-17T19:04:12Z",
      "updated_at": "2025-12-06T21:40:25Z",
      "pushed_at": "2025-11-29T00:51:56Z"
    },
    "stats": {
      "stars": 878,
      "forks": 75,
      "watchers": 878,
      "open_issues": 3,
      "size": 4528
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 116055
      },
      "license": "Apache License 2.0",
      "topics": [
        "open-weight",
        "text-to-speech"
      ]
    },
    "content": {
      "readme": "![Banner](banner.gif)\n\n<div align=\"center\">\n  <a href=\"https://huggingface.co/nari-labs/Dia2-2B\"><img src=\"https://img.shields.io/badge/HF%20Repo-Dia2--2B-orange?style=for-the-badge\"></a>\n  <a href=\"https://discord.gg/bJq6vjRRKv\"><img src=\"https://img.shields.io/badge/Discord-Join%20Chat-7289DA?logo=discord&style=for-the-badge\"></a>\n  <a href=\"https://github.com/nari-labs/dia2/blob/main/LICENSE\"><img src=\"https://img.shields.io/badge/License-Apache_2.0-blue.svg?style=for-the-badge\"></a>\n</div>\n\n\n**Dia2** is a **streaming dialogue TTS model** created by Nari Labs.\n\nThe model does not need the entire text to produce the audio, and can start generating as the first few words are given as input. You can condition the output on audio, enabling natural conversations in realtime.\n\nWe provide model checkpoints (1B, 2B) and inference code to accelerate research. The model only supports up to 2 minutes of generation in English.\n\nâš ï¸ Quality and voices vary per generation, as the model is not fine-tuned on a specific voice. Use with prefix or fine-tune in order to obtain stable output.\n\nTry it now on Hugging Face [Spaces](https://huggingface.co/spaces/nari-labs/Dia2-2B)\n\n## Upcoming\n\n- Bonsai (JAX) implementation\n- Dia2 TTS Server: Real streaming support\n- Sori: Dia2-powered speech-to-speech engine written in Rust\n\n## Quickstart\n\n> **Requirement** â€” install [uv](https://docs.astral.sh/uv/) and use CUDA 12.8+\n> drivers. All commands below run through `uv run â€¦` as a rule.\n\n1. **Install dependencies (one-time):**\n   ```bash\n   uv sync\n   ```\n2. **Prepare a script:** edit `input.txt` using `[S1]` / `[S2]` speaker tags.\n3. **Generate audio:**\n   ```bash\n   uv run -m dia2.cli \\\n     --hf nari-labs/Dia2-2B \\\n     --input input.txt \\\n     --cfg 6.0 --temperature 0.8 \\\n     --cuda-graph --verbose \\\n     output.wav\n   ```\n   The first run downloads weights/tokenizer/Mimi. The CLI auto-selects CUDA when available (otherwise CPU) and defaults to bfloat16 precisionâ€”override with `--device` / `--dtype` if needed.\n4. **Conditional Generation (recommended for stable use):**\n   ```bash\n   uv run -m dia2.cli \\\n     --hf nari-labs/Dia2-2B \\\n     --input input.txt \\\n     --prefix-speaker-1 example_prefix1.wav \\\n     --prefix-speaker-2 example_prefix2.wav \\\n     --cuda-graph --verbose \\\n     output_conditioned.wav\n   ```\n   Condition the generation on previous conversational context in order to generate natural output for your speech-to-speech system. For example, place the voice of your assistant as prefix speaker 1, place user's audio input as prefix speaker 2, and generate the response to user's input.\n\n   Whisper is used to transcribe each prefix file, which takes additional time. We include example prefix files as `example_prefix1.wav` and `example_prefix2.wav` (both files are output created by the model).\n6. **Gradio for Easy Usage**\n   ```bash\n   uv run gradio_app.py\n   ```\n\n### Programmatic Usage\n```python\nfrom dia2 import Dia2, GenerationConfig, SamplingConfig\n\ndia = Dia2.from_repo(\"nari-labs/Dia2-2B\", device=\"cuda\", dtype=\"bfloat16\")\nconfig = GenerationConfig(\n    cfg_scale=2.0,\n    audio=SamplingConfig(temperature=0.8, top_k=50),\n    use_cuda_graph=True,\n)\nresult = dia.generate(\"[S1] Hello Dia2!\", config=config, output_wav=\"hello.wav\", verbose=True)\n```\nGeneration runs until the runtime config's `max_context_steps` (1500, 2 minutes)\nor until EOS is detected. `GenerationResult` includes audio tokens, waveform tensor,\nand word timestamps relative to Mimiâ€™s ~12.5 Hz frame rate.\n\n## Hugging Face\n\n| Variant | Repo |\n| --- | --- |\n| Dia2-1B | [`nari-labs/Dia2-1B`](https://huggingface.co/nari-labs/Dia2-1B)\n| Dia2-2B | [`nari-labs/Dia2-2B`](https://huggingface.co/nari-labs/Dia2-2B)\n\n## License & Attribution\n\nLicensed under [Apache 2.0](LICENSE). All third-party assets (Kyutai Mimi codec, etc.) retain their original licenses.\n\n## Disclaimer\n\nThis project offers a high-fidelity speech generation model intended for research and educational use. The following uses are **strictly forbidden**:\n\n- **Identity Misuse**: Do not produce audio resembling real individuals without permission.\n- **Deceptive Content**: Do not use this model to generate misleading content (e.g. fake news)\n- **Illegal or Malicious Use**: Do not use this model for activities that are illegal or intended to cause harm.\n\nBy using this model, you agree to uphold relevant legal standards and ethical responsibilities. We **are not responsible** for any misuse and firmly oppose any unethical usage of this technology.\n\n## Acknowledgements\n- We thank the [TPU Research Cloud](https://sites.research.google/trc/about/) program for providing compute for training.\n- Our work was heavily inspired by [KyutaiTTS](https://kyutai.org/next/tts) and [Sesame](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice)\n\n---\nQuestions? Join our [Discord](https://discord.gg/bJq6vjRRKv) or open an issue.\n",
      "default_branch": "main"
    },
    "fetched_at": "2025-12-07T02:53:33.101897"
  },
  {
    "basic_info": {
      "name": "react2shell-scanner",
      "full_name": "assetnote/react2shell-scanner",
      "owner": "assetnote",
      "description": "High Fidelity Detection Mechanism for RSC/Next.js RCE (CVE-2025-55182 & CVE-2025-66478)",
      "url": "https://github.com/assetnote/react2shell-scanner",
      "clone_url": "https://github.com/assetnote/react2shell-scanner.git",
      "ssh_url": "git@github.com:assetnote/react2shell-scanner.git",
      "homepage": null,
      "created_at": "2025-12-04T06:55:04Z",
      "updated_at": "2025-12-07T02:51:04Z",
      "pushed_at": "2025-12-05T22:31:06Z"
    },
    "stats": {
      "stars": 803,
      "forks": 90,
      "watchers": 803,
      "open_issues": 6,
      "size": 36
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 23911
      },
      "license": null,
      "topics": []
    },
    "content": {
      "readme": "# react2shell-scanner\n\nA command-line tool for detecting CVE-2025-55182 and CVE-2025-66478 in Next.js applications using React Server Components.\n\nFor technical details on the vulnerability and detection methodology, see our blog post: https://slcyber.io/research-center/high-fidelity-detection-mechanism-for-rsc-next-js-rce-cve-2025-55182-cve-2025-66478\n\n## How It Works\n\nBy default, the scanner sends a crafted multipart POST request containing an RCE proof-of-concept payload that executes a deterministic math operation (`41*271 = 11111`). Vulnerable hosts return the result in the `X-Action-Redirect` response header as `/login?a=11111`.\n\nThe scanner tests the root path first. If not vulnerable, it follows same-host redirects (e.g., `/` to `/en/`) and tests the redirect destination. Cross-origin redirects are not followed.\n\n### Safe Check Mode\n\nThe `--safe-check` flag uses an alternative detection method that relies on side-channel indicators (500 status code with specific error digest) without executing code on the target. Use this mode when RCE execution is not desired.\n\n### WAF Bypass\n\nThe `--waf-bypass` flag prepends random junk data to the multipart request body. This can help evade WAF content inspection that only analyzes the first portion of request bodies. The default size is 128KB, configurable via `--waf-bypass-size`. When WAF bypass is enabled, the timeout is automatically increased to 20 seconds (unless explicitly set).\n\n### Vercel WAF Bypass\n\nThe `--vercel-waf-bypass` flag uses an alternative payload variant specifically designed to bypass Vercel WAF protections. This uses a different multipart structure with an additional form field.\n\n### Windows Mode\n\nThe `--windows` flag switches the payload from Unix shell (`echo $((41*271))`) to PowerShell (`powershell -c \"41*271\"`) for targets running on Windows.\n\n## Requirements\n\n- Python 3.9+\n- requests\n- tqdm\n\n## Installation\n\n```\npip install -r requirements.txt\n```\n\n## Usage\n\nScan a single host:\n\n```\npython3 scanner.py -u https://example.com\n```\n\nScan a list of hosts:\n\n```\npython3 scanner.py -l hosts.txt\n```\n\nScan with multiple threads and save results:\n\n```\npython3 scanner.py -l hosts.txt -t 20 -o results.json\n```\n\nScan with custom headers:\n\n```\npython3 scanner.py -u https://example.com -H \"Authorization: Bearer token\" -H \"Cookie: session=abc\"\n```\n\nUse safe side-channel detection:\n\n```\npython3 scanner.py -u https://example.com --safe-check\n```\n\nScan Windows targets:\n\n```\npython3 scanner.py -u https://example.com --windows\n```\n\nScan with WAF bypass:\n\n```\npython3 scanner.py -u https://example.com --waf-bypass\n```\n\n## Options\n\n```\n-u, --url         Single URL to check\n-l, --list        File containing hosts (one per line)\n-t, --threads     Number of concurrent threads (default: 10)\n--timeout         Request timeout in seconds (default: 10)\n-o, --output      Output file for results (JSON)\n--all-results     Save all results, not just vulnerable hosts\n-k, --insecure    Disable SSL certificate verification\n-H, --header      Custom header (can be used multiple times)\n-v, --verbose     Show response details for vulnerable hosts\n-q, --quiet       Only output vulnerable hosts\n--no-color        Disable colored output\n--safe-check      Use safe side-channel detection instead of RCE PoC\n--windows         Use Windows PowerShell payload instead of Unix shell\n--waf-bypass      Add junk data to bypass WAF content inspection\n--waf-bypass-size Size of junk data in KB (default: 128)\n```\n\n## Credits\n\nThe RCE PoC was originally disclosed by [@maple3142](https://x.com/maple3142) -- we are incredibly grateful for their work in publishing a working PoC.\n\nThis tooling originally was built out as a safe way to detect the RCE. This functionality is still available via `--safe-check`, the \"safe detection\" mode.\n\n- Assetnote Security Research Team - [Adam Kues, Tomais Williamson, Dylan Pindur, Patrik GrobshÃ¤user, Shubham Shah](https://x.com/assetnote)\n- [xEHLE_](https://x.com/xEHLE_) - RCE output reflection in resp header\n- [Nagli](https://x.com/galnagli)\n\n## Output\n\nResults are printed to the terminal. When using `-o`, vulnerable hosts are saved to a JSON file containing the full HTTP request and response for verification.\n",
      "default_branch": "master"
    },
    "fetched_at": "2025-12-07T02:53:34.413440"
  },
  {
    "basic_info": {
      "name": "f1-race-replay",
      "full_name": "IAmTomShaw/f1-race-replay",
      "owner": "IAmTomShaw",
      "description": null,
      "url": "https://github.com/IAmTomShaw/f1-race-replay",
      "clone_url": "https://github.com/IAmTomShaw/f1-race-replay.git",
      "ssh_url": "git@github.com:IAmTomShaw/f1-race-replay.git",
      "homepage": null,
      "created_at": "2025-11-21T17:37:05Z",
      "updated_at": "2025-12-07T01:38:54Z",
      "pushed_at": "2025-12-06T20:14:33Z"
    },
    "stats": {
      "stars": 802,
      "forks": 114,
      "watchers": 802,
      "open_issues": 14,
      "size": 377
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 101469
      },
      "license": null,
      "topics": []
    },
    "content": {
      "readme": "# F1 Race Replay ğŸï¸ ğŸ\n\nA Python application for visualizing Formula 1 race telemetry and replaying race events with interactive controls and a graphical interface.\n\n![Race Replay Preview](./resources/preview.png)\n\n## Features\n\n- **Race Replay Visualization:** Watch the race unfold with real-time driver positions on a rendered track.\n- **Leaderboard:** See live driver positions and current tyre compounds.\n- **Lap & Time Display:** Track the current lap and total race time.\n- **Driver Status:** Drivers who retire or go out are marked as \"OUT\" on the leaderboard.\n- **Interactive Controls:** Pause, rewind, fast forward, and adjust playback speed using on-screen buttons or keyboard shortcuts.\n- **Legend:** On-screen legend explains all controls.\n- **Driver Telemetry Insights:** View speed, gear, DRS status, and current lap for selected drivers when selected on the leaderboard.\n\n## Controls\n\n- **Pause/Resume:** SPACE or Pause button\n- **Rewind/Fast Forward:** â† / â†’ or Rewind/Fast Forward buttons\n- **Playback Speed:** â†‘ / â†“ or Speed button (cycles through 0.5x, 1x, 2x, 4x)\n- **Set Speed Directly:** Keys 1â€“4\n\n## Qualifying Session Support (in development)\n\nRecently added support for Qualifying session replays with telemetry visualization including speed, gear, throttle, and brake over the lap distance. This feature is still being refined.\n\n## Requirements\n\n- Python 3.8+\n- [FastF1](https://github.com/theOehrly/Fast-F1)\n- [Arcade](https://api.arcade.academy/en/latest/)\n- numpy\n\nInstall dependencies:\n```bash\npip install -r requirements.txt\n```\n\nFastF1 cache folder will be created automatically on first run. If it is not created, you can manually create a folder named `.fastf1-cache` in the project root.\n\n## Usage\n\nRun the main script and specify the year and round:\n```bash\npython main.py --year 2025 --round 12\n```\n\nTo run a Sprint session (if the event has one), add `--sprint`:\n```bash\npython main.py --year 2025 --round 12 --sprint\n```\n\nThe application will load a pre-computed telemetry dataset if you have run it before for the same event. To force re-computation of telemetry data, use the `--refresh-data` flag:\n```bash\npython main.py --year 2025 --round 12 --refresh-data\n```\n\n### Qualifying Session Replay\n\nTo run a Qualifying session replay, use the `--qualifying` flag:\n```bash\npython main.py --year 2025 --round 12 --qualifying\n```\n\nTo run a Sprint Qualifying session (if the event has one), add `--sprint`:\n```bash\npython main.py --year 2025 --round 12 --qualifying --sprint\n```\n\n## File Structure\n\n```\nf1-race-replay/\nâ”œâ”€â”€ main.py                    # Entry point, handles session loading and starts the replay\nâ”œâ”€â”€ requirements.txt           # Python dependencies\nâ”œâ”€â”€ README.md                  # Project documentation\nâ”œâ”€â”€ roadmap.md                 # Planned features and project vision\nâ”œâ”€â”€ resources/\nâ”‚   â””â”€â”€ preview.png           # Race replay preview image\nâ”œâ”€â”€ src/\nâ”‚   â”œâ”€â”€ f1_data.py            # Telemetry loading, processing, and frame generation\nâ”‚   â”œâ”€â”€ arcade_replay.py      # Visualization and UI logic\nâ”‚   â””â”€â”€ ui_components.py      # UI components like buttons and leaderboard\nâ”‚   â”œâ”€â”€ interfaces/\nâ”‚   â”‚   â””â”€â”€ qualifying.py     # Qualifying session interface and telemetry visualization\nâ”‚   â”‚   â””â”€â”€ race_replay.py    # Race replay interface and telemetry visualization\nâ”‚   â””â”€â”€ lib/\nâ”‚       â””â”€â”€ tyres.py          # Type definitions for telemetry data structures\nâ”‚       â””â”€â”€ time.py           # Time formatting utilities\nâ””â”€â”€ .fastf1-cache/            # FastF1 cache folder (created automatically upon first run)\nâ””â”€â”€ computed_data/            # Computed telemetry data (created automatically upon first run)\n```\n\n## Customization\n\n- Change track width, colors, and UI layout in `src/arcade_replay.py`.\n- Adjust telemetry processing in `src/f1_data.py`.\n\n## Contributing\n\nThere have been serveral contributions from the community that have helped enhance this project. I have added a [contributors.md](./contributors.md) file to acknowledge those who have contributed features and improvements.\n\nIf you would like to contribute, feel free to:\n\n- Open pull requests for UI improvements or new features.\n- Report issues on GitHub.\n\nPlease see [roadmap.md](./roadmap.md) for planned features and project vision.\n\n# Known Issues\n\n- The leaderboard appears to be inaccurate for the first few corners of the race. The leaderboard is also temporarily affected by a driver going in the pits. At the end of the race the leadeboard is sometimes affected by the drivers final x,y positions being further ahead than other drivers. These issues are known issues caused by innacuracies in the telemetry and being worked on for future releases. Its likely that these issues will be fixed in stages as improving the leaderboard accuracy is a complex task.\n\n## ğŸ“ License\n\nThis project is licensed under the MIT License.\n\n## âš ï¸ Disclaimer\n\nNo copyright infringement intended. Formula 1 and related trademarks are the property of their respective owners. All data used is sourced ",
      "default_branch": "main"
    },
    "fetched_at": "2025-12-07T02:53:35.710304"
  },
  {
    "basic_info": {
      "name": "AgentEvolver",
      "full_name": "modelscope/AgentEvolver",
      "owner": "modelscope",
      "description": "AgentEvolver: Towards Efficient Self-Evolving Agent System",
      "url": "https://github.com/modelscope/AgentEvolver",
      "clone_url": "https://github.com/modelscope/AgentEvolver.git",
      "ssh_url": "git@github.com:modelscope/AgentEvolver.git",
      "homepage": "https://modelscope.github.io/AgentEvolver/",
      "created_at": "2025-11-13T08:09:51Z",
      "updated_at": "2025-12-07T02:42:37Z",
      "pushed_at": "2025-12-04T08:47:22Z"
    },
    "stats": {
      "stars": 795,
      "forks": 93,
      "watchers": 795,
      "open_issues": 8,
      "size": 20753
    },
    "tech_info": {
      "language": "Python",
      "languages": {
        "Python": 1173029,
        "Shell": 11430,
        "Dockerfile": 4664
      },
      "license": "Apache License 2.0",
      "topics": [
        "agent",
        "agent-system",
        "llm",
        "reinforcement-learning",
        "self-evolving"
      ]
    },
    "content": {
      "readme": "<p align=\"center\">\n <img src=\"docs/img/logo.png\" alt=\"AgentEvolver Logo\" width=\"70%\">\n</p>\n<h2 align=\"center\">AgentEvolver: Towards Efficient Self-Evolving Agent System</h2>\n\n<!-- --- -->\n\n<p align=\"center\">\n  <!-- <a href=\"https://arxiv.org/abs/0000\"><img src=\"https://img.shields.io/badge/cs.MA-0000-B31C1C?logo=arxiv&logoColor=B31C1C\" alt=\"arxiv\"/></a> -->\n  <a href=\"https://www.python.org/\"><img src=\"https://img.shields.io/badge/python-3.11+-blue\" alt=\"Python Version\"></a>\n  <a href=\"./LICENSE\"><img src=\"https://img.shields.io/badge/license-Apache--2.0-black\" alt=\"License\"></a>\n  <a href=\"https://modelscope.github.io/AgentEvolver/\"><img src=\"https://img.shields.io/badge/docs-online-blue?logo=markdown\" alt=\"Documentation\"></a>\n  <a href=\"https://arxiv.org/abs/2511.10395\"><img src=\"https://img.shields.io/badge/arXiv-2511.10395-b31b1b.svg\" alt=\"arXiv\"></a>\n  <a href=\"https://deepwiki.com/modelscope/AgentEvolver\"><img src=\"https://deepwiki.com/badge.svg\" alt=\"deepwiki\"></a>\n  <a href=\"https://github.com/modelscope/AgentEvolver\"><img src=\"https://img.shields.io/github/stars/modelscope/AgentEvolver?style=social\" alt=\"GitHub Stars\"></a>\n</p>\n\n\n<!-- <p align=\"center\">\n  <strong>AgentEvolver: An Efficient Self-Evolving Agent System</strong><br>\n</p> -->\n\n**AgentEvolver** is an end-to-end, self-evolving training framework that unifies self-questioning, self-navigating, and self-attributing into a cohesive system. It empowers agents to autonomously\nimprove their capabilities, aiming for efficient, cost-effective, and continuous capability evolution.\n\n\n## ğŸ“° News\n- **[2025-12]** ğŸ“¢ New preprint [CuES](https://www.arxiv.org/abs/2512.01311) on an extended self-questioning method released with [code](research/CuES/README.md).\n- **[2025-11]** ğŸ“„ [The AgentEvolver Technical Report is now available](https://arxiv.org/abs/2511.10395), detailing the frameworkâ€™s architecture, methodology, and key findings.\n- **[2025-11]** ğŸ§© AgentEvolver v1 has been released now!\n\n\n## âœ¨ Why AgentEvolver\n\n\n\nğŸ§  AgentEvolver provides three **Self-Evolving Mechanisms** from Environment to Policy:\n\n- **Automatic Task Generation (Self-Questioning)** â€“ Explore the environment and autonomously create diverse tasks, eliminating costly manual dataset construction.\n- **Experience-guided Exploration (Self-Navigating)** â€“ Summarize and reuse cross-task experience, guiding higher-quality rollouts and improving exploration efficiency.\n- **Attribution-based Credit Assignment (Self-Attributing)** â€“ Process long trajectories to uncover the causal contribution of intermediate steps, enabling fine-grained and efficient policy optimization.\n\n<p align=\"center\">\n <img src=\"docs/img/flowchart.png\" alt=\"AgentEvolver Flowchart\" width=\"80%\">\n</p>\n\n\n\n\n## ğŸ”§ Architecture Design\nAgentEvolver adopts a service-oriented dataflow architecture, seamlessly integrating environment sandboxes, LLMs, and experience management into modular services.\n\n<p align=\"center\">\n <img src=\"docs/img/system.png\" alt=\"system framework\" width=\"80%\">\n</p>\n\n\n- **Environment Compatibility** â€“ Standardized interfaces for seamless integration with a wide range of external environments and tool APIs.\n- **Flexible Context Manager** â€“ Built-in utilities for managing multi-turn contexts and complex interaction logic, supporting diverse deployment scenarios.\n- **Modular & Extensible Architecture** â€“ Decoupled components allow easy customization, secondary development, and future algorithm upgrades.\n\n\n## ğŸŒŸ Benchmark Performance\n\nPerformance comparison on the AppWorld and BFCL-v3 benchmarks. AgentEvolver achieves superior results while using substantially fewer parameters than larger baseline models.\n\n<p align=\"center\">\n <img src=\"docs/img/performance.png\" alt=\"Benchmark Performance\" width=\"80%\">\n</p>\n\nPerformance on two benchmarks. Columns show avg@8 and best@8 for each benchmark, plus their averages (Avg.). All values are in percent (%). **Bolded numbers** highlight the best results.\n\n| **Model** | **Params** | **AppWorld** | | **BFCL v3** | | **Avg.** | |\n|:---|:---:|:---:|:---:|:---:|:---:|:---:|:---:|\n| | | avg@8 | best@8 | avg@8 | best@8 | avg@8 | best@8 |\n| Qwen2.5-7B | 7B | 1.8 | 5.6 | 29.8 | 42.4 | 15.8 | 24.0 |\n| +Questioning | 7B | 23.2 | 40.3 | 49.0 | 60.6 | 36.1 | 50.5 |\n| +Questioning&Navigating | 7B | 26.3 | 43.1 | 53.3 | 61.0 | 39.8 | 52.1 |\n| +Questioning&Attributing | 7B | 25.7 | 43.7 | 56.8 | 65.3 | 41.3 | 54.5 |\n| **AgentEvolver (overall)** | **7B** | **32.4** | **51.2** | **57.9** | **69.0** | **45.2** | **60.1** |\n| | | | | | | | |\n| Qwen2.5-14B | 14B | 18.0 | 31.4 | 41.6 | 54.1 | 29.8 | 42.8 |\n| +Questioning | 14B | 44.3 | 65.5 | 60.3 | 72.1 | 52.3 | 68.8 |\n| +Questioning&Navigating | 14B | 45.4 | 65.3 | 62.8 | 74.5 | 54.1 | 69.9 |\n| +Questioning&Attributing | 14B | 47.8 | 65.6 | 64.9 | 76.3 | 56.4 | 71.0 |\n| **AgentEvolver (overall)** | **14B** | **48.7** | **69.4** | **66.5** | **76.7** | **57.6** | **73.1** |\n\n\n## ğŸš€ Quick Start\n### Step 1. Basic Dependency Installation\n\nMake sure you have",
      "default_branch": "main"
    },
    "fetched_at": "2025-12-07T02:53:36.985401"
  }
]